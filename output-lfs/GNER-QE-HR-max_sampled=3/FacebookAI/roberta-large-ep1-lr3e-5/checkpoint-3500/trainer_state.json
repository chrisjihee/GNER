{
  "best_metric": 0.46696855774357215,
  "best_model_checkpoint": "output-lfs/GNER-QE-HR-max_sampled=3/FacebookAI/roberta-large-ep1-lr3e-5/checkpoint-3500",
  "epoch": 0.2779984114376489,
  "eval_steps": 500,
  "global_step": 3500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003971405877680699,
      "grad_norm": 29.07769203186035,
      "learning_rate": 3e-05,
      "loss": 1.8103,
      "step": 50
    },
    {
      "epoch": 0.007942811755361398,
      "grad_norm": 24.590059280395508,
      "learning_rate": 3e-05,
      "loss": 1.3767,
      "step": 100
    },
    {
      "epoch": 0.011914217633042097,
      "grad_norm": 63.349884033203125,
      "learning_rate": 3e-05,
      "loss": 1.2643,
      "step": 150
    },
    {
      "epoch": 0.015885623510722795,
      "grad_norm": 5.586268901824951,
      "learning_rate": 3e-05,
      "loss": 1.1966,
      "step": 200
    },
    {
      "epoch": 0.019857029388403495,
      "grad_norm": 11.481144905090332,
      "learning_rate": 3e-05,
      "loss": 1.2275,
      "step": 250
    },
    {
      "epoch": 0.023828435266084195,
      "grad_norm": 19.53371810913086,
      "learning_rate": 3e-05,
      "loss": 1.1516,
      "step": 300
    },
    {
      "epoch": 0.02779984114376489,
      "grad_norm": 19.823856353759766,
      "learning_rate": 3e-05,
      "loss": 1.1913,
      "step": 350
    },
    {
      "epoch": 0.03177124702144559,
      "grad_norm": 4.258643627166748,
      "learning_rate": 3e-05,
      "loss": 1.1288,
      "step": 400
    },
    {
      "epoch": 0.035742652899126294,
      "grad_norm": 8.112639427185059,
      "learning_rate": 3e-05,
      "loss": 1.1004,
      "step": 450
    },
    {
      "epoch": 0.03971405877680699,
      "grad_norm": 5.2044148445129395,
      "learning_rate": 3e-05,
      "loss": 1.2167,
      "step": 500
    },
    {
      "epoch": 0.03971405877680699,
      "eval_loss": 2.938620090484619,
      "eval_mse": 2.9384300586689993,
      "eval_pearson": 0.41942621147570347,
      "eval_runtime": 38.6312,
      "eval_samples_per_second": 1427.138,
      "eval_spearmanr": 0.4287620265699433,
      "eval_steps_per_second": 11.907,
      "step": 500
    },
    {
      "epoch": 0.043685464654487687,
      "grad_norm": 3.7838988304138184,
      "learning_rate": 3e-05,
      "loss": 1.1852,
      "step": 550
    },
    {
      "epoch": 0.04765687053216839,
      "grad_norm": 8.936241149902344,
      "learning_rate": 3e-05,
      "loss": 1.1918,
      "step": 600
    },
    {
      "epoch": 0.051628276409849086,
      "grad_norm": 8.181702613830566,
      "learning_rate": 3e-05,
      "loss": 1.1491,
      "step": 650
    },
    {
      "epoch": 0.05559968228752978,
      "grad_norm": 9.571146011352539,
      "learning_rate": 3e-05,
      "loss": 1.2263,
      "step": 700
    },
    {
      "epoch": 0.059571088165210485,
      "grad_norm": 14.840932846069336,
      "learning_rate": 3e-05,
      "loss": 1.0883,
      "step": 750
    },
    {
      "epoch": 0.06354249404289118,
      "grad_norm": 11.891220092773438,
      "learning_rate": 3e-05,
      "loss": 1.1513,
      "step": 800
    },
    {
      "epoch": 0.06751389992057188,
      "grad_norm": 24.335979461669922,
      "learning_rate": 3e-05,
      "loss": 1.154,
      "step": 850
    },
    {
      "epoch": 0.07148530579825259,
      "grad_norm": 21.777679443359375,
      "learning_rate": 3e-05,
      "loss": 1.2048,
      "step": 900
    },
    {
      "epoch": 0.07545671167593328,
      "grad_norm": 13.947416305541992,
      "learning_rate": 3e-05,
      "loss": 1.1721,
      "step": 950
    },
    {
      "epoch": 0.07942811755361398,
      "grad_norm": 21.692564010620117,
      "learning_rate": 3e-05,
      "loss": 1.1519,
      "step": 1000
    },
    {
      "epoch": 0.07942811755361398,
      "eval_loss": 2.0107123851776123,
      "eval_mse": 2.0106529939601048,
      "eval_pearson": 0.42896782216417423,
      "eval_runtime": 38.8524,
      "eval_samples_per_second": 1419.01,
      "eval_spearmanr": 0.4311442478337726,
      "eval_steps_per_second": 11.84,
      "step": 1000
    },
    {
      "epoch": 0.08339952343129468,
      "grad_norm": 9.984248161315918,
      "learning_rate": 3e-05,
      "loss": 1.125,
      "step": 1050
    },
    {
      "epoch": 0.08737092930897537,
      "grad_norm": 23.761734008789062,
      "learning_rate": 3e-05,
      "loss": 1.1415,
      "step": 1100
    },
    {
      "epoch": 0.09134233518665608,
      "grad_norm": 21.307907104492188,
      "learning_rate": 3e-05,
      "loss": 1.1999,
      "step": 1150
    },
    {
      "epoch": 0.09531374106433678,
      "grad_norm": 14.275742530822754,
      "learning_rate": 3e-05,
      "loss": 1.0938,
      "step": 1200
    },
    {
      "epoch": 0.09928514694201747,
      "grad_norm": 19.326757431030273,
      "learning_rate": 3e-05,
      "loss": 1.1312,
      "step": 1250
    },
    {
      "epoch": 0.10325655281969817,
      "grad_norm": 9.7329740524292,
      "learning_rate": 3e-05,
      "loss": 1.0733,
      "step": 1300
    },
    {
      "epoch": 0.10722795869737888,
      "grad_norm": 11.256546974182129,
      "learning_rate": 3e-05,
      "loss": 1.1021,
      "step": 1350
    },
    {
      "epoch": 0.11119936457505956,
      "grad_norm": 21.97261619567871,
      "learning_rate": 3e-05,
      "loss": 1.1621,
      "step": 1400
    },
    {
      "epoch": 0.11517077045274027,
      "grad_norm": 9.046419143676758,
      "learning_rate": 3e-05,
      "loss": 1.0826,
      "step": 1450
    },
    {
      "epoch": 0.11914217633042097,
      "grad_norm": 7.419495105743408,
      "learning_rate": 3e-05,
      "loss": 1.1078,
      "step": 1500
    },
    {
      "epoch": 0.11914217633042097,
      "eval_loss": 2.6812829971313477,
      "eval_mse": 2.680927370319015,
      "eval_pearson": 0.444087473294839,
      "eval_runtime": 38.8995,
      "eval_samples_per_second": 1417.295,
      "eval_spearmanr": 0.4484674017283564,
      "eval_steps_per_second": 11.825,
      "step": 1500
    },
    {
      "epoch": 0.12311358220810167,
      "grad_norm": 5.910266876220703,
      "learning_rate": 3e-05,
      "loss": 1.1336,
      "step": 1550
    },
    {
      "epoch": 0.12708498808578236,
      "grad_norm": 5.327118396759033,
      "learning_rate": 3e-05,
      "loss": 1.0957,
      "step": 1600
    },
    {
      "epoch": 0.13105639396346305,
      "grad_norm": 11.582329750061035,
      "learning_rate": 3e-05,
      "loss": 1.104,
      "step": 1650
    },
    {
      "epoch": 0.13502779984114377,
      "grad_norm": 15.32013988494873,
      "learning_rate": 3e-05,
      "loss": 1.1184,
      "step": 1700
    },
    {
      "epoch": 0.13899920571882446,
      "grad_norm": 15.145014762878418,
      "learning_rate": 3e-05,
      "loss": 1.0739,
      "step": 1750
    },
    {
      "epoch": 0.14297061159650518,
      "grad_norm": 13.471181869506836,
      "learning_rate": 3e-05,
      "loss": 1.1244,
      "step": 1800
    },
    {
      "epoch": 0.14694201747418587,
      "grad_norm": 11.650949478149414,
      "learning_rate": 3e-05,
      "loss": 1.0838,
      "step": 1850
    },
    {
      "epoch": 0.15091342335186655,
      "grad_norm": 14.913143157958984,
      "learning_rate": 3e-05,
      "loss": 1.0555,
      "step": 1900
    },
    {
      "epoch": 0.15488482922954727,
      "grad_norm": 13.307653427124023,
      "learning_rate": 3e-05,
      "loss": 1.0419,
      "step": 1950
    },
    {
      "epoch": 0.15885623510722796,
      "grad_norm": 5.686983585357666,
      "learning_rate": 3e-05,
      "loss": 1.0832,
      "step": 2000
    },
    {
      "epoch": 0.15885623510722796,
      "eval_loss": 2.1238110065460205,
      "eval_mse": 2.123850582027906,
      "eval_pearson": 0.4543759124850369,
      "eval_runtime": 38.6003,
      "eval_samples_per_second": 1428.28,
      "eval_spearmanr": 0.46011143184310815,
      "eval_steps_per_second": 11.917,
      "step": 2000
    },
    {
      "epoch": 0.16282764098490865,
      "grad_norm": 9.605429649353027,
      "learning_rate": 3e-05,
      "loss": 1.0516,
      "step": 2050
    },
    {
      "epoch": 0.16679904686258937,
      "grad_norm": 7.390788555145264,
      "learning_rate": 3e-05,
      "loss": 1.0745,
      "step": 2100
    },
    {
      "epoch": 0.17077045274027006,
      "grad_norm": 9.827960968017578,
      "learning_rate": 3e-05,
      "loss": 1.0532,
      "step": 2150
    },
    {
      "epoch": 0.17474185861795075,
      "grad_norm": 7.238013744354248,
      "learning_rate": 3e-05,
      "loss": 1.0689,
      "step": 2200
    },
    {
      "epoch": 0.17871326449563146,
      "grad_norm": 9.535943031311035,
      "learning_rate": 3e-05,
      "loss": 1.0759,
      "step": 2250
    },
    {
      "epoch": 0.18268467037331215,
      "grad_norm": 6.233800888061523,
      "learning_rate": 3e-05,
      "loss": 1.0372,
      "step": 2300
    },
    {
      "epoch": 0.18665607625099284,
      "grad_norm": 20.931596755981445,
      "learning_rate": 3e-05,
      "loss": 1.092,
      "step": 2350
    },
    {
      "epoch": 0.19062748212867356,
      "grad_norm": 21.159305572509766,
      "learning_rate": 3e-05,
      "loss": 1.089,
      "step": 2400
    },
    {
      "epoch": 0.19459888800635425,
      "grad_norm": 10.598812103271484,
      "learning_rate": 3e-05,
      "loss": 1.1008,
      "step": 2450
    },
    {
      "epoch": 0.19857029388403494,
      "grad_norm": 11.811028480529785,
      "learning_rate": 3e-05,
      "loss": 1.0952,
      "step": 2500
    },
    {
      "epoch": 0.19857029388403494,
      "eval_loss": 2.272148370742798,
      "eval_mse": 2.272170848102794,
      "eval_pearson": 0.4061681717873114,
      "eval_runtime": 38.4655,
      "eval_samples_per_second": 1433.284,
      "eval_spearmanr": 0.41027155167935014,
      "eval_steps_per_second": 11.959,
      "step": 2500
    },
    {
      "epoch": 0.20254169976171565,
      "grad_norm": 7.3629255294799805,
      "learning_rate": 3e-05,
      "loss": 1.089,
      "step": 2550
    },
    {
      "epoch": 0.20651310563939634,
      "grad_norm": 7.063347816467285,
      "learning_rate": 3e-05,
      "loss": 1.0674,
      "step": 2600
    },
    {
      "epoch": 0.21048451151707703,
      "grad_norm": 12.66003131866455,
      "learning_rate": 3e-05,
      "loss": 1.0183,
      "step": 2650
    },
    {
      "epoch": 0.21445591739475775,
      "grad_norm": 17.65431785583496,
      "learning_rate": 3e-05,
      "loss": 1.0199,
      "step": 2700
    },
    {
      "epoch": 0.21842732327243844,
      "grad_norm": 12.940313339233398,
      "learning_rate": 3e-05,
      "loss": 1.0336,
      "step": 2750
    },
    {
      "epoch": 0.22239872915011913,
      "grad_norm": 35.45135498046875,
      "learning_rate": 3e-05,
      "loss": 1.0281,
      "step": 2800
    },
    {
      "epoch": 0.22637013502779985,
      "grad_norm": 9.311742782592773,
      "learning_rate": 3e-05,
      "loss": 1.095,
      "step": 2850
    },
    {
      "epoch": 0.23034154090548054,
      "grad_norm": 27.913692474365234,
      "learning_rate": 3e-05,
      "loss": 1.0065,
      "step": 2900
    },
    {
      "epoch": 0.23431294678316125,
      "grad_norm": 16.655370712280273,
      "learning_rate": 3e-05,
      "loss": 1.0317,
      "step": 2950
    },
    {
      "epoch": 0.23828435266084194,
      "grad_norm": 8.765400886535645,
      "learning_rate": 3e-05,
      "loss": 1.0034,
      "step": 3000
    },
    {
      "epoch": 0.23828435266084194,
      "eval_loss": 1.969057321548462,
      "eval_mse": 1.96863358840968,
      "eval_pearson": 0.46434020567793116,
      "eval_runtime": 38.7037,
      "eval_samples_per_second": 1424.464,
      "eval_spearmanr": 0.46555677582256905,
      "eval_steps_per_second": 11.885,
      "step": 3000
    },
    {
      "epoch": 0.24225575853852263,
      "grad_norm": 12.36958122253418,
      "learning_rate": 3e-05,
      "loss": 1.0014,
      "step": 3050
    },
    {
      "epoch": 0.24622716441620335,
      "grad_norm": 5.550976753234863,
      "learning_rate": 3e-05,
      "loss": 1.0092,
      "step": 3100
    },
    {
      "epoch": 0.25019857029388404,
      "grad_norm": 14.161152839660645,
      "learning_rate": 3e-05,
      "loss": 0.9602,
      "step": 3150
    },
    {
      "epoch": 0.2541699761715647,
      "grad_norm": 7.035027027130127,
      "learning_rate": 3e-05,
      "loss": 1.0666,
      "step": 3200
    },
    {
      "epoch": 0.2581413820492454,
      "grad_norm": 6.633437156677246,
      "learning_rate": 3e-05,
      "loss": 1.0334,
      "step": 3250
    },
    {
      "epoch": 0.2621127879269261,
      "grad_norm": 11.46681022644043,
      "learning_rate": 3e-05,
      "loss": 1.0159,
      "step": 3300
    },
    {
      "epoch": 0.26608419380460685,
      "grad_norm": 11.090191841125488,
      "learning_rate": 3e-05,
      "loss": 1.0139,
      "step": 3350
    },
    {
      "epoch": 0.27005559968228754,
      "grad_norm": 8.21738052368164,
      "learning_rate": 3e-05,
      "loss": 1.0044,
      "step": 3400
    },
    {
      "epoch": 0.27402700555996823,
      "grad_norm": 10.110311508178711,
      "learning_rate": 3e-05,
      "loss": 0.9719,
      "step": 3450
    },
    {
      "epoch": 0.2779984114376489,
      "grad_norm": 10.72104263305664,
      "learning_rate": 3e-05,
      "loss": 1.0175,
      "step": 3500
    },
    {
      "epoch": 0.2779984114376489,
      "eval_loss": 2.6004016399383545,
      "eval_mse": 2.5992152564309206,
      "eval_pearson": 0.46696855774357215,
      "eval_runtime": 38.6664,
      "eval_samples_per_second": 1425.839,
      "eval_spearmanr": 0.4790225760486888,
      "eval_steps_per_second": 11.897,
      "step": 3500
    }
  ],
  "logging_steps": 50,
  "max_steps": 12590,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.08751919366144e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
