{"id":"3","dataset":"crossner_ai","split":"test","instance":{"id":"3","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, location, organization, metric, algorithm, country, person, researcher, conference, product, programming language, task, field and O.\nSentence: Unlike neural network s and Support vector machine , the AdaBoost training process selects only those features known to improve the predictive power of the model , reducing dimensionality and potentially improving execution time as irrelevant features need not be computed .","prompt_labels":"Unlike(O) neural(B-algorithm) network(I-algorithm) s(O) and(O) Support(B-algorithm) vector(I-algorithm) machine(I-algorithm) ,(O) the(O) AdaBoost(B-algorithm) training(O) process(O) selects(O) only(O) those(O) features(O) known(O) to(O) improve(O) the(O) predictive(O) power(O) of(O) the(O) model(O) ,(O) reducing(O) dimensionality(O) and(O) potentially(O) improving(O) execution(O) time(O) as(O) irrelevant(O) features(O) need(O) not(O) be(O) computed(O) .(O)","words":["Unlike","neural","network","s","and","Support","vector","machine",",","the","AdaBoost","training","process","selects","only","those","features","known","to","improve","the","predictive","power","of","the","model",",","reducing","dimensionality","and","potentially","improving","execution","time","as","irrelevant","features","need","not","be","computed","."],"labels":["O","B-algorithm","I-algorithm","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","B-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"]},"label_list":["university","location","organization","metric","algorithm","country","person","researcher","conference","product","programming language","task","field"]}
{"id":"7","dataset":"crossner_ai","split":"test","instance":{"id":"7","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, person, task, algorithm, researcher, country, conference, field, location, programming language, organization, metric, university and O.\nSentence: The model is initially fit on a training dataset , The model ( e.g. a neural net or a naive Bayes classifier ) is trained on the training dataset using a supervised learning method , for example using optimization methods such as gradient descent or stochastic gradient descent .","prompt_labels":"The(O) model(O) is(O) initially(O) fit(O) on(O) a(O) training(O) dataset(O) ,(O) The(O) model(O) ((O) e.g.(O) a(O) neural(B-algorithm) net(I-algorithm) or(O) a(O) naive(B-algorithm) Bayes(I-algorithm) classifier(I-algorithm) )(O) is(O) trained(O) on(O) the(O) training(O) dataset(O) using(O) a(O) supervised(B-field) learning(I-field) method(O) ,(O) for(O) example(O) using(O) optimization(O) methods(O) such(O) as(O) gradient(B-algorithm) descent(I-algorithm) or(O) stochastic(B-algorithm) gradient(I-algorithm) descent(I-algorithm) .(O)","words":["The","model","is","initially","fit","on","a","training","dataset",",","The","model","(","e.g.","a","neural","net","or","a","naive","Bayes","classifier",")","is","trained","on","the","training","dataset","using","a","supervised","learning","method",",","for","example","using","optimization","methods","such","as","gradient","descent","or","stochastic","gradient","descent","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","B-algorithm","I-algorithm","I-algorithm","O"]},"label_list":["product","person","task","algorithm","researcher","country","conference","field","location","programming language","organization","metric","university"]}
{"id":"16","dataset":"crossner_ai","split":"test","instance":{"id":"16","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, conference, location, task, organization, country, metric, university, researcher, person, product, field, programming language and O.\nSentence: Engelberger 's most famous co-invention , the Unimate industrial robotic arm , was among the first inductees into the Robot Hall of Fame in 2003 .","prompt_labels":"Engelberger(B-researcher) 's(O) most(O) famous(O) co-invention(O) ,(O) the(O) Unimate(B-product) industrial(I-product) robotic(I-product) arm(I-product) ,(O) was(O) among(O) the(O) first(O) inductees(O) into(O) the(O) Robot(B-location) Hall(I-location) of(I-location) Fame(I-location) in(O) 2003(O) .(O)","words":["Engelberger","'s","most","famous","co-invention",",","the","Unimate","industrial","robotic","arm",",","was","among","the","first","inductees","into","the","Robot","Hall","of","Fame","in","2003","."],"labels":["B-researcher","O","O","O","O","O","O","B-product","I-product","I-product","I-product","O","O","O","O","O","O","O","O","B-location","I-location","I-location","I-location","O","O","O"]},"label_list":["algorithm","conference","location","task","organization","country","metric","university","researcher","person","product","field","programming language"]}
{"id":"55","dataset":"crossner_ai","split":"test","instance":{"id":"55","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, metric, location, conference, task, field, organization, product, programming language, university, country, person, researcher and O.\nSentence: Association for Computational Linguistics , published by","prompt_labels":"Association(B-conference) for(I-conference) Computational(I-conference) Linguistics(I-conference) ,(O) published(O) by(O)","words":["Association","for","Computational","Linguistics",",","published","by"],"labels":["B-conference","I-conference","I-conference","I-conference","O","O","O"]},"label_list":["algorithm","metric","location","conference","task","field","organization","product","programming language","university","country","person","researcher"]}
{"id":"84","dataset":"crossner_ai","split":"test","instance":{"id":"84","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, task, algorithm, conference, location, university, metric, person, researcher, organization, country, field, programming language and O.\nSentence: The WaveNet model proposed in 2016 achieves great performance on speech quality .","prompt_labels":"The(O) WaveNet(B-product) model(O) proposed(O) in(O) 2016(O) achieves(O) great(O) performance(O) on(O) speech(O) quality(O) .(O)","words":["The","WaveNet","model","proposed","in","2016","achieves","great","performance","on","speech","quality","."],"labels":["O","B-product","O","O","O","O","O","O","O","O","O","O","O"]},"label_list":["product","task","algorithm","conference","location","university","metric","person","researcher","organization","country","field","programming language"]}
{"id":"199","dataset":"crossner_ai","split":"test","instance":{"id":"199","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, location, university, organization, country, product, programming language, field, conference, researcher, metric, task, person and O.\nSentence: Modular Audio Recognition Framework ( MARF ) is an open-source research platform and a collection of voice , sound , speech , text and natural language processing ( NLP ) algorithm s written in Java and arranged into a modular and extensible framework that attempts to facilitate addition of new algorithm s .","prompt_labels":"Modular(B-product) Audio(I-product) Recognition(I-product) Framework(I-product) ((O) MARF(B-product) )(O) is(O) an(O) open-source(O) research(O) platform(O) and(O) a(O) collection(O) of(O) voice(O) ,(O) sound(O) ,(O) speech(O) ,(O) text(O) and(O) natural(B-field) language(I-field) processing(I-field) ((O) NLP(B-field) )(O) algorithm(O) s(O) written(O) in(O) Java(B-programming language) and(O) arranged(O) into(O) a(O) modular(O) and(O) extensible(O) framework(O) that(O) attempts(O) to(O) facilitate(O) addition(O) of(O) new(O) algorithm(O) s(O) .(O)","words":["Modular","Audio","Recognition","Framework","(","MARF",")","is","an","open-source","research","platform","and","a","collection","of","voice",",","sound",",","speech",",","text","and","natural","language","processing","(","NLP",")","algorithm","s","written","in","Java","and","arranged","into","a","modular","and","extensible","framework","that","attempts","to","facilitate","addition","of","new","algorithm","s","."],"labels":["B-product","I-product","I-product","I-product","O","B-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","I-field","O","B-field","O","O","O","O","O","B-programming language","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"]},"label_list":["algorithm","location","university","organization","country","product","programming language","field","conference","researcher","metric","task","person"]}
{"id":"304","dataset":"crossner_ai","split":"test","instance":{"id":"304","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, organization, person, metric, location, product, country, researcher, conference, algorithm, task, university, field and O.\nSentence: An example of this would be a variable such as outside temperature ( mathtemp / math ) , which in a given application might be recorded to several decimal places of precision ( depending on the sensing apparatus ) .","prompt_labels":"An(O) example(O) of(O) this(O) would(O) be(O) a(O) variable(O) such(O) as(O) outside(O) temperature(O) ((O) mathtemp(O) /(O) math(O) )(O) ,(O) which(O) in(O) a(O) given(O) application(O) might(O) be(O) recorded(O) to(O) several(O) decimal(O) places(O) of(O) precision(O) ((O) depending(O) on(O) the(O) sensing(O) apparatus(O) )(O) .(O)","words":["An","example","of","this","would","be","a","variable","such","as","outside","temperature","(","mathtemp","/","math",")",",","which","in","a","given","application","might","be","recorded","to","several","decimal","places","of","precision","(","depending","on","the","sensing","apparatus",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"]},"label_list":["programming language","organization","person","metric","location","product","country","researcher","conference","algorithm","task","university","field"]}
{"id":"354","dataset":"crossner_ai","split":"test","instance":{"id":"354","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, university, organization, product, location, researcher, task, metric, conference, field, programming language, algorithm, country and O.\nSentence: Here is an example of R code :","prompt_labels":"Here(O) is(O) an(O) example(O) of(O) R(B-programming language) code(O) :(O)","words":["Here","is","an","example","of","R","code",":"],"labels":["O","O","O","O","O","B-programming language","O","O"]},"label_list":["person","university","organization","product","location","researcher","task","metric","conference","field","programming language","algorithm","country"]}
{"id":"356","dataset":"crossner_ai","split":"test","instance":{"id":"356","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, conference, university, metric, algorithm, field, country, organization, location, programming language, researcher, task, product and O.\nSentence: Research stagnated after machine learning research by Marvin Minsky and Seymour Papert ( 1969 ) ,","prompt_labels":"Research(O) stagnated(O) after(O) machine(B-field) learning(I-field) research(O) by(O) Marvin(B-researcher) Minsky(I-researcher) and(O) Seymour(B-researcher) Papert(I-researcher) ((O) 1969(O) )(O) ,(O)","words":["Research","stagnated","after","machine","learning","research","by","Marvin","Minsky","and","Seymour","Papert","(","1969",")",","],"labels":["O","O","O","B-field","I-field","O","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","O","O","O"]},"label_list":["person","conference","university","metric","algorithm","field","country","organization","location","programming language","researcher","task","product"]}
{"id":"392","dataset":"crossner_ai","split":"test","instance":{"id":"392","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, location, researcher, product, conference, country, metric, task, person, programming language, field, organization, algorithm and O.\nSentence: To illustrate the basic principles of bagging , below is an analysis on the relationship between ozone and temperature ( data from Rousseeuw and Leroy ( 1986 ) , analysis done in R ) .","prompt_labels":"To(O) illustrate(O) the(O) basic(O) principles(O) of(O) bagging(O) ,(O) below(O) is(O) an(O) analysis(O) on(O) the(O) relationship(O) between(O) ozone(O) and(O) temperature(O) ((O) data(O) from(O) Rousseeuw(B-researcher) and(O) Leroy(B-researcher) ((O) 1986(O) )(O) ,(O) analysis(O) done(O) in(O) R(B-programming language) )(O) .(O)","words":["To","illustrate","the","basic","principles","of","bagging",",","below","is","an","analysis","on","the","relationship","between","ozone","and","temperature","(","data","from","Rousseeuw","and","Leroy","(","1986",")",",","analysis","done","in","R",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-researcher","O","B-researcher","O","O","O","O","O","O","O","B-programming language","O","O"]},"label_list":["university","location","researcher","product","conference","country","metric","task","person","programming language","field","organization","algorithm"]}
{"id":"7","dataset":"crossner_literature","split":"test","instance":{"id":"7","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, literary genre, writer, country, poem, magazine, organization, book, award, person, event and O.\nSentence: In late November 1851 , Dickens moved into Tavistock House where he wrote Bleak House ( 1852-53 ) , Hard Times ( 1854 ) , and Little Dorrit ( 1856 ) ..","prompt_labels":"In(O) late(O) November(O) 1851(O) ,(O) Dickens(B-writer) moved(O) into(O) Tavistock(B-location) House(I-location) where(O) he(O) wrote(O) Bleak(B-book) House(I-book) ((O) 1852-53(O) )(O) ,(O) Hard(B-book) Times(I-book) ((O) 1854(O) )(O) ,(O) and(O) Little(B-book) Dorrit(I-book) ((O) 1856(O) )(O) ..(O)","words":["In","late","November","1851",",","Dickens","moved","into","Tavistock","House","where","he","wrote","Bleak","House","(","1852-53",")",",","Hard","Times","(","1854",")",",","and","Little","Dorrit","(","1856",")",".."],"labels":["O","O","O","O","O","B-writer","O","O","B-location","I-location","O","O","O","B-book","I-book","O","O","O","O","B-book","I-book","O","O","O","O","O","B-book","I-book","O","O","O","O"]},"label_list":["location","literary genre","writer","country","poem","magazine","organization","book","award","person","event"]}
{"id":"16","dataset":"crossner_literature","split":"test","instance":{"id":"16","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, writer, literary genre, award, magazine, event, country, book, person, organization, location and O.\nSentence: Her novel Quartet in Autumn ( 1977 ) was nominated for the Booker Prize that year , and she was elected as a Fellow of the Royal Society of Literature .","prompt_labels":"Her(O) novel(B-literary genre) Quartet(B-book) in(I-book) Autumn(I-book) ((O) 1977(O) )(O) was(O) nominated(O) for(O) the(O) Booker(B-award) Prize(I-award) that(O) year(O) ,(O) and(O) she(O) was(O) elected(O) as(O) a(O) Fellow(O) of(O) the(O) Royal(B-organization) Society(I-organization) of(I-organization) Literature(I-organization) .(O)","words":["Her","novel","Quartet","in","Autumn","(","1977",")","was","nominated","for","the","Booker","Prize","that","year",",","and","she","was","elected","as","a","Fellow","of","the","Royal","Society","of","Literature","."],"labels":["O","B-literary genre","B-book","I-book","I-book","O","O","O","O","O","O","O","B-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O"]},"label_list":["poem","writer","literary genre","award","magazine","event","country","book","person","organization","location"]}
{"id":"32","dataset":"crossner_literature","split":"test","instance":{"id":"32","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, poem, country, event, organization, literary genre, writer, book, location, magazine and O.\nSentence: During this part of his career , he was often seen as a rival to the The New Yorker ' s Pauline Kael , who had originally attacked the auteur theory in her essay Circles and Squares .","prompt_labels":"During(O) this(O) part(O) of(O) his(O) career(O) ,(O) he(O) was(O) often(O) seen(O) as(O) a(O) rival(O) to(O) the(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) '(O) s(O) Pauline(B-writer) Kael(I-writer) ,(O) who(O) had(O) originally(O) attacked(O) the(O) auteur(O) theory(O) in(O) her(O) essay(O) Circles(B-book) and(I-book) Squares(I-book) .(O)","words":["During","this","part","of","his","career",",","he","was","often","seen","as","a","rival","to","the","The","New","Yorker","'","s","Pauline","Kael",",","who","had","originally","attacked","the","auteur","theory","in","her","essay","Circles","and","Squares","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","I-magazine","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","O"]},"label_list":["award","person","poem","country","event","organization","literary genre","writer","book","location","magazine"]}
{"id":"101","dataset":"crossner_literature","split":"test","instance":{"id":"101","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, location, event, award, organization, magazine, country, person, literary genre, book, poem and O.\nSentence: In his later writings , some believe that he proposed ideas such as purification of venial sin s after death in purgatory ( The Great Divorce and Letters to Malcolm ) and mortal sin ( The Screwtape Letters ) , which are generally considered to be Roman Catholic teachings , although they are also widely held in Anglicanism ( particularly in high church Anglo-Catholic circles ) .","prompt_labels":"In(O) his(O) later(O) writings(O) ,(O) some(O) believe(O) that(O) he(O) proposed(O) ideas(O) such(O) as(O) purification(O) of(O) venial(O) sin(O) s(O) after(O) death(O) in(O) purgatory(O) ((O) The(B-book) Great(I-book) Divorce(I-book) and(O) Letters(B-book) to(I-book) Malcolm(I-book) )(O) and(O) mortal(O) sin(O) ((O) The(B-book) Screwtape(I-book) Letters(I-book) )(O) ,(O) which(O) are(O) generally(O) considered(O) to(O) be(O) Roman(O) Catholic(O) teachings(O) ,(O) although(O) they(O) are(O) also(O) widely(O) held(O) in(O) Anglicanism(O) ((O) particularly(O) in(O) high(O) church(O) Anglo-Catholic(O) circles(O) )(O) .(O)","words":["In","his","later","writings",",","some","believe","that","he","proposed","ideas","such","as","purification","of","venial","sin","s","after","death","in","purgatory","(","The","Great","Divorce","and","Letters","to","Malcolm",")","and","mortal","sin","(","The","Screwtape","Letters",")",",","which","are","generally","considered","to","be","Roman","Catholic","teachings",",","although","they","are","also","widely","held","in","Anglicanism","(","particularly","in","high","church","Anglo-Catholic","circles",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","O","B-book","I-book","I-book","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"]},"label_list":["writer","location","event","award","organization","magazine","country","person","literary genre","book","poem"]}
{"id":"123","dataset":"crossner_literature","split":"test","instance":{"id":"123","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, poem, magazine, location, writer, event, country, book, person, literary genre and O.\nSentence: However , in late 1970 , Pauline Kael , in her negative The New Yorker review of the Maysles ' subsequent documentary Gimme Shelter , alleged that Salesman was set up and acted by its principals , rather than actually being direct cinema .","prompt_labels":"However(O) ,(O) in(O) late(O) 1970(O) ,(O) Pauline(B-writer) Kael(I-writer) ,(O) in(O) her(O) negative(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) review(O) of(O) the(O) Maysles(B-person) '(O) subsequent(O) documentary(O) Gimme(O) Shelter(O) ,(O) alleged(O) that(O) Salesman(O) was(O) set(O) up(O) and(O) acted(O) by(O) its(O) principals(O) ,(O) rather(O) than(O) actually(O) being(O) direct(O) cinema(O) .(O)","words":["However",",","in","late","1970",",","Pauline","Kael",",","in","her","negative","The","New","Yorker","review","of","the","Maysles","'","subsequent","documentary","Gimme","Shelter",",","alleged","that","Salesman","was","set","up","and","acted","by","its","principals",",","rather","than","actually","being","direct","cinema","."],"labels":["O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","B-magazine","I-magazine","I-magazine","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"]},"label_list":["organization","award","poem","magazine","location","writer","event","country","book","person","literary genre"]}
{"id":"135","dataset":"crossner_literature","split":"test","instance":{"id":"135","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, writer, poem, person, organization, country, magazine, event, location, award, book and O.\nSentence: His books Difference and Repetition ( 1968 ) and The Logic of Sense ( 1969 ) led Michel Foucault to declare that one day , perhaps , this century will be called Deleuzian .","prompt_labels":"His(O) books(O) Difference(B-book) and(I-book) Repetition(I-book) ((O) 1968(O) )(O) and(O) The(B-book) Logic(I-book) of(I-book) Sense(I-book) ((O) 1969(O) )(O) led(O) Michel(B-writer) Foucault(I-writer) to(O) declare(O) that(O) one(O) day(O) ,(O) perhaps(O) ,(O) this(O) century(O) will(O) be(O) called(O) Deleuzian(B-writer) .(O)","words":["His","books","Difference","and","Repetition","(","1968",")","and","The","Logic","of","Sense","(","1969",")","led","Michel","Foucault","to","declare","that","one","day",",","perhaps",",","this","century","will","be","called","Deleuzian","."],"labels":["O","O","B-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","O"]},"label_list":["literary genre","writer","poem","person","organization","country","magazine","event","location","award","book"]}
{"id":"214","dataset":"crossner_literature","split":"test","instance":{"id":"214","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, literary genre, poem, writer, location, book, organization, country, magazine, award and O.\nSentence: Lunar Paraphrase can be read as such a response , despite its mention of religious figures .","prompt_labels":"Lunar(B-poem) Paraphrase(I-poem) can(O) be(O) read(O) as(O) such(O) a(O) response(O) ,(O) despite(O) its(O) mention(O) of(O) religious(O) figures(O) .(O)","words":["Lunar","Paraphrase","can","be","read","as","such","a","response",",","despite","its","mention","of","religious","figures","."],"labels":["B-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"]},"label_list":["event","person","literary genre","poem","writer","location","book","organization","country","magazine","award"]}
{"id":"256","dataset":"crossner_literature","split":"test","instance":{"id":"256","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, event, organization, magazine, person, writer, country, location, book, literary genre, award and O.\nSentence: At the 2012 Pride of Britain Awards shown on ITV on 30 October , Fry , along with Michael Caine , Elton John , Richard Branson and Simon Cowell , recited Rudyard Kipling ' s poem If - in tribute to the 2012 British Olympic and Paralympic athletes .","prompt_labels":"At(O) the(O) 2012(O) Pride(B-award) of(I-award) Britain(I-award) Awards(I-award) shown(O) on(O) ITV(O) on(O) 30(O) October(O) ,(O) Fry(B-person) ,(O) along(O) with(O) Michael(B-person) Caine(I-person) ,(O) Elton(B-person) John(I-person) ,(O) Richard(B-person) Branson(I-person) and(O) Simon(B-person) Cowell(I-person) ,(O) recited(O) Rudyard(B-writer) Kipling(I-writer) '(O) s(O) poem(B-literary genre) If(B-poem) -(I-poem) in(O) tribute(O) to(O) the(O) 2012(O) British(O) Olympic(O) and(O) Paralympic(O) athletes(O) .(O)","words":["At","the","2012","Pride","of","Britain","Awards","shown","on","ITV","on","30","October",",","Fry",",","along","with","Michael","Caine",",","Elton","John",",","Richard","Branson","and","Simon","Cowell",",","recited","Rudyard","Kipling","'","s","poem","If","-","in","tribute","to","the","2012","British","Olympic","and","Paralympic","athletes","."],"labels":["O","O","O","B-award","I-award","I-award","I-award","O","O","O","O","O","O","O","B-person","O","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","O","B-writer","I-writer","O","O","B-literary genre","B-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O"]},"label_list":["poem","event","organization","magazine","person","writer","country","location","book","literary genre","award"]}
{"id":"312","dataset":"crossner_literature","split":"test","instance":{"id":"312","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, organization, person, award, book, poem, country, magazine, event, location, writer and O.\nSentence: Pierre Teilhard de Chardin was born in the Château of Sarcenat , Orcines commune , some 4 km north-west of Clermont-Ferrand , Auvergne , French Third Republic , on 1 May 1881 , as the fourth of eleven children of librarian Emmanuel Teilhard de Chardin ( 1844-1932 ) and of Berthe-Adèle , née de Dompierre d 'Hornoys of Picardy , a great-grandniece of Voltaire .","prompt_labels":"Pierre(B-person) Teilhard(I-person) de(I-person) Chardin(I-person) was(O) born(O) in(O) the(O) Château(B-location) of(I-location) Sarcenat(I-location) ,(O) Orcines(B-location) commune(I-location) ,(O) some(O) 4(O) km(O) north-west(O) of(O) Clermont-Ferrand(B-location) ,(O) Auvergne(B-location) ,(O) French(B-country) Third(I-country) Republic(I-country) ,(O) on(O) 1(O) May(O) 1881(O) ,(O) as(O) the(O) fourth(O) of(O) eleven(O) children(O) of(O) librarian(O) Emmanuel(B-person) Teilhard(I-person) de(I-person) Chardin(I-person) ((O) 1844-1932(O) )(O) and(O) of(O) Berthe-Adèle(B-person) ,(I-person) née(I-person) de(I-person) Dompierre(I-person) d(I-person) 'Hornoys(I-person) of(I-person) Picardy(I-person) ,(O) a(O) great-grandniece(O) of(O) Voltaire(B-writer) .(O)","words":["Pierre","Teilhard","de","Chardin","was","born","in","the","Château","of","Sarcenat",",","Orcines","commune",",","some","4","km","north-west","of","Clermont-Ferrand",",","Auvergne",",","French","Third","Republic",",","on","1","May","1881",",","as","the","fourth","of","eleven","children","of","librarian","Emmanuel","Teilhard","de","Chardin","(","1844-1932",")","and","of","Berthe-Adèle",",","née","de","Dompierre","d","'Hornoys","of","Picardy",",","a","great-grandniece","of","Voltaire","."],"labels":["B-person","I-person","I-person","I-person","O","O","O","O","B-location","I-location","I-location","O","B-location","I-location","O","O","O","O","O","O","B-location","O","B-location","O","B-country","I-country","I-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","I-person","I-person","O","O","O","O","O","B-person","I-person","I-person","I-person","I-person","I-person","I-person","I-person","I-person","O","O","O","O","B-writer","O"]},"label_list":["literary genre","organization","person","award","book","poem","country","magazine","event","location","writer"]}
{"id":"394","dataset":"crossner_literature","split":"test","instance":{"id":"394","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, book, organization, poem, event, location, writer, person, award, literary genre, country and O.\nSentence: 1969 , Crichton also wrote a review for The New Republic ( as J. Michael Crichton ) , critiquing Slaughterhouse Five by Kurt Vonnegut .","prompt_labels":"1969(O) ,(O) Crichton(B-writer) also(O) wrote(O) a(O) review(O) for(O) The(B-magazine) New(I-magazine) Republic(I-magazine) ((O) as(O) J.(B-writer) Michael(I-writer) Crichton(I-writer) )(O) ,(O) critiquing(O) Slaughterhouse(B-book) Five(I-book) by(O) Kurt(B-writer) Vonnegut(I-writer) .(O)","words":["1969",",","Crichton","also","wrote","a","review","for","The","New","Republic","(","as","J.","Michael","Crichton",")",",","critiquing","Slaughterhouse","Five","by","Kurt","Vonnegut","."],"labels":["O","O","B-writer","O","O","O","O","O","B-magazine","I-magazine","I-magazine","O","O","B-writer","I-writer","I-writer","O","O","O","B-book","I-book","O","B-writer","I-writer","O"]},"label_list":["magazine","book","organization","poem","event","location","writer","person","award","literary genre","country"]}
{"id":"29","dataset":"crossner_music","split":"test","instance":{"id":"29","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, person, event, location, organization, award, music genre, band, album, musical instrument, song, country and O.\nSentence: Her charting singles include Crucify , Silent All These Years , God , Cornflake Girl , Caught a Lite Sneeze , Professional Widow , Spark , 1000 Oceans , Flavor and A Sorta Fairytale , her most commercially successful single in the U.S. to date .","prompt_labels":"Her(O) charting(O) singles(O) include(O) Crucify(B-song) ,(O) Silent(B-song) All(I-song) These(I-song) Years(I-song) ,(O) God(B-song) ,(O) Cornflake(B-song) Girl(I-song) ,(O) Caught(B-song) a(I-song) Lite(I-song) Sneeze(I-song) ,(O) Professional(B-song) Widow(I-song) ,(O) Spark(B-song) ,(O) 1000(B-song) Oceans(I-song) ,(O) Flavor(B-song) and(O) A(B-song) Sorta(I-song) Fairytale(I-song) ,(O) her(O) most(O) commercially(O) successful(O) single(O) in(O) the(O) U.S.(B-country) to(O) date(O) .(O)","words":["Her","charting","singles","include","Crucify",",","Silent","All","These","Years",",","God",",","Cornflake","Girl",",","Caught","a","Lite","Sneeze",",","Professional","Widow",",","Spark",",","1000","Oceans",",","Flavor","and","A","Sorta","Fairytale",",","her","most","commercially","successful","single","in","the","U.S.","to","date","."],"labels":["O","O","O","O","B-song","O","B-song","I-song","I-song","I-song","O","B-song","O","B-song","I-song","O","B-song","I-song","I-song","I-song","O","B-song","I-song","O","B-song","O","B-song","I-song","O","B-song","O","B-song","I-song","I-song","O","O","O","O","O","O","O","O","B-country","O","O","O"]},"label_list":["musical artist","person","event","location","organization","award","music genre","band","album","musical instrument","song","country"]}
{"id":"36","dataset":"crossner_music","split":"test","instance":{"id":"36","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, musical instrument, musical artist, song, album, music genre, organization, band, person, country, award, event and O.\nSentence: Duke turned 65 in the spring of 1964 but showed no signs of slowing down as he continued to make vital and innovative recordings , including The Far East Suite ( 1966 ) , New Orleans Suite ( 1970 ) , Latin American Suite ( 1972 ) and The Afro-Eurasian Eclipse ( 1971 ) , much of it inspired by his world tours .","prompt_labels":"Duke(B-musical artist) turned(O) 65(O) in(O) the(O) spring(O) of(O) 1964(O) but(O) showed(O) no(O) signs(O) of(O) slowing(O) down(O) as(O) he(O) continued(O) to(O) make(O) vital(O) and(O) innovative(O) recordings(O) ,(O) including(O) The(B-album) Far(I-album) East(I-album) Suite(I-album) ((O) 1966(O) )(O) ,(O) New(B-album) Orleans(I-album) Suite(I-album) ((O) 1970(O) )(O) ,(O) Latin(B-album) American(I-album) Suite(I-album) ((O) 1972(O) )(O) and(O) The(B-album) Afro-Eurasian(I-album) Eclipse(I-album) ((O) 1971(O) )(O) ,(O) much(O) of(O) it(O) inspired(O) by(O) his(O) world(O) tours(O) .(O)","words":["Duke","turned","65","in","the","spring","of","1964","but","showed","no","signs","of","slowing","down","as","he","continued","to","make","vital","and","innovative","recordings",",","including","The","Far","East","Suite","(","1966",")",",","New","Orleans","Suite","(","1970",")",",","Latin","American","Suite","(","1972",")","and","The","Afro-Eurasian","Eclipse","(","1971",")",",","much","of","it","inspired","by","his","world","tours","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","O","O","O","O","O","O","O","O","O","O","O","O","O"]},"label_list":["location","musical instrument","musical artist","song","album","music genre","organization","band","person","country","award","event"]}
{"id":"69","dataset":"crossner_music","split":"test","instance":{"id":"69","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, album, band, person, award, song, location, country, organization, musical instrument, event, musical artist and O.\nSentence: Today , zydeco integrates genres such as Rhythm and blues , Soul music , brass band , reggae , Hip hop music , ska , Rock music , Afro-Caribbean music and other styles , in addition to the traditional forms .","prompt_labels":"Today(O) ,(O) zydeco(O) integrates(O) genres(O) such(O) as(O) Rhythm(B-music genre) and(I-music genre) blues(I-music genre) ,(O) Soul(B-music genre) music(I-music genre) ,(O) brass(B-music genre) band(I-music genre) ,(O) reggae(B-music genre) ,(O) Hip(B-music genre) hop(I-music genre) music(I-music genre) ,(O) ska(B-music genre) ,(O) Rock(B-music genre) music(I-music genre) ,(O) Afro-Caribbean(B-music genre) music(I-music genre) and(O) other(O) styles(O) ,(O) in(O) addition(O) to(O) the(O) traditional(O) forms(O) .(O)","words":["Today",",","zydeco","integrates","genres","such","as","Rhythm","and","blues",",","Soul","music",",","brass","band",",","reggae",",","Hip","hop","music",",","ska",",","Rock","music",",","Afro-Caribbean","music","and","other","styles",",","in","addition","to","the","traditional","forms","."],"labels":["O","O","O","O","O","O","O","B-music genre","I-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O"]},"label_list":["music genre","album","band","person","award","song","location","country","organization","musical instrument","event","musical artist"]}
{"id":"171","dataset":"crossner_music","split":"test","instance":{"id":"171","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, person, album, song, musical artist, band, award, event, music genre, location, organization, country and O.\nSentence: Many early rock and roll songs are based on blues : That 's All Right Mama , Johnny B. Goode , Blue Suede Shoes , Whole Lotta Shakin ' Goin On , Shake , Rattle , and Roll , and Long Tall Sally .","prompt_labels":"Many(O) early(O) rock(B-music genre) and(I-music genre) roll(I-music genre) songs(O) are(O) based(O) on(O) blues(B-music genre) :(O) That(B-song) 's(I-song) All(I-song) Right(I-song) Mama(I-song) ,(O) Johnny(B-song) B.(I-song) Goode(I-song) ,(O) Blue(B-song) Suede(I-song) Shoes(I-song) ,(O) Whole(B-song) Lotta(I-song) Shakin(I-song) '(I-song) Goin(I-song) On(I-song) ,(O) Shake(B-song) ,(I-song) Rattle(I-song) ,(I-song) and(I-song) Roll(I-song) ,(O) and(O) Long(B-song) Tall(I-song) Sally(I-song) .(O)","words":["Many","early","rock","and","roll","songs","are","based","on","blues",":","That","'s","All","Right","Mama",",","Johnny","B.","Goode",",","Blue","Suede","Shoes",",","Whole","Lotta","Shakin","'","Goin","On",",","Shake",",","Rattle",",","and","Roll",",","and","Long","Tall","Sally","."],"labels":["O","O","B-music genre","I-music genre","I-music genre","O","O","O","O","B-music genre","O","B-song","I-song","I-song","I-song","I-song","O","B-song","I-song","I-song","O","B-song","I-song","I-song","O","B-song","I-song","I-song","I-song","I-song","I-song","O","B-song","I-song","I-song","I-song","I-song","I-song","O","O","B-song","I-song","I-song","O"]},"label_list":["musical instrument","person","album","song","musical artist","band","award","event","music genre","location","organization","country"]}
{"id":"191","dataset":"crossner_music","split":"test","instance":{"id":"191","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, musical instrument, award, location, musical artist, country, music genre, person, song, album, band, event and O.\nSentence: He is also a recipient of the AFI Life Achievement Award for his contributions to the cinema , and has won an Academy Awards , a Palme d 'Or , Cannes Film Festival Best Director Award , Silver Lion , Grammy Award , Emmy Award , Golden Globes , British Academy Film Awards , and Directors Guild of America Award s .","prompt_labels":"He(O) is(O) also(O) a(O) recipient(O) of(O) the(O) AFI(B-award) Life(I-award) Achievement(I-award) Award(I-award) for(O) his(O) contributions(O) to(O) the(O) cinema(O) ,(O) and(O) has(O) won(O) an(O) Academy(B-award) Awards(I-award) ,(O) a(O) Palme(B-award) d(I-award) 'Or(I-award) ,(O) Cannes(B-award) Film(I-award) Festival(I-award) Best(I-award) Director(I-award) Award(I-award) ,(O) Silver(B-award) Lion(I-award) ,(O) Grammy(B-award) Award(I-award) ,(O) Emmy(B-award) Award(I-award) ,(O) Golden(B-award) Globes(I-award) ,(O) British(B-award) Academy(I-award) Film(I-award) Awards(I-award) ,(O) and(O) Directors(B-award) Guild(I-award) of(I-award) America(I-award) Award(I-award) s(O) .(O)","words":["He","is","also","a","recipient","of","the","AFI","Life","Achievement","Award","for","his","contributions","to","the","cinema",",","and","has","won","an","Academy","Awards",",","a","Palme","d","'Or",",","Cannes","Film","Festival","Best","Director","Award",",","Silver","Lion",",","Grammy","Award",",","Emmy","Award",",","Golden","Globes",",","British","Academy","Film","Awards",",","and","Directors","Guild","of","America","Award","s","."],"labels":["O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","O","B-award","I-award","O","B-award","I-award","O","B-award","I-award","O","B-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O","O"]},"label_list":["organization","musical instrument","award","location","musical artist","country","music genre","person","song","album","band","event"]}
{"id":"207","dataset":"crossner_music","split":"test","instance":{"id":"207","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical instrument, country, event, song, person, music genre, award, album, organization, location, musical artist and O.\nSentence: For Revolver he selected five non- Heavy metal music records that influenced him : The Cure 's Pornography , Helium 's No Guitars , Mogwai EP + 2 , My Bloody Valentine 's Loveless and The Smashing Pumpkins ' Siamese Dream .","prompt_labels":"For(O) Revolver(B-band) he(O) selected(O) five(O) non-(O) Heavy(B-music genre) metal(I-music genre) music(I-music genre) records(O) that(O) influenced(O) him(O) :(O) The(O) Cure(B-band) 's(O) Pornography(B-album) ,(O) Helium(B-band) 's(O) No(B-album) Guitars(I-album) ,(O) Mogwai(B-band) EP(B-album) +(I-album) 2(I-album) ,(O) My(B-band) Bloody(I-band) Valentine(I-band) 's(O) Loveless(B-album) and(O) The(B-band) Smashing(I-band) Pumpkins(I-band) '(O) Siamese(B-album) Dream(I-album) .(O)","words":["For","Revolver","he","selected","five","non-","Heavy","metal","music","records","that","influenced","him",":","The","Cure","'s","Pornography",",","Helium","'s","No","Guitars",",","Mogwai","EP","+","2",",","My","Bloody","Valentine","'s","Loveless","and","The","Smashing","Pumpkins","'","Siamese","Dream","."],"labels":["O","B-band","O","O","O","O","B-music genre","I-music genre","I-music genre","O","O","O","O","O","O","B-band","O","B-album","O","B-band","O","B-album","I-album","O","B-band","B-album","I-album","I-album","O","B-band","I-band","I-band","O","B-album","O","B-band","I-band","I-band","O","B-album","I-album","O"]},"label_list":["band","musical instrument","country","event","song","person","music genre","award","album","organization","location","musical artist"]}
{"id":"258","dataset":"crossner_music","split":"test","instance":{"id":"258","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, band, location, music genre, musical instrument, event, album, song, organization, country, award, person and O.\nSentence: Hank Williams , Jr ( and , to an even greater extent , Hank Williams III ) , Gary Allan , Shania Twain , Brooks & amp ; Dunn , Faith Hill , Garth Brooks , Alan Jackson , Dwight Yoakam , Steve Earle , Dolly Parton , Rosanne Cash and Linda Ronstadt moved country further towards rock influence .","prompt_labels":"Hank(B-musical artist) Williams(I-musical artist) ,(I-musical artist) Jr(I-musical artist) ((O) and(O) ,(O) to(O) an(O) even(O) greater(O) extent(O) ,(O) Hank(B-musical artist) Williams(I-musical artist) III(I-musical artist) )(O) ,(O) Gary(B-musical artist) Allan(I-musical artist) ,(O) Shania(B-musical artist) Twain(I-musical artist) ,(O) Brooks(B-band) &(I-band) amp(I-band) ;(I-band) Dunn(I-band) ,(O) Faith(B-musical artist) Hill(I-musical artist) ,(O) Garth(B-musical artist) Brooks(I-musical artist) ,(O) Alan(B-musical artist) Jackson(I-musical artist) ,(O) Dwight(B-musical artist) Yoakam(I-musical artist) ,(O) Steve(B-musical artist) Earle(I-musical artist) ,(O) Dolly(B-musical artist) Parton(I-musical artist) ,(O) Rosanne(B-musical artist) Cash(I-musical artist) and(O) Linda(B-musical artist) Ronstadt(I-musical artist) moved(O) country(O) further(O) towards(O) rock(B-music genre) influence(O) .(O)","words":["Hank","Williams",",","Jr","(","and",",","to","an","even","greater","extent",",","Hank","Williams","III",")",",","Gary","Allan",",","Shania","Twain",",","Brooks","&","amp",";","Dunn",",","Faith","Hill",",","Garth","Brooks",",","Alan","Jackson",",","Dwight","Yoakam",",","Steve","Earle",",","Dolly","Parton",",","Rosanne","Cash","and","Linda","Ronstadt","moved","country","further","towards","rock","influence","."],"labels":["B-musical artist","I-musical artist","I-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-band","I-band","I-band","I-band","I-band","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O","B-music genre","O","O"]},"label_list":["musical artist","band","location","music genre","musical instrument","event","album","song","organization","country","award","person"]}
{"id":"320","dataset":"crossner_music","split":"test","instance":{"id":"320","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, award, song, location, event, organization, musical instrument, album, country, music genre, musical artist, person and O.\nSentence: In Scotland , The Corries , Silly Wizard , Capercaillie , Runrig , Jackie Leven , Julie Fowlis , Karine Polwart , Alasdair Roberts , Dick Gaughan , Wolfstone , Boys of the Lough , and The Silencers have kept Scottish folk vibrant and fresh by mixing traditional Scottish and Gaelic folk songs with more contemporary genres .","prompt_labels":"In(O) Scotland(B-country) ,(O) The(B-band) Corries(I-band) ,(O) Silly(B-band) Wizard(I-band) ,(O) Capercaillie(B-band) ,(O) Runrig(B-band) ,(O) Jackie(B-musical artist) Leven(I-musical artist) ,(O) Julie(B-musical artist) Fowlis(I-musical artist) ,(O) Karine(B-musical artist) Polwart(I-musical artist) ,(O) Alasdair(B-musical artist) Roberts(I-musical artist) ,(O) Dick(B-musical artist) Gaughan(I-musical artist) ,(O) Wolfstone(B-band) ,(O) Boys(B-band) of(I-band) the(I-band) Lough(I-band) ,(O) and(O) The(B-band) Silencers(I-band) have(O) kept(O) Scottish(B-music genre) folk(I-music genre) vibrant(O) and(O) fresh(O) by(O) mixing(O) traditional(B-music genre) Scottish(I-music genre) and(O) Gaelic(B-music genre) folk(I-music genre) songs(O) with(O) more(O) contemporary(O) genres(O) .(O)","words":["In","Scotland",",","The","Corries",",","Silly","Wizard",",","Capercaillie",",","Runrig",",","Jackie","Leven",",","Julie","Fowlis",",","Karine","Polwart",",","Alasdair","Roberts",",","Dick","Gaughan",",","Wolfstone",",","Boys","of","the","Lough",",","and","The","Silencers","have","kept","Scottish","folk","vibrant","and","fresh","by","mixing","traditional","Scottish","and","Gaelic","folk","songs","with","more","contemporary","genres","."],"labels":["O","B-country","O","B-band","I-band","O","B-band","I-band","O","B-band","O","B-band","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-band","O","B-band","I-band","I-band","I-band","O","O","B-band","I-band","O","O","B-music genre","I-music genre","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O","O","O","O","O"]},"label_list":["band","award","song","location","event","organization","musical instrument","album","country","music genre","musical artist","person"]}
{"id":"389","dataset":"crossner_music","split":"test","instance":{"id":"389","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, band, award, country, organization, location, song, music genre, person, musical instrument, musical artist, album and O.\nSentence: The music arose as a synthesis of traditional Creole music , some Cajun music influences , and African-American traditions , including Rhythm and blues , blues , jazz , and Gospel music .","prompt_labels":"The(O) music(O) arose(O) as(O) a(O) synthesis(O) of(O) traditional(O) Creole(B-music genre) music(I-music genre) ,(O) some(O) Cajun(B-music genre) music(I-music genre) influences(O) ,(O) and(O) African-American(O) traditions(O) ,(O) including(O) Rhythm(B-music genre) and(I-music genre) blues(I-music genre) ,(O) blues(B-music genre) ,(O) jazz(B-music genre) ,(O) and(O) Gospel(B-music genre) music(I-music genre) .(O)","words":["The","music","arose","as","a","synthesis","of","traditional","Creole","music",",","some","Cajun","music","influences",",","and","African-American","traditions",",","including","Rhythm","and","blues",",","blues",",","jazz",",","and","Gospel","music","."],"labels":["O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","B-music genre","I-music genre","I-music genre","O","B-music genre","O","B-music genre","O","O","B-music genre","I-music genre","O"]},"label_list":["event","band","award","country","organization","location","song","music genre","person","musical instrument","musical artist","album"]}
{"id":"425","dataset":"crossner_music","split":"test","instance":{"id":"425","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, musical artist, album, song, band, award, music genre, country, musical instrument, organization, location, event and O.\nSentence: For his work in the Western film Unforgiven ( 1992 ) and the sports drama Million Dollar Baby ( 2004 ) , Eastwood won Academy Awards for Academy Award for Best Director and Academy Award for Best Picture , as well as receiving nominations for Academy Award for Best Actor .","prompt_labels":"For(O) his(O) work(O) in(O) the(O) Western(O) film(O) Unforgiven(O) ((O) 1992(O) )(O) and(O) the(O) sports(O) drama(O) Million(O) Dollar(O) Baby(O) ((O) 2004(O) )(O) ,(O) Eastwood(B-person) won(O) Academy(B-award) Awards(I-award) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) ,(O) as(O) well(O) as(O) receiving(O) nominations(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) .(O)","words":["For","his","work","in","the","Western","film","Unforgiven","(","1992",")","and","the","sports","drama","Million","Dollar","Baby","(","2004",")",",","Eastwood","won","Academy","Awards","for","Academy","Award","for","Best","Director","and","Academy","Award","for","Best","Picture",",","as","well","as","receiving","nominations","for","Academy","Award","for","Best","Actor","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","O","B-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O"]},"label_list":["person","musical artist","album","song","band","award","music genre","country","musical instrument","organization","location","event"]}
{"id":"36","dataset":"crossner_politics","split":"test","instance":{"id":"36","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, event, election, politician, location, organization, person and O.\nSentence: The ACLU was founded in 1920 by a committee including Helen Keller , Roger Baldwin , Crystal Eastman , Walter Nelles , Morris Ernst , Albert DeSilver , Arthur Garfield Hays , Jane Addams , Felix Frankfurter , Elizabeth Gurley Flynn , and Rose Schneiderman .","prompt_labels":"The(O) ACLU(B-organization) was(O) founded(O) in(O) 1920(O) by(O) a(O) committee(O) including(O) Helen(B-person) Keller(I-person) ,(O) Roger(B-person) Baldwin(I-person) ,(O) Crystal(B-person) Eastman(I-person) ,(O) Walter(B-person) Nelles(I-person) ,(O) Morris(B-person) Ernst(I-person) ,(O) Albert(B-person) DeSilver(I-person) ,(O) Arthur(B-person) Garfield(I-person) Hays(I-person) ,(O) Jane(B-person) Addams(I-person) ,(O) Felix(B-person) Frankfurter(I-person) ,(O) Elizabeth(B-person) Gurley(I-person) Flynn(I-person) ,(O) and(O) Rose(B-person) Schneiderman(I-person) .(O)","words":["The","ACLU","was","founded","in","1920","by","a","committee","including","Helen","Keller",",","Roger","Baldwin",",","Crystal","Eastman",",","Walter","Nelles",",","Morris","Ernst",",","Albert","DeSilver",",","Arthur","Garfield","Hays",",","Jane","Addams",",","Felix","Frankfurter",",","Elizabeth","Gurley","Flynn",",","and","Rose","Schneiderman","."],"labels":["O","B-organization","O","O","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","I-person","O","O","B-person","I-person","O"]},"label_list":["country","political party","event","election","politician","location","organization","person"]}
{"id":"76","dataset":"crossner_politics","split":"test","instance":{"id":"76","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, political party, country, politician, location, organization, election and O.\nSentence: The Kingdom of Hungary and the First Austrian Republic were treated as its successors de jure , whereas the independence of the West Slavs and South Slavs of the Empire as the First Czechoslovak Republic , the Second Polish Republic and the Kingdom of Yugoslavia , respectively , and most of the territorial demands of the Kingdom of Romania were also recognized by the victorious powers in 1920 .","prompt_labels":"The(O) Kingdom(B-country) of(I-country) Hungary(I-country) and(O) the(O) First(B-country) Austrian(I-country) Republic(I-country) were(O) treated(O) as(O) its(O) successors(O) de(O) jure(O) ,(O) whereas(O) the(O) independence(O) of(O) the(O) West(O) Slavs(O) and(O) South(O) Slavs(O) of(O) the(O) Empire(O) as(O) the(O) First(B-country) Czechoslovak(I-country) Republic(I-country) ,(O) the(O) Second(B-country) Polish(I-country) Republic(I-country) and(O) the(O) Kingdom(B-country) of(I-country) Yugoslavia(I-country) ,(O) respectively(O) ,(O) and(O) most(O) of(O) the(O) territorial(O) demands(O) of(O) the(O) Kingdom(B-country) of(I-country) Romania(I-country) were(O) also(O) recognized(O) by(O) the(O) victorious(O) powers(O) in(O) 1920(O) .(O)","words":["The","Kingdom","of","Hungary","and","the","First","Austrian","Republic","were","treated","as","its","successors","de","jure",",","whereas","the","independence","of","the","West","Slavs","and","South","Slavs","of","the","Empire","as","the","First","Czechoslovak","Republic",",","the","Second","Polish","Republic","and","the","Kingdom","of","Yugoslavia",",","respectively",",","and","most","of","the","territorial","demands","of","the","Kingdom","of","Romania","were","also","recognized","by","the","victorious","powers","in","1920","."],"labels":["O","B-country","I-country","I-country","O","O","B-country","I-country","I-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","I-country","O","O","B-country","I-country","I-country","O","O","B-country","I-country","I-country","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","I-country","O","O","O","O","O","O","O","O","O","O"]},"label_list":["person","event","political party","country","politician","location","organization","election"]}
{"id":"152","dataset":"crossner_politics","split":"test","instance":{"id":"152","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, country, organization, politician, location, event, person and O.\nSentence: New Zealand 's 48th Parliament operated with both a coalition and a looser agreement : the government was a coalition between the New Zealand Labour Party and the Progressives , while United Future and New Zealand First had an agreement to support the government on confidence matters , while the Green Party abstained .","prompt_labels":"New(B-country) Zealand(I-country) 's(O) 48th(B-organization) Parliament(I-organization) operated(O) with(O) both(O) a(O) coalition(O) and(O) a(O) looser(O) agreement(O) :(O) the(O) government(O) was(O) a(O) coalition(O) between(O) the(O) New(B-political party) Zealand(I-political party) Labour(I-political party) Party(I-political party) and(O) the(O) Progressives(B-political party) ,(O) while(O) United(B-political party) Future(I-political party) and(O) New(B-political party) Zealand(I-political party) First(I-political party) had(O) an(O) agreement(O) to(O) support(O) the(O) government(O) on(O) confidence(O) matters(O) ,(O) while(O) the(O) Green(B-political party) Party(I-political party) abstained(O) .(O)","words":["New","Zealand","'s","48th","Parliament","operated","with","both","a","coalition","and","a","looser","agreement",":","the","government","was","a","coalition","between","the","New","Zealand","Labour","Party","and","the","Progressives",",","while","United","Future","and","New","Zealand","First","had","an","agreement","to","support","the","government","on","confidence","matters",",","while","the","Green","Party","abstained","."],"labels":["B-country","I-country","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","O","O","B-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O"]},"label_list":["election","political party","country","organization","politician","location","event","person"]}
{"id":"154","dataset":"crossner_politics","split":"test","instance":{"id":"154","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, location, political party, politician, country, person, organization and O.\nSentence: After a period of negotiations , the party formed a pact with Republican Left of Catalonia , Initiative for Catalonia Greens and the United and Alternative Left , and governed in Catalonia until 2010 .","prompt_labels":"After(O) a(O) period(O) of(O) negotiations(O) ,(O) the(O) party(O) formed(O) a(O) pact(O) with(O) Republican(B-political party) Left(I-political party) of(I-political party) Catalonia(I-political party) ,(O) Initiative(B-political party) for(I-political party) Catalonia(I-political party) Greens(I-political party) and(O) the(O) United(B-political party) and(I-political party) Alternative(I-political party) Left(I-political party) ,(O) and(O) governed(O) in(O) Catalonia(B-location) until(O) 2010(O) .(O)","words":["After","a","period","of","negotiations",",","the","party","formed","a","pact","with","Republican","Left","of","Catalonia",",","Initiative","for","Catalonia","Greens","and","the","United","and","Alternative","Left",",","and","governed","in","Catalonia","until","2010","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","B-location","O","O","O"]},"label_list":["event","election","location","political party","politician","country","person","organization"]}
{"id":"223","dataset":"crossner_politics","split":"test","instance":{"id":"223","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, event, country, person, political party, election, organization, location and O.\nSentence: He was re-elected in the elections of 1966 Manitoba general election , 1969 Manitoba general election , 1973 Manitoba general election and 1977 Manitoba general election , each time by a significant margin .","prompt_labels":"He(O) was(O) re-elected(O) in(O) the(O) elections(O) of(O) 1966(B-election) Manitoba(I-election) general(I-election) election(I-election) ,(O) 1969(B-election) Manitoba(I-election) general(I-election) election(I-election) ,(O) 1973(B-election) Manitoba(I-election) general(I-election) election(I-election) and(O) 1977(B-election) Manitoba(I-election) general(I-election) election(I-election) ,(O) each(O) time(O) by(O) a(O) significant(O) margin(O) .(O)","words":["He","was","re-elected","in","the","elections","of","1966","Manitoba","general","election",",","1969","Manitoba","general","election",",","1973","Manitoba","general","election","and","1977","Manitoba","general","election",",","each","time","by","a","significant","margin","."],"labels":["O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O"]},"label_list":["politician","event","country","person","political party","election","organization","location"]}
{"id":"232","dataset":"crossner_politics","split":"test","instance":{"id":"232","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, election, location, organization, country, person, politician and O.\nSentence: Operations started in July 1933 , initially linking Cairo with Alexandria and Mersa Matruh using de Havilland DH.84 Dragon equipment .","prompt_labels":"Operations(O) started(O) in(O) July(O) 1933(O) ,(O) initially(O) linking(O) Cairo(B-location) with(O) Alexandria(B-location) and(O) Mersa(B-location) Matruh(I-location) using(O) de(O) Havilland(O) DH.84(O) Dragon(O) equipment(O) .(O)","words":["Operations","started","in","July","1933",",","initially","linking","Cairo","with","Alexandria","and","Mersa","Matruh","using","de","Havilland","DH.84","Dragon","equipment","."],"labels":["O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","I-location","O","O","O","O","O","O","O"]},"label_list":["event","political party","election","location","organization","country","person","politician"]}
{"id":"366","dataset":"crossner_politics","split":"test","instance":{"id":"366","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, event, person, organization, location, country, politician and O.\nSentence: At the 2005 election the members elected were from the following political parties : nine Sinn Féin , five Ulster Unionist Party ( UUP ) , five Social Democratic and Labour Party ( SDLP ) and four Democratic Unionist Party ( DUP ) .","prompt_labels":"At(O) the(O) 2005(O) election(O) the(O) members(O) elected(O) were(O) from(O) the(O) following(O) political(O) parties(O) :(O) nine(O) Sinn(B-political party) Féin(I-political party) ,(O) five(O) Ulster(B-political party) Unionist(I-political party) Party(I-political party) ((O) UUP(B-political party) )(O) ,(O) five(O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) ((O) SDLP(B-political party) )(O) and(O) four(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) ((O) DUP(B-political party) )(O) .(O)","words":["At","the","2005","election","the","members","elected","were","from","the","following","political","parties",":","nine","Sinn","Féin",",","five","Ulster","Unionist","Party","(","UUP",")",",","five","Social","Democratic","and","Labour","Party","(","SDLP",")","and","four","Democratic","Unionist","Party","(","DUP",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O"]},"label_list":["election","political party","event","person","organization","location","country","politician"]}
{"id":"398","dataset":"crossner_politics","split":"test","instance":{"id":"398","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, location, event, country, person, political party, election and O.\nSentence: Democratic candidates won by large margins in the Southern states in every presidential election from the 1876 United States presidential election to 1948 United States presidential election except for 1928 United States presidential election , when the Democratic candidate was Al Smith , a Catholic New Yorker ; and even in that election , the divided South provided Smith with nearly three-fourths of his electoral votes .","prompt_labels":"Democratic(O) candidates(O) won(O) by(O) large(O) margins(O) in(O) the(O) Southern(O) states(O) in(O) every(O) presidential(O) election(O) from(O) the(O) 1876(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) to(O) 1948(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) except(O) for(O) 1928(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) when(O) the(O) Democratic(O) candidate(O) was(O) Al(B-politician) Smith(I-politician) ,(O) a(O) Catholic(O) New(O) Yorker(O) ;(O) and(O) even(O) in(O) that(O) election(O) ,(O) the(O) divided(O) South(O) provided(O) Smith(B-politician) with(O) nearly(O) three-fourths(O) of(O) his(O) electoral(O) votes(O) .(O)","words":["Democratic","candidates","won","by","large","margins","in","the","Southern","states","in","every","presidential","election","from","the","1876","United","States","presidential","election","to","1948","United","States","presidential","election","except","for","1928","United","States","presidential","election",",","when","the","Democratic","candidate","was","Al","Smith",",","a","Catholic","New","Yorker",";","and","even","in","that","election",",","the","divided","South","provided","Smith","with","nearly","three-fourths","of","his","electoral","votes","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","O","O","O","O","O","O","O","O"]},"label_list":["politician","organization","location","event","country","person","political party","election"]}
{"id":"480","dataset":"crossner_politics","split":"test","instance":{"id":"480","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, person, politician, election, organization, country, location, event and O.\nSentence: Since the publication of this paper , the growth diagnostics strategy has been adopted by a number of international institutions including the World Bank , the Inter-American Development Bank , the Asian Development Bank , the UK 's Department for International Development and the Millennium Challenge Corporation .","prompt_labels":"Since(O) the(O) publication(O) of(O) this(O) paper(O) ,(O) the(O) growth(O) diagnostics(O) strategy(O) has(O) been(O) adopted(O) by(O) a(O) number(O) of(O) international(O) institutions(O) including(O) the(O) World(B-organization) Bank(I-organization) ,(O) the(O) Inter-American(B-organization) Development(I-organization) Bank(I-organization) ,(O) the(O) Asian(B-organization) Development(I-organization) Bank(I-organization) ,(O) the(O) UK(B-country) 's(O) Department(B-organization) for(I-organization) International(I-organization) Development(I-organization) and(O) the(O) Millennium(B-organization) Challenge(I-organization) Corporation(I-organization) .(O)","words":["Since","the","publication","of","this","paper",",","the","growth","diagnostics","strategy","has","been","adopted","by","a","number","of","international","institutions","including","the","World","Bank",",","the","Inter-American","Development","Bank",",","the","Asian","Development","Bank",",","the","UK","'s","Department","for","International","Development","and","the","Millennium","Challenge","Corporation","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-country","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O"]},"label_list":["political party","person","politician","election","organization","country","location","event"]}
{"id":"583","dataset":"crossner_politics","split":"test","instance":{"id":"583","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, election, politician, event, country, person, location, organization and O.\nSentence: Röttgen , in his capacity as environment minister , led the German delegations to the 2009 United Nations Climate Change Conference in Copenhagen , the 2010 United Nations Climate Change Conference in Cancún and the 2011 United Nations Climate Change Conference in Durban , respectively .","prompt_labels":"Röttgen(B-politician) ,(O) in(O) his(O) capacity(O) as(O) environment(O) minister(O) ,(O) led(O) the(O) German(O) delegations(O) to(O) the(O) 2009(B-event) United(I-event) Nations(I-event) Climate(I-event) Change(I-event) Conference(I-event) in(O) Copenhagen(B-location) ,(O) the(O) 2010(B-event) United(I-event) Nations(I-event) Climate(I-event) Change(I-event) Conference(I-event) in(O) Cancún(B-location) and(O) the(O) 2011(B-event) United(I-event) Nations(I-event) Climate(I-event) Change(I-event) Conference(I-event) in(O) Durban(B-location) ,(O) respectively(O) .(O)","words":["Röttgen",",","in","his","capacity","as","environment","minister",",","led","the","German","delegations","to","the","2009","United","Nations","Climate","Change","Conference","in","Copenhagen",",","the","2010","United","Nations","Climate","Change","Conference","in","Cancún","and","the","2011","United","Nations","Climate","Change","Conference","in","Durban",",","respectively","."],"labels":["B-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","I-event","O","B-location","O","O","B-event","I-event","I-event","I-event","I-event","I-event","O","B-location","O","O","B-event","I-event","I-event","I-event","I-event","I-event","O","B-location","O","O","O"]},"label_list":["political party","election","politician","event","country","person","location","organization"]}
{"id":"37","dataset":"crossner_science","split":"test","instance":{"id":"37","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, university, chemical element, award, protein, event, organization, discipline, scientist, country, enzyme, astronomical object, theory, location, academic journal, chemical compound and O.\nSentence: Mikhail Rabinovich published over 250 peer reviewed articles in leading scientific journals ( Science , Neuron , Journal of Neuroscience , PLoS Computational Biology , Reviews of Modern Physics , Physics of Fluids , Physical Review Letters ) that are actively cited ( over 3000 citations ) .","prompt_labels":"Mikhail(B-scientist) Rabinovich(I-scientist) published(O) over(O) 250(O) peer(O) reviewed(O) articles(O) in(O) leading(O) scientific(O) journals(O) ((O) Science(B-academic journal) ,(O) Neuron(B-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Neuroscience(I-academic journal) ,(O) PLoS(B-academic journal) Computational(I-academic journal) Biology(I-academic journal) ,(O) Reviews(B-academic journal) of(I-academic journal) Modern(I-academic journal) Physics(I-academic journal) ,(O) Physics(B-academic journal) of(I-academic journal) Fluids(I-academic journal) ,(O) Physical(B-academic journal) Review(I-academic journal) Letters(I-academic journal) )(O) that(O) are(O) actively(O) cited(O) ((O) over(O) 3000(O) citations(O) )(O) .(O)","words":["Mikhail","Rabinovich","published","over","250","peer","reviewed","articles","in","leading","scientific","journals","(","Science",",","Neuron",",","Journal","of","Neuroscience",",","PLoS","Computational","Biology",",","Reviews","of","Modern","Physics",",","Physics","of","Fluids",",","Physical","Review","Letters",")","that","are","actively","cited","(","over","3000","citations",")","."],"labels":["B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","B-academic journal","O","B-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","O","O","O","O","O","O"]},"label_list":["person","university","chemical element","award","protein","event","organization","discipline","scientist","country","enzyme","astronomical object","theory","location","academic journal","chemical compound"]}
{"id":"117","dataset":"crossner_science","split":"test","instance":{"id":"117","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, event, university, scientist, academic journal, enzyme, theory, award, organization, country, protein, chemical element, person, location, chemical compound, astronomical object and O.\nSentence: All of these low-numbered asteroids have numbers between and were discovered between 1876 and the 1930s , predominantly by astronomers Auguste Charlois , Johann Palisa , Max Wolf and Karl Reinmuth ( also see category ) .","prompt_labels":"All(O) of(O) these(O) low-numbered(O) asteroids(O) have(O) numbers(O) between(O) and(O) were(O) discovered(O) between(O) 1876(O) and(O) the(O) 1930s(O) ,(O) predominantly(O) by(O) astronomers(O) Auguste(B-scientist) Charlois(I-scientist) ,(O) Johann(B-scientist) Palisa(I-scientist) ,(O) Max(B-scientist) Wolf(I-scientist) and(O) Karl(B-scientist) Reinmuth(I-scientist) ((O) also(O) see(O) category(O) )(O) .(O)","words":["All","of","these","low-numbered","asteroids","have","numbers","between","and","were","discovered","between","1876","and","the","1930s",",","predominantly","by","astronomers","Auguste","Charlois",",","Johann","Palisa",",","Max","Wolf","and","Karl","Reinmuth","(","also","see","category",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O","O","O"]},"label_list":["discipline","event","university","scientist","academic journal","enzyme","theory","award","organization","country","protein","chemical element","person","location","chemical compound","astronomical object"]}
{"id":"159","dataset":"crossner_science","split":"test","instance":{"id":"159","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, chemical compound, person, astronomical object, protein, organization, chemical element, award, enzyme, location, country, theory, academic journal, scientist, university, event and O.\nSentence: During his career he held academic posts at the universities of University of Manchester ( 1951-6 ) , Durham University ( 1956-92 ) , the University of Ceylon and the University of Hong Kong , and was head of department at Durham where he remains an emeritus professor .","prompt_labels":"During(O) his(O) career(O) he(O) held(O) academic(O) posts(O) at(O) the(O) universities(O) of(O) University(B-university) of(I-university) Manchester(I-university) ((O) 1951-6(O) )(O) ,(O) Durham(B-university) University(I-university) ((O) 1956-92(O) )(O) ,(O) the(O) University(B-university) of(I-university) Ceylon(I-university) and(O) the(O) University(B-university) of(I-university) Hong(I-university) Kong(I-university) ,(O) and(O) was(O) head(O) of(O) department(O) at(O) Durham(B-university) where(O) he(O) remains(O) an(O) emeritus(O) professor(O) .(O)","words":["During","his","career","he","held","academic","posts","at","the","universities","of","University","of","Manchester","(","1951-6",")",",","Durham","University","(","1956-92",")",",","the","University","of","Ceylon","and","the","University","of","Hong","Kong",",","and","was","head","of","department","at","Durham","where","he","remains","an","emeritus","professor","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-university","I-university","I-university","O","O","O","O","B-university","I-university","O","O","O","O","O","B-university","I-university","I-university","O","O","B-university","I-university","I-university","I-university","O","O","O","O","O","O","O","B-university","O","O","O","O","O","O","O"]},"label_list":["discipline","chemical compound","person","astronomical object","protein","organization","chemical element","award","enzyme","location","country","theory","academic journal","scientist","university","event"]}
{"id":"163","dataset":"crossner_science","split":"test","instance":{"id":"163","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, protein, chemical element, theory, person, university, enzyme, award, discipline, chemical compound, scientist, astronomical object, organization, academic journal, location, country and O.\nSentence: On the other three gas giant s ( Saturn , Uranus and Neptune ) eclipses only occur at certain periods during the planet 's orbit , due to their higher inclination between the orbits of the moon and the orbital plane of the planet .","prompt_labels":"On(O) the(O) other(O) three(O) gas(O) giant(O) s(O) ((O) Saturn(B-astronomical object) ,(O) Uranus(B-astronomical object) and(O) Neptune(B-astronomical object) )(O) eclipses(O) only(O) occur(O) at(O) certain(O) periods(O) during(O) the(O) planet(O) 's(O) orbit(O) ,(O) due(O) to(O) their(O) higher(O) inclination(O) between(O) the(O) orbits(O) of(O) the(O) moon(O) and(O) the(O) orbital(O) plane(O) of(O) the(O) planet(O) .(O)","words":["On","the","other","three","gas","giant","s","(","Saturn",",","Uranus","and","Neptune",")","eclipses","only","occur","at","certain","periods","during","the","planet","'s","orbit",",","due","to","their","higher","inclination","between","the","orbits","of","the","moon","and","the","orbital","plane","of","the","planet","."],"labels":["O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"]},"label_list":["event","protein","chemical element","theory","person","university","enzyme","award","discipline","chemical compound","scientist","astronomical object","organization","academic journal","location","country"]}
{"id":"173","dataset":"crossner_science","split":"test","instance":{"id":"173","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, scientist, discipline, chemical element, award, enzyme, protein, person, chemical compound, astronomical object, organization, theory, location, event, university, academic journal and O.\nSentence: The study showed that CRISPR is could effectively be used as a gene-editing tool in human 2PN zygotes , which could lead potentially pregnancy viable if implanted .","prompt_labels":"The(O) study(O) showed(O) that(O) CRISPR(O) is(O) could(O) effectively(O) be(O) used(O) as(O) a(O) gene-editing(O) tool(O) in(O) human(O) 2PN(O) zygotes(O) ,(O) which(O) could(O) lead(O) potentially(O) pregnancy(O) viable(O) if(O) implanted(O) .(O)","words":["The","study","showed","that","CRISPR","is","could","effectively","be","used","as","a","gene-editing","tool","in","human","2PN","zygotes",",","which","could","lead","potentially","pregnancy","viable","if","implanted","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"]},"label_list":["country","scientist","discipline","chemical element","award","enzyme","protein","person","chemical compound","astronomical object","organization","theory","location","event","university","academic journal"]}
{"id":"215","dataset":"crossner_science","split":"test","instance":{"id":"215","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, enzyme, discipline, theory, academic journal, award, person, location, university, scientist, organization, event, astronomical object, protein, chemical compound, country and O.\nSentence: It was discovered on 16 October 1977 , by Ingrid van Houten-Groeneveld and Cornelis van Houten at Leiden , and Tom Gehrels at Palomar Observatory in California .","prompt_labels":"It(O) was(O) discovered(O) on(O) 16(O) October(O) 1977(O) ,(O) by(O) Ingrid(B-scientist) van(I-scientist) Houten-Groeneveld(I-scientist) and(O) Cornelis(B-scientist) van(I-scientist) Houten(I-scientist) at(O) Leiden(B-location) ,(O) and(O) Tom(B-scientist) Gehrels(I-scientist) at(O) Palomar(B-location) Observatory(I-location) in(O) California(B-location) .(O)","words":["It","was","discovered","on","16","October","1977",",","by","Ingrid","van","Houten-Groeneveld","and","Cornelis","van","Houten","at","Leiden",",","and","Tom","Gehrels","at","Palomar","Observatory","in","California","."],"labels":["O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","B-location","O","O","B-scientist","I-scientist","O","B-location","I-location","O","B-location","O"]},"label_list":["chemical element","enzyme","discipline","theory","academic journal","award","person","location","university","scientist","organization","event","astronomical object","protein","chemical compound","country"]}
{"id":"314","dataset":"crossner_science","split":"test","instance":{"id":"314","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, protein, chemical element, discipline, academic journal, country, astronomical object, location, scientist, event, organization, chemical compound, university, enzyme, person, award and O.\nSentence: He was named one of the most valued reviewers of 2010 by the Editors of Elsevier and Nuclear Physics Other journals for which Dr. Poenaru peer review ed articles include Physical Review Letters , Physical Review C , Journal of Physics G : Nuclear and Particle Physics and Canadian Journal of Physics .","prompt_labels":"He(O) was(O) named(O) one(O) of(O) the(O) most(O) valued(O) reviewers(O) of(O) 2010(O) by(O) the(O) Editors(O) of(O) Elsevier(B-organization) and(O) Nuclear(B-academic journal) Physics(I-academic journal) Other(O) journals(O) for(O) which(O) Dr.(O) Poenaru(B-scientist) peer(O) review(O) ed(O) articles(O) include(O) Physical(B-academic journal) Review(I-academic journal) Letters(I-academic journal) ,(O) Physical(B-academic journal) Review(I-academic journal) C(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Physics(I-academic journal) G(I-academic journal) :(O) Nuclear(B-academic journal) and(I-academic journal) Particle(I-academic journal) Physics(I-academic journal) and(O) Canadian(B-academic journal) Journal(I-academic journal) of(I-academic journal) Physics(I-academic journal) .(O)","words":["He","was","named","one","of","the","most","valued","reviewers","of","2010","by","the","Editors","of","Elsevier","and","Nuclear","Physics","Other","journals","for","which","Dr.","Poenaru","peer","review","ed","articles","include","Physical","Review","Letters",",","Physical","Review","C",",","Journal","of","Physics","G",":","Nuclear","and","Particle","Physics","and","Canadian","Journal","of","Physics","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","B-academic journal","I-academic journal","O","O","O","O","O","B-scientist","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O"]},"label_list":["theory","protein","chemical element","discipline","academic journal","country","astronomical object","location","scientist","event","organization","chemical compound","university","enzyme","person","award"]}
{"id":"435","dataset":"crossner_science","split":"test","instance":{"id":"435","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, country, event, person, astronomical object, academic journal, theory, chemical element, organization, chemical compound, enzyme, award, location, protein, scientist, university and O.\nSentence: Sydney Goldstein , who also wrote the Royal Society memoir for Kármán , reviewed the autobiography Sydney Goldstein ( 1968 ) Journal of Fluid Mechanics 33 ( 2 ) and remembered an eminent engineer and scientist , warm-hearted and witty , much traveled , well-known by many , devoted to international collaboration , who , in his own words , as a scientist found the military ' the most comfortable group to deal with ' .","prompt_labels":"Sydney(B-scientist) Goldstein(I-scientist) ,(O) who(O) also(O) wrote(O) the(O) Royal(B-organization) Society(I-organization) memoir(O) for(O) Kármán(B-scientist) ,(O) reviewed(O) the(O) autobiography(O) Sydney(B-scientist) Goldstein(I-scientist) ((O) 1968(O) )(O) Journal(B-academic journal) of(I-academic journal) Fluid(I-academic journal) Mechanics(I-academic journal) 33(O) ((O) 2(O) )(O) and(O) remembered(O) an(O) eminent(O) engineer(O) and(O) scientist(O) ,(O) warm-hearted(O) and(O) witty(O) ,(O) much(O) traveled(O) ,(O) well-known(O) by(O) many(O) ,(O) devoted(O) to(O) international(O) collaboration(O) ,(O) who(O) ,(O) in(O) his(O) own(O) words(O) ,(O) as(O) a(O) scientist(O) found(O) the(O) military(O) '(O) the(O) most(O) comfortable(O) group(O) to(O) deal(O) with(O) '(O) .(O)","words":["Sydney","Goldstein",",","who","also","wrote","the","Royal","Society","memoir","for","Kármán",",","reviewed","the","autobiography","Sydney","Goldstein","(","1968",")","Journal","of","Fluid","Mechanics","33","(","2",")","and","remembered","an","eminent","engineer","and","scientist",",","warm-hearted","and","witty",",","much","traveled",",","well-known","by","many",",","devoted","to","international","collaboration",",","who",",","in","his","own","words",",","as","a","scientist","found","the","military","'","the","most","comfortable","group","to","deal","with","'","."],"labels":["B-scientist","I-scientist","O","O","O","O","O","B-organization","I-organization","O","O","B-scientist","O","O","O","O","B-scientist","I-scientist","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"]},"label_list":["discipline","country","event","person","astronomical object","academic journal","theory","chemical element","organization","chemical compound","enzyme","award","location","protein","scientist","university"]}
{"id":"441","dataset":"crossner_science","split":"test","instance":{"id":"441","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, theory, scientist, university, chemical element, astronomical object, organization, country, discipline, event, person, chemical compound, enzyme, location, protein, award and O.\nSentence: In fact , when math E / math belongs to the spectrum , the inequality becomes an equality ( the Aubry-André formula ) , proved by Jean Bourgain and Svetlana Jitomirskaya .","prompt_labels":"In(O) fact(O) ,(O) when(O) math(O) E(O) /(O) math(O) belongs(O) to(O) the(O) spectrum(O) ,(O) the(O) inequality(O) becomes(O) an(O) equality(O) ((O) the(O) Aubry-André(B-theory) formula(I-theory) )(O) ,(O) proved(O) by(O) Jean(B-scientist) Bourgain(I-scientist) and(O) Svetlana(B-scientist) Jitomirskaya(I-scientist) .(O)","words":["In","fact",",","when","math","E","/","math","belongs","to","the","spectrum",",","the","inequality","becomes","an","equality","(","the","Aubry-André","formula",")",",","proved","by","Jean","Bourgain","and","Svetlana","Jitomirskaya","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-theory","I-theory","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O"]},"label_list":["academic journal","theory","scientist","university","chemical element","astronomical object","organization","country","discipline","event","person","chemical compound","enzyme","location","protein","award"]}
{"id":"481","dataset":"crossner_science","split":"test","instance":{"id":"481","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, theory, enzyme, person, scientist, award, astronomical object, location, organization, university, chemical compound, protein, country, academic journal, discipline, event and O.\nSentence: This would be achieved using biotechnology methods to confer to them the capacity to alter nucleotides in the chromosomes of infected individuals through sequence-specific editing systems like CRISPR , ZFN or TALEN .","prompt_labels":"This(O) would(O) be(O) achieved(O) using(O) biotechnology(B-discipline) methods(O) to(O) confer(O) to(O) them(O) the(O) capacity(O) to(O) alter(O) nucleotides(B-chemical compound) in(O) the(O) chromosomes(O) of(O) infected(O) individuals(O) through(O) sequence-specific(O) editing(O) systems(O) like(O) CRISPR(O) ,(O) ZFN(O) or(O) TALEN(O) .(O)","words":["This","would","be","achieved","using","biotechnology","methods","to","confer","to","them","the","capacity","to","alter","nucleotides","in","the","chromosomes","of","infected","individuals","through","sequence-specific","editing","systems","like","CRISPR",",","ZFN","or","TALEN","."],"labels":["O","O","O","O","O","B-discipline","O","O","O","O","O","O","O","O","O","B-chemical compound","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"]},"label_list":["chemical element","theory","enzyme","person","scientist","award","astronomical object","location","organization","university","chemical compound","protein","country","academic journal","discipline","event"]}
{"id":"31","dataset":"mit-movie","split":"test","instance":{"id":"31","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, trailer, plot, title, average ratings, rating, character, director, song, year, actor, review and O.\nSentence: find movies with robert diniero in it","prompt_labels":"find(O) movies(O) with(O) robert(B-actor) diniero(I-actor) in(O) it(O)","words":["find","movies","with","robert","diniero","in","it"],"labels":["O","O","O","B-actor","I-actor","O","O"]},"label_list":["genre","trailer","plot","title","average ratings","rating","character","director","song","year","actor","review"]}
{"id":"101","dataset":"mit-movie","split":"test","instance":{"id":"101","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, actor, rating, year, title, character, average ratings, genre, trailer, plot, song, director and O.\nSentence: list of actors a beautiful mind","prompt_labels":"list(O) of(O) actors(O) a(B-title) beautiful(I-title) mind(I-title)","words":["list","of","actors","a","beautiful","mind"],"labels":["O","O","O","B-title","I-title","I-title"]},"label_list":["review","actor","rating","year","title","character","average ratings","genre","trailer","plot","song","director"]}
{"id":"705","dataset":"mit-movie","split":"test","instance":{"id":"705","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, average ratings, trailer, director, plot, genre, review, character, actor, title, song, rating and O.\nSentence: who is the actor that stars in the new tv show awake","prompt_labels":"who(O) is(O) the(O) actor(O) that(O) stars(O) in(O) the(O) new(O) tv(O) show(O) awake(B-title)","words":["who","is","the","actor","that","stars","in","the","new","tv","show","awake"],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-title"]},"label_list":["year","average ratings","trailer","director","plot","genre","review","character","actor","title","song","rating"]}
{"id":"814","dataset":"mit-movie","split":"test","instance":{"id":"814","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, actor, genre, review, character, average ratings, song, trailer, rating, title, year, plot and O.\nSentence: find a comedy about basketball","prompt_labels":"find(O) a(O) comedy(B-genre) about(O) basketball(B-plot)","words":["find","a","comedy","about","basketball"],"labels":["O","O","B-genre","O","B-plot"]},"label_list":["director","actor","genre","review","character","average ratings","song","trailer","rating","title","year","plot"]}
{"id":"953","dataset":"mit-movie","split":"test","instance":{"id":"953","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, song, year, plot, rating, average ratings, character, director, actor, trailer, review, title and O.\nSentence: are there any movies starring halle berry before 1997","prompt_labels":"are(O) there(O) any(O) movies(O) starring(O) halle(B-actor) berry(I-actor) before(O) 1997(O)","words":["are","there","any","movies","starring","halle","berry","before","1997"],"labels":["O","O","O","O","O","B-actor","I-actor","O","O"]},"label_list":["genre","song","year","plot","rating","average ratings","character","director","actor","trailer","review","title"]}
{"id":"1592","dataset":"mit-movie","split":"test","instance":{"id":"1592","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, director, year, rating, review, average ratings, song, plot, title, trailer, genre, actor and O.\nSentence: list a humphrey bogart thriller","prompt_labels":"list(O) a(O) humphrey(B-actor) bogart(I-actor) thriller(B-genre)","words":["list","a","humphrey","bogart","thriller"],"labels":["O","O","B-actor","I-actor","B-genre"]},"label_list":["character","director","year","rating","review","average ratings","song","plot","title","trailer","genre","actor"]}
{"id":"1605","dataset":"mit-movie","split":"test","instance":{"id":"1605","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, rating, character, song, director, year, actor, title, plot, trailer, average ratings, genre and O.\nSentence: list a chick film in the past ten decades","prompt_labels":"list(O) a(O) chick(B-genre) film(O) in(O) the(O) past(B-year) ten(I-year) decades(I-year)","words":["list","a","chick","film","in","the","past","ten","decades"],"labels":["O","O","B-genre","O","O","O","B-year","I-year","I-year"]},"label_list":["review","rating","character","song","director","year","actor","title","plot","trailer","average ratings","genre"]}
{"id":"1745","dataset":"mit-movie","split":"test","instance":{"id":"1745","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, genre, average ratings, song, character, actor, trailer, plot, year, review, director, rating and O.\nSentence: pg 13 comedy directed by scott f evans","prompt_labels":"pg(B-rating) 13(I-rating) comedy(B-genre) directed(O) by(O) scott(B-director) f(I-director) evans(I-director)","words":["pg","13","comedy","directed","by","scott","f","evans"],"labels":["B-rating","I-rating","B-genre","O","O","B-director","I-director","I-director"]},"label_list":["title","genre","average ratings","song","character","actor","trailer","plot","year","review","director","rating"]}
{"id":"1975","dataset":"mit-movie","split":"test","instance":{"id":"1975","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, song, title, rating, year, genre, review, character, director, actor, average ratings, trailer and O.\nSentence: what is domino about","prompt_labels":"what(O) is(O) domino(B-title) about(O)","words":["what","is","domino","about"],"labels":["O","O","B-title","O"]},"label_list":["plot","song","title","rating","year","genre","review","character","director","actor","average ratings","trailer"]}
{"id":"2292","dataset":"mit-movie","split":"test","instance":{"id":"2292","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, trailer, genre, song, average ratings, actor, plot, rating, title, year, character, director and O.\nSentence: which documentary received a rating of six","prompt_labels":"which(O) documentary(B-genre) received(O) a(O) rating(O) of(O) six(B-average ratings)","words":["which","documentary","received","a","rating","of","six"],"labels":["O","B-genre","O","O","O","O","B-average ratings"]},"label_list":["review","trailer","genre","song","average ratings","actor","plot","rating","title","year","character","director"]}
{"id":"113","dataset":"mit-restaurant","split":"test","instance":{"id":"113","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Price, Hours, Restaurant Name, Location, Cuisine, Amenity, Rating and O.\nSentence: call dominos","prompt_labels":"call(O) dominos(B-Restaurant Name)","words":["call","dominos"],"labels":["O","B-Restaurant Name"]},"label_list":["Dish","Price","Hours","Restaurant Name","Location","Cuisine","Amenity","Rating"]}
{"id":"211","dataset":"mit-restaurant","split":"test","instance":{"id":"211","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Amenity, Restaurant Name, Dish, Cuisine, Rating, Price, Location and O.\nSentence: can you tell me where to get some cheap vegetarian food","prompt_labels":"can(O) you(O) tell(O) me(O) where(O) to(O) get(O) some(O) cheap(B-Price) vegetarian(B-Cuisine) food(O)","words":["can","you","tell","me","where","to","get","some","cheap","vegetarian","food"],"labels":["O","O","O","O","O","O","O","O","B-Price","B-Cuisine","O"]},"label_list":["Hours","Amenity","Restaurant Name","Dish","Cuisine","Rating","Price","Location"]}
{"id":"421","dataset":"mit-restaurant","split":"test","instance":{"id":"421","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Restaurant Name, Price, Hours, Rating, Amenity, Cuisine, Location and O.\nSentence: find me an italian restaurant which has received high ratings for its service","prompt_labels":"find(O) me(O) an(O) italian(B-Cuisine) restaurant(O) which(O) has(O) received(O) high(B-Rating) ratings(I-Rating) for(I-Rating) its(I-Rating) service(I-Rating)","words":["find","me","an","italian","restaurant","which","has","received","high","ratings","for","its","service"],"labels":["O","O","O","B-Cuisine","O","O","O","O","B-Rating","I-Rating","I-Rating","I-Rating","I-Rating"]},"label_list":["Dish","Restaurant Name","Price","Hours","Rating","Amenity","Cuisine","Location"]}
{"id":"470","dataset":"mit-restaurant","split":"test","instance":{"id":"470","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Cuisine, Dish, Location, Amenity, Hours, Restaurant Name, Rating and O.\nSentence: give me the locations of the buffets in town","prompt_labels":"give(O) me(O) the(O) locations(B-Location) of(O) the(O) buffets(B-Cuisine) in(B-Location) town(I-Location)","words":["give","me","the","locations","of","the","buffets","in","town"],"labels":["O","O","O","B-Location","O","O","B-Cuisine","B-Location","I-Location"]},"label_list":["Price","Cuisine","Dish","Location","Amenity","Hours","Restaurant Name","Rating"]}
{"id":"831","dataset":"mit-restaurant","split":"test","instance":{"id":"831","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Cuisine, Hours, Location, Restaurant Name, Rating, Price, Dish and O.\nSentence: is there a mcdonalds between here and my destination","prompt_labels":"is(O) there(O) a(O) mcdonalds(B-Restaurant Name) between(B-Location) here(I-Location) and(I-Location) my(I-Location) destination(I-Location)","words":["is","there","a","mcdonalds","between","here","and","my","destination"],"labels":["O","O","O","B-Restaurant Name","B-Location","I-Location","I-Location","I-Location","I-Location"]},"label_list":["Amenity","Cuisine","Hours","Location","Restaurant Name","Rating","Price","Dish"]}
{"id":"994","dataset":"mit-restaurant","split":"test","instance":{"id":"994","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Price, Restaurant Name, Dish, Location, Hours, Rating, Cuisine and O.\nSentence: make a reservation for us at the french restaurant for tonight at 8 pm also give us the dress code for the restaurant","prompt_labels":"make(O) a(O) reservation(O) for(O) us(O) at(O) the(O) french(B-Cuisine) restaurant(O) for(O) tonight(O) at(O) 8(O) pm(O) also(O) give(O) us(O) the(O) dress(B-Amenity) code(I-Amenity) for(O) the(O) restaurant(O)","words":["make","a","reservation","for","us","at","the","french","restaurant","for","tonight","at","8","pm","also","give","us","the","dress","code","for","the","restaurant"],"labels":["O","O","O","O","O","O","O","B-Cuisine","O","O","O","O","O","O","O","O","O","O","B-Amenity","I-Amenity","O","O","O"]},"label_list":["Amenity","Price","Restaurant Name","Dish","Location","Hours","Rating","Cuisine"]}
{"id":"1170","dataset":"mit-restaurant","split":"test","instance":{"id":"1170","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Rating, Dish, Price, Restaurant Name, Amenity, Hours, Cuisine and O.\nSentence: what places are cheap here","prompt_labels":"what(O) places(O) are(O) cheap(B-Price) here(B-Location)","words":["what","places","are","cheap","here"],"labels":["O","O","O","B-Price","B-Location"]},"label_list":["Location","Rating","Dish","Price","Restaurant Name","Amenity","Hours","Cuisine"]}
{"id":"1322","dataset":"mit-restaurant","split":"test","instance":{"id":"1322","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Location, Cuisine, Dish, Restaurant Name, Rating, Price, Amenity and O.\nSentence: where can i get barbecue no farther than 5 miles","prompt_labels":"where(O) can(O) i(O) get(O) barbecue(B-Cuisine) no(O) farther(O) than(O) 5(B-Location) miles(I-Location)","words":["where","can","i","get","barbecue","no","farther","than","5","miles"],"labels":["O","O","O","O","B-Cuisine","O","O","O","B-Location","I-Location"]},"label_list":["Hours","Location","Cuisine","Dish","Restaurant Name","Rating","Price","Amenity"]}
{"id":"1335","dataset":"mit-restaurant","split":"test","instance":{"id":"1335","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Restaurant Name, Price, Amenity, Dish, Location, Hours, Rating and O.\nSentence: where can i get some fast food","prompt_labels":"where(O) can(O) i(O) get(O) some(O) fast(B-Cuisine) food(I-Cuisine)","words":["where","can","i","get","some","fast","food"],"labels":["O","O","O","O","O","B-Cuisine","I-Cuisine"]},"label_list":["Cuisine","Restaurant Name","Price","Amenity","Dish","Location","Hours","Rating"]}
{"id":"1515","dataset":"mit-restaurant","split":"test","instance":{"id":"1515","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Hours, Cuisine, Location, Price, Amenity, Restaurant Name, Dish and O.\nSentence: will i be able to find a romantic restaurant for my date tonight","prompt_labels":"will(O) i(O) be(O) able(O) to(O) find(O) a(O) romantic(B-Amenity) restaurant(O) for(O) my(O) date(O) tonight(B-Hours)","words":["will","i","be","able","to","find","a","romantic","restaurant","for","my","date","tonight"],"labels":["O","O","O","O","O","O","O","B-Amenity","O","O","O","O","B-Hours"]},"label_list":["Rating","Hours","Cuisine","Location","Price","Amenity","Restaurant Name","Dish"]}
