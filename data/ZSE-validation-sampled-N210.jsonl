{"id":"2","dataset":"mit-movie","split":"dev","instance":{"id":"2","prompt_labels":"list(O) the(O) five(B-average ratings) star(I-average ratings) rated(O) movies(O) starring(O) mel(B-actor) gibson(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, average ratings, song, review, trailer, plot, title, genre, actor, rating, character, year and O.\nSentence: list the five star rated movies starring mel gibson","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","the","five","star","rated","movies","starring","mel","gibson"],"labels":["O","O","B-average ratings","I-average ratings","O","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["director","average_ratings","song","review","trailer","plot","title","genre","actor","rating","character","year"]}
{"id":"7","dataset":"mit-movie","split":"dev","instance":{"id":"7","prompt_labels":"can(O) you(O) get(O) a(O) soundtrac(B-song) for(O) the(O) harry(B-title) potter(I-title) films(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, trailer, review, genre, year, character, title, rating, average ratings, director, song, plot and O.\nSentence: can you get a soundtrac for the harry potter films","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","get","a","soundtrac","for","the","harry","potter","films"],"labels":["O","O","O","O","B-song","O","O","B-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["actor","trailer","review","genre","year","character","title","rating","average_ratings","director","song","plot"]}
{"id":"17","dataset":"mit-movie","split":"dev","instance":{"id":"17","prompt_labels":"are(O) there(O) any(O) films(O) directed(O) by(O) shawn(B-director) levy(I-director) about(O) large(B-plot) families(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, genre, plot, director, rating, title, trailer, character, year, actor, average ratings and O.\nSentence: are there any films directed by shawn levy about large families","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","films","directed","by","shawn","levy","about","large","families"],"labels":["O","O","O","O","O","O","B-director","I-director","O","B-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["review","song","genre","plot","director","rating","title","trailer","character","year","actor","average_ratings"]}
{"id":"38","dataset":"mit-movie","split":"dev","instance":{"id":"38","prompt_labels":"are(O) there(O) any(O) drama(B-genre) movies(O) with(O) seth(B-actor) green(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, title, trailer, plot, character, genre, actor, director, average ratings, review, rating, year and O.\nSentence: are there any drama movies with seth green","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","drama","movies","with","seth","green"],"labels":["O","O","O","B-genre","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["song","title","trailer","plot","character","genre","actor","director","average_ratings","review","rating","year"]}
{"id":"42","dataset":"mit-movie","split":"dev","instance":{"id":"42","prompt_labels":"show(O) me(O) the(O) milos(B-actor) forman(I-actor) movies(O) from(O) the(O) 1980s(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, average ratings, plot, genre, character, year, review, song, actor, director, trailer, title and O.\nSentence: show me the milos forman movies from the 1980s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","the","milos","forman","movies","from","the","1980s"],"labels":["O","O","O","B-actor","I-actor","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["rating","average_ratings","plot","genre","character","year","review","song","actor","director","trailer","title"]}
{"id":"49","dataset":"mit-movie","split":"dev","instance":{"id":"49","prompt_labels":"show(O) me(O) morgan(B-actor) freeman(I-actor) movies(O) from(O) the(O) 90s(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, genre, rating, character, plot, title, average ratings, song, year, actor, review, director and O.\nSentence: show me morgan freeman movies from the 90s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","morgan","freeman","movies","from","the","90s"],"labels":["O","O","B-actor","I-actor","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["trailer","genre","rating","character","plot","title","average_ratings","song","year","actor","review","director"]}
{"id":"68","dataset":"mit-movie","split":"dev","instance":{"id":"68","prompt_labels":"the(O) new(O) batman(B-title) movie(O) looks(O) epic(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, character, rating, genre, actor, plot, title, year, trailer, director, average ratings and O.\nSentence: the new batman movie looks epic","prediction_output":null,"prediction_outputs":null,"group":null,"words":["the","new","batman","movie","looks","epic"],"labels":["O","O","B-title","O","O","O"],"target_index":null,"target_label":null},"label_list":["review","song","character","rating","genre","actor","plot","title","year","trailer","director","average_ratings"]}
{"id":"74","dataset":"mit-movie","split":"dev","instance":{"id":"74","prompt_labels":"who(O) was(O) the(O) actress(O) in(O) the(B-title) goodbye(I-title) girl(I-title) with(O) richard(B-actor) dreyfuss(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, average ratings, plot, review, genre, year, title, trailer, character, rating, song, actor and O.\nSentence: who was the actress in the goodbye girl with richard dreyfuss","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","was","the","actress","in","the","goodbye","girl","with","richard","dreyfuss"],"labels":["O","O","O","O","O","B-title","I-title","I-title","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["director","average_ratings","plot","review","genre","year","title","trailer","character","rating","song","actor"]}
{"id":"81","dataset":"mit-movie","split":"dev","instance":{"id":"81","prompt_labels":"i(O) would(O) like(O) a(O) list(O) of(O) movies(O) about(O) dancing(B-plot) from(O) the(O) past(B-year) 10(I-year) years(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, year, rating, genre, director, title, actor, song, average ratings, plot, trailer, review and O.\nSentence: i would like a list of movies about dancing from the past 10 years","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","would","like","a","list","of","movies","about","dancing","from","the","past","10","years"],"labels":["O","O","O","O","O","O","O","O","B-plot","O","O","B-year","I-year","I-year"],"target_index":null,"target_label":null},"label_list":["character","year","rating","genre","director","title","actor","song","average_ratings","plot","trailer","review"]}
{"id":"83","dataset":"mit-movie","split":"dev","instance":{"id":"83","prompt_labels":"find(O) action(B-genre) movies(O) featuring(O) comic(B-plot) book(I-plot) characters(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, director, rating, year, character, song, plot, actor, trailer, average ratings, title, review and O.\nSentence: find action movies featuring comic book characters","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","action","movies","featuring","comic","book","characters"],"labels":["O","B-genre","O","O","B-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["genre","director","rating","year","character","song","plot","actor","trailer","average_ratings","title","review"]}
{"id":"85","dataset":"mit-movie","split":"dev","instance":{"id":"85","prompt_labels":"name(O) a(O) movie(O) starring(O) britney(B-actor) spears(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, average ratings, year, plot, song, trailer, rating, director, actor, genre, title, review and O.\nSentence: name a movie starring britney spears","prediction_output":null,"prediction_outputs":null,"group":null,"words":["name","a","movie","starring","britney","spears"],"labels":["O","O","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["character","average_ratings","year","plot","song","trailer","rating","director","actor","genre","title","review"]}
{"id":"92","dataset":"mit-movie","split":"dev","instance":{"id":"92","prompt_labels":"what(O) science(B-genre) fiction(I-genre) movies(O) were(O) directed(O) by(O) george(B-director) lucas(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, rating, character, year, director, plot, review, trailer, title, genre, song, average ratings and O.\nSentence: what science fiction movies were directed by george lucas","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","science","fiction","movies","were","directed","by","george","lucas"],"labels":["O","B-genre","I-genre","O","O","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["actor","rating","character","year","director","plot","review","trailer","title","genre","song","average_ratings"]}
{"id":"98","dataset":"mit-movie","split":"dev","instance":{"id":"98","prompt_labels":"i(O) am(O) looking(O) for(O) a(O) movie(O) about(O) talking(B-plot) animals(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, genre, average ratings, rating, character, review, trailer, director, actor, plot, song, title and O.\nSentence: i am looking for a movie about talking animals","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","am","looking","for","a","movie","about","talking","animals"],"labels":["O","O","O","O","O","O","O","B-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["year","genre","average_ratings","rating","character","review","trailer","director","actor","plot","song","title"]}
{"id":"103","dataset":"mit-movie","split":"dev","instance":{"id":"103","prompt_labels":"did(O) angelina(B-actor) jolie(I-actor) play(O) a(O) russian(B-plot) in(O) salt(B-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, year, character, title, plot, trailer, director, genre, song, actor, review, rating and O.\nSentence: did angelina jolie play a russian in salt","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","angelina","jolie","play","a","russian","in","salt"],"labels":["O","B-actor","I-actor","O","O","B-plot","O","B-title"],"target_index":null,"target_label":null},"label_list":["average_ratings","year","character","title","plot","trailer","director","genre","song","actor","review","rating"]}
{"id":"107","dataset":"mit-movie","split":"dev","instance":{"id":"107","prompt_labels":"has(O) ashton(B-actor) kutcher(I-actor) made(O) any(O) movies(O) with(O) zombies(B-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, song, actor, rating, director, review, title, trailer, year, character, plot, average ratings and O.\nSentence: has ashton kutcher made any movies with zombies","prediction_output":null,"prediction_outputs":null,"group":null,"words":["has","ashton","kutcher","made","any","movies","with","zombies"],"labels":["O","B-actor","I-actor","O","O","O","O","B-plot"],"target_index":null,"target_label":null},"label_list":["genre","song","actor","rating","director","review","title","trailer","year","character","plot","average_ratings"]}
{"id":"118","dataset":"mit-movie","split":"dev","instance":{"id":"118","prompt_labels":"show(O) me(O) a(O) good(O) spy(B-genre) movie(O) based(O) in(O) england(B-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, plot, review, character, song, average ratings, director, rating, title, year, actor, trailer and O.\nSentence: show me a good spy movie based in england","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","a","good","spy","movie","based","in","england"],"labels":["O","O","O","O","B-genre","O","O","O","B-plot"],"target_index":null,"target_label":null},"label_list":["genre","plot","review","character","song","average_ratings","director","rating","title","year","actor","trailer"]}
{"id":"131","dataset":"mit-movie","split":"dev","instance":{"id":"131","prompt_labels":"what(O) was(O) the(O) release(B-year) year(I-year) for(O) notes(B-title) on(I-title) a(I-title) scandal(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, review, genre, average ratings, rating, trailer, actor, song, title, plot, year, director and O.\nSentence: what was the release year for notes on a scandal","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","release","year","for","notes","on","a","scandal"],"labels":["O","O","O","B-year","I-year","O","B-title","I-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["character","review","genre","average_ratings","rating","trailer","actor","song","title","plot","year","director"]}
{"id":"137","dataset":"mit-movie","split":"dev","instance":{"id":"137","prompt_labels":"did(O) guillermo(B-director) del(I-director) toro(I-director) direct(O) any(O) moves(O) rated(O) pg(B-rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, plot, genre, title, character, actor, rating, song, director, year, average ratings, trailer and O.\nSentence: did guillermo del toro direct any moves rated pg","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","guillermo","del","toro","direct","any","moves","rated","pg"],"labels":["O","B-director","I-director","I-director","O","O","O","O","B-rating"],"target_index":null,"target_label":null},"label_list":["review","plot","genre","title","character","actor","rating","song","director","year","average_ratings","trailer"]}
{"id":"138","dataset":"mit-movie","split":"dev","instance":{"id":"138","prompt_labels":"who(O) played(O) james(B-character) in(O) james(B-title) and(I-title) the(I-title) giant(I-title) peach(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, average ratings, genre, trailer, actor, director, song, year, character, title, rating, review and O.\nSentence: who played james in james and the giant peach","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","played","james","in","james","and","the","giant","peach"],"labels":["O","O","B-character","O","B-title","I-title","I-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["plot","average_ratings","genre","trailer","actor","director","song","year","character","title","rating","review"]}
{"id":"143","dataset":"mit-movie","split":"dev","instance":{"id":"143","prompt_labels":"any(O) good(B-review) horror(B-genre) films(O) that(O) came(O) out(O) in(O) 2008(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, actor, review, title, year, average ratings, director, trailer, genre, character, song, rating and O.\nSentence: any good horror films that came out in 2008","prediction_output":null,"prediction_outputs":null,"group":null,"words":["any","good","horror","films","that","came","out","in","2008"],"labels":["O","B-review","B-genre","O","O","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["plot","actor","review","title","year","average_ratings","director","trailer","genre","character","song","rating"]}
{"id":"147","dataset":"mit-movie","split":"dev","instance":{"id":"147","prompt_labels":"what(O) is(O) a(O) recent(O) george(B-actor) clooney(I-actor) movie(O) with(O) high(B-average ratings) viewers(I-average ratings) rating(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, plot, average ratings, actor, director, year, review, song, rating, title, genre, trailer and O.\nSentence: what is a recent george clooney movie with high viewers rating","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","a","recent","george","clooney","movie","with","high","viewers","rating"],"labels":["O","O","O","O","B-actor","I-actor","O","O","B-average ratings","I-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["character","plot","average_ratings","actor","director","year","review","song","rating","title","genre","trailer"]}
{"id":"152","dataset":"mit-movie","split":"dev","instance":{"id":"152","prompt_labels":"show(O) me(O) terry(B-director) gilliam(I-director) movies(O) starring(O) jeff(B-actor) bridges(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, director, genre, review, rating, actor, character, plot, song, average ratings, year, trailer and O.\nSentence: show me terry gilliam movies starring jeff bridges","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","terry","gilliam","movies","starring","jeff","bridges"],"labels":["O","O","B-director","I-director","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["title","director","genre","review","rating","actor","character","plot","song","average_ratings","year","trailer"]}
{"id":"156","dataset":"mit-movie","split":"dev","instance":{"id":"156","prompt_labels":"who(O) directed(B-director) 310(B-title) to(I-title) yuma(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, year, review, genre, director, trailer, plot, character, title, actor, rating, song and O.\nSentence: who directed 310 to yuma","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","directed","310","to","yuma"],"labels":["O","B-director","B-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["average_ratings","year","review","genre","director","trailer","plot","character","title","actor","rating","song"]}
{"id":"158","dataset":"mit-movie","split":"dev","instance":{"id":"158","prompt_labels":"is(O) there(O) a(O) horror(B-genre) movie(O) starring(O) jennifer(B-actor) lopez(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, character, year, title, trailer, plot, genre, song, average ratings, director, review, rating and O.\nSentence: is there a horror movie starring jennifer lopez","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","horror","movie","starring","jennifer","lopez"],"labels":["O","O","O","B-genre","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["actor","character","year","title","trailer","plot","genre","song","average_ratings","director","review","rating"]}
{"id":"165","dataset":"mit-movie","split":"dev","instance":{"id":"165","prompt_labels":"what(O) was(O) the(B-title) fog(I-title) rated(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, song, trailer, actor, average ratings, genre, review, character, year, rating, director, plot and O.\nSentence: what was the fog rated","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","fog","rated"],"labels":["O","O","B-title","I-title","O"],"target_index":null,"target_label":null},"label_list":["title","song","trailer","actor","average_ratings","genre","review","character","year","rating","director","plot"]}
{"id":"167","dataset":"mit-movie","split":"dev","instance":{"id":"167","prompt_labels":"what(O) was(O) goodfellas(B-title) rated(B-rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, year, character, director, plot, genre, average ratings, song, actor, review, trailer, title and O.\nSentence: what was goodfellas rated","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","goodfellas","rated"],"labels":["O","O","B-title","B-rating"],"target_index":null,"target_label":null},"label_list":["rating","year","character","director","plot","genre","average_ratings","song","actor","review","trailer","title"]}
{"id":"170","dataset":"mit-movie","split":"dev","instance":{"id":"170","prompt_labels":"what(O) are(O) some(O) horror(B-genre) movies(O) from(O) the(O) 1970s(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, trailer, review, director, year, plot, character, title, genre, actor, song, average ratings and O.\nSentence: what are some horror movies from the 1970s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","some","horror","movies","from","the","1970s"],"labels":["O","O","O","B-genre","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["rating","trailer","review","director","year","plot","character","title","genre","actor","song","average_ratings"]}
{"id":"176","dataset":"mit-movie","split":"dev","instance":{"id":"176","prompt_labels":"directors(O) of(O) all(O) the(O) batman(B-title) movies(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, title, genre, average ratings, character, director, trailer, review, rating, plot, song, actor and O.\nSentence: directors of all the batman movies","prediction_output":null,"prediction_outputs":null,"group":null,"words":["directors","of","all","the","batman","movies"],"labels":["O","O","O","O","B-title","O"],"target_index":null,"target_label":null},"label_list":["year","title","genre","average_ratings","character","director","trailer","review","rating","plot","song","actor"]}
{"id":"197","dataset":"mit-movie","split":"dev","instance":{"id":"197","prompt_labels":"name(O) a(O) kirk(B-actor) douglas(I-actor) science(B-genre) fiction(I-genre) film(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, actor, director, title, review, year, plot, song, genre, average ratings, trailer, character and O.\nSentence: name a kirk douglas science fiction film","prediction_output":null,"prediction_outputs":null,"group":null,"words":["name","a","kirk","douglas","science","fiction","film"],"labels":["O","O","B-actor","I-actor","B-genre","I-genre","O"],"target_index":null,"target_label":null},"label_list":["rating","actor","director","title","review","year","plot","song","genre","average_ratings","trailer","character"]}
{"id":"199","dataset":"mit-movie","split":"dev","instance":{"id":"199","prompt_labels":"who(O) stars(O) in(O) the(O) girl(B-title) with(I-title) the(I-title) dragon(I-title) tattoo(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, trailer, year, title, rating, song, genre, director, review, average ratings, character, actor and O.\nSentence: who stars in the girl with the dragon tattoo","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","stars","in","the","girl","with","the","dragon","tattoo"],"labels":["O","O","O","O","B-title","I-title","I-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["plot","trailer","year","title","rating","song","genre","director","review","average_ratings","character","actor"]}
{"id":"4","dataset":"mit-restaurant","split":"dev","instance":{"id":"4","prompt_labels":"any(O) good(O) cheap(B-Price) german(B-Cuisine) restaurants(O) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Amenity, Cuisine, Price, Location, Rating, Restaurant Name, Hours and O.\nSentence: any good cheap german restaurants nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["any","good","cheap","german","restaurants","nearby"],"labels":["O","O","B-Price","B-Cuisine","O","B-Location"],"target_index":null,"target_label":null},"label_list":["dish","amenity","cuisine","price","location","rating","restaurant_name","hours"]}
{"id":"9","dataset":"mit-restaurant","split":"dev","instance":{"id":"9","prompt_labels":"any(O) place(O) along(B-Location) the(I-Location) road(I-Location) has(O) a(O) good(B-Rating) beer(B-Dish) selection(O) that(O) also(O) serves(O) ribs(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Location, Rating, Restaurant Name, Dish, Cuisine, Amenity, Price and O.\nSentence: any place along the road has a good beer selection that also serves ribs","prediction_output":null,"prediction_outputs":null,"group":null,"words":["any","place","along","the","road","has","a","good","beer","selection","that","also","serves","ribs"],"labels":["O","O","B-Location","I-Location","I-Location","O","O","B-Rating","B-Dish","O","O","O","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["hours","location","rating","restaurant_name","dish","cuisine","amenity","price"]}
{"id":"10","dataset":"mit-restaurant","split":"dev","instance":{"id":"10","prompt_labels":"any(O) places(O) around(B-Location) here(I-Location) that(O) has(O) a(O) nice(B-Amenity) view(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Cuisine, Location, Amenity, Rating, Hours, Restaurant Name, Dish and O.\nSentence: any places around here that has a nice view","prediction_output":null,"prediction_outputs":null,"group":null,"words":["any","places","around","here","that","has","a","nice","view"],"labels":["O","O","B-Location","I-Location","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["price","cuisine","location","amenity","rating","hours","restaurant_name","dish"]}
{"id":"15","dataset":"mit-restaurant","split":"dev","instance":{"id":"15","prompt_labels":"anything(O) on(O) the(O) avenue(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Cuisine, Restaurant Name, Amenity, Rating, Hours, Location, Price and O.\nSentence: anything on the avenue","prediction_output":null,"prediction_outputs":null,"group":null,"words":["anything","on","the","avenue"],"labels":["O","O","O","B-Location"],"target_index":null,"target_label":null},"label_list":["dish","cuisine","restaurant_name","amenity","rating","hours","location","price"]}
{"id":"18","dataset":"mit-restaurant","split":"dev","instance":{"id":"18","prompt_labels":"are(O) reservations(O) available(O) for(O) four(O) people(O) for(O) 8(O) pm(O) tonight(O) at(O) 112(B-Restaurant Name) eatery(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Amenity, Rating, Dish, Location, Cuisine, Price and O.\nSentence: are reservations available for four people for 8 pm tonight at 112 eatery","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","reservations","available","for","four","people","for","8","pm","tonight","at","112","eatery"],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["hours","restaurant_name","amenity","rating","dish","location","cuisine","price"]}
{"id":"33","dataset":"mit-restaurant","split":"dev","instance":{"id":"33","prompt_labels":"are(O) there(O) any(O) donut(B-Restaurant Name) and(I-Restaurant Name) donuts(I-Restaurant Name) within(B-Location) 5(I-Location) minutes(I-Location) drive(I-Location) that(O) has(O) an(O) extensive(B-Amenity) beer(I-Amenity) menu(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Location, Price, Dish, Amenity, Cuisine, Rating and O.\nSentence: are there any donut and donuts within 5 minutes drive that has an extensive beer menu","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","donut","and","donuts","within","5","minutes","drive","that","has","an","extensive","beer","menu"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name","B-Location","I-Location","I-Location","I-Location","O","O","O","B-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["hours","restaurant_name","location","price","dish","amenity","cuisine","rating"]}
{"id":"34","dataset":"mit-restaurant","split":"dev","instance":{"id":"34","prompt_labels":"are(O) there(O) any(O) eatery(O) at(O) the(O) hotel(B-Location) downtown(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Dish, Rating, Amenity, Cuisine, Restaurant Name, Hours, Location and O.\nSentence: are there any eatery at the hotel downtown","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","eatery","at","the","hotel","downtown"],"labels":["O","O","O","O","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["price","dish","rating","amenity","cuisine","restaurant_name","hours","location"]}
{"id":"51","dataset":"mit-restaurant","split":"dev","instance":{"id":"51","prompt_labels":"are(O) there(O) any(O) italian(B-Cuisine) eateries(O) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Price, Rating, Location, Dish, Amenity, Cuisine and O.\nSentence: are there any italian eateries nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","italian","eateries","nearby"],"labels":["O","O","O","B-Cuisine","O","B-Location"],"target_index":null,"target_label":null},"label_list":["restaurant_name","hours","price","rating","location","dish","amenity","cuisine"]}
{"id":"54","dataset":"mit-restaurant","split":"dev","instance":{"id":"54","prompt_labels":"are(O) there(O) any(O) kid(B-Amenity) friendly(I-Amenity) restaurants(O) close(B-Location) by(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Restaurant Name, Hours, Cuisine, Location, Rating, Dish and O.\nSentence: are there any kid friendly restaurants close by","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","kid","friendly","restaurants","close","by"],"labels":["O","O","O","B-Amenity","I-Amenity","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["price","amenity","restaurant_name","hours","cuisine","location","rating","dish"]}
{"id":"62","dataset":"mit-restaurant","split":"dev","instance":{"id":"62","prompt_labels":"are(O) there(O) any(O) nice(B-Rating) taco(B-Cuisine) places(O) nearby(B-Location) open(B-Hours) for(I-Hours) breakfast(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Cuisine, Rating, Hours, Amenity, Restaurant Name, Price, Location and O.\nSentence: are there any nice taco places nearby open for breakfast","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","nice","taco","places","nearby","open","for","breakfast"],"labels":["O","O","O","B-Rating","B-Cuisine","O","B-Location","B-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["dish","cuisine","rating","hours","amenity","restaurant_name","price","location"]}
{"id":"64","dataset":"mit-restaurant","split":"dev","instance":{"id":"64","prompt_labels":"are(O) there(O) any(O) places(O) around(B-Location) here(I-Location) that(O) has(O) tomato(B-Dish) sauce(I-Dish) based(I-Dish) dishes(I-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Dish, Restaurant Name, Amenity, Hours, Location, Cuisine, Rating and O.\nSentence: are there any places around here that has tomato sauce based dishes","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","places","around","here","that","has","tomato","sauce","based","dishes"],"labels":["O","O","O","O","B-Location","I-Location","O","O","B-Dish","I-Dish","I-Dish","I-Dish"],"target_index":null,"target_label":null},"label_list":["price","dish","restaurant_name","amenity","hours","location","cuisine","rating"]}
{"id":"89","dataset":"mit-restaurant","split":"dev","instance":{"id":"89","prompt_labels":"are(O) there(O) any(O) spanish(B-Cuisine) restaurants(O) that(O) are(O) cheap(B-Price)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Cuisine, Price, Amenity, Hours, Restaurant Name, Rating, Dish and O.\nSentence: are there any spanish restaurants that are cheap","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","spanish","restaurants","that","are","cheap"],"labels":["O","O","O","B-Cuisine","O","O","O","B-Price"],"target_index":null,"target_label":null},"label_list":["location","cuisine","price","amenity","hours","restaurant_name","rating","dish"]}
{"id":"91","dataset":"mit-restaurant","split":"dev","instance":{"id":"91","prompt_labels":"are(O) there(O) any(O) sub(B-Cuisine) sandwich(I-Cuisine) shops(O) that(O) also(O) serve(O) beer(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Price, Rating, Location, Cuisine, Hours, Restaurant Name, Amenity and O.\nSentence: are there any sub sandwich shops that also serve beer","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","sub","sandwich","shops","that","also","serve","beer"],"labels":["O","O","O","B-Cuisine","I-Cuisine","O","O","O","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["dish","price","rating","location","cuisine","hours","restaurant_name","amenity"]}
{"id":"111","dataset":"mit-restaurant","split":"dev","instance":{"id":"111","prompt_labels":"call(O) cheeseboard(B-Restaurant Name) in(O) berkeley(B-Location) for(O) me(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Hours, Amenity, Price, Cuisine, Rating, Location, Restaurant Name and O.\nSentence: call cheeseboard in berkeley for me","prediction_output":null,"prediction_outputs":null,"group":null,"words":["call","cheeseboard","in","berkeley","for","me"],"labels":["O","B-Restaurant Name","O","B-Location","O","O"],"target_index":null,"target_label":null},"label_list":["dish","hours","amenity","price","cuisine","rating","location","restaurant_name"]}
{"id":"116","dataset":"mit-restaurant","split":"dev","instance":{"id":"116","prompt_labels":"can(O) i(O) bring(O) my(O) pet(B-Amenity) iguana(O) to(O) the(O) japanese(B-Cuisine) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Location, Dish, Cuisine, Restaurant Name, Hours, Amenity, Rating and O.\nSentence: can i bring my pet iguana to the japanese restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","i","bring","my","pet","iguana","to","the","japanese","restaurant"],"labels":["O","O","O","O","B-Amenity","O","O","O","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["price","location","dish","cuisine","restaurant_name","hours","amenity","rating"]}
{"id":"128","dataset":"mit-restaurant","split":"dev","instance":{"id":"128","prompt_labels":"can(O) i(O) have(O) the(O) phone(O) number(O) for(O) kfc(B-Restaurant Name) in(O) los(B-Location) angeles(I-Location) ca(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Amenity, Hours, Cuisine, Dish, Restaurant Name, Rating, Price and O.\nSentence: can i have the phone number for kfc in los angeles ca","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","i","have","the","phone","number","for","kfc","in","los","angeles","ca"],"labels":["O","O","O","O","O","O","O","B-Restaurant Name","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["location","amenity","hours","cuisine","dish","restaurant_name","rating","price"]}
{"id":"132","dataset":"mit-restaurant","split":"dev","instance":{"id":"132","prompt_labels":"can(O) you(O) find(O) a(O) bar(B-Amenity) that(O) serves(O) tapas(B-Dish) and(O) takes(O) reservations(B-Amenity) for(O) happy(B-Amenity) hour(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Amenity, Restaurant Name, Rating, Dish, Price, Hours, Location and O.\nSentence: can you find a bar that serves tapas and takes reservations for happy hour","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","a","bar","that","serves","tapas","and","takes","reservations","for","happy","hour"],"labels":["O","O","O","O","B-Amenity","O","O","B-Dish","O","O","B-Amenity","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["cuisine","amenity","restaurant_name","rating","dish","price","hours","location"]}
{"id":"133","dataset":"mit-restaurant","split":"dev","instance":{"id":"133","prompt_labels":"can(O) you(O) find(O) a(O) burger(B-Cuisine) joint(I-Cuisine) with(O) a(O) smoking(B-Amenity) section(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Location, Hours, Cuisine, Rating, Price, Dish, Amenity and O.\nSentence: can you find a burger joint with a smoking section","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","a","burger","joint","with","a","smoking","section"],"labels":["O","O","O","O","B-Cuisine","I-Cuisine","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["restaurant_name","location","hours","cuisine","rating","price","dish","amenity"]}
{"id":"136","dataset":"mit-restaurant","split":"dev","instance":{"id":"136","prompt_labels":"can(O) you(O) find(O) a(O) highly(B-Rating) rated(I-Rating) long(B-Restaurant Name) john(I-Restaurant Name) silvers(I-Restaurant Name) open(B-Hours) this(I-Hours) late(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Price, Amenity, Restaurant Name, Location, Rating, Dish and O.\nSentence: can you find a highly rated long john silvers open this late","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","a","highly","rated","long","john","silvers","open","this","late"],"labels":["O","O","O","O","B-Rating","I-Rating","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name","B-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["hours","cuisine","price","amenity","restaurant_name","location","rating","dish"]}
{"id":"144","dataset":"mit-restaurant","split":"dev","instance":{"id":"144","prompt_labels":"can(O) you(O) find(O) a(O) site(O) where(O) i(O) can(O) see(O) reviews(O) on(O) restaurant(O) downtown(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Restaurant Name, Price, Hours, Amenity, Rating, Dish, Cuisine and O.\nSentence: can you find a site where i can see reviews on restaurant downtown","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","a","site","where","i","can","see","reviews","on","restaurant","downtown"],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-Location"],"target_index":null,"target_label":null},"label_list":["location","restaurant_name","price","hours","amenity","rating","dish","cuisine"]}
{"id":"145","dataset":"mit-restaurant","split":"dev","instance":{"id":"145","prompt_labels":"can(O) you(O) find(O) a(O) steak(B-Cuisine) house(I-Cuisine) that(O) serves(O) wine(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Cuisine, Rating, Hours, Restaurant Name, Amenity, Price, Dish and O.\nSentence: can you find a steak house that serves wine","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","a","steak","house","that","serves","wine"],"labels":["O","O","O","O","B-Cuisine","I-Cuisine","O","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["location","cuisine","rating","hours","restaurant_name","amenity","price","dish"]}
{"id":"146","dataset":"mit-restaurant","split":"dev","instance":{"id":"146","prompt_labels":"can(O) you(O) find(O) a(O) thai(B-Cuisine) japanese(I-Cuisine) fusion(I-Cuisine) restaurant(O) in(B-Location) town(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Hours, Location, Amenity, Price, Dish, Cuisine and O.\nSentence: can you find a thai japanese fusion restaurant in town","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","a","thai","japanese","fusion","restaurant","in","town"],"labels":["O","O","O","O","B-Cuisine","I-Cuisine","I-Cuisine","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["rating","restaurant_name","hours","location","amenity","price","dish","cuisine"]}
{"id":"161","dataset":"mit-restaurant","split":"dev","instance":{"id":"161","prompt_labels":"can(O) you(O) find(O) me(O) a(O) pizzeria(B-Cuisine) that(O) delivers(B-Amenity) after(B-Hours) midnight(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Restaurant Name, Hours, Dish, Location, Price, Cuisine and O.\nSentence: can you find me a pizzeria that delivers after midnight","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","me","a","pizzeria","that","delivers","after","midnight"],"labels":["O","O","O","O","O","B-Cuisine","O","B-Amenity","B-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["amenity","rating","restaurant_name","hours","dish","location","price","cuisine"]}
{"id":"167","dataset":"mit-restaurant","split":"dev","instance":{"id":"167","prompt_labels":"can(O) you(O) find(O) me(O) chinese(B-Cuisine) restaurant(O) with(O) a(O) smoking(B-Amenity) section(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Location, Rating, Price, Restaurant Name, Cuisine, Dish, Hours and O.\nSentence: can you find me chinese restaurant with a smoking section","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","me","chinese","restaurant","with","a","smoking","section"],"labels":["O","O","O","O","B-Cuisine","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["amenity","location","rating","price","restaurant_name","cuisine","dish","hours"]}
{"id":"171","dataset":"mit-restaurant","split":"dev","instance":{"id":"171","prompt_labels":"can(O) you(O) find(O) out(O) if(O) the(O) best(B-Restaurant Name) little(I-Restaurant Name) restaurant(I-Restaurant Name) has(O) dancing(B-Amenity) and(O) a(O) good(B-Amenity) looking(I-Amenity) atmosphere(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Price, Restaurant Name, Dish, Cuisine, Location, Hours and O.\nSentence: can you find out if the best little restaurant has dancing and a good looking atmosphere","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","out","if","the","best","little","restaurant","has","dancing","and","a","good","looking","atmosphere"],"labels":["O","O","O","O","O","O","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name","O","B-Amenity","O","O","B-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["rating","amenity","price","restaurant_name","dish","cuisine","location","hours"]}
{"id":"175","dataset":"mit-restaurant","split":"dev","instance":{"id":"175","prompt_labels":"can(O) you(O) find(O) the(O) waterfront(B-Location) restaurant(O) albertos(B-Restaurant Name) deli(I-Restaurant Name) of(O) course(O) thats(O) open(B-Hours) until(I-Hours) 11(I-Hours) pm(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Price, Rating, Cuisine, Dish, Location, Amenity, Restaurant Name and O.\nSentence: can you find the waterfront restaurant albertos deli of course thats open until 11 pm","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","the","waterfront","restaurant","albertos","deli","of","course","thats","open","until","11","pm"],"labels":["O","O","O","O","B-Location","O","B-Restaurant Name","I-Restaurant Name","O","O","O","B-Hours","I-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["hours","price","rating","cuisine","dish","location","amenity","restaurant_name"]}
{"id":"181","dataset":"mit-restaurant","split":"dev","instance":{"id":"181","prompt_labels":"can(O) you(O) help(O) me(O) find(O) a(O) fancy(B-Amenity) restaurant(O) with(O) 5(B-Rating) star(I-Rating) ratings(I-Rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Amenity, Hours, Cuisine, Dish, Restaurant Name, Rating, Price and O.\nSentence: can you help me find a fancy restaurant with 5 star ratings","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","help","me","find","a","fancy","restaurant","with","5","star","ratings"],"labels":["O","O","O","O","O","O","B-Amenity","O","O","B-Rating","I-Rating","I-Rating"],"target_index":null,"target_label":null},"label_list":["location","amenity","hours","cuisine","dish","restaurant_name","rating","price"]}
{"id":"182","dataset":"mit-restaurant","split":"dev","instance":{"id":"182","prompt_labels":"can(O) you(O) help(O) me(O) find(O) a(O) high(B-Price) end(I-Price) restaurant(O) where(O) i(O) can(O) have(O) lunch(B-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Rating, Cuisine, Restaurant Name, Price, Dish, Amenity, Location and O.\nSentence: can you help me find a high end restaurant where i can have lunch","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","help","me","find","a","high","end","restaurant","where","i","can","have","lunch"],"labels":["O","O","O","O","O","O","B-Price","I-Price","O","O","O","O","O","B-Amenity"],"target_index":null,"target_label":null},"label_list":["hours","rating","cuisine","restaurant_name","price","dish","amenity","location"]}
{"id":"183","dataset":"mit-restaurant","split":"dev","instance":{"id":"183","prompt_labels":"can(O) you(O) help(O) me(O) find(O) a(O) korean(B-Cuisine) restaurant(O) that(O) is(O) close(B-Location) by(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Dish, Price, Amenity, Location, Rating, Hours, Cuisine and O.\nSentence: can you help me find a korean restaurant that is close by","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","help","me","find","a","korean","restaurant","that","is","close","by"],"labels":["O","O","O","O","O","O","B-Cuisine","O","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["restaurant_name","dish","price","amenity","location","rating","hours","cuisine"]}
{"id":"198","dataset":"mit-restaurant","split":"dev","instance":{"id":"198","prompt_labels":"can(O) you(O) make(O) me(O) a(O) reservation(B-Cuisine) for(O) todai(B-Cuisine) for(O) thursday(B-Restaurant Name) for(O) two(O) people(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Cuisine, Rating, Amenity, Hours, Price, Restaurant Name and O.\nSentence: can you make me a reservation for todai for thursday for two people","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","make","me","a","reservation","for","todai","for","thursday","for","two","people"],"labels":["O","O","O","O","O","B-Cuisine","O","B-Cuisine","O","B-Restaurant Name","O","O","O"],"target_index":null,"target_label":null},"label_list":["dish","location","cuisine","rating","amenity","hours","price","restaurant_name"]}
{"id":"4","dataset":"crossner_ai","split":"dev","instance":{"id":"4","prompt_labels":"Segmenting(B-task) the(I-task) text(I-task) into(I-task) topics(I-task) or(O) discourse(O) turns(O) might(O) be(O) useful(O) in(O) some(O) natural(O) processing(O) tasks(O) :(O) it(O) can(O) improve(O) information(B-task) retrieval(I-task) or(O) speech(B-task) recognition(I-task) significantly(O) ((O) by(O) indexing(O) /(O) recognizing(O) documents(O) more(O) precisely(O) or(O) by(O) giving(O) the(O) specific(O) part(O) of(O) a(O) document(O) corresponding(O) to(O) the(O) query(O) as(O) a(O) result(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, task, location, product, country, metric, researcher, person, conference, algorithm, field, programming language, organization and O.\nSentence: Segmenting the text into topics or discourse turns might be useful in some natural processing tasks : it can improve information retrieval or speech recognition significantly ( by indexing / recognizing documents more precisely or by giving the specific part of a document corresponding to the query as a result ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Segmenting","the","text","into","topics","or","discourse","turns","might","be","useful","in","some","natural","processing","tasks",":","it","can","improve","information","retrieval","or","speech","recognition","significantly","(","by","indexing","/","recognizing","documents","more","precisely","or","by","giving","the","specific","part","of","a","document","corresponding","to","the","query","as","a","result",")","."],"labels":["B-task","I-task","I-task","I-task","I-task","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-task","I-task","O","B-task","I-task","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","task","location","product","country","metric","researcher","person","conference","algorithm","field","programming_language","organization"]}
{"id":"19","dataset":"crossner_ai","split":"dev","instance":{"id":"19","prompt_labels":"Unsupervised(B-field) learning(I-field) ,(O) on(O) the(O) other(O) hand(O) ,(O) assumes(O) training(O) data(O) that(O) has(O) not(O) been(O) hand-labeled(O) ,(O) and(O) attempts(O) to(O) find(O) inherent(O) patterns(O) in(O) the(O) data(O) that(O) can(O) then(O) be(O) used(O) to(O) determine(O) the(O) correct(O) output(O) value(O) for(O) new(O) data(O) instances(O) ..(O) A(O) combination(O) of(O) the(O) two(O) that(O) has(O) recently(O) been(O) explored(O) is(O) semi-supervised(B-field) learning(I-field) ,(O) which(O) uses(O) a(O) combination(O) of(O) labeled(O) and(O) unlabeled(O) data(O) ((O) typically(O) a(O) small(O) set(O) of(O) labeled(O) data(O) combined(O) with(O) a(O) large(O) amount(O) of(O) unlabeled(O) data(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, metric, person, researcher, algorithm, task, location, programming language, university, country, conference, field, organization and O.\nSentence: Unsupervised learning , on the other hand , assumes training data that has not been hand-labeled , and attempts to find inherent patterns in the data that can then be used to determine the correct output value for new data instances .. A combination of the two that has recently been explored is semi-supervised learning , which uses a combination of labeled and unlabeled data ( typically a small set of labeled data combined with a large amount of unlabeled data ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Unsupervised","learning",",","on","the","other","hand",",","assumes","training","data","that","has","not","been","hand-labeled",",","and","attempts","to","find","inherent","patterns","in","the","data","that","can","then","be","used","to","determine","the","correct","output","value","for","new","data","instances","..","A","combination","of","the","two","that","has","recently","been","explored","is","semi-supervised","learning",",","which","uses","a","combination","of","labeled","and","unlabeled","data","(","typically","a","small","set","of","labeled","data","combined","with","a","large","amount","of","unlabeled","data",")","."],"labels":["B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["product","metric","person","researcher","algorithm","task","location","programming_language","university","country","conference","field","organization"]}
{"id":"25","dataset":"crossner_ai","split":"dev","instance":{"id":"25","prompt_labels":"Expo(B-location) II(I-location) was(O) announced(O) as(O) being(O) the(O) locale(O) for(O) the(O) world(O) premiere(O) of(O) several(O) films(O) never(O) before(O) seen(O) in(O) 3D(O) ,(O) including(O) The(O) Diamond(O) Wizard(O) and(O) the(O) Universal(O) short(O) ,(O) Hawaiian(O) Nights(O) with(O) Mamie(B-person) Van(I-person) Doren(I-person) and(O) Pinky(B-person) Lee(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, algorithm, person, researcher, metric, university, product, location, conference, organization, country, task, field and O.\nSentence: Expo II was announced as being the locale for the world premiere of several films never before seen in 3D , including The Diamond Wizard and the Universal short , Hawaiian Nights with Mamie Van Doren and Pinky Lee .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Expo","II","was","announced","as","being","the","locale","for","the","world","premiere","of","several","films","never","before","seen","in","3D",",","including","The","Diamond","Wizard","and","the","Universal","short",",","Hawaiian","Nights","with","Mamie","Van","Doren","and","Pinky","Lee","."],"labels":["B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","I-person","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["programming_language","algorithm","person","researcher","metric","university","product","location","conference","organization","country","task","field"]}
{"id":"33","dataset":"crossner_ai","split":"dev","instance":{"id":"33","prompt_labels":"Only(O) a(O) few(O) non-Japanese(O) companies(O) ultimately(O) managed(O) to(O) survive(O) in(O) this(O) market(O) ,(O) the(O) major(O) ones(O) being(O) :(O) Adept(B-organization) Technology(I-organization) ,(O) Stubli(B-organization) ,(O) the(O) Sweden(B-country) -(O) Switzerland(B-country) company(O) ABB(B-organization) Asea(I-organization) Brown(I-organization) Boveri(I-organization) ,(O) the(O) Germany(B-country) company(O) KUKA(B-organization) Robotics(I-organization) and(O) the(O) Italy(B-country) company(O) Comau(B-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, country, programming language, product, field, researcher, location, conference, task, university, algorithm, person, organization and O.\nSentence: Only a few non-Japanese companies ultimately managed to survive in this market , the major ones being : Adept Technology , Stubli , the Sweden - Switzerland company ABB Asea Brown Boveri , the Germany company KUKA Robotics and the Italy company Comau .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Only","a","few","non-Japanese","companies","ultimately","managed","to","survive","in","this","market",",","the","major","ones","being",":","Adept","Technology",",","Stubli",",","the","Sweden","-","Switzerland","company","ABB","Asea","Brown","Boveri",",","the","Germany","company","KUKA","Robotics","and","the","Italy","company","Comau","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","B-organization","O","O","B-country","O","B-country","O","B-organization","I-organization","I-organization","I-organization","O","O","B-country","O","B-organization","I-organization","O","O","B-country","O","B-organization","O"],"target_index":null,"target_label":null},"label_list":["metric","country","programming_language","product","field","researcher","location","conference","task","university","algorithm","person","organization"]}
{"id":"34","dataset":"crossner_ai","split":"dev","instance":{"id":"34","prompt_labels":"The(O) research(O) activities(O) include(O) an(O) annual(O) research(O) conference(O) ,(O) the(O) RuleML(B-conference) Symposium(I-conference) ,(O) also(O) known(O) as(O) RuleML(B-conference) for(O) short(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, algorithm, metric, field, task, conference, product, organization, researcher, programming language, university, person and O.\nSentence: The research activities include an annual research conference , the RuleML Symposium , also known as RuleML for short .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","research","activities","include","an","annual","research","conference",",","the","RuleML","Symposium",",","also","known","as","RuleML","for","short","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-conference","I-conference","O","O","O","O","B-conference","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","country","algorithm","metric","field","task","conference","product","organization","researcher","programming_language","university","person"]}
{"id":"42","dataset":"crossner_ai","split":"dev","instance":{"id":"42","prompt_labels":"Such(O) networks(O) are(O) commonly(O) trained(O) under(O) a(O) Cross(B-metric) entropy(I-metric) ((O) or(O) cross-entropy(B-metric) )(O) regime(O) ,(O) giving(O) a(O) non-linear(O) variant(O) of(O) multinomial(B-algorithm) logistic(I-algorithm) regression(I-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, conference, field, metric, person, researcher, country, university, location, task, organization, algorithm, programming language and O.\nSentence: Such networks are commonly trained under a Cross entropy ( or cross-entropy ) regime , giving a non-linear variant of multinomial logistic regression .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Such","networks","are","commonly","trained","under","a","Cross","entropy","(","or","cross-entropy",")","regime",",","giving","a","non-linear","variant","of","multinomial","logistic","regression","."],"labels":["O","O","O","O","O","O","O","B-metric","I-metric","O","O","B-metric","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["product","conference","field","metric","person","researcher","country","university","location","task","organization","algorithm","programming_language"]}
{"id":"48","dataset":"crossner_ai","split":"dev","instance":{"id":"48","prompt_labels":"Due(O) to(O) limits(O) in(O) computing(O) power(O) ,(O) current(O) in(O) silico(O) methods(O) usually(O) must(O) trade(O) speed(O) for(O) accuracy(B-metric) ;(O) e.g.(O) ,(O) use(O) rapid(O) protein(B-algorithm) docking(I-algorithm) methods(O) instead(O) of(O) computationally(O) costly(O) free(B-algorithm) energy(I-algorithm) calculation(I-algorithm) s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, university, task, metric, researcher, person, field, product, algorithm, country, programming language, location, conference and O.\nSentence: Due to limits in computing power , current in silico methods usually must trade speed for accuracy ; e.g. , use rapid protein docking methods instead of computationally costly free energy calculation s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Due","to","limits","in","computing","power",",","current","in","silico","methods","usually","must","trade","speed","for","accuracy",";","e.g.",",","use","rapid","protein","docking","methods","instead","of","computationally","costly","free","energy","calculation","s","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O"],"target_index":null,"target_label":null},"label_list":["organization","university","task","metric","researcher","person","field","product","algorithm","country","programming_language","location","conference"]}
{"id":"56","dataset":"crossner_ai","split":"dev","instance":{"id":"56","prompt_labels":"Convex(O) algorithms(O) ,(O) such(O) as(O) AdaBoost(B-algorithm) and(O) LogitBoost(B-algorithm) ,(O) can(O) be(O) defeated(O) by(O) random(O) noise(O) such(O) they(O) can(O) 't(O) learn(O) basic(O) and(O) learnable(O) combinations(O) of(O) weak(O) hypotheses(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, university, researcher, organization, person, country, conference, task, metric, programming language, field, location, product and O.\nSentence: Convex algorithms , such as AdaBoost and LogitBoost , can be defeated by random noise such they can 't learn basic and learnable combinations of weak hypotheses .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Convex","algorithms",",","such","as","AdaBoost","and","LogitBoost",",","can","be","defeated","by","random","noise","such","they","can","'t","learn","basic","and","learnable","combinations","of","weak","hypotheses","."],"labels":["O","O","O","O","O","B-algorithm","O","B-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["algorithm","university","researcher","organization","person","country","conference","task","metric","programming_language","field","location","product"]}
{"id":"63","dataset":"crossner_ai","split":"dev","instance":{"id":"63","prompt_labels":"In(O) many(O) applications(O) the(O) units(O) of(O) these(O) networks(O) apply(O) a(O) sigmoid(B-algorithm) function(I-algorithm) as(O) an(O) activation(O) function(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, metric, country, researcher, product, organization, field, university, conference, programming language, task, algorithm and O.\nSentence: In many applications the units of these networks apply a sigmoid function as an activation function .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","many","applications","the","units","of","these","networks","apply","a","sigmoid","function","as","an","activation","function","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","person","metric","country","researcher","product","organization","field","university","conference","programming_language","task","algorithm"]}
{"id":"67","dataset":"crossner_ai","split":"dev","instance":{"id":"67","prompt_labels":"In(O) machine(B-field) learning(I-field) ,(O) the(O) perceptron(B-algorithm) is(O) an(O) algorithm(O) for(O) supervised(B-field) learning(I-field) of(O) binary(B-task) classification(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, programming language, metric, country, person, task, product, conference, organization, algorithm, location, field, researcher and O.\nSentence: In machine learning , the perceptron is an algorithm for supervised learning of binary classification .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","machine","learning",",","the","perceptron","is","an","algorithm","for","supervised","learning","of","binary","classification","."],"labels":["O","B-field","I-field","O","O","B-algorithm","O","O","O","O","B-field","I-field","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["university","programming_language","metric","country","person","task","product","conference","organization","algorithm","location","field","researcher"]}
{"id":"78","dataset":"crossner_ai","split":"dev","instance":{"id":"78","prompt_labels":"Text(B-field) analysis(I-field) involves(O) information(B-task) retrieval(I-task) ,(O) lexical(B-task) analysis(I-task) to(O) study(O) word(O) frequency(O) distributions(O) ,(O) pattern(B-field) recognition(I-field) ,(O) tagging(B-task) /(I-task) annotation(I-task) ,(O) information(B-task) extraction(I-task) ,(O) data(B-field) mining(I-field) techniques(O) including(O) link(B-task) and(I-task) association(I-task) analysis(I-task) ,(O) visualization(B-task) ,(O) and(O) predictive(B-task) analytics(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, algorithm, field, researcher, location, task, metric, organization, product, programming language, conference, person, university and O.\nSentence: Text analysis involves information retrieval , lexical analysis to study word frequency distributions , pattern recognition , tagging / annotation , information extraction , data mining techniques including link and association analysis , visualization , and predictive analytics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Text","analysis","involves","information","retrieval",",","lexical","analysis","to","study","word","frequency","distributions",",","pattern","recognition",",","tagging","/","annotation",",","information","extraction",",","data","mining","techniques","including","link","and","association","analysis",",","visualization",",","and","predictive","analytics","."],"labels":["B-field","I-field","O","B-task","I-task","O","B-task","I-task","O","O","O","O","O","O","B-field","I-field","O","B-task","I-task","I-task","O","B-task","I-task","O","B-field","I-field","O","O","B-task","I-task","I-task","I-task","O","B-task","O","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["country","algorithm","field","researcher","location","task","metric","organization","product","programming_language","conference","person","university"]}
{"id":"81","dataset":"crossner_ai","split":"dev","instance":{"id":"81","prompt_labels":"As(O) a(O) performance(O) metric(O) ,(O) the(O) uncertainty(B-metric) coefficient(I-metric) has(O) the(O) advantage(O) over(O) simple(O) accuracy(B-metric) in(O) that(O) it(O) is(O) not(O) affected(O) by(O) the(O) relative(O) sizes(O) of(O) the(O) different(O) classes(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, university, task, conference, location, programming language, country, metric, field, person, organization, researcher, algorithm and O.\nSentence: As a performance metric , the uncertainty coefficient has the advantage over simple accuracy in that it is not affected by the relative sizes of the different classes .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["As","a","performance","metric",",","the","uncertainty","coefficient","has","the","advantage","over","simple","accuracy","in","that","it","is","not","affected","by","the","relative","sizes","of","the","different","classes","."],"labels":["O","O","O","O","O","O","B-metric","I-metric","O","O","O","O","O","B-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["product","university","task","conference","location","programming_language","country","metric","field","person","organization","researcher","algorithm"]}
{"id":"90","dataset":"crossner_ai","split":"dev","instance":{"id":"90","prompt_labels":"In(O) 2000(O) Manuel(B-person) Toharia(I-person) ,(O) a(O) speaker(O) at(O) previous(O) Campus(B-conference) Parties(I-conference) ,(O) and(O) director(O) of(O) Prncipe(B-organization) Felipe(I-organization) 's(I-organization) Museum(I-organization) of(I-organization) Sciences(I-organization) in(O) Valencia(B-location) 's(I-location) City(I-location) of(I-location) arts(I-location) and(I-location) Sciences(I-location) suggested(O) that(O) Ragageles(B-person) expand(O) and(O) make(O) the(O) event(O) more(O) international(O) by(O) moving(O) it(O) to(O) the(O) famous(O) museum(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, metric, algorithm, organization, product, university, conference, researcher, person, task, field, programming language, location and O.\nSentence: In 2000 Manuel Toharia , a speaker at previous Campus Parties , and director of Prncipe Felipe 's Museum of Sciences in Valencia 's City of arts and Sciences suggested that Ragageles expand and make the event more international by moving it to the famous museum .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2000","Manuel","Toharia",",","a","speaker","at","previous","Campus","Parties",",","and","director","of","Prncipe","Felipe","'s","Museum","of","Sciences","in","Valencia","'s","City","of","arts","and","Sciences","suggested","that","Ragageles","expand","and","make","the","event","more","international","by","moving","it","to","the","famous","museum","."],"labels":["O","O","B-person","I-person","O","O","O","O","O","B-conference","I-conference","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-location","I-location","I-location","I-location","I-location","I-location","I-location","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","metric","algorithm","organization","product","university","conference","researcher","person","task","field","programming_language","location"]}
{"id":"102","dataset":"crossner_ai","split":"dev","instance":{"id":"102","prompt_labels":"Unimate(B-product) was(O) the(O) first(O) industrial(B-product) robot(I-product) ,(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, university, programming language, algorithm, field, task, organization, conference, metric, researcher, product, location, person and O.\nSentence: Unimate was the first industrial robot ,","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Unimate","was","the","first","industrial","robot",","],"labels":["B-product","O","O","O","B-product","I-product","O"],"target_index":null,"target_label":null},"label_list":["country","university","programming_language","algorithm","field","task","organization","conference","metric","researcher","product","location","person"]}
{"id":"109","dataset":"crossner_ai","split":"dev","instance":{"id":"109","prompt_labels":"We(O) make(O) as(O) well(O) as(O) possible(O) precise(O) by(O) measuring(O) the(O) mean(B-metric) squared(I-metric) error(I-metric) between(O) mathy(O) /(O) math(O) and(O) math(O) \\(O) hat(O) {(O) f(O) }(O) ((O) x(O) ;(O) D(O) )(O) /(O) math(O) :(O) we(O) want(O) math(O) ((O) y(O) -(O) \\(O) hat(O) {(O) f(O) }(O) ((O) x(O) ;(O) D(O) )(O) )(O) ^(O) 2(O) /(O) math(O) to(O) be(O) minimal(O) ,(O) both(O) for(O) mathx(O) _(O) 1(O) ,(O) \\(O) dots(O) ,(O) x(O) _(O) n(O) /(O) math(O) and(O) for(O) points(O) outside(O) of(O) our(O) sample(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, university, task, location, researcher, conference, organization, field, algorithm, country, programming language, person, product and O.\nSentence: We make as well as possible precise by measuring the mean squared error between mathy / math and math \\ hat { f } ( x ; D ) / math : we want math ( y - \\ hat { f } ( x ; D ) ) ^ 2 / math to be minimal , both for mathx _ 1 , \\ dots , x _ n / math and for points outside of our sample .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["We","make","as","well","as","possible","precise","by","measuring","the","mean","squared","error","between","mathy","/","math","and","math","\\","hat","{","f","}","(","x",";","D",")","/","math",":","we","want","math","(","y","-","\\","hat","{","f","}","(","x",";","D",")",")","^","2","/","math","to","be","minimal",",","both","for","mathx","_","1",",","\\","dots",",","x","_","n","/","math","and","for","points","outside","of","our","sample","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","university","task","location","researcher","conference","organization","field","algorithm","country","programming_language","person","product"]}
{"id":"111","dataset":"crossner_ai","split":"dev","instance":{"id":"111","prompt_labels":"At(O) the(O) 2018(B-conference) Conference(I-conference) on(I-conference) Neural(I-conference) Information(I-conference) Processing(I-conference) Systems(I-conference) ((O) NeurIPS(B-conference) )(O) researchers(O) from(O) Google(B-organization) presented(O) the(O) work(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, conference, person, metric, location, task, field, organization, researcher, country, product, university, algorithm and O.\nSentence: At the 2018 Conference on Neural Information Processing Systems ( NeurIPS ) researchers from Google presented the work .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["At","the","2018","Conference","on","Neural","Information","Processing","Systems","(","NeurIPS",")","researchers","from","Google","presented","the","work","."],"labels":["O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","B-conference","O","O","O","B-organization","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","conference","person","metric","location","task","field","organization","researcher","country","product","university","algorithm"]}
{"id":"112","dataset":"crossner_ai","split":"dev","instance":{"id":"112","prompt_labels":"The(O) Baum-Welch(B-algorithm) algorithm(I-algorithm) uses(O) the(O) well(O) known(O) EM(B-algorithm) algorithm(I-algorithm) to(O) find(O) the(O) maximum(B-metric) likelihood(I-metric) estimate(I-metric) of(O) the(O) parameters(O) of(O) a(O) hidden(B-algorithm) Markov(I-algorithm) model(I-algorithm) given(O) a(O) set(O) of(O) observed(O) feature(O) vectors(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, university, location, organization, person, conference, field, programming language, country, algorithm, researcher, metric, task and O.\nSentence: The Baum-Welch algorithm uses the well known EM algorithm to find the maximum likelihood estimate of the parameters of a hidden Markov model given a set of observed feature vectors .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Baum-Welch","algorithm","uses","the","well","known","EM","algorithm","to","find","the","maximum","likelihood","estimate","of","the","parameters","of","a","hidden","Markov","model","given","a","set","of","observed","feature","vectors","."],"labels":["O","B-algorithm","I-algorithm","O","O","O","O","B-algorithm","I-algorithm","O","O","O","B-metric","I-metric","I-metric","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["product","university","location","organization","person","conference","field","programming_language","country","algorithm","researcher","metric","task"]}
{"id":"120","dataset":"crossner_ai","split":"dev","instance":{"id":"120","prompt_labels":"He(O) was(O) one(O) of(O) the(O) founding(O) members(O) and(O) former(O) chair(O) ((O) 2006-2008(O) )(O) of(O) the(O) Special(B-conference) Interest(I-conference) Group(I-conference) on(I-conference) Web(I-conference) as(I-conference) Corpus(I-conference) ((O) SIGWAC(B-conference) )(O) of(O) the(O) Association(B-conference) for(I-conference) Computational(I-conference) Linguistics(I-conference) and(O) also(O) one(O) of(O) the(O) founding(O) organizers(O) of(O) SENSEVAL(B-conference) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, algorithm, researcher, programming language, organization, field, country, task, person, location, metric, university, product and O.\nSentence: He was one of the founding members and former chair ( 2006-2008 ) of the Special Interest Group on Web as Corpus ( SIGWAC ) of the Association for Computational Linguistics and also one of the founding organizers of SENSEVAL .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","one","of","the","founding","members","and","former","chair","(","2006-2008",")","of","the","Special","Interest","Group","on","Web","as","Corpus","(","SIGWAC",")","of","the","Association","for","Computational","Linguistics","and","also","one","of","the","founding","organizers","of","SENSEVAL","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","B-conference","O","O","O","B-conference","I-conference","I-conference","I-conference","O","O","O","O","O","O","O","O","B-conference","O"],"target_index":null,"target_label":null},"label_list":["conference","algorithm","researcher","programming_language","organization","field","country","task","person","location","metric","university","product"]}
{"id":"125","dataset":"crossner_ai","split":"dev","instance":{"id":"125","prompt_labels":"An(O) example(O) of(O) a(O) semantic(B-algorithm) network(I-algorithm) is(O) WordNet(B-product) ,(O) a(O) lexical(O) database(O) of(O) English(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, algorithm, metric, country, programming language, researcher, location, conference, university, field, person, product, organization and O.\nSentence: An example of a semantic network is WordNet , a lexical database of English .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["An","example","of","a","semantic","network","is","WordNet",",","a","lexical","database","of","English","."],"labels":["O","O","O","O","B-algorithm","I-algorithm","O","B-product","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["task","algorithm","metric","country","programming_language","researcher","location","conference","university","field","person","product","organization"]}
{"id":"127","dataset":"crossner_ai","split":"dev","instance":{"id":"127","prompt_labels":"Artificial(B-field) intelligence(I-field) has(O) retained(O) the(O) most(O) attention(O) regarding(O) applied(O) ontology(O) in(O) subfields(O) like(O) natural(B-field) language(I-field) processing(I-field) within(O) machine(B-task) and(O) knowledge(B-task) representation(I-task) ,(O) but(O) ontology(O) editors(O) are(O) being(O) used(O) often(O) in(O) a(O) range(O) of(O) fields(O) like(O) education(O) without(O) the(O) intent(O) to(O) contribute(O) to(O) AI(B-field) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, organization, conference, location, person, algorithm, researcher, university, programming language, metric, field, product, country and O.\nSentence: Artificial intelligence has retained the most attention regarding applied ontology in subfields like natural language processing within machine and knowledge representation , but ontology editors are being used often in a range of fields like education without the intent to contribute to AI .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Artificial","intelligence","has","retained","the","most","attention","regarding","applied","ontology","in","subfields","like","natural","language","processing","within","machine","and","knowledge","representation",",","but","ontology","editors","are","being","used","often","in","a","range","of","fields","like","education","without","the","intent","to","contribute","to","AI","."],"labels":["B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","I-field","O","B-task","O","B-task","I-task","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","O"],"target_index":null,"target_label":null},"label_list":["task","organization","conference","location","person","algorithm","researcher","university","programming_language","metric","field","product","country"]}
{"id":"128","dataset":"crossner_ai","split":"dev","instance":{"id":"128","prompt_labels":"This(O) update(O) rule(O) is(O) in(O) fact(O) the(O) stochastic(B-algorithm) gradient(I-algorithm) descent(I-algorithm) update(O) for(O) linear(B-algorithm) regression(I-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, conference, product, metric, person, field, algorithm, university, location, country, organization, task, programming language and O.\nSentence: This update rule is in fact the stochastic gradient descent update for linear regression .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","update","rule","is","in","fact","the","stochastic","gradient","descent","update","for","linear","regression","."],"labels":["O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","B-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["researcher","conference","product","metric","person","field","algorithm","university","location","country","organization","task","programming_language"]}
{"id":"134","dataset":"crossner_ai","split":"dev","instance":{"id":"134","prompt_labels":"The(O) following(O) MATLAB(B-product) code(O) demonstrates(O) a(O) concrete(O) solution(O) for(O) solving(O) the(O) non-linear(O) system(O) of(O) equations(O) presented(O) in(O) the(O) previous(O) section(O) :(O) See(O) also(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, person, researcher, field, location, conference, product, university, algorithm, programming language, country, organization, metric and O.\nSentence: The following MATLAB code demonstrates a concrete solution for solving the non-linear system of equations presented in the previous section : See also","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","following","MATLAB","code","demonstrates","a","concrete","solution","for","solving","the","non-linear","system","of","equations","presented","in","the","previous","section",":","See","also"],"labels":["O","O","B-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["task","person","researcher","field","location","conference","product","university","algorithm","programming_language","country","organization","metric"]}
{"id":"135","dataset":"crossner_ai","split":"dev","instance":{"id":"135","prompt_labels":"Pattern(B-product) recognition(I-product) systems(I-product) are(O) in(O) many(O) cases(O) trained(O) from(O) labeled(O) training(O) data(O) ((O) supervised(B-field) learning(I-field) )(O) but(O) when(O) no(O) labeled(O) data(O) are(O) available(O) other(O) algorithms(O) can(O) be(O) used(O) to(O) discover(O) previously(O) unknown(O) patterns(O) ((O) unsupervised(B-field) learning(I-field) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, product, location, organization, field, programming language, person, university, conference, metric, researcher, country, task and O.\nSentence: Pattern recognition systems are in many cases trained from labeled training data ( supervised learning ) but when no labeled data are available other algorithms can be used to discover previously unknown patterns ( unsupervised learning ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Pattern","recognition","systems","are","in","many","cases","trained","from","labeled","training","data","(","supervised","learning",")","but","when","no","labeled","data","are","available","other","algorithms","can","be","used","to","discover","previously","unknown","patterns","(","unsupervised","learning",")","."],"labels":["B-product","I-product","I-product","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","O"],"target_index":null,"target_label":null},"label_list":["algorithm","product","location","organization","field","programming_language","person","university","conference","metric","researcher","country","task"]}
{"id":"142","dataset":"crossner_ai","split":"dev","instance":{"id":"142","prompt_labels":"In(O) 2018(O) and(O) 2019(O) ,(O) the(O) Championship(O) was(O) be(O) held(O) in(O) Houston(B-location) and(O) Detroit(B-location) ,(O) Michigan(B-location) at(O) the(O) TCF(B-location) Center(I-location) and(O) Ford(B-location) Field(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, field, person, conference, organization, metric, product, algorithm, task, programming language, location, university, researcher and O.\nSentence: In 2018 and 2019 , the Championship was be held in Houston and Detroit , Michigan at the TCF Center and Ford Field .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2018","and","2019",",","the","Championship","was","be","held","in","Houston","and","Detroit",",","Michigan","at","the","TCF","Center","and","Ford","Field","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","O","B-location","I-location","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["country","field","person","conference","organization","metric","product","algorithm","task","programming_language","location","university","researcher"]}
{"id":"143","dataset":"crossner_ai","split":"dev","instance":{"id":"143","prompt_labels":"Classification(B-task) can(O) be(O) thought(O) of(O) as(O) two(O) separate(O) problems(O) -(O) binary(B-task) classification(I-task) and(O) multiclass(B-task) classification(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, conference, university, algorithm, metric, task, person, researcher, organization, programming language, field, product and O.\nSentence: Classification can be thought of as two separate problems - binary classification and multiclass classification .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Classification","can","be","thought","of","as","two","separate","problems","-","binary","classification","and","multiclass","classification","."],"labels":["B-task","O","O","O","O","O","O","O","O","O","B-task","I-task","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["country","location","conference","university","algorithm","metric","task","person","researcher","organization","programming_language","field","product"]}
{"id":"144","dataset":"crossner_ai","split":"dev","instance":{"id":"144","prompt_labels":"Two(O) examples(O) of(O) popular(O) parallel(B-product) robots(I-product) are(O) the(O) Stewart(B-product) platform(I-product) and(O) the(O) Delta(B-product) robot(I-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, field, task, product, conference, metric, organization, person, country, location, programming language, algorithm, researcher and O.\nSentence: Two examples of popular parallel robots are the Stewart platform and the Delta robot .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Two","examples","of","popular","parallel","robots","are","the","Stewart","platform","and","the","Delta","robot","."],"labels":["O","O","O","O","B-product","I-product","O","O","B-product","I-product","O","O","B-product","I-product","O"],"target_index":null,"target_label":null},"label_list":["university","field","task","product","conference","metric","organization","person","country","location","programming_language","algorithm","researcher"]}
{"id":"147","dataset":"crossner_ai","split":"dev","instance":{"id":"147","prompt_labels":"This(O) is(O) done(O) by(O) modeling(O) the(O) received(O) signal(O) then(O) using(O) a(O) statistical(O) estimation(O) method(O) such(O) as(O) maximum(B-algorithm) likelihood(I-algorithm) ((O) ML(B-algorithm) )(O) ,(O) majority(B-algorithm) voting(I-algorithm) ((O) MV(B-algorithm) )(O) or(O) maximum(B-algorithm) a(I-algorithm) posteriori(I-algorithm) ((O) MAP(B-algorithm) )(O) to(O) make(O) a(O) decision(O) about(O) which(O) target(O) in(O) the(O) library(O) best(O) fits(O) the(O) model(O) built(O) using(O) the(O) received(O) signal(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, conference, field, algorithm, university, country, organization, person, location, task, researcher, product, programming language and O.\nSentence: This is done by modeling the received signal then using a statistical estimation method such as maximum likelihood ( ML ) , majority voting ( MV ) or maximum a posteriori ( MAP ) to make a decision about which target in the library best fits the model built using the received signal .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","is","done","by","modeling","the","received","signal","then","using","a","statistical","estimation","method","such","as","maximum","likelihood","(","ML",")",",","majority","voting","(","MV",")","or","maximum","a","posteriori","(","MAP",")","to","make","a","decision","about","which","target","in","the","library","best","fits","the","model","built","using","the","received","signal","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","B-algorithm","O","O","B-algorithm","I-algorithm","O","B-algorithm","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","conference","field","algorithm","university","country","organization","person","location","task","researcher","product","programming_language"]}
{"id":"174","dataset":"crossner_ai","split":"dev","instance":{"id":"174","prompt_labels":"This(O) is(O) done(O) using(O) standard(O) neural(O) net(O) training(O) algorithms(O) such(O) as(O) stochastic(B-algorithm) gradient(I-algorithm) descent(I-algorithm) with(O) backpropagation(B-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, organization, metric, university, task, person, country, programming language, location, conference, product, algorithm, researcher and O.\nSentence: This is done using standard neural net training algorithms such as stochastic gradient descent with backpropagation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","is","done","using","standard","neural","net","training","algorithms","such","as","stochastic","gradient","descent","with","backpropagation","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","O"],"target_index":null,"target_label":null},"label_list":["field","organization","metric","university","task","person","country","programming_language","location","conference","product","algorithm","researcher"]}
{"id":"196","dataset":"crossner_ai","split":"dev","instance":{"id":"196","prompt_labels":"Challenges(O) in(O) natural(B-field) language(I-field) processing(I-field) frequently(O) involve(O) speech(B-task) recognition(I-task) ,(O) natural(B-task) language(I-task) understanding(I-task) ,(O) and(O) natural(B-task) language(I-task) generation(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, field, organization, person, conference, algorithm, country, researcher, programming language, task, product, location, metric and O.\nSentence: Challenges in natural language processing frequently involve speech recognition , natural language understanding , and natural language generation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Challenges","in","natural","language","processing","frequently","involve","speech","recognition",",","natural","language","understanding",",","and","natural","language","generation","."],"labels":["O","O","B-field","I-field","I-field","O","O","B-task","I-task","O","B-task","I-task","I-task","O","O","B-task","I-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["university","field","organization","person","conference","algorithm","country","researcher","programming_language","task","product","location","metric"]}
{"id":"199","dataset":"crossner_ai","split":"dev","instance":{"id":"199","prompt_labels":"The(O) term(O) Semantic(B-product) Web(I-product) was(O) coined(O) by(O) Tim(B-researcher) Berners-Lee(I-researcher) ,(O) the(O) inventor(O) of(O) the(O) World(B-product) Wide(I-product) Web(I-product) and(O) director(O) of(O) the(O) World(B-organization) Wide(I-organization) Web(I-organization) Consortium(I-organization) ((O) W3C(B-organization) )(O) ,(O) which(O) oversees(O) the(O) development(O) of(O) proposed(O) Semantic(B-product) Web(I-product) standards(I-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, person, organization, product, country, location, researcher, metric, task, field, conference, programming language, algorithm and O.\nSentence: The term Semantic Web was coined by Tim Berners-Lee , the inventor of the World Wide Web and director of the World Wide Web Consortium ( W3C ) , which oversees the development of proposed Semantic Web standards .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","term","Semantic","Web","was","coined","by","Tim","Berners-Lee",",","the","inventor","of","the","World","Wide","Web","and","director","of","the","World","Wide","Web","Consortium","(","W3C",")",",","which","oversees","the","development","of","proposed","Semantic","Web","standards","."],"labels":["O","O","B-product","I-product","O","O","O","B-researcher","I-researcher","O","O","O","O","O","B-product","I-product","I-product","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","O","O","B-product","I-product","I-product","O"],"target_index":null,"target_label":null},"label_list":["university","person","organization","product","country","location","researcher","metric","task","field","conference","programming_language","algorithm"]}
{"id":"0","dataset":"crossner_literature","split":"dev","instance":{"id":"0","prompt_labels":"In(O) 1982(O) ,(O) she(O) wrote(O) the(O) novel(B-literary genre) The(B-book) Color(I-book) Purple(I-book) ,(O) for(O) which(O) she(O) won(O) the(O) National(B-award) Book(I-award) Award(I-award) for(I-award) hardcover(I-award) fiction(I-award) ,(O) and(O) the(O) Pulitzer(B-award) Prize(I-award) for(I-award) Fiction(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, location, writer, organization, country, person, literary genre, poem, event, magazine, award and O.\nSentence: In 1982 , she wrote the novel The Color Purple , for which she won the National Book Award for hardcover fiction , and the Pulitzer Prize for Fiction .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1982",",","she","wrote","the","novel","The","Color","Purple",",","for","which","she","won","the","National","Book","Award","for","hardcover","fiction",",","and","the","Pulitzer","Prize","for","Fiction","."],"labels":["O","O","O","O","O","O","B-literary genre","B-book","I-book","I-book","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["book","location","writer","organization","country","person","literary_genre","poem","event","magazine","award"]}
{"id":"14","dataset":"crossner_literature","split":"dev","instance":{"id":"14","prompt_labels":"He(O) is(O) the(O) protagonist(O) of(O) Robert(B-book) Coover(I-book) '(O) s(O) short(B-literary genre) story(I-literary genre) Charlie(B-book) in(I-book) the(I-book) House(I-book) of(I-book) Rue(I-book) ((O) 1980(O) ;(O) reprinted(O) in(O) Coover(B-writer) 's(O) 1987(O) collection(O) A(B-book) Night(I-book) at(I-book) the(I-book) Movies(I-book) )(O) ,(O) and(O) of(O) Glen(B-writer) David(I-writer) Gold(I-writer) '(O) s(O) Sunnyside(B-book) ((O) 2009(O) )(O) ,(O) a(O) historical(B-literary genre) novel(I-literary genre) set(O) in(O) the(O) First(B-event) World(I-event) War(I-event) period(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, country, book, magazine, organization, poem, event, location, writer, literary genre, person and O.\nSentence: He is the protagonist of Robert Coover ' s short story Charlie in the House of Rue ( 1980 ; reprinted in Coover 's 1987 collection A Night at the Movies ) , and of Glen David Gold ' s Sunnyside ( 2009 ) , a historical novel set in the First World War period .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","is","the","protagonist","of","Robert","Coover","'","s","short","story","Charlie","in","the","House","of","Rue","(","1980",";","reprinted","in","Coover","'s","1987","collection","A","Night","at","the","Movies",")",",","and","of","Glen","David","Gold","'","s","Sunnyside","(","2009",")",",","a","historical","novel","set","in","the","First","World","War","period","."],"labels":["O","O","O","O","O","B-book","I-book","O","O","B-literary genre","I-literary genre","B-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","O","B-writer","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-writer","I-writer","I-writer","O","O","B-book","O","O","O","O","O","B-literary genre","I-literary genre","O","O","O","B-event","I-event","I-event","O","O"],"target_index":null,"target_label":null},"label_list":["award","country","book","magazine","organization","poem","event","location","writer","literary_genre","person"]}
{"id":"24","dataset":"crossner_literature","split":"dev","instance":{"id":"24","prompt_labels":"He(O) reprised(O) the(O) role(O) in(O) Evil(B-book) Under(I-book) the(I-book) Sun(I-book) ((O) 1982(O) )(O) and(O) Appointment(B-book) with(I-book) Death(I-book) ((O) 1988(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, writer, literary genre, magazine, poem, book, location, award, country, person and O.\nSentence: He reprised the role in Evil Under the Sun ( 1982 ) and Appointment with Death ( 1988 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","reprised","the","role","in","Evil","Under","the","Sun","(","1982",")","and","Appointment","with","Death","(","1988",")","."],"labels":["O","O","O","O","O","B-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","event","writer","literary_genre","magazine","poem","book","location","award","country","person"]}
{"id":"26","dataset":"crossner_literature","split":"dev","instance":{"id":"26","prompt_labels":"His(O) father(O) Noah(B-person) Webster(I-person) Sr.(I-person) ((O) 1722-1813(O) )(O) was(O) a(O) descendant(O) of(O) Connecticut(B-location) Governor(O) John(B-writer) Webster(I-writer) ;(O) his(O) mother(O) Mercy(B-person) ((I-person) Steele(I-person) )(I-person) Webster(I-person) ((O) 1727-1794(O) )(O) was(O) a(O) descendant(O) of(O) Governor(O) William(B-person) Bradford(I-person) of(O) Plymouth(B-country) Colony(I-country) .(O) Noah(B-person) had(O) two(O) brothers(O) ,(O) Abraham(B-person) ((O) 1751-1831(O) )(O) and(O) Charles(B-person) ((O) b(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, person, event, writer, literary genre, organization, book, magazine, country, award, location and O.\nSentence: His father Noah Webster Sr. ( 1722-1813 ) was a descendant of Connecticut Governor John Webster ; his mother Mercy ( Steele ) Webster ( 1727-1794 ) was a descendant of Governor William Bradford of Plymouth Colony . Noah had two brothers , Abraham ( 1751-1831 ) and Charles ( b .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","father","Noah","Webster","Sr.","(","1722-1813",")","was","a","descendant","of","Connecticut","Governor","John","Webster",";","his","mother","Mercy","(","Steele",")","Webster","(","1727-1794",")","was","a","descendant","of","Governor","William","Bradford","of","Plymouth","Colony",".","Noah","had","two","brothers",",","Abraham","(","1751-1831",")","and","Charles","(","b","."],"labels":["O","O","B-person","I-person","I-person","O","O","O","O","O","O","O","B-location","O","B-writer","I-writer","O","O","O","B-person","I-person","I-person","I-person","I-person","O","O","O","O","O","O","O","O","B-person","I-person","O","B-country","I-country","O","B-person","O","O","O","O","B-person","O","O","O","O","B-person","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","person","event","writer","literary_genre","organization","book","magazine","country","award","location"]}
{"id":"27","dataset":"crossner_literature","split":"dev","instance":{"id":"27","prompt_labels":"It(O) was(O) a(O) finalist(O) for(O) the(O) National(B-award) Book(I-award) Award(I-award) for(I-award) Fiction(I-award) !(O) --(O) National(B-award) Book(I-award) Awards(I-award) were(O) called(O) American(O) for(O) several(O) years(O) from(O) 1980(O) ,(O) but(O) must(O) not(O) be(O) confused(O) with(O) American(B-award) Book(I-award) Awards(I-award) --(O) in(O) 1979(O) ((O) which(O) ultimately(O) went(O) to(O) Tim(B-writer) O(I-writer) 'Brien(I-writer) for(O) Going(B-book) After(I-book) Cacciato(I-book) )(O) in(O) the(O) film(O) as(O) an(O) official(O) in(O) one(O) of(O) Garp(B-person) 's(O) high(O) school(O) wrestling(O) matches(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, organization, event, country, magazine, literary genre, location, poem, book, person, award and O.\nSentence: It was a finalist for the National Book Award for Fiction ! -- National Book Awards were called American for several years from 1980 , but must not be confused with American Book Awards -- in 1979 ( which ultimately went to Tim O 'Brien for Going After Cacciato ) in the film as an official in one of Garp 's high school wrestling matches .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","a","finalist","for","the","National","Book","Award","for","Fiction","!","--","National","Book","Awards","were","called","American","for","several","years","from","1980",",","but","must","not","be","confused","with","American","Book","Awards","--","in","1979","(","which","ultimately","went","to","Tim","O","'Brien","for","Going","After","Cacciato",")","in","the","film","as","an","official","in","one","of","Garp","'s","high","school","wrestling","matches","."],"labels":["O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","O","O","O","O","O","O","O","O","B-writer","I-writer","I-writer","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","B-person","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["writer","organization","event","country","magazine","literary_genre","location","poem","book","person","award"]}
{"id":"32","dataset":"crossner_literature","split":"dev","instance":{"id":"32","prompt_labels":"Ansgar(B-writer) received(O) the(O) mission(O) of(O) evangelizing(O) pagan(O) Denmark(B-country) ,(O) Norway(B-country) and(O) Sweden(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, person, writer, organization, magazine, poem, event, country, literary genre, book and O.\nSentence: Ansgar received the mission of evangelizing pagan Denmark , Norway and Sweden .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Ansgar","received","the","mission","of","evangelizing","pagan","Denmark",",","Norway","and","Sweden","."],"labels":["B-writer","O","O","O","O","O","O","B-country","O","B-country","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["location","award","person","writer","organization","magazine","poem","event","country","literary_genre","book"]}
{"id":"36","dataset":"crossner_literature","split":"dev","instance":{"id":"36","prompt_labels":"In(O) 1973(O) ,(O) Sakharov(B-person) was(O) nominated(O) for(O) the(O) Nobel(B-award) Peace(I-award) Prize(I-award) and(O) in(O) 1974(O) was(O) awarded(O) the(O) Prix(B-award) mondial(I-award) Cino(I-award) Del(I-award) Duca(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, magazine, location, award, writer, person, literary genre, book, poem, organization, country and O.\nSentence: In 1973 , Sakharov was nominated for the Nobel Peace Prize and in 1974 was awarded the Prix mondial Cino Del Duca .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1973",",","Sakharov","was","nominated","for","the","Nobel","Peace","Prize","and","in","1974","was","awarded","the","Prix","mondial","Cino","Del","Duca","."],"labels":["O","O","O","B-person","O","O","O","O","B-award","I-award","I-award","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["event","magazine","location","award","writer","person","literary_genre","book","poem","organization","country"]}
{"id":"43","dataset":"crossner_literature","split":"dev","instance":{"id":"43","prompt_labels":"Although(O) the(O) reforms(O) brought(O) by(O) Nikita(B-person) Khrushchev(I-person) freed(O) him(O) from(O) exile(O) in(O) 1956(O) ,(O) the(O) publication(O) of(O) Cancer(B-book) Ward(I-book) ((O) 1968(O) )(O) ,(O) August(B-book) 1914(I-book) ((O) 1971(O) )(O) ,(O) and(O) The(B-book) Gulag(I-book) Archipelago(I-book) ((O) 1973(O) )(O) beyond(O) the(O) Soviet(B-country) Union(I-country) angered(O) authorities(O) ,(O) and(O) Solzhenitsyn(B-writer) lost(O) his(O) Soviet(O) citizenship(O) in(O) 1974(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, organization, location, person, poem, magazine, country, book, writer, literary genre and O.\nSentence: Although the reforms brought by Nikita Khrushchev freed him from exile in 1956 , the publication of Cancer Ward ( 1968 ) , August 1914 ( 1971 ) , and The Gulag Archipelago ( 1973 ) beyond the Soviet Union angered authorities , and Solzhenitsyn lost his Soviet citizenship in 1974 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Although","the","reforms","brought","by","Nikita","Khrushchev","freed","him","from","exile","in","1956",",","the","publication","of","Cancer","Ward","(","1968",")",",","August","1914","(","1971",")",",","and","The","Gulag","Archipelago","(","1973",")","beyond","the","Soviet","Union","angered","authorities",",","and","Solzhenitsyn","lost","his","Soviet","citizenship","in","1974","."],"labels":["O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","B-book","I-book","O","O","O","O","B-book","I-book","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","B-country","I-country","O","O","O","O","B-writer","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","event","organization","location","person","poem","magazine","country","book","writer","literary_genre"]}
{"id":"53","dataset":"crossner_literature","split":"dev","instance":{"id":"53","prompt_labels":"Rolling(B-magazine) Stone(I-magazine) magazine(O) ranked(O) him(O) number(O) 13(O) in(O) its(O) list(O) of(O) 100(O) Greatest(O) Artists(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, person, award, location, magazine, poem, organization, writer, book, literary genre and O.\nSentence: Rolling Stone magazine ranked him number 13 in its list of 100 Greatest Artists .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Rolling","Stone","magazine","ranked","him","number","13","in","its","list","of","100","Greatest","Artists","."],"labels":["B-magazine","I-magazine","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","event","person","award","location","magazine","poem","organization","writer","book","literary_genre"]}
{"id":"55","dataset":"crossner_literature","split":"dev","instance":{"id":"55","prompt_labels":"Mere(B-book) Christianity(I-book) was(O) voted(O) best(O) book(O) of(O) the(O) 20th(O) century(O) by(O) Christianity(B-magazine) Today(I-magazine) in(O) 2000(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, award, organization, person, writer, event, literary genre, poem, country, magazine, location and O.\nSentence: Mere Christianity was voted best book of the 20th century by Christianity Today in 2000 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Mere","Christianity","was","voted","best","book","of","the","20th","century","by","Christianity","Today","in","2000","."],"labels":["B-book","I-book","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","O","O","O"],"target_index":null,"target_label":null},"label_list":["book","award","organization","person","writer","event","literary_genre","poem","country","magazine","location"]}
{"id":"67","dataset":"crossner_literature","split":"dev","instance":{"id":"67","prompt_labels":"This(O) aversion(O) to(O) war(O) also(O) led(O) Einstein(B-person) to(O) befriend(O) author(O) Upton(B-writer) Sinclair(I-writer) and(O) film(O) star(O) Charlie(B-person) Chaplin(I-person) ,(O) both(O) noted(O) for(O) their(O) pacifism(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, organization, person, writer, poem, literary genre, award, country, book, magazine and O.\nSentence: This aversion to war also led Einstein to befriend author Upton Sinclair and film star Charlie Chaplin , both noted for their pacifism .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","aversion","to","war","also","led","Einstein","to","befriend","author","Upton","Sinclair","and","film","star","Charlie","Chaplin",",","both","noted","for","their","pacifism","."],"labels":["O","O","O","O","O","O","B-person","O","O","O","B-writer","I-writer","O","O","O","B-person","I-person","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","location","organization","person","writer","poem","literary_genre","award","country","book","magazine"]}
{"id":"74","dataset":"crossner_literature","split":"dev","instance":{"id":"74","prompt_labels":"He(O) is(O) best(O) remembered(O) for(O) his(O) science(B-literary genre) fiction(I-literary genre) ,(O) including(O) The(B-book) Demolished(I-book) Man(I-book) ,(O) winner(O) of(O) the(O) inaugural(O) Hugo(B-award) Award(I-award) in(O) 1953(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, magazine, award, event, organization, country, book, person, literary genre, location, writer and O.\nSentence: He is best remembered for his science fiction , including The Demolished Man , winner of the inaugural Hugo Award in 1953 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","is","best","remembered","for","his","science","fiction",",","including","The","Demolished","Man",",","winner","of","the","inaugural","Hugo","Award","in","1953","."],"labels":["O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","B-book","I-book","I-book","O","O","O","O","O","B-award","I-award","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","magazine","award","event","organization","country","book","person","literary_genre","location","writer"]}
{"id":"81","dataset":"crossner_literature","split":"dev","instance":{"id":"81","prompt_labels":"It(O) was(O) adapted(O) by(O) Talbot(B-writer) Jennings(I-writer) ,(O) Tess(B-writer) Slesinger(I-writer) ,(O) and(O) Claudine(B-writer) West(I-writer) from(O) the(O) play(O) by(O) Owen(B-writer) Davis(I-writer) and(O) Donald(B-writer) Davis(I-writer) ,(O) which(O) was(O) in(O) itself(O) based(O) on(O) the(O) 1931(O) The(B-book) Good(I-book) Earth(I-book) by(O) Nobel(B-award) Prize(I-award) -winning(O) author(O) Pearl(B-writer) S.(I-writer) Buck(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, poem, country, magazine, person, book, event, location, literary genre, writer, award and O.\nSentence: It was adapted by Talbot Jennings , Tess Slesinger , and Claudine West from the play by Owen Davis and Donald Davis , which was in itself based on the 1931 The Good Earth by Nobel Prize -winning author Pearl S. Buck .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","adapted","by","Talbot","Jennings",",","Tess","Slesinger",",","and","Claudine","West","from","the","play","by","Owen","Davis","and","Donald","Davis",",","which","was","in","itself","based","on","the","1931","The","Good","Earth","by","Nobel","Prize","-winning","author","Pearl","S.","Buck","."],"labels":["O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","O","B-writer","I-writer","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","O","B-award","I-award","O","O","B-writer","I-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["organization","poem","country","magazine","person","book","event","location","literary_genre","writer","award"]}
{"id":"84","dataset":"crossner_literature","split":"dev","instance":{"id":"84","prompt_labels":"In(O) New(B-location) York(I-location) ,(O) he(O) socialized(O) at(O) the(O) Hydra(B-organization) Club(I-organization) ,(O) an(O) organization(O) of(O) New(B-location) York(I-location) 's(O) science(B-literary genre) fiction(I-literary genre) writers(O) ,(O) including(O) such(O) luminaries(O) as(O) Isaac(B-writer) Asimov(I-writer) ,(O) James(B-writer) Blish(I-writer) ,(O) Anthony(B-writer) Boucher(I-writer) ,(O) Avram(B-writer) Davidson(I-writer) ,(O) Judith(B-writer) Merril(I-writer) ,(O) and(O) Theodore(B-writer) Sturgeon(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, writer, poem, award, book, country, event, organization, literary genre, location, person and O.\nSentence: In New York , he socialized at the Hydra Club , an organization of New York 's science fiction writers , including such luminaries as Isaac Asimov , James Blish , Anthony Boucher , Avram Davidson , Judith Merril , and Theodore Sturgeon .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","New","York",",","he","socialized","at","the","Hydra","Club",",","an","organization","of","New","York","'s","science","fiction","writers",",","including","such","luminaries","as","Isaac","Asimov",",","James","Blish",",","Anthony","Boucher",",","Avram","Davidson",",","Judith","Merril",",","and","Theodore","Sturgeon","."],"labels":["O","B-location","I-location","O","O","O","O","O","B-organization","I-organization","O","O","O","O","B-location","I-location","O","B-literary genre","I-literary genre","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["magazine","writer","poem","award","book","country","event","organization","literary_genre","location","person"]}
{"id":"93","dataset":"crossner_literature","split":"dev","instance":{"id":"93","prompt_labels":"The(O) crowds(O) were(O) delighted(O) with(O) the(O) stories(O) of(O) romances(O) ,(O) the(O) wickedness(O) of(O) Macaire(B-poem) ,(O) and(O) the(O) misfortunes(O) of(O) Blanziflor(B-poem) ,(O) the(O) terrors(O) of(O) the(O) Babilonia(B-poem) Infernale(I-poem) and(O) the(O) blessedness(O) of(O) the(O) Gerusalemme(B-poem) celeste(I-poem) ,(O) and(O) the(O) singers(O) of(O) religious(O) poetry(B-literary genre) vied(O) with(O) those(O) of(O) the(O) chansons(B-poem) de(I-poem) geste(I-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, event, magazine, organization, book, poem, literary genre, writer, person, location and O.\nSentence: The crowds were delighted with the stories of romances , the wickedness of Macaire , and the misfortunes of Blanziflor , the terrors of the Babilonia Infernale and the blessedness of the Gerusalemme celeste , and the singers of religious poetry vied with those of the chansons de geste .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","crowds","were","delighted","with","the","stories","of","romances",",","the","wickedness","of","Macaire",",","and","the","misfortunes","of","Blanziflor",",","the","terrors","of","the","Babilonia","Infernale","and","the","blessedness","of","the","Gerusalemme","celeste",",","and","the","singers","of","religious","poetry","vied","with","those","of","the","chansons","de","geste","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-poem","O","O","O","O","O","B-poem","O","O","O","O","O","B-poem","I-poem","O","O","O","O","O","B-poem","I-poem","O","O","O","O","O","O","B-literary genre","O","O","O","O","O","B-poem","I-poem","I-poem","O"],"target_index":null,"target_label":null},"label_list":["country","award","event","magazine","organization","book","poem","literary_genre","writer","person","location"]}
{"id":"95","dataset":"crossner_literature","split":"dev","instance":{"id":"95","prompt_labels":"In(O) 1991(O) ,(O) Puzo(B-writer) 's(O) speculative(B-literary genre) fiction(I-literary genre) The(B-book) Fourth(I-book) K(I-book) was(O) published(O) ;(O) it(O) hypothesizes(O) a(O) member(O) of(O) the(O) Kennedy(B-person) family(O) who(O) becomes(O) President(O) of(O) the(O) United(B-country) States(I-country) early(O) in(O) the(O) 2000s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, organization, event, writer, magazine, literary genre, poem, award, book, country and O.\nSentence: In 1991 , Puzo 's speculative fiction The Fourth K was published ; it hypothesizes a member of the Kennedy family who becomes President of the United States early in the 2000s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1991",",","Puzo","'s","speculative","fiction","The","Fourth","K","was","published",";","it","hypothesizes","a","member","of","the","Kennedy","family","who","becomes","President","of","the","United","States","early","in","the","2000s","."],"labels":["O","O","O","B-writer","O","B-literary genre","I-literary genre","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","B-person","O","O","O","O","O","O","B-country","I-country","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","location","organization","event","writer","magazine","literary_genre","poem","award","book","country"]}
{"id":"98","dataset":"crossner_literature","split":"dev","instance":{"id":"98","prompt_labels":"The(O) title(O) page(O) of(O) the(O) collection(O) Cathay(B-book) ((O) 1915(O) )(O) ,(O) refers(O) to(O) the(O) poet(O) Rihaku(B-writer) ,(O) the(O) pronunciation(O) in(O) Japanese(O) of(O) the(O) Tang(B-country) dynasty(I-country) Chinese(O) poet(O) ,(O) Li(B-writer) Bai(I-writer) ,(O) whose(O) poems(B-literary genre) were(O) much(O) beloved(O) in(O) China(B-country) and(O) Japan(B-country) for(O) their(O) technical(O) mastery(O) and(O) much(O) translated(O) in(O) the(O) West(O) because(O) of(O) their(O) seeming(O) simplicity(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, location, poem, event, organization, book, literary genre, person, award, magazine, country and O.\nSentence: The title page of the collection Cathay ( 1915 ) , refers to the poet Rihaku , the pronunciation in Japanese of the Tang dynasty Chinese poet , Li Bai , whose poems were much beloved in China and Japan for their technical mastery and much translated in the West because of their seeming simplicity .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","title","page","of","the","collection","Cathay","(","1915",")",",","refers","to","the","poet","Rihaku",",","the","pronunciation","in","Japanese","of","the","Tang","dynasty","Chinese","poet",",","Li","Bai",",","whose","poems","were","much","beloved","in","China","and","Japan","for","their","technical","mastery","and","much","translated","in","the","West","because","of","their","seeming","simplicity","."],"labels":["O","O","O","O","O","O","B-book","O","O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","B-country","I-country","O","O","O","B-writer","I-writer","O","O","B-literary genre","O","O","O","O","B-country","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["writer","location","poem","event","organization","book","literary_genre","person","award","magazine","country"]}
{"id":"100","dataset":"crossner_literature","split":"dev","instance":{"id":"100","prompt_labels":"During(O) his(O) first(O) years(O) as(O) bishop(O) ,(O) Athanasius(B-writer) visited(O) the(O) churches(O) of(O) his(O) territory(O) ,(O) which(O) at(O) that(O) time(O) included(O) all(O) of(O) Egypt(B-country) and(O) Libya(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, event, country, award, magazine, person, book, poem, location, organization, literary genre and O.\nSentence: During his first years as bishop , Athanasius visited the churches of his territory , which at that time included all of Egypt and Libya .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","his","first","years","as","bishop",",","Athanasius","visited","the","churches","of","his","territory",",","which","at","that","time","included","all","of","Egypt","and","Libya","."],"labels":["O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["writer","event","country","award","magazine","person","book","poem","location","organization","literary_genre"]}
{"id":"132","dataset":"crossner_literature","split":"dev","instance":{"id":"132","prompt_labels":"Some(O) of(O) these(O) friends(O) include(O) :(O) David(B-writer) Amram(I-writer) ,(O) Bob(B-writer) Kaufman(I-writer) ;(O) Diane(B-writer) di(I-writer) Prima(I-writer) ;(O) Jim(B-writer) Cohn(I-writer) ;(O) poets(O) associated(O) with(O) the(O) Black(B-organization) Mountain(I-organization) College(I-organization) such(O) as(O) Charles(B-writer) Olson(I-writer) ,(O) Robert(B-writer) Creeley(I-writer) ,(O) and(O) Denise(B-writer) Levertov(I-writer) ;(O) poets(O) associated(O) with(O) the(O) New(B-organization) York(I-organization) School(I-organization) such(O) as(O) Frank(B-writer) O(I-writer) 'Hara(I-writer) and(O) Kenneth(B-writer) Koch(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, book, poem, magazine, event, person, literary genre, organization, award, location, writer and O.\nSentence: Some of these friends include : David Amram , Bob Kaufman ; Diane di Prima ; Jim Cohn ; poets associated with the Black Mountain College such as Charles Olson , Robert Creeley , and Denise Levertov ; poets associated with the New York School such as Frank O 'Hara and Kenneth Koch .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","of","these","friends","include",":","David","Amram",",","Bob","Kaufman",";","Diane","di","Prima",";","Jim","Cohn",";","poets","associated","with","the","Black","Mountain","College","such","as","Charles","Olson",",","Robert","Creeley",",","and","Denise","Levertov",";","poets","associated","with","the","New","York","School","such","as","Frank","O","'Hara","and","Kenneth","Koch","."],"labels":["O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-writer","I-writer","O","B-writer","I-writer","O","O","B-writer","I-writer","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["country","book","poem","magazine","event","person","literary_genre","organization","award","location","writer"]}
{"id":"150","dataset":"crossner_literature","split":"dev","instance":{"id":"150","prompt_labels":"Among(O) books(O) on(O) the(O) list(O) considered(O) to(O) be(O) the(O) Great(B-literary genre) American(I-literary genre) Novel(I-literary genre) were(O) Moby-Dick(B-book) ,(O) Adventures(B-book) of(I-book) Huckleberry(I-book) Finn(I-book) ,(O) The(B-book) Great(I-book) Gatsby(I-book) ,(O) The(B-book) Grapes(I-book) of(I-book) Wrath(I-book) ,(O) The(B-book) Catcher(I-book) in(I-book) the(I-book) Rye(I-book) ,(O) Invisible(B-book) Man(I-book) ,(O) and(O) To(B-book) Kill(I-book) a(I-book) Mockingbird(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, poem, award, book, literary genre, location, writer, person, magazine, country, event and O.\nSentence: Among books on the list considered to be the Great American Novel were Moby-Dick , Adventures of Huckleberry Finn , The Great Gatsby , The Grapes of Wrath , The Catcher in the Rye , Invisible Man , and To Kill a Mockingbird .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Among","books","on","the","list","considered","to","be","the","Great","American","Novel","were","Moby-Dick",",","Adventures","of","Huckleberry","Finn",",","The","Great","Gatsby",",","The","Grapes","of","Wrath",",","The","Catcher","in","the","Rye",",","Invisible","Man",",","and","To","Kill","a","Mockingbird","."],"labels":["O","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","I-literary genre","O","B-book","O","B-book","I-book","I-book","I-book","O","B-book","I-book","I-book","O","B-book","I-book","I-book","I-book","O","B-book","I-book","I-book","I-book","I-book","O","B-book","I-book","O","O","B-book","I-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["organization","poem","award","book","literary_genre","location","writer","person","magazine","country","event"]}
{"id":"159","dataset":"crossner_literature","split":"dev","instance":{"id":"159","prompt_labels":"In(O) 2000(O) ,(O) Anderson(B-writer) starred(O) in(O) the(O) film(O) The(O) House(O) of(O) Mirth(O) with(O) Eric(B-person) Stoltz(I-person) -(O) Terence(B-person) Davies(I-person) '(O) adaptation(O) of(O) the(O) Edith(B-writer) Wharton(I-writer) novel(B-literary genre) of(O) the(O) The(B-book) House(I-book) of(I-book) Mirth(I-book) -(O) for(O) which(O) she(O) won(O) critical(O) acclaim(O) and(O) awards(O) such(O) as(O) the(O) British(B-award) Independent(I-award) Film(I-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) ,(O) Village(B-award) Voice(I-award) Film(I-award) Poll(I-award) Best(I-award) Lead(I-award) Performance(I-award) ,(O) and(O) a(O) nomination(O) for(O) the(O) National(B-award) Society(I-award) of(I-award) Film(I-award) Critics(I-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, literary genre, award, location, writer, person, book, event, country, poem, magazine and O.\nSentence: In 2000 , Anderson starred in the film The House of Mirth with Eric Stoltz - Terence Davies ' adaptation of the Edith Wharton novel of the The House of Mirth - for which she won critical acclaim and awards such as the British Independent Film Award for Best Actress , Village Voice Film Poll Best Lead Performance , and a nomination for the National Society of Film Critics Award for Best Actress .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2000",",","Anderson","starred","in","the","film","The","House","of","Mirth","with","Eric","Stoltz","-","Terence","Davies","'","adaptation","of","the","Edith","Wharton","novel","of","the","The","House","of","Mirth","-","for","which","she","won","critical","acclaim","and","awards","such","as","the","British","Independent","Film","Award","for","Best","Actress",",","Village","Voice","Film","Poll","Best","Lead","Performance",",","and","a","nomination","for","the","National","Society","of","Film","Critics","Award","for","Best","Actress","."],"labels":["O","O","O","B-writer","O","O","O","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","O","O","O","B-writer","I-writer","B-literary genre","O","O","B-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["organization","literary_genre","award","location","writer","person","book","event","country","poem","magazine"]}
{"id":"160","dataset":"crossner_literature","split":"dev","instance":{"id":"160","prompt_labels":"In(O) the(O) sultanate(O) ,(O) Burgess(B-writer) sketched(O) the(O) novel(B-literary genre) that(O) ,(O) when(O) it(O) was(O) published(O) in(O) 1961(O) ,(O) was(O) to(O) be(O) entitled(O) Devil(B-book) of(I-book) a(I-book) State(I-book) and(O) ,(O) although(O) it(O) dealt(O) with(O) Brunei(B-country) ,(O) for(O) libel(O) reasons(O) the(O) action(O) had(O) to(O) be(O) transposed(O) to(O) an(O) imaginary(O) East(B-location) African(I-location) territory(O) similar(O) to(O) Zanzibar(B-country) ,(O) named(O) Dunia(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, writer, country, organization, poem, magazine, book, award, location, literary genre, event and O.\nSentence: In the sultanate , Burgess sketched the novel that , when it was published in 1961 , was to be entitled Devil of a State and , although it dealt with Brunei , for libel reasons the action had to be transposed to an imaginary East African territory similar to Zanzibar , named Dunia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","sultanate",",","Burgess","sketched","the","novel","that",",","when","it","was","published","in","1961",",","was","to","be","entitled","Devil","of","a","State","and",",","although","it","dealt","with","Brunei",",","for","libel","reasons","the","action","had","to","be","transposed","to","an","imaginary","East","African","territory","similar","to","Zanzibar",",","named","Dunia","."],"labels":["O","O","O","O","B-writer","O","O","B-literary genre","O","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","I-book","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","B-country","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","writer","country","organization","poem","magazine","book","award","location","literary_genre","event"]}
{"id":"167","dataset":"crossner_literature","split":"dev","instance":{"id":"167","prompt_labels":"In(O) the(O) same(O) year(O) ,(O) he(O) produced(O) the(O) first(O) French(O) language(O) editions(O) of(O) Joseph(B-writer) Conrad(I-writer) '(O) s(O) Heart(B-book) of(I-book) Darkness(I-book) and(O) Lord(B-book) Jim(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, location, magazine, writer, literary genre, organization, country, award, person, poem, event and O.\nSentence: In the same year , he produced the first French language editions of Joseph Conrad ' s Heart of Darkness and Lord Jim .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","same","year",",","he","produced","the","first","French","language","editions","of","Joseph","Conrad","'","s","Heart","of","Darkness","and","Lord","Jim","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","B-book","I-book","I-book","O","B-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["book","location","magazine","writer","literary_genre","organization","country","award","person","poem","event"]}
{"id":"170","dataset":"crossner_literature","split":"dev","instance":{"id":"170","prompt_labels":"After(O) leaving(O) formal(O) education(O) ,(O) he(O) embarked(O) upon(O) a(O) self-directed(O) course(O) of(O) literature(O) ,(O) including(O) Robinson(B-book) Crusoe(I-book) ,(O) Gulliver(B-book) 's(I-book) Travels(I-book) ,(O) the(O) fairy(B-literary genre) tales(I-literary genre) of(O) Hans(B-writer) Christian(I-writer) Andersen(I-writer) and(O) Madame(B-writer) d(I-writer) 'Aulnoy(I-writer) ,(O) the(O) Arabian(B-book) Nights(I-book) and(O) the(O) poems(B-literary genre) of(O) Edgar(B-writer) Allan(I-writer) Poe(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, literary genre, magazine, book, event, organization, location, award, writer, poem and O.\nSentence: After leaving formal education , he embarked upon a self-directed course of literature , including Robinson Crusoe , Gulliver 's Travels , the fairy tales of Hans Christian Andersen and Madame d 'Aulnoy , the Arabian Nights and the poems of Edgar Allan Poe .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","leaving","formal","education",",","he","embarked","upon","a","self-directed","course","of","literature",",","including","Robinson","Crusoe",",","Gulliver","'s","Travels",",","the","fairy","tales","of","Hans","Christian","Andersen","and","Madame","d","'Aulnoy",",","the","Arabian","Nights","and","the","poems","of","Edgar","Allan","Poe","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","O","B-book","I-book","I-book","O","O","B-literary genre","I-literary genre","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","B-book","I-book","O","O","B-literary genre","O","B-writer","I-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["person","country","literary_genre","magazine","book","event","organization","location","award","writer","poem"]}
{"id":"174","dataset":"crossner_literature","split":"dev","instance":{"id":"174","prompt_labels":"Caravaggio(O) was(O) entered(O) into(O) the(O) 36th(B-event) Berlin(I-event) International(I-event) Film(I-event) Festival(I-event) ,(O) where(O) it(O) won(O) the(O) Silver(B-award) Bear(I-award) for(O) an(O) outstanding(O) single(O) achievement(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, event, book, country, organization, writer, poem, literary genre, magazine, award and O.\nSentence: Caravaggio was entered into the 36th Berlin International Film Festival , where it won the Silver Bear for an outstanding single achievement .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Caravaggio","was","entered","into","the","36th","Berlin","International","Film","Festival",",","where","it","won","the","Silver","Bear","for","an","outstanding","single","achievement","."],"labels":["O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","O","O","O","O","O","B-award","I-award","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","location","event","book","country","organization","writer","poem","literary_genre","magazine","award"]}
{"id":"178","dataset":"crossner_literature","split":"dev","instance":{"id":"178","prompt_labels":"Guillaume(B-writer) Apollinaire(I-writer) ,(O) Andr(B-writer) Salmon(I-writer) and(O) Max(B-writer) Jacob(I-writer) sought(O) him(O) out(O) in(O) his(O) truncated(O) apartment(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, book, magazine, person, writer, poem, event, award, literary genre, country and O.\nSentence: Guillaume Apollinaire , Andr Salmon and Max Jacob sought him out in his truncated apartment .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Guillaume","Apollinaire",",","Andr","Salmon","and","Max","Jacob","sought","him","out","in","his","truncated","apartment","."],"labels":["B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","location","book","magazine","person","writer","poem","event","award","literary_genre","country"]}
{"id":"181","dataset":"crossner_literature","split":"dev","instance":{"id":"181","prompt_labels":"Like(O) his(O) contemporaries(O) Algernon(B-writer) Blackwood(I-writer) and(O) Arthur(B-writer) Machen(I-writer) ,(O) Rohmer(B-writer) claimed(O) membership(O) to(O) one(O) of(O) the(O) factions(O) of(O) the(O) qabbalistic(O) Hermetic(B-organization) Order(I-organization) of(I-organization) the(I-organization) Golden(I-organization) Dawn(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, award, organization, literary genre, event, magazine, book, poem, location, country, person and O.\nSentence: Like his contemporaries Algernon Blackwood and Arthur Machen , Rohmer claimed membership to one of the factions of the qabbalistic Hermetic Order of the Golden Dawn .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Like","his","contemporaries","Algernon","Blackwood","and","Arthur","Machen",",","Rohmer","claimed","membership","to","one","of","the","factions","of","the","qabbalistic","Hermetic","Order","of","the","Golden","Dawn","."],"labels":["O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["writer","award","organization","literary_genre","event","magazine","book","poem","location","country","person"]}
{"id":"185","dataset":"crossner_literature","split":"dev","instance":{"id":"185","prompt_labels":"Kamel(B-writer) Daoud(I-writer) has(O) written(O) a(O) novel(B-literary genre) The(B-book) Meursault(I-book) Investigation(I-book) ((O) 2013(O) /(O) 2014(O) )(O) ,(O) first(O) published(O) in(O) Algeria(B-country) in(O) 2013(O) ,(O) and(O) then(O) republished(O) in(O) France(B-country) to(O) critical(O) acclaim(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, location, book, award, writer, country, person, organization, poem, event, literary genre and O.\nSentence: Kamel Daoud has written a novel The Meursault Investigation ( 2013 / 2014 ) , first published in Algeria in 2013 , and then republished in France to critical acclaim .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Kamel","Daoud","has","written","a","novel","The","Meursault","Investigation","(","2013","/","2014",")",",","first","published","in","Algeria","in","2013",",","and","then","republished","in","France","to","critical","acclaim","."],"labels":["B-writer","I-writer","O","O","O","B-literary genre","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","B-country","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","location","book","award","writer","country","person","organization","poem","event","literary_genre"]}
{"id":"193","dataset":"crossner_literature","split":"dev","instance":{"id":"193","prompt_labels":"In(O) view(O) of(O) the(O) success(O) of(O) her(O) novels(B-literary genre) ,(O) particularly(O) Jane(B-book) Eyre(I-book) ,(O) Bront(B-writer) was(O) persuaded(O) by(O) her(O) publisher(O) to(O) make(O) occasional(O) visits(O) to(O) London(B-location) ,(O) where(O) she(O) revealed(O) her(O) TRUE(O) identity(O) and(O) began(O) to(O) move(O) in(O) more(O) exalted(O) social(O) circles(O) ,(O) becoming(O) friends(O) with(O) Harriet(B-writer) Martineau(I-writer) and(O) Elizabeth(B-writer) Gaskell(I-writer) ,(O) and(O) acquainted(O) with(O) William(B-writer) Makepeace(I-writer) Thackeray(I-writer) and(O) G.H.(B-writer) Lewes(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, magazine, writer, poem, event, person, country, location, award, book, organization and O.\nSentence: In view of the success of her novels , particularly Jane Eyre , Bront was persuaded by her publisher to make occasional visits to London , where she revealed her TRUE identity and began to move in more exalted social circles , becoming friends with Harriet Martineau and Elizabeth Gaskell , and acquainted with William Makepeace Thackeray and G.H. Lewes .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","view","of","the","success","of","her","novels",",","particularly","Jane","Eyre",",","Bront","was","persuaded","by","her","publisher","to","make","occasional","visits","to","London",",","where","she","revealed","her","TRUE","identity","and","began","to","move","in","more","exalted","social","circles",",","becoming","friends","with","Harriet","Martineau","and","Elizabeth","Gaskell",",","and","acquainted","with","William","Makepeace","Thackeray","and","G.H.","Lewes","."],"labels":["O","O","O","O","O","O","O","B-literary genre","O","O","B-book","I-book","O","B-writer","O","O","O","O","O","O","O","O","O","O","B-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","O","O","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","magazine","writer","poem","event","person","country","location","award","book","organization"]}
{"id":"196","dataset":"crossner_literature","split":"dev","instance":{"id":"196","prompt_labels":"In(O) May(O) 1999(O) ,(O) after(O) the(O) Council(B-organization) of(I-organization) Fashion(I-organization) Designers(I-organization) of(I-organization) America(I-organization) recognized(O) Cher(B-writer) with(O) an(O) award(O) for(O) her(O) influence(O) on(O) fashion(O) ,(O) Robin(B-writer) Givhan(I-writer) of(O) the(O) Los(B-organization) Angeles(I-organization) Times(I-organization) called(O) her(O) a(O) fashion(O) visionary(O) for(O) striking(O) just(O) the(O) right(O) note(O) of(O) contemporary(O) wretched(O) excess(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, magazine, writer, country, event, literary genre, book, award, location, poem and O.\nSentence: In May 1999 , after the Council of Fashion Designers of America recognized Cher with an award for her influence on fashion , Robin Givhan of the Los Angeles Times called her a fashion visionary for striking just the right note of contemporary wretched excess .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","May","1999",",","after","the","Council","of","Fashion","Designers","of","America","recognized","Cher","with","an","award","for","her","influence","on","fashion",",","Robin","Givhan","of","the","Los","Angeles","Times","called","her","a","fashion","visionary","for","striking","just","the","right","note","of","contemporary","wretched","excess","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-writer","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","person","magazine","writer","country","event","literary_genre","book","award","location","poem"]}
{"id":"15","dataset":"crossner_music","split":"dev","instance":{"id":"15","prompt_labels":"The(O) best-selling(O) album(O) in(O) the(O) band(O) 's(O) catalog(O) ,(O) I(O) Against(B-album) I(O) is(O) an(O) album(O) that(O) mixes(O) American(O) hardcore(B-music genre) punk(I-music genre) with(O) funk(B-music genre) ,(O) Soul(B-music genre) music(I-music genre) ,(O) reggae(B-music genre) and(O) Heavy(B-music genre) metal(I-music genre) music(I-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, song, country, organization, musical artist, music genre, location, person, award, musical instrument, album, event and O.\nSentence: The best-selling album in the band 's catalog , I Against I is an album that mixes American hardcore punk with funk , Soul music , reggae and Heavy metal music .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","best-selling","album","in","the","band","'s","catalog",",","I","Against","I","is","an","album","that","mixes","American","hardcore","punk","with","funk",",","Soul","music",",","reggae","and","Heavy","metal","music","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-album","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["band","song","country","organization","musical_artist","music_genre","location","person","award","musical_instrument","album","event"]}
{"id":"17","dataset":"crossner_music","split":"dev","instance":{"id":"17","prompt_labels":"Artists(O) from(O) outside(O) California(B-location) who(O) were(O) associated(O) with(O) early(O) alternative(B-music genre) country(I-music genre) included(O) singer-songwriters(O) such(O) as(O) Lucinda(B-musical artist) Williams(I-musical artist) ,(O) Lyle(B-musical artist) Lovett(I-musical artist) and(O) Steve(B-musical artist) Earle(I-musical artist) ,(O) the(O) Nashville(O) country(O) rock(O) band(O) Jason(B-band) and(I-band) the(I-band) Scorchers(I-band) and(O) the(O) British(O) post-punk(B-music genre) band(O) The(B-band) Mekons(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, band, musical artist, musical instrument, location, person, song, organization, event, album, award, country and O.\nSentence: Artists from outside California who were associated with early alternative country included singer-songwriters such as Lucinda Williams , Lyle Lovett and Steve Earle , the Nashville country rock band Jason and the Scorchers and the British post-punk band The Mekons .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Artists","from","outside","California","who","were","associated","with","early","alternative","country","included","singer-songwriters","such","as","Lucinda","Williams",",","Lyle","Lovett","and","Steve","Earle",",","the","Nashville","country","rock","band","Jason","and","the","Scorchers","and","the","British","post-punk","band","The","Mekons","."],"labels":["O","O","O","B-location","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O","O","O","B-band","I-band","I-band","I-band","O","O","O","B-music genre","O","B-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["music_genre","band","musical_artist","musical_instrument","location","person","song","organization","event","album","award","country"]}
{"id":"25","dataset":"crossner_music","split":"dev","instance":{"id":"25","prompt_labels":"Western(B-music genre) music(I-music genre) 's(O) influence(O) would(O) continue(O) to(O) grow(O) within(O) the(O) country(B-music genre) music(I-music genre) sphere(O) ,(O) Western(B-music genre) musicians(O) like(O) Michael(B-musical artist) Martin(I-musical artist) Murphey(I-musical artist) ,(O) New(B-music genre) Mexico(I-music genre) music(I-music genre) artists(O) Al(B-musical artist) Hurricane(I-musical artist) and(O) Antonia(B-person) Apodaca(I-person) ,(O) Tejano(B-music genre) music(I-music genre) performer(O) Little(B-musical artist) Joe(I-musical artist) ,(O) and(O) even(O) folk(O) revivalist(O) John(B-musical artist) Denver(I-musical artist) ,(O) all(O) first(O) rose(O) to(O) prominence(O) during(O) this(O) time(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, music genre, person, song, event, location, band, musical instrument, album, country, organization, musical artist and O.\nSentence: Western music 's influence would continue to grow within the country music sphere , Western musicians like Michael Martin Murphey , New Mexico music artists Al Hurricane and Antonia Apodaca , Tejano music performer Little Joe , and even folk revivalist John Denver , all first rose to prominence during this time .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Western","music","'s","influence","would","continue","to","grow","within","the","country","music","sphere",",","Western","musicians","like","Michael","Martin","Murphey",",","New","Mexico","music","artists","Al","Hurricane","and","Antonia","Apodaca",",","Tejano","music","performer","Little","Joe",",","and","even","folk","revivalist","John","Denver",",","all","first","rose","to","prominence","during","this","time","."],"labels":["B-music genre","I-music genre","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","B-music genre","O","O","B-musical artist","I-musical artist","I-musical artist","O","B-music genre","I-music genre","I-music genre","O","B-musical artist","I-musical artist","O","B-person","I-person","O","B-music genre","I-music genre","O","B-musical artist","I-musical artist","O","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","music_genre","person","song","event","location","band","musical_instrument","album","country","organization","musical_artist"]}
{"id":"31","dataset":"crossner_music","split":"dev","instance":{"id":"31","prompt_labels":"In(O) the(O) summer(O) of(O) 2004(O) ,(O) Diab(B-musical artist) ,(O) having(O) left(O) Alam(B-organization) El(I-organization) Phan(I-organization) ,(O) released(O) his(O) first(O) album(O) with(O) Rotana(B-organization) Records(I-organization) ,(O) Leily(B-album) Nahary(I-album) ,(O) which(O) he(O) followed(O) up(O) with(O) the(O) hugely(O) successful(O) Kammel(B-album) Kalamak(I-album) ((O) 2005(O) )(O) ,(O) and(O) El(B-album) Lilady(I-album) ((O) 2007(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, band, music genre, country, award, organization, song, album, musical instrument, person, event, musical artist and O.\nSentence: In the summer of 2004 , Diab , having left Alam El Phan , released his first album with Rotana Records , Leily Nahary , which he followed up with the hugely successful Kammel Kalamak ( 2005 ) , and El Lilady ( 2007 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","summer","of","2004",",","Diab",",","having","left","Alam","El","Phan",",","released","his","first","album","with","Rotana","Records",",","Leily","Nahary",",","which","he","followed","up","with","the","hugely","successful","Kammel","Kalamak","(","2005",")",",","and","El","Lilady","(","2007",")","."],"labels":["O","O","O","O","O","O","B-musical artist","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","B-organization","I-organization","O","B-album","I-album","O","O","O","O","O","O","O","O","O","B-album","I-album","O","O","O","O","O","B-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","band","music_genre","country","award","organization","song","album","musical_instrument","person","event","musical_artist"]}
{"id":"36","dataset":"crossner_music","split":"dev","instance":{"id":"36","prompt_labels":"Rocks(B-album) and(I-album) Honey(I-album) was(O) released(O) in(O) 2013(O) and(O) features(O) the(O) single(O) Believe(B-song) in(I-song) Me(I-song) which(O) she(O) performed(O) representing(O) the(O) United(B-country) Kingdom(I-country) at(O) the(O) Eurovision(B-album) Song(I-album) Contest(I-album) 2013(I-album) in(O) Malm(B-location) ,(O) Sweden(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, band, award, musical instrument, musical artist, song, country, music genre, person, album, organization, location and O.\nSentence: Rocks and Honey was released in 2013 and features the single Believe in Me which she performed representing the United Kingdom at the Eurovision Song Contest 2013 in Malm , Sweden .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Rocks","and","Honey","was","released","in","2013","and","features","the","single","Believe","in","Me","which","she","performed","representing","the","United","Kingdom","at","the","Eurovision","Song","Contest","2013","in","Malm",",","Sweden","."],"labels":["B-album","I-album","I-album","O","O","O","O","O","O","O","O","B-song","I-song","I-song","O","O","O","O","O","B-country","I-country","O","O","B-album","I-album","I-album","I-album","O","B-location","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["event","band","award","musical_instrument","musical_artist","song","country","music_genre","person","album","organization","location"]}
{"id":"45","dataset":"crossner_music","split":"dev","instance":{"id":"45","prompt_labels":"The(O) venue(O) hosted(O) Badminton(B-event) at(I-event) the(I-event) 2012(I-event) Summer(I-event) Olympics(I-event) and(O) Gymnastics(B-event) at(I-event) the(I-event) 2012(I-event) Summer(I-event) Olympics(I-event) at(O) the(O) 2012(B-event) Summer(I-event) Olympics(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, music genre, person, award, event, band, song, musical instrument, organization, musical artist, album, country and O.\nSentence: The venue hosted Badminton at the 2012 Summer Olympics and Gymnastics at the 2012 Summer Olympics at the 2012 Summer Olympics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","venue","hosted","Badminton","at","the","2012","Summer","Olympics","and","Gymnastics","at","the","2012","Summer","Olympics","at","the","2012","Summer","Olympics","."],"labels":["O","O","O","B-event","I-event","I-event","I-event","I-event","I-event","O","B-event","I-event","I-event","I-event","I-event","I-event","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["location","music_genre","person","award","event","band","song","musical_instrument","organization","musical_artist","album","country"]}
{"id":"47","dataset":"crossner_music","split":"dev","instance":{"id":"47","prompt_labels":"One(O) of(O) the(O) main(O) differences(O) between(O) American(O) and(O) European(O) pop(B-music genre) is(O) that(O) Europop(B-music genre) is(O) generally(O) more(O) Dance(B-music genre) music(I-music genre) and(O) Trance(B-music genre) music(I-music genre) oriented(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, musical artist, location, album, musical instrument, award, song, band, country, organization, person, event and O.\nSentence: One of the main differences between American and European pop is that Europop is generally more Dance music and Trance music oriented .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["One","of","the","main","differences","between","American","and","European","pop","is","that","Europop","is","generally","more","Dance","music","and","Trance","music","oriented","."],"labels":["O","O","O","O","O","O","O","O","O","B-music genre","O","O","B-music genre","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["music_genre","musical_artist","location","album","musical_instrument","award","song","band","country","organization","person","event"]}
{"id":"48","dataset":"crossner_music","split":"dev","instance":{"id":"48","prompt_labels":"All(O) ten(O) of(O) Cannibal(B-band) Corpse(I-band) 's(O) albums(O) ,(O) the(O) live(O) album(O) Live(B-album) Cannibalism(I-album) ,(O) the(O) boxed(O) set(O) 15(B-album) Year(I-album) Killing(I-album) Spree(I-album) ,(O) the(O) EP(O) Worm(B-album) Infested(I-album) ,(O) and(O) the(O) single(O) Hammer(B-song) Smashed(I-song) Face(I-song) were(O) re-released(O) in(O) Australia(B-country) between(O) 2006(O) and(O) 2007(O) ,(O) finally(O) classified(O) by(O) ARIA(B-organization) and(O) allowed(O) for(O) sale(O) in(O) Australia(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, band, person, musical artist, location, musical instrument, organization, country, music genre, song, album, event and O.\nSentence: All ten of Cannibal Corpse 's albums , the live album Live Cannibalism , the boxed set 15 Year Killing Spree , the EP Worm Infested , and the single Hammer Smashed Face were re-released in Australia between 2006 and 2007 , finally classified by ARIA and allowed for sale in Australia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["All","ten","of","Cannibal","Corpse","'s","albums",",","the","live","album","Live","Cannibalism",",","the","boxed","set","15","Year","Killing","Spree",",","the","EP","Worm","Infested",",","and","the","single","Hammer","Smashed","Face","were","re-released","in","Australia","between","2006","and","2007",",","finally","classified","by","ARIA","and","allowed","for","sale","in","Australia","."],"labels":["O","O","O","B-band","I-band","O","O","O","O","O","O","B-album","I-album","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","B-album","I-album","O","O","O","O","B-song","I-song","I-song","O","O","O","B-country","O","O","O","O","O","O","O","O","B-organization","O","O","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["award","band","person","musical_artist","location","musical_instrument","organization","country","music_genre","song","album","event"]}
{"id":"55","dataset":"crossner_music","split":"dev","instance":{"id":"55","prompt_labels":"The(O) film(O) won(O) Academy(B-award) Awards(I-award) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) ((O) James(B-person) Cagney(I-person) )(O) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Score(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Sound(I-award) Mixing(I-award) ((O) Nathan(B-musical artist) Levinson(I-musical artist) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, album, music genre, location, person, musical artist, band, event, musical instrument, award, country, organization and O.\nSentence: The film won Academy Awards for Academy Award for Best Actor ( James Cagney ) , Academy Award for Best Original Score and Academy Award for Best Sound Mixing ( Nathan Levinson ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","film","won","Academy","Awards","for","Academy","Award","for","Best","Actor","(","James","Cagney",")",",","Academy","Award","for","Best","Original","Score","and","Academy","Award","for","Best","Sound","Mixing","(","Nathan","Levinson",")","."],"labels":["O","O","O","B-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","B-person","I-person","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-musical artist","I-musical artist","O","O"],"target_index":null,"target_label":null},"label_list":["song","album","music_genre","location","person","musical_artist","band","event","musical_instrument","award","country","organization"]}
{"id":"70","dataset":"crossner_music","split":"dev","instance":{"id":"70","prompt_labels":"Alexander(B-musical artist) has(O) had(O) an(O) active(O) career(O) on(O) stage(O) ,(O) appearing(O) in(O) several(O) Broadway(B-organization) musicals(O) including(O) Jerome(O) Robbins(O) '(O) Broadway(O) in(O) 1989(O) ,(O) for(O) which(O) he(O) won(O) the(O) Tony(B-award) Award(I-award) as(O) Best(B-award) Leading(I-award) Actor(I-award) in(I-award) a(I-award) Musical(I-award) and(O) a(O) Grammy(B-award) Award(I-award) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Musical(I-award) Theater(I-award) Album(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, song, album, music genre, organization, musical artist, musical instrument, band, country, award, location, event and O.\nSentence: Alexander has had an active career on stage , appearing in several Broadway musicals including Jerome Robbins ' Broadway in 1989 , for which he won the Tony Award as Best Leading Actor in a Musical and a Grammy Award for Grammy Award for Best Musical Theater Album .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Alexander","has","had","an","active","career","on","stage",",","appearing","in","several","Broadway","musicals","including","Jerome","Robbins","'","Broadway","in","1989",",","for","which","he","won","the","Tony","Award","as","Best","Leading","Actor","in","a","Musical","and","a","Grammy","Award","for","Grammy","Award","for","Best","Musical","Theater","Album","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["person","song","album","music_genre","organization","musical_artist","musical_instrument","band","country","award","location","event"]}
{"id":"86","dataset":"crossner_music","split":"dev","instance":{"id":"86","prompt_labels":"Other(O) early(O) examples(O) include(O) the(O) Mercury(B-award) Music(I-award) Prize(I-award) winning(O) albums(O) New(B-album) Forms(I-album) ((O) 1997(O) )(O) from(O) Reprazent(B-band) and(O) OK(B-band) ((O) 1998(O) )(O) from(O) Talvin(B-musical artist) Singh(I-musical artist) ,(O) 4hero(B-band) '(O) s(O) Mercury-nominated(O) Two(B-album) Pages(I-album) from(O) 1998(O) ,(O) and(O) Pendulum(B-band) '(O) s(O) Hold(B-album) Your(I-album) Colour(I-album) in(O) 2005(O) ((O) the(O) best(O) selling(O) drum(B-musical instrument) and(O) bass(B-musical instrument) album(O) of(O) all(O) time(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, location, song, person, musical instrument, band, album, country, organization, music genre, event, award and O.\nSentence: Other early examples include the Mercury Music Prize winning albums New Forms ( 1997 ) from Reprazent and OK ( 1998 ) from Talvin Singh , 4hero ' s Mercury-nominated Two Pages from 1998 , and Pendulum ' s Hold Your Colour in 2005 ( the best selling drum and bass album of all time ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","early","examples","include","the","Mercury","Music","Prize","winning","albums","New","Forms","(","1997",")","from","Reprazent","and","OK","(","1998",")","from","Talvin","Singh",",","4hero","'","s","Mercury-nominated","Two","Pages","from","1998",",","and","Pendulum","'","s","Hold","Your","Colour","in","2005","(","the","best","selling","drum","and","bass","album","of","all","time",")","."],"labels":["O","O","O","O","O","B-award","I-award","I-award","O","O","B-album","I-album","O","O","O","O","B-band","O","B-band","O","O","O","O","B-musical artist","I-musical artist","O","B-band","O","O","O","B-album","I-album","O","O","O","O","B-band","O","O","B-album","I-album","I-album","O","O","O","O","O","O","B-musical instrument","O","B-musical instrument","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","location","song","person","musical_instrument","band","album","country","organization","music_genre","event","award"]}
{"id":"91","dataset":"crossner_music","split":"dev","instance":{"id":"91","prompt_labels":"Black(B-album) Holes(I-album) and(I-album) Revelations(I-album) ((O) 2006(O) )(O) incorporated(O) Electronic(B-music genre) music(I-music genre) and(O) Pop(B-music genre) music(I-music genre) elements(O) ,(O) displayed(O) in(O) singles(O) such(O) as(O) Supermassive(B-song) Black(I-song) Hole(I-song) ,(O) Their(O) seventh(O) album(O) ,(O) Drones(B-album) ((O) 2015(O) )(O) ,(O) was(O) a(O) concept(O) album(O) about(O) drone(B-album) warfare(I-album) and(O) returned(O) to(O) a(O) harder(O) rock(B-music genre) sound(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, organization, country, location, event, musical artist, band, album, award, song, person, music genre and O.\nSentence: Black Holes and Revelations ( 2006 ) incorporated Electronic music and Pop music elements , displayed in singles such as Supermassive Black Hole , Their seventh album , Drones ( 2015 ) , was a concept album about drone warfare and returned to a harder rock sound .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Black","Holes","and","Revelations","(","2006",")","incorporated","Electronic","music","and","Pop","music","elements",",","displayed","in","singles","such","as","Supermassive","Black","Hole",",","Their","seventh","album",",","Drones","(","2015",")",",","was","a","concept","album","about","drone","warfare","and","returned","to","a","harder","rock","sound","."],"labels":["B-album","I-album","I-album","I-album","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O","O","O","O","O","O","B-song","I-song","I-song","O","O","O","O","O","B-album","O","O","O","O","O","O","O","O","O","B-album","I-album","O","O","O","O","O","B-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","organization","country","location","event","musical_artist","band","album","award","song","person","music_genre"]}
{"id":"93","dataset":"crossner_music","split":"dev","instance":{"id":"93","prompt_labels":"Barney(B-person) Bubbles(I-person) directed(O) several(O) videos(O) ,(O) including(O) the(B-band) Specials(I-band) '(O) Ghost(B-song) Town(I-song) ,(O) Squeeze(B-band) '(O) s(O) Is(B-song) That(I-song) Love(I-song) and(O) Tempted(B-song) ,(O) Elvis(B-musical artist) Costello(I-musical artist) '(O) s(O) Clubland(B-song) and(O) New(B-song) Lace(I-song) Sleeves(I-song) ,(O) and(O) Fun(B-band) Boy(I-band) Three(I-band) '(O) s(O) The(B-song) Lunatics(I-song) Have(I-song) Taken(I-song) Over(I-song) the(I-song) Asylum(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, band, country, organization, person, musical artist, event, award, musical instrument, song, album, location and O.\nSentence: Barney Bubbles directed several videos , including the Specials ' Ghost Town , Squeeze ' s Is That Love and Tempted , Elvis Costello ' s Clubland and New Lace Sleeves , and Fun Boy Three ' s The Lunatics Have Taken Over the Asylum .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Barney","Bubbles","directed","several","videos",",","including","the","Specials","'","Ghost","Town",",","Squeeze","'","s","Is","That","Love","and","Tempted",",","Elvis","Costello","'","s","Clubland","and","New","Lace","Sleeves",",","and","Fun","Boy","Three","'","s","The","Lunatics","Have","Taken","Over","the","Asylum","."],"labels":["B-person","I-person","O","O","O","O","O","B-band","I-band","O","B-song","I-song","O","B-band","O","O","B-song","I-song","I-song","O","B-song","O","B-musical artist","I-musical artist","O","O","B-song","O","B-song","I-song","I-song","O","O","B-band","I-band","I-band","O","O","B-song","I-song","I-song","I-song","I-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["music_genre","band","country","organization","person","musical_artist","event","award","musical_instrument","song","album","location"]}
{"id":"94","dataset":"crossner_music","split":"dev","instance":{"id":"94","prompt_labels":"Their(O) first(O) album(O) ,(O) Let(B-album) Them(I-album) Eat(I-album) Bingo(I-album) ,(O) included(O) the(O) number(O) one(O) single(O) Dub(B-song) Be(I-song) Good(I-song) to(I-song) Me(I-song) ,(O) which(O) caused(O) a(O) legal(O) dispute(O) revolving(O) around(O) allegations(O) of(O) infringement(O) of(O) copyright(O) through(O) the(O) liberal(O) use(O) of(O) unauthorised(O) samples(O) :(O) the(O) bassline(B-music genre) was(O) a(O) note-for-note(O) lift(O) from(O) The(B-song) Guns(I-song) of(I-song) Brixton(I-song) by(O) The(B-band) Clash(I-band) and(O) the(O) lyrics(O) borrowed(O) heavily(O) from(O) Just(B-song) Be(I-song) Good(I-song) to(I-song) Me(I-song) by(O) The(B-band) S.O.S.(I-band) Band(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, song, award, album, event, band, organization, musical artist, person, musical instrument, music genre, country and O.\nSentence: Their first album , Let Them Eat Bingo , included the number one single Dub Be Good to Me , which caused a legal dispute revolving around allegations of infringement of copyright through the liberal use of unauthorised samples : the bassline was a note-for-note lift from The Guns of Brixton by The Clash and the lyrics borrowed heavily from Just Be Good to Me by The S.O.S. Band .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Their","first","album",",","Let","Them","Eat","Bingo",",","included","the","number","one","single","Dub","Be","Good","to","Me",",","which","caused","a","legal","dispute","revolving","around","allegations","of","infringement","of","copyright","through","the","liberal","use","of","unauthorised","samples",":","the","bassline","was","a","note-for-note","lift","from","The","Guns","of","Brixton","by","The","Clash","and","the","lyrics","borrowed","heavily","from","Just","Be","Good","to","Me","by","The","S.O.S.","Band","."],"labels":["O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","O","O","B-song","I-song","I-song","I-song","I-song","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","O","O","O","O","O","B-song","I-song","I-song","I-song","O","B-band","I-band","O","O","O","O","O","O","B-song","I-song","I-song","I-song","I-song","O","B-band","I-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["location","song","award","album","event","band","organization","musical_artist","person","musical_instrument","music_genre","country"]}
{"id":"95","dataset":"crossner_music","split":"dev","instance":{"id":"95","prompt_labels":"In(O) June(O) 1932(O) he(O) joined(O) the(O) British(B-organization) Astronomical(I-organization) Association(I-organization) ,(O) in(O) November(O) of(O) the(O) same(O) year(O) he(O) became(O) a(O) Fellow(B-award) of(I-award) the(I-award) Royal(I-award) Astronomical(I-award) Society(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, person, musical artist, location, song, musical instrument, music genre, country, event, organization, award, band and O.\nSentence: In June 1932 he joined the British Astronomical Association , in November of the same year he became a Fellow of the Royal Astronomical Society .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","June","1932","he","joined","the","British","Astronomical","Association",",","in","November","of","the","same","year","he","became","a","Fellow","of","the","Royal","Astronomical","Society","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["album","person","musical_artist","location","song","musical_instrument","music_genre","country","event","organization","award","band"]}
{"id":"100","dataset":"crossner_music","split":"dev","instance":{"id":"100","prompt_labels":"Taylor(B-musical artist) Momsen(I-musical artist) ,(O) Marcus(B-band) Durant(I-band) ,(O) Brandi(B-musical artist) Carlile(I-musical artist) and(O) Taylor(B-musical artist) Hawkins(I-musical artist) contributed(O) vocals(O) to(O) Soundgarden(B-band) ,(O) who(O) performed(O) Rusty(B-song) Cage(I-song) ,(O) Flower(B-song) ,(O) Outshined(B-song) ,(O) Drawing(B-song) Flies(I-song) ,(O) Loud(B-song) Love(I-song) ,(O) I(B-song) Awake(I-song) ,(O) The(B-song) Day(I-song) I(I-song) Tried(I-song) to(I-song) Live(I-song) and(O) Black(B-song) Hole(I-song) Sun(I-song) ,(O) making(O) this(O) their(O) only(O) performance(O) since(O) Cornell(B-musical artist) 's(O) death(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, album, musical artist, country, organization, event, music genre, song, location, band, musical instrument, award and O.\nSentence: Taylor Momsen , Marcus Durant , Brandi Carlile and Taylor Hawkins contributed vocals to Soundgarden , who performed Rusty Cage , Flower , Outshined , Drawing Flies , Loud Love , I Awake , The Day I Tried to Live and Black Hole Sun , making this their only performance since Cornell 's death .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Taylor","Momsen",",","Marcus","Durant",",","Brandi","Carlile","and","Taylor","Hawkins","contributed","vocals","to","Soundgarden",",","who","performed","Rusty","Cage",",","Flower",",","Outshined",",","Drawing","Flies",",","Loud","Love",",","I","Awake",",","The","Day","I","Tried","to","Live","and","Black","Hole","Sun",",","making","this","their","only","performance","since","Cornell","'s","death","."],"labels":["B-musical artist","I-musical artist","O","B-band","I-band","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","B-band","O","O","O","B-song","I-song","O","B-song","O","B-song","O","B-song","I-song","O","B-song","I-song","O","B-song","I-song","O","B-song","I-song","I-song","I-song","I-song","I-song","O","B-song","I-song","I-song","O","O","O","O","O","O","O","B-musical artist","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","album","musical_artist","country","organization","event","music_genre","song","location","band","musical_instrument","award"]}
{"id":"113","dataset":"crossner_music","split":"dev","instance":{"id":"113","prompt_labels":"They(O) have(O) won(O) two(O) Grammy(B-award) Award(I-award) s(O) ,(O) six(O) American(B-award) Music(I-award) Awards(I-award) ,(O) two(O) Billboard(B-award) Music(I-award) Award(I-award) ,(O) four(O) MTV(B-award) Video(I-award) Music(I-award) Award(I-award) s(O) ,(O) 10(O) MTV(B-award) Europe(I-award) Music(I-award) Award(I-award) and(O) three(O) World(B-award) Music(I-award) Awards(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, song, person, organization, music genre, musical instrument, band, location, award, event, musical artist, album and O.\nSentence: They have won two Grammy Award s , six American Music Awards , two Billboard Music Award , four MTV Video Music Award s , 10 MTV Europe Music Award and three World Music Awards .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["They","have","won","two","Grammy","Award","s",",","six","American","Music","Awards",",","two","Billboard","Music","Award",",","four","MTV","Video","Music","Award","s",",","10","MTV","Europe","Music","Award","and","three","World","Music","Awards","."],"labels":["O","O","O","O","B-award","I-award","O","O","O","B-award","I-award","I-award","O","O","B-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["country","song","person","organization","music_genre","musical_instrument","band","location","award","event","musical_artist","album"]}
{"id":"116","dataset":"crossner_music","split":"dev","instance":{"id":"116","prompt_labels":"I(B-song) Don(I-song) 't(I-song) Stand(I-song) a(I-song) Ghost(I-song) of(I-song) a(I-song) Chance(I-song) With(I-song) You(I-song) was(O) his(O) most(O) successful(O) composition(O) ,(O) recorded(O) by(O) Duke(B-musical artist) Ellington(I-musical artist) ,(O) Frank(B-musical artist) Sinatra(I-musical artist) ,(O) Thelonious(B-musical artist) Monk(I-musical artist) ,(O) Billie(B-musical artist) Holiday(I-musical artist) ,(O) and(O) Mildred(B-musical artist) Bailey(I-musical artist) ,(O) among(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, album, song, person, musical instrument, event, award, band, music genre, location, musical artist and O.\nSentence: I Don 't Stand a Ghost of a Chance With You was his most successful composition , recorded by Duke Ellington , Frank Sinatra , Thelonious Monk , Billie Holiday , and Mildred Bailey , among others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["I","Don","'t","Stand","a","Ghost","of","a","Chance","With","You","was","his","most","successful","composition",",","recorded","by","Duke","Ellington",",","Frank","Sinatra",",","Thelonious","Monk",",","Billie","Holiday",",","and","Mildred","Bailey",",","among","others","."],"labels":["B-song","I-song","I-song","I-song","I-song","I-song","I-song","I-song","I-song","I-song","I-song","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","organization","album","song","person","musical_instrument","event","award","band","music_genre","location","musical_artist"]}
{"id":"118","dataset":"crossner_music","split":"dev","instance":{"id":"118","prompt_labels":"Urban(O) male(O) performers(O) included(O) popular(O) black(O) musicians(O) of(O) the(O) era(O) ,(O) such(O) as(O) Tampa(B-musical artist) Red(I-musical artist) ,(O) Big(B-musical artist) Bill(I-musical artist) Broonzy(I-musical artist) and(O) Leroy(B-musical artist) Carr(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical instrument, country, person, award, musical artist, band, album, location, music genre, song, organization and O.\nSentence: Urban male performers included popular black musicians of the era , such as Tampa Red , Big Bill Broonzy and Leroy Carr .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Urban","male","performers","included","popular","black","musicians","of","the","era",",","such","as","Tampa","Red",",","Big","Bill","Broonzy","and","Leroy","Carr","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["event","musical_instrument","country","person","award","musical_artist","band","album","location","music_genre","song","organization"]}
{"id":"124","dataset":"crossner_music","split":"dev","instance":{"id":"124","prompt_labels":"High-pitched(O) screaming(O) is(O) occasionally(O) utilized(O) in(O) death(B-music genre) metal(I-music genre) ,(O) being(O) heard(O) in(O) songs(O) by(O) Death(B-band) ,(O) Aborted(B-band) ,(O) Exhumed(B-band) ,(O) Dying(B-band) Fetus(I-band) ,(O) Cannibal(B-band) Corpse(I-band) ,(O) and(O) Deicide(B-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, event, location, award, album, song, music genre, organization, musical instrument, band, country, person and O.\nSentence: High-pitched screaming is occasionally utilized in death metal , being heard in songs by Death , Aborted , Exhumed , Dying Fetus , Cannibal Corpse , and Deicide .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["High-pitched","screaming","is","occasionally","utilized","in","death","metal",",","being","heard","in","songs","by","Death",",","Aborted",",","Exhumed",",","Dying","Fetus",",","Cannibal","Corpse",",","and","Deicide","."],"labels":["O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","B-band","O","B-band","O","B-band","O","B-band","I-band","O","B-band","I-band","O","O","B-band","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","event","location","award","album","song","music_genre","organization","musical_instrument","band","country","person"]}
{"id":"130","dataset":"crossner_music","split":"dev","instance":{"id":"130","prompt_labels":"Following(O) the(O) number-one(O) success(O) of(O) MARRS(B-band) '(O) Pump(B-song) Up(I-song) The(I-song) Volume(I-song) in(O) October(O) ,(O) in(O) 1987(O) to(O) 1989(O) ,(O) UK(B-country) acts(O) such(O) as(O) The(B-band) Beatmasters(I-band) ,(O) Krush(B-band) ,(O) Coldcut(B-band) ,(O) Yazz(B-band) ,(O) Bomb(B-band) The(I-band) Bass(I-band) ,(O) S-Express(B-band) ,(O) and(O) Italy(B-country) 's(O) Black(B-band) Box(I-band) opened(O) the(O) doors(O) to(O) house(B-music genre) music(I-music genre) success(O) on(O) the(O) UK(B-country) charts(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, country, musical artist, band, album, event, musical instrument, song, person, music genre, organization and O.\nSentence: Following the number-one success of MARRS ' Pump Up The Volume in October , in 1987 to 1989 , UK acts such as The Beatmasters , Krush , Coldcut , Yazz , Bomb The Bass , S-Express , and Italy 's Black Box opened the doors to house music success on the UK charts .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Following","the","number-one","success","of","MARRS","'","Pump","Up","The","Volume","in","October",",","in","1987","to","1989",",","UK","acts","such","as","The","Beatmasters",",","Krush",",","Coldcut",",","Yazz",",","Bomb","The","Bass",",","S-Express",",","and","Italy","'s","Black","Box","opened","the","doors","to","house","music","success","on","the","UK","charts","."],"labels":["O","O","O","O","O","B-band","O","B-song","I-song","I-song","I-song","O","O","O","O","O","O","O","O","B-country","O","O","O","B-band","I-band","O","B-band","O","B-band","O","B-band","O","B-band","I-band","I-band","O","B-band","O","O","B-country","O","B-band","I-band","O","O","O","O","B-music genre","I-music genre","O","O","O","B-country","O","O"],"target_index":null,"target_label":null},"label_list":["award","location","country","musical_artist","band","album","event","musical_instrument","song","person","music_genre","organization"]}
{"id":"149","dataset":"crossner_music","split":"dev","instance":{"id":"149","prompt_labels":"Sidney(B-musical artist) Simien(I-musical artist) ((O) April(O) 9(O) ,(O) 1938(O) -(O) February(O) 25(O) ,(O) 1998(O) )(O) ,(O) known(O) as(O) Rockin(B-musical artist) '(I-musical artist) Sidney(I-musical artist) and(O) Count(B-musical artist) Rockin(I-musical artist) '(I-musical artist) Sidney(I-musical artist) ,(O) was(O) an(O) American(O) Rhythm(B-music genre) and(I-music genre) blues(I-music genre) ,(O) zydeco(B-music genre) ,(O) and(O) Soul(B-music genre) music(I-music genre) musician(O) who(O) began(O) recording(O) in(O) the(O) late(O) 1950s(O) and(O) continued(O) performing(O) until(O) his(O) death(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, location, event, musical instrument, album, musical artist, song, music genre, award, band, person and O.\nSentence: Sidney Simien ( April 9 , 1938 - February 25 , 1998 ) , known as Rockin ' Sidney and Count Rockin ' Sidney , was an American Rhythm and blues , zydeco , and Soul music musician who began recording in the late 1950s and continued performing until his death .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Sidney","Simien","(","April","9",",","1938","-","February","25",",","1998",")",",","known","as","Rockin","'","Sidney","and","Count","Rockin","'","Sidney",",","was","an","American","Rhythm","and","blues",",","zydeco",",","and","Soul","music","musician","who","began","recording","in","the","late","1950s","and","continued","performing","until","his","death","."],"labels":["B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","I-musical artist","O","O","O","O","B-music genre","I-music genre","I-music genre","O","B-music genre","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","country","location","event","musical_instrument","album","musical_artist","song","music_genre","award","band","person"]}
{"id":"167","dataset":"crossner_music","split":"dev","instance":{"id":"167","prompt_labels":"It(O) is(O) generally(O) divided(O) into(O) two(O) major(O) subgenres(O) ,(O) with(O) the(O) jazz(B-music genre) -influenced(O) New(B-music genre) Orleans(I-music genre) blues(I-music genre) based(O) on(O) the(O) musical(O) traditions(O) of(O) that(O) city(O) and(O) the(O) slower(O) tempo(O) swamp(B-music genre) blues(I-music genre) incorporating(O) influences(O) from(O) zydeco(B-music genre) and(O) Cajun(B-music genre) music(I-music genre) from(O) around(O) Baton(B-location) Rouge(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, music genre, band, country, album, organization, musical artist, event, location, award, musical instrument, person and O.\nSentence: It is generally divided into two major subgenres , with the jazz -influenced New Orleans blues based on the musical traditions of that city and the slower tempo swamp blues incorporating influences from zydeco and Cajun music from around Baton Rouge .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","is","generally","divided","into","two","major","subgenres",",","with","the","jazz","-influenced","New","Orleans","blues","based","on","the","musical","traditions","of","that","city","and","the","slower","tempo","swamp","blues","incorporating","influences","from","zydeco","and","Cajun","music","from","around","Baton","Rouge","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-music genre","O","B-music genre","I-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","B-music genre","O","B-music genre","I-music genre","O","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["song","music_genre","band","country","album","organization","musical_artist","event","location","award","musical_instrument","person"]}
{"id":"169","dataset":"crossner_music","split":"dev","instance":{"id":"169","prompt_labels":"The(O) total(O) playing(O) time(O) covered(O) three(O) sides(O) of(O) an(O) LP(O) ,(O) so(O) they(O) decided(O) to(O) expand(O) it(O) into(O) a(O) double(O) by(O) including(O) previously(O) unreleased(O) tracks(O) from(O) the(O) sessions(O) for(O) the(O) earlier(O) albums(O) Led(B-album) Zeppelin(I-album) III(I-album) ,(O) Led(B-album) Zeppelin(I-album) IV(I-album) and(O) Houses(B-album) of(I-album) the(I-album) Holy(I-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, musical instrument, award, location, song, musical artist, organization, country, band, music genre, event, person and O.\nSentence: The total playing time covered three sides of an LP , so they decided to expand it into a double by including previously unreleased tracks from the sessions for the earlier albums Led Zeppelin III , Led Zeppelin IV and Houses of the Holy .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","total","playing","time","covered","three","sides","of","an","LP",",","so","they","decided","to","expand","it","into","a","double","by","including","previously","unreleased","tracks","from","the","sessions","for","the","earlier","albums","Led","Zeppelin","III",",","Led","Zeppelin","IV","and","Houses","of","the","Holy","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O","B-album","I-album","I-album","O","B-album","I-album","I-album","I-album","O"],"target_index":null,"target_label":null},"label_list":["album","musical_instrument","award","location","song","musical_artist","organization","country","band","music_genre","event","person"]}
{"id":"170","dataset":"crossner_music","split":"dev","instance":{"id":"170","prompt_labels":"Some(O) of(O) the(O) early(O) stars(O) on(O) the(O) Opry(B-location) were(O) Uncle(B-musical artist) Dave(I-musical artist) Macon(I-musical artist) ,(O) Roy(B-musical artist) Acuff(I-musical artist) and(O) African(O) American(O) harmonica(B-musical instrument) player(O) DeFord(B-musical artist) Bailey(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, country, location, song, musical artist, award, band, music genre, organization, person, musical instrument, event and O.\nSentence: Some of the early stars on the Opry were Uncle Dave Macon , Roy Acuff and African American harmonica player DeFord Bailey .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","of","the","early","stars","on","the","Opry","were","Uncle","Dave","Macon",",","Roy","Acuff","and","African","American","harmonica","player","DeFord","Bailey","."],"labels":["O","O","O","O","O","O","O","B-location","O","B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","B-musical instrument","O","B-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["album","country","location","song","musical_artist","award","band","music_genre","organization","person","musical_instrument","event"]}
{"id":"175","dataset":"crossner_music","split":"dev","instance":{"id":"175","prompt_labels":"Dimmu(B-band) Borgir(I-band) 's(O) following(O) full-length(O) albums(O) Spiritual(B-album) Black(I-album) Dimensions(I-album) in(O) 1999(O) and(O) 2001(O) 's(O) Puritanical(B-album) Euphoric(I-album) Misanthropia(I-album) ,(O) both(O) met(O) critical(O) acclaim.(O) only(O) to(O) be(O) replaced(O) with(O) Nicholas(B-musical artist) Barker(I-musical artist) of(O) Cradle(B-band) of(I-band) Filth(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, location, musical instrument, musical artist, album, event, band, person, music genre, award, country, organization and O.\nSentence: Dimmu Borgir 's following full-length albums Spiritual Black Dimensions in 1999 and 2001 's Puritanical Euphoric Misanthropia , both met critical acclaim. only to be replaced with Nicholas Barker of Cradle of Filth .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Dimmu","Borgir","'s","following","full-length","albums","Spiritual","Black","Dimensions","in","1999","and","2001","'s","Puritanical","Euphoric","Misanthropia",",","both","met","critical","acclaim.","only","to","be","replaced","with","Nicholas","Barker","of","Cradle","of","Filth","."],"labels":["B-band","I-band","O","O","O","O","B-album","I-album","I-album","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-band","I-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["song","location","musical_instrument","musical_artist","album","event","band","person","music_genre","award","country","organization"]}
{"id":"178","dataset":"crossner_music","split":"dev","instance":{"id":"178","prompt_labels":"Depeche(B-band) Mode(O) 's(O) releases(O) have(O) been(O) nominated(O) for(O) five(O) Grammy(B-award) Awards(I-award) :(O) Devotional(B-song) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Music(I-award) Film(I-award) ;(O) I(B-song) Feel(I-song) Loved(I-song) and(O) Suffer(B-song) Well(I-song) ,(O) both(O) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Dance(I-award) Recording(I-award) ;(O) Sounds(B-album) of(I-album) the(I-album) Universe(I-album) for(O) Best(B-award) Alternative(I-award) Album(I-award) ;(O) and(O) Wrong(B-song) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Music(I-award) Video(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, song, event, band, musical instrument, album, award, music genre, musical artist, country, person and O.\nSentence: Depeche Mode 's releases have been nominated for five Grammy Awards : Devotional for Grammy Award for Best Music Film ; I Feel Loved and Suffer Well , both for Grammy Award for Best Dance Recording ; Sounds of the Universe for Best Alternative Album ; and Wrong for Grammy Award for Best Music Video .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Depeche","Mode","'s","releases","have","been","nominated","for","five","Grammy","Awards",":","Devotional","for","Grammy","Award","for","Best","Music","Film",";","I","Feel","Loved","and","Suffer","Well",",","both","for","Grammy","Award","for","Best","Dance","Recording",";","Sounds","of","the","Universe","for","Best","Alternative","Album",";","and","Wrong","for","Grammy","Award","for","Best","Music","Video","."],"labels":["B-band","O","O","O","O","O","O","O","O","B-award","I-award","O","B-song","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-song","I-song","I-song","O","B-song","I-song","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-album","I-album","I-album","I-album","O","B-award","I-award","I-award","O","O","B-song","O","B-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["location","organization","song","event","band","musical_instrument","album","award","music_genre","musical_artist","country","person"]}
{"id":"180","dataset":"crossner_music","split":"dev","instance":{"id":"180","prompt_labels":"Alexander(B-musical artist) von(I-musical artist) Meilenwald(I-musical artist) from(O) German(B-country) band(O) Nagelfar(B-band) considers(O) Ungod(B-band) '(O) s(O) 1993(O) debut(O) Circle(B-album) of(I-album) the(I-album) Seven(I-album) Infernal(I-album) Pacts(I-album) ,(O) Desaster(B-band) '(O) s(O) 1994(O) demo(O) Lost(B-album) in(I-album) the(I-album) Ages(I-album) ,(O) Tha-Norr(B-band) '(O) s(O) 1995(O) album(O) Wolfenzeitalter(B-album) ,(O) Lunar(B-band) Aurora(I-band) '(O) s(O) 1996(O) debut(O) Weltengnger(B-album) and(O) Katharsis(B-band) '(O) s(O) 2000(O) debut(O) 666(B-album) Alexander(B-musical artist) von(I-musical artist) Meilenwald(I-musical artist) :(O) 5(O) Klassiker(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, band, country, musical instrument, song, location, event, person, album, award, musical artist, organization and O.\nSentence: Alexander von Meilenwald from German band Nagelfar considers Ungod ' s 1993 debut Circle of the Seven Infernal Pacts , Desaster ' s 1994 demo Lost in the Ages , Tha-Norr ' s 1995 album Wolfenzeitalter , Lunar Aurora ' s 1996 debut Weltengnger and Katharsis ' s 2000 debut 666 Alexander von Meilenwald : 5 Klassiker .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Alexander","von","Meilenwald","from","German","band","Nagelfar","considers","Ungod","'","s","1993","debut","Circle","of","the","Seven","Infernal","Pacts",",","Desaster","'","s","1994","demo","Lost","in","the","Ages",",","Tha-Norr","'","s","1995","album","Wolfenzeitalter",",","Lunar","Aurora","'","s","1996","debut","Weltengnger","and","Katharsis","'","s","2000","debut","666","Alexander","von","Meilenwald",":","5","Klassiker","."],"labels":["B-musical artist","I-musical artist","I-musical artist","O","B-country","O","B-band","O","B-band","O","O","O","O","B-album","I-album","I-album","I-album","I-album","I-album","O","B-band","O","O","O","O","B-album","I-album","I-album","I-album","O","B-band","O","O","O","O","B-album","O","B-band","I-band","O","O","O","O","B-album","O","B-band","O","O","O","O","B-album","B-musical artist","I-musical artist","I-musical artist","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["music_genre","band","country","musical_instrument","song","location","event","person","album","award","musical_artist","organization"]}
{"id":"187","dataset":"crossner_music","split":"dev","instance":{"id":"187","prompt_labels":"Fredriksson(B-musical artist) returned(O) in(O) 2006(O) with(O) an(O) album(O) of(O) Swedish(O) cover(O) songs(O) ,(O) titled(O) Min(B-album) bste(I-album) vn(I-album) ((O) My(B-album) Best(I-album) Friend(I-album) )(O) ,(O) while(O) Gessle(B-musical artist) recorded(O) two(O) more(O) solo(O) albums(O) ,(O) En(B-album) hndig(I-album) man(I-album) ((O) A(B-album) Handy(I-album) Man(I-album) )(O) ((O) 2007(O) )(O) and(O) Party(B-album) Crasher(I-album) ((O) 2008(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, song, music genre, award, album, musical instrument, event, country, band, musical artist, location, organization and O.\nSentence: Fredriksson returned in 2006 with an album of Swedish cover songs , titled Min bste vn ( My Best Friend ) , while Gessle recorded two more solo albums , En hndig man ( A Handy Man ) ( 2007 ) and Party Crasher ( 2008 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Fredriksson","returned","in","2006","with","an","album","of","Swedish","cover","songs",",","titled","Min","bste","vn","(","My","Best","Friend",")",",","while","Gessle","recorded","two","more","solo","albums",",","En","hndig","man","(","A","Handy","Man",")","(","2007",")","and","Party","Crasher","(","2008",")","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O","B-album","I-album","I-album","O","O","O","B-musical artist","O","O","O","O","O","O","B-album","I-album","I-album","O","B-album","I-album","I-album","O","O","O","O","O","B-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","song","music_genre","award","album","musical_instrument","event","country","band","musical_artist","location","organization"]}
{"id":"191","dataset":"crossner_music","split":"dev","instance":{"id":"191","prompt_labels":"It(O) was(O) designed(O) by(O) Kenzo(B-person) Tange(I-person) and(O) built(O) between(O) 1961(O) and(O) 1964(O) to(O) house(O) Swimming(O) at(O) the(O) 1964(B-event) Summer(I-event) Olympics(I-event) and(O) Diving(O) at(O) the(O) 1964(B-event) Summer(I-event) Olympics(I-event) events(O) in(O) the(O) 1964(B-event) Summer(I-event) Olympics(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, album, event, country, song, band, organization, musical instrument, award, musical artist, music genre, person and O.\nSentence: It was designed by Kenzo Tange and built between 1961 and 1964 to house Swimming at the 1964 Summer Olympics and Diving at the 1964 Summer Olympics events in the 1964 Summer Olympics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","designed","by","Kenzo","Tange","and","built","between","1961","and","1964","to","house","Swimming","at","the","1964","Summer","Olympics","and","Diving","at","the","1964","Summer","Olympics","events","in","the","1964","Summer","Olympics","."],"labels":["O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","B-event","I-event","I-event","O","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["location","album","event","country","song","band","organization","musical_instrument","award","musical_artist","music_genre","person"]}
{"id":"19","dataset":"crossner_politics","split":"dev","instance":{"id":"19","prompt_labels":"In(O) November(O) 2017(O) ,(O) the(O) RI(B-political party) formed(O) ,(O) along(O) with(O) Della(B-politician) Vedova(I-politician) 's(O) Forza(B-political party) Europa(I-political party) ((O) FE(B-political party) )(O) and(O) some(O) members(O) of(O) the(O) Civics(B-political party) and(I-political party) Innovators(I-political party) ((O) CI(B-political party) )(O) ,(O) More(B-political party) Europe(I-political party) ((O) +(B-political party) Eu(I-political party) )(O) ,(O) a(O) pro-Europeanist(O) list(O) for(O) the(O) 2018(B-election) Italian(I-election) general(I-election) election(I-election) ,(O) led(O) by(O) Bonino(B-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, organization, political party, country, person, election, politician and O.\nSentence: In November 2017 , the RI formed , along with Della Vedova 's Forza Europa ( FE ) and some members of the Civics and Innovators ( CI ) , More Europe ( + Eu ) , a pro-Europeanist list for the 2018 Italian general election , led by Bonino .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","November","2017",",","the","RI","formed",",","along","with","Della","Vedova","'s","Forza","Europa","(","FE",")","and","some","members","of","the","Civics","and","Innovators","(","CI",")",",","More","Europe","(","+","Eu",")",",","a","pro-Europeanist","list","for","the","2018","Italian","general","election",",","led","by","Bonino","."],"labels":["O","O","O","O","O","B-political party","O","O","O","O","B-politician","I-politician","O","B-political party","I-political party","O","B-political party","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","B-political party","I-political party","O","B-political party","I-political party","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","B-politician","O"],"target_index":null,"target_label":null},"label_list":["event","location","organization","political_party","country","person","election","politician"]}
{"id":"22","dataset":"crossner_politics","split":"dev","instance":{"id":"22","prompt_labels":"For(O) the(O) 2016(B-election) Belarusian(I-election) parliamentary(I-election) election(I-election) ,(O) the(O) party(O) formed(O) an(O) alliance(O) with(O) the(O) BPF(B-political party) Party(I-political party) ,(O) the(O) Belarusian(B-political party) Christian(I-political party) Democracy(I-political party) ,(O) the(O) Social(B-political party) Democratic(I-political party) Party(I-political party) ((O) Assembly(B-political party) )(O) ,(O) the(O) '(B-political party) Za(I-political party) svabodu(I-political party) '(I-political party) movement(I-political party) ,(O) the(O) Belarusian(B-political party) Green(I-political party) Party(I-political party) ,(O) the(O) Belarusian(B-political party) Liberal(I-political party) Party(I-political party) of(I-political party) Freedom(I-political party) and(I-political party) Progress(I-political party) ,(O) the(O) Trade(B-political party) Union(I-political party) of(I-political party) Electric(I-political party) Industry(I-political party) and(O) independent(O) candidates(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, politician, political party, person, country, election, location and O.\nSentence: For the 2016 Belarusian parliamentary election , the party formed an alliance with the BPF Party , the Belarusian Christian Democracy , the Social Democratic Party ( Assembly ) , the ' Za svabodu ' movement , the Belarusian Green Party , the Belarusian Liberal Party of Freedom and Progress , the Trade Union of Electric Industry and independent candidates .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","the","2016","Belarusian","parliamentary","election",",","the","party","formed","an","alliance","with","the","BPF","Party",",","the","Belarusian","Christian","Democracy",",","the","Social","Democratic","Party","(","Assembly",")",",","the","'","Za","svabodu","'","movement",",","the","Belarusian","Green","Party",",","the","Belarusian","Liberal","Party","of","Freedom","and","Progress",",","the","Trade","Union","of","Electric","Industry","and","independent","candidates","."],"labels":["O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","organization","politician","political_party","person","country","election","location"]}
{"id":"24","dataset":"crossner_politics","split":"dev","instance":{"id":"24","prompt_labels":"In(O) March(O) 2002(O) the(O) PRL(B-political party) merged(O) with(O) the(O) German-speaking(O) Partei(B-political party) fr(I-political party) Freiheit(I-political party) und(I-political party) Fortschritt(I-political party) ((O) PFF(B-political party) )(O) of(O) the(O) East(B-political party) Cantons(I-political party) ,(O) the(O) Democratic(B-political party) Front(I-political party) of(I-political party) Francophones(I-political party) ((O) FDF(B-political party) )(O) and(O) the(O) Mouvement(B-political party) des(I-political party) Citoyens(I-political party) pour(I-political party) le(I-political party) Changement(I-political party) ((O) MCC(B-political party) )(O) into(O) the(O) Mouvement(B-political party) Rformateur(I-political party) ((O) MR(B-political party) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, political party, election, country, location, organization, event, person and O.\nSentence: In March 2002 the PRL merged with the German-speaking Partei fr Freiheit und Fortschritt ( PFF ) of the East Cantons , the Democratic Front of Francophones ( FDF ) and the Mouvement des Citoyens pour le Changement ( MCC ) into the Mouvement Rformateur ( MR ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","March","2002","the","PRL","merged","with","the","German-speaking","Partei","fr","Freiheit","und","Fortschritt","(","PFF",")","of","the","East","Cantons",",","the","Democratic","Front","of","Francophones","(","FDF",")","and","the","Mouvement","des","Citoyens","pour","le","Changement","(","MCC",")","into","the","Mouvement","Rformateur","(","MR",")","."],"labels":["O","O","O","O","B-political party","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","O","B-political party","O","O"],"target_index":null,"target_label":null},"label_list":["politician","political_party","election","country","location","organization","event","person"]}
{"id":"42","dataset":"crossner_politics","split":"dev","instance":{"id":"42","prompt_labels":"In(O) the(O) February(B-election) 1974(I-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) however(O) ,(O) the(O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) ((O) SDLP(B-political party) )(O) contested(O) the(O) seat(O) ,(O) dividing(O) the(O) nationalist(O) vote(O) and(O) allowing(O) Harry(B-politician) West(I-politician) of(O) the(O) UUP(B-political party) to(O) win(O) with(O) the(O) support(O) of(O) the(O) Vanguard(B-political party) Progressive(I-political party) Unionist(I-political party) Party(I-political party) and(O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, event, politician, location, political party, person, election and O.\nSentence: In the February 1974 United Kingdom general election , however , the Social Democratic and Labour Party ( SDLP ) contested the seat , dividing the nationalist vote and allowing Harry West of the UUP to win with the support of the Vanguard Progressive Unionist Party and the Democratic Unionist Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","February","1974","United","Kingdom","general","election",",","however",",","the","Social","Democratic","and","Labour","Party","(","SDLP",")","contested","the","seat",",","dividing","the","nationalist","vote","and","allowing","Harry","West","of","the","UUP","to","win","with","the","support","of","the","Vanguard","Progressive","Unionist","Party","and","the","Democratic","Unionist","Party","."],"labels":["O","O","B-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","B-political party","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["country","organization","event","politician","location","political_party","person","election"]}
{"id":"44","dataset":"crossner_politics","split":"dev","instance":{"id":"44","prompt_labels":"The(O) party(O) did(O) do(O) quite(O) well(O) for(O) a(O) new(O) party(O) ,(O) joining(O) the(O) opposition(O) led(O) by(O) the(O) Democratic(B-political party) Party(I-political party) of(I-political party) Japan(I-political party) ((O) DPJ(B-political party) )(O) and(O) also(O) including(O) the(O) New(B-political party) Kmeit(I-political party) ,(O) the(O) Social(B-political party) Democratic(I-political party) Party(I-political party) and(O) Japanese(B-political party) Communist(I-political party) Party(I-political party) ,(O) and(O) thus(O) helped(O) contest(O) elections(O) against(O) the(O) ruling(O) conservative(O) Liberal(B-political party) Democratic(I-political party) Party(I-political party) ((O) LDP(B-political party) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, country, election, event, organization, location, politician, person and O.\nSentence: The party did do quite well for a new party , joining the opposition led by the Democratic Party of Japan ( DPJ ) and also including the New Kmeit , the Social Democratic Party and Japanese Communist Party , and thus helped contest elections against the ruling conservative Liberal Democratic Party ( LDP ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","party","did","do","quite","well","for","a","new","party",",","joining","the","opposition","led","by","the","Democratic","Party","of","Japan","(","DPJ",")","and","also","including","the","New","Kmeit",",","the","Social","Democratic","Party","and","Japanese","Communist","Party",",","and","thus","helped","contest","elections","against","the","ruling","conservative","Liberal","Democratic","Party","(","LDP",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","B-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","country","election","event","organization","location","politician","person"]}
{"id":"49","dataset":"crossner_politics","split":"dev","instance":{"id":"49","prompt_labels":"AP(B-political party) was(O) born(O) as(O) an(O) anti-establishment(O) party(O) in(O) 1993(O) ,(O) in(O) parallel(O) with(O) the(O) rise(O) of(O) Lega(B-political party) Nord(I-political party) in(O) Italy(B-country) ,(O) of(O) which(O) it(O) has(O) been(O) long(O) considered(O) the(O) Sanmarinese(O) counterpart(O) ,(O) but(O) has(O) since(O) then(O) become(O) a(O) stable(O) political(O) force(O) in(O) San(B-location) Marino(I-location) ,(O) participating(O) in(O) government(O) coalitions(O) with(O) the(O) centrist(O) Sammarinese(B-political party) Christian(I-political party) Democratic(I-political party) Party(I-political party) ((O) PDCS(B-political party) )(O) as(O) well(O) as(O) with(O) the(O) centre-left(O) Party(B-political party) of(I-political party) Socialists(I-political party) and(I-political party) Democrats(I-political party) ((O) PSD(B-political party) )(O) since(O) 2002(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, location, organization, person, politician, political party, country and O.\nSentence: AP was born as an anti-establishment party in 1993 , in parallel with the rise of Lega Nord in Italy , of which it has been long considered the Sanmarinese counterpart , but has since then become a stable political force in San Marino , participating in government coalitions with the centrist Sammarinese Christian Democratic Party ( PDCS ) as well as with the centre-left Party of Socialists and Democrats ( PSD ) since 2002 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["AP","was","born","as","an","anti-establishment","party","in","1993",",","in","parallel","with","the","rise","of","Lega","Nord","in","Italy",",","of","which","it","has","been","long","considered","the","Sanmarinese","counterpart",",","but","has","since","then","become","a","stable","political","force","in","San","Marino",",","participating","in","government","coalitions","with","the","centrist","Sammarinese","Christian","Democratic","Party","(","PDCS",")","as","well","as","with","the","centre-left","Party","of","Socialists","and","Democrats","(","PSD",")","since","2002","."],"labels":["B-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","election","location","organization","person","politician","political_party","country"]}
{"id":"52","dataset":"crossner_politics","split":"dev","instance":{"id":"52","prompt_labels":"Adams(B-politician) previously(O) held(O) the(O) seat(O) from(O) 1983(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) to(O) 1992(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) when(O) he(O) lost(O) it(O) to(O) Joe(B-politician) Hendron(I-politician) of(O) the(O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) but(O) regained(O) it(O) in(O) 1997(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, political party, country, organization, location, event, election and O.\nSentence: Adams previously held the seat from 1983 United Kingdom general election to 1992 United Kingdom general election when he lost it to Joe Hendron of the Social Democratic and Labour Party but regained it in 1997 United Kingdom general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Adams","previously","held","the","seat","from","1983","United","Kingdom","general","election","to","1992","United","Kingdom","general","election","when","he","lost","it","to","Joe","Hendron","of","the","Social","Democratic","and","Labour","Party","but","regained","it","in","1997","United","Kingdom","general","election","."],"labels":["B-politician","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","B-politician","I-politician","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["person","politician","political_party","country","organization","location","event","election"]}
{"id":"58","dataset":"crossner_politics","split":"dev","instance":{"id":"58","prompt_labels":"The(O) 1936(O) election(O) produced(O) a(O) minority(O) government(O) ,(O) with(O) 23(O) Manitoba(B-political party) Liberal(I-political party) Party(I-political party) ,(O) 16(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Manitoba(I-political party) ,(O) 7(O) Independent(B-political party) Labour(I-political party) Party(I-political party) members(O) ,(O) the(O) 5(O) Social(O) Crediters(O) and(O) 4(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, country, event, politician, location, person, political party and O.\nSentence: The 1936 election produced a minority government , with 23 Manitoba Liberal Party , 16 Progressive Conservative Party of Manitoba , 7 Independent Labour Party members , the 5 Social Crediters and 4 others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","1936","election","produced","a","minority","government",",","with","23","Manitoba","Liberal","Party",",","16","Progressive","Conservative","Party","of","Manitoba",",","7","Independent","Labour","Party","members",",","the","5","Social","Crediters","and","4","others","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","organization","country","event","politician","location","person","political_party"]}
{"id":"59","dataset":"crossner_politics","split":"dev","instance":{"id":"59","prompt_labels":"It(O) was(O) widely(O) expected(O) that(O) a(O) coalition(O) between(O) supporters(O) of(O) the(O) Orange(B-political party) Movement(I-political party) would(O) form(O) Ukraine(B-country) 's(O) next(O) government(O) ,(O) but(O) after(O) three(O) months(O) of(O) negotiations(O) and(O) a(O) failure(O) to(O) reach(O) an(O) agreement(O) the(O) proposed(O) coalition(O) collapsed(O) following(O) the(O) decision(O) of(O) the(O) Socialist(B-political party) Party(I-political party) of(I-political party) Ukraine(I-political party) to(O) support(O) the(O) formation(O) of(O) the(O) anti-crisis(O) coalition(O) with(O) Party(B-political party) of(I-political party) Regions(I-political party) and(O) the(O) Communist(B-political party) Party(I-political party) of(I-political party) Ukraine(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, political party, person, event, organization, politician, location and O.\nSentence: It was widely expected that a coalition between supporters of the Orange Movement would form Ukraine 's next government , but after three months of negotiations and a failure to reach an agreement the proposed coalition collapsed following the decision of the Socialist Party of Ukraine to support the formation of the anti-crisis coalition with Party of Regions and the Communist Party of Ukraine .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","widely","expected","that","a","coalition","between","supporters","of","the","Orange","Movement","would","form","Ukraine","'s","next","government",",","but","after","three","months","of","negotiations","and","a","failure","to","reach","an","agreement","the","proposed","coalition","collapsed","following","the","decision","of","the","Socialist","Party","of","Ukraine","to","support","the","formation","of","the","anti-crisis","coalition","with","Party","of","Regions","and","the","Communist","Party","of","Ukraine","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["country","election","political_party","person","event","organization","politician","location"]}
{"id":"61","dataset":"crossner_politics","split":"dev","instance":{"id":"61","prompt_labels":"The(O) UUP(B-political party) had(O) their(O) best(O) result(O) in(O) the(O) election(O) ,(O) in(O) part(O) due(O) to(O) no(O) candidate(O) from(O) either(O) the(O) UK(B-political party) Unionist(I-political party) Party(I-political party) or(O) Northern(B-political party) Ireland(I-political party) Unionist(I-political party) Party(I-political party) defending(O) one(O) of(O) the(O) seats(O) won(O) in(O) 1998(B-election) Northern(I-election) Ireland(I-election) Assembly(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, person, election, event, politician, political party, country and O.\nSentence: The UUP had their best result in the election , in part due to no candidate from either the UK Unionist Party or Northern Ireland Unionist Party defending one of the seats won in 1998 Northern Ireland Assembly election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","UUP","had","their","best","result","in","the","election",",","in","part","due","to","no","candidate","from","either","the","UK","Unionist","Party","or","Northern","Ireland","Unionist","Party","defending","one","of","the","seats","won","in","1998","Northern","Ireland","Assembly","election","."],"labels":["O","B-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["organization","location","person","election","event","politician","political_party","country"]}
{"id":"74","dataset":"crossner_politics","split":"dev","instance":{"id":"74","prompt_labels":"The(O) main(O) line(O) of(O) conflict(O) in(O) France(B-country) during(O) the(O) 19th(O) century(O) was(O) between(O) monarchists(O) ((O) mainly(O) Legitimists(O) and(O) Orlanist(O) s(O) ,(O) but(O) also(O) Bonapartism(O) )(O) and(O) republicans(O) ((O) Radical-Socialists(O) ,(O) Opportunist(B-organization) Republicans(I-organization) ,(O) and(O) later(O) socialists(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, location, event, election, politician, political party, organization and O.\nSentence: The main line of conflict in France during the 19th century was between monarchists ( mainly Legitimists and Orlanist s , but also Bonapartism ) and republicans ( Radical-Socialists , Opportunist Republicans , and later socialists ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","main","line","of","conflict","in","France","during","the","19th","century","was","between","monarchists","(","mainly","Legitimists","and","Orlanist","s",",","but","also","Bonapartism",")","and","republicans","(","Radical-Socialists",",","Opportunist","Republicans",",","and","later","socialists",")","."],"labels":["O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","country","location","event","election","politician","political_party","organization"]}
{"id":"79","dataset":"crossner_politics","split":"dev","instance":{"id":"79","prompt_labels":"He(O) was(O) the(O) American(B-political party) Independent(I-political party) Party(I-political party) vice(O) presidential(O) nominee(O) under(O) John(B-politician) G.(I-politician) Schmitz(I-politician) in(O) 1972(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) and(O) the(O) American(O) Party(O) presidential(O) nominee(O) in(O) 1976(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, location, election, person, event, political party, country and O.\nSentence: He was the American Independent Party vice presidential nominee under John G. Schmitz in 1972 United States presidential election and the American Party presidential nominee in 1976 United States presidential election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","the","American","Independent","Party","vice","presidential","nominee","under","John","G.","Schmitz","in","1972","United","States","presidential","election","and","the","American","Party","presidential","nominee","in","1976","United","States","presidential","election","."],"labels":["O","O","O","B-political party","I-political party","I-political party","O","O","O","O","B-politician","I-politician","I-politician","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["organization","politician","location","election","person","event","political_party","country"]}
{"id":"96","dataset":"crossner_politics","split":"dev","instance":{"id":"96","prompt_labels":"Although(O) several(O) parties(O) are(O) typically(O) represented(O) in(O) parliament(O) ,(O) Canada(B-country) has(O) historically(O) had(O) two(O) dominant(O) political(O) parties(O) :(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) and(O) the(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) ((O) preceded(O) by(O) the(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) and(O) the(O) Conservative(B-political party) Party(I-political party) ((O) 1867-1942(O) )(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, political party, politician, election, location, country, event and O.\nSentence: Although several parties are typically represented in parliament , Canada has historically had two dominant political parties : the Liberal Party of Canada and the Conservative Party of Canada ( preceded by the Progressive Conservative Party of Canada and the Conservative Party ( 1867-1942 ) ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Although","several","parties","are","typically","represented","in","parliament",",","Canada","has","historically","had","two","dominant","political","parties",":","the","Liberal","Party","of","Canada","and","the","Conservative","Party","of","Canada","(","preceded","by","the","Progressive","Conservative","Party","of","Canada","and","the","Conservative","Party","(","1867-1942",")",")","."],"labels":["O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","person","political_party","politician","election","location","country","event"]}
{"id":"98","dataset":"crossner_politics","split":"dev","instance":{"id":"98","prompt_labels":"Hugo(B-politician) Chvez(I-politician) ,(O) the(O) central(O) figure(O) of(O) the(O) Venezuelan(O) political(O) landscape(O) since(O) 1998(B-election) Venezuelan(I-election) presidential(I-election) election(I-election) as(O) a(O) political(O) outsider(O) ,(O) died(O) in(O) office(O) in(O) early(O) 2013(O) ,(O) and(O) was(O) succeeded(O) by(O) Nicols(B-politician) Maduro(I-politician) ,(O) initially(O) as(O) interim(O) President(O) ,(O) before(O) winning(O) 2013(B-election) Venezuelan(I-election) presidential(I-election) election(I-election) and(O) re-election(O) in(O) 2018(B-election) Venezuelan(I-election) presidential(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, politician, political party, event, person, election, country and O.\nSentence: Hugo Chvez , the central figure of the Venezuelan political landscape since 1998 Venezuelan presidential election as a political outsider , died in office in early 2013 , and was succeeded by Nicols Maduro , initially as interim President , before winning 2013 Venezuelan presidential election and re-election in 2018 Venezuelan presidential election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Hugo","Chvez",",","the","central","figure","of","the","Venezuelan","political","landscape","since","1998","Venezuelan","presidential","election","as","a","political","outsider",",","died","in","office","in","early","2013",",","and","was","succeeded","by","Nicols","Maduro",",","initially","as","interim","President",",","before","winning","2013","Venezuelan","presidential","election","and","re-election","in","2018","Venezuelan","presidential","election","."],"labels":["B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["location","organization","politician","political_party","event","person","election","country"]}
{"id":"99","dataset":"crossner_politics","split":"dev","instance":{"id":"99","prompt_labels":"Australia(B-country) has(O) a(O) de(O) facto(O) two-party(O) system(O) ,(O) with(O) the(O) Australian(B-political party) Labor(I-political party) Party(I-political party) and(O) the(O) Coalition(O) of(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) ,(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) ,(O) the(O) Liberal(B-political party) National(I-political party) Party(I-political party) of(I-political party) Queensland(I-political party) and(O) Country(B-political party) Liberal(I-political party) Party(I-political party) dominating(O) Parliamentary(O) elections(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, location, person, political party, politician, organization, election and O.\nSentence: Australia has a de facto two-party system , with the Australian Labor Party and the Coalition of the Liberal Party of Australia , National Party of Australia , the Liberal National Party of Queensland and Country Liberal Party dominating Parliamentary elections .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Australia","has","a","de","facto","two-party","system",",","with","the","Australian","Labor","Party","and","the","Coalition","of","the","Liberal","Party","of","Australia",",","National","Party","of","Australia",",","the","Liberal","National","Party","of","Queensland","and","Country","Liberal","Party","dominating","Parliamentary","elections","."],"labels":["B-country","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","event","location","person","political_party","politician","organization","election"]}
{"id":"106","dataset":"crossner_politics","split":"dev","instance":{"id":"106","prompt_labels":"Ante(B-politician) Starevi(I-politician) Croatian(B-political party) Party(I-political party) of(I-political party) Rights(I-political party) dr(I-political party) ,(O) Dalmatian(B-political party) Action(I-political party) ,(O) the(O) Democratic(B-political party) Centre(I-political party) ,(O) the(O) Istrian(B-political party) Democratic(I-political party) Assembly(I-political party) ,(O) the(O) Liberal(B-political party) Party(I-political party) ,(O) the(O) Party(B-political party) of(I-political party) Liberal(I-political party) Democrats(I-political party) ,(O) the(O) Serb(B-political party) Democratic(I-political party) Party(I-political party) ,(O) the(O) Slavonia-Baranja(B-political party) Croatian(I-political party) Party(I-political party) and(O) the(O) Social(B-political party) Democratic(I-political party) Action(I-political party) of(I-political party) Croatia(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, election, location, organization, political party, event, person and O.\nSentence: Ante Starevi Croatian Party of Rights dr , Dalmatian Action , the Democratic Centre , the Istrian Democratic Assembly , the Liberal Party , the Party of Liberal Democrats , the Serb Democratic Party , the Slavonia-Baranja Croatian Party and the Social Democratic Action of Croatia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Ante","Starevi","Croatian","Party","of","Rights","dr",",","Dalmatian","Action",",","the","Democratic","Centre",",","the","Istrian","Democratic","Assembly",",","the","Liberal","Party",",","the","Party","of","Liberal","Democrats",",","the","Serb","Democratic","Party",",","the","Slavonia-Baranja","Croatian","Party","and","the","Social","Democratic","Action","of","Croatia","."],"labels":["B-politician","I-politician","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","O","O","B-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["politician","country","election","location","organization","political_party","event","person"]}
{"id":"114","dataset":"crossner_politics","split":"dev","instance":{"id":"114","prompt_labels":"Hisense(B-organization) has(O) 13(O) manufacturing(O) facilities(O) in(O) China(B-country) ((O) located(O) in(O) the(O) provinces(O) /(O) cities(O) of(O) :(O) Guangdong(B-location) ,(O) Guizhou(B-location) ,(O) Huzhou(B-location) ,(O) Jiangsu(B-location) ,(O) Liaoning(B-location) ,(O) Linyi(B-location) ,(O) Shandong(B-location) ,(O) Sichuan(B-location) ,(O) Yangzhou(B-location) ,(O) Yingkou(B-location) ,(O) Xinjiang(B-location) ,(O) Zibo(B-location) and(O) the(O) municipality(O) of(O) Beijing(B-location) )(O) and(O) several(O) outside(O) China(O) ,(O) namely(O) in(O) Hungary(B-country) ,(O) South(B-country) Africa(I-country) ,(O) Egypt(B-country) ,(O) Algeria(B-country) ,(O) Slovenia(B-country) ,(O) France(B-country) and(O) Mexico(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, politician, country, election, location, event, political party and O.\nSentence: Hisense has 13 manufacturing facilities in China ( located in the provinces / cities of : Guangdong , Guizhou , Huzhou , Jiangsu , Liaoning , Linyi , Shandong , Sichuan , Yangzhou , Yingkou , Xinjiang , Zibo and the municipality of Beijing ) and several outside China , namely in Hungary , South Africa , Egypt , Algeria , Slovenia , France and Mexico .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Hisense","has","13","manufacturing","facilities","in","China","(","located","in","the","provinces","/","cities","of",":","Guangdong",",","Guizhou",",","Huzhou",",","Jiangsu",",","Liaoning",",","Linyi",",","Shandong",",","Sichuan",",","Yangzhou",",","Yingkou",",","Xinjiang",",","Zibo","and","the","municipality","of","Beijing",")","and","several","outside","China",",","namely","in","Hungary",",","South","Africa",",","Egypt",",","Algeria",",","Slovenia",",","France","and","Mexico","."],"labels":["B-organization","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","O","O","O","B-location","O","O","O","O","O","O","O","O","B-country","O","B-country","I-country","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["organization","person","politician","country","election","location","event","political_party"]}
{"id":"118","dataset":"crossner_politics","split":"dev","instance":{"id":"118","prompt_labels":"Vice(O) presidents(O) Chen(B-politician) Cheng(I-politician) ,(O) Yen(B-politician) Chia-kan(I-politician) ,(O) and(O) Lien(B-politician) Chan(I-politician) all(O) served(O) as(O) premier(O) concurrently(O) as(O) vice(O) president(O) during(O) part(O) of(O) their(O) terms(O) ,(O) and(O) vice(O) president(O) Annette(B-politician) Lu(I-politician) has(O) at(O) times(O) been(O) mentioned(O) as(O) a(O) possible(O) candidate(O) for(O) premiership(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, organization, location, politician, election, political party, event and O.\nSentence: Vice presidents Chen Cheng , Yen Chia-kan , and Lien Chan all served as premier concurrently as vice president during part of their terms , and vice president Annette Lu has at times been mentioned as a possible candidate for premiership .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Vice","presidents","Chen","Cheng",",","Yen","Chia-kan",",","and","Lien","Chan","all","served","as","premier","concurrently","as","vice","president","during","part","of","their","terms",",","and","vice","president","Annette","Lu","has","at","times","been","mentioned","as","a","possible","candidate","for","premiership","."],"labels":["O","O","B-politician","I-politician","O","B-politician","I-politician","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","country","organization","location","politician","election","political_party","event"]}
{"id":"128","dataset":"crossner_politics","split":"dev","instance":{"id":"128","prompt_labels":"The(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) government(O) of(O) the(O) time(O) ,(O) headed(O) by(O) Prime(O) Minister(O) John(B-politician) Diefenbaker(I-politician) ,(O) did(O) not(O) accept(O) the(O) invitation(O) to(O) establish(O) a(O) new(O) Canadian(O) flag(O) ,(O) so(O) Pearson(B-politician) made(O) it(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) policy(O) in(O) 1961(O) ,(O) and(O) part(O) of(O) the(O) party(O) 's(O) election(O) platform(O) in(O) the(O) 1962(B-election) Canadian(I-election) federal(I-election) election(I-election) and(O) 1963(B-election) Canadian(I-election) federal(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, political party, politician, event, organization, location, election and O.\nSentence: The Progressive Conservative Party of Canada government of the time , headed by Prime Minister John Diefenbaker , did not accept the invitation to establish a new Canadian flag , so Pearson made it Liberal Party of Canada policy in 1961 , and part of the party 's election platform in the 1962 Canadian federal election and 1963 Canadian federal election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Progressive","Conservative","Party","of","Canada","government","of","the","time",",","headed","by","Prime","Minister","John","Diefenbaker",",","did","not","accept","the","invitation","to","establish","a","new","Canadian","flag",",","so","Pearson","made","it","Liberal","Party","of","Canada","policy","in","1961",",","and","part","of","the","party","'s","election","platform","in","the","1962","Canadian","federal","election","and","1963","Canadian","federal","election","."],"labels":["O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["person","country","political_party","politician","event","organization","location","election"]}
{"id":"143","dataset":"crossner_politics","split":"dev","instance":{"id":"143","prompt_labels":"Prominent(O) members(O) of(O) the(O) SPL(B-political party) joined(O) the(O) new(O) Communist(B-political party) Party(I-political party) of(I-political party) America(I-political party) ,(O) which(O) eventually(O) merged(O) with(O) the(O) Communist(B-political party) Labor(I-political party) Party(I-political party) of(I-political party) America(I-political party) to(O) form(O) first(O) the(O) Workers(B-political party) Party(I-political party) of(I-political party) America(I-political party) and(O) eventually(O) the(O) Communist(B-political party) Party(I-political party) USA(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, event, country, political party, person, election, organization and O.\nSentence: Prominent members of the SPL joined the new Communist Party of America , which eventually merged with the Communist Labor Party of America to form first the Workers Party of America and eventually the Communist Party USA .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Prominent","members","of","the","SPL","joined","the","new","Communist","Party","of","America",",","which","eventually","merged","with","the","Communist","Labor","Party","of","America","to","form","first","the","Workers","Party","of","America","and","eventually","the","Communist","Party","USA","."],"labels":["O","O","O","O","B-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["politician","location","event","country","political_party","person","election","organization"]}
{"id":"144","dataset":"crossner_politics","split":"dev","instance":{"id":"144","prompt_labels":"He(O) contested(O) the(O) seat(O) of(O) St(B-location) Marylebone(I-location) for(O) the(O) Labour(B-political party) Party(I-political party) at(O) the(O) 1950(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) West(B-organization) Woolwich(I-organization) in(O) 1951(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) South(B-organization) Nottingham(I-organization) in(O) 1959(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, event, location, election, political party, organization, person, country and O.\nSentence: He contested the seat of St Marylebone for the Labour Party at the 1950 United Kingdom general election , West Woolwich in 1951 United Kingdom general election and South Nottingham in 1959 United Kingdom general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","contested","the","seat","of","St","Marylebone","for","the","Labour","Party","at","the","1950","United","Kingdom","general","election",",","West","Woolwich","in","1951","United","Kingdom","general","election","and","South","Nottingham","in","1959","United","Kingdom","general","election","."],"labels":["O","O","O","O","O","B-location","I-location","O","O","B-political party","I-political party","O","O","B-election","I-election","I-election","I-election","I-election","O","B-organization","I-organization","O","B-election","I-election","I-election","I-election","I-election","O","B-organization","I-organization","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["politician","event","location","election","political_party","organization","person","country"]}
{"id":"147","dataset":"crossner_politics","split":"dev","instance":{"id":"147","prompt_labels":"In(O) the(O) 1969(B-election) Malaysian(I-election) general(I-election) election(I-election) ,(O) MCA(B-political party) lost(O) more(O) than(O) half(O) its(O) seats(O) to(O) the(O) new(O) ,(O) mainly(O) Chinese(O) Malaysian(O) ,(O) opposition(O) parties(O) Democratic(B-political party) Action(I-political party) Party(I-political party) ((O) DAP(B-political party) )(O) and(O) Parti(B-political party) Gerakan(I-political party) Rakyat(I-political party) Malaysia(I-political party) ((O) Gerakan(B-political party) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, politician, political party, location, person, organization, event and O.\nSentence: In the 1969 Malaysian general election , MCA lost more than half its seats to the new , mainly Chinese Malaysian , opposition parties Democratic Action Party ( DAP ) and Parti Gerakan Rakyat Malaysia ( Gerakan ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","1969","Malaysian","general","election",",","MCA","lost","more","than","half","its","seats","to","the","new",",","mainly","Chinese","Malaysian",",","opposition","parties","Democratic","Action","Party","(","DAP",")","and","Parti","Gerakan","Rakyat","Malaysia","(","Gerakan",")","."],"labels":["O","O","B-election","I-election","I-election","I-election","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O"],"target_index":null,"target_label":null},"label_list":["country","election","politician","political_party","location","person","organization","event"]}
{"id":"154","dataset":"crossner_politics","split":"dev","instance":{"id":"154","prompt_labels":"For(O) the(O) next(O) two(O) years(O) ,(O) Villalpando(B-politician) was(O) a(O) prolific(O) activist(O) in(O) both(O) Republican(O) and(O) Hispanic(O) circles(O) ,(O) serving(O) on(O) the(O) boards(O) of(O) the(O) Texas(B-location) Federation(B-organization) of(I-organization) Republican(I-organization) Women(I-organization) ,(O) the(O) Southwest(B-organization) Voter(I-organization) Registration(I-organization) Education(I-organization) Project(I-organization) ,(O) the(O) League(B-organization) of(I-organization) United(I-organization) Latin(I-organization) American(I-organization) Citizens(I-organization) ,(O) and(O) the(O) American(B-organization) GI(I-organization) Forum(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, politician, election, political party, person, country, event, organization and O.\nSentence: For the next two years , Villalpando was a prolific activist in both Republican and Hispanic circles , serving on the boards of the Texas Federation of Republican Women , the Southwest Voter Registration Education Project , the League of United Latin American Citizens , and the American GI Forum .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","the","next","two","years",",","Villalpando","was","a","prolific","activist","in","both","Republican","and","Hispanic","circles",",","serving","on","the","boards","of","the","Texas","Federation","of","Republican","Women",",","the","Southwest","Voter","Registration","Education","Project",",","the","League","of","United","Latin","American","Citizens",",","and","the","American","GI","Forum","."],"labels":["O","O","O","O","O","O","B-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["location","politician","election","political_party","person","country","event","organization"]}
{"id":"156","dataset":"crossner_politics","split":"dev","instance":{"id":"156","prompt_labels":"In(O) the(O) following(O) table(O) ,(O) gains(O) for(O) the(O) Australian(B-political party) Labor(I-political party) Party(I-political party) are(O) highlighted(O) in(O) red(O) ,(O) for(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) and(O) its(O) predecessors(O) ((O) including(O) the(O) Protectionist(B-political party) Party(I-political party) )(O) in(O) blue(O) ,(O) for(O) the(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) and(O) its(O) predecessors(O) as(O) well(O) as(O) the(O) unrelated(O) Australian(B-political party) Greens(I-political party) in(O) green(O) !(O) --(O) The(O) National(O) 's(O) little(O) colour(O) block(O) is(O) green(O) too(O) in(O) this(O) article(O) ,(O) and(O) I(O) think(O) it(O) makes(O) it(O) confusing(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, event, election, person, political party, location, country, organization and O.\nSentence: In the following table , gains for the Australian Labor Party are highlighted in red , for the Liberal Party of Australia and its predecessors ( including the Protectionist Party ) in blue , for the National Party of Australia and its predecessors as well as the unrelated Australian Greens in green ! -- The National 's little colour block is green too in this article , and I think it makes it confusing .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","following","table",",","gains","for","the","Australian","Labor","Party","are","highlighted","in","red",",","for","the","Liberal","Party","of","Australia","and","its","predecessors","(","including","the","Protectionist","Party",")","in","blue",",","for","the","National","Party","of","Australia","and","its","predecessors","as","well","as","the","unrelated","Australian","Greens","in","green","!","--","The","National","'s","little","colour","block","is","green","too","in","this","article",",","and","I","think","it","makes","it","confusing","."],"labels":["O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["politician","event","election","person","political_party","location","country","organization"]}
{"id":"157","dataset":"crossner_politics","split":"dev","instance":{"id":"157","prompt_labels":"Despite(O) the(O) unsuccessful(O) attempt(O) at(O) the(O) 1987(B-election) Sarawak(I-election) state(I-election) election(I-election) ,(O) Abdul(B-politician) Rahman(I-politician) continued(O) his(O) struggle(O) with(O) his(O) allies(O) ,(O) Sarawak(B-location) Dayak(B-political party) People(I-political party) 's(I-political party) Party(I-political party) against(O) Taib(B-politician) 's(O) led(O) Sarawak(B-location) Barisan(B-political party) Nasional(I-political party) until(O) 1991(B-election) Sarawak(I-election) state(I-election) election(I-election) when(O) Taib(B-politician) 's(O) coalition(O) won(O) an(O) overwhelming(O) majority(O) of(O) 49(O) out(O) of(O) 56(O) seats(O) in(O) the(O) state(O) assembly(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, election, politician, political party, event, location, person, country and O.\nSentence: Despite the unsuccessful attempt at the 1987 Sarawak state election , Abdul Rahman continued his struggle with his allies , Sarawak Dayak People 's Party against Taib 's led Sarawak Barisan Nasional until 1991 Sarawak state election when Taib 's coalition won an overwhelming majority of 49 out of 56 seats in the state assembly .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Despite","the","unsuccessful","attempt","at","the","1987","Sarawak","state","election",",","Abdul","Rahman","continued","his","struggle","with","his","allies",",","Sarawak","Dayak","People","'s","Party","against","Taib","'s","led","Sarawak","Barisan","Nasional","until","1991","Sarawak","state","election","when","Taib","'s","coalition","won","an","overwhelming","majority","of","49","out","of","56","seats","in","the","state","assembly","."],"labels":["O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-politician","I-politician","O","O","O","O","O","O","O","B-location","B-political party","I-political party","I-political party","I-political party","O","B-politician","O","O","B-location","B-political party","I-political party","O","B-election","I-election","I-election","I-election","O","B-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","election","politician","political_party","event","location","person","country"]}
{"id":"167","dataset":"crossner_politics","split":"dev","instance":{"id":"167","prompt_labels":"During(O) the(O) 2005(B-election) German(I-election) federal(I-election) election(I-election) campaign(O) ,(O) Angela(B-politician) Merkel(I-politician) ,(O) leader(O) of(O) the(O) Christian(B-political party) Democratic(I-political party) Union(I-political party) of(I-political party) Germany(I-political party) /(O) Christian(B-political party) Social(I-political party) Union(I-political party) in(I-political party) Bavaria(I-political party) ,(O) announced(O) that(O) Kirchhof(B-politician) would(O) serve(O) as(O) minister(O) of(O) finance(O) if(O) she(O) formed(O) a(O) government(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, political party, country, event, organization, location, person and O.\nSentence: During the 2005 German federal election campaign , Angela Merkel , leader of the Christian Democratic Union of Germany / Christian Social Union in Bavaria , announced that Kirchhof would serve as minister of finance if she formed a government .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","the","2005","German","federal","election","campaign",",","Angela","Merkel",",","leader","of","the","Christian","Democratic","Union","of","Germany","/","Christian","Social","Union","in","Bavaria",",","announced","that","Kirchhof","would","serve","as","minister","of","finance","if","she","formed","a","government","."],"labels":["O","O","B-election","I-election","I-election","I-election","O","O","B-politician","I-politician","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","B-politician","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","politician","political_party","country","event","organization","location","person"]}
{"id":"170","dataset":"crossner_politics","split":"dev","instance":{"id":"170","prompt_labels":"The(O) seat(O) was(O) retained(O) in(O) elections(O) in(O) 1990(B-election) Costa(I-election) Rican(I-election) general(I-election) election(I-election) and(O) 1994(B-election) Costa(I-election) Rican(I-election) general(I-election) election(I-election) ,(O) but(O) a(O) loss(O) of(O) support(O) in(O) the(O) 1998(B-election) Costa(I-election) Rican(I-election) general(I-election) election(I-election) saw(O) its(O) share(O) of(O) the(O) vote(O) drop(O) to(O) 0.5(O) %(O) ,(O) resulting(O) in(O) it(O) losing(O) its(O) solitary(O) seat(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, organization, politician, event, location, election, political party and O.\nSentence: The seat was retained in elections in 1990 Costa Rican general election and 1994 Costa Rican general election , but a loss of support in the 1998 Costa Rican general election saw its share of the vote drop to 0.5 % , resulting in it losing its solitary seat .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","seat","was","retained","in","elections","in","1990","Costa","Rican","general","election","and","1994","Costa","Rican","general","election",",","but","a","loss","of","support","in","the","1998","Costa","Rican","general","election","saw","its","share","of","the","vote","drop","to","0.5","%",",","resulting","in","it","losing","its","solitary","seat","."],"labels":["O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","country","organization","politician","event","location","election","political_party"]}
{"id":"171","dataset":"crossner_politics","split":"dev","instance":{"id":"171","prompt_labels":"Conversely(O) ,(O) many(O) Indo-Fijian(O) supporters(O) of(O) the(O) National(B-political party) Federation(I-political party) Party(I-political party) ((O) NFP(B-political party) )(O) in(O) the(O) 2001(B-election) Fijian(I-election) general(I-election) election(I-election) may(O) not(O) have(O) been(O) aware(O) that(O) votes(O) for(O) NFP(B-political party) candidates(O) ,(O) all(O) of(O) whom(O) lost(O) ,(O) were(O) to(O) be(O) transferred(O) to(O) the(O) indigenous-dominated(O) Soqosoqo(B-political party) Duavata(I-political party) ni(I-political party) Lewenivanua(I-political party) ((O) SDL(B-political party) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, country, organization, location, election, person, event and O.\nSentence: Conversely , many Indo-Fijian supporters of the National Federation Party ( NFP ) in the 2001 Fijian general election may not have been aware that votes for NFP candidates , all of whom lost , were to be transferred to the indigenous-dominated Soqosoqo Duavata ni Lewenivanua ( SDL ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Conversely",",","many","Indo-Fijian","supporters","of","the","National","Federation","Party","(","NFP",")","in","the","2001","Fijian","general","election","may","not","have","been","aware","that","votes","for","NFP","candidates",",","all","of","whom","lost",",","were","to","be","transferred","to","the","indigenous-dominated","Soqosoqo","Duavata","ni","Lewenivanua","(","SDL",")","."],"labels":["O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","politician","country","organization","location","election","person","event"]}
{"id":"176","dataset":"crossner_politics","split":"dev","instance":{"id":"176","prompt_labels":"Williams(B-politician) stood(O) unsuccessfully(O) for(O) Labour(B-political party) in(O) Coventry(B-location) at(O) the(O) 1923(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 1924(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) s(O) ,(O) but(O) was(O) elected(O) to(O) the(O) National(B-organization) Executive(I-organization) Committee(I-organization) of(O) the(O) party(O) ,(O) serving(O) as(O) its(O) chair(O) in(O) 1925(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, location, country, person, politician, organization, event, election and O.\nSentence: Williams stood unsuccessfully for Labour in Coventry at the 1923 United Kingdom general election and 1924 United Kingdom general election s , but was elected to the National Executive Committee of the party , serving as its chair in 1925 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Williams","stood","unsuccessfully","for","Labour","in","Coventry","at","the","1923","United","Kingdom","general","election","and","1924","United","Kingdom","general","election","s",",","but","was","elected","to","the","National","Executive","Committee","of","the","party",",","serving","as","its","chair","in","1925","."],"labels":["B-politician","O","O","O","B-political party","O","B-location","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","location","country","person","politician","organization","event","election"]}
{"id":"184","dataset":"crossner_politics","split":"dev","instance":{"id":"184","prompt_labels":"The(O) Queensland(B-political party) Greens(I-political party) enjoyed(O) growing(O) support(O) in(O) state(O) elections(O) ,(O) increasing(O) their(O) vote(O) from(O) 2.5(O) per(O) cent(O) at(O) the(O) 2001(B-election) Queensland(I-election) state(I-election) election(I-election) ((O) when(O) they(O) contested(O) 31(O) of(O) the(O) Parliament(O) 's(O) 89(O) seats(O) )(O) ,(O) to(O) 6.76(O) per(O) cent(O) in(O) 2004(B-election) Queensland(I-election) state(I-election) election(I-election) ((O) from(O) 72(O) seats(O) )(O) ,(O) to(O) 7.99(O) per(O) cent(O) in(O) 2006(B-election) Queensland(I-election) state(I-election) election(I-election) ((O) from(O) 75(O) seats(O) )(O) ,(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, election, organization, country, person, event, location, politician and O.\nSentence: The Queensland Greens enjoyed growing support in state elections , increasing their vote from 2.5 per cent at the 2001 Queensland state election ( when they contested 31 of the Parliament 's 89 seats ) , to 6.76 per cent in 2004 Queensland state election ( from 72 seats ) , to 7.99 per cent in 2006 Queensland state election ( from 75 seats ) ,","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Queensland","Greens","enjoyed","growing","support","in","state","elections",",","increasing","their","vote","from","2.5","per","cent","at","the","2001","Queensland","state","election","(","when","they","contested","31","of","the","Parliament","'s","89","seats",")",",","to","6.76","per","cent","in","2004","Queensland","state","election","(","from","72","seats",")",",","to","7.99","per","cent","in","2006","Queensland","state","election","(","from","75","seats",")",","],"labels":["O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","election","organization","country","person","event","location","politician"]}
{"id":"2","dataset":"crossner_science","split":"dev","instance":{"id":"2","prompt_labels":"Labeled(O) genomic(O) DNA(O) is(O) extracted(O) from(O) nuclei(O) and(O) fragmented(O) by(O) HaeIII(O) digestion(O) and(O) sonication(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, discipline, scientist, event, protein, chemical element, university, chemical compound, country, location, person, award, organization, enzyme, theory, academic journal and O.\nSentence: Labeled genomic DNA is extracted from nuclei and fragmented by HaeIII digestion and sonication .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Labeled","genomic","DNA","is","extracted","from","nuclei","and","fragmented","by","HaeIII","digestion","and","sonication","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","discipline","scientist","event","protein","chemical_element","university","chemical_compound","country","location","person","award","organization","enzyme","theory","academic_journal"]}
{"id":"3","dataset":"crossner_science","split":"dev","instance":{"id":"3","prompt_labels":"He(O) attended(O) the(O) U.S.(B-university) Air(I-university) Force(I-university) Institute(I-university) of(I-university) Technology(I-university) for(O) a(O) year(O) ,(O) earning(O) a(O) bachelor(O) 's(O) degree(O) in(O) aeromechanics(B-discipline) ,(O) and(O) received(O) his(O) test(O) pilot(O) training(O) at(O) Edwards(B-organization) Air(I-organization) Force(I-organization) Base(I-organization) in(O) California(B-location) before(O) his(O) assignment(O) as(O) a(O) test(O) pilot(O) at(O) Wright-Patterson(B-organization) Air(I-organization) Force(I-organization) Base(I-organization) in(O) Ohio(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, award, protein, scientist, theory, organization, astronomical object, academic journal, event, person, university, chemical element, discipline, enzyme, country, location and O.\nSentence: He attended the U.S. Air Force Institute of Technology for a year , earning a bachelor 's degree in aeromechanics , and received his test pilot training at Edwards Air Force Base in California before his assignment as a test pilot at Wright-Patterson Air Force Base in Ohio .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","attended","the","U.S.","Air","Force","Institute","of","Technology","for","a","year",",","earning","a","bachelor","'s","degree","in","aeromechanics",",","and","received","his","test","pilot","training","at","Edwards","Air","Force","Base","in","California","before","his","assignment","as","a","test","pilot","at","Wright-Patterson","Air","Force","Base","in","Ohio","."],"labels":["O","O","O","B-university","I-university","I-university","I-university","I-university","I-university","O","O","O","O","O","O","O","O","O","O","B-discipline","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-location","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","award","protein","scientist","theory","organization","astronomical_object","academic_journal","event","person","university","chemical_element","discipline","enzyme","country","location"]}
{"id":"19","dataset":"crossner_science","split":"dev","instance":{"id":"19","prompt_labels":"PCH1(O) reduces(O) hypocotyl(O) growth(O) during(O) long(O) nights(O) by(O) preferentially(O) binding(O) and(O) stabilizing(O) the(O) active(O) form(O) of(O) Phytochrome(O) ((O) phyB(O) )(O) ,(O) prolonging(O) its(O) activity(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, theory, academic journal, scientist, protein, organization, university, award, chemical element, discipline, country, astronomical object, event, location, person, enzyme and O.\nSentence: PCH1 reduces hypocotyl growth during long nights by preferentially binding and stabilizing the active form of Phytochrome ( phyB ) , prolonging its activity .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["PCH1","reduces","hypocotyl","growth","during","long","nights","by","preferentially","binding","and","stabilizing","the","active","form","of","Phytochrome","(","phyB",")",",","prolonging","its","activity","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","theory","academic_journal","scientist","protein","organization","university","award","chemical_element","discipline","country","astronomical_object","event","location","person","enzyme"]}
{"id":"34","dataset":"crossner_science","split":"dev","instance":{"id":"34","prompt_labels":"Helicase(B-enzyme) s(O) unwind(O) the(O) strands(O) to(O) facilitate(O) the(O) advance(O) of(O) sequence-reading(O) enzymes(O) such(O) as(O) DNA(B-enzyme) polymerase(I-enzyme) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, enzyme, academic journal, astronomical object, theory, person, protein, scientist, organization, chemical element, country, event, university, discipline, chemical compound and O.\nSentence: Helicase s unwind the strands to facilitate the advance of sequence-reading enzymes such as DNA polymerase .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Helicase","s","unwind","the","strands","to","facilitate","the","advance","of","sequence-reading","enzymes","such","as","DNA","polymerase","."],"labels":["B-enzyme","O","O","O","O","O","O","O","O","O","O","O","O","O","B-enzyme","I-enzyme","O"],"target_index":null,"target_label":null},"label_list":["location","award","enzyme","academic_journal","astronomical_object","theory","person","protein","scientist","organization","chemical_element","country","event","university","discipline","chemical_compound"]}
{"id":"36","dataset":"crossner_science","split":"dev","instance":{"id":"36","prompt_labels":"Lincoln(B-person) looks(O) much(O) as(O) it(O) did(O) during(O) the(O) Lincoln(B-event) County(I-event) War(I-event) ((O) 1878-1881(O) )(O) when(O) its(O) single(O) street(O) was(O) peopled(O) with(O) characters(O) like(O) Billy(B-person) the(I-person) Kid(I-person) ,(O) John(B-person) Chisum(I-person) and(O) Lawrence(B-person) Murphy(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, astronomical object, chemical element, person, protein, theory, discipline, chemical compound, location, enzyme, country, organization, university, event, academic journal, scientist and O.\nSentence: Lincoln looks much as it did during the Lincoln County War ( 1878-1881 ) when its single street was peopled with characters like Billy the Kid , John Chisum and Lawrence Murphy .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Lincoln","looks","much","as","it","did","during","the","Lincoln","County","War","(","1878-1881",")","when","its","single","street","was","peopled","with","characters","like","Billy","the","Kid",",","John","Chisum","and","Lawrence","Murphy","."],"labels":["B-person","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","I-person","O","B-person","I-person","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["award","astronomical_object","chemical_element","person","protein","theory","discipline","chemical_compound","location","enzyme","country","organization","university","event","academic_journal","scientist"]}
{"id":"45","dataset":"crossner_science","split":"dev","instance":{"id":"45","prompt_labels":"The(O) Asian(B-organization) Football(I-organization) Confederation(I-organization) ,(O) Oceania(B-organization) Football(I-organization) Confederation(I-organization) and(O) CONCACAF(B-organization) ((O) the(O) governing(O) body(O) of(O) football(O) in(O) North(O) and(O) Central(O) America(B-country) and(O) the(O) Caribbean(B-location) )(O) use(O) blue(O) text(O) on(O) their(O) logos(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, astronomical object, country, chemical element, organization, protein, university, person, theory, discipline, chemical compound, enzyme, event, award, scientist, academic journal and O.\nSentence: The Asian Football Confederation , Oceania Football Confederation and CONCACAF ( the governing body of football in North and Central America and the Caribbean ) use blue text on their logos .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Asian","Football","Confederation",",","Oceania","Football","Confederation","and","CONCACAF","(","the","governing","body","of","football","in","North","and","Central","America","and","the","Caribbean",")","use","blue","text","on","their","logos","."],"labels":["O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","O","O","O","O","B-country","O","O","B-location","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","astronomical_object","country","chemical_element","organization","protein","university","person","theory","discipline","chemical_compound","enzyme","event","award","scientist","academic_journal"]}
{"id":"49","dataset":"crossner_science","split":"dev","instance":{"id":"49","prompt_labels":"He(O) also(O) received(O) the(O) Rumford(B-award) Medal(I-award) of(I-award) the(I-award) British(I-award) Royal(I-award) Society(I-award) in(O) 1896(O) ,(O) jointly(O) with(O) Philipp(B-scientist) Lenard(I-scientist) ,(O) who(O) had(O) already(O) shown(O) that(O) a(O) portion(O) of(O) the(O) cathode(O) rays(O) could(O) pass(O) through(O) a(O) thin(O) film(O) of(O) a(O) metal(O) such(O) as(O) aluminum(B-chemical element) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, location, event, award, chemical element, astronomical object, organization, person, chemical compound, discipline, enzyme, university, scientist, theory, protein, country and O.\nSentence: He also received the Rumford Medal of the British Royal Society in 1896 , jointly with Philipp Lenard , who had already shown that a portion of the cathode rays could pass through a thin film of a metal such as aluminum .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","also","received","the","Rumford","Medal","of","the","British","Royal","Society","in","1896",",","jointly","with","Philipp","Lenard",",","who","had","already","shown","that","a","portion","of","the","cathode","rays","could","pass","through","a","thin","film","of","a","metal","such","as","aluminum","."],"labels":["O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical element","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","location","event","award","chemical_element","astronomical_object","organization","person","chemical_compound","discipline","enzyme","university","scientist","theory","protein","country"]}
{"id":"58","dataset":"crossner_science","split":"dev","instance":{"id":"58","prompt_labels":"Thompson(B-scientist) endowed(O) the(O) Rumford(B-award) medal(I-award) s(O) of(O) the(O) Royal(B-organization) Society(I-organization) and(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) ,(O) and(O) endowed(O) a(O) professorship(O) at(O) Harvard(B-university) University(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, astronomical object, person, protein, location, scientist, university, organization, chemical compound, country, enzyme, award, theory, event, academic journal, chemical element and O.\nSentence: Thompson endowed the Rumford medal s of the Royal Society and the American Academy of Arts and Sciences , and endowed a professorship at Harvard University .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Thompson","endowed","the","Rumford","medal","s","of","the","Royal","Society","and","the","American","Academy","of","Arts","and","Sciences",",","and","endowed","a","professorship","at","Harvard","University","."],"labels":["B-scientist","O","O","B-award","I-award","O","O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","B-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["discipline","astronomical_object","person","protein","location","scientist","university","organization","chemical_compound","country","enzyme","award","theory","event","academic_journal","chemical_element"]}
{"id":"62","dataset":"crossner_science","split":"dev","instance":{"id":"62","prompt_labels":"Matja(B-scientist) Perc(I-scientist) is(O) editorial(O) board(O) member(O) at(O) Physical(B-academic journal) Review(I-academic journal) E(I-academic journal) ,(O) New(B-academic journal) Journal(I-academic journal) of(I-academic journal) Physics(I-academic journal) ,(O) EPL(B-academic journal) ,(O) European(B-academic journal) Physical(I-academic journal) Journal(I-academic journal) B(I-academic journal) ,(O) Advances(B-academic journal) in(I-academic journal) Complex(I-academic journal) Systems(I-academic journal) ,(O) Frontiers(B-academic journal) in(I-academic journal) Interdisciplinary(I-academic journal) Physics(I-academic journal) ,(O) International(B-academic journal) Journal(I-academic journal) of(I-academic journal) Bifurcation(I-academic journal) and(I-academic journal) Chaos(I-academic journal) ,(O) PLOS(B-academic journal) ONE(I-academic journal) ,(O) Scientific(B-academic journal) Reports(I-academic journal) ,(O) Royal(B-academic journal) Society(I-academic journal) Open(I-academic journal) Science(I-academic journal) ,(O) '(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, chemical compound, university, enzyme, event, protein, organization, academic journal, award, scientist, country, discipline, theory, location, person, astronomical object and O.\nSentence: Matja Perc is editorial board member at Physical Review E , New Journal of Physics , EPL , European Physical Journal B , Advances in Complex Systems , Frontiers in Interdisciplinary Physics , International Journal of Bifurcation and Chaos , PLOS ONE , Scientific Reports , Royal Society Open Science , ' .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Matja","Perc","is","editorial","board","member","at","Physical","Review","E",",","New","Journal","of","Physics",",","EPL",",","European","Physical","Journal","B",",","Advances","in","Complex","Systems",",","Frontiers","in","Interdisciplinary","Physics",",","International","Journal","of","Bifurcation","and","Chaos",",","PLOS","ONE",",","Scientific","Reports",",","Royal","Society","Open","Science",",","'","."],"labels":["B-scientist","I-scientist","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","chemical_compound","university","enzyme","event","protein","organization","academic_journal","award","scientist","country","discipline","theory","location","person","astronomical_object"]}
{"id":"68","dataset":"crossner_science","split":"dev","instance":{"id":"68","prompt_labels":"The(O) concept(O) that(O) the(O) composition(O) of(O) plant(O) communities(O) such(O) as(O) temperate(O) broadleaf(O) forest(O) changes(O) by(O) a(O) process(O) of(O) ecological(O) succession(O) was(O) developed(O) by(O) Henry(B-scientist) Chandler(I-scientist) Cowles(I-scientist) ,(O) Arthur(B-scientist) Tansley(I-scientist) and(O) Frederic(B-scientist) Clements(I-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, chemical element, scientist, university, astronomical object, chemical compound, organization, protein, country, enzyme, location, person, award, event, academic journal, discipline and O.\nSentence: The concept that the composition of plant communities such as temperate broadleaf forest changes by a process of ecological succession was developed by Henry Chandler Cowles , Arthur Tansley and Frederic Clements .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","concept","that","the","composition","of","plant","communities","such","as","temperate","broadleaf","forest","changes","by","a","process","of","ecological","succession","was","developed","by","Henry","Chandler","Cowles",",","Arthur","Tansley","and","Frederic","Clements","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O"],"target_index":null,"target_label":null},"label_list":["theory","chemical_element","scientist","university","astronomical_object","chemical_compound","organization","protein","country","enzyme","location","person","award","event","academic_journal","discipline"]}
{"id":"75","dataset":"crossner_science","split":"dev","instance":{"id":"75","prompt_labels":"The(O) Voyager(O) Program(O) ((O) namely(O) Voyager(O) 2(O) )(O) only(O) nominally(O) confirmed(O) the(O) existence(O) of(O) similar(O) belts(O) around(O) Uranus(B-astronomical object) and(O) Neptune(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, chemical element, organization, university, theory, discipline, astronomical object, academic journal, scientist, chemical compound, award, location, enzyme, country, person, event and O.\nSentence: The Voyager Program ( namely Voyager 2 ) only nominally confirmed the existence of similar belts around Uranus and Neptune .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Voyager","Program","(","namely","Voyager","2",")","only","nominally","confirmed","the","existence","of","similar","belts","around","Uranus","and","Neptune","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["protein","chemical_element","organization","university","theory","discipline","astronomical_object","academic_journal","scientist","chemical_compound","award","location","enzyme","country","person","event"]}
{"id":"77","dataset":"crossner_science","split":"dev","instance":{"id":"77","prompt_labels":"In(O) the(O) second(O) half(O) of(O) the(O) 20th(O) century(O) ,(O) plate(B-theory) tectonics(I-theory) theory(I-theory) was(O) developed(O) by(O) several(O) contributors(O) including(O) Alfred(B-scientist) Wegener(I-scientist) ,(O) Maurice(B-scientist) Ewing(I-scientist) ,(O) Robert(B-scientist) S.(I-scientist) Dietz(I-scientist) ,(O) Harry(B-scientist) Hammond(I-scientist) Hess(I-scientist) ,(O) Hugo(B-scientist) Benioff(I-scientist) ,(O) Walter(B-scientist) C.(I-scientist) Pitman(I-scientist) ,(I-scientist) III(I-scientist) ,(O) Frederick(B-scientist) Vine(I-scientist) ,(O) Drummond(B-scientist) Matthews(I-scientist) ,(O) Keith(B-scientist) Runcorn(I-scientist) ,(O) Bryan(B-scientist) L.(I-scientist) Isacks(I-scientist) ,(O) Edward(B-scientist) Bullard(I-scientist) ,(O) Xavier(B-scientist) Le(I-scientist) Pichon(I-scientist) ,(O) Dan(B-scientist) McKenzie(I-scientist) ,(O) W.(B-scientist) Jason(I-scientist) Morgan(I-scientist) and(O) John(B-scientist) Tuzo(I-scientist) Wilson(I-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, protein, theory, location, scientist, person, university, discipline, country, academic journal, astronomical object, organization, chemical compound, enzyme, event, award and O.\nSentence: In the second half of the 20th century , plate tectonics theory was developed by several contributors including Alfred Wegener , Maurice Ewing , Robert S. Dietz , Harry Hammond Hess , Hugo Benioff , Walter C. Pitman , III , Frederick Vine , Drummond Matthews , Keith Runcorn , Bryan L. Isacks , Edward Bullard , Xavier Le Pichon , Dan McKenzie , W. Jason Morgan and John Tuzo Wilson .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","second","half","of","the","20th","century",",","plate","tectonics","theory","was","developed","by","several","contributors","including","Alfred","Wegener",",","Maurice","Ewing",",","Robert","S.","Dietz",",","Harry","Hammond","Hess",",","Hugo","Benioff",",","Walter","C.","Pitman",",","III",",","Frederick","Vine",",","Drummond","Matthews",",","Keith","Runcorn",",","Bryan","L.","Isacks",",","Edward","Bullard",",","Xavier","Le","Pichon",",","Dan","McKenzie",",","W.","Jason","Morgan","and","John","Tuzo","Wilson","."],"labels":["O","O","O","O","O","O","O","O","O","B-theory","I-theory","I-theory","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","protein","theory","location","scientist","person","university","discipline","country","academic_journal","astronomical_object","organization","chemical_compound","enzyme","event","award"]}
{"id":"79","dataset":"crossner_science","split":"dev","instance":{"id":"79","prompt_labels":"He(O) served(O) as(O) President(O) of(O) the(O) Ecological(B-organization) Society(I-organization) of(I-organization) America(I-organization) in(O) 1917(O) ,(O) the(O) Association(B-organization) of(I-organization) American(I-organization) Geographers(I-organization) in(O) 1923(O) and(O) President(O) of(O) the(O) Board(O) of(O) Directors(O) of(O) the(O) Society(B-organization) for(I-organization) Biodemography(I-organization) and(I-organization) Social(I-organization) Biology(I-organization) from(O) 1934(O) to(O) 1938(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, location, chemical element, country, organization, discipline, protein, event, person, scientist, academic journal, theory, chemical compound, astronomical object, award, enzyme and O.\nSentence: He served as President of the Ecological Society of America in 1917 , the Association of American Geographers in 1923 and President of the Board of Directors of the Society for Biodemography and Social Biology from 1934 to 1938 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","served","as","President","of","the","Ecological","Society","of","America","in","1917",",","the","Association","of","American","Geographers","in","1923","and","President","of","the","Board","of","Directors","of","the","Society","for","Biodemography","and","Social","Biology","from","1934","to","1938","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","location","chemical_element","country","organization","discipline","protein","event","person","scientist","academic_journal","theory","chemical_compound","astronomical_object","award","enzyme"]}
{"id":"90","dataset":"crossner_science","split":"dev","instance":{"id":"90","prompt_labels":"He(O) was(O) a(O) fellow(B-award) of(I-award) the(I-award) Society(I-award) of(I-award) Experimental(I-award) Test(I-award) Pilots(I-award) ((O) SETP(B-organization) )(O) and(O) the(O) American(B-organization) Astronautical(I-organization) Society(I-organization) ,(O) as(O) well(O) as(O) an(O) associate(O) fellow(B-award) of(I-award) the(I-award) American(I-award) Institute(I-award) of(I-award) Aeronautics(I-award) and(I-award) Astronautics(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, protein, theory, scientist, chemical element, person, event, location, organization, enzyme, university, astronomical object, country, award, chemical compound, discipline and O.\nSentence: He was a fellow of the Society of Experimental Test Pilots ( SETP ) and the American Astronautical Society , as well as an associate fellow of the American Institute of Aeronautics and Astronautics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","a","fellow","of","the","Society","of","Experimental","Test","Pilots","(","SETP",")","and","the","American","Astronautical","Society",",","as","well","as","an","associate","fellow","of","the","American","Institute","of","Aeronautics","and","Astronautics","."],"labels":["O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","B-organization","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","protein","theory","scientist","chemical_element","person","event","location","organization","enzyme","university","astronomical_object","country","award","chemical_compound","discipline"]}
{"id":"104","dataset":"crossner_science","split":"dev","instance":{"id":"104","prompt_labels":"The(O) discovery(O) of(O) several(O) other(O) trans-Neptunian(B-astronomical object) object(O) s(O) ,(O) such(O) as(O) 50000(B-astronomical object) Quaoar(I-astronomical object) and(O) 90377(B-astronomical object) Sedna(I-astronomical object) ,(O) continued(O) to(O) erode(O) arguments(O) that(O) Pluto(B-astronomical object) was(O) exceptional(O) from(O) the(O) rest(O) of(O) the(O) trans-Neptunian(B-astronomical object) population(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, event, award, astronomical object, location, country, theory, organization, academic journal, university, chemical compound, enzyme, chemical element, scientist, person, protein and O.\nSentence: The discovery of several other trans-Neptunian object s , such as 50000 Quaoar and 90377 Sedna , continued to erode arguments that Pluto was exceptional from the rest of the trans-Neptunian population .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","discovery","of","several","other","trans-Neptunian","object","s",",","such","as","50000","Quaoar","and","90377","Sedna",",","continued","to","erode","arguments","that","Pluto","was","exceptional","from","the","rest","of","the","trans-Neptunian","population","."],"labels":["O","O","O","O","O","B-astronomical object","O","O","O","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","B-astronomical object","O","O"],"target_index":null,"target_label":null},"label_list":["discipline","event","award","astronomical_object","location","country","theory","organization","academic_journal","university","chemical_compound","enzyme","chemical_element","scientist","person","protein"]}
{"id":"108","dataset":"crossner_science","split":"dev","instance":{"id":"108","prompt_labels":"The(O) pyrimidines(B-chemical compound) ,(O) thymine(B-chemical compound) ,(O) cytosine(B-chemical compound) and(O) uracil(B-chemical compound) ,(O) form(O) the(O) complementary(O) bases(O) to(O) the(O) purine(B-chemical compound) bases(O) in(O) DNA(O) and(O) RNA(O) ,(O) and(O) are(O) also(O) components(O) of(O) Cytidine(B-chemical compound) triphosphate(I-chemical compound) ,(O) Uridine(B-chemical compound) monophosphate(I-chemical compound) ,(O) Uridine(B-chemical compound) diphosphate(I-chemical compound) and(O) Uridine(B-chemical compound) triphosphate(I-chemical compound) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, university, chemical element, location, academic journal, organization, award, country, discipline, event, scientist, person, astronomical object, protein, chemical compound, enzyme and O.\nSentence: The pyrimidines , thymine , cytosine and uracil , form the complementary bases to the purine bases in DNA and RNA , and are also components of Cytidine triphosphate , Uridine monophosphate , Uridine diphosphate and Uridine triphosphate .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","pyrimidines",",","thymine",",","cytosine","and","uracil",",","form","the","complementary","bases","to","the","purine","bases","in","DNA","and","RNA",",","and","are","also","components","of","Cytidine","triphosphate",",","Uridine","monophosphate",",","Uridine","diphosphate","and","Uridine","triphosphate","."],"labels":["O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","O","O","O","O","O","O","B-chemical compound","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","O"],"target_index":null,"target_label":null},"label_list":["theory","university","chemical_element","location","academic_journal","organization","award","country","discipline","event","scientist","person","astronomical_object","protein","chemical_compound","enzyme"]}
{"id":"112","dataset":"crossner_science","split":"dev","instance":{"id":"112","prompt_labels":"It(O) was(O) discovered(O) on(O) 24(O) September(O) 1960(O) ,(O) by(O) astronomers(O) Cornelis(B-scientist) Johannes(I-scientist) van(I-scientist) Houten(I-scientist) ,(O) Ingrid(B-scientist) van(I-scientist) Houten-Groeneveld(I-scientist) and(O) Tom(B-scientist) Gehrels(I-scientist) at(O) Palomar(B-location) Observatory(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, country, person, theory, location, award, organization, scientist, academic journal, event, chemical compound, discipline, enzyme, protein, astronomical object, university and O.\nSentence: It was discovered on 24 September 1960 , by astronomers Cornelis Johannes van Houten , Ingrid van Houten-Groeneveld and Tom Gehrels at Palomar Observatory .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","discovered","on","24","September","1960",",","by","astronomers","Cornelis","Johannes","van","Houten",",","Ingrid","van","Houten-Groeneveld","and","Tom","Gehrels","at","Palomar","Observatory","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","country","person","theory","location","award","organization","scientist","academic_journal","event","chemical_compound","discipline","enzyme","protein","astronomical_object","university"]}
{"id":"121","dataset":"crossner_science","split":"dev","instance":{"id":"121","prompt_labels":"This(O) subcategory(O) includes(O) Pluto(B-astronomical object) ,(O) Haumea(B-astronomical object) ,(O) Makemake(B-astronomical object) and(O) Eris(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, country, event, chemical element, protein, university, discipline, theory, organization, award, enzyme, astronomical object, location, academic journal, scientist, person and O.\nSentence: This subcategory includes Pluto , Haumea , Makemake and Eris .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","subcategory","includes","Pluto",",","Haumea",",","Makemake","and","Eris","."],"labels":["O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","country","event","chemical_element","protein","university","discipline","theory","organization","award","enzyme","astronomical_object","location","academic_journal","scientist","person"]}
{"id":"128","dataset":"crossner_science","split":"dev","instance":{"id":"128","prompt_labels":"This(O) range(O) ,(O) as(O) well(O) as(O) the(O) relative(O) speeds(O) between(O) the(O) planets(O) ,(O) led(O) Kepler(B-scientist) to(O) conclude(O) that(O) the(O) Solar(O) System(O) was(O) composed(O) of(O) two(O) basses(O) ((O) Saturn(B-astronomical object) and(O) Jupiter(B-astronomical object) )(O) ,(O) a(O) tenor(O) ((O) Mars(B-astronomical object) )(O) ,(O) two(O) altos(O) ((O) Venus(B-astronomical object) and(O) Earth(B-astronomical object) )(O) ,(O) and(O) a(O) soprano(O) ((O) Mercury(B-astronomical object) )(O) ,(O) which(O) had(O) sung(O) in(O) perfect(O) concord(O) ,(O) at(O) the(O) beginning(O) of(O) time(O) ,(O) and(O) could(O) potentially(O) arrange(O) themselves(O) to(O) do(O) so(O) again(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, chemical compound, award, event, country, scientist, organization, theory, chemical element, astronomical object, location, university, discipline, protein, enzyme, academic journal and O.\nSentence: This range , as well as the relative speeds between the planets , led Kepler to conclude that the Solar System was composed of two basses ( Saturn and Jupiter ) , a tenor ( Mars ) , two altos ( Venus and Earth ) , and a soprano ( Mercury ) , which had sung in perfect concord , at the beginning of time , and could potentially arrange themselves to do so again .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","range",",","as","well","as","the","relative","speeds","between","the","planets",",","led","Kepler","to","conclude","that","the","Solar","System","was","composed","of","two","basses","(","Saturn","and","Jupiter",")",",","a","tenor","(","Mars",")",",","two","altos","(","Venus","and","Earth",")",",","and","a","soprano","(","Mercury",")",",","which","had","sung","in","perfect","concord",",","at","the","beginning","of","time",",","and","could","potentially","arrange","themselves","to","do","so","again","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","B-astronomical object","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","chemical_compound","award","event","country","scientist","organization","theory","chemical_element","astronomical_object","location","university","discipline","protein","enzyme","academic_journal"]}
{"id":"133","dataset":"crossner_science","split":"dev","instance":{"id":"133","prompt_labels":"The(O) term(O) redox(O) state(O) is(O) often(O) used(O) to(O) describe(O) the(O) balance(O) of(O) Glutathione(B-chemical compound) ,(O) NADsup(B-chemical compound) +(I-chemical compound) /(I-chemical compound) sup(I-chemical compound) /(I-chemical compound) NADH(I-chemical compound) and(O) Nicotinamide(B-chemical compound) adenine(I-chemical compound) dinucleotide(I-chemical compound) phosphate(I-chemical compound) in(O) a(O) biological(O) system(O) such(O) as(O) a(O) cell(O) or(O) organ(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, country, person, award, organization, discipline, astronomical object, chemical element, theory, location, university, scientist, enzyme, event, academic journal, protein and O.\nSentence: The term redox state is often used to describe the balance of Glutathione , NADsup + / sup / NADH and Nicotinamide adenine dinucleotide phosphate in a biological system such as a cell or organ .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","term","redox","state","is","often","used","to","describe","the","balance","of","Glutathione",",","NADsup","+","/","sup","/","NADH","and","Nicotinamide","adenine","dinucleotide","phosphate","in","a","biological","system","such","as","a","cell","or","organ","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","O","B-chemical compound","I-chemical compound","I-chemical compound","I-chemical compound","I-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","I-chemical compound","I-chemical compound","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","country","person","award","organization","discipline","astronomical_object","chemical_element","theory","location","university","scientist","enzyme","event","academic_journal","protein"]}
{"id":"136","dataset":"crossner_science","split":"dev","instance":{"id":"136","prompt_labels":"Gas(O) giants(O) with(O) a(O) large(O) radius(O) and(O) very(O) low(O) density(O) are(O) sometimes(O) called(O) puffy(O) planets(O) COROT-1b(B-astronomical object) ,(O) TrES-4(B-astronomical object) ,(O) WASP-12b(B-astronomical object) ,(O) WASP-17b(B-astronomical object) ,(O) and(O) Kepler-7b(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, award, location, university, discipline, organization, country, theory, event, protein, astronomical object, scientist, person, chemical element, chemical compound, academic journal and O.\nSentence: Gas giants with a large radius and very low density are sometimes called puffy planets COROT-1b , TrES-4 , WASP-12b , WASP-17b , and Kepler-7b .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Gas","giants","with","a","large","radius","and","very","low","density","are","sometimes","called","puffy","planets","COROT-1b",",","TrES-4",",","WASP-12b",",","WASP-17b",",","and","Kepler-7b","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["enzyme","award","location","university","discipline","organization","country","theory","event","protein","astronomical_object","scientist","person","chemical_element","chemical_compound","academic_journal"]}
{"id":"147","dataset":"crossner_science","split":"dev","instance":{"id":"147","prompt_labels":"In(O) fact(O) ,(O) it(O) is(O) the(O) third(O) dimmest(O) of(O) the(O) first(O) twenty-three(O) asteroids(O) discovered(O) ,(O) with(O) only(O) 13(B-astronomical object) Egeria(I-astronomical object) and(O) 17(B-astronomical object) Thetis(I-astronomical object) having(O) lower(O) mean(O) opposition(O) magnitude(O) s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, organization, protein, chemical element, location, country, astronomical object, university, award, discipline, person, chemical compound, academic journal, theory, event, scientist and O.\nSentence: In fact , it is the third dimmest of the first twenty-three asteroids discovered , with only 13 Egeria and 17 Thetis having lower mean opposition magnitude s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","fact",",","it","is","the","third","dimmest","of","the","first","twenty-three","asteroids","discovered",",","with","only","13","Egeria","and","17","Thetis","having","lower","mean","opposition","magnitude","s","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["enzyme","organization","protein","chemical_element","location","country","astronomical_object","university","award","discipline","person","chemical_compound","academic_journal","theory","event","scientist"]}
{"id":"151","dataset":"crossner_science","split":"dev","instance":{"id":"151","prompt_labels":"The(O) scientists(O) found(O) that(O) while(O) CRISPR(O) could(O) effectively(O) cleave(O) the(O) -globin(O) gene(O) ((O) HBB(B-protein) )(O) ,(O) the(O) efficiency(O) of(O) homologous(O) recombination(O) directed(O) repair(O) of(O) HBB(B-protein) was(O) highly(O) inefficient(O) and(O) did(O) not(O) do(O) so(O) in(O) a(O) majority(O) of(O) the(O) trials(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, protein, event, university, theory, discipline, chemical compound, location, country, chemical element, scientist, organization, enzyme, academic journal, person, astronomical object and O.\nSentence: The scientists found that while CRISPR could effectively cleave the -globin gene ( HBB ) , the efficiency of homologous recombination directed repair of HBB was highly inefficient and did not do so in a majority of the trials .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","scientists","found","that","while","CRISPR","could","effectively","cleave","the","-globin","gene","(","HBB",")",",","the","efficiency","of","homologous","recombination","directed","repair","of","HBB","was","highly","inefficient","and","did","not","do","so","in","a","majority","of","the","trials","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-protein","O","O","O","O","O","O","O","O","O","O","B-protein","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","protein","event","university","theory","discipline","chemical_compound","location","country","chemical_element","scientist","organization","enzyme","academic_journal","person","astronomical_object"]}
{"id":"155","dataset":"crossner_science","split":"dev","instance":{"id":"155","prompt_labels":"Known(O) for(O) his(O) research(O) on(O) Mitogen-activated(B-enzyme) protein(I-enzyme) kinase(I-enzyme) ((O) MAPK(B-enzyme) )(O) cascade(O) in(O) plants(O) ,(O) he(O) is(O) a(O) three-time(O) Alexander(B-award) von(I-award) Humboldt(I-award) Fellow(I-award) and(O) an(O) elected(O) fellow(B-award) of(I-award) the(I-award) National(I-award) Academy(I-award) of(I-award) Sciences(I-award) ,(O) India(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, scientist, enzyme, location, academic journal, astronomical object, protein, event, theory, chemical compound, discipline, country, university, award, chemical element, person and O.\nSentence: Known for his research on Mitogen-activated protein kinase ( MAPK ) cascade in plants , he is a three-time Alexander von Humboldt Fellow and an elected fellow of the National Academy of Sciences , India .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Known","for","his","research","on","Mitogen-activated","protein","kinase","(","MAPK",")","cascade","in","plants",",","he","is","a","three-time","Alexander","von","Humboldt","Fellow","and","an","elected","fellow","of","the","National","Academy","of","Sciences",",","India","."],"labels":["O","O","O","O","O","B-enzyme","I-enzyme","I-enzyme","O","B-enzyme","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["organization","scientist","enzyme","location","academic_journal","astronomical_object","protein","event","theory","chemical_compound","discipline","country","university","award","chemical_element","person"]}
{"id":"164","dataset":"crossner_science","split":"dev","instance":{"id":"164","prompt_labels":"Published(O) in(O) 1993(O) ,(O) it(O) won(O) the(O) 1994(O) Nebula(B-award) Award(I-award) for(I-award) Best(I-award) Novel(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, enzyme, scientist, discipline, award, university, country, location, astronomical object, protein, chemical compound, theory, academic journal, chemical element, event, person and O.\nSentence: Published in 1993 , it won the 1994 Nebula Award for Best Novel .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Published","in","1993",",","it","won","the","1994","Nebula","Award","for","Best","Novel","."],"labels":["O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["organization","enzyme","scientist","discipline","award","university","country","location","astronomical_object","protein","chemical_compound","theory","academic_journal","chemical_element","event","person"]}
{"id":"165","dataset":"crossner_science","split":"dev","instance":{"id":"165","prompt_labels":"After(O) school(O) ,(O) he(O) studied(O) physics(B-discipline) and(O) mathematics(B-discipline) at(O) the(O) University(B-university) of(I-university) Gttingen(I-university) and(O) University(B-university) of(I-university) Hamburg(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, scientist, event, academic journal, astronomical object, country, protein, enzyme, university, theory, person, organization, chemical compound, discipline, chemical element, location and O.\nSentence: After school , he studied physics and mathematics at the University of Gttingen and University of Hamburg .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","school",",","he","studied","physics","and","mathematics","at","the","University","of","Gttingen","and","University","of","Hamburg","."],"labels":["O","O","O","O","O","B-discipline","O","B-discipline","O","O","B-university","I-university","I-university","O","B-university","I-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["award","scientist","event","academic_journal","astronomical_object","country","protein","enzyme","university","theory","person","organization","chemical_compound","discipline","chemical_element","location"]}
{"id":"167","dataset":"crossner_science","split":"dev","instance":{"id":"167","prompt_labels":"She(O) also(O) played(O) at(O) 1986(O) ,(O) 1989(O) ,(O) 1991(O) ,(O) 1993(O) ,(O) 1995(O) AFC(B-event) Championship(I-event) ,(O) 1990(O) and(O) Football(O) at(O) the(O) 1994(O) Asian(B-event) Games(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, theory, discipline, event, award, protein, scientist, chemical compound, organization, university, location, person, astronomical object, enzyme, chemical element, country and O.\nSentence: She also played at 1986 , 1989 , 1991 , 1993 , 1995 AFC Championship , 1990 and Football at the 1994 Asian Games .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","also","played","at","1986",",","1989",",","1991",",","1993",",","1995","AFC","Championship",",","1990","and","Football","at","the","1994","Asian","Games","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","O","O","O","O","O","O","O","B-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","theory","discipline","event","award","protein","scientist","chemical_compound","organization","university","location","person","astronomical_object","enzyme","chemical_element","country"]}
{"id":"168","dataset":"crossner_science","split":"dev","instance":{"id":"168","prompt_labels":"A(O) physician(O) and(O) professor(O) of(O) public(O) health(O) ,(O) he(O) worked(O) first(O) in(O) social(B-discipline) medicine(I-discipline) at(O) the(O) University(B-university) of(I-university) Sassari(I-university) ((O) 1969-1974(O) )(O) and(O) then(O) in(O) occupational(O) health(O) at(O) the(O) Sapienza(B-university) University(I-university) of(I-university) Rome(I-university) ((O) 1975-1999(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, event, protein, country, scientist, discipline, university, person, chemical compound, location, enzyme, organization, astronomical object, award, chemical element, theory and O.\nSentence: A physician and professor of public health , he worked first in social medicine at the University of Sassari ( 1969-1974 ) and then in occupational health at the Sapienza University of Rome ( 1975-1999 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","physician","and","professor","of","public","health",",","he","worked","first","in","social","medicine","at","the","University","of","Sassari","(","1969-1974",")","and","then","in","occupational","health","at","the","Sapienza","University","of","Rome","(","1975-1999",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-discipline","I-discipline","O","O","B-university","I-university","I-university","O","O","O","O","O","O","O","O","O","O","B-university","I-university","I-university","I-university","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","event","protein","country","scientist","discipline","university","person","chemical_compound","location","enzyme","organization","astronomical_object","award","chemical_element","theory"]}
{"id":"172","dataset":"crossner_science","split":"dev","instance":{"id":"172","prompt_labels":"Instead(O) ,(O) for(O) the(O) UEFA(B-event) Euro(I-event) 2008(I-event) articles(O) ((O) UEFA(B-event) Euro(I-event) 2008(I-event) ,(O) UEFA(B-event) Euro(I-event) 2008(I-event) Group(O) A(O) ,(O) UEFA(B-event) Euro(I-event) 2008(I-event) Group(O) B(O) ,(O) UEFA(B-event) Euro(I-event) 2008(I-event) Group(O) C(O) ,(O) UEFA(B-event) Euro(I-event) 2008(I-event) Group(O) D(O) and(O) UEFA(B-event) Euro(I-event) 2008(I-event) knockout(O) stage(O) )(O) ,(O) the(O) old(O) partial(O) URL(O) string(O) ones(O) :(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, university, scientist, discipline, astronomical object, enzyme, award, academic journal, chemical element, event, chemical compound, theory, protein, country, organization, person and O.\nSentence: Instead , for the UEFA Euro 2008 articles ( UEFA Euro 2008 , UEFA Euro 2008 Group A , UEFA Euro 2008 Group B , UEFA Euro 2008 Group C , UEFA Euro 2008 Group D and UEFA Euro 2008 knockout stage ) , the old partial URL string ones :","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Instead",",","for","the","UEFA","Euro","2008","articles","(","UEFA","Euro","2008",",","UEFA","Euro","2008","Group","A",",","UEFA","Euro","2008","Group","B",",","UEFA","Euro","2008","Group","C",",","UEFA","Euro","2008","Group","D","and","UEFA","Euro","2008","knockout","stage",")",",","the","old","partial","URL","string","ones",":"],"labels":["O","O","O","O","B-event","I-event","I-event","O","O","B-event","I-event","I-event","O","B-event","I-event","I-event","O","O","O","B-event","I-event","I-event","O","O","O","B-event","I-event","I-event","O","O","O","B-event","I-event","I-event","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","university","scientist","discipline","astronomical_object","enzyme","award","academic_journal","chemical_element","event","chemical_compound","theory","protein","country","organization","person"]}
{"id":"182","dataset":"crossner_science","split":"dev","instance":{"id":"182","prompt_labels":"Lysostaphin(B-enzyme) can(O) lyse(O) Staphylococcus(O) ,(O) but(O) Micrococcus(O) bacteria(O) are(O) resistant(O) to(O) the(O) chemical(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, university, astronomical object, event, chemical compound, organization, chemical element, discipline, award, academic journal, scientist, enzyme, location, theory, protein and O.\nSentence: Lysostaphin can lyse Staphylococcus , but Micrococcus bacteria are resistant to the chemical .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Lysostaphin","can","lyse","Staphylococcus",",","but","Micrococcus","bacteria","are","resistant","to","the","chemical","."],"labels":["B-enzyme","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","country","university","astronomical_object","event","chemical_compound","organization","chemical_element","discipline","award","academic_journal","scientist","enzyme","location","theory","protein"]}
