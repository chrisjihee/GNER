{"id":"25","dataset":"mit-movie","split":"dev","instance":{"id":"25","prompt_labels":"what(O) was(O) the(O) first(O) movie(O) in(O) color(B-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, average ratings, actor, trailer, song, review, character, director, plot, rating, title, genre and O.\nSentence: what was the first movie in color","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","first","movie","in","color"],"labels":["O","O","O","O","O","O","B-plot"],"target_index":null,"target_label":null},"label_list":["year","average_ratings","actor","trailer","song","review","character","director","plot","rating","title","genre"]}
{"id":"35","dataset":"mit-movie","split":"dev","instance":{"id":"35","prompt_labels":"did(O) george(B-director) clooney(I-director) direct(O) any(O) comedy(B-genre) films(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, trailer, genre, song, rating, year, review, director, average ratings, character, title, actor and O.\nSentence: did george clooney direct any comedy films","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","george","clooney","direct","any","comedy","films"],"labels":["O","B-director","I-director","O","O","B-genre","O"],"target_index":null,"target_label":null},"label_list":["plot","trailer","genre","song","rating","year","review","director","average_ratings","character","title","actor"]}
{"id":"50","dataset":"mit-movie","split":"dev","instance":{"id":"50","prompt_labels":"find(O) me(O) the(O) movies(O) that(O) starred(O) anne(B-actor) hathaway(I-actor) and(O) julie(B-actor) andrews(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, character, average ratings, rating, director, genre, year, actor, title, plot, trailer and O.\nSentence: find me the movies that starred anne hathaway and julie andrews","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","the","movies","that","starred","anne","hathaway","and","julie","andrews"],"labels":["O","O","O","O","O","O","B-actor","I-actor","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["review","song","character","average_ratings","rating","director","genre","year","actor","title","plot","trailer"]}
{"id":"71","dataset":"mit-movie","split":"dev","instance":{"id":"71","prompt_labels":"favorite(O) quote(O) from(O) action(B-genre) movies(I-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, trailer, rating, year, genre, review, title, actor, average ratings, director, character, plot and O.\nSentence: favorite quote from action movies","prediction_output":null,"prediction_outputs":null,"group":null,"words":["favorite","quote","from","action","movies"],"labels":["O","O","O","B-genre","I-genre"],"target_index":null,"target_label":null},"label_list":["song","trailer","rating","year","genre","review","title","actor","average_ratings","director","character","plot"]}
{"id":"105","dataset":"mit-movie","split":"dev","instance":{"id":"105","prompt_labels":"show(O) me(O) a(O) comedy(B-genre) movie(O) with(O) eddie(B-actor) murphy(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, song, genre, character, director, average ratings, actor, title, review, rating, trailer, year and O.\nSentence: show me a comedy movie with eddie murphy","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","a","comedy","movie","with","eddie","murphy"],"labels":["O","O","O","B-genre","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["plot","song","genre","character","director","average_ratings","actor","title","review","rating","trailer","year"]}
{"id":"109","dataset":"mit-movie","split":"dev","instance":{"id":"109","prompt_labels":"show(O) me(O) movies(O) starring(O) michael(B-actor) j(I-actor) fox(I-actor) from(O) 1993(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, average ratings, plot, character, title, year, trailer, rating, actor, genre, director, song and O.\nSentence: show me movies starring michael j fox from 1993","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","movies","starring","michael","j","fox","from","1993"],"labels":["O","O","O","O","B-actor","I-actor","I-actor","O","B-year"],"target_index":null,"target_label":null},"label_list":["review","average_ratings","plot","character","title","year","trailer","rating","actor","genre","director","song"]}
{"id":"142","dataset":"mit-movie","split":"dev","instance":{"id":"142","prompt_labels":"what(O) techno(B-genre) thriller(O) gets(O) a(O) low(B-average ratings) rating(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, actor, director, review, song, genre, rating, title, plot, average ratings, trailer, year and O.\nSentence: what techno thriller gets a low rating","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","techno","thriller","gets","a","low","rating"],"labels":["O","B-genre","O","O","O","B-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["character","actor","director","review","song","genre","rating","title","plot","average_ratings","trailer","year"]}
{"id":"152","dataset":"mit-movie","split":"dev","instance":{"id":"152","prompt_labels":"show(O) me(O) terry(B-director) gilliam(I-director) movies(O) starring(O) jeff(B-actor) bridges(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, director, genre, review, rating, actor, character, plot, song, average ratings, year, trailer and O.\nSentence: show me terry gilliam movies starring jeff bridges","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","terry","gilliam","movies","starring","jeff","bridges"],"labels":["O","O","B-director","I-director","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["title","director","genre","review","rating","actor","character","plot","song","average_ratings","year","trailer"]}
{"id":"154","dataset":"mit-movie","split":"dev","instance":{"id":"154","prompt_labels":"when(O) did(O) interview(B-title) with(I-title) a(I-title) vampire(I-title) come(O) out(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, plot, director, character, rating, title, average ratings, year, review, trailer, actor, song and O.\nSentence: when did interview with a vampire come out","prediction_output":null,"prediction_outputs":null,"group":null,"words":["when","did","interview","with","a","vampire","come","out"],"labels":["O","O","B-title","I-title","I-title","I-title","O","O"],"target_index":null,"target_label":null},"label_list":["genre","plot","director","character","rating","title","average_ratings","year","review","trailer","actor","song"]}
{"id":"177","dataset":"mit-movie","split":"dev","instance":{"id":"177","prompt_labels":"what(O) are(O) the(O) best(B-review) werewolf(B-plot) movies(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, title, year, plot, rating, song, genre, trailer, review, director, actor, character and O.\nSentence: what are the best werewolf movies","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","the","best","werewolf","movies"],"labels":["O","O","O","B-review","B-plot","O"],"target_index":null,"target_label":null},"label_list":["average_ratings","title","year","plot","rating","song","genre","trailer","review","director","actor","character"]}
{"id":"57","dataset":"mit-restaurant","split":"dev","instance":{"id":"57","prompt_labels":"are(O) there(O) any(O) maid(B-Restaurant Name) cafe(I-Restaurant Name) in(B-Location) town(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Amenity, Dish, Price, Restaurant Name, Rating, Location, Hours and O.\nSentence: are there any maid cafe in town","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","maid","cafe","in","town"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["cuisine","amenity","dish","price","restaurant_name","rating","location","hours"]}
{"id":"83","dataset":"mit-restaurant","split":"dev","instance":{"id":"83","prompt_labels":"are(O) there(O) any(O) restaurants(O) that(O) will(O) let(O) me(O) take(B-Amenity) my(I-Amenity) dog(I-Amenity) in(I-Amenity) with(I-Amenity) me(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Price, Rating, Amenity, Cuisine, Dish, Hours, Restaurant Name and O.\nSentence: are there any restaurants that will let me take my dog in with me","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","restaurants","that","will","let","me","take","my","dog","in","with","me"],"labels":["O","O","O","O","O","O","O","O","B-Amenity","I-Amenity","I-Amenity","I-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["location","price","rating","amenity","cuisine","dish","hours","restaurant_name"]}
{"id":"103","dataset":"mit-restaurant","split":"dev","instance":{"id":"103","prompt_labels":"at(O) which(O) french(B-Cuisine) restaurant(O) can(O) i(O) dine(B-Amenity) outdoors(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Cuisine, Amenity, Location, Dish, Price, Rating and O.\nSentence: at which french restaurant can i dine outdoors","prediction_output":null,"prediction_outputs":null,"group":null,"words":["at","which","french","restaurant","can","i","dine","outdoors"],"labels":["O","O","B-Cuisine","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["restaurant_name","hours","cuisine","amenity","location","dish","price","rating"]}
{"id":"109","dataset":"mit-restaurant","split":"dev","instance":{"id":"109","prompt_labels":"burgers(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Location, Amenity, Price, Rating, Cuisine, Dish, Restaurant Name and O.\nSentence: burgers","prediction_output":null,"prediction_outputs":null,"group":null,"words":["burgers"],"labels":["B-Dish"],"target_index":null,"target_label":null},"label_list":["hours","location","amenity","price","rating","cuisine","dish","restaurant_name"]}
{"id":"126","dataset":"mit-restaurant","split":"dev","instance":{"id":"126","prompt_labels":"can(O) i(O) get(O) raw(B-Cuisine) vegan(I-Cuisine) food(O) in(O) honolulu(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Restaurant Name, Rating, Location, Cuisine, Price, Amenity, Hours and O.\nSentence: can i get raw vegan food in honolulu","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","i","get","raw","vegan","food","in","honolulu"],"labels":["O","O","O","B-Cuisine","I-Cuisine","O","O","B-Location"],"target_index":null,"target_label":null},"label_list":["dish","restaurant_name","rating","location","cuisine","price","amenity","hours"]}
{"id":"138","dataset":"mit-restaurant","split":"dev","instance":{"id":"138","prompt_labels":"can(O) you(O) find(O) a(O) pizza(B-Cuisine) place(O) with(O) a(O) buffet(B-Amenity) within(B-Location) 15(I-Location) miles(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Dish, Hours, Amenity, Location, Price, Restaurant Name, Cuisine and O.\nSentence: can you find a pizza place with a buffet within 15 miles","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","a","pizza","place","with","a","buffet","within","15","miles"],"labels":["O","O","O","O","B-Cuisine","O","O","O","B-Amenity","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["rating","dish","hours","amenity","location","price","restaurant_name","cuisine"]}
{"id":"150","dataset":"mit-restaurant","split":"dev","instance":{"id":"150","prompt_labels":"can(O) you(O) find(O) east(B-Restaurant Name) dedham(B-Location) pizzeria(B-Cuisine) that(O) have(O) a(O) dine(O) at(O) bar(O) location(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Cuisine, Hours, Amenity, Rating, Price, Restaurant Name, Location and O.\nSentence: can you find east dedham pizzeria that have a dine at bar location","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","east","dedham","pizzeria","that","have","a","dine","at","bar","location"],"labels":["O","O","O","B-Restaurant Name","B-Location","B-Cuisine","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["dish","cuisine","hours","amenity","rating","price","restaurant_name","location"]}
{"id":"161","dataset":"mit-restaurant","split":"dev","instance":{"id":"161","prompt_labels":"can(O) you(O) find(O) me(O) a(O) pizzeria(B-Cuisine) that(O) delivers(B-Amenity) after(B-Hours) midnight(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Restaurant Name, Hours, Dish, Location, Price, Cuisine and O.\nSentence: can you find me a pizzeria that delivers after midnight","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","me","a","pizzeria","that","delivers","after","midnight"],"labels":["O","O","O","O","O","B-Cuisine","O","B-Amenity","B-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["amenity","rating","restaurant_name","hours","dish","location","price","cuisine"]}
{"id":"162","dataset":"mit-restaurant","split":"dev","instance":{"id":"162","prompt_labels":"can(O) you(O) find(O) me(O) a(O) place(O) nearby(B-Location) thats(O) open(O) after(B-Hours) 12(I-Hours) pm(I-Hours) with(O) bean(B-Dish) dishes(I-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Hours, Restaurant Name, Rating, Location, Cuisine, Price, Amenity and O.\nSentence: can you find me a place nearby thats open after 12 pm with bean dishes","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","me","a","place","nearby","thats","open","after","12","pm","with","bean","dishes"],"labels":["O","O","O","O","O","O","B-Location","O","O","B-Hours","I-Hours","I-Hours","O","B-Dish","I-Dish"],"target_index":null,"target_label":null},"label_list":["dish","hours","restaurant_name","rating","location","cuisine","price","amenity"]}
{"id":"189","dataset":"mit-restaurant","split":"dev","instance":{"id":"189","prompt_labels":"can(O) you(O) locate(O) a(O) 4(B-Rating) star(I-Rating) or(I-Rating) higher(I-Rating) restaurant(O) that(O) serves(O) italian(B-Cuisine) food(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Price, Amenity, Dish, Cuisine, Rating, Location and O.\nSentence: can you locate a 4 star or higher restaurant that serves italian food","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","locate","a","4","star","or","higher","restaurant","that","serves","italian","food"],"labels":["O","O","O","O","B-Rating","I-Rating","I-Rating","I-Rating","O","O","O","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["hours","restaurant_name","price","amenity","dish","cuisine","rating","location"]}
{"id":"1","dataset":"crossner_ai","split":"dev","instance":{"id":"1","prompt_labels":"From(O) this(O) perspective(O) ,(O) SVM(B-algorithm) is(O) closely(O) related(O) to(O) other(O) fundamental(O) classification(O) algorithms(O) such(O) as(O) regularized(B-algorithm) least-squares(I-algorithm) logistic(B-algorithm) regression(I-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, researcher, metric, programming language, person, conference, product, algorithm, organization, field, location, country, university and O.\nSentence: From this perspective , SVM is closely related to other fundamental classification algorithms such as regularized least-squares logistic regression .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["From","this","perspective",",","SVM","is","closely","related","to","other","fundamental","classification","algorithms","such","as","regularized","least-squares","logistic","regression","."],"labels":["O","O","O","O","B-algorithm","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","B-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["task","researcher","metric","programming_language","person","conference","product","algorithm","organization","field","location","country","university"]}
{"id":"22","dataset":"crossner_ai","split":"dev","instance":{"id":"22","prompt_labels":"With(O) this(O) company(O) he(O) was(O) developing(O) data-mining(B-field) and(O) database(B-field) technology(O) ,(O) more(O) specific(O) high-level(O) ontologies(O) for(O) intelligence(O) and(O) automated(B-task) natural(I-task) language(B-task) understanding(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, conference, organization, university, field, researcher, person, task, product, location, algorithm, metric, country and O.\nSentence: With this company he was developing data-mining and database technology , more specific high-level ontologies for intelligence and automated natural language understanding .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["With","this","company","he","was","developing","data-mining","and","database","technology",",","more","specific","high-level","ontologies","for","intelligence","and","automated","natural","language","understanding","."],"labels":["O","O","O","O","O","O","B-field","O","B-field","O","O","O","O","O","O","O","O","O","B-task","I-task","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["programming_language","conference","organization","university","field","researcher","person","task","product","location","algorithm","metric","country"]}
{"id":"52","dataset":"crossner_ai","split":"dev","instance":{"id":"52","prompt_labels":"In(O) computer(B-field) vision(I-field) and(O) image(B-field) processing(I-field) ,(O) the(O) notion(O) of(O) scale(O) space(O) representation(O) and(O) Gaussian(O) derivative(O) operators(O) is(O) as(O) a(O) canonical(O) multi-scale(O) representation(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, researcher, organization, country, programming language, product, metric, field, task, university, conference, location, algorithm and O.\nSentence: In computer vision and image processing , the notion of scale space representation and Gaussian derivative operators is as a canonical multi-scale representation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","computer","vision","and","image","processing",",","the","notion","of","scale","space","representation","and","Gaussian","derivative","operators","is","as","a","canonical","multi-scale","representation","."],"labels":["O","B-field","I-field","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","researcher","organization","country","programming_language","product","metric","field","task","university","conference","location","algorithm"]}
{"id":"63","dataset":"crossner_ai","split":"dev","instance":{"id":"63","prompt_labels":"In(O) many(O) applications(O) the(O) units(O) of(O) these(O) networks(O) apply(O) a(O) sigmoid(B-algorithm) function(I-algorithm) as(O) an(O) activation(O) function(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, metric, country, researcher, product, organization, field, university, conference, programming language, task, algorithm and O.\nSentence: In many applications the units of these networks apply a sigmoid function as an activation function .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","many","applications","the","units","of","these","networks","apply","a","sigmoid","function","as","an","activation","function","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","person","metric","country","researcher","product","organization","field","university","conference","programming_language","task","algorithm"]}
{"id":"64","dataset":"crossner_ai","split":"dev","instance":{"id":"64","prompt_labels":"In(O) 2001(O) ,(O) Mehler(B-researcher) was(O) elected(O) a(O) foreign(O) honorary(O) member(O) of(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) ,(O) and(O) in(O) 2003(O) ,(O) he(O) was(O) elected(O) a(O) Fellow(O) of(O) the(O) American(B-organization) Association(I-organization) for(I-organization) the(I-organization) Advancement(I-organization) of(I-organization) Science(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, product, task, conference, university, algorithm, location, country, organization, field, metric, researcher, person and O.\nSentence: In 2001 , Mehler was elected a foreign honorary member of the American Academy of Arts and Sciences , and in 2003 , he was elected a Fellow of the American Association for the Advancement of Science .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2001",",","Mehler","was","elected","a","foreign","honorary","member","of","the","American","Academy","of","Arts","and","Sciences",",","and","in","2003",",","he","was","elected","a","Fellow","of","the","American","Association","for","the","Advancement","of","Science","."],"labels":["O","O","O","B-researcher","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["programming_language","product","task","conference","university","algorithm","location","country","organization","field","metric","researcher","person"]}
{"id":"69","dataset":"crossner_ai","split":"dev","instance":{"id":"69","prompt_labels":"The(O) condensation(B-algorithm) algorithm(I-algorithm) has(O) also(O) been(O) used(O) for(O) facial(B-product) recognition(I-product) system(I-product) in(O) a(O) video(O) sequence(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, product, algorithm, task, university, programming language, country, organization, location, metric, conference, researcher, person and O.\nSentence: The condensation algorithm has also been used for facial recognition system in a video sequence .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","condensation","algorithm","has","also","been","used","for","facial","recognition","system","in","a","video","sequence","."],"labels":["O","B-algorithm","I-algorithm","O","O","O","O","O","B-product","I-product","I-product","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","product","algorithm","task","university","programming_language","country","organization","location","metric","conference","researcher","person"]}
{"id":"79","dataset":"crossner_ai","split":"dev","instance":{"id":"79","prompt_labels":"Several(O) metrics(O) use(O) WordNet(B-product) ,(O) a(O) manually(O) constructed(O) lexical(O) database(O) of(O) English(O) words(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, organization, conference, researcher, country, metric, task, product, field, location, algorithm, person, university and O.\nSentence: Several metrics use WordNet , a manually constructed lexical database of English words .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Several","metrics","use","WordNet",",","a","manually","constructed","lexical","database","of","English","words","."],"labels":["O","O","O","B-product","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","organization","conference","researcher","country","metric","task","product","field","location","algorithm","person","university"]}
{"id":"87","dataset":"crossner_ai","split":"dev","instance":{"id":"87","prompt_labels":"This(O) overall(O) framework(O) has(O) been(O) applied(O) to(O) a(O) large(O) variety(O) of(O) problems(O) in(O) computer(B-field) vision(I-field) ,(O) including(O) feature(B-task) detection(I-task) ,(O) feature(B-task) classification(I-task) ,(O) image(B-task) segmentation(I-task) ,(O) image(B-task) matching(I-task) ,(O) motion(B-task) estimation(I-task) ,(O) computation(B-task) of(I-task) shape(I-task) cues(I-task) and(O) object(B-task) recognition(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, conference, task, country, field, organization, person, researcher, product, algorithm, metric, programming language, location and O.\nSentence: This overall framework has been applied to a large variety of problems in computer vision , including feature detection , feature classification , image segmentation , image matching , motion estimation , computation of shape cues and object recognition .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","overall","framework","has","been","applied","to","a","large","variety","of","problems","in","computer","vision",",","including","feature","detection",",","feature","classification",",","image","segmentation",",","image","matching",",","motion","estimation",",","computation","of","shape","cues","and","object","recognition","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","I-task","I-task","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["university","conference","task","country","field","organization","person","researcher","product","algorithm","metric","programming_language","location"]}
{"id":"99","dataset":"crossner_ai","split":"dev","instance":{"id":"99","prompt_labels":"Zurada(B-researcher) has(O) served(O) the(O) engineering(O) profession(O) as(O) a(O) long-time(O) volunteer(O) of(O) IEEE(B-organization) :(O) as(O) 2014(O) IEEE(O) Vice-President-Technical(O) Activities(O) ((O) TAB(O) Chair(O) )(O) ,(O) as(O) President(O) of(O) IEEE(B-conference) Computational(I-conference) Intelligence(I-conference) Society(I-conference) in(O) 2004-05(O) and(O) the(O) ADCOM(B-conference) member(O) in(O) 2009-14(O) ,(O) 2016-18(O) and(O) earlier(O) years(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, programming language, product, country, researcher, algorithm, task, university, location, metric, field, organization, person and O.\nSentence: Zurada has served the engineering profession as a long-time volunteer of IEEE : as 2014 IEEE Vice-President-Technical Activities ( TAB Chair ) , as President of IEEE Computational Intelligence Society in 2004-05 and the ADCOM member in 2009-14 , 2016-18 and earlier years .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Zurada","has","served","the","engineering","profession","as","a","long-time","volunteer","of","IEEE",":","as","2014","IEEE","Vice-President-Technical","Activities","(","TAB","Chair",")",",","as","President","of","IEEE","Computational","Intelligence","Society","in","2004-05","and","the","ADCOM","member","in","2009-14",",","2016-18","and","earlier","years","."],"labels":["B-researcher","O","O","O","O","O","O","O","O","O","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","O","O","O","O","B-conference","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["conference","programming_language","product","country","researcher","algorithm","task","university","location","metric","field","organization","person"]}
{"id":"173","dataset":"crossner_ai","split":"dev","instance":{"id":"173","prompt_labels":"For(O) example(O) ,(O) the(O) best(O) system(O) entering(O) MUC-7(B-conference) scored(O) 93.39(O) %(O) of(O) F-measure(B-metric) while(O) human(O) annotators(O) scored(O) 97.6(O) %(O) and(O) 96.95(O) %(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, university, conference, location, product, programming language, researcher, country, metric, task, field, organization, algorithm and O.\nSentence: For example , the best system entering MUC-7 scored 93.39 % of F-measure while human annotators scored 97.6 % and 96.95 % .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","example",",","the","best","system","entering","MUC-7","scored","93.39","%","of","F-measure","while","human","annotators","scored","97.6","%","and","96.95","%","."],"labels":["O","O","O","O","O","O","O","B-conference","O","O","O","O","B-metric","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","university","conference","location","product","programming_language","researcher","country","metric","task","field","organization","algorithm"]}
{"id":"0","dataset":"crossner_literature","split":"dev","instance":{"id":"0","prompt_labels":"In(O) 1982(O) ,(O) she(O) wrote(O) the(O) novel(B-literary genre) The(B-book) Color(I-book) Purple(I-book) ,(O) for(O) which(O) she(O) won(O) the(O) National(B-award) Book(I-award) Award(I-award) for(I-award) hardcover(I-award) fiction(I-award) ,(O) and(O) the(O) Pulitzer(B-award) Prize(I-award) for(I-award) Fiction(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, location, writer, organization, country, person, literary genre, poem, event, magazine, award and O.\nSentence: In 1982 , she wrote the novel The Color Purple , for which she won the National Book Award for hardcover fiction , and the Pulitzer Prize for Fiction .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1982",",","she","wrote","the","novel","The","Color","Purple",",","for","which","she","won","the","National","Book","Award","for","hardcover","fiction",",","and","the","Pulitzer","Prize","for","Fiction","."],"labels":["O","O","O","O","O","O","B-literary genre","B-book","I-book","I-book","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["book","location","writer","organization","country","person","literary_genre","poem","event","magazine","award"]}
{"id":"3","dataset":"crossner_literature","split":"dev","instance":{"id":"3","prompt_labels":"Atwood(B-writer) has(O) strong(O) views(O) on(O) environmental(O) issues(O) ,(O) and(O) she(O) and(O) Graeme(B-writer) Gibson(I-writer) were(O) the(O) joint(O) honorary(O) presidents(O) of(O) the(O) Rare(B-organization) Bird(I-organization) Club(I-organization) within(O) BirdLife(B-organization) International(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, poem, book, award, writer, organization, country, person, event, location, literary genre and O.\nSentence: Atwood has strong views on environmental issues , and she and Graeme Gibson were the joint honorary presidents of the Rare Bird Club within BirdLife International .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Atwood","has","strong","views","on","environmental","issues",",","and","she","and","Graeme","Gibson","were","the","joint","honorary","presidents","of","the","Rare","Bird","Club","within","BirdLife","International","."],"labels":["B-writer","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["magazine","poem","book","award","writer","organization","country","person","event","location","literary_genre"]}
{"id":"44","dataset":"crossner_literature","split":"dev","instance":{"id":"44","prompt_labels":"Hesser(B-writer) lives(O) in(O) Brooklyn(B-location) Heights(I-location) with(O) her(O) husband(O) ,(O) Tad(B-writer) Friend(I-writer) ,(O) a(O) staff(O) writer(O) for(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) ,(O) and(O) their(O) two(O) children(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, award, writer, magazine, literary genre, organization, poem, person, book, event and O.\nSentence: Hesser lives in Brooklyn Heights with her husband , Tad Friend , a staff writer for The New Yorker , and their two children .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Hesser","lives","in","Brooklyn","Heights","with","her","husband",",","Tad","Friend",",","a","staff","writer","for","The","New","Yorker",",","and","their","two","children","."],"labels":["B-writer","O","O","B-location","I-location","O","O","O","O","B-writer","I-writer","O","O","O","O","O","B-magazine","I-magazine","I-magazine","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","location","award","writer","magazine","literary_genre","organization","poem","person","book","event"]}
{"id":"92","dataset":"crossner_literature","split":"dev","instance":{"id":"92","prompt_labels":"Before(O) writing(O) Dracula(B-book) ,(O) Stoker(B-writer) met(O) Ármin(B-writer) Vámbéry(I-writer) ,(O) a(O) Hungarian-Jewish(O) writer(O) and(O) traveller(O) ((O) born(O) in(O) Szent-György(B-location) ,(O) Kingdom(B-country) of(I-country) Hungary(I-country) now(O) Svätý(B-location) Jur(I-location) ,(O) Slovakia(B-country) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, poem, person, book, country, magazine, award, location, writer, literary genre and O.\nSentence: Before writing Dracula , Stoker met Ármin Vámbéry , a Hungarian-Jewish writer and traveller ( born in Szent-György , Kingdom of Hungary now Svätý Jur , Slovakia ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Before","writing","Dracula",",","Stoker","met","Ármin","Vámbéry",",","a","Hungarian-Jewish","writer","and","traveller","(","born","in","Szent-György",",","Kingdom","of","Hungary","now","Svätý","Jur",",","Slovakia",")","."],"labels":["O","O","B-book","O","B-writer","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","B-location","O","B-country","I-country","I-country","O","B-location","I-location","O","B-country","O","O"],"target_index":null,"target_label":null},"label_list":["organization","event","poem","person","book","country","magazine","award","location","writer","literary_genre"]}
{"id":"106","dataset":"crossner_literature","split":"dev","instance":{"id":"106","prompt_labels":"91(O) A(O) subsequent(O) journey(O) through(O) the(O) British(B-location) East(I-location) Africa(I-location) colonies(I-location) and(O) the(O) Belgian(B-country) Congo(I-country) formed(O) the(O) basis(O) of(O) two(O) books(O) ;(O) the(O) travelogue(O) Remote(B-book) People(I-book) ((O) 1931(O) )(O) and(O) the(O) comic(B-literary genre) novel(I-literary genre) Black(B-book) Mischief(I-book) ((O) 1932(O) )(O) .(O) Sykes(B-writer) ,(O) p(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, book, award, person, writer, magazine, organization, location, country, literary genre, event and O.\nSentence: 91 A subsequent journey through the British East Africa colonies and the Belgian Congo formed the basis of two books ; the travelogue Remote People ( 1931 ) and the comic novel Black Mischief ( 1932 ) . Sykes , p .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["91","A","subsequent","journey","through","the","British","East","Africa","colonies","and","the","Belgian","Congo","formed","the","basis","of","two","books",";","the","travelogue","Remote","People","(","1931",")","and","the","comic","novel","Black","Mischief","(","1932",")",".","Sykes",",","p","."],"labels":["O","O","O","O","O","O","B-location","I-location","I-location","I-location","O","O","B-country","I-country","O","O","O","O","O","O","O","O","O","B-book","I-book","O","O","O","O","O","B-literary genre","I-literary genre","B-book","I-book","O","O","O","O","B-writer","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","book","award","person","writer","magazine","organization","location","country","literary_genre","event"]}
{"id":"134","dataset":"crossner_literature","split":"dev","instance":{"id":"134","prompt_labels":"Disraeli(B-writer) 's(O) early(O) silver(B-literary genre) fork(I-literary genre) novels(I-literary genre) Vivian(B-book) Grey(I-book) ((O) 1826(O) )(O) and(O) The(B-book) Young(I-book) Duke(I-book) ((O) 1831(O) )(O) featured(O) romanticised(O) depictions(O) of(O) aristocratic(O) life(O) ((O) despite(O) his(O) ignorance(O) of(O) it(O) )(O) with(O) character(O) sketches(O) of(O) well-known(O) public(O) figures(O) lightly(O) disguised(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, poem, literary genre, book, magazine, event, person, location, country, writer and O.\nSentence: Disraeli 's early silver fork novels Vivian Grey ( 1826 ) and The Young Duke ( 1831 ) featured romanticised depictions of aristocratic life ( despite his ignorance of it ) with character sketches of well-known public figures lightly disguised .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Disraeli","'s","early","silver","fork","novels","Vivian","Grey","(","1826",")","and","The","Young","Duke","(","1831",")","featured","romanticised","depictions","of","aristocratic","life","(","despite","his","ignorance","of","it",")","with","character","sketches","of","well-known","public","figures","lightly","disguised","."],"labels":["B-writer","O","O","B-literary genre","I-literary genre","I-literary genre","B-book","I-book","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","award","poem","literary_genre","book","magazine","event","person","location","country","writer"]}
{"id":"167","dataset":"crossner_literature","split":"dev","instance":{"id":"167","prompt_labels":"In(O) the(O) same(O) year(O) ,(O) he(O) produced(O) the(O) first(O) French(O) language(O) editions(O) of(O) Joseph(B-writer) Conrad(I-writer) '(O) s(O) Heart(B-book) of(I-book) Darkness(I-book) and(O) Lord(B-book) Jim(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, location, magazine, writer, literary genre, organization, country, award, person, poem, event and O.\nSentence: In the same year , he produced the first French language editions of Joseph Conrad ' s Heart of Darkness and Lord Jim .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","same","year",",","he","produced","the","first","French","language","editions","of","Joseph","Conrad","'","s","Heart","of","Darkness","and","Lord","Jim","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","B-book","I-book","I-book","O","B-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["book","location","magazine","writer","literary_genre","organization","country","award","person","poem","event"]}
{"id":"171","dataset":"crossner_literature","split":"dev","instance":{"id":"171","prompt_labels":"But(O) opponents(O) of(O) this(O) proposition(O) claim(O) that(O) Rabindranath(B-writer) Tagore(I-writer) mentioned(O) only(O) the(O) border(O) states(O) of(O) India(B-country) to(O) include(O) complete(O) India(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, book, event, writer, poem, literary genre, award, organization, location, person, magazine and O.\nSentence: But opponents of this proposition claim that Rabindranath Tagore mentioned only the border states of India to include complete India .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["But","opponents","of","this","proposition","claim","that","Rabindranath","Tagore","mentioned","only","the","border","states","of","India","to","include","complete","India","."],"labels":["O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","B-country","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["country","book","event","writer","poem","literary_genre","award","organization","location","person","magazine"]}
{"id":"190","dataset":"crossner_literature","split":"dev","instance":{"id":"190","prompt_labels":"Another(O) poem(O) that(O) is(O) ambiguous(O) in(O) this(O) respect(O) is(O) The(B-poem) Virgin(I-poem) Carrying(I-poem) a(I-poem) Lantern(I-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, magazine, country, literary genre, book, person, writer, poem, organization, location, event and O.\nSentence: Another poem that is ambiguous in this respect is The Virgin Carrying a Lantern .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Another","poem","that","is","ambiguous","in","this","respect","is","The","Virgin","Carrying","a","Lantern","."],"labels":["O","O","O","O","O","O","O","O","O","B-poem","I-poem","I-poem","I-poem","I-poem","O"],"target_index":null,"target_label":null},"label_list":["award","magazine","country","literary_genre","book","person","writer","poem","organization","location","event"]}
{"id":"198","dataset":"crossner_literature","split":"dev","instance":{"id":"198","prompt_labels":"When(O) Martin(B-writer) Gardner(I-writer) retired(O) from(O) writing(O) his(O) Mathematical(B-book) Games(I-book) column(O) for(O) Scientific(B-magazine) American(I-magazine) magazine(O) ,(O) Hofstadter(B-writer) succeeded(O) him(O) in(O) 1981-1983(O) with(O) a(O) column(O) titled(O) Metamagical(B-book) Themas(I-book) ((O) an(O) anagram(O) of(O) Mathematical(B-book) Games(I-book) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, event, literary genre, person, location, country, poem, award, writer, book, organization and O.\nSentence: When Martin Gardner retired from writing his Mathematical Games column for Scientific American magazine , Hofstadter succeeded him in 1981-1983 with a column titled Metamagical Themas ( an anagram of Mathematical Games ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["When","Martin","Gardner","retired","from","writing","his","Mathematical","Games","column","for","Scientific","American","magazine",",","Hofstadter","succeeded","him","in","1981-1983","with","a","column","titled","Metamagical","Themas","(","an","anagram","of","Mathematical","Games",")","."],"labels":["O","B-writer","I-writer","O","O","O","O","B-book","I-book","O","O","B-magazine","I-magazine","O","O","B-writer","O","O","O","O","O","O","O","O","B-book","I-book","O","O","O","O","B-book","I-book","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","event","literary_genre","person","location","country","poem","award","writer","book","organization"]}
{"id":"8","dataset":"crossner_music","split":"dev","instance":{"id":"8","prompt_labels":"The(O) album(O) was(O) certified(O) seven-times(O) platinum(O) in(O) Australia(B-country) by(O) the(O) Australian(B-organization) Recording(I-organization) Industry(I-organization) Association(I-organization) ((O) ARIA(B-organization) )(O) ,(O) five-times(O) platinum(O) in(O) the(O) UK(B-country) by(O) the(O) British(B-organization) Phonographic(I-organization) Industry(I-organization) ((O) BPI(B-organization) )(O) ,(O) and(O) platinum(O) in(O) the(O) US(B-country) by(O) the(O) Recording(B-organization) Industry(I-organization) Association(I-organization) of(I-organization) America(I-organization) ((O) RIAA(B-organization) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, musical artist, country, person, band, event, album, song, location, musical instrument, organization, music genre and O.\nSentence: The album was certified seven-times platinum in Australia by the Australian Recording Industry Association ( ARIA ) , five-times platinum in the UK by the British Phonographic Industry ( BPI ) , and platinum in the US by the Recording Industry Association of America ( RIAA ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","album","was","certified","seven-times","platinum","in","Australia","by","the","Australian","Recording","Industry","Association","(","ARIA",")",",","five-times","platinum","in","the","UK","by","the","British","Phonographic","Industry","(","BPI",")",",","and","platinum","in","the","US","by","the","Recording","Industry","Association","of","America","(","RIAA",")","."],"labels":["O","O","O","O","O","O","O","B-country","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","B-country","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","B-country","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O"],"target_index":null,"target_label":null},"label_list":["award","musical_artist","country","person","band","event","album","song","location","musical_instrument","organization","music_genre"]}
{"id":"16","dataset":"crossner_music","split":"dev","instance":{"id":"16","prompt_labels":"Since(O) the(O) introduction(O) of(O) the(O) 50(O) /(O) 50(O) voting(O) system(O) in(O) 2009(O) ,(O) the(O) juries(O) and(O) the(O) voters(O) have(O) disagreed(O) on(O) the(O) winner(O) on(O) five(O) occasions(O) ,(O) in(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2011(I-event) ,(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2015(I-event) ,(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2016(I-event) ,(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2018(I-event) and(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2019(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical artist, organization, person, band, location, album, country, musical instrument, award, music genre, song and O.\nSentence: Since the introduction of the 50 / 50 voting system in 2009 , the juries and the voters have disagreed on the winner on five occasions , in Eurovision Song Contest 2011 , Eurovision Song Contest 2015 , Eurovision Song Contest 2016 , Eurovision Song Contest 2018 and Eurovision Song Contest 2019 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Since","the","introduction","of","the","50","/","50","voting","system","in","2009",",","the","juries","and","the","voters","have","disagreed","on","the","winner","on","five","occasions",",","in","Eurovision","Song","Contest","2011",",","Eurovision","Song","Contest","2015",",","Eurovision","Song","Contest","2016",",","Eurovision","Song","Contest","2018","and","Eurovision","Song","Contest","2019","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","O","B-event","I-event","I-event","I-event","O","B-event","I-event","I-event","I-event","O","B-event","I-event","I-event","I-event","O","B-event","I-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["event","musical_artist","organization","person","band","location","album","country","musical_instrument","award","music_genre","song"]}
{"id":"102","dataset":"crossner_music","split":"dev","instance":{"id":"102","prompt_labels":"Steve(B-musical artist) Young(I-musical artist) ,(O) David(B-musical artist) Allan(I-musical artist) Coe(I-musical artist) ,(O) John(B-musical artist) Prine(I-musical artist) ,(O) Billy(B-musical artist) Joe(I-musical artist) Shaver(I-musical artist) ,(O) Gary(B-musical artist) Stewart(I-musical artist) ,(O) Townes(B-musical artist) Van(I-musical artist) Zandt(I-musical artist) ,(O) Kris(B-musical artist) Kristofferson(I-musical artist) ,(O) Michael(B-musical artist) Martin(I-musical artist) Murphey(I-musical artist) ,(O) Tompall(B-musical artist) Glaser(I-musical artist) ,(O) Steve(B-musical artist) Earle(I-musical artist) ,(O) and(O) the(O) later(O) career(O) renaissance(O) of(O) Johnny(B-musical artist) Cash(I-musical artist) ,(O) along(O) with(O) a(O) few(O) female(O) vocalists(O) such(O) as(O) Jessi(B-musical artist) Colter(I-musical artist) ,(O) Sammi(B-musical artist) Smith(I-musical artist) ,(O) Tanya(B-musical artist) Tucker(I-musical artist) ,(O) and(O) Rosanne(B-musical artist) Cash(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical instrument, album, award, music genre, musical artist, location, organization, country, band, song, person and O.\nSentence: Steve Young , David Allan Coe , John Prine , Billy Joe Shaver , Gary Stewart , Townes Van Zandt , Kris Kristofferson , Michael Martin Murphey , Tompall Glaser , Steve Earle , and the later career renaissance of Johnny Cash , along with a few female vocalists such as Jessi Colter , Sammi Smith , Tanya Tucker , and Rosanne Cash .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Steve","Young",",","David","Allan","Coe",",","John","Prine",",","Billy","Joe","Shaver",",","Gary","Stewart",",","Townes","Van","Zandt",",","Kris","Kristofferson",",","Michael","Martin","Murphey",",","Tompall","Glaser",",","Steve","Earle",",","and","the","later","career","renaissance","of","Johnny","Cash",",","along","with","a","few","female","vocalists","such","as","Jessi","Colter",",","Sammi","Smith",",","Tanya","Tucker",",","and","Rosanne","Cash","."],"labels":["B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["event","musical_instrument","album","award","music_genre","musical_artist","location","organization","country","band","song","person"]}
{"id":"117","dataset":"crossner_music","split":"dev","instance":{"id":"117","prompt_labels":"Jones(B-musical artist) supports(O) a(O) number(O) of(O) other(O) charities(O) ,(O) including(O) the(O) NAACP(B-organization) ,(O) GLAAD(B-organization) ,(O) Peace(B-organization) Games(I-organization) ,(O) AmfAR(B-organization) ,(O) and(O) the(O) Maybach(B-organization) Foundation(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, musical artist, location, album, music genre, band, musical instrument, person, event, organization, country, award and O.\nSentence: Jones supports a number of other charities , including the NAACP , GLAAD , Peace Games , AmfAR , and the Maybach Foundation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Jones","supports","a","number","of","other","charities",",","including","the","NAACP",",","GLAAD",",","Peace","Games",",","AmfAR",",","and","the","Maybach","Foundation","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","B-organization","O","B-organization","O","B-organization","I-organization","O","B-organization","O","O","O","B-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["song","musical_artist","location","album","music_genre","band","musical_instrument","person","event","organization","country","award"]}
{"id":"124","dataset":"crossner_music","split":"dev","instance":{"id":"124","prompt_labels":"High-pitched(O) screaming(O) is(O) occasionally(O) utilized(O) in(O) death(B-music genre) metal(I-music genre) ,(O) being(O) heard(O) in(O) songs(O) by(O) Death(B-band) ,(O) Aborted(B-band) ,(O) Exhumed(B-band) ,(O) Dying(B-band) Fetus(I-band) ,(O) Cannibal(B-band) Corpse(I-band) ,(O) and(O) Deicide(B-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, event, location, award, album, song, music genre, organization, musical instrument, band, country, person and O.\nSentence: High-pitched screaming is occasionally utilized in death metal , being heard in songs by Death , Aborted , Exhumed , Dying Fetus , Cannibal Corpse , and Deicide .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["High-pitched","screaming","is","occasionally","utilized","in","death","metal",",","being","heard","in","songs","by","Death",",","Aborted",",","Exhumed",",","Dying","Fetus",",","Cannibal","Corpse",",","and","Deicide","."],"labels":["O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","B-band","O","B-band","O","B-band","O","B-band","I-band","O","B-band","I-band","O","O","B-band","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","event","location","award","album","song","music_genre","organization","musical_instrument","band","country","person"]}
{"id":"143","dataset":"crossner_music","split":"dev","instance":{"id":"143","prompt_labels":"In(O) 1994(O) ,(O) Yauch(B-musical artist) and(O) activist(O) Erin(B-musical artist) Potts(I-musical artist) The(O) events(O) became(O) annual(O) ,(O) and(O) shortly(O) after(O) went(O) international(O) with(O) acts(O) such(O) as(O) Live(B-band) ,(O) Mike(B-musical artist) Mills(I-musical artist) and(O) Michael(B-musical artist) Stipe(I-musical artist) of(O) R.E.M.(B-band) ,(O) Rage(B-band) Against(I-band) the(I-band) Machine(I-band) ,(O) The(B-band) Smashing(I-band) Pumpkins(I-band) ,(O) and(O) U2(B-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical instrument, event, album, organization, song, musical artist, location, country, award, person, music genre and O.\nSentence: In 1994 , Yauch and activist Erin Potts The events became annual , and shortly after went international with acts such as Live , Mike Mills and Michael Stipe of R.E.M. , Rage Against the Machine , The Smashing Pumpkins , and U2 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1994",",","Yauch","and","activist","Erin","Potts","The","events","became","annual",",","and","shortly","after","went","international","with","acts","such","as","Live",",","Mike","Mills","and","Michael","Stipe","of","R.E.M.",",","Rage","Against","the","Machine",",","The","Smashing","Pumpkins",",","and","U2","."],"labels":["O","O","O","B-musical artist","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-band","O","B-band","I-band","I-band","I-band","O","B-band","I-band","I-band","O","O","B-band","O"],"target_index":null,"target_label":null},"label_list":["band","musical_instrument","event","album","organization","song","musical_artist","location","country","award","person","music_genre"]}
{"id":"156","dataset":"crossner_music","split":"dev","instance":{"id":"156","prompt_labels":"His(O) best-known(O) songs(O) include(O) I(B-song) Ain(I-song) 't(I-song) Marching(I-song) Anymore(I-song) ,(O) Changes(B-song) ,(O) Crucifixion(B-song) ,(O) Draft(B-song) Dodger(I-song) Rag(I-song) ,(O) Love(B-song) Me(I-song) ,(I-song) I(I-song) 'm(I-song) a(I-song) Liberal(I-song) ,(O) Outside(B-song) of(I-song) a(I-song) Small(I-song) Circle(I-song) of(I-song) Friends(I-song) ,(O) Power(B-song) and(I-song) the(I-song) Glory(I-song) ,(O) There(B-song) but(I-song) for(I-song) Fortune(I-song) ,(O) and(O) The(B-song) War(I-song) Is(I-song) Over(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, musical artist, band, organization, country, musical instrument, song, music genre, person, event, album and O.\nSentence: His best-known songs include I Ain 't Marching Anymore , Changes , Crucifixion , Draft Dodger Rag , Love Me , I 'm a Liberal , Outside of a Small Circle of Friends , Power and the Glory , There but for Fortune , and The War Is Over .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","best-known","songs","include","I","Ain","'t","Marching","Anymore",",","Changes",",","Crucifixion",",","Draft","Dodger","Rag",",","Love","Me",",","I","'m","a","Liberal",",","Outside","of","a","Small","Circle","of","Friends",",","Power","and","the","Glory",",","There","but","for","Fortune",",","and","The","War","Is","Over","."],"labels":["O","O","O","O","B-song","I-song","I-song","I-song","I-song","O","B-song","O","B-song","O","B-song","I-song","I-song","O","B-song","I-song","I-song","I-song","I-song","I-song","I-song","O","B-song","I-song","I-song","I-song","I-song","I-song","I-song","O","B-song","I-song","I-song","I-song","O","B-song","I-song","I-song","I-song","O","O","B-song","I-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["award","location","musical_artist","band","organization","country","musical_instrument","song","music_genre","person","event","album"]}
{"id":"168","dataset":"crossner_music","split":"dev","instance":{"id":"168","prompt_labels":"The(O) album(O) was(O) certified(O) gold(O) by(O) both(O) British(B-organization) Phonographic(I-organization) Industry(I-organization) and(O) the(O) Recording(B-organization) Industry(I-organization) Association(I-organization) of(I-organization) America(I-organization) in(O) December(O) 2007(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, award, song, location, album, organization, music genre, event, band, musical instrument, person, country and O.\nSentence: The album was certified gold by both British Phonographic Industry and the Recording Industry Association of America in December 2007 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","album","was","certified","gold","by","both","British","Phonographic","Industry","and","the","Recording","Industry","Association","of","America","in","December","2007","."],"labels":["O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","award","song","location","album","organization","music_genre","event","band","musical_instrument","person","country"]}
{"id":"188","dataset":"crossner_music","split":"dev","instance":{"id":"188","prompt_labels":"They(O) were(O) inducted(O) by(O) Chuck(B-musical artist) D(I-musical artist) and(O) LL(B-musical artist) Cool(I-musical artist) J(I-musical artist) on(O) April(O) 14(O) ,(O) 2012(O) therefore(O) the(O) group(O) didn(O) 't(O) perform(O) ;(O) instead(O) Black(B-musical artist) Thought(I-musical artist) ,(O) Travie(B-musical artist) from(O) Gym(B-band) Class(I-band) Heroes(I-band) and(O) Kid(B-musical artist) Rock(I-musical artist) performed(O) a(O) medley(O) of(O) their(O) songs(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, person, organization, album, song, musical artist, event, music genre, location, award, country, musical instrument and O.\nSentence: They were inducted by Chuck D and LL Cool J on April 14 , 2012 therefore the group didn 't perform ; instead Black Thought , Travie from Gym Class Heroes and Kid Rock performed a medley of their songs .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["They","were","inducted","by","Chuck","D","and","LL","Cool","J","on","April","14",",","2012","therefore","the","group","didn","'t","perform",";","instead","Black","Thought",",","Travie","from","Gym","Class","Heroes","and","Kid","Rock","performed","a","medley","of","their","songs","."],"labels":["O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","O","B-band","I-band","I-band","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["band","person","organization","album","song","musical_artist","event","music_genre","location","award","country","musical_instrument"]}
{"id":"197","dataset":"crossner_music","split":"dev","instance":{"id":"197","prompt_labels":"Mercury(B-musical artist) wrote(O) numerous(O) hits(O) for(O) Queen(B-band) ,(O) including(O) Killer(B-song) Queen(I-song) ,(O) Bohemian(B-song) Rhapsody(I-song) ,(O) Somebody(B-song) to(I-song) Love(I-song) ,(O) We(B-song) Are(I-song) the(I-song) Champions(I-song) ,(O) Don(B-song) 't(I-song) Stop(I-song) Me(I-song) Now(I-song) ,(O) and(O) Crazy(B-song) Little(I-song) Thing(I-song) Called(I-song) Love(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, song, album, band, organization, award, location, event, musical artist, person, musical instrument, music genre and O.\nSentence: Mercury wrote numerous hits for Queen , including Killer Queen , Bohemian Rhapsody , Somebody to Love , We Are the Champions , Don 't Stop Me Now , and Crazy Little Thing Called Love .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Mercury","wrote","numerous","hits","for","Queen",",","including","Killer","Queen",",","Bohemian","Rhapsody",",","Somebody","to","Love",",","We","Are","the","Champions",",","Don","'t","Stop","Me","Now",",","and","Crazy","Little","Thing","Called","Love","."],"labels":["B-musical artist","O","O","O","O","B-band","O","O","B-song","I-song","O","B-song","I-song","O","B-song","I-song","I-song","O","B-song","I-song","I-song","I-song","O","B-song","I-song","I-song","I-song","I-song","O","O","B-song","I-song","I-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["country","song","album","band","organization","award","location","event","musical_artist","person","musical_instrument","music_genre"]}
{"id":"30","dataset":"crossner_politics","split":"dev","instance":{"id":"30","prompt_labels":"In(O) that(O) election(O) he(O) defeated(O) William(B-politician) Ross(I-politician) of(O) the(O) Ulster(B-political party) Unionist(I-political party) Party(I-political party) who(O) had(O) represented(O) East(B-location) Londonderry(I-location) since(O) 1983(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) its(O) predecessor(O) seat(O) of(O) Londonderry(B-location) between(O) February(B-election) 1974(I-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 1983(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, political party, election, politician, location, person, event and O.\nSentence: In that election he defeated William Ross of the Ulster Unionist Party who had represented East Londonderry since 1983 United Kingdom general election and its predecessor seat of Londonderry between February 1974 United Kingdom general election and 1983 United Kingdom general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","that","election","he","defeated","William","Ross","of","the","Ulster","Unionist","Party","who","had","represented","East","Londonderry","since","1983","United","Kingdom","general","election","and","its","predecessor","seat","of","Londonderry","between","February","1974","United","Kingdom","general","election","and","1983","United","Kingdom","general","election","."],"labels":["O","O","O","O","O","B-politician","I-politician","O","O","B-political party","I-political party","I-political party","O","O","O","B-location","I-location","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","B-location","O","B-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["country","organization","political_party","election","politician","location","person","event"]}
{"id":"49","dataset":"crossner_politics","split":"dev","instance":{"id":"49","prompt_labels":"AP(B-political party) was(O) born(O) as(O) an(O) anti-establishment(O) party(O) in(O) 1993(O) ,(O) in(O) parallel(O) with(O) the(O) rise(O) of(O) Lega(B-political party) Nord(I-political party) in(O) Italy(B-country) ,(O) of(O) which(O) it(O) has(O) been(O) long(O) considered(O) the(O) Sanmarinese(O) counterpart(O) ,(O) but(O) has(O) since(O) then(O) become(O) a(O) stable(O) political(O) force(O) in(O) San(B-location) Marino(I-location) ,(O) participating(O) in(O) government(O) coalitions(O) with(O) the(O) centrist(O) Sammarinese(B-political party) Christian(I-political party) Democratic(I-political party) Party(I-political party) ((O) PDCS(B-political party) )(O) as(O) well(O) as(O) with(O) the(O) centre-left(O) Party(B-political party) of(I-political party) Socialists(I-political party) and(I-political party) Democrats(I-political party) ((O) PSD(B-political party) )(O) since(O) 2002(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, location, organization, person, politician, political party, country and O.\nSentence: AP was born as an anti-establishment party in 1993 , in parallel with the rise of Lega Nord in Italy , of which it has been long considered the Sanmarinese counterpart , but has since then become a stable political force in San Marino , participating in government coalitions with the centrist Sammarinese Christian Democratic Party ( PDCS ) as well as with the centre-left Party of Socialists and Democrats ( PSD ) since 2002 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["AP","was","born","as","an","anti-establishment","party","in","1993",",","in","parallel","with","the","rise","of","Lega","Nord","in","Italy",",","of","which","it","has","been","long","considered","the","Sanmarinese","counterpart",",","but","has","since","then","become","a","stable","political","force","in","San","Marino",",","participating","in","government","coalitions","with","the","centrist","Sammarinese","Christian","Democratic","Party","(","PDCS",")","as","well","as","with","the","centre-left","Party","of","Socialists","and","Democrats","(","PSD",")","since","2002","."],"labels":["B-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","election","location","organization","person","politician","political_party","country"]}
{"id":"64","dataset":"crossner_politics","split":"dev","instance":{"id":"64","prompt_labels":"Whereas(O) elsewhere(O) there(O) are(O) effectively(O) three(O) fundamental(O) battles(O) fought(O) in(O) elections(O) -(O) between(O) the(O) Ulster(B-political party) Unionist(I-political party) Party(I-political party) and(O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) to(O) be(O) the(O) leading(O) unionist(B-political party) party(I-political party) ,(O) between(O) the(O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) and(O) Sinn(B-political party) Féin(I-political party) to(O) be(O) the(O) leading(O) nationalist(B-political party) party(I-political party) ,(O) and(O) between(O) unionism(O) and(O) nationalism(O) as(O) a(O) whole(O) ,(O) North(B-location) Down(I-location) is(O) different(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, political party, politician, organization, election, location, country and O.\nSentence: Whereas elsewhere there are effectively three fundamental battles fought in elections - between the Ulster Unionist Party and the Democratic Unionist Party to be the leading unionist party , between the Social Democratic and Labour Party and Sinn Féin to be the leading nationalist party , and between unionism and nationalism as a whole , North Down is different .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Whereas","elsewhere","there","are","effectively","three","fundamental","battles","fought","in","elections","-","between","the","Ulster","Unionist","Party","and","the","Democratic","Unionist","Party","to","be","the","leading","unionist","party",",","between","the","Social","Democratic","and","Labour","Party","and","Sinn","Féin","to","be","the","leading","nationalist","party",",","and","between","unionism","and","nationalism","as","a","whole",",","North","Down","is","different","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","O","O","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","person","political_party","politician","organization","election","location","country"]}
{"id":"67","dataset":"crossner_politics","split":"dev","instance":{"id":"67","prompt_labels":"Ayaan(B-politician) Hirsi(I-politician) Ali(I-politician) is(O) a(O) Fellow(O) with(O) the(O) Hoover(B-organization) Institution(I-organization) at(O) Stanford(B-organization) University(I-organization) ,(O) a(O) Fellow(O) with(O) the(O) Future(B-organization) of(I-organization) Diplomacy(I-organization) Project(I-organization) at(O) the(O) Belfer(B-organization) Center(I-organization) for(I-organization) Science(I-organization) and(I-organization) International(I-organization) Affairs(I-organization) at(O) The(O) Harvard(B-organization) Kennedy(I-organization) School(I-organization) ,(O) a(O) visiting(O) scholar(O) at(O) the(O) American(B-organization) Enterprise(I-organization) Institute(I-organization) in(O) Washington(B-location) ,(I-location) D.C.(I-location) ,(O) and(O) a(O) member(O) of(O) the(O) Council(B-organization) on(I-organization) Foreign(I-organization) Relations(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, location, politician, election, organization, political party, country and O.\nSentence: Ayaan Hirsi Ali is a Fellow with the Hoover Institution at Stanford University , a Fellow with the Future of Diplomacy Project at the Belfer Center for Science and International Affairs at The Harvard Kennedy School , a visiting scholar at the American Enterprise Institute in Washington , D.C. , and a member of the Council on Foreign Relations .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Ayaan","Hirsi","Ali","is","a","Fellow","with","the","Hoover","Institution","at","Stanford","University",",","a","Fellow","with","the","Future","of","Diplomacy","Project","at","the","Belfer","Center","for","Science","and","International","Affairs","at","The","Harvard","Kennedy","School",",","a","visiting","scholar","at","the","American","Enterprise","Institute","in","Washington",",","D.C.",",","and","a","member","of","the","Council","on","Foreign","Relations","."],"labels":["B-politician","I-politician","I-politician","O","O","O","O","O","B-organization","I-organization","O","B-organization","I-organization","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-location","I-location","I-location","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["event","person","location","politician","election","organization","political_party","country"]}
{"id":"68","dataset":"crossner_politics","split":"dev","instance":{"id":"68","prompt_labels":"Most(O) major(O) federal(O) political(O) parties(O) ,(O) including(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) ,(O) the(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) ,(O) the(O) New(B-political party) Democratic(I-political party) Party(I-political party) and(O) the(O) Green(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) support(O) maintaining(O) the(O) status(O) quo(O) with(O) Quebec(B-location) remaining(O) part(O) of(O) Canada(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, politician, event, political party, country, person, election, organization and O.\nSentence: Most major federal political parties , including the Liberal Party of Canada , the Conservative Party of Canada , the New Democratic Party and the Green Party of Canada support maintaining the status quo with Quebec remaining part of Canada .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Most","major","federal","political","parties",",","including","the","Liberal","Party","of","Canada",",","the","Conservative","Party","of","Canada",",","the","New","Democratic","Party","and","the","Green","Party","of","Canada","support","maintaining","the","status","quo","with","Quebec","remaining","part","of","Canada","."],"labels":["O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","B-location","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["location","politician","event","political_party","country","person","election","organization"]}
{"id":"104","dataset":"crossner_politics","split":"dev","instance":{"id":"104","prompt_labels":"Those(O) were(O) the(O) Croatian(B-political party) Democratic(I-political party) Union(I-political party) ,(O) the(O) Croatian(B-political party) Peasant(I-political party) Party(I-political party) ,(O) the(O) Croatian(B-political party) People(I-political party) 's(I-political party) Party(I-political party) -(I-political party) Liberal(I-political party) Democrats(I-political party) ,(O) the(O) Croatian(B-political party) Social(I-political party) Liberal(I-political party) Party(I-political party) ,(O) Social(B-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Croatia(I-political party) and(O) the(O) Bridge(B-political party) of(I-political party) Independent(I-political party) Lists(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, location, election, country, politician, person, organization and O.\nSentence: Those were the Croatian Democratic Union , the Croatian Peasant Party , the Croatian People 's Party - Liberal Democrats , the Croatian Social Liberal Party , Social Democratic Party of Croatia and the Bridge of Independent Lists .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Those","were","the","Croatian","Democratic","Union",",","the","Croatian","Peasant","Party",",","the","Croatian","People","'s","Party","-","Liberal","Democrats",",","the","Croatian","Social","Liberal","Party",",","Social","Democratic","Party","of","Croatia","and","the","Bridge","of","Independent","Lists","."],"labels":["O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["political_party","event","location","election","country","politician","person","organization"]}
{"id":"122","dataset":"crossner_politics","split":"dev","instance":{"id":"122","prompt_labels":"Stanley(B-politician) ((O) then(O) known(O) as(O) the(O) Honourable(O) Edward(B-politician) Lyulph(I-politician) Stanley(I-politician) )(O) contested(O) Oldham(B-location) ,(O) in(O) the(O) Liberal(B-political party) interest(O) ,(O) at(O) elections(O) in(O) 1872(O) ,(O) 1874(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) 1880(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 1885(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, person, organization, event, political party, politician, election and O.\nSentence: Stanley ( then known as the Honourable Edward Lyulph Stanley ) contested Oldham , in the Liberal interest , at elections in 1872 , 1874 United Kingdom general election , 1880 United Kingdom general election and 1885 United Kingdom general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Stanley","(","then","known","as","the","Honourable","Edward","Lyulph","Stanley",")","contested","Oldham",",","in","the","Liberal","interest",",","at","elections","in","1872",",","1874","United","Kingdom","general","election",",","1880","United","Kingdom","general","election","and","1885","United","Kingdom","general","election","."],"labels":["B-politician","O","O","O","O","O","O","B-politician","I-politician","I-politician","O","O","B-location","O","O","O","B-political party","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["location","country","person","organization","event","political_party","politician","election"]}
{"id":"123","dataset":"crossner_politics","split":"dev","instance":{"id":"123","prompt_labels":"Malagodi(B-politician) managed(O) to(O) draw(O) some(O) votes(O) from(O) the(O) Italian(B-political party) Social(I-political party) Movement(I-political party) ,(O) the(O) Monarchist(B-political party) National(I-political party) Party(I-political party) and(O) especially(O) Christian(O) Democracy(O) ,(O) whose(O) electoral(O) base(O) was(O) composed(O) also(O) by(O) conservatives(O) suspicious(O) of(O) the(O) Socialists(O) ,(O) increasing(O) the(O) party(O) 's(O) share(O) to(O) a(O) historical(O) record(O) of(O) 7.0(O) %(O) in(O) the(O) 1963(B-election) Italian(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, politician, election, political party, event, person, location and O.\nSentence: Malagodi managed to draw some votes from the Italian Social Movement , the Monarchist National Party and especially Christian Democracy , whose electoral base was composed also by conservatives suspicious of the Socialists , increasing the party 's share to a historical record of 7.0 % in the 1963 Italian general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Malagodi","managed","to","draw","some","votes","from","the","Italian","Social","Movement",",","the","Monarchist","National","Party","and","especially","Christian","Democracy",",","whose","electoral","base","was","composed","also","by","conservatives","suspicious","of","the","Socialists",",","increasing","the","party","'s","share","to","a","historical","record","of","7.0","%","in","the","1963","Italian","general","election","."],"labels":["B-politician","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["organization","country","politician","election","political_party","event","person","location"]}
{"id":"129","dataset":"crossner_politics","split":"dev","instance":{"id":"129","prompt_labels":"He(O) ran(O) as(O) a(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) in(O) the(O) 1911(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) finishing(O) second(O) of(O) three(O) candidates(O) in(O) the(O) riding(O) of(O) Edmonton(B-location) ((O) the(O) victorious(O) candidate(O) was(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) Frank(B-politician) Oliver(I-politician) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, election, event, organization, politician, location, country, political party and O.\nSentence: He ran as a Conservative Party of Canada in the 1911 Canadian federal election , finishing second of three candidates in the riding of Edmonton ( the victorious candidate was Liberal Party of Canada Frank Oliver ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","ran","as","a","Conservative","Party","of","Canada","in","the","1911","Canadian","federal","election",",","finishing","second","of","three","candidates","in","the","riding","of","Edmonton","(","the","victorious","candidate","was","Liberal","Party","of","Canada","Frank","Oliver",")","."],"labels":["O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","B-location","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","B-politician","I-politician","O","O"],"target_index":null,"target_label":null},"label_list":["person","election","event","organization","politician","location","country","political_party"]}
{"id":"133","dataset":"crossner_politics","split":"dev","instance":{"id":"133","prompt_labels":"Since(O) 1952(O) ,(O) control(O) of(O) the(O) House(O) has(O) changed(O) hands(O) five(O) times(O) ,(O) all(O) of(O) which(O) were(O) in(O) midterm(O) elections(O) ((O) 1954(B-election) United(I-election) States(I-election) House(I-election) of(I-election) Representatives(I-election) elections(I-election) ,(O) 1994(B-election) United(I-election) States(I-election) House(I-election) of(I-election) Representatives(I-election) elections(I-election) ,(O) 2006(B-election) United(I-election) States(I-election) House(I-election) of(I-election) Representatives(I-election) elections(I-election) ,(O) 2010(B-election) United(I-election) States(I-election) House(I-election) of(I-election) Representatives(I-election) elections(I-election) and(O) 2018(B-election) United(I-election) States(I-election) House(I-election) of(I-election) Representatives(I-election) elections(I-election) )(O) and(O) all(O) of(O) which(O) were(O) at(O) the(O) expense(O) of(O) the(O) incumbent(O) President(O) 's(O) party(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, politician, event, organization, political party, election, location and O.\nSentence: Since 1952 , control of the House has changed hands five times , all of which were in midterm elections ( 1954 United States House of Representatives elections , 1994 United States House of Representatives elections , 2006 United States House of Representatives elections , 2010 United States House of Representatives elections and 2018 United States House of Representatives elections ) and all of which were at the expense of the incumbent President 's party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Since","1952",",","control","of","the","House","has","changed","hands","five","times",",","all","of","which","were","in","midterm","elections","(","1954","United","States","House","of","Representatives","elections",",","1994","United","States","House","of","Representatives","elections",",","2006","United","States","House","of","Representatives","elections",",","2010","United","States","House","of","Representatives","elections","and","2018","United","States","House","of","Representatives","elections",")","and","all","of","which","were","at","the","expense","of","the","incumbent","President","'s","party","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","country","politician","event","organization","political_party","election","location"]}
{"id":"13","dataset":"crossner_science","split":"dev","instance":{"id":"13","prompt_labels":"In(O) addition(O) to(O) his(O) steady(O) research(O) output(O) ,(O) Naqvi(B-scientist) has(O) manifested(O) his(O) commitment(O) to(O) teaching(O) by(O) contributing(O) to(O) journals(O) devoted(O) to(O) didactical(O) aspects(O) of(O) science(O) ((O) American(B-academic journal) Journal(I-academic journal) of(I-academic journal) Physics(I-academic journal) ,(O) European(B-academic journal) Journal(I-academic journal) of(I-academic journal) Physics(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Chemical(I-academic journal) Education(I-academic journal) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, enzyme, scientist, university, chemical compound, award, chemical element, astronomical object, theory, location, protein, discipline, event, academic journal, person, organization and O.\nSentence: In addition to his steady research output , Naqvi has manifested his commitment to teaching by contributing to journals devoted to didactical aspects of science ( American Journal of Physics , European Journal of Physics , Journal of Chemical Education ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","addition","to","his","steady","research","output",",","Naqvi","has","manifested","his","commitment","to","teaching","by","contributing","to","journals","devoted","to","didactical","aspects","of","science","(","American","Journal","of","Physics",",","European","Journal","of","Physics",",","Journal","of","Chemical","Education",")","."],"labels":["O","O","O","O","O","O","O","O","B-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O"],"target_index":null,"target_label":null},"label_list":["country","enzyme","scientist","university","chemical_compound","award","chemical_element","astronomical_object","theory","location","protein","discipline","event","academic_journal","person","organization"]}
{"id":"30","dataset":"crossner_science","split":"dev","instance":{"id":"30","prompt_labels":"His(O) Nobel(B-award) Prize(I-award) is(O) now(O) kept(O) on(O) display(O) at(O) the(O) International(B-organization) Red(I-organization) Cross(I-organization) and(I-organization) Red(I-organization) Crescent(I-organization) Movement(I-organization) in(O) Geneva(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, award, university, theory, enzyme, protein, country, person, organization, location, chemical element, chemical compound, scientist, event, astronomical object, discipline and O.\nSentence: His Nobel Prize is now kept on display at the International Red Cross and Red Crescent Movement in Geneva .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","Nobel","Prize","is","now","kept","on","display","at","the","International","Red","Cross","and","Red","Crescent","Movement","in","Geneva","."],"labels":["O","B-award","I-award","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","award","university","theory","enzyme","protein","country","person","organization","location","chemical_element","chemical_compound","scientist","event","astronomical_object","discipline"]}
{"id":"43","dataset":"crossner_science","split":"dev","instance":{"id":"43","prompt_labels":"Heartstone(B-event) has(O) also(O) been(O) a(O) part(O) of(O) a(O) number(O) of(O) esport(O) demonstration(O) event(O) s(O) at(O) international(O) competitions(O) ,(O) such(O) as(O) the(O) 2017(B-event) Asian(I-event) Indoor(I-event) and(I-event) Martial(I-event) Arts(I-event) Games(I-event) and(O) 2018(B-event) Asian(I-event) Games(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, person, event, discipline, organization, chemical compound, scientist, academic journal, award, theory, astronomical object, enzyme, country, university, protein, location and O.\nSentence: Heartstone has also been a part of a number of esport demonstration event s at international competitions , such as the 2017 Asian Indoor and Martial Arts Games and 2018 Asian Games .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Heartstone","has","also","been","a","part","of","a","number","of","esport","demonstration","event","s","at","international","competitions",",","such","as","the","2017","Asian","Indoor","and","Martial","Arts","Games","and","2018","Asian","Games","."],"labels":["B-event","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","I-event","I-event","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","person","event","discipline","organization","chemical_compound","scientist","academic_journal","award","theory","astronomical_object","enzyme","country","university","protein","location"]}
{"id":"48","dataset":"crossner_science","split":"dev","instance":{"id":"48","prompt_labels":"John(B-scientist) Hopkinson(I-scientist) ,(O) FRS(B-award) ,(O) ((O) 27(O) July(O) 1849(O) -(O) 27(O) August(O) 1898(O) )(O) was(O) a(O) United(B-country) Kingdom(I-country) physicist(O) ,(O) electrical(O) engineer(O) ,(O) Fellow(B-award) of(I-award) the(I-award) Royal(I-award) Society(I-award) and(O) President(O) of(O) the(O) Institution(B-organization) of(I-organization) Electrical(I-organization) Engineers(I-organization) twice(O) in(O) 1890(O) and(O) 1896(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, academic journal, scientist, person, theory, organization, location, university, chemical compound, award, event, country, astronomical object, chemical element, protein, discipline and O.\nSentence: John Hopkinson , FRS , ( 27 July 1849 - 27 August 1898 ) was a United Kingdom physicist , electrical engineer , Fellow of the Royal Society and President of the Institution of Electrical Engineers twice in 1890 and 1896 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["John","Hopkinson",",","FRS",",","(","27","July","1849","-","27","August","1898",")","was","a","United","Kingdom","physicist",",","electrical","engineer",",","Fellow","of","the","Royal","Society","and","President","of","the","Institution","of","Electrical","Engineers","twice","in","1890","and","1896","."],"labels":["B-scientist","I-scientist","O","B-award","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["enzyme","academic_journal","scientist","person","theory","organization","location","university","chemical_compound","award","event","country","astronomical_object","chemical_element","protein","discipline"]}
{"id":"54","dataset":"crossner_science","split":"dev","instance":{"id":"54","prompt_labels":"In(O) August(O) 2017(O) a(O) group(O) of(O) scientists(O) from(O) Oregon(B-location) published(O) an(O) article(O) in(O) Nature(B-academic journal) journal(O) detailing(O) the(O) successful(O) use(O) of(O) CRISPR(O) to(O) edit(O) out(O) a(O) mutation(O) responsible(O) for(O) congenital(O) heart(O) disease(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, country, enzyme, award, university, academic journal, person, protein, theory, location, organization, astronomical object, scientist, chemical element, chemical compound, event and O.\nSentence: In August 2017 a group of scientists from Oregon published an article in Nature journal detailing the successful use of CRISPR to edit out a mutation responsible for congenital heart disease .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","August","2017","a","group","of","scientists","from","Oregon","published","an","article","in","Nature","journal","detailing","the","successful","use","of","CRISPR","to","edit","out","a","mutation","responsible","for","congenital","heart","disease","."],"labels":["O","O","O","O","O","O","O","O","B-location","O","O","O","O","B-academic journal","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["discipline","country","enzyme","award","university","academic_journal","person","protein","theory","location","organization","astronomical_object","scientist","chemical_element","chemical_compound","event"]}
{"id":"56","dataset":"crossner_science","split":"dev","instance":{"id":"56","prompt_labels":"UCSI(B-university) University(I-university) ,(I-university) Sarawak(I-university) Campus(I-university) ,(O) University(B-university) College(I-university) of(I-university) Technology(I-university) Sarawak(I-university) ((O) UCTS(B-university) )(O) Tunku(B-university) Abdul(I-university) Rahman(I-university) University(I-university) College(I-university) ((I-university) Sabah(I-university) campus(I-university) )(I-university) ,(O) International(B-university) University(I-university) College(I-university) Of(I-university) Technology(I-university) Twintech(I-university) ((I-university) Sabah(I-university) campus(I-university) )(I-university) ,(O) and(O) Open(B-university) University(I-university) Malaysia(I-university) ((I-university) Sabah(I-university) campus(I-university) )(I-university) have(O) local(O) private(O) university(O) branch(O) campuses(O) in(O) East(B-location) Malaysia(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, university, enzyme, chemical element, academic journal, organization, event, theory, chemical compound, location, discipline, protein, astronomical object, person, scientist and O.\nSentence: UCSI University , Sarawak Campus , University College of Technology Sarawak ( UCTS ) Tunku Abdul Rahman University College ( Sabah campus ) , International University College Of Technology Twintech ( Sabah campus ) , and Open University Malaysia ( Sabah campus ) have local private university branch campuses in East Malaysia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["UCSI","University",",","Sarawak","Campus",",","University","College","of","Technology","Sarawak","(","UCTS",")","Tunku","Abdul","Rahman","University","College","(","Sabah","campus",")",",","International","University","College","Of","Technology","Twintech","(","Sabah","campus",")",",","and","Open","University","Malaysia","(","Sabah","campus",")","have","local","private","university","branch","campuses","in","East","Malaysia","."],"labels":["B-university","I-university","I-university","I-university","I-university","O","B-university","I-university","I-university","I-university","I-university","O","B-university","O","B-university","I-university","I-university","I-university","I-university","I-university","I-university","I-university","I-university","O","B-university","I-university","I-university","I-university","I-university","I-university","I-university","I-university","I-university","I-university","O","O","B-university","I-university","I-university","I-university","I-university","I-university","I-university","O","O","O","O","O","O","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["country","award","university","enzyme","chemical_element","academic_journal","organization","event","theory","chemical_compound","location","discipline","protein","astronomical_object","person","scientist"]}
{"id":"61","dataset":"crossner_science","split":"dev","instance":{"id":"61","prompt_labels":"Robot(O) designer(O) Hans(B-scientist) Moravec(I-scientist) ,(O) cyberneticist(O) Kevin(B-scientist) Warwick(I-scientist) and(O) inventor(O) Ray(B-person) Kurzweil(I-person) have(O) predicted(O) that(O) humans(O) and(O) machines(O) will(O) merge(O) in(O) the(O) future(O) into(O) cyborg(O) s(O) that(O) are(O) more(O) capable(O) and(O) powerful(O) than(O) either(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, university, award, astronomical object, event, location, enzyme, academic journal, chemical element, chemical compound, discipline, theory, scientist, protein, country and O.\nSentence: Robot designer Hans Moravec , cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborg s that are more capable and powerful than either .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Robot","designer","Hans","Moravec",",","cyberneticist","Kevin","Warwick","and","inventor","Ray","Kurzweil","have","predicted","that","humans","and","machines","will","merge","in","the","future","into","cyborg","s","that","are","more","capable","and","powerful","than","either","."],"labels":["O","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","person","university","award","astronomical_object","event","location","enzyme","academic_journal","chemical_element","chemical_compound","discipline","theory","scientist","protein","country"]}
{"id":"112","dataset":"crossner_science","split":"dev","instance":{"id":"112","prompt_labels":"It(O) was(O) discovered(O) on(O) 24(O) September(O) 1960(O) ,(O) by(O) astronomers(O) Cornelis(B-scientist) Johannes(I-scientist) van(I-scientist) Houten(I-scientist) ,(O) Ingrid(B-scientist) van(I-scientist) Houten-Groeneveld(I-scientist) and(O) Tom(B-scientist) Gehrels(I-scientist) at(O) Palomar(B-location) Observatory(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, country, person, theory, location, award, organization, scientist, academic journal, event, chemical compound, discipline, enzyme, protein, astronomical object, university and O.\nSentence: It was discovered on 24 September 1960 , by astronomers Cornelis Johannes van Houten , Ingrid van Houten-Groeneveld and Tom Gehrels at Palomar Observatory .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","discovered","on","24","September","1960",",","by","astronomers","Cornelis","Johannes","van","Houten",",","Ingrid","van","Houten-Groeneveld","and","Tom","Gehrels","at","Palomar","Observatory","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","country","person","theory","location","award","organization","scientist","academic_journal","event","chemical_compound","discipline","enzyme","protein","astronomical_object","university"]}
{"id":"142","dataset":"crossner_science","split":"dev","instance":{"id":"142","prompt_labels":"In(O) 2013(O) ,(O) the(O) exoplanet(O) Kepler-62f(B-astronomical object) was(O) discovered(O) ,(O) along(O) with(O) Kepler-62e(B-astronomical object) and(O) Kepler-62c(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, academic journal, enzyme, astronomical object, location, country, event, chemical element, organization, chemical compound, award, scientist, protein, theory, person, discipline and O.\nSentence: In 2013 , the exoplanet Kepler-62f was discovered , along with Kepler-62e and Kepler-62c .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2013",",","the","exoplanet","Kepler-62f","was","discovered",",","along","with","Kepler-62e","and","Kepler-62c","."],"labels":["O","O","O","O","O","B-astronomical object","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["university","academic_journal","enzyme","astronomical_object","location","country","event","chemical_element","organization","chemical_compound","award","scientist","protein","theory","person","discipline"]}
{"id":"144","dataset":"crossner_science","split":"dev","instance":{"id":"144","prompt_labels":"DNA(O) cytosine(O) methylation(O) is(O) catalyzed(O) by(O) DNA(B-enzyme) methyltransferase(I-enzyme) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, scientist, chemical element, enzyme, university, protein, location, organization, chemical compound, theory, discipline, award, person, country, astronomical object, academic journal and O.\nSentence: DNA cytosine methylation is catalyzed by DNA methyltransferase .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["DNA","cytosine","methylation","is","catalyzed","by","DNA","methyltransferase","."],"labels":["O","O","O","O","O","O","B-enzyme","I-enzyme","O"],"target_index":null,"target_label":null},"label_list":["event","scientist","chemical_element","enzyme","university","protein","location","organization","chemical_compound","theory","discipline","award","person","country","astronomical_object","academic_journal"]}
