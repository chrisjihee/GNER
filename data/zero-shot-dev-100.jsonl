{"id": "109", "dataset": "crossner_ai", "split": "dev", "label_list": ["university", "organization", "location", "programming language", "conference", "metric", "product", "field", "person", "task", "algorithm", "country", "researcher"], "instance": {"id": "109", "words": ["We", "make", "as", "well", "as", "possible", "precise", "by", "measuring", "the", "mean", "squared", "error", "between", "mathy", "/", "math", "and", "math", "\\", "hat", "{", "f", "}", "(", "x", ";", "D", ")", "/", "math", ":", "we", "want", "math", "(", "y", "-", "\\", "hat", "{", "f", "}", "(", "x", ";", "D", ")", ")", "^", "2", "/", "math", "to", "be", "minimal", ",", "both", "for", "mathx", "_", "1", ",", "\\", "dots", ",", "x", "_", "n", "/", "math", "and", "for", "points", "outside", "of", "our", "sample", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, organization, location, programming language, conference, metric, product, field, person, task, algorithm, country, researcher and O.\nSentence: We make as well as possible precise by measuring the mean squared error between mathy / math and math \\ hat { f } ( x ; D ) / math : we want math ( y - \\ hat { f } ( x ; D ) ) ^ 2 / math to be minimal , both for mathx _ 1 , \\ dots , x _ n / math and for points outside of our sample .", "prompt_labels": "We(O) make(O) as(O) well(O) as(O) possible(O) precise(O) by(O) measuring(O) the(O) mean(B-metric) squared(I-metric) error(I-metric) between(O) mathy(O) /(O) math(O) and(O) math(O) \\(O) hat(O) {(O) f(O) }(O) ((O) x(O) ;(O) D(O) )(O) /(O) math(O) :(O) we(O) want(O) math(O) ((O) y(O) -(O) \\(O) hat(O) {(O) f(O) }(O) ((O) x(O) ;(O) D(O) )(O) )(O) ^(O) 2(O) /(O) math(O) to(O) be(O) minimal(O) ,(O) both(O) for(O) mathx(O) _(O) 1(O) ,(O) \\(O) dots(O) ,(O) x(O) _(O) n(O) /(O) math(O) and(O) for(O) points(O) outside(O) of(O) our(O) sample(O) .(O)"}}
{"id": "105", "dataset": "crossner_ai", "split": "dev", "label_list": ["metric", "algorithm", "person", "field", "programming language", "researcher", "organization", "task", "university", "product", "location", "country", "conference"], "instance": {"id": "105", "words": ["For", "many", "years", "starting", "from", "1986", ",", "Miller", "directed", "the", "development", "of", "WordNet", ",", "a", "large", "computer-readable", "electronic", "reference", "usable", "in", "applications", "such", "as", "search", "engines", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-researcher", "O", "O", "O", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, algorithm, person, field, programming language, researcher, organization, task, university, product, location, country, conference and O.\nSentence: For many years starting from 1986 , Miller directed the development of WordNet , a large computer-readable electronic reference usable in applications such as search engines .", "prompt_labels": "For(O) many(O) years(O) starting(O) from(O) 1986(O) ,(O) Miller(B-researcher) directed(O) the(O) development(O) of(O) WordNet(B-product) ,(O) a(O) large(O) computer-readable(O) electronic(O) reference(O) usable(O) in(O) applications(O) such(O) as(O) search(B-product) engines(I-product) .(O)"}}
{"id": "45", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "person", "metric", "university", "field", "organization", "location", "programming language", "product", "country", "conference", "researcher", "algorithm"], "instance": {"id": "45", "words": ["Following", "his", "PhD", ",", "Ghahramani", "moved", "to", "the", "University", "of", "Toronto", "in", "1995", "as", "an", "ITRC", "Postdoctoral", "Fellow", "in", "the", "Artificial", "Intelligence", "Lab", ",", "working", "with", "Geoffrey", "Hinton", "."], "labels": ["O", "O", "O", "O", "B-researcher", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "B-researcher", "I-researcher", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, person, metric, university, field, organization, location, programming language, product, country, conference, researcher, algorithm and O.\nSentence: Following his PhD , Ghahramani moved to the University of Toronto in 1995 as an ITRC Postdoctoral Fellow in the Artificial Intelligence Lab , working with Geoffrey Hinton .", "prompt_labels": "Following(O) his(O) PhD(O) ,(O) Ghahramani(B-researcher) moved(O) to(O) the(O) University(B-university) of(I-university) Toronto(I-university) in(O) 1995(O) as(O) an(O) ITRC(B-organization) Postdoctoral(O) Fellow(O) in(O) the(O) Artificial(B-organization) Intelligence(I-organization) Lab(I-organization) ,(O) working(O) with(O) Geoffrey(B-researcher) Hinton(I-researcher) .(O)"}}
{"id": "296", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "country", "metric", "researcher", "programming language", "field", "task", "location", "algorithm", "university", "person", "product", "organization"], "instance": {"id": "296", "words": ["IAI", "is", "the", "world", "'s", "largest", "manufacturer", "of", "cartesian", "coordinate", "robot", "s", "and", "is", "an", "established", "leader", "in", "low", "cost", ",", "high", "performance", "SCARA", "robot", "s", "."], "labels": ["B-organization", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, country, metric, researcher, programming language, field, task, location, algorithm, university, person, product, organization and O.\nSentence: IAI is the world 's largest manufacturer of cartesian coordinate robot s and is an established leader in low cost , high performance SCARA robot s .", "prompt_labels": "IAI(B-organization) is(O) the(O) world(O) 's(O) largest(O) manufacturer(O) of(O) cartesian(B-product) coordinate(I-product) robot(I-product) s(O) and(O) is(O) an(O) established(O) leader(O) in(O) low(O) cost(O) ,(O) high(O) performance(O) SCARA(B-product) robot(I-product) s(O) .(O)"}}
{"id": "225", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "university", "organization", "conference", "algorithm", "metric", "researcher", "programming language", "product", "country", "location", "person", "task"], "instance": {"id": "225", "words": ["in", "their", "work", "for", "pedestrian", "detection", ",", "that", "was", "first", "described", "at", "the", "BMVC", "in", "2009", "."], "labels": ["O", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "B-conference", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, university, organization, conference, algorithm, metric, researcher, programming language, product, country, location, person, task and O.\nSentence: in their work for pedestrian detection , that was first described at the BMVC in 2009 .", "prompt_labels": "in(O) their(O) work(O) for(O) pedestrian(B-task) detection(I-task) ,(O) that(O) was(O) first(O) described(O) at(O) the(O) BMVC(B-conference) in(O) 2009(O) .(O)"}}
{"id": "31", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "person", "university", "metric", "country", "organization", "location", "researcher", "algorithm", "field", "task", "product", "programming language"], "instance": {"id": "31", "words": ["The", "Arduino", "integrated", "development", "environment", "(", "IDE", ")", "is", "a", "cross-platform", "application", "(", "for", "Windows", ",", "macOS", ",", "and", "Linux", ")", "that", "is", "written", "in", "the", "programming", "language", "Java", "."], "labels": ["O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O", "B-product", "O", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "B-programming language", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, person, university, metric, country, organization, location, researcher, algorithm, field, task, product, programming language and O.\nSentence: The Arduino integrated development environment ( IDE ) is a cross-platform application ( for Windows , macOS , and Linux ) that is written in the programming language Java .", "prompt_labels": "The(O) Arduino(B-product) integrated(O) development(O) environment(O) ((O) IDE(O) )(O) is(O) a(O) cross-platform(O) application(O) ((O) for(O) Windows(B-product) ,(O) macOS(B-product) ,(O) and(O) Linux(B-product) )(O) that(O) is(O) written(O) in(O) the(O) programming(O) language(O) Java(B-programming language) .(O)"}}
{"id": "132", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "country", "organization", "product", "person", "university", "metric", "programming language", "researcher", "task", "algorithm", "field", "conference"], "instance": {"id": "132", "words": ["He", "was", "honored", "with", "the", "2019", "Lifetime", "Achievement", "Award", "from", "the", "Association", "for", "Computational", "Linguistics", "(", "ACL", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, organization, product, person, university, metric, programming language, researcher, task, algorithm, field, conference and O.\nSentence: He was honored with the 2019 Lifetime Achievement Award from the Association for Computational Linguistics ( ACL ) .", "prompt_labels": "He(O) was(O) honored(O) with(O) the(O) 2019(O) Lifetime(O) Achievement(O) Award(O) from(O) the(O) Association(B-conference) for(I-conference) Computational(I-conference) Linguistics(I-conference) ((O) ACL(B-conference) )(O) .(O)"}}
{"id": "273", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "person", "university", "metric", "location", "task", "organization", "programming language", "algorithm", "country", "product", "conference", "field"], "instance": {"id": "273", "words": ["The", "SMC", "is", "very", "similar", "to", "the", "more", "popular", "Jaccard", "index", "."], "labels": ["O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, person, university, metric, location, task, organization, programming language, algorithm, country, product, conference, field and O.\nSentence: The SMC is very similar to the more popular Jaccard index .", "prompt_labels": "The(O) SMC(B-organization) is(O) very(O) similar(O) to(O) the(O) more(O) popular(O) Jaccard(B-metric) index(I-metric) .(O)"}}
{"id": "248", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "metric", "person", "programming language", "product", "algorithm", "university", "task", "location", "organization", "researcher", "conference", "country"], "instance": {"id": "248", "words": ["When", "the", "value", "being", "predicted", "is", "continuously", "distributed", ",", "the", "mean", "squared", "error", ",", "root", "mean", "squared", "error", "or", "median", "absolute", "deviation", "could", "be", "used", "to", "summarize", "the", "errors", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "B-metric", "I-metric", "I-metric", "I-metric", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, metric, person, programming language, product, algorithm, university, task, location, organization, researcher, conference, country and O.\nSentence: When the value being predicted is continuously distributed , the mean squared error , root mean squared error or median absolute deviation could be used to summarize the errors .", "prompt_labels": "When(O) the(O) value(O) being(O) predicted(O) is(O) continuously(O) distributed(O) ,(O) the(O) mean(B-metric) squared(I-metric) error(I-metric) ,(O) root(B-metric) mean(I-metric) squared(I-metric) error(I-metric) or(O) median(B-metric) absolute(I-metric) deviation(I-metric) could(O) be(O) used(O) to(O) summarize(O) the(O) errors(O) .(O)"}}
{"id": "3", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "organization", "programming language", "country", "researcher", "algorithm", "task", "person", "conference", "location", "product", "metric", "university"], "instance": {"id": "3", "words": ["The", "first", "picture", "to", "be", "scanned", ",", "stored", ",", "and", "recreated", "in", "digital", "pixels", "was", "displayed", "on", "the", "Standards", "Eastern", "Automatic", "Computer", "(", "SEAC", ")", "at", "NIST", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "I-product", "O", "B-product", "O", "O", "B-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, organization, programming language, country, researcher, algorithm, task, person, conference, location, product, metric, university and O.\nSentence: The first picture to be scanned , stored , and recreated in digital pixels was displayed on the Standards Eastern Automatic Computer ( SEAC ) at NIST .", "prompt_labels": "The(O) first(O) picture(O) to(O) be(O) scanned(O) ,(O) stored(O) ,(O) and(O) recreated(O) in(O) digital(O) pixels(O) was(O) displayed(O) on(O) the(O) Standards(B-product) Eastern(I-product) Automatic(I-product) Computer(I-product) ((O) SEAC(B-product) )(O) at(O) NIST(B-organization) .(O)"}}
{"id": "52", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "metric", "person", "country", "product", "programming language", "location", "university", "organization", "task", "algorithm", "field", "researcher"], "instance": {"id": "52", "words": ["In", "computer", "vision", "and", "image", "processing", ",", "the", "notion", "of", "scale", "space", "representation", "and", "Gaussian", "derivative", "operators", "is", "as", "a", "canonical", "multi-scale", "representation", "."], "labels": ["O", "B-field", "I-field", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, metric, person, country, product, programming language, location, university, organization, task, algorithm, field, researcher and O.\nSentence: In computer vision and image processing , the notion of scale space representation and Gaussian derivative operators is as a canonical multi-scale representation .", "prompt_labels": "In(O) computer(B-field) vision(I-field) and(O) image(B-field) processing(I-field) ,(O) the(O) notion(O) of(O) scale(O) space(O) representation(O) and(O) Gaussian(O) derivative(O) operators(O) is(O) as(O) a(O) canonical(O) multi-scale(O) representation(O) .(O)"}}
{"id": "260", "dataset": "crossner_ai", "split": "dev", "label_list": ["metric", "task", "researcher", "location", "programming language", "person", "country", "organization", "university", "field", "algorithm", "product", "conference"], "instance": {"id": "260", "words": ["Long", "short-term", "memory", "(", "LSTM", ")", "networks", "were", "invented", "by", "Sepp", "Hochreiter", "and", "Jürgen", "Schmidhuber", "in", "1997", "and", "set", "accuracy", "records", "in", "multiple", "applications", "domains", "."], "labels": ["B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, task, researcher, location, programming language, person, country, organization, university, field, algorithm, product, conference and O.\nSentence: Long short-term memory ( LSTM ) networks were invented by Sepp Hochreiter and Jürgen Schmidhuber in 1997 and set accuracy records in multiple applications domains .", "prompt_labels": "Long(B-algorithm) short-term(I-algorithm) memory(I-algorithm) ((O) LSTM(B-algorithm) )(O) networks(O) were(O) invented(O) by(O) Sepp(B-researcher) Hochreiter(I-researcher) and(O) Jürgen(B-researcher) Schmidhuber(I-researcher) in(O) 1997(O) and(O) set(O) accuracy(O) records(O) in(O) multiple(O) applications(O) domains(O) .(O)"}}
{"id": "229", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "country", "task", "location", "conference", "field", "product", "researcher", "metric", "programming language", "university", "organization", "person"], "instance": {"id": "229", "words": ["The", "Rancho", "Arm", "was", "developed", "as", "a", "robotic", "arm", "to", "help", "handicapped", "patients", "at", "the", "Rancho", "Los", "Amigos", "National", "Rehabilitation", "Center", "in", "Downey", ",", "California", ";", "this", "computer-controlled", "arm", "was", "bought", "by", "Stanford", "University", "in", "1963", "."], "labels": ["O", "B-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "I-location", "I-location", "O", "B-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, country, task, location, conference, field, product, researcher, metric, programming language, university, organization, person and O.\nSentence: The Rancho Arm was developed as a robotic arm to help handicapped patients at the Rancho Los Amigos National Rehabilitation Center in Downey , California ; this computer-controlled arm was bought by Stanford University in 1963 .", "prompt_labels": "The(O) Rancho(B-product) Arm(I-product) was(O) developed(O) as(O) a(O) robotic(O) arm(O) to(O) help(O) handicapped(O) patients(O) at(O) the(O) Rancho(B-location) Los(I-location) Amigos(I-location) National(I-location) Rehabilitation(I-location) Center(I-location) in(O) Downey(B-location) ,(O) California(B-location) ;(O) this(O) computer-controlled(O) arm(O) was(O) bought(O) by(O) Stanford(B-university) University(I-university) in(O) 1963(O) .(O)"}}
{"id": "55", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "location", "researcher", "algorithm", "field", "person", "country", "product", "task", "university", "conference", "metric", "organization"], "instance": {"id": "55", "words": ["Lafferty", "served", "many", "prestigious", "positions", ",", "including", ":", "1", ")", "program", "co-chair", "and", "general", "co-chair", "of", "the", "Neural", "Information", "Processing", "Systems", "(", "Conference", "on", "Neural", "Information", "Processing", "Systems", ")", "Foundation", "conferences", ";", "2", ")", "co-director", "of", "CMU", "'s", "new", "Ph.D.", "Machine", "Learning", "Ph.D.", "Program", ";", "3", ")", "associate", "editor", "of", "the", "Journal", "of", "Machine", "Learning", "Research"], "labels": ["B-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, location, researcher, algorithm, field, person, country, product, task, university, conference, metric, organization and O.\nSentence: Lafferty served many prestigious positions , including : 1 ) program co-chair and general co-chair of the Neural Information Processing Systems ( Conference on Neural Information Processing Systems ) Foundation conferences ; 2 ) co-director of CMU 's new Ph.D. Machine Learning Ph.D. Program ; 3 ) associate editor of the Journal of Machine Learning Research", "prompt_labels": "Lafferty(B-researcher) served(O) many(O) prestigious(O) positions(O) ,(O) including(O) :(O) 1(O) )(O) program(O) co-chair(O) and(O) general(O) co-chair(O) of(O) the(O) Neural(B-conference) Information(I-conference) Processing(I-conference) Systems(I-conference) ((O) Conference(B-conference) on(I-conference) Neural(I-conference) Information(I-conference) Processing(I-conference) Systems(I-conference) )(O) Foundation(O) conferences(O) ;(O) 2(O) )(O) co-director(O) of(O) CMU(B-university) 's(O) new(O) Ph.D.(O) Machine(B-field) Learning(I-field) Ph.D.(O) Program(O) ;(O) 3(O) )(O) associate(O) editor(O) of(O) the(O) Journal(B-conference) of(I-conference) Machine(I-conference) Learning(I-conference) Research(I-conference)"}}
{"id": "51", "dataset": "crossner_ai", "split": "dev", "label_list": ["country", "product", "metric", "university", "field", "conference", "programming language", "organization", "person", "algorithm", "location", "task", "researcher"], "instance": {"id": "51", "words": ["It", "has", "a", "rich", "set", "of", "features", ",", "libraries", "for", "constraint", "logic", "programming", ",", "multithreading", ",", "unit", "testing", ",", "GUI", ",", "interfacing", "to", "Java", ",", "ODBC", "and", "others", ",", "literate", "programming", ",", "a", "web", "server", ",", "SGML", ",", "RDF", ",", "RDFS", ",", "developer", "tools", "(", "including", "an", "IDE", "with", "a", "GUI", "debugger", "and", "GUI", "profiler", ")", ",", "and", "extensive", "documentation", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-programming language", "O", "B-product", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, product, metric, university, field, conference, programming language, organization, person, algorithm, location, task, researcher and O.\nSentence: It has a rich set of features , libraries for constraint logic programming , multithreading , unit testing , GUI , interfacing to Java , ODBC and others , literate programming , a web server , SGML , RDF , RDFS , developer tools ( including an IDE with a GUI debugger and GUI profiler ) , and extensive documentation .", "prompt_labels": "It(O) has(O) a(O) rich(O) set(O) of(O) features(O) ,(O) libraries(O) for(O) constraint(B-algorithm) logic(I-algorithm) programming(I-algorithm) ,(O) multithreading(O) ,(O) unit(O) testing(O) ,(O) GUI(O) ,(O) interfacing(O) to(O) Java(B-programming language) ,(O) ODBC(B-product) and(O) others(O) ,(O) literate(B-algorithm) programming(I-algorithm) ,(O) a(O) web(O) server(O) ,(O) SGML(O) ,(O) RDF(O) ,(O) RDFS(O) ,(O) developer(O) tools(O) ((O) including(O) an(O) IDE(O) with(O) a(O) GUI(O) debugger(O) and(O) GUI(O) profiler(O) )(O) ,(O) and(O) extensive(O) documentation(O) .(O)"}}
{"id": "19", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "person", "programming language", "task", "organization", "product", "university", "field", "metric", "location", "conference", "country", "algorithm"], "instance": {"id": "19", "words": ["Unsupervised", "learning", ",", "on", "the", "other", "hand", ",", "assumes", "training", "data", "that", "has", "not", "been", "hand-labeled", ",", "and", "attempts", "to", "find", "inherent", "patterns", "in", "the", "data", "that", "can", "then", "be", "used", "to", "determine", "the", "correct", "output", "value", "for", "new", "data", "instances", "..", "A", "combination", "of", "the", "two", "that", "has", "recently", "been", "explored", "is", "semi-supervised", "learning", ",", "which", "uses", "a", "combination", "of", "labeled", "and", "unlabeled", "data", "(", "typically", "a", "small", "set", "of", "labeled", "data", "combined", "with", "a", "large", "amount", "of", "unlabeled", "data", ")", "."], "labels": ["B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, person, programming language, task, organization, product, university, field, metric, location, conference, country, algorithm and O.\nSentence: Unsupervised learning , on the other hand , assumes training data that has not been hand-labeled , and attempts to find inherent patterns in the data that can then be used to determine the correct output value for new data instances .. A combination of the two that has recently been explored is semi-supervised learning , which uses a combination of labeled and unlabeled data ( typically a small set of labeled data combined with a large amount of unlabeled data ) .", "prompt_labels": "Unsupervised(B-field) learning(I-field) ,(O) on(O) the(O) other(O) hand(O) ,(O) assumes(O) training(O) data(O) that(O) has(O) not(O) been(O) hand-labeled(O) ,(O) and(O) attempts(O) to(O) find(O) inherent(O) patterns(O) in(O) the(O) data(O) that(O) can(O) then(O) be(O) used(O) to(O) determine(O) the(O) correct(O) output(O) value(O) for(O) new(O) data(O) instances(O) ..(O) A(O) combination(O) of(O) the(O) two(O) that(O) has(O) recently(O) been(O) explored(O) is(O) semi-supervised(B-field) learning(I-field) ,(O) which(O) uses(O) a(O) combination(O) of(O) labeled(O) and(O) unlabeled(O) data(O) ((O) typically(O) a(O) small(O) set(O) of(O) labeled(O) data(O) combined(O) with(O) a(O) large(O) amount(O) of(O) unlabeled(O) data(O) )(O) .(O)"}}
{"id": "204", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "conference", "organization", "person", "field", "metric", "university", "algorithm", "country", "location", "researcher", "product", "task"], "instance": {"id": "204", "words": ["Sepp", "Hochreiter", ",", "Y.", "Bengio", ",", "P.", "Frasconi", ",", "and", "Jürgen", "Schmidhuber", "."], "labels": ["B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O", "B-researcher", "I-researcher", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, conference, organization, person, field, metric, university, algorithm, country, location, researcher, product, task and O.\nSentence: Sepp Hochreiter , Y. Bengio , P. Frasconi , and Jürgen Schmidhuber .", "prompt_labels": "Sepp(B-researcher) Hochreiter(I-researcher) ,(O) Y.(B-researcher) Bengio(I-researcher) ,(O) P.(B-researcher) Frasconi(I-researcher) ,(O) and(O) Jürgen(B-researcher) Schmidhuber(I-researcher) .(O)"}}
{"id": "266", "dataset": "crossner_ai", "split": "dev", "label_list": ["product", "programming language", "field", "researcher", "person", "university", "algorithm", "organization", "conference", "country", "task", "location", "metric"], "instance": {"id": "266", "words": ["The", "counts", "of", "TP", ",", "TN", ",", "FP", ",", "and", "FN", "are", "usually", "kept", "on", "a", "table", "known", "as", "the", "confusion", "matrix", "."], "labels": ["O", "O", "O", "B-metric", "O", "B-metric", "O", "B-metric", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, programming language, field, researcher, person, university, algorithm, organization, conference, country, task, location, metric and O.\nSentence: The counts of TP , TN , FP , and FN are usually kept on a table known as the confusion matrix .", "prompt_labels": "The(O) counts(O) of(O) TP(B-metric) ,(O) TN(B-metric) ,(O) FP(B-metric) ,(O) and(O) FN(B-metric) are(O) usually(O) kept(O) on(O) a(O) table(O) known(O) as(O) the(O) confusion(B-metric) matrix(I-metric) .(O)"}}
{"id": "295", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "person", "conference", "metric", "product", "university", "programming language", "country", "researcher", "location", "algorithm", "field", "organization"], "instance": {"id": "295", "words": ["An", "empirical", "whitening", "transform", "is", "obtained", "by", "estimating", "the", "covariance", "(", "e.g.", "by", "maximum", "likelihood", ")", "and", "subsequently", "constructing", "a", "corresponding", "estimated", "whitening", "matrix", "(", "e.g.", "by", "Cholesky", "decomposition", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, person, conference, metric, product, university, programming language, country, researcher, location, algorithm, field, organization and O.\nSentence: An empirical whitening transform is obtained by estimating the covariance ( e.g. by maximum likelihood ) and subsequently constructing a corresponding estimated whitening matrix ( e.g. by Cholesky decomposition ) .", "prompt_labels": "An(O) empirical(O) whitening(O) transform(O) is(O) obtained(O) by(O) estimating(O) the(O) covariance(O) ((O) e.g.(O) by(O) maximum(B-algorithm) likelihood(I-algorithm) )(O) and(O) subsequently(O) constructing(O) a(O) corresponding(O) estimated(O) whitening(O) matrix(O) ((O) e.g.(O) by(O) Cholesky(B-algorithm) decomposition(I-algorithm) )(O) .(O)"}}
{"id": "142", "dataset": "crossner_ai", "split": "dev", "label_list": ["metric", "location", "algorithm", "product", "country", "programming language", "university", "researcher", "organization", "person", "conference", "task", "field"], "instance": {"id": "142", "words": ["In", "2018", "and", "2019", ",", "the", "Championship", "was", "be", "held", "in", "Houston", "and", "Detroit", ",", "Michigan", "at", "the", "TCF", "Center", "and", "Ford", "Field", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-location", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, location, algorithm, product, country, programming language, university, researcher, organization, person, conference, task, field and O.\nSentence: In 2018 and 2019 , the Championship was be held in Houston and Detroit , Michigan at the TCF Center and Ford Field .", "prompt_labels": "In(O) 2018(O) and(O) 2019(O) ,(O) the(O) Championship(O) was(O) be(O) held(O) in(O) Houston(B-location) and(O) Detroit(B-location) ,(O) Michigan(B-location) at(O) the(O) TCF(B-location) Center(I-location) and(O) Ford(B-location) Field(I-location) .(O)"}}
{"id": "349", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "product", "programming language", "location", "person", "field", "conference", "algorithm", "organization", "university", "researcher", "country", "metric"], "instance": {"id": "349", "words": ["Other", "active", "youth-led", "climate", "groups", "include", "Extinction", "Rebellion", ",", "the", "Sunrise", "Movement", ",", "SustainUS", ",", "the", ",", "among", "others", "working", "at", "both", "the", "transnational", "and", "local", "levels", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, product, programming language, location, person, field, conference, algorithm, organization, university, researcher, country, metric and O.\nSentence: Other active youth-led climate groups include Extinction Rebellion , the Sunrise Movement , SustainUS , the , among others working at both the transnational and local levels .", "prompt_labels": "Other(O) active(O) youth-led(O) climate(O) groups(O) include(O) Extinction(B-organization) Rebellion(I-organization) ,(O) the(O) Sunrise(B-organization) Movement(I-organization) ,(O) SustainUS(B-organization) ,(O) the(O) ,(O) among(O) others(O) working(O) at(O) both(O) the(O) transnational(O) and(O) local(O) levels(O) .(O)"}}
{"id": "23", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "conference", "algorithm", "task", "country", "metric", "field", "location", "programming language", "person", "university", "product", "organization"], "instance": {"id": "23", "words": ["However", ",", "in", "the", "last", "years", ",", "one", "can", "observe", "appearing", "of", "different", "e-services", "and", "related", "initiatives", "in", "developing", "countries", "such", "as", "Project", "Nemmadi", ",", "MCA21", "Mission", "Mode", "Project", "or", "Digital", "India", "even", "more", ",", "in", "India", ";", "Electronic", "Government", "Directorate", "in", "Pakistan", ";", "etc", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-organization", "I-organization", "I-organization", "O", "B-country", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, conference, algorithm, task, country, metric, field, location, programming language, person, university, product, organization and O.\nSentence: However , in the last years , one can observe appearing of different e-services and related initiatives in developing countries such as Project Nemmadi , MCA21 Mission Mode Project or Digital India even more , in India ; Electronic Government Directorate in Pakistan ; etc .", "prompt_labels": "However(O) ,(O) in(O) the(O) last(O) years(O) ,(O) one(O) can(O) observe(O) appearing(O) of(O) different(O) e-services(O) and(O) related(O) initiatives(O) in(O) developing(O) countries(O) such(O) as(O) Project(O) Nemmadi(O) ,(O) MCA21(O) Mission(O) Mode(O) Project(O) or(O) Digital(O) India(O) even(O) more(O) ,(O) in(O) India(B-country) ;(O) Electronic(B-organization) Government(I-organization) Directorate(I-organization) in(O) Pakistan(B-country) ;(O) etc(O) .(O)"}}
{"id": "159", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "programming language", "field", "task", "conference", "product", "country", "researcher", "organization", "person", "location", "university", "metric"], "instance": {"id": "159", "words": ["The", "information", "is", "a", "blend", "of", "sitemaps", "and", "RSS", "and", "is", "created", "using", "the", "Information", "Model", "(", "IM", ")", "and", "Biomedical", "Resource", "Ontology", "(", "BRO", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, programming language, field, task, conference, product, country, researcher, organization, person, location, university, metric and O.\nSentence: The information is a blend of sitemaps and RSS and is created using the Information Model ( IM ) and Biomedical Resource Ontology ( BRO ) .", "prompt_labels": "The(O) information(O) is(O) a(O) blend(O) of(O) sitemaps(O) and(O) RSS(O) and(O) is(O) created(O) using(O) the(O) Information(B-algorithm) Model(I-algorithm) ((O) IM(B-algorithm) )(O) and(O) Biomedical(B-algorithm) Resource(I-algorithm) Ontology(I-algorithm) ((O) BRO(B-algorithm) )(O) .(O)"}}
{"id": "185", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "product", "algorithm", "university", "researcher", "country", "person", "metric", "conference", "location", "programming language", "task", "organization"], "instance": {"id": "185", "words": ["Aspects", "of", "ontology", "editors", "include", ":", "visual", "navigation", "possibilities", "within", "the", "knowledge", "model", ",", "inference", "engine", "s", "and", "extraction", ";", "support", "for", "modules", ";", "the", "import", "and", "export", "of", "foreign", "knowledge", "representation", "languages", "for", "ontology", "matching", ";", "and", "the", "support", "of", "meta-ontologies", "such", "as", "OWL-S", ",", "Dublin", "Core", ",", "etc", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "O", "B-task", "O", "B-task", "I-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "B-task", "O", "O", "B-product", "O", "B-product", "I-product", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, product, algorithm, university, researcher, country, person, metric, conference, location, programming language, task, organization and O.\nSentence: Aspects of ontology editors include : visual navigation possibilities within the knowledge model , inference engine s and extraction ; support for modules ; the import and export of foreign knowledge representation languages for ontology matching ; and the support of meta-ontologies such as OWL-S , Dublin Core , etc .", "prompt_labels": "Aspects(O) of(O) ontology(O) editors(O) include(O) :(O) visual(B-task) navigation(I-task) possibilities(O) within(O) the(O) knowledge(B-task) model(I-task) ,(O) inference(B-task) engine(I-task) s(O) and(O) extraction(B-task) ;(O) support(B-task) for(I-task) modules(I-task) ;(O) the(O) import(O) and(O) export(O) of(O) foreign(O) knowledge(B-task) representation(I-task) languages(O) for(O) ontology(B-task) matching(I-task) ;(O) and(O) the(O) support(O) of(O) meta-ontologies(B-task) such(O) as(O) OWL-S(B-product) ,(O) Dublin(B-product) Core(I-product) ,(O) etc(O) .(O)"}}
{"id": "168", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "university", "country", "task", "researcher", "product", "field", "person", "organization", "location", "programming language", "conference", "metric"], "instance": {"id": "168", "words": ["Headquartered", "in", "Rochester", "Hills", ",", "Michigan", ",", "the", "company", "had", "10", "regional", "locations", "in", "the", "U.S.", ",", "Canada", ",", "Mexico", "and", "Brazil", "."], "labels": ["O", "O", "B-location", "I-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "B-country", "O", "B-country", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, university, country, task, researcher, product, field, person, organization, location, programming language, conference, metric and O.\nSentence: Headquartered in Rochester Hills , Michigan , the company had 10 regional locations in the U.S. , Canada , Mexico and Brazil .", "prompt_labels": "Headquartered(O) in(O) Rochester(B-location) Hills(I-location) ,(O) Michigan(B-location) ,(O) the(O) company(O) had(O) 10(O) regional(O) locations(O) in(O) the(B-country) U.S.(I-country) ,(O) Canada(B-country) ,(O) Mexico(B-country) and(O) Brazil(B-country) .(O)"}}
{"id": "217", "dataset": "crossner_ai", "split": "dev", "label_list": ["country", "person", "researcher", "field", "product", "task", "location", "algorithm", "conference", "university", "metric", "organization", "programming language"], "instance": {"id": "217", "words": ["A", "frustrating", "outcome", "of", "the", "same", "study", "by", "Stanford", "(", "and", "other", "attempts", "to", "improve", "named", "recognition", "translation", ")", "is", "that", "many", "times", ",", "a", "decrease", "in", "the", "Bilingual", "evaluation", "understudy", "scores", "for", "translation", "will", "result", "from", "the", "inclusion", "of", "methods", "for", "named", "entity", "translation", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-university", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, researcher, field, product, task, location, algorithm, conference, university, metric, organization, programming language and O.\nSentence: A frustrating outcome of the same study by Stanford ( and other attempts to improve named recognition translation ) is that many times , a decrease in the Bilingual evaluation understudy scores for translation will result from the inclusion of methods for named entity translation .", "prompt_labels": "A(O) frustrating(O) outcome(O) of(O) the(O) same(O) study(O) by(O) Stanford(B-university) ((O) and(O) other(O) attempts(O) to(O) improve(O) named(B-task) recognition(I-task) translation(I-task) )(O) is(O) that(O) many(O) times(O) ,(O) a(O) decrease(O) in(O) the(O) Bilingual(B-metric) evaluation(I-metric) understudy(I-metric) scores(O) for(O) translation(O) will(O) result(O) from(O) the(O) inclusion(O) of(O) methods(O) for(O) named(B-task) entity(I-task) translation(I-task) .(O)"}}
{"id": "177", "dataset": "crossner_ai", "split": "dev", "label_list": ["country", "task", "person", "researcher", "organization", "product", "metric", "location", "programming language", "algorithm", "university", "conference", "field"], "instance": {"id": "177", "words": ["The", "SSD", "is", "also", "known", "as", "mean", "squared", "error", "."], "labels": ["O", "B-metric", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, task, person, researcher, organization, product, metric, location, programming language, algorithm, university, conference, field and O.\nSentence: The SSD is also known as mean squared error .", "prompt_labels": "The(O) SSD(B-metric) is(O) also(O) known(O) as(O) mean(B-metric) squared(I-metric) error(I-metric) .(O)"}}
{"id": "324", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "product", "metric", "conference", "organization", "country", "person", "researcher", "location", "field", "task", "algorithm", "university"], "instance": {"id": "324", "words": ["He", "spent", "his", "childhood", "years", "in", "Paris", ",", "France", ",", "where", "his", "parents", "had", "emigrated", "from", "Lithuania", "in", "the", "early", "1920s", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-location", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, product, metric, conference, organization, country, person, researcher, location, field, task, algorithm, university and O.\nSentence: He spent his childhood years in Paris , France , where his parents had emigrated from Lithuania in the early 1920s .", "prompt_labels": "He(O) spent(O) his(O) childhood(O) years(O) in(O) Paris(B-location) ,(O) France(B-country) ,(O) where(O) his(O) parents(O) had(O) emigrated(O) from(O) Lithuania(B-country) in(O) the(O) early(O) 1920s(O) .(O)"}}
{"id": "58", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "metric", "organization", "field", "researcher", "university", "conference", "product", "location", "task", "country", "algorithm", "programming language"], "instance": {"id": "58", "words": ["The", "natural", "gradient", "of", "mathE", "f", "(", "x", ")", "/", "math", ",", "complying", "with", "the", "Fisher", "information", "metric", "(", "an", "informational", "distance", "measure", "between", "probability", "distributions", "and", "the", "curvature", "of", "the", "relative", "entropy", ")", ",", "now", "reads"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, metric, organization, field, researcher, university, conference, product, location, task, country, algorithm, programming language and O.\nSentence: The natural gradient of mathE f ( x ) / math , complying with the Fisher information metric ( an informational distance measure between probability distributions and the curvature of the relative entropy ) , now reads", "prompt_labels": "The(O) natural(O) gradient(O) of(O) mathE(O) f(O) ((O) x(O) )(O) /(O) math(O) ,(O) complying(O) with(O) the(O) Fisher(B-metric) information(I-metric) metric(I-metric) ((O) an(O) informational(O) distance(O) measure(O) between(O) probability(O) distributions(O) and(O) the(O) curvature(O) of(O) the(O) relative(B-metric) entropy(I-metric) )(O) ,(O) now(O) reads(O)"}}
{"id": "111", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "product", "location", "country", "conference", "algorithm", "metric", "university", "task", "researcher", "organization", "person", "field"], "instance": {"id": "111", "words": ["At", "the", "2018", "Conference", "on", "Neural", "Information", "Processing", "Systems", "(", "NeurIPS", ")", "researchers", "from", "Google", "presented", "the", "work", "."], "labels": ["O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "O", "O", "B-organization", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, product, location, country, conference, algorithm, metric, university, task, researcher, organization, person, field and O.\nSentence: At the 2018 Conference on Neural Information Processing Systems ( NeurIPS ) researchers from Google presented the work .", "prompt_labels": "At(O) the(O) 2018(B-conference) Conference(I-conference) on(I-conference) Neural(I-conference) Information(I-conference) Processing(I-conference) Systems(I-conference) ((O) NeurIPS(B-conference) )(O) researchers(O) from(O) Google(B-organization) presented(O) the(O) work(O) .(O)"}}
{"id": "162", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "researcher", "field", "organization", "task", "conference", "metric", "country", "person", "product", "location", "programming language", "university"], "instance": {"id": "162", "words": ["SSIM", "is", "designed", "to", "improve", "on", "traditional", "methods", "such", "as", "peak", "signal-to-noise", "ratio", "(", "PSNR", ")", "and", "mean", "squared", "error", "(", "MSE", ")", "."], "labels": ["B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "B-metric", "O", "O", "B-metric", "I-metric", "I-metric", "O", "B-metric", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, researcher, field, organization, task, conference, metric, country, person, product, location, programming language, university and O.\nSentence: SSIM is designed to improve on traditional methods such as peak signal-to-noise ratio ( PSNR ) and mean squared error ( MSE ) .", "prompt_labels": "SSIM(B-metric) is(O) designed(O) to(O) improve(O) on(O) traditional(O) methods(O) such(O) as(O) peak(B-metric) signal-to-noise(I-metric) ratio(I-metric) ((O) PSNR(B-metric) )(O) and(O) mean(B-metric) squared(I-metric) error(I-metric) ((O) MSE(B-metric) )(O) .(O)"}}
{"id": "178", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "programming language", "organization", "product", "algorithm", "field", "person", "country", "university", "researcher", "task", "conference", "metric"], "instance": {"id": "178", "words": ["Decision", "tree", "learning", ",", "neural", "networks", ",", "or", "a", "naive", "Bayes", "classifier", "could", "be", "used", "in", "combination", "with", "measures", "of", "model", "quality", "such", "as", "balanced", "accuracy"], "labels": ["B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, programming language, organization, product, algorithm, field, person, country, university, researcher, task, conference, metric and O.\nSentence: Decision tree learning , neural networks , or a naive Bayes classifier could be used in combination with measures of model quality such as balanced accuracy", "prompt_labels": "Decision(B-algorithm) tree(I-algorithm) learning(I-algorithm) ,(O) neural(B-algorithm) networks(I-algorithm) ,(O) or(O) a(O) naive(B-algorithm) Bayes(I-algorithm) classifier(I-algorithm) could(O) be(O) used(O) in(O) combination(O) with(O) measures(O) of(O) model(O) quality(O) such(O) as(O) balanced(B-metric) accuracy(I-metric)"}}
{"id": "21", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "task", "product", "country", "organization", "researcher", "programming language", "conference", "field", "university", "person", "algorithm", "metric"], "instance": {"id": "21", "words": ["Webber", "became", "a", "Fellow", "of", "the", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "in", "1991", ","], "labels": ["B-researcher", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, task, product, country, organization, researcher, programming language, conference, field, university, person, algorithm, metric and O.\nSentence: Webber became a Fellow of the Association for the Advancement of Artificial Intelligence in 1991 ,", "prompt_labels": "Webber(B-researcher) became(O) a(O) Fellow(O) of(O) the(O) Association(B-conference) for(I-conference) the(I-conference) Advancement(I-conference) of(I-conference) Artificial(I-conference) Intelligence(I-conference) in(O) 1991(O) ,(O)"}}
{"id": "292", "dataset": "crossner_ai", "split": "dev", "label_list": ["university", "task", "person", "researcher", "location", "algorithm", "programming language", "conference", "metric", "country", "product", "field", "organization"], "instance": {"id": "292", "words": ["The", "manipulator", "is", "what", "makes", "the", "robot", "move", ",", "and", "the", "design", "of", "these", "systems", "can", "be", "categorized", "into", "several", "common", "types", ",", "such", "as", "SCARA", "and", "cartesian", "coordinate", "robot", ",", "which", "use", "different", "coordinate", "systems", "to", "direct", "the", "arms", "of", "the", "machine", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, task, person, researcher, location, algorithm, programming language, conference, metric, country, product, field, organization and O.\nSentence: The manipulator is what makes the robot move , and the design of these systems can be categorized into several common types , such as SCARA and cartesian coordinate robot , which use different coordinate systems to direct the arms of the machine .", "prompt_labels": "The(O) manipulator(O) is(O) what(O) makes(O) the(O) robot(O) move(O) ,(O) and(O) the(O) design(O) of(O) these(O) systems(O) can(O) be(O) categorized(O) into(O) several(O) common(O) types(O) ,(O) such(O) as(O) SCARA(O) and(O) cartesian(O) coordinate(O) robot(O) ,(O) which(O) use(O) different(O) coordinate(O) systems(O) to(O) direct(O) the(O) arms(O) of(O) the(O) machine(O) .(O)"}}
{"id": "130", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "product", "algorithm", "programming language", "metric", "organization", "university", "person", "task", "conference", "country", "location", "field"], "instance": {"id": "130", "words": ["The", "most", "recent", "school", "of", "thought", "on", "Honda", "'s", "strategy", "was", "put", "forward", "by", "Gary", "Hamel", "and", "C.", "K.", "Prahalad", "in", "1989", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, product, algorithm, programming language, metric, organization, university, person, task, conference, country, location, field and O.\nSentence: The most recent school of thought on Honda 's strategy was put forward by Gary Hamel and C. K. Prahalad in 1989 .", "prompt_labels": "The(O) most(O) recent(O) school(O) of(O) thought(O) on(O) Honda(B-organization) 's(O) strategy(O) was(O) put(O) forward(O) by(O) Gary(B-person) Hamel(I-person) and(O) C.(B-person) K.(I-person) Prahalad(I-person) in(O) 1989(O) .(O)"}}
{"id": "73", "dataset": "crossner_ai", "split": "dev", "label_list": ["product", "programming language", "researcher", "person", "metric", "country", "field", "conference", "university", "task", "algorithm", "location", "organization"], "instance": {"id": "73", "words": ["Stochastic", "gradient", "descent", "is", "a", "popular", "algorithm", "for", "training", "a", "wide", "range", "of", "models", "in", "machine", "learning", ",", "including", "(", "linear", ")", "support", "vector", "machine", "s", ",", "logistic", "regression", "(", "see", ",", "e.g.", ",", "Vowpal", "Wabbit", ")", "and", "graphical", "model", "s.Jenny", "Rose", "Finkel", ",", "Alex", "Kleeman", ",", "Christopher", "D.", "Manning", "(", "2008", ")", "."], "labels": ["B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "B-algorithm", "I-algorithm", "B-researcher", "I-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "I-researcher", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, programming language, researcher, person, metric, country, field, conference, university, task, algorithm, location, organization and O.\nSentence: Stochastic gradient descent is a popular algorithm for training a wide range of models in machine learning , including ( linear ) support vector machine s , logistic regression ( see , e.g. , Vowpal Wabbit ) and graphical model s.Jenny Rose Finkel , Alex Kleeman , Christopher D. Manning ( 2008 ) .", "prompt_labels": "Stochastic(B-algorithm) gradient(I-algorithm) descent(I-algorithm) is(O) a(O) popular(O) algorithm(O) for(O) training(O) a(O) wide(O) range(O) of(O) models(O) in(O) machine(B-field) learning(I-field) ,(O) including(O) ((O) linear(O) )(O) support(B-algorithm) vector(I-algorithm) machine(I-algorithm) s(O) ,(O) logistic(B-algorithm) regression(I-algorithm) ((O) see(O) ,(O) e.g.(O) ,(O) Vowpal(B-algorithm) Wabbit(I-algorithm) )(O) and(O) graphical(B-algorithm) model(I-algorithm) s.Jenny(B-researcher) Rose(I-researcher) Finkel(I-researcher) ,(O) Alex(B-researcher) Kleeman(I-researcher) ,(O) Christopher(B-researcher) D.(I-researcher) Manning(I-researcher) ((O) 2008(O) )(O) .(O)"}}
{"id": "70", "dataset": "crossner_ai", "split": "dev", "label_list": ["country", "product", "location", "person", "conference", "field", "researcher", "task", "programming language", "algorithm", "metric", "organization", "university"], "instance": {"id": "70", "words": ["Information", "Dissemination", "is", "also", "part", "of", "ELRA", "'s", "missions", "which", "is", "carried", "through", "both", "the", "organisation", "of", "the", "conference", "LREC", "and", "the", "Language", "Resources", "and", "Evaluation", "Journal", "edited", "by", "Springer", "."], "labels": ["B-task", "I-task", "O", "O", "O", "O", "B-conference", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "B-conference", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, product, location, person, conference, field, researcher, task, programming language, algorithm, metric, organization, university and O.\nSentence: Information Dissemination is also part of ELRA 's missions which is carried through both the organisation of the conference LREC and the Language Resources and Evaluation Journal edited by Springer .", "prompt_labels": "Information(B-task) Dissemination(I-task) is(O) also(O) part(O) of(O) ELRA(B-conference) 's(O) missions(O) which(O) is(O) carried(O) through(O) both(O) the(O) organisation(O) of(O) the(O) conference(O) LREC(B-conference) and(O) the(O) Language(B-conference) Resources(I-conference) and(I-conference) Evaluation(I-conference) Journal(I-conference) edited(O) by(O) Springer(B-conference) .(O)"}}
{"id": "172", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "person", "organization", "university", "metric", "algorithm", "programming language", "field", "country", "product", "conference", "location", "researcher"], "instance": {"id": "172", "words": ["The", "Apple", "iOS", "operating", "system", "used", "on", "the", "iPhone", ",", "iPad", "and", "iPod", "Touch", "uses", "VoiceOver", "speech", "synthesis", "accessibility", "."], "labels": ["O", "B-product", "I-product", "I-product", "I-product", "O", "O", "O", "B-product", "O", "B-product", "O", "B-product", "I-product", "O", "B-product", "I-product", "I-product", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, person, organization, university, metric, algorithm, programming language, field, country, product, conference, location, researcher and O.\nSentence: The Apple iOS operating system used on the iPhone , iPad and iPod Touch uses VoiceOver speech synthesis accessibility .", "prompt_labels": "The(O) Apple(B-product) iOS(I-product) operating(I-product) system(I-product) used(O) on(O) the(O) iPhone(B-product) ,(O) iPad(B-product) and(O) iPod(B-product) Touch(I-product) uses(O) VoiceOver(B-product) speech(I-product) synthesis(I-product) accessibility(O) .(O)"}}
{"id": "145", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "conference", "person", "country", "programming language", "task", "algorithm", "researcher", "metric", "university", "organization", "product", "field"], "instance": {"id": "145", "words": ["(", "Nevertheless", ",", "the", "ReLU", "activation", "function", ",", "which", "is", "non-differentiable", "at", "0", ",", "has", "become", "quite", "popular", ",", "e.g.", "in", "AlexNet", ")"], "labels": ["O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, conference, person, country, programming language, task, algorithm, researcher, metric, university, organization, product, field and O.\nSentence: ( Nevertheless , the ReLU activation function , which is non-differentiable at 0 , has become quite popular , e.g. in AlexNet )", "prompt_labels": "((O) Nevertheless(O) ,(O) the(O) ReLU(B-algorithm) activation(I-algorithm) function(I-algorithm) ,(O) which(O) is(O) non-differentiable(O) at(O) 0(O) ,(O) has(O) become(O) quite(O) popular(O) ,(O) e.g.(O) in(O) AlexNet(B-algorithm) )(O)"}}
{"id": "321", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "programming language", "metric", "university", "product", "field", "location", "country", "algorithm", "organization", "conference", "person", "task"], "instance": {"id": "321", "words": ["Application", "areas", "of", "kernel", "methods", "are", "diverse", "and", "include", "geostatistics", ",", "kriging", ",", "inverse", "distance", "weighting", ",", "3D", "reconstruction", ",", "bioinformatics", ",", "chemoinformatics", ",", "information", "extraction", "and", "handwriting", "recognition", "."], "labels": ["O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "B-field", "O", "B-algorithm", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-task", "I-task", "O", "B-field", "O", "B-field", "O", "B-task", "I-task", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, programming language, metric, university, product, field, location, country, algorithm, organization, conference, person, task and O.\nSentence: Application areas of kernel methods are diverse and include geostatistics , kriging , inverse distance weighting , 3D reconstruction , bioinformatics , chemoinformatics , information extraction and handwriting recognition .", "prompt_labels": "Application(O) areas(O) of(O) kernel(B-algorithm) methods(I-algorithm) are(O) diverse(O) and(O) include(O) geostatistics(B-field) ,(O) kriging(B-algorithm) ,(O) inverse(B-algorithm) distance(I-algorithm) weighting(I-algorithm) ,(O) 3D(B-task) reconstruction(I-task) ,(O) bioinformatics(B-field) ,(O) chemoinformatics(B-field) ,(O) information(B-task) extraction(I-task) and(O) handwriting(B-task) recognition(I-task) .(O)"}}
{"id": "39", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "organization", "algorithm", "university", "metric", "conference", "field", "product", "person", "location", "programming language", "task", "country"], "instance": {"id": "39", "words": ["General", "sampling", "from", "the", "truncated", "normal", "can", "be", "achieved", "using", "approximations", "to", "the", "normal", "CDF", "and", "the", "probit", "function", ",", "and", "R", "has", "a", "function", "codertnorm", "(", ")", "/", "code", "for", "generating", "truncated-normal", "samples", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "B-programming language", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, organization, algorithm, university, metric, conference, field, product, person, location, programming language, task, country and O.\nSentence: General sampling from the truncated normal can be achieved using approximations to the normal CDF and the probit function , and R has a function codertnorm ( ) / code for generating truncated-normal samples .", "prompt_labels": "General(O) sampling(O) from(O) the(O) truncated(O) normal(O) can(O) be(O) achieved(O) using(O) approximations(O) to(O) the(O) normal(O) CDF(B-algorithm) and(O) the(O) probit(B-algorithm) function(I-algorithm) ,(O) and(O) R(B-programming language) has(O) a(O) function(O) codertnorm(O) ((O) )(O) /(O) code(O) for(O) generating(O) truncated-normal(O) samples(O) .(O)"}}
{"id": "311", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "programming language", "conference", "task", "product", "person", "university", "country", "location", "organization", "field", "algorithm", "metric"], "instance": {"id": "311", "words": ["During", "the", "VEX", "Robotics", "World", "Championship", ",", "a", "Parade", "of", "Nations", "is", "held", "in", "Freedom", "Hall", "that", "includes", "hundreds", "of", "students", "from", "more", "than", "30", "countries", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, programming language, conference, task, product, person, university, country, location, organization, field, algorithm, metric and O.\nSentence: During the VEX Robotics World Championship , a Parade of Nations is held in Freedom Hall that includes hundreds of students from more than 30 countries .", "prompt_labels": "During(O) the(O) VEX(O) Robotics(O) World(O) Championship(O) ,(O) a(O) Parade(O) of(O) Nations(O) is(O) held(O) in(O) Freedom(B-location) Hall(I-location) that(O) includes(O) hundreds(O) of(O) students(O) from(O) more(O) than(O) 30(O) countries(O) .(O)"}}
{"id": "307", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "metric", "location", "product", "conference", "organization", "programming language", "university", "algorithm", "task", "field", "researcher", "country"], "instance": {"id": "307", "words": ["Collaborative", "filtering", "encompasses", "techniques", "for", "matching", "people", "with", "similar", "interests", "and", "making", "recommender", "system", "on", "this", "basis", "."], "labels": ["B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, metric, location, product, conference, organization, programming language, university, algorithm, task, field, researcher, country and O.\nSentence: Collaborative filtering encompasses techniques for matching people with similar interests and making recommender system on this basis .", "prompt_labels": "Collaborative(B-algorithm) filtering(I-algorithm) encompasses(O) techniques(O) for(O) matching(O) people(O) with(O) similar(O) interests(O) and(O) making(O) recommender(B-product) system(I-product) on(O) this(O) basis(O) .(O)"}}
{"id": "8", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "algorithm", "location", "product", "organization", "task", "researcher", "conference", "university", "country", "metric", "person", "field"], "instance": {"id": "8", "words": ["Representing", "words", "considering", "their", "context", "through", "fixed", "size", "dense", "vectors", "(", "word", "embedding", "s", ")", "has", "become", "one", "the", "most", "fundamental", "blocks", "in", "several", "NLP", "systems.", "an", "unsupervised", "disambiguation", "system", "uses", "the", "similarity", "between", "word", "senses", "in", "a", "fixed", "context", "window", "to", "select", "the", "most", "suitable", "word", "sense", "using", "a", "pre-trained", "word", "embedding", "model", "and", "WordNet", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "O", "O", "B-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, algorithm, location, product, organization, task, researcher, conference, university, country, metric, person, field and O.\nSentence: Representing words considering their context through fixed size dense vectors ( word embedding s ) has become one the most fundamental blocks in several NLP systems. an unsupervised disambiguation system uses the similarity between word senses in a fixed context window to select the most suitable word sense using a pre-trained word embedding model and WordNet .", "prompt_labels": "Representing(O) words(O) considering(O) their(O) context(O) through(O) fixed(O) size(O) dense(O) vectors(O) ((O) word(O) embedding(O) s(O) )(O) has(O) become(O) one(O) the(O) most(O) fundamental(O) blocks(O) in(O) several(O) NLP(B-field) systems.(O) an(O) unsupervised(B-product) disambiguation(I-product) system(I-product) uses(O) the(O) similarity(O) between(O) word(O) senses(O) in(O) a(O) fixed(O) context(O) window(O) to(O) select(O) the(O) most(O) suitable(O) word(O) sense(O) using(O) a(O) pre-trained(O) word(O) embedding(O) model(O) and(O) WordNet(B-product) .(O)"}}
{"id": "282", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "organization", "university", "person", "product", "algorithm", "conference", "researcher", "task", "programming language", "country", "metric", "location"], "instance": {"id": "282", "words": ["In", "practice", ",", "machine", "learning", "algorithms", "cope", "with", "that", "either", "by", "employing", "a", "convex", "approximation", "to", "the", "0-1", "loss", "function", "(", "like", "hinge", "loss", "for", "Support", "vector", "machine", ")", ",", "which", "is", "easier", "to", "optimize", ",", "or", "by", "imposing", "assumptions", "on", "the", "distribution", "mathP", "(", "x", ",", "y", ")", "/", "math", "(", "and", "thus", "stop", "being", "agnostic", "learning", "algorithms", "to", "which", "the", "above", "result", "applies", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, organization, university, person, product, algorithm, conference, researcher, task, programming language, country, metric, location and O.\nSentence: In practice , machine learning algorithms cope with that either by employing a convex approximation to the 0-1 loss function ( like hinge loss for Support vector machine ) , which is easier to optimize , or by imposing assumptions on the distribution mathP ( x , y ) / math ( and thus stop being agnostic learning algorithms to which the above result applies ) .", "prompt_labels": "In(O) practice(O) ,(O) machine(O) learning(O) algorithms(O) cope(O) with(O) that(O) either(O) by(O) employing(O) a(O) convex(B-algorithm) approximation(I-algorithm) to(O) the(O) 0-1(O) loss(O) function(O) ((O) like(O) hinge(B-metric) loss(I-metric) for(O) Support(B-algorithm) vector(I-algorithm) machine(I-algorithm) )(O) ,(O) which(O) is(O) easier(O) to(O) optimize(O) ,(O) or(O) by(O) imposing(O) assumptions(O) on(O) the(O) distribution(O) mathP(O) ((O) x(O) ,(O) y(O) )(O) /(O) math(O) ((O) and(O) thus(O) stop(O) being(O) agnostic(O) learning(O) algorithms(O) to(O) which(O) the(O) above(O) result(O) applies(O) )(O) .(O)"}}
{"id": "154", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "programming language", "university", "conference", "product", "researcher", "location", "organization", "field", "metric", "person", "country", "algorithm"], "instance": {"id": "154", "words": ["The", "commonly", "used", "metrics", "are", "the", "mean", "squared", "error", "and", "root", "mean", "squared", "error", ",", "the", "latter", "having", "been", "used", "in", "the", "Netflix", "Prize", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "B-metric", "I-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, programming language, university, conference, product, researcher, location, organization, field, metric, person, country, algorithm and O.\nSentence: The commonly used metrics are the mean squared error and root mean squared error , the latter having been used in the Netflix Prize .", "prompt_labels": "The(O) commonly(O) used(O) metrics(O) are(O) the(O) mean(B-metric) squared(I-metric) error(I-metric) and(O) root(B-metric) mean(I-metric) squared(I-metric) error(I-metric) ,(O) the(O) latter(O) having(O) been(O) used(O) in(O) the(O) Netflix(O) Prize(O) .(O)"}}
{"id": "251", "dataset": "crossner_ai", "split": "dev", "label_list": ["university", "conference", "researcher", "product", "location", "field", "algorithm", "country", "metric", "person", "organization", "programming language", "task"], "instance": {"id": "251", "words": ["Roger", "Schank", ",", "1969", ",", "A", "conceptual", "dependency", "parser", "for", "natural", "language", "Proceedings", "of", "the", "1969", "on", "Computational", "linguistics", ",", "Sång-Säby", ",", "Sweden", ",", "pages", "1-3", "This", "model", ",", "partially", "influenced", "by", "the", "work", "of", "Sydney", "Lamb", ",", "was", "extensively", "used", "by", "Schank", "'s", "students", "at", "Yale", "University", ",", "such", "as", "Robert", "Wilensky", ",", "Wendy", "Lehnert", ",", "and", "Janet", "Kolodner", "."], "labels": ["B-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-location", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O", "B-researcher", "O", "O", "O", "B-university", "I-university", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O", "B-researcher", "I-researcher", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, conference, researcher, product, location, field, algorithm, country, metric, person, organization, programming language, task and O.\nSentence: Roger Schank , 1969 , A conceptual dependency parser for natural language Proceedings of the 1969 on Computational linguistics , Sång-Säby , Sweden , pages 1-3 This model , partially influenced by the work of Sydney Lamb , was extensively used by Schank 's students at Yale University , such as Robert Wilensky , Wendy Lehnert , and Janet Kolodner .", "prompt_labels": "Roger(B-researcher) Schank(I-researcher) ,(O) 1969(O) ,(O) A(O) conceptual(O) dependency(O) parser(O) for(O) natural(O) language(O) Proceedings(B-conference) of(I-conference) the(I-conference) 1969(I-conference) on(I-conference) Computational(I-conference) linguistics(I-conference) ,(O) Sång-Säby(B-location) ,(O) Sweden(B-country) ,(O) pages(O) 1-3(O) This(O) model(O) ,(O) partially(O) influenced(O) by(O) the(O) work(O) of(O) Sydney(B-researcher) Lamb(I-researcher) ,(O) was(O) extensively(O) used(O) by(O) Schank(B-researcher) 's(O) students(O) at(O) Yale(B-university) University(I-university) ,(O) such(O) as(O) Robert(B-researcher) Wilensky(I-researcher) ,(O) Wendy(B-researcher) Lehnert(I-researcher) ,(O) and(O) Janet(B-researcher) Kolodner(I-researcher) .(O)"}}
{"id": "136", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "organization", "field", "conference", "metric", "researcher", "university", "product", "country", "task", "programming language", "location", "person"], "instance": {"id": "136", "words": ["It", "was", "first", "used", "by", "Lawrence", "J.", "Fogel", "in", "the", "US", "in", "1960", "in", "order", "to", "use", "simulated", "evolution", "as", "a", "learning", "process", "aiming", "to", "generate", "artificial", "intelligence", "."], "labels": ["O", "O", "O", "O", "O", "B-researcher", "I-researcher", "I-researcher", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, organization, field, conference, metric, researcher, university, product, country, task, programming language, location, person and O.\nSentence: It was first used by Lawrence J. Fogel in the US in 1960 in order to use simulated evolution as a learning process aiming to generate artificial intelligence .", "prompt_labels": "It(O) was(O) first(O) used(O) by(O) Lawrence(B-researcher) J.(I-researcher) Fogel(I-researcher) in(O) the(O) US(B-country) in(O) 1960(O) in(O) order(O) to(O) use(O) simulated(O) evolution(O) as(O) a(O) learning(O) process(O) aiming(O) to(O) generate(O) artificial(B-field) intelligence(I-field) .(O)"}}
{"id": "207", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "person", "product", "conference", "metric", "country", "organization", "task", "researcher", "algorithm", "university", "location", "field"], "instance": {"id": "207", "words": ["Learning", "classifier", "systems", "(", "LCS", ")", "are", "a", "family", "of", "rule-based", "machine", "learning", "algorithms", "that", "combine", "a", "discovery", "component", ",", "typically", "a", "genetic", "algorithm", ",", "with", "a", "learning", "component", ",", "performing", "either", "supervised", "learning", ",", "reinforcement", "learning", ",", "or", "unsupervised", "learning", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "O", "B-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, person, product, conference, metric, country, organization, task, researcher, algorithm, university, location, field and O.\nSentence: Learning classifier systems ( LCS ) are a family of rule-based machine learning algorithms that combine a discovery component , typically a genetic algorithm , with a learning component , performing either supervised learning , reinforcement learning , or unsupervised learning .", "prompt_labels": "Learning(O) classifier(O) systems(O) ((O) LCS(O) )(O) are(O) a(O) family(O) of(O) rule-based(O) machine(O) learning(O) algorithms(O) that(O) combine(O) a(O) discovery(O) component(O) ,(O) typically(O) a(O) genetic(B-algorithm) algorithm(I-algorithm) ,(O) with(O) a(O) learning(O) component(O) ,(O) performing(O) either(O) supervised(B-field) learning(I-field) ,(O) reinforcement(B-field) learning(I-field) ,(O) or(O) unsupervised(B-field) learning(I-field) .(O)"}}
{"id": "126", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "field", "metric", "country", "product", "task", "conference", "university", "person", "researcher", "algorithm", "location", "organization"], "instance": {"id": "126", "words": ["Speech", "recognition", "is", "an", "interdisciplinary", "subfield", "of", "computer", "science", "and", "computational", "linguistics", "that", "develops", "methodologies", "and", "technologies", "that", "enable", "the", "recognition", "and", "translation", "of", "spoken", "language", "into", "text", "by", "computers", "."], "labels": ["B-task", "I-task", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "I-task", "I-task", "I-task", "I-task", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, field, metric, country, product, task, conference, university, person, researcher, algorithm, location, organization and O.\nSentence: Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers .", "prompt_labels": "Speech(B-task) recognition(I-task) is(O) an(O) interdisciplinary(O) subfield(O) of(O) computer(B-field) science(I-field) and(O) computational(B-field) linguistics(I-field) that(O) develops(O) methodologies(O) and(O) technologies(O) that(O) enable(O) the(O) recognition(B-task) and(I-task) translation(I-task) of(I-task) spoken(I-task) language(I-task) into(O) text(O) by(O) computers(O) .(O)"}}
{"id": "344", "dataset": "crossner_ai", "split": "dev", "label_list": ["university", "algorithm", "location", "researcher", "country", "task", "product", "organization", "conference", "person", "programming language", "metric", "field"], "instance": {"id": "344", "words": ["GATE", "includes", "an", "information", "extraction", "system", "called", "ANNIE", "(", "A", "Nearly-New", "Information", "Extraction", "System", ")", "which", "is", "a", "set", "of", "modules", "comprising", "a", "tokenizer", ",", "a", "gazetteer", ",", "a", "sentence", "splitter", ",", "a", "Part-of-speech", "tagging", ",", "a", "Named", "entity", "recognition", "transducer", "and", "a", "coreference", "tagger", "."], "labels": ["B-product", "O", "O", "B-task", "I-task", "O", "O", "B-product", "O", "B-product", "I-product", "I-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "B-product", "I-product", "I-product", "I-product", "O", "O", "B-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, algorithm, location, researcher, country, task, product, organization, conference, person, programming language, metric, field and O.\nSentence: GATE includes an information extraction system called ANNIE ( A Nearly-New Information Extraction System ) which is a set of modules comprising a tokenizer , a gazetteer , a sentence splitter , a Part-of-speech tagging , a Named entity recognition transducer and a coreference tagger .", "prompt_labels": "GATE(B-product) includes(O) an(O) information(B-task) extraction(I-task) system(O) called(O) ANNIE(B-product) ((O) A(B-product) Nearly-New(I-product) Information(I-product) Extraction(I-product) System(I-product) )(O) which(O) is(O) a(O) set(O) of(O) modules(O) comprising(O) a(O) tokenizer(O) ,(O) a(O) gazetteer(O) ,(O) a(O) sentence(O) splitter(O) ,(O) a(O) Part-of-speech(B-task) tagging(I-task) ,(O) a(O) Named(B-product) entity(I-product) recognition(I-product) transducer(I-product) and(O) a(O) coreference(B-product) tagger(I-product) .(O)"}}
{"id": "135", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "algorithm", "programming language", "organization", "location", "university", "country", "researcher", "person", "field", "task", "metric", "product"], "instance": {"id": "135", "words": ["Pattern", "recognition", "systems", "are", "in", "many", "cases", "trained", "from", "labeled", "training", "data", "(", "supervised", "learning", ")", "but", "when", "no", "labeled", "data", "are", "available", "other", "algorithms", "can", "be", "used", "to", "discover", "previously", "unknown", "patterns", "(", "unsupervised", "learning", ")", "."], "labels": ["B-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, algorithm, programming language, organization, location, university, country, researcher, person, field, task, metric, product and O.\nSentence: Pattern recognition systems are in many cases trained from labeled training data ( supervised learning ) but when no labeled data are available other algorithms can be used to discover previously unknown patterns ( unsupervised learning ) .", "prompt_labels": "Pattern(B-product) recognition(I-product) systems(I-product) are(O) in(O) many(O) cases(O) trained(O) from(O) labeled(O) training(O) data(O) ((O) supervised(B-field) learning(I-field) )(O) but(O) when(O) no(O) labeled(O) data(O) are(O) available(O) other(O) algorithms(O) can(O) be(O) used(O) to(O) discover(O) previously(O) unknown(O) patterns(O) ((O) unsupervised(B-field) learning(I-field) )(O) .(O)"}}
{"id": "254", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "product", "researcher", "location", "programming language", "metric", "algorithm", "task", "country", "university", "conference", "organization", "person"], "instance": {"id": "254", "words": ["SURF", "was", "first", "published", "by", "Herbert", "Bay", ",", "Tinne", "Tuytelaars", ",", "and", "Luc", "Van", "Gool", ",", "and", "presented", "at", "the", "2006", "European", "Conference", "on", "Computer", "Vision", "."], "labels": ["B-product", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O", "B-researcher", "I-researcher", "I-researcher", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, product, researcher, location, programming language, metric, algorithm, task, country, university, conference, organization, person and O.\nSentence: SURF was first published by Herbert Bay , Tinne Tuytelaars , and Luc Van Gool , and presented at the 2006 European Conference on Computer Vision .", "prompt_labels": "SURF(B-product) was(O) first(O) published(O) by(O) Herbert(B-researcher) Bay(I-researcher) ,(O) Tinne(B-researcher) Tuytelaars(I-researcher) ,(O) and(O) Luc(B-researcher) Van(I-researcher) Gool(I-researcher) ,(O) and(O) presented(O) at(O) the(O) 2006(B-conference) European(I-conference) Conference(I-conference) on(I-conference) Computer(I-conference) Vision(I-conference) .(O)"}}
{"id": "298", "dataset": "crossner_ai", "split": "dev", "label_list": ["university", "programming language", "task", "researcher", "metric", "conference", "organization", "person", "location", "field", "algorithm", "product", "country"], "instance": {"id": "298", "words": ["In", "computer", "science", ",", "computational", "learning", "theory", "(", "or", "just", "learning", "theory", ")", "is", "a", "subfield", "of", "artificial", "intelligence", "devoted", "to", "studying", "the", "design", "and", "analysis", "of", "machine", "learning", "algorithms", "."], "labels": ["O", "B-field", "I-field", "O", "B-field", "I-field", "I-field", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, programming language, task, researcher, metric, conference, organization, person, location, field, algorithm, product, country and O.\nSentence: In computer science , computational learning theory ( or just learning theory ) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms .", "prompt_labels": "In(O) computer(B-field) science(I-field) ,(O) computational(B-field) learning(I-field) theory(I-field) ((O) or(O) just(O) learning(B-field) theory(I-field) )(O) is(O) a(O) subfield(O) of(O) artificial(B-field) intelligence(I-field) devoted(O) to(O) studying(O) the(O) design(O) and(O) analysis(O) of(O) machine(B-field) learning(I-field) algorithms(O) .(O)"}}
{"id": "333", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "metric", "university", "task", "field", "organization", "algorithm", "conference", "researcher", "programming language", "location", "product", "country"], "instance": {"id": "333", "words": ["The", "true-positive", "rate", "is", "also", "known", "as", "sensitivity", ",", "recall", "or", "probability", "of", "detection", "math", "to", "the", "discrimination", "threshold", ")", "of", "the", "detection", "probability", "in", "the", "y-axis", "versus", "the", "cumulative", "distribution", "function", "of", "the", "false-alarm", "probability", "on", "the", "x-axis", "."], "labels": ["O", "B-metric", "I-metric", "O", "O", "O", "O", "B-metric", "O", "B-metric", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "B-metric", "I-metric", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, metric, university, task, field, organization, algorithm, conference, researcher, programming language, location, product, country and O.\nSentence: The true-positive rate is also known as sensitivity , recall or probability of detection math to the discrimination threshold ) of the detection probability in the y-axis versus the cumulative distribution function of the false-alarm probability on the x-axis .", "prompt_labels": "The(O) true-positive(B-metric) rate(I-metric) is(O) also(O) known(O) as(O) sensitivity(B-metric) ,(O) recall(B-metric) or(O) probability(B-metric) of(I-metric) detection(I-metric) math(O) to(O) the(O) discrimination(O) threshold(O) )(O) of(O) the(O) detection(O) probability(O) in(O) the(O) y-axis(O) versus(O) the(O) cumulative(B-algorithm) distribution(I-algorithm) function(I-algorithm) of(O) the(O) false-alarm(B-metric) probability(I-metric) on(O) the(O) x-axis(O) .(O)"}}
{"id": "245", "dataset": "crossner_ai", "split": "dev", "label_list": ["university", "country", "location", "programming language", "product", "researcher", "task", "field", "person", "organization", "conference", "metric", "algorithm"], "instance": {"id": "245", "words": ["Applications", "of", "DSP", "include", "audio", "signal", "processing", ",", "audio", "compression", ",", "digital", "image", "processing", ",", "video", "compression", ",", "speech", "processing", ",", "speech", "recognition", ",", "digital", "communication", "s", ",", "digital", "synthesizer", "s", ",", "radar", ",", "sonar", ",", "financial", "signal", "processing", ",", "seismology", "and", "biomedicine", "."], "labels": ["O", "O", "B-field", "O", "B-task", "I-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "B-task", "B-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "O", "B-task", "I-task", "O", "O", "B-field", "O", "B-field", "O", "B-field", "I-field", "I-field", "O", "B-field", "O", "B-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, country, location, programming language, product, researcher, task, field, person, organization, conference, metric, algorithm and O.\nSentence: Applications of DSP include audio signal processing , audio compression , digital image processing , video compression , speech processing , speech recognition , digital communication s , digital synthesizer s , radar , sonar , financial signal processing , seismology and biomedicine .", "prompt_labels": "Applications(O) of(O) DSP(B-field) include(O) audio(B-task) signal(I-task) processing(I-task) ,(O) audio(B-task) compression(I-task) ,(O) digital(B-task) image(B-task) processing(B-task) ,(O) video(B-task) compression(I-task) ,(O) speech(B-task) processing(I-task) ,(O) speech(B-task) recognition(I-task) ,(O) digital(B-task) communication(I-task) s(O) ,(O) digital(B-task) synthesizer(I-task) s(O) ,(O) radar(B-field) ,(O) sonar(B-field) ,(O) financial(B-field) signal(I-field) processing(I-field) ,(O) seismology(B-field) and(O) biomedicine(B-field) .(O)"}}
{"id": "334", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "product", "field", "task", "country", "metric", "programming language", "algorithm", "location", "organization", "researcher", "university", "person"], "instance": {"id": "334", "words": ["In", "English", ",", "WordNet", "is", "an", "example", "of", "a", "semantic", "network", "."], "labels": ["O", "O", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, product, field, task, country, metric, programming language, algorithm, location, organization, researcher, university, person and O.\nSentence: In English , WordNet is an example of a semantic network .", "prompt_labels": "In(O) English(O) ,(O) WordNet(B-product) is(O) an(O) example(O) of(O) a(O) semantic(O) network(O) .(O)"}}
{"id": "233", "dataset": "crossner_ai", "split": "dev", "label_list": ["organization", "country", "university", "location", "conference", "algorithm", "field", "metric", "product", "programming language", "person", "researcher", "task"], "instance": {"id": "233", "words": ["This", "was", "won", "by", "an", "United", "States", "team", "from", "Newton", "Labs", ",", "and", "the", "competition", "was", "shown", "on", "CNN", "."], "labels": ["O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, university, location, conference, algorithm, field, metric, product, programming language, person, researcher, task and O.\nSentence: This was won by an United States team from Newton Labs , and the competition was shown on CNN .", "prompt_labels": "This(O) was(O) won(O) by(O) an(O) United(B-country) States(I-country) team(O) from(O) Newton(B-organization) Labs(I-organization) ,(O) and(O) the(O) competition(O) was(O) shown(O) on(O) CNN(B-organization) .(O)"}}
{"id": "139", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "person", "university", "algorithm", "organization", "field", "researcher", "location", "country", "product", "metric", "conference", "programming language"], "instance": {"id": "139", "words": ["One", "of", "the", "first", "versions", "of", "the", "theorem", "was", "proved", "by", "George", "Cybenko", "in", "1989", "for", "sigmoid", "function", "activation", "functions.", "Cybenko", "G.", "(", "1989", ")", ",", "2", "(", "4", ")", ",", "303-314", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, person, university, algorithm, organization, field, researcher, location, country, product, metric, conference, programming language and O.\nSentence: One of the first versions of the theorem was proved by George Cybenko in 1989 for sigmoid function activation functions. Cybenko G. ( 1989 ) , 2 ( 4 ) , 303-314 .", "prompt_labels": "One(O) of(O) the(O) first(O) versions(O) of(O) the(O) theorem(O) was(O) proved(O) by(O) George(B-researcher) Cybenko(I-researcher) in(O) 1989(O) for(O) sigmoid(B-algorithm) function(I-algorithm) activation(O) functions.(O) Cybenko(B-researcher) G.(I-researcher) ((O) 1989(O) )(O) ,(O) 2(O) ((O) 4(O) )(O) ,(O) 303-314(O) .(O)"}}
{"id": "247", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "researcher", "programming language", "conference", "product", "person", "algorithm", "task", "location", "country", "organization", "metric", "university"], "instance": {"id": "247", "words": ["With", "David", "E.", "Rumelhart", "and", "Ronald", "J.", "Williams", ",", "Hinton", "was", "co-author", "of", "a", "highly", "cited", "paper", "published", "in", "1986", "that", "popularized", "the", "backpropagation", "algorithm", "for", "training", "multi-layer", "neural", "networks", ",", "The", "dramatic", "image-recognition", "milestone", "of", "the", "AlexNet", "designed", "by", "his", "student", "Alex", "Krizhevsky", "{", "{", "cite", "web"], "labels": ["O", "B-researcher", "I-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "I-researcher", "O", "B-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "B-task", "O", "O", "O", "B-algorithm", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, researcher, programming language, conference, product, person, algorithm, task, location, country, organization, metric, university and O.\nSentence: With David E. Rumelhart and Ronald J. Williams , Hinton was co-author of a highly cited paper published in 1986 that popularized the backpropagation algorithm for training multi-layer neural networks , The dramatic image-recognition milestone of the AlexNet designed by his student Alex Krizhevsky { { cite web", "prompt_labels": "With(O) David(B-researcher) E.(I-researcher) Rumelhart(I-researcher) and(O) Ronald(B-researcher) J.(I-researcher) Williams(I-researcher) ,(O) Hinton(B-researcher) was(O) co-author(O) of(O) a(O) highly(O) cited(O) paper(O) published(O) in(O) 1986(O) that(O) popularized(O) the(O) backpropagation(B-algorithm) algorithm(I-algorithm) for(O) training(O) multi-layer(B-algorithm) neural(I-algorithm) networks(I-algorithm) ,(O) The(O) dramatic(O) image-recognition(B-task) milestone(O) of(O) the(O) AlexNet(B-algorithm) designed(O) by(O) his(O) student(O) Alex(B-researcher) Krizhevsky(I-researcher) {(O) {(O) cite(O) web(O)"}}
{"id": "120", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "algorithm", "programming language", "researcher", "university", "location", "product", "country", "metric", "task", "field", "organization", "conference"], "instance": {"id": "120", "words": ["He", "was", "one", "of", "the", "founding", "members", "and", "former", "chair", "(", "2006-2008", ")", "of", "the", "Special", "Interest", "Group", "on", "Web", "as", "Corpus", "(", "SIGWAC", ")", "of", "the", "Association", "for", "Computational", "Linguistics", "and", "also", "one", "of", "the", "founding", "organizers", "of", "SENSEVAL", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, algorithm, programming language, researcher, university, location, product, country, metric, task, field, organization, conference and O.\nSentence: He was one of the founding members and former chair ( 2006-2008 ) of the Special Interest Group on Web as Corpus ( SIGWAC ) of the Association for Computational Linguistics and also one of the founding organizers of SENSEVAL .", "prompt_labels": "He(O) was(O) one(O) of(O) the(O) founding(O) members(O) and(O) former(O) chair(O) ((O) 2006-2008(O) )(O) of(O) the(O) Special(B-conference) Interest(I-conference) Group(I-conference) on(I-conference) Web(I-conference) as(I-conference) Corpus(I-conference) ((O) SIGWAC(B-conference) )(O) of(O) the(O) Association(B-conference) for(I-conference) Computational(I-conference) Linguistics(I-conference) and(O) also(O) one(O) of(O) the(O) founding(O) organizers(O) of(O) SENSEVAL(B-conference) .(O)"}}
{"id": "18", "dataset": "crossner_ai", "split": "dev", "label_list": ["metric", "country", "university", "field", "person", "location", "product", "algorithm", "researcher", "task", "programming language", "organization", "conference"], "instance": {"id": "18", "words": ["The", "company", "was", "founded", "by", "Kiichiro", "Toyoda", "in", "1937", ",", "as", "a", "spinoff", "from", "Sakichi", "Toyoda", "company", "Toyota", "Industries", "to", "create", "automobiles", "."], "labels": ["O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-organization", "I-organization", "O", "O", "B-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, country, university, field, person, location, product, algorithm, researcher, task, programming language, organization, conference and O.\nSentence: The company was founded by Kiichiro Toyoda in 1937 , as a spinoff from Sakichi Toyoda company Toyota Industries to create automobiles .", "prompt_labels": "The(O) company(O) was(O) founded(O) by(O) Kiichiro(B-person) Toyoda(I-person) in(O) 1937(O) ,(O) as(O) a(O) spinoff(O) from(O) Sakichi(B-person) Toyoda(I-person) company(O) Toyota(B-organization) Industries(I-organization) to(O) create(O) automobiles(B-product) .(O)"}}
{"id": "176", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "product", "task", "researcher", "country", "algorithm", "conference", "field", "programming language", "university", "metric", "person", "organization"], "instance": {"id": "176", "words": ["Generally", "speaking", "all", "learning", "displays", "incremental", "change", "over", "time", ",", "but", "describes", "an", "Sigmoid", "function", "which", "has", "different", "appearances", "depending", "on", "the", "time", "scale", "of", "observation", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, product, task, researcher, country, algorithm, conference, field, programming language, university, metric, person, organization and O.\nSentence: Generally speaking all learning displays incremental change over time , but describes an Sigmoid function which has different appearances depending on the time scale of observation .", "prompt_labels": "Generally(O) speaking(O) all(O) learning(O) displays(O) incremental(O) change(O) over(O) time(O) ,(O) but(O) describes(O) an(O) Sigmoid(B-algorithm) function(I-algorithm) which(O) has(O) different(O) appearances(O) depending(O) on(O) the(O) time(O) scale(O) of(O) observation(O) .(O)"}}
{"id": "101", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "algorithm", "organization", "person", "task", "researcher", "field", "programming language", "product", "university", "metric", "location", "country"], "instance": {"id": "101", "words": ["Techniques", "such", "as", "dynamic", "Markov", "Networks", ",", "Convolutional", "neural", "network", "and", "Long", "short-term", "memory", "are", "often", "employed", "to", "exploit", "the", "inter-frame", "correlations", "."], "labels": ["O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, algorithm, organization, person, task, researcher, field, programming language, product, university, metric, location, country and O.\nSentence: Techniques such as dynamic Markov Networks , Convolutional neural network and Long short-term memory are often employed to exploit the inter-frame correlations .", "prompt_labels": "Techniques(O) such(O) as(O) dynamic(B-algorithm) Markov(I-algorithm) Networks(I-algorithm) ,(O) Convolutional(B-algorithm) neural(I-algorithm) network(I-algorithm) and(O) Long(B-algorithm) short-term(I-algorithm) memory(I-algorithm) are(O) often(O) employed(O) to(O) exploit(O) the(O) inter-frame(O) correlations(O) .(O)"}}
{"id": "326", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "field", "product", "task", "organization", "person", "metric", "conference", "programming language", "university", "location", "country", "algorithm"], "instance": {"id": "326", "words": ["In", "1969", "Victor", "Scheinman", "at", "Stanford", "University", "invented", "the", "Stanford", "arm", ",", "an", "all-electric", ",", "6-axis", "articulated", "robot", "designed", "to", "permit", "an", "arm", "solution", "."], "labels": ["O", "O", "B-researcher", "I-researcher", "O", "B-university", "I-university", "O", "O", "B-product", "I-product", "O", "O", "O", "O", "B-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, field, product, task, organization, person, metric, conference, programming language, university, location, country, algorithm and O.\nSentence: In 1969 Victor Scheinman at Stanford University invented the Stanford arm , an all-electric , 6-axis articulated robot designed to permit an arm solution .", "prompt_labels": "In(O) 1969(O) Victor(B-researcher) Scheinman(I-researcher) at(O) Stanford(B-university) University(I-university) invented(O) the(O) Stanford(B-product) arm(I-product) ,(O) an(O) all-electric(O) ,(O) 6-axis(B-product) articulated(I-product) robot(I-product) designed(O) to(O) permit(O) an(O) arm(O) solution(O) .(O)"}}
{"id": "78", "dataset": "crossner_ai", "split": "dev", "label_list": ["university", "field", "organization", "location", "metric", "country", "product", "person", "programming language", "algorithm", "conference", "researcher", "task"], "instance": {"id": "78", "words": ["Text", "analysis", "involves", "information", "retrieval", ",", "lexical", "analysis", "to", "study", "word", "frequency", "distributions", ",", "pattern", "recognition", ",", "tagging", "/", "annotation", ",", "information", "extraction", ",", "data", "mining", "techniques", "including", "link", "and", "association", "analysis", ",", "visualization", ",", "and", "predictive", "analytics", "."], "labels": ["B-field", "I-field", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-task", "I-task", "I-task", "O", "B-task", "I-task", "O", "B-field", "I-field", "O", "O", "B-task", "I-task", "I-task", "I-task", "O", "B-task", "O", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, field, organization, location, metric, country, product, person, programming language, algorithm, conference, researcher, task and O.\nSentence: Text analysis involves information retrieval , lexical analysis to study word frequency distributions , pattern recognition , tagging / annotation , information extraction , data mining techniques including link and association analysis , visualization , and predictive analytics .", "prompt_labels": "Text(B-field) analysis(I-field) involves(O) information(B-task) retrieval(I-task) ,(O) lexical(B-task) analysis(I-task) to(O) study(O) word(O) frequency(O) distributions(O) ,(O) pattern(B-field) recognition(I-field) ,(O) tagging(B-task) /(I-task) annotation(I-task) ,(O) information(B-task) extraction(I-task) ,(O) data(B-field) mining(I-field) techniques(O) including(O) link(B-task) and(I-task) association(I-task) analysis(I-task) ,(O) visualization(B-task) ,(O) and(O) predictive(B-task) analytics(I-task) .(O)"}}
{"id": "268", "dataset": "crossner_ai", "split": "dev", "label_list": ["product", "person", "university", "organization", "location", "researcher", "metric", "algorithm", "field", "task", "conference", "programming language", "country"], "instance": {"id": "268", "words": ["It", "has", "been", "applied", "successfully", "to", "various", "problems", ",", "including", "robot", "control", ",", "elevator", "scheduling", ",", "telecommunications", ",", ",", "checkers", "and", "Go", "(", "AlphaGo", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "O", "O", "B-task", "O", "B-task", "O", "B-product", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, person, university, organization, location, researcher, metric, algorithm, field, task, conference, programming language, country and O.\nSentence: It has been applied successfully to various problems , including robot control , elevator scheduling , telecommunications , , checkers and Go ( AlphaGo ) .", "prompt_labels": "It(O) has(O) been(O) applied(O) successfully(O) to(O) various(O) problems(O) ,(O) including(O) robot(B-task) control(I-task) ,(O) elevator(B-task) scheduling(I-task) ,(O) telecommunications(B-task) ,(O) ,(O) checkers(B-task) and(O) Go(B-task) ((O) AlphaGo(B-product) )(O) .(O)"}}
{"id": "48", "dataset": "crossner_ai", "split": "dev", "label_list": ["product", "researcher", "metric", "task", "country", "location", "conference", "programming language", "field", "algorithm", "person", "organization", "university"], "instance": {"id": "48", "words": ["Due", "to", "limits", "in", "computing", "power", ",", "current", "in", "silico", "methods", "usually", "must", "trade", "speed", "for", "accuracy", ";", "e.g.", ",", "use", "rapid", "protein", "docking", "methods", "instead", "of", "computationally", "costly", "free", "energy", "calculation", "s", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, researcher, metric, task, country, location, conference, programming language, field, algorithm, person, organization, university and O.\nSentence: Due to limits in computing power , current in silico methods usually must trade speed for accuracy ; e.g. , use rapid protein docking methods instead of computationally costly free energy calculation s .", "prompt_labels": "Due(O) to(O) limits(O) in(O) computing(O) power(O) ,(O) current(O) in(O) silico(O) methods(O) usually(O) must(O) trade(O) speed(O) for(O) accuracy(B-metric) ;(O) e.g.(O) ,(O) use(O) rapid(O) protein(B-algorithm) docking(I-algorithm) methods(O) instead(O) of(O) computationally(O) costly(O) free(B-algorithm) energy(I-algorithm) calculation(I-algorithm) s(O) .(O)"}}
{"id": "210", "dataset": "crossner_ai", "split": "dev", "label_list": ["organization", "field", "algorithm", "metric", "task", "university", "researcher", "product", "country", "person", "location", "programming language", "conference"], "instance": {"id": "210", "words": ["An", "illustration", "of", "their", "capabilities", "is", "given", "by", "the", "ImageNet", "Large", "Scale", "Visual", "Recognition", "Challenge", ";", "this", "is", "a", "benchmark", "in", "object", "classification", "and", "detection", ",", "with", "millions", "of", "images", "and", "hundreds", "of", "object", "classes", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "I-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, field, algorithm, metric, task, university, researcher, product, country, person, location, programming language, conference and O.\nSentence: An illustration of their capabilities is given by the ImageNet Large Scale Visual Recognition Challenge ; this is a benchmark in object classification and detection , with millions of images and hundreds of object classes .", "prompt_labels": "An(O) illustration(O) of(O) their(O) capabilities(O) is(O) given(O) by(O) the(O) ImageNet(B-conference) Large(I-conference) Scale(I-conference) Visual(I-conference) Recognition(I-conference) Challenge(I-conference) ;(O) this(O) is(O) a(O) benchmark(O) in(O) object(B-task) classification(I-task) and(I-task) detection(I-task) ,(O) with(O) millions(O) of(O) images(O) and(O) hundreds(O) of(O) object(O) classes(O) .(O)"}}
{"id": "337", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "person", "programming language", "field", "metric", "university", "country", "organization", "algorithm", "task", "location", "conference", "product"], "instance": {"id": "337", "words": ["Their", "'", "parallel", "'", "distinction", ",", "as", "opposed", "to", "a", "serial", "manipulator", ",", "is", "that", "the", "end", "effector", "(", "or", "'", "hand", "'", ")", "of", "this", "linkage", "(", "or", "'", "arm", "'", ")", "is", "directly", "connected", "to", "its", "base", "by", "a", "number", "of", "(", "usually", "three", "or", "six", ")", "separate", "and", "independent", "linkages", "working", "simultaneously", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, person, programming language, field, metric, university, country, organization, algorithm, task, location, conference, product and O.\nSentence: Their ' parallel ' distinction , as opposed to a serial manipulator , is that the end effector ( or ' hand ' ) of this linkage ( or ' arm ' ) is directly connected to its base by a number of ( usually three or six ) separate and independent linkages working simultaneously .", "prompt_labels": "Their(O) '(O) parallel(O) '(O) distinction(O) ,(O) as(O) opposed(O) to(O) a(O) serial(B-product) manipulator(I-product) ,(O) is(O) that(O) the(O) end(O) effector(O) ((O) or(O) '(O) hand(O) '(O) )(O) of(O) this(O) linkage(O) ((O) or(O) '(O) arm(O) '(O) )(O) is(O) directly(O) connected(O) to(O) its(O) base(O) by(O) a(O) number(O) of(O) ((O) usually(O) three(O) or(O) six(O) )(O) separate(O) and(O) independent(O) linkages(O) working(O) simultaneously(O) .(O)"}}
{"id": "38", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "algorithm", "location", "field", "product", "programming language", "task", "organization", "metric", "person", "country", "researcher", "university"], "instance": {"id": "38", "words": ["Image", "segmentation", "using", "k-means", "clustering", "algorithms", "has", "long", "been", "used", "for", "pattern", "recognition", ",", "object", "detection", ",", "and", "medical", "imaging", "."], "labels": ["B-task", "I-task", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-task", "I-task", "O", "O", "B-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, algorithm, location, field, product, programming language, task, organization, metric, person, country, researcher, university and O.\nSentence: Image segmentation using k-means clustering algorithms has long been used for pattern recognition , object detection , and medical imaging .", "prompt_labels": "Image(B-task) segmentation(I-task) using(O) k-means(B-algorithm) clustering(I-algorithm) algorithms(I-algorithm) has(O) long(O) been(O) used(O) for(O) pattern(B-field) recognition(I-field) ,(O) object(B-task) detection(I-task) ,(O) and(O) medical(B-field) imaging(I-field) .(O)"}}
{"id": "29", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "field", "algorithm", "metric", "task", "product", "country", "university", "location", "conference", "researcher", "organization", "programming language"], "instance": {"id": "29", "words": ["The", "EM", "algorithm", "is", "used", "to", "find", "(", "local", ")", "maximum", "likelihood", "parameters", "of", "a", "statistical", "model", "in", "cases", "where", "the", "equations", "cannot", "be", "solved", "directly", "."], "labels": ["O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, field, algorithm, metric, task, product, country, university, location, conference, researcher, organization, programming language and O.\nSentence: The EM algorithm is used to find ( local ) maximum likelihood parameters of a statistical model in cases where the equations cannot be solved directly .", "prompt_labels": "The(O) EM(B-algorithm) algorithm(I-algorithm) is(O) used(O) to(O) find(O) ((O) local(O) )(O) maximum(B-metric) likelihood(I-metric) parameters(O) of(O) a(O) statistical(O) model(O) in(O) cases(O) where(O) the(O) equations(O) cannot(O) be(O) solved(O) directly(O) .(O)"}}
{"id": "44", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "university", "location", "organization", "algorithm", "metric", "researcher", "country", "programming language", "product", "conference", "task", "field"], "instance": {"id": "44", "words": ["Two", "professors", ",", "Hal", "Abelson", "and", "Gerald", "Jay", "Sussman", ",", "chose", "to", "remain", "neutral", "-", "their", "group", "was", "referred", "to", "variously", "as", "Switzerland", "and", "Project", "MAC", "for", "the", "next", "30", "years", "."], "labels": ["O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, university, location, organization, algorithm, metric, researcher, country, programming language, product, conference, task, field and O.\nSentence: Two professors , Hal Abelson and Gerald Jay Sussman , chose to remain neutral - their group was referred to variously as Switzerland and Project MAC for the next 30 years .", "prompt_labels": "Two(O) professors(O) ,(O) Hal(B-researcher) Abelson(I-researcher) and(O) Gerald(B-researcher) Jay(I-researcher) Sussman(I-researcher) ,(O) chose(O) to(O) remain(O) neutral(O) -(O) their(O) group(O) was(O) referred(O) to(O) variously(O) as(O) Switzerland(B-country) and(O) Project(O) MAC(O) for(O) the(O) next(O) 30(O) years(O) .(O)"}}
{"id": "62", "dataset": "crossner_ai", "split": "dev", "label_list": ["metric", "person", "researcher", "field", "organization", "university", "programming language", "country", "algorithm", "conference", "location", "product", "task"], "instance": {"id": "62", "words": ["New", "features", "in", "Office", "XP", "include", "smart", "tags", ",", "a", "selection-based", "search", "feature", "that", "recognizes", "different", "types", "of", "text", "in", "a", "document", "so", "that", "users", "can", "perform", "additional", "actions", ";", "a", "task", "pane", "interface", "that", "consolidates", "popular", "menu", "bar", "commands", "on", "the", "right", "side", "of", "the", "screen", "to", "facilitate", "quick", "access", "to", "them", ";", "new", "document", "collaboration", "capabilities", ",", "support", "for", "MSN", "Groups", "and", "SharePoint", ";", "and", "integrated", "handwriting", "recognition", "and", "speech", "recognition", "capabilities", "."], "labels": ["O", "O", "O", "B-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "O", "B-product", "I-product", "O", "B-product", "O", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, person, researcher, field, organization, university, programming language, country, algorithm, conference, location, product, task and O.\nSentence: New features in Office XP include smart tags , a selection-based search feature that recognizes different types of text in a document so that users can perform additional actions ; a task pane interface that consolidates popular menu bar commands on the right side of the screen to facilitate quick access to them ; new document collaboration capabilities , support for MSN Groups and SharePoint ; and integrated handwriting recognition and speech recognition capabilities .", "prompt_labels": "New(O) features(O) in(O) Office(B-product) XP(I-product) include(O) smart(O) tags(O) ,(O) a(O) selection-based(O) search(O) feature(O) that(O) recognizes(O) different(O) types(O) of(O) text(O) in(O) a(O) document(O) so(O) that(O) users(O) can(O) perform(O) additional(O) actions(O) ;(O) a(O) task(O) pane(O) interface(O) that(O) consolidates(O) popular(O) menu(O) bar(O) commands(O) on(O) the(O) right(O) side(O) of(O) the(O) screen(O) to(O) facilitate(O) quick(O) access(O) to(O) them(O) ;(O) new(O) document(B-task) collaboration(I-task) capabilities(O) ,(O) support(O) for(O) MSN(B-product) Groups(I-product) and(O) SharePoint(B-product) ;(O) and(O) integrated(O) handwriting(B-task) recognition(I-task) and(O) speech(B-task) recognition(I-task) capabilities(O) .(O)"}}
{"id": "66", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "programming language", "person", "conference", "location", "field", "university", "organization", "metric", "researcher", "task", "product", "country"], "instance": {"id": "66", "words": ["An", "updated", "measurement", "noise", "variance", "estimate", "can", "be", "obtained", "from", "the", "maximum", "likelihood", "calculation"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, programming language, person, conference, location, field, university, organization, metric, researcher, task, product, country and O.\nSentence: An updated measurement noise variance estimate can be obtained from the maximum likelihood calculation", "prompt_labels": "An(O) updated(O) measurement(O) noise(O) variance(O) estimate(O) can(O) be(O) obtained(O) from(O) the(O) maximum(B-algorithm) likelihood(I-algorithm) calculation(O)"}}
{"id": "320", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "product", "algorithm", "conference", "organization", "researcher", "programming language", "location", "metric", "country", "field", "university", "task"], "instance": {"id": "320", "words": ["The", "basic", "concepts", "involved", "in", "spectral", "estimation", "include", "autocorrelation", ",", "multi-D", "Fourier", "transform", ",", "mean", "square", "error", "and", "entropy", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-metric", "I-metric", "I-metric", "O", "B-metric", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, product, algorithm, conference, organization, researcher, programming language, location, metric, country, field, university, task and O.\nSentence: The basic concepts involved in spectral estimation include autocorrelation , multi-D Fourier transform , mean square error and entropy .", "prompt_labels": "The(O) basic(O) concepts(O) involved(O) in(O) spectral(O) estimation(O) include(O) autocorrelation(B-algorithm) ,(O) multi-D(B-algorithm) Fourier(I-algorithm) transform(I-algorithm) ,(O) mean(B-metric) square(I-metric) error(I-metric) and(O) entropy(B-metric) .(O)"}}
{"id": "91", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "field", "country", "university", "conference", "organization", "metric", "algorithm", "product", "person", "programming language", "location", "researcher"], "instance": {"id": "91", "words": ["Within", "20", "minutes", ",", "a", "facial", "recognition", "system", "identifies", "personal", "information", "including", "family", "name", ",", "ID", "number", "and", "address", "which", "are", "displayed", "in", "the", "street", "on", "an", "advertising", "screen", "."], "labels": ["O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, field, country, university, conference, organization, metric, algorithm, product, person, programming language, location, researcher and O.\nSentence: Within 20 minutes , a facial recognition system identifies personal information including family name , ID number and address which are displayed in the street on an advertising screen .", "prompt_labels": "Within(O) 20(O) minutes(O) ,(O) a(O) facial(B-product) recognition(I-product) system(I-product) identifies(O) personal(O) information(O) including(O) family(O) name(O) ,(O) ID(O) number(O) and(O) address(O) which(O) are(O) displayed(O) in(O) the(O) street(O) on(O) an(O) advertising(O) screen(O) .(O)"}}
{"id": "67", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "person", "organization", "country", "programming language", "location", "university", "researcher", "metric", "field", "algorithm", "task", "product"], "instance": {"id": "67", "words": ["In", "machine", "learning", ",", "the", "perceptron", "is", "an", "algorithm", "for", "supervised", "learning", "of", "binary", "classification", "."], "labels": ["O", "B-field", "I-field", "O", "O", "B-algorithm", "O", "O", "O", "O", "B-field", "I-field", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, person, organization, country, programming language, location, university, researcher, metric, field, algorithm, task, product and O.\nSentence: In machine learning , the perceptron is an algorithm for supervised learning of binary classification .", "prompt_labels": "In(O) machine(B-field) learning(I-field) ,(O) the(O) perceptron(B-algorithm) is(O) an(O) algorithm(O) for(O) supervised(B-field) learning(I-field) of(O) binary(B-task) classification(I-task) .(O)"}}
{"id": "96", "dataset": "crossner_ai", "split": "dev", "label_list": ["organization", "location", "university", "programming language", "country", "algorithm", "person", "researcher", "product", "task", "metric", "field", "conference"], "instance": {"id": "96", "words": ["Evolutionary", "programming", "was", "introduced", "by", "Lawrence", "J.", "Fogel", "in", "the", "US", ",", "while", "John", "Henry", "Holland", "called", "his", "method", "a", "genetic", "algorithm", "."], "labels": ["B-algorithm", "I-algorithm", "O", "O", "O", "B-researcher", "I-researcher", "I-researcher", "O", "O", "B-country", "O", "O", "B-researcher", "I-researcher", "I-researcher", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, university, programming language, country, algorithm, person, researcher, product, task, metric, field, conference and O.\nSentence: Evolutionary programming was introduced by Lawrence J. Fogel in the US , while John Henry Holland called his method a genetic algorithm .", "prompt_labels": "Evolutionary(B-algorithm) programming(I-algorithm) was(O) introduced(O) by(O) Lawrence(B-researcher) J.(I-researcher) Fogel(I-researcher) in(O) the(O) US(B-country) ,(O) while(O) John(B-researcher) Henry(I-researcher) Holland(I-researcher) called(O) his(O) method(O) a(O) genetic(B-algorithm) algorithm(I-algorithm) .(O)"}}
{"id": "137", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "organization", "field", "university", "metric", "location", "person", "task", "country", "product", "programming language", "algorithm", "researcher"], "instance": {"id": "137", "words": ["Reinforcement", "learning", "is", "one", "of", "three", "basic", "machine", "learning", "paradigms", ",", "alongside", "supervised", "learning", "and", "unsupervised", "learning", "."], "labels": ["B-field", "I-field", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, organization, field, university, metric, location, person, task, country, product, programming language, algorithm, researcher and O.\nSentence: Reinforcement learning is one of three basic machine learning paradigms , alongside supervised learning and unsupervised learning .", "prompt_labels": "Reinforcement(B-field) learning(I-field) is(O) one(O) of(O) three(O) basic(O) machine(B-field) learning(I-field) paradigms(O) ,(O) alongside(O) supervised(B-field) learning(I-field) and(O) unsupervised(B-field) learning(I-field) .(O)"}}
{"id": "297", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "location", "algorithm", "university", "programming language", "task", "product", "researcher", "person", "metric", "country", "organization", "field"], "instance": {"id": "297", "words": ["Formal", "concept", "analysis", "finds", "practical", "application", "in", "fields", "including", "data", "mining", ",", "text", "mining", ",", "machine", "learning", ",", "knowledge", "management", ",", "semantic", "web", ",", "software", "development", ",", "chemistry", "and", "biology", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "B-field", "O", "B-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, location, algorithm, university, programming language, task, product, researcher, person, metric, country, organization, field and O.\nSentence: Formal concept analysis finds practical application in fields including data mining , text mining , machine learning , knowledge management , semantic web , software development , chemistry and biology .", "prompt_labels": "Formal(O) concept(O) analysis(O) finds(O) practical(O) application(O) in(O) fields(O) including(O) data(B-field) mining(I-field) ,(O) text(B-field) mining(I-field) ,(O) machine(B-field) learning(I-field) ,(O) knowledge(B-field) management(I-field) ,(O) semantic(B-field) web(I-field) ,(O) software(B-field) development(I-field) ,(O) chemistry(B-field) and(O) biology(B-field) .(O)"}}
{"id": "69", "dataset": "crossner_ai", "split": "dev", "label_list": ["organization", "metric", "researcher", "conference", "programming language", "algorithm", "field", "location", "product", "task", "person", "country", "university"], "instance": {"id": "69", "words": ["The", "condensation", "algorithm", "has", "also", "been", "used", "for", "facial", "recognition", "system", "in", "a", "video", "sequence", "."], "labels": ["O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, metric, researcher, conference, programming language, algorithm, field, location, product, task, person, country, university and O.\nSentence: The condensation algorithm has also been used for facial recognition system in a video sequence .", "prompt_labels": "The(O) condensation(B-algorithm) algorithm(I-algorithm) has(O) also(O) been(O) used(O) for(O) facial(B-product) recognition(I-product) system(I-product) in(O) a(O) video(O) sequence(O) .(O)"}}
{"id": "117", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "person", "task", "programming language", "product", "researcher", "university", "country", "algorithm", "location", "organization", "metric", "field"], "instance": {"id": "117", "words": ["The", "four", "outcomes", "can", "be", "formulated", "in", "a", "2", "×", "2", "contingency", "table", "or", "confusion", "matrix", ",", "as", "follows", ":"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O", "B-metric", "I-metric", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, person, task, programming language, product, researcher, university, country, algorithm, location, organization, metric, field and O.\nSentence: The four outcomes can be formulated in a 2 × 2 contingency table or confusion matrix , as follows :", "prompt_labels": "The(O) four(O) outcomes(O) can(O) be(O) formulated(O) in(O) a(O) 2(O) ×(O) 2(O) contingency(B-metric) table(I-metric) or(O) confusion(B-metric) matrix(I-metric) ,(O) as(O) follows(O) :(O)"}}
{"id": "338", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "algorithm", "product", "person", "programming language", "country", "researcher", "metric", "field", "location", "organization", "university", "conference"], "instance": {"id": "338", "words": ["His", "thesis", "advisor", "was", "Professor", "Cordell", "Green", ",", "and", "his", "thesis", "/", "oral", "committee", "included", "Professors", "Edward", "Feigenbaum", "Joshua", "Lederberg", ",", "Paul", "Cohen", ",", "Allen", "Newell", ",", "Herbert", "Simon", ",", "."], "labels": ["O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, algorithm, product, person, programming language, country, researcher, metric, field, location, organization, university, conference and O.\nSentence: His thesis advisor was Professor Cordell Green , and his thesis / oral committee included Professors Edward Feigenbaum Joshua Lederberg , Paul Cohen , Allen Newell , Herbert Simon , .", "prompt_labels": "His(O) thesis(O) advisor(O) was(O) Professor(O) Cordell(B-researcher) Green(I-researcher) ,(O) and(O) his(O) thesis(O) /(O) oral(O) committee(O) included(O) Professors(O) Edward(B-researcher) Feigenbaum(I-researcher) Joshua(B-researcher) Lederberg(I-researcher) ,(O) Paul(B-researcher) Cohen(I-researcher) ,(O) Allen(B-researcher) Newell(I-researcher) ,(O) Herbert(B-researcher) Simon(I-researcher) ,(O) .(O)"}}
{"id": "238", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "researcher", "algorithm", "task", "field", "conference", "university", "organization", "programming language", "product", "metric", "country", "location"], "instance": {"id": "238", "words": ["It", "is", "based", "on", "the", "Bilingual", "evaluation", "understudy", "metric", ",", "but", "with", "some", "alterations", "."], "labels": ["O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, researcher, algorithm, task, field, conference, university, organization, programming language, product, metric, country, location and O.\nSentence: It is based on the Bilingual evaluation understudy metric , but with some alterations .", "prompt_labels": "It(O) is(O) based(O) on(O) the(O) Bilingual(B-metric) evaluation(I-metric) understudy(I-metric) metric(I-metric) ,(O) but(O) with(O) some(O) alterations(O) .(O)"}}
{"id": "181", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "conference", "task", "field", "metric", "person", "university", "algorithm", "product", "organization", "researcher", "location", "country"], "instance": {"id": "181", "words": ["In", "information", "theory", "and", "computer", "science", ",", "a", "code", "is", "usually", "considered", "as", "an", "algorithm", "that", "uniquely", "represents", "symbols", "from", "some", "source", "alphabet", ",", "by", "encoded", "strings", ",", "which", "may", "be", "in", "some", "other", "target", "alphabet", "."], "labels": ["O", "B-field", "I-field", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, conference, task, field, metric, person, university, algorithm, product, organization, researcher, location, country and O.\nSentence: In information theory and computer science , a code is usually considered as an algorithm that uniquely represents symbols from some source alphabet , by encoded strings , which may be in some other target alphabet .", "prompt_labels": "In(O) information(B-field) theory(I-field) and(O) computer(B-field) science(I-field) ,(O) a(O) code(O) is(O) usually(O) considered(O) as(O) an(O) algorithm(O) that(O) uniquely(O) represents(O) symbols(O) from(O) some(O) source(O) alphabet(O) ,(O) by(O) encoded(O) strings(O) ,(O) which(O) may(O) be(O) in(O) some(O) other(O) target(O) alphabet(O) .(O)"}}
{"id": "310", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "metric", "algorithm", "organization", "conference", "location", "country", "task", "field", "product", "person", "programming language", "university"], "instance": {"id": "310", "words": ["QC", "has", "not", "been", "evaluated", "against", "traditional", "modern", "clustering", "algorithms", ",", "aside", "from", "Jaccard", "index", "."], "labels": ["B-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, metric, algorithm, organization, conference, location, country, task, field, product, person, programming language, university and O.\nSentence: QC has not been evaluated against traditional modern clustering algorithms , aside from Jaccard index .", "prompt_labels": "QC(B-algorithm) has(O) not(O) been(O) evaluated(O) against(O) traditional(O) modern(O) clustering(O) algorithms(O) ,(O) aside(O) from(O) Jaccard(B-metric) index(I-metric) .(O)"}}
{"id": "160", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "organization", "product", "task", "field", "algorithm", "location", "person", "metric", "conference", "university", "programming language", "country"], "instance": {"id": "160", "words": ["Recent", "text", "recognition", "is", "based", "on", "Recurrent", "neural", "network", "(", "Long", "short-term", "memory", ")", "and", "does", "not", "require", "a", "language", "model", "."], "labels": ["O", "B-task", "I-task", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, organization, product, task, field, algorithm, location, person, metric, conference, university, programming language, country and O.\nSentence: Recent text recognition is based on Recurrent neural network ( Long short-term memory ) and does not require a language model .", "prompt_labels": "Recent(O) text(B-task) recognition(I-task) is(O) based(O) on(O) Recurrent(B-algorithm) neural(I-algorithm) network(I-algorithm) ((O) Long(B-algorithm) short-term(I-algorithm) memory(I-algorithm) )(O) and(O) does(O) not(O) require(O) a(O) language(B-algorithm) model(I-algorithm) .(O)"}}
{"id": "284", "dataset": "crossner_ai", "split": "dev", "label_list": ["metric", "university", "organization", "task", "programming language", "product", "algorithm", "person", "field", "country", "researcher", "location", "conference"], "instance": {"id": "284", "words": ["It", "is", "now", "also", "commonly", "used", "in", "speech", "recognition", ",", "speech", "synthesis", ",", "diarization", ",", "Xavier", "Anguera", "et", "al", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "O", "B-researcher", "I-researcher", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, university, organization, task, programming language, product, algorithm, person, field, country, researcher, location, conference and O.\nSentence: It is now also commonly used in speech recognition , speech synthesis , diarization , Xavier Anguera et al .", "prompt_labels": "It(O) is(O) now(O) also(O) commonly(O) used(O) in(O) speech(B-task) recognition(I-task) ,(O) speech(B-task) synthesis(I-task) ,(O) diarization(B-task) ,(O) Xavier(B-researcher) Anguera(I-researcher) et(O) al(O) .(O)"}}
{"id": "16", "dataset": "crossner_ai", "split": "dev", "label_list": ["university", "researcher", "programming language", "product", "conference", "field", "metric", "task", "location", "algorithm", "organization", "person", "country"], "instance": {"id": "16", "words": ["An", "implementation", "of", "several", "whitening", "procedures", "in", "R", ",", "including", "ZCA-whitening", "and", "PCA", "whitening", "but", "also", "CCA", "whitening", ",", "is", "available", "in", "the", "whitening", "R", "package", "published", "on", "CRAN", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-programming language", "O", "O", "B-algorithm", "O", "B-algorithm", "I-algorithm", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "O", "O", "B-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, researcher, programming language, product, conference, field, metric, task, location, algorithm, organization, person, country and O.\nSentence: An implementation of several whitening procedures in R , including ZCA-whitening and PCA whitening but also CCA whitening , is available in the whitening R package published on CRAN .", "prompt_labels": "An(O) implementation(O) of(O) several(O) whitening(O) procedures(O) in(O) R(B-programming language) ,(O) including(O) ZCA-whitening(B-algorithm) and(O) PCA(B-algorithm) whitening(I-algorithm) but(O) also(O) CCA(B-algorithm) whitening(I-algorithm) ,(O) is(O) available(O) in(O) the(O) whitening(B-product) R(I-product) package(I-product) published(O) on(O) CRAN(B-product) .(O)"}}
{"id": "331", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "conference", "location", "country", "field", "metric", "product", "university", "person", "researcher", "programming language", "task", "organization"], "instance": {"id": "331", "words": ["In", "particular", ",", "RLS", "is", "designed", "to", "minimize", "the", "mean", "squared", "error", "between", "the", "predicted", "values", "and", "the", "TRUE", "labels", ",", "subject", "to", "regularization", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, conference, location, country, field, metric, product, university, person, researcher, programming language, task, organization and O.\nSentence: In particular , RLS is designed to minimize the mean squared error between the predicted values and the TRUE labels , subject to regularization .", "prompt_labels": "In(O) particular(O) ,(O) RLS(O) is(O) designed(O) to(O) minimize(O) the(O) mean(B-metric) squared(I-metric) error(I-metric) between(O) the(O) predicted(O) values(O) and(O) the(O) TRUE(O) labels(O) ,(O) subject(O) to(O) regularization(O) .(O)"}}
{"id": "35", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "location", "metric", "person", "task", "product", "university", "country", "researcher", "organization", "algorithm", "programming language", "conference"], "instance": {"id": "35", "words": ["Concepts", "are", "used", "as", "formal", "tools", "or", "models", "in", "mathematics", ",", "computer", "science", ",", "databases", "and", "artificial", "intelligence", "where", "they", "are", "sometimes", "called", "classes", ",", "schema", "or", "categories", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "O", "B-field", "I-field", "O", "B-field", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, location, metric, person, task, product, university, country, researcher, organization, algorithm, programming language, conference and O.\nSentence: Concepts are used as formal tools or models in mathematics , computer science , databases and artificial intelligence where they are sometimes called classes , schema or categories .", "prompt_labels": "Concepts(O) are(O) used(O) as(O) formal(O) tools(O) or(O) models(O) in(O) mathematics(B-field) ,(O) computer(B-field) science(I-field) ,(O) databases(B-field) and(O) artificial(B-field) intelligence(I-field) where(O) they(O) are(O) sometimes(O) called(O) classes(O) ,(O) schema(O) or(O) categories(O) .(O)"}}
{"id": "114", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "person", "product", "conference", "organization", "programming language", "field", "researcher", "metric", "university", "location", "country", "task"], "instance": {"id": "114", "words": ["The", "Hough", "transform", "is", "a", "feature", "extraction", "technique", "used", "in", "image", "analysis", ",", "computer", "vision", ",", "and", "digital", "image", "processing", "."], "labels": ["O", "B-algorithm", "I-algorithm", "O", "O", "B-task", "I-task", "O", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "O", "B-field", "I-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, person, product, conference, organization, programming language, field, researcher, metric, university, location, country, task and O.\nSentence: The Hough transform is a feature extraction technique used in image analysis , computer vision , and digital image processing .", "prompt_labels": "The(O) Hough(B-algorithm) transform(I-algorithm) is(O) a(O) feature(B-task) extraction(I-task) technique(O) used(O) in(O) image(B-field) analysis(I-field) ,(O) computer(B-field) vision(I-field) ,(O) and(O) digital(B-field) image(I-field) processing(I-field) .(O)"}}
{"id": "205", "dataset": "crossner_ai", "split": "dev", "label_list": ["metric", "algorithm", "field", "programming language", "product", "person", "researcher", "country", "conference", "university", "task", "location", "organization"], "instance": {"id": "205", "words": ["For", "example", ",", "A.L.I.C.E.", "uses", "a", "markup", "language", "called", "AIML", ",", "which", "is", "specific", "to", "its", "function", "as", "a", "dialogue", "system", ",", "and", "has", "since", "been", "adopted", "by", "various", "other", "developers", "of", ",", "so-called", ",", "Alicebot", "s", "."], "labels": ["O", "O", "O", "B-product", "O", "O", "O", "O", "O", "B-programming language", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, algorithm, field, programming language, product, person, researcher, country, conference, university, task, location, organization and O.\nSentence: For example , A.L.I.C.E. uses a markup language called AIML , which is specific to its function as a dialogue system , and has since been adopted by various other developers of , so-called , Alicebot s .", "prompt_labels": "For(O) example(O) ,(O) A.L.I.C.E.(B-product) uses(O) a(O) markup(O) language(O) called(O) AIML(B-programming language) ,(O) which(O) is(O) specific(O) to(O) its(O) function(O) as(O) a(O) dialogue(B-product) system(I-product) ,(O) and(O) has(O) since(O) been(O) adopted(O) by(O) various(O) other(O) developers(O) of(O) ,(O) so-called(O) ,(O) Alicebot(B-product) s(O) .(O)"}}
{"id": "203", "dataset": "crossner_ai", "split": "dev", "label_list": ["metric", "conference", "algorithm", "organization", "university", "location", "person", "country", "task", "researcher", "programming language", "product", "field"], "instance": {"id": "203", "words": ["In", "2016", ",", "she", "was", "selected", "as", "the", "ACL", "(", "Association", "for", "Computational", "Linguistics", ")", "Lifetime", "Achievement", "Award", "winner", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "O", "B-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, conference, algorithm, organization, university, location, person, country, task, researcher, programming language, product, field and O.\nSentence: In 2016 , she was selected as the ACL ( Association for Computational Linguistics ) Lifetime Achievement Award winner .", "prompt_labels": "In(O) 2016(O) ,(O) she(O) was(O) selected(O) as(O) the(O) ACL(B-conference) ((O) Association(B-conference) for(I-conference) Computational(I-conference) Linguistics(I-conference) )(O) Lifetime(O) Achievement(O) Award(O) winner(O) .(O)"}}
{"id": "246", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "university", "researcher", "conference", "task", "person", "location", "algorithm", "organization", "product", "metric", "country", "programming language"], "instance": {"id": "246", "words": ["(", "February", "20", ",", "1912", "-", "August", "11", ",", "2011", ")", "was", "an", "American", "inventor", ",", "best", "known", "for", "creating", "Unimate", ",", "the", "first", "industrial", "robot", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, university, researcher, conference, task, person, location, algorithm, organization, product, metric, country, programming language and O.\nSentence: ( February 20 , 1912 - August 11 , 2011 ) was an American inventor , best known for creating Unimate , the first industrial robot .", "prompt_labels": "((O) February(O) 20(O) ,(O) 1912(O) -(O) August(O) 11(O) ,(O) 2011(O) )(O) was(O) an(O) American(O) inventor(O) ,(O) best(O) known(O) for(O) creating(O) Unimate(B-product) ,(O) the(O) first(O) industrial(O) robot(O) .(O)"}}
{"id": "232", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "university", "researcher", "algorithm", "metric", "field", "location", "person", "product", "task", "country", "programming language", "organization"], "instance": {"id": "232", "words": ["Alternatively", ",", "it", "can", "be", "used", "directly", "with", "the", "Perl", "Module", "TM", "(", "which", "also", "supports", "LTM", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-programming language", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, university, researcher, algorithm, metric, field, location, person, product, task, country, programming language, organization and O.\nSentence: Alternatively , it can be used directly with the Perl Module TM ( which also supports LTM ) .", "prompt_labels": "Alternatively(O) ,(O) it(O) can(O) be(O) used(O) directly(O) with(O) the(O) Perl(B-programming language) Module(O) TM(O) ((O) which(O) also(O) supports(O) LTM(O) )(O) .(O)"}}
{"id": "127", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "university", "programming language", "conference", "person", "organization", "country", "algorithm", "location", "metric", "researcher", "product", "task"], "instance": {"id": "127", "words": ["Artificial", "intelligence", "has", "retained", "the", "most", "attention", "regarding", "applied", "ontology", "in", "subfields", "like", "natural", "language", "processing", "within", "machine", "and", "knowledge", "representation", ",", "but", "ontology", "editors", "are", "being", "used", "often", "in", "a", "range", "of", "fields", "like", "education", "without", "the", "intent", "to", "contribute", "to", "AI", "."], "labels": ["B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "I-field", "O", "B-task", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, university, programming language, conference, person, organization, country, algorithm, location, metric, researcher, product, task and O.\nSentence: Artificial intelligence has retained the most attention regarding applied ontology in subfields like natural language processing within machine and knowledge representation , but ontology editors are being used often in a range of fields like education without the intent to contribute to AI .", "prompt_labels": "Artificial(B-field) intelligence(I-field) has(O) retained(O) the(O) most(O) attention(O) regarding(O) applied(O) ontology(O) in(O) subfields(O) like(O) natural(B-field) language(I-field) processing(I-field) within(O) machine(B-task) and(O) knowledge(B-task) representation(I-task) ,(O) but(O) ontology(O) editors(O) are(O) being(O) used(O) often(O) in(O) a(O) range(O) of(O) fields(O) like(O) education(O) without(O) the(O) intent(O) to(O) contribute(O) to(O) AI(B-field) .(O)"}}
{"id": "54", "dataset": "crossner_ai", "split": "dev", "label_list": ["metric", "product", "location", "organization", "conference", "country", "university", "task", "person", "field", "programming language", "researcher", "algorithm"], "instance": {"id": "54", "words": ["For", "regression", "analysis", "problems", "the", "squared", "error", "can", "be", "used", "as", "a", "loss", "function", ",", "for", "classification", "the", "cross", "entropy", "can", "be", "used", "."], "labels": ["O", "B-task", "I-task", "O", "O", "B-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "O", "B-metric", "I-metric", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, product, location, organization, conference, country, university, task, person, field, programming language, researcher, algorithm and O.\nSentence: For regression analysis problems the squared error can be used as a loss function , for classification the cross entropy can be used .", "prompt_labels": "For(O) regression(B-task) analysis(I-task) problems(O) the(O) squared(B-metric) error(I-metric) can(O) be(O) used(O) as(O) a(O) loss(O) function(O) ,(O) for(O) classification(B-task) the(O) cross(B-metric) entropy(I-metric) can(O) be(O) used(O) .(O)"}}
{"id": "215", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "algorithm", "person", "organization", "conference", "country", "university", "researcher", "task", "programming language", "product", "field", "metric"], "instance": {"id": "215", "words": ["Getoor", "has", "multiple", "best", "paper", "awards", ",", "an", "NSF", "Career", "Award", ",", "and", "is", "an", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "(", "AAAI", ")", "Fellow", "."], "labels": ["B-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, algorithm, person, organization, conference, country, university, researcher, task, programming language, product, field, metric and O.\nSentence: Getoor has multiple best paper awards , an NSF Career Award , and is an Association for the Advancement of Artificial Intelligence ( AAAI ) Fellow .", "prompt_labels": "Getoor(B-researcher) has(O) multiple(O) best(O) paper(O) awards(O) ,(O) an(O) NSF(O) Career(O) Award(O) ,(O) and(O) is(O) an(O) Association(B-conference) for(I-conference) the(I-conference) Advancement(I-conference) of(I-conference) Artificial(I-conference) Intelligence(I-conference) ((O) AAAI(B-conference) )(O) Fellow(O) .(O)"}}
{"id": "277", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "magazine", "organization", "book", "literary genre", "award", "event", "country", "poem", "location", "person"], "instance": {"id": "277", "words": ["In", "Switzerland", ",", "Johann", "David", "Wyss", "published", "The", "Swiss", "Family", "Robinson", "in", "1812", ",", "with", "the", "aim", "of", "teaching", "children", "about", "family", "values", ",", "good", "husbandry", ",", "the", "uses", "of", "the", "natural", "world", "and", "self-reliance", "."], "labels": ["O", "B-country", "O", "B-writer", "I-writer", "I-writer", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, magazine, organization, book, literary genre, award, event, country, poem, location, person and O.\nSentence: In Switzerland , Johann David Wyss published The Swiss Family Robinson in 1812 , with the aim of teaching children about family values , good husbandry , the uses of the natural world and self-reliance .", "prompt_labels": "In(O) Switzerland(B-country) ,(O) Johann(B-writer) David(I-writer) Wyss(I-writer) published(O) The(B-book) Swiss(I-book) Family(I-book) Robinson(I-book) in(O) 1812(O) ,(O) with(O) the(O) aim(O) of(O) teaching(O) children(O) about(O) family(O) values(O) ,(O) good(O) husbandry(O) ,(O) the(O) uses(O) of(O) the(O) natural(O) world(O) and(O) self-reliance(O) .(O)"}}
{"id": "96", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "person", "poem", "award", "magazine", "country", "location", "organization", "writer", "literary genre", "book"], "instance": {"id": "96", "words": ["The", "friends", "joined", "in", "keeping", "up", "the", "delusion", "that", "Erskine", "and", "not", "Scott", "was", "the", "author", "of", "the", "portions", "of", "The", "Bridal", "of", "Triermain", ",", "and", "wrote", "a", "preface", "intended", "to", "throw", "out", "the", "knowing", "ones", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, poem, award, magazine, country, location, organization, writer, literary genre, book and O.\nSentence: The friends joined in keeping up the delusion that Erskine and not Scott was the author of the portions of The Bridal of Triermain , and wrote a preface intended to throw out the knowing ones .", "prompt_labels": "The(O) friends(O) joined(O) in(O) keeping(O) up(O) the(O) delusion(O) that(O) Erskine(B-location) and(O) not(O) Scott(B-location) was(O) the(O) author(O) of(O) the(O) portions(O) of(O) The(B-poem) Bridal(I-poem) of(I-poem) Triermain(I-poem) ,(O) and(O) wrote(O) a(O) preface(O) intended(O) to(O) throw(O) out(O) the(O) knowing(O) ones(O) .(O)"}}
{"id": "349", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "event", "person", "magazine", "organization", "book", "award", "country", "poem", "literary genre", "location"], "instance": {"id": "349", "words": ["Novels", "and", "series", "such", "as", "the", "Cornelius", "Quartet", ",", "Mother", "London", ",", "King", "of", "the", "City", ",", "the", "Pyat", "Quartet", "and", "the", "short", "story", "collection", "London", "Bone", "have", "established", "him", "in", "the", "eyes", "of", "critics", "such", "as", "Iain", "Sinclair", ",", "Peter", "Ackroyd", "and", "Allan", "Massie", "in", "publications", "including", "The", "Times", "Literary", "Supplement", "and", "the", "London", "Review", "of", "Books", "as", "a", "major", "contemporary", "literary", "novelist", "."], "labels": ["B-literary genre", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "B-book", "I-book", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "B-book", "I-book", "O", "O", "B-literary genre", "I-literary genre", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-person", "I-person", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "I-magazine", "O", "O", "B-magazine", "I-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, event, person, magazine, organization, book, award, country, poem, literary genre, location and O.\nSentence: Novels and series such as the Cornelius Quartet , Mother London , King of the City , the Pyat Quartet and the short story collection London Bone have established him in the eyes of critics such as Iain Sinclair , Peter Ackroyd and Allan Massie in publications including The Times Literary Supplement and the London Review of Books as a major contemporary literary novelist .", "prompt_labels": "Novels(B-literary genre) and(O) series(O) such(O) as(O) the(O) Cornelius(B-book) Quartet(I-book) ,(O) Mother(B-book) London(I-book) ,(O) King(B-book) of(I-book) the(I-book) City(I-book) ,(O) the(O) Pyat(B-book) Quartet(I-book) and(O) the(O) short(B-literary genre) story(I-literary genre) collection(O) London(B-book) Bone(I-book) have(O) established(O) him(O) in(O) the(O) eyes(O) of(O) critics(O) such(O) as(O) Iain(B-writer) Sinclair(I-writer) ,(O) Peter(B-writer) Ackroyd(I-writer) and(O) Allan(B-person) Massie(I-person) in(O) publications(O) including(O) The(B-magazine) Times(I-magazine) Literary(I-magazine) Supplement(I-magazine) and(O) the(O) London(B-magazine) Review(I-magazine) of(I-magazine) Books(I-magazine) as(O) a(O) major(O) contemporary(O) literary(O) novelist(O) .(O)"}}
{"id": "138", "dataset": "crossner_literature", "split": "dev", "label_list": ["book", "poem", "award", "event", "organization", "writer", "literary genre", "magazine", "person", "country", "location"], "instance": {"id": "138", "words": ["The", "Letterman", "Foundation", "for", "Courtesy", "and", "Grooming", "is", "a", "private", "foundation", "through", "which", "Letterman", "has", "donated", "millions", "of", "dollars", "to", "charities", "and", "other", "non-profits", "in", "Indiana", "and", "Montana", ",", "celebrity-affiliated", "organizations", "such", "as", "Paul", "Newman", "'", "s", "Hole", "in", "the", "Wall", "Gang", "Camp", ",", "Ball", "State", "University", ",", "the", "American", "Cancer", "Society", ",", "the", "Salvation", "Army", ",", "and", "Médecins", "Sans", "Frontières", "."], "labels": ["B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, poem, award, event, organization, writer, literary genre, magazine, person, country, location and O.\nSentence: The Letterman Foundation for Courtesy and Grooming is a private foundation through which Letterman has donated millions of dollars to charities and other non-profits in Indiana and Montana , celebrity-affiliated organizations such as Paul Newman ' s Hole in the Wall Gang Camp , Ball State University , the American Cancer Society , the Salvation Army , and Médecins Sans Frontières .", "prompt_labels": "The(B-organization) Letterman(I-organization) Foundation(I-organization) for(I-organization) Courtesy(I-organization) and(I-organization) Grooming(I-organization) is(O) a(O) private(O) foundation(O) through(O) which(O) Letterman(B-writer) has(O) donated(O) millions(O) of(O) dollars(O) to(O) charities(O) and(O) other(O) non-profits(O) in(O) Indiana(B-location) and(O) Montana(B-location) ,(O) celebrity-affiliated(O) organizations(O) such(O) as(O) Paul(B-person) Newman(I-person) '(O) s(O) Hole(B-organization) in(I-organization) the(I-organization) Wall(I-organization) Gang(I-organization) Camp(I-organization) ,(O) Ball(B-organization) State(I-organization) University(I-organization) ,(O) the(O) American(B-organization) Cancer(I-organization) Society(I-organization) ,(O) the(O) Salvation(B-organization) Army(I-organization) ,(O) and(O) Médecins(B-organization) Sans(I-organization) Frontières(I-organization) .(O)"}}
{"id": "287", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "location", "organization", "book", "event", "award", "literary genre", "poem", "magazine", "country", "person"], "instance": {"id": "287", "words": ["Qualification", "for", "Euro", "1988", "meant", "winning", "UEFA", "Euro", "1988", "qualifying", "Group", "7", "containing", "Bulgaria", ",", "Luxembourg", "and", "Scotland", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "B-country", "O", "B-country", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, location, organization, book, event, award, literary genre, poem, magazine, country, person and O.\nSentence: Qualification for Euro 1988 meant winning UEFA Euro 1988 qualifying Group 7 containing Bulgaria , Luxembourg and Scotland .", "prompt_labels": "Qualification(O) for(O) Euro(O) 1988(O) meant(O) winning(O) UEFA(B-event) Euro(I-event) 1988(I-event) qualifying(I-event) Group(I-event) 7(I-event) containing(O) Bulgaria(B-country) ,(O) Luxembourg(B-country) and(O) Scotland(B-country) .(O)"}}
{"id": "319", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "person", "writer", "poem", "award", "location", "organization", "book", "magazine", "event", "literary genre"], "instance": {"id": "319", "words": ["In", "2006", ",", "a", "musical", "version", "of", "Fell", "'s", "play", "was", "staged", "during", "the", "New", "York", "Musical", "Theatre", "Festival", ",", "produced", "by", "George", "DeMarco", "and", "David", "Gerard", ",", "both", "of", "whom", "produced", "the", "1996", "production", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, writer, poem, award, location, organization, book, magazine, event, literary genre and O.\nSentence: In 2006 , a musical version of Fell 's play was staged during the New York Musical Theatre Festival , produced by George DeMarco and David Gerard , both of whom produced the 1996 production .", "prompt_labels": "In(O) 2006(O) ,(O) a(O) musical(O) version(O) of(O) Fell(B-writer) 's(O) play(O) was(O) staged(O) during(O) the(O) New(B-event) York(I-event) Musical(I-event) Theatre(I-event) Festival(I-event) ,(O) produced(O) by(O) George(B-person) DeMarco(I-person) and(O) David(B-person) Gerard(I-person) ,(O) both(O) of(O) whom(O) produced(O) the(O) 1996(O) production(O) .(O)"}}
{"id": "367", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "country", "writer", "book", "person", "literary genre", "event", "poem", "magazine", "award", "location"], "instance": {"id": "367", "words": ["In", "1965", ",", "he", "founded", "with", "Daniel", "Weissbort", "the", "journal", "Modern", "Poetry", "in", "Translation", ",", "which", "involved", "bringing", "to", "the", "attention", "of", "the", "West", "the", "work", "of", "Czesław", "Miłosz", ",", "who", "would", "later", "go", "on", "to", "win", "the", "Nobel", "Prize", "in", "Literature", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-magazine", "I-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, writer, book, person, literary genre, event, poem, magazine, award, location and O.\nSentence: In 1965 , he founded with Daniel Weissbort the journal Modern Poetry in Translation , which involved bringing to the attention of the West the work of Czesław Miłosz , who would later go on to win the Nobel Prize in Literature .", "prompt_labels": "In(O) 1965(O) ,(O) he(O) founded(O) with(O) Daniel(B-writer) Weissbort(I-writer) the(O) journal(O) Modern(B-magazine) Poetry(I-magazine) in(I-magazine) Translation(I-magazine) ,(O) which(O) involved(O) bringing(O) to(O) the(O) attention(O) of(O) the(O) West(B-organization) the(O) work(O) of(O) Czesław(B-writer) Miłosz(I-writer) ,(O) who(O) would(O) later(O) go(O) on(O) to(O) win(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) .(O)"}}
{"id": "281", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "book", "award", "country", "writer", "literary genre", "poem", "magazine", "location", "organization", "event"], "instance": {"id": "281", "words": ["Meanwhile", ",", "Housman", "pursued", "his", "classical", "studies", "independently", ",", "and", "published", "scholarly", "articles", "on", "Horace", ",", "Propertius", ",", "Ovid", ",", "Aeschylus", ",", "Euripides", "and", "Sophocles", ".ref", "Name", "=", "Poets", "/", "He", "also", "completed", "an", "edition", "of", "Propertius", ",", "which", "however", "was", "rejected", "by", "both", "Oxford", "University", "Press", "and", "Macmillan", "in", "1885", ",", "and", "was", "destroyed", "after", "his", "death", "."], "labels": ["O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "B-writer", "O", "B-writer", "O", "B-writer", "O", "B-writer", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "O", "B-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, book, award, country, writer, literary genre, poem, magazine, location, organization, event and O.\nSentence: Meanwhile , Housman pursued his classical studies independently , and published scholarly articles on Horace , Propertius , Ovid , Aeschylus , Euripides and Sophocles .ref Name = Poets / He also completed an edition of Propertius , which however was rejected by both Oxford University Press and Macmillan in 1885 , and was destroyed after his death .", "prompt_labels": "Meanwhile(O) ,(O) Housman(B-writer) pursued(O) his(O) classical(O) studies(O) independently(O) ,(O) and(O) published(O) scholarly(O) articles(O) on(O) Horace(B-writer) ,(O) Propertius(B-writer) ,(O) Ovid(B-writer) ,(O) Aeschylus(B-writer) ,(O) Euripides(B-writer) and(O) Sophocles(B-writer) .ref(O) Name(O) =(O) Poets(O) /(O) He(O) also(O) completed(O) an(O) edition(O) of(O) Propertius(B-writer) ,(O) which(O) however(O) was(O) rejected(O) by(O) both(O) Oxford(B-magazine) University(I-magazine) Press(I-magazine) and(O) Macmillan(B-magazine) in(O) 1885(O) ,(O) and(O) was(O) destroyed(O) after(O) his(O) death(O) .(O)"}}
{"id": "272", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "location", "literary genre", "award", "country", "person", "poem", "book", "organization", "event", "writer"], "instance": {"id": "272", "words": ["Because", "of", "this", ",", "United", "Kingdom", "lacks", "the", "charismatic", "leader", "needed", "to", "keep", "the", "country", "together", "and", "Nazi", "Germany", "successfully", "conquers", "Great", "Britain", "via", "Operation", "Sea", "Lion", "in", "1940", "."], "labels": ["O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, location, literary genre, award, country, person, poem, book, organization, event, writer and O.\nSentence: Because of this , United Kingdom lacks the charismatic leader needed to keep the country together and Nazi Germany successfully conquers Great Britain via Operation Sea Lion in 1940 .", "prompt_labels": "Because(O) of(O) this(O) ,(O) United(B-country) Kingdom(I-country) lacks(O) the(O) charismatic(O) leader(O) needed(O) to(O) keep(O) the(O) country(O) together(O) and(O) Nazi(B-country) Germany(I-country) successfully(O) conquers(O) Great(O) Britain(O) via(O) Operation(B-event) Sea(I-event) Lion(I-event) in(O) 1940(O) .(O)"}}
{"id": "291", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "writer", "literary genre", "country", "award", "poem", "book", "magazine", "organization", "person", "location"], "instance": {"id": "291", "words": ["The", "operetta", "Candide", "was", "originally", "conceived", "by", "playwright", "Lillian", "Hellman", ",", "as", "a", "play", "with", "incidental", "music", "."], "labels": ["O", "O", "B-book", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, writer, literary genre, country, award, poem, book, magazine, organization, person, location and O.\nSentence: The operetta Candide was originally conceived by playwright Lillian Hellman , as a play with incidental music .", "prompt_labels": "The(O) operetta(O) Candide(B-book) was(O) originally(O) conceived(O) by(O) playwright(O) Lillian(B-writer) Hellman(I-writer) ,(O) as(O) a(O) play(O) with(O) incidental(O) music(O) .(O)"}}
{"id": "288", "dataset": "crossner_literature", "split": "dev", "label_list": ["book", "event", "award", "poem", "country", "location", "writer", "organization", "literary genre", "person", "magazine"], "instance": {"id": "288", "words": ["Tarkovsky", "considered", "Thomas", "Mann", "and", "E.T.A.", "Hoffmann", ",", "and", "also", "thought", "about", "Henrik", "Ibsen", "'", "s", "Peer", "Gynt", "."], "labels": ["B-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, event, award, poem, country, location, writer, organization, literary genre, person, magazine and O.\nSentence: Tarkovsky considered Thomas Mann and E.T.A. Hoffmann , and also thought about Henrik Ibsen ' s Peer Gynt .", "prompt_labels": "Tarkovsky(B-writer) considered(O) Thomas(B-writer) Mann(I-writer) and(O) E.T.A.(B-writer) Hoffmann(I-writer) ,(O) and(O) also(O) thought(O) about(O) Henrik(B-writer) Ibsen(I-writer) '(O) s(O) Peer(B-book) Gynt(I-book) .(O)"}}
{"id": "317", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "award", "country", "literary genre", "writer", "event", "person", "book", "poem", "location", "magazine"], "instance": {"id": "317", "words": ["The", "Ghost", "Writer", ",", "a", "thriller", "focusing", "on", "a", "ghostwriter", "working", "on", "the", "memoirs", "of", "a", "character", "based", "loosely", "on", "former", "British", "prime", "minister", "Tony", "Blair", ",", "swept", "the", "European", "Film", "Awards", "in", "2010", ",", "winning", "six", "awards", ",", "including", "best", "movie", ",", "director", ",", "actor", "and", "screenplay", "."], "labels": ["B-book", "I-book", "I-book", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, country, literary genre, writer, event, person, book, poem, location, magazine and O.\nSentence: The Ghost Writer , a thriller focusing on a ghostwriter working on the memoirs of a character based loosely on former British prime minister Tony Blair , swept the European Film Awards in 2010 , winning six awards , including best movie , director , actor and screenplay .", "prompt_labels": "The(B-book) Ghost(I-book) Writer(I-book) ,(O) a(O) thriller(B-literary genre) focusing(O) on(O) a(O) ghostwriter(O) working(O) on(O) the(O) memoirs(O) of(O) a(O) character(O) based(O) loosely(O) on(O) former(O) British(O) prime(O) minister(O) Tony(B-person) Blair(I-person) ,(O) swept(O) the(O) European(B-award) Film(I-award) Awards(I-award) in(O) 2010(O) ,(O) winning(O) six(O) awards(O) ,(O) including(O) best(O) movie(O) ,(O) director(O) ,(O) actor(O) and(O) screenplay(O) .(O)"}}
{"id": "38", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "location", "book", "writer", "country", "literary genre", "award", "poem", "organization", "magazine", "person"], "instance": {"id": "38", "words": ["Marsters", "moved", "to", "Chicago", ",", "where", "his", "first", "professional", "acting", "role", "was", "Ferdinand", "in", "The", "Tempest", "at", "the", "Goodman", "Theatre", "in", "1987", "."], "labels": ["B-person", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "B-location", "I-location", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, book, writer, country, literary genre, award, poem, organization, magazine, person and O.\nSentence: Marsters moved to Chicago , where his first professional acting role was Ferdinand in The Tempest at the Goodman Theatre in 1987 .", "prompt_labels": "Marsters(B-person) moved(O) to(O) Chicago(B-location) ,(O) where(O) his(O) first(O) professional(O) acting(O) role(O) was(O) Ferdinand(O) in(O) The(B-book) Tempest(I-book) at(O) the(O) Goodman(B-location) Theatre(I-location) in(O) 1987(O) .(O)"}}
{"id": "31", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "magazine", "organization", "award", "literary genre", "event", "person", "writer", "location", "poem", "book"], "instance": {"id": "31", "words": ["According", "to", "Entertainment", "Weekly", ",", "Raimi", "had", "expressed", "an", "interest", "in", "directing", "a", "film", "version", "of", "The", "Hobbit", ",", "the", "prequel", "to", "the", "Lord", "of", "the", "Rings", "trilogy", "."], "labels": ["O", "O", "B-magazine", "I-magazine", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, magazine, organization, award, literary genre, event, person, writer, location, poem, book and O.\nSentence: According to Entertainment Weekly , Raimi had expressed an interest in directing a film version of The Hobbit , the prequel to the Lord of the Rings trilogy .", "prompt_labels": "According(O) to(O) Entertainment(B-magazine) Weekly(I-magazine) ,(O) Raimi(B-person) had(O) expressed(O) an(O) interest(O) in(O) directing(O) a(O) film(O) version(O) of(O) The(B-book) Hobbit(I-book) ,(O) the(O) prequel(O) to(O) the(O) Lord(B-book) of(I-book) the(I-book) Rings(I-book) trilogy(O) .(O)"}}
{"id": "382", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "event", "writer", "location", "literary genre", "poem", "organization", "person", "book", "award", "country"], "instance": {"id": "382", "words": ["In", "a", "2006", "interview", "with", "Tatler", "magazine", ",", "Rowling", "noted", "that", ",", "like", "Graham", "Greene", ",", "my", "faith", "is", "sometimes", "about", "if", "my", "faith", "will", "return", "."], "labels": ["O", "O", "O", "O", "O", "B-magazine", "I-magazine", "O", "B-writer", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, event, writer, location, literary genre, poem, organization, person, book, award, country and O.\nSentence: In a 2006 interview with Tatler magazine , Rowling noted that , like Graham Greene , my faith is sometimes about if my faith will return .", "prompt_labels": "In(O) a(O) 2006(O) interview(O) with(O) Tatler(B-magazine) magazine(I-magazine) ,(O) Rowling(B-writer) noted(O) that(O) ,(O) like(O) Graham(B-writer) Greene(I-writer) ,(O) my(O) faith(O) is(O) sometimes(O) about(O) if(O) my(O) faith(O) will(O) return(O) .(O)"}}
{"id": "228", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "poem", "organization", "event", "literary genre", "writer", "location", "award", "country", "book", "person"], "instance": {"id": "228", "words": ["He", "also", "wrote", "realistic", "novels", "that", "received", "critical", "acclaim", ",", "including", "Kipps", "and", "a", "critique", "of", "English", "culture", "during", "the", "Edwardian", "period", ",", "Tono-Bungay", "."], "labels": ["O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "O", "O", "O", "O", "B-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, poem, organization, event, literary genre, writer, location, award, country, book, person and O.\nSentence: He also wrote realistic novels that received critical acclaim , including Kipps and a critique of English culture during the Edwardian period , Tono-Bungay .", "prompt_labels": "He(O) also(O) wrote(O) realistic(B-literary genre) novels(I-literary genre) that(O) received(O) critical(O) acclaim(O) ,(O) including(O) Kipps(B-book) and(O) a(O) critique(O) of(O) English(O) culture(O) during(O) the(O) Edwardian(O) period(O) ,(O) Tono-Bungay(B-book) .(O)"}}
{"id": "165", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "poem", "literary genre", "event", "writer", "book", "award", "magazine", "organization", "location", "country"], "instance": {"id": "165", "words": ["In", "addition", "to", "receiving", "a", "star", "on", "the", "Hollywood", "Walk", "of", "Fame", ",", "media", "appearances", "included", "write-ups", "in", "CCM", "Magazine", ",", "and", "a", "performance", "on", "The", "View", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, poem, literary genre, event, writer, book, award, magazine, organization, location, country and O.\nSentence: In addition to receiving a star on the Hollywood Walk of Fame , media appearances included write-ups in CCM Magazine , and a performance on The View .", "prompt_labels": "In(O) addition(O) to(O) receiving(O) a(O) star(O) on(O) the(O) Hollywood(B-location) Walk(I-location) of(I-location) Fame(I-location) ,(O) media(O) appearances(O) included(O) write-ups(O) in(O) CCM(B-magazine) Magazine(I-magazine) ,(O) and(O) a(O) performance(O) on(O) The(O) View(O) .(O)"}}
{"id": "150", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "location", "award", "book", "country", "writer", "literary genre", "person", "poem", "organization", "magazine"], "instance": {"id": "150", "words": ["Among", "books", "on", "the", "list", "considered", "to", "be", "the", "Great", "American", "Novel", "were", "Moby-Dick", ",", "Adventures", "of", "Huckleberry", "Finn", ",", "The", "Great", "Gatsby", ",", "The", "Grapes", "of", "Wrath", ",", "The", "Catcher", "in", "the", "Rye", ",", "Invisible", "Man", ",", "and", "To", "Kill", "a", "Mockingbird", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "I-literary genre", "O", "B-book", "O", "B-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "O", "O", "B-book", "I-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, award, book, country, writer, literary genre, person, poem, organization, magazine and O.\nSentence: Among books on the list considered to be the Great American Novel were Moby-Dick , Adventures of Huckleberry Finn , The Great Gatsby , The Grapes of Wrath , The Catcher in the Rye , Invisible Man , and To Kill a Mockingbird .", "prompt_labels": "Among(O) books(O) on(O) the(O) list(O) considered(O) to(O) be(O) the(O) Great(B-literary genre) American(I-literary genre) Novel(I-literary genre) were(O) Moby-Dick(B-book) ,(O) Adventures(B-book) of(I-book) Huckleberry(I-book) Finn(I-book) ,(O) The(B-book) Great(I-book) Gatsby(I-book) ,(O) The(B-book) Grapes(I-book) of(I-book) Wrath(I-book) ,(O) The(B-book) Catcher(I-book) in(I-book) the(I-book) Rye(I-book) ,(O) Invisible(B-book) Man(I-book) ,(O) and(O) To(B-book) Kill(I-book) a(I-book) Mockingbird(I-book) .(O)"}}
{"id": "32", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "literary genre", "organization", "person", "location", "magazine", "writer", "book", "country", "poem", "award"], "instance": {"id": "32", "words": ["Ansgar", "received", "the", "mission", "of", "evangelizing", "pagan", "Denmark", ",", "Norway", "and", "Sweden", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, literary genre, organization, person, location, magazine, writer, book, country, poem, award and O.\nSentence: Ansgar received the mission of evangelizing pagan Denmark , Norway and Sweden .", "prompt_labels": "Ansgar(B-writer) received(O) the(O) mission(O) of(O) evangelizing(O) pagan(O) Denmark(B-country) ,(O) Norway(B-country) and(O) Sweden(B-country) .(O)"}}
{"id": "33", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "magazine", "award", "book", "literary genre", "poem", "country", "organization", "event", "writer", "location"], "instance": {"id": "33", "words": ["The", "poem", "is", "quoted", "by", "Sue", "Bridehead", "in", "Thomas", "Hardy", "'", "s", "1895", "novel", ",", "Jude", "the", "Obscure", "and", "also", "by", "Edward", "Ashburnham", "in", "Ford", "Madox", ".", "Ford", "'", "s", "The", "Good", "Soldier", "."], "labels": ["O", "B-literary genre", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "B-literary genre", "O", "B-book", "I-book", "I-book", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "O", "O", "B-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, magazine, award, book, literary genre, poem, country, organization, event, writer, location and O.\nSentence: The poem is quoted by Sue Bridehead in Thomas Hardy ' s 1895 novel , Jude the Obscure and also by Edward Ashburnham in Ford Madox . Ford ' s The Good Soldier .", "prompt_labels": "The(O) poem(B-literary genre) is(O) quoted(O) by(O) Sue(B-writer) Bridehead(I-writer) in(O) Thomas(B-writer) Hardy(I-writer) '(O) s(O) 1895(O) novel(B-literary genre) ,(O) Jude(B-book) the(I-book) Obscure(I-book) and(O) also(O) by(O) Edward(B-writer) Ashburnham(I-writer) in(O) Ford(B-writer) Madox(I-writer) .(O) Ford(B-writer) '(O) s(O) The(B-book) Good(I-book) Soldier(I-book) .(O)"}}
{"id": "378", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "book", "literary genre", "magazine", "poem", "location", "organization", "person", "event", "award", "country"], "instance": {"id": "378", "words": ["The", "Tale", "of", "Jemima", "Puddle-Duck", "and", "The", "Tale", "of", "Tom", "Kitten", "are", "representative", "of", "Hill", "Top", "Farm", "and", "her", "farming", "life", "and", "reflect", "her", "happiness", "with", "her", "country", "life.", "John", "Heelis", ",", "(", "1999", ")", "The", "Tale", "of", "Mrs", "William", "Heelis", "-", "Beatrix", "Potter", ";", "Lear", ",", "Ch", "."], "labels": ["B-book", "I-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, book, literary genre, magazine, poem, location, organization, person, event, award, country and O.\nSentence: The Tale of Jemima Puddle-Duck and The Tale of Tom Kitten are representative of Hill Top Farm and her farming life and reflect her happiness with her country life. John Heelis , ( 1999 ) The Tale of Mrs William Heelis - Beatrix Potter ; Lear , Ch .", "prompt_labels": "The(B-book) Tale(I-book) of(I-book) Jemima(I-book) Puddle-Duck(I-book) and(O) The(B-book) Tale(I-book) of(I-book) Tom(I-book) Kitten(I-book) are(O) representative(O) of(O) Hill(B-location) Top(I-location) Farm(I-location) and(O) her(O) farming(O) life(O) and(O) reflect(O) her(O) happiness(O) with(O) her(O) country(O) life.(O) John(B-writer) Heelis(I-writer) ,(O) ((O) 1999(O) )(O) The(B-book) Tale(I-book) of(I-book) Mrs(I-book) William(I-book) Heelis(I-book) -(I-book) Beatrix(I-book) Potter(I-book) ;(O) Lear(O) ,(O) Ch(O) .(O)"}}
{"id": "178", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "event", "location", "country", "organization", "magazine", "literary genre", "person", "writer", "book", "poem"], "instance": {"id": "178", "words": ["Guillaume", "Apollinaire", ",", "André", "Salmon", "and", "Max", "Jacob", "sought", "him", "out", "in", "his", "truncated", "apartment", "."], "labels": ["B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, location, country, organization, magazine, literary genre, person, writer, book, poem and O.\nSentence: Guillaume Apollinaire , André Salmon and Max Jacob sought him out in his truncated apartment .", "prompt_labels": "Guillaume(B-writer) Apollinaire(I-writer) ,(O) André(B-writer) Salmon(I-writer) and(O) Max(B-writer) Jacob(I-writer) sought(O) him(O) out(O) in(O) his(O) truncated(O) apartment(O) .(O)"}}
{"id": "276", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "writer", "event", "magazine", "organization", "poem", "country", "location", "person", "book", "award"], "instance": {"id": "276", "words": ["The", "Amber", "Diceless", "Roleplaying", "Game", "is", "a", "role-playing", "game", "created", "and", "written", "by", "Erick", "Wujcik", ",", "set", "in", "the", "fictional", "universe", "created", "by", "author", "Roger", "Zelazny", "for", "his", "Chronicles", "of", "Amber", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, writer, event, magazine, organization, poem, country, location, person, book, award and O.\nSentence: The Amber Diceless Roleplaying Game is a role-playing game created and written by Erick Wujcik , set in the fictional universe created by author Roger Zelazny for his Chronicles of Amber .", "prompt_labels": "The(O) Amber(O) Diceless(O) Roleplaying(O) Game(O) is(O) a(O) role-playing(O) game(O) created(O) and(O) written(O) by(O) Erick(B-writer) Wujcik(I-writer) ,(O) set(O) in(O) the(O) fictional(O) universe(O) created(O) by(O) author(O) Roger(B-writer) Zelazny(I-writer) for(O) his(O) Chronicles(B-book) of(I-book) Amber(I-book) .(O)"}}
{"id": "233", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "writer", "organization", "location", "person", "poem", "award", "country", "book", "event", "literary genre"], "instance": {"id": "233", "words": ["He", "wrote", "many", ",", "including", "such", "favorites", "as", "If", "I", "Ran", "the", "Zoo", "(", "1950", ")", ",", "Horton", "Hears", "a", "Who", "!", "(", "1955", ")", ",", "If", "I", "Ran", "the", "Circus", "(", "1956", ")", ",", "The", "Cat", "in", "the", "Hat", "(", "1957", ")", ",", "How", "the", "Grinch", "Stole", "Christmas", "!", "(", "1957", ")", ",", "and", "Green", "Eggs", "and", "Ham", "(", "1960", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, writer, organization, location, person, poem, award, country, book, event, literary genre and O.\nSentence: He wrote many , including such favorites as If I Ran the Zoo ( 1950 ) , Horton Hears a Who ! ( 1955 ) , If I Ran the Circus ( 1956 ) , The Cat in the Hat ( 1957 ) , How the Grinch Stole Christmas ! ( 1957 ) , and Green Eggs and Ham ( 1960 ) .", "prompt_labels": "He(O) wrote(O) many(O) ,(O) including(O) such(O) favorites(O) as(O) If(B-book) I(I-book) Ran(I-book) the(I-book) Zoo(I-book) ((O) 1950(O) )(O) ,(O) Horton(B-book) Hears(I-book) a(I-book) Who(I-book) !(I-book) ((O) 1955(O) )(O) ,(O) If(B-book) I(I-book) Ran(I-book) the(I-book) Circus(I-book) ((O) 1956(O) )(O) ,(O) The(B-book) Cat(I-book) in(I-book) the(I-book) Hat(I-book) ((O) 1957(O) )(O) ,(O) How(B-book) the(I-book) Grinch(I-book) Stole(I-book) Christmas(I-book) !(I-book) ((O) 1957(O) )(O) ,(O) and(O) Green(B-book) Eggs(I-book) and(I-book) Ham(I-book) ((O) 1960(O) )(O) .(O)"}}
{"id": "135", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "location", "award", "book", "writer", "magazine", "event", "literary genre", "organization", "poem", "country"], "instance": {"id": "135", "words": ["In", "1959", ",", "Capp", "recorded", "and", "released", "an", "album", "for", "Folkways", "Records", "(", "now", "owned", "by", "the", "Smithsonian", ")", "on", "which", "he", "identified", "and", "described", "The", "Mechanics", "of", "the", "Comic", "Strip", "."], "labels": ["O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, award, book, writer, magazine, event, literary genre, organization, poem, country and O.\nSentence: In 1959 , Capp recorded and released an album for Folkways Records ( now owned by the Smithsonian ) on which he identified and described The Mechanics of the Comic Strip .", "prompt_labels": "In(O) 1959(O) ,(O) Capp(B-writer) recorded(O) and(O) released(O) an(O) album(O) for(O) Folkways(B-organization) Records(I-organization) ((O) now(O) owned(O) by(O) the(O) Smithsonian(B-organization) )(O) on(O) which(O) he(O) identified(O) and(O) described(O) The(O) Mechanics(O) of(O) the(O) Comic(O) Strip(O) .(O)"}}
{"id": "238", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "country", "award", "book", "literary genre", "writer", "poem", "person", "magazine", "organization", "event"], "instance": {"id": "238", "words": ["The", "decade", "-", "and", "the", "Australian", "phase", "of", "Carey", "'s", "career", "-", "culminated", "with", "the", "publication", "of", "Oscar", "and", "Lucinda", "(", "1988", ")", ",", "which", "won", "the", "Booker", "Prize", "(", "as", "it", "was", "then", "known", ")", "and", "brought", "the", "author", "international", "recognition", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, award, book, literary genre, writer, poem, person, magazine, organization, event and O.\nSentence: The decade - and the Australian phase of Carey 's career - culminated with the publication of Oscar and Lucinda ( 1988 ) , which won the Booker Prize ( as it was then known ) and brought the author international recognition .", "prompt_labels": "The(O) decade(O) -(O) and(O) the(O) Australian(O) phase(O) of(O) Carey(B-writer) 's(O) career(O) -(O) culminated(O) with(O) the(O) publication(O) of(O) Oscar(B-book) and(I-book) Lucinda(I-book) ((O) 1988(O) )(O) ,(O) which(O) won(O) the(O) Booker(B-award) Prize(I-award) ((O) as(O) it(O) was(O) then(O) known(O) )(O) and(O) brought(O) the(O) author(O) international(O) recognition(O) .(O)"}}
{"id": "187", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "book", "location", "magazine", "event", "writer", "organization", "country", "person", "poem", "award"], "instance": {"id": "187", "words": ["H.", "P.", "Lovecraft", "stated", "that", "in", "sheer", "daemonic", "strangeness", "and", "fertility", "of", "conception", ",", "Clark", "Ashton", "Smith", "is", "perhaps", "unexcelled", ",", "and", "Ray", "Bradbury", "said", "that", "Smith", "filled", "my", "mind", "with", "incredible", "worlds", ",", "impossibly", "beautiful", "cities", ",", "and", "still", "more", "fantastic", "creatures", "."], "labels": ["B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, book, location, magazine, event, writer, organization, country, person, poem, award and O.\nSentence: H. P. Lovecraft stated that in sheer daemonic strangeness and fertility of conception , Clark Ashton Smith is perhaps unexcelled , and Ray Bradbury said that Smith filled my mind with incredible worlds , impossibly beautiful cities , and still more fantastic creatures .", "prompt_labels": "H.(B-writer) P.(I-writer) Lovecraft(I-writer) stated(O) that(O) in(O) sheer(O) daemonic(O) strangeness(O) and(O) fertility(O) of(O) conception(O) ,(O) Clark(B-writer) Ashton(I-writer) Smith(I-writer) is(O) perhaps(O) unexcelled(O) ,(O) and(O) Ray(B-writer) Bradbury(I-writer) said(O) that(O) Smith(B-writer) filled(O) my(O) mind(O) with(O) incredible(O) worlds(O) ,(O) impossibly(O) beautiful(O) cities(O) ,(O) and(O) still(O) more(O) fantastic(O) creatures(O) .(O)"}}
{"id": "241", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "book", "person", "award", "magazine", "literary genre", "event", "location", "poem", "organization", "country"], "instance": {"id": "241", "words": ["Lewis", "was", "a", "prolific", "writer", ",", "and", "his", "circle", "of", "literary", "friends", "became", "an", "informal", "discussion", "society", "known", "as", "the", "Inklings", ",", "including", "J.", "R.", "R.", "Tolkien", ",", "Nevill", "Coghill", ",", "Lord", "David", "Cecil", ",", "Charles", "Williams", ",", "Owen", "Barfield", ",", "and", "his", "brother", "Warren", "Lewis", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "B-writer", "I-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, book, person, award, magazine, literary genre, event, location, poem, organization, country and O.\nSentence: Lewis was a prolific writer , and his circle of literary friends became an informal discussion society known as the Inklings , including J. R. R. Tolkien , Nevill Coghill , Lord David Cecil , Charles Williams , Owen Barfield , and his brother Warren Lewis .", "prompt_labels": "Lewis(B-writer) was(O) a(O) prolific(O) writer(O) ,(O) and(O) his(O) circle(O) of(O) literary(O) friends(O) became(O) an(O) informal(O) discussion(O) society(O) known(O) as(O) the(O) Inklings(B-organization) ,(O) including(O) J.(B-writer) R.(I-writer) R.(I-writer) Tolkien(I-writer) ,(O) Nevill(B-writer) Coghill(I-writer) ,(O) Lord(B-writer) David(I-writer) Cecil(I-writer) ,(O) Charles(B-writer) Williams(I-writer) ,(O) Owen(B-writer) Barfield(I-writer) ,(O) and(O) his(O) brother(O) Warren(B-person) Lewis(I-person) .(O)"}}
{"id": "239", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "location", "person", "award", "writer", "book", "literary genre", "magazine", "poem", "country", "event"], "instance": {"id": "239", "words": ["Wilson", "appeared", "in", "another", "Wes", "Anderson", "film", ",", "The", "Darjeeling", "Limited", ",", "which", "screened", "at", "the", "45th", "annual", "New", "York", "Film", "Festival", ",", "the", "Venice", "Film", "Festival", "and", "opened", "September", "30", ",", "2007", ",", "co-starring", "Jason", "Schwartzman", "and", "Adrien", "Brody", "."], "labels": ["B-person", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, person, award, writer, book, literary genre, magazine, poem, country, event and O.\nSentence: Wilson appeared in another Wes Anderson film , The Darjeeling Limited , which screened at the 45th annual New York Film Festival , the Venice Film Festival and opened September 30 , 2007 , co-starring Jason Schwartzman and Adrien Brody .", "prompt_labels": "Wilson(B-person) appeared(O) in(O) another(O) Wes(B-person) Anderson(I-person) film(O) ,(O) The(O) Darjeeling(O) Limited(O) ,(O) which(O) screened(O) at(O) the(O) 45th(O) annual(O) New(B-event) York(I-event) Film(I-event) Festival(I-event) ,(O) the(O) Venice(B-event) Film(I-event) Festival(I-event) and(O) opened(O) September(O) 30(O) ,(O) 2007(O) ,(O) co-starring(O) Jason(B-person) Schwartzman(I-person) and(O) Adrien(B-person) Brody(I-person) .(O)"}}
{"id": "234", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "organization", "magazine", "country", "award", "book", "writer", "person", "literary genre", "event", "poem"], "instance": {"id": "234", "words": ["Pauline", "Kael", "in", "The", "New", "Yorker", "described", "it", "as", "heaven", "-", "alive", "in", "a", "way", "that", "movies", "rarely", "are", "."], "labels": ["B-writer", "I-writer", "O", "B-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, magazine, country, award, book, writer, person, literary genre, event, poem and O.\nSentence: Pauline Kael in The New Yorker described it as heaven - alive in a way that movies rarely are .", "prompt_labels": "Pauline(B-writer) Kael(I-writer) in(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) described(O) it(O) as(O) heaven(O) -(O) alive(O) in(O) a(O) way(O) that(O) movies(O) rarely(O) are(O) .(O)"}}
{"id": "87", "dataset": "crossner_literature", "split": "dev", "label_list": ["poem", "magazine", "book", "writer", "location", "literary genre", "award", "organization", "country", "event", "person"], "instance": {"id": "87", "words": ["The", "Great", "Hunt", "is", "a", "fantasy", "novel", "by", "United", "States", "author", "Robert", "Jordan", ",", "the", "second", "book", "of", "The", "Wheel", "of", "Time", "series", "."], "labels": ["B-book", "I-book", "I-book", "O", "O", "B-literary genre", "I-literary genre", "O", "B-country", "I-country", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, magazine, book, writer, location, literary genre, award, organization, country, event, person and O.\nSentence: The Great Hunt is a fantasy novel by United States author Robert Jordan , the second book of The Wheel of Time series .", "prompt_labels": "The(B-book) Great(I-book) Hunt(I-book) is(O) a(O) fantasy(B-literary genre) novel(I-literary genre) by(O) United(B-country) States(I-country) author(O) Robert(B-writer) Jordan(I-writer) ,(O) the(O) second(O) book(O) of(O) The(B-book) Wheel(I-book) of(I-book) Time(I-book) series(O) .(O)"}}
{"id": "229", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "country", "book", "literary genre", "organization", "award", "event", "location", "magazine", "poem", "person"], "instance": {"id": "229", "words": ["His", "editions", "of", "Juvenal", ",", "Manilius", "and", "Lucan", "are", "still", "considered", "authoritative", "."], "labels": ["O", "O", "O", "B-writer", "O", "B-writer", "O", "B-writer", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, country, book, literary genre, organization, award, event, location, magazine, poem, person and O.\nSentence: His editions of Juvenal , Manilius and Lucan are still considered authoritative .", "prompt_labels": "His(O) editions(O) of(O) Juvenal(B-writer) ,(O) Manilius(B-writer) and(O) Lucan(B-writer) are(O) still(O) considered(O) authoritative(O) .(O)"}}
{"id": "110", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "writer", "book", "person", "country", "magazine", "location", "event", "poem", "literary genre", "organization"], "instance": {"id": "110", "words": ["Nominated", "for", "Academy", "Awards", "in", "nine", "categories", ",", "it", "won", "an", "Academy", "Award", "for", "Best", "Original", "Screenplay", "by", "Herman", "J.", "Mankiewicz", "and", "Welles", "."], "labels": ["O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, writer, book, person, country, magazine, location, event, poem, literary genre, organization and O.\nSentence: Nominated for Academy Awards in nine categories , it won an Academy Award for Best Original Screenplay by Herman J. Mankiewicz and Welles .", "prompt_labels": "Nominated(O) for(O) Academy(B-award) Awards(I-award) in(O) nine(O) categories(O) ,(O) it(O) won(O) an(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Screenplay(I-award) by(O) Herman(B-writer) J.(I-writer) Mankiewicz(I-writer) and(O) Welles(B-writer) .(O)"}}
{"id": "231", "dataset": "crossner_literature", "split": "dev", "label_list": ["book", "writer", "award", "organization", "country", "person", "literary genre", "event", "magazine", "location", "poem"], "instance": {"id": "231", "words": ["A", "Clockwork", "Orange", "is", "a", "dystopian", "satirical", "black", "comedy", "novel", "by", "English", "writer", "Anthony", "Burgess", ",", "published", "in", "1962", "."], "labels": ["B-book", "I-book", "I-book", "O", "O", "B-literary genre", "I-literary genre", "I-literary genre", "I-literary genre", "I-literary genre", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, writer, award, organization, country, person, literary genre, event, magazine, location, poem and O.\nSentence: A Clockwork Orange is a dystopian satirical black comedy novel by English writer Anthony Burgess , published in 1962 .", "prompt_labels": "A(B-book) Clockwork(I-book) Orange(I-book) is(O) a(O) dystopian(B-literary genre) satirical(I-literary genre) black(I-literary genre) comedy(I-literary genre) novel(I-literary genre) by(O) English(O) writer(O) Anthony(B-writer) Burgess(I-writer) ,(O) published(O) in(O) 1962(O) .(O)"}}
{"id": "83", "dataset": "crossner_literature", "split": "dev", "label_list": ["poem", "literary genre", "country", "person", "writer", "magazine", "book", "event", "location", "award", "organization"], "instance": {"id": "83", "words": [")", "Huxley", "received", "screen", "credit", "for", "Pride", "and", "Prejudice", "(", "1940", ")", "and", "was", "paid", "for", "his", "work", "on", "a", "number", "of", "other", "films", ",", "including", "Jane", "Eyre", "(", "1944", ")", "."], "labels": ["O", "B-writer", "O", "B-award", "I-award", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, literary genre, country, person, writer, magazine, book, event, location, award, organization and O.\nSentence: ) Huxley received screen credit for Pride and Prejudice ( 1940 ) and was paid for his work on a number of other films , including Jane Eyre ( 1944 ) .", "prompt_labels": ")(O) Huxley(B-writer) received(O) screen(B-award) credit(I-award) for(O) Pride(B-book) and(I-book) Prejudice(I-book) ((O) 1940(O) )(O) and(O) was(O) paid(O) for(O) his(O) work(O) on(O) a(O) number(O) of(O) other(O) films(O) ,(O) including(O) Jane(O) Eyre(O) ((O) 1944(O) )(O) .(O)"}}
{"id": "44", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "location", "country", "magazine", "writer", "poem", "book", "person", "event", "literary genre", "organization"], "instance": {"id": "44", "words": ["Hesser", "lives", "in", "Brooklyn", "Heights", "with", "her", "husband", ",", "Tad", "Friend", ",", "a", "staff", "writer", "for", "The", "New", "Yorker", ",", "and", "their", "two", "children", "."], "labels": ["B-writer", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, country, magazine, writer, poem, book, person, event, literary genre, organization and O.\nSentence: Hesser lives in Brooklyn Heights with her husband , Tad Friend , a staff writer for The New Yorker , and their two children .", "prompt_labels": "Hesser(B-writer) lives(O) in(O) Brooklyn(B-location) Heights(I-location) with(O) her(O) husband(O) ,(O) Tad(B-writer) Friend(I-writer) ,(O) a(O) staff(O) writer(O) for(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) ,(O) and(O) their(O) two(O) children(O) .(O)"}}
{"id": "34", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "location", "person", "organization", "writer", "poem", "country", "literary genre", "magazine", "book", "event"], "instance": {"id": "34", "words": ["In", "a", "1971", "interview", "with", "The", "London", "Magazine", ",", "Hughes", "cited", "his", "main", "influences", "as", "including", "William", "Blake", ",", "John", "Donne", ",", "Hopkins", "and", "T.", "S.", "Eliot", "."], "labels": ["O", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "O", "B-writer", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "O", "B-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, person, organization, writer, poem, country, literary genre, magazine, book, event and O.\nSentence: In a 1971 interview with The London Magazine , Hughes cited his main influences as including William Blake , John Donne , Hopkins and T. S. Eliot .", "prompt_labels": "In(O) a(O) 1971(O) interview(O) with(O) The(B-magazine) London(I-magazine) Magazine(I-magazine) ,(O) Hughes(B-writer) cited(O) his(O) main(O) influences(O) as(O) including(O) William(B-writer) Blake(I-writer) ,(O) John(B-writer) Donne(I-writer) ,(O) Hopkins(B-writer) and(O) T.(B-writer) S.(I-writer) Eliot(I-writer) .(O)"}}
{"id": "215", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "literary genre", "magazine", "organization", "award", "writer", "book", "event", "person", "location", "poem"], "instance": {"id": "215", "words": ["Hasanaginica", "was", "the", "only", "authentic", "ballad", "included", "into", "La", "Guzla", ",", "an", "1827", "literary", "hoax", "of", "Prosper", "Mérimée", "."], "labels": ["B-book", "O", "O", "O", "O", "O", "O", "O", "B-poem", "I-poem", "O", "O", "B-event", "I-event", "I-event", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, literary genre, magazine, organization, award, writer, book, event, person, location, poem and O.\nSentence: Hasanaginica was the only authentic ballad included into La Guzla , an 1827 literary hoax of Prosper Mérimée .", "prompt_labels": "Hasanaginica(B-book) was(O) the(O) only(O) authentic(O) ballad(O) included(O) into(O) La(B-poem) Guzla(I-poem) ,(O) an(O) 1827(B-event) literary(I-event) hoax(I-event) of(O) Prosper(B-writer) Mérimée(I-writer) .(O)"}}
{"id": "148", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "magazine", "writer", "organization", "country", "location", "event", "award", "person", "poem", "book"], "instance": {"id": "148", "words": ["Many", "have", "been", "successfully", "adapted", "into", "films", ",", "including", "the", "novels", "Rebecca", ",", "My", "Cousin", "Rachel", ",", "and", "Jamaica", "Inn", ",", "and", "the", "short", "stories", "The", "Birds", "and", "Don", "'t", "Look", "Now", "/", "Not", "After", "Midnight", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "B-book", "O", "B-book", "I-book", "I-book", "O", "O", "B-book", "I-book", "O", "O", "O", "B-literary genre", "I-literary genre", "B-book", "I-book", "O", "B-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, magazine, writer, organization, country, location, event, award, person, poem, book and O.\nSentence: Many have been successfully adapted into films , including the novels Rebecca , My Cousin Rachel , and Jamaica Inn , and the short stories The Birds and Don 't Look Now / Not After Midnight .", "prompt_labels": "Many(O) have(O) been(O) successfully(O) adapted(O) into(O) films(O) ,(O) including(O) the(O) novels(B-literary genre) Rebecca(B-book) ,(O) My(B-book) Cousin(I-book) Rachel(I-book) ,(O) and(O) Jamaica(B-book) Inn(I-book) ,(O) and(O) the(O) short(B-literary genre) stories(I-literary genre) The(B-book) Birds(I-book) and(O) Don(B-book) 't(I-book) Look(I-book) Now(I-book) /(O) Not(B-book) After(I-book) Midnight(I-book) .(O)"}}
{"id": "180", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "writer", "event", "organization", "person", "poem", "literary genre", "award", "magazine", "book", "country"], "instance": {"id": "180", "words": ["Fredric", "Brown", "employed", "this", "subgenre", "to", "satirize", "the", "science", "fiction", "pulps", "and", "their", "adolescent", "readers", "-", "and", "fears", "of", "foreign", "invasion", "-", "in", "the", "classic", "What", "Mad", "Universe", "(", "1949", ")", "."], "labels": ["B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "I-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, writer, event, organization, person, poem, literary genre, award, magazine, book, country and O.\nSentence: Fredric Brown employed this subgenre to satirize the science fiction pulps and their adolescent readers - and fears of foreign invasion - in the classic What Mad Universe ( 1949 ) .", "prompt_labels": "Fredric(B-writer) Brown(I-writer) employed(O) this(O) subgenre(O) to(O) satirize(O) the(O) science(B-literary genre) fiction(I-literary genre) pulps(I-literary genre) and(O) their(O) adolescent(O) readers(O) -(O) and(O) fears(O) of(O) foreign(O) invasion(O) -(O) in(O) the(O) classic(O) What(B-book) Mad(I-book) Universe(I-book) ((O) 1949(O) )(O) .(O)"}}
{"id": "211", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "country", "writer", "location", "magazine", "literary genre", "poem", "book", "award", "organization", "person"], "instance": {"id": "211", "words": ["Earlier", ",", "in", "1921", ",", "he", "was", "asked", "by", "the", "biochemist", "and", "president", "of", "the", "World", "Zionist", "Organization", ",", "Chaim", "Weizmann", ",", "to", "help", "raise", "funds", "for", "the", "planned", "university", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, writer, location, magazine, literary genre, poem, book, award, organization, person and O.\nSentence: Earlier , in 1921 , he was asked by the biochemist and president of the World Zionist Organization , Chaim Weizmann , to help raise funds for the planned university .", "prompt_labels": "Earlier(O) ,(O) in(O) 1921(O) ,(O) he(O) was(O) asked(O) by(O) the(O) biochemist(O) and(O) president(O) of(O) the(O) World(B-organization) Zionist(I-organization) Organization(I-organization) ,(O) Chaim(B-person) Weizmann(I-person) ,(O) to(O) help(O) raise(O) funds(O) for(O) the(O) planned(O) university(O) .(O)"}}
{"id": "216", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "magazine", "event", "book", "literary genre", "location", "writer", "country", "person", "poem", "organization"], "instance": {"id": "216", "words": ["The", "list", "has", "also", "no", "films", "or", "directors", "from", "Tarkovsky", "'s", "native", "Russia", ",", "although", "he", "rated", "Soviet", "directors", "such", "as", "Boris", "Barnet", ",", "Sergei", "Parajanov", "and", "Alexander", "Dovzhenko", "highly", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "B-country", "O", "O", "O", "O", "B-country", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, magazine, event, book, literary genre, location, writer, country, person, poem, organization and O.\nSentence: The list has also no films or directors from Tarkovsky 's native Russia , although he rated Soviet directors such as Boris Barnet , Sergei Parajanov and Alexander Dovzhenko highly .", "prompt_labels": "The(O) list(O) has(O) also(O) no(O) films(O) or(O) directors(O) from(O) Tarkovsky(B-writer) 's(O) native(O) Russia(B-country) ,(O) although(O) he(O) rated(O) Soviet(B-country) directors(O) such(O) as(O) Boris(B-person) Barnet(I-person) ,(O) Sergei(B-person) Parajanov(I-person) and(O) Alexander(B-person) Dovzhenko(I-person) highly(O) .(O)"}}
{"id": "101", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "poem", "writer", "award", "book", "organization", "person", "event", "literary genre", "country", "location"], "instance": {"id": "101", "words": ["Nimzowitsch", "'s", "vanity", "and", "faith", "in", "his", "ideas", "of", "overprotection", "provoked", "Hans", "Kmoch", "to", "write", "a", "parody", "about", "him", "in", "February", "1928", "in", "the", "Wiener", "Schachzeitung", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, poem, writer, award, book, organization, person, event, literary genre, country, location and O.\nSentence: Nimzowitsch 's vanity and faith in his ideas of overprotection provoked Hans Kmoch to write a parody about him in February 1928 in the Wiener Schachzeitung .", "prompt_labels": "Nimzowitsch(B-person) 's(O) vanity(O) and(O) faith(O) in(O) his(O) ideas(O) of(O) overprotection(O) provoked(O) Hans(B-person) Kmoch(I-person) to(O) write(O) a(O) parody(O) about(O) him(O) in(O) February(O) 1928(O) in(O) the(O) Wiener(B-magazine) Schachzeitung(I-magazine) .(O)"}}
{"id": "62", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "magazine", "writer", "organization", "person", "book", "location", "country", "event", "award", "poem"], "instance": {"id": "62", "words": ["In", "1979", "a", "cartoon", "based", "on", "Seton", "'s", "1922", "book", "Bannertail", "was", "produced", "in", "Japan", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "B-book", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, magazine, writer, organization, person, book, location, country, event, award, poem and O.\nSentence: In 1979 a cartoon based on Seton 's 1922 book Bannertail was produced in Japan .", "prompt_labels": "In(O) 1979(O) a(O) cartoon(O) based(O) on(O) Seton(B-writer) 's(O) 1922(O) book(O) Bannertail(B-book) was(O) produced(O) in(O) Japan(B-country) .(O)"}}
{"id": "387", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "book", "organization", "event", "person", "poem", "writer", "award", "literary genre", "country", "location"], "instance": {"id": "387", "words": ["The", "dramatist", ",", "author", "and", "philosopher", "Voltaire", "created", ",", "with", "the", "support", "of", "the", "Académie", "française", ",", "a", "twelve-volume", "annotated", "set", "of", "Corneille", "'s", "dramatic", "works", ",", "the", "Commentaires", "sur", "Corneille", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, book, organization, event, person, poem, writer, award, literary genre, country, location and O.\nSentence: The dramatist , author and philosopher Voltaire created , with the support of the Académie française , a twelve-volume annotated set of Corneille 's dramatic works , the Commentaires sur Corneille .", "prompt_labels": "The(O) dramatist(O) ,(O) author(O) and(O) philosopher(O) Voltaire(B-writer) created(O) ,(O) with(O) the(O) support(O) of(O) the(O) Académie(B-organization) française(I-organization) ,(O) a(O) twelve-volume(O) annotated(O) set(O) of(O) Corneille(B-writer) 's(O) dramatic(O) works(O) ,(O) the(O) Commentaires(B-book) sur(I-book) Corneille(I-book) .(O)"}}
{"id": "175", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "country", "award", "event", "organization", "literary genre", "writer", "location", "person", "poem", "book"], "instance": {"id": "175", "words": ["On", "June", "17", ",", "1915", ",", "shortly", "after", "submitting", "his", "patent", "application", "for", "the", "doll", "'s", "design", ",", "Johnny", "Gruelle", "applied", "for", "a", "registered", "trademark", "for", "the", "Raggedy", "Ann", "name", ",", "which", "he", "created", "by", "combining", "words", "from", "two", "of", "James", "Whitcomb", "Riley", "poems", ",", "The", "Raggedy", "Man", "and", "Little", "Orphant", "Annie", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "B-literary genre", "O", "B-poem", "I-poem", "I-poem", "O", "B-poem", "I-poem", "I-poem", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, country, award, event, organization, literary genre, writer, location, person, poem, book and O.\nSentence: On June 17 , 1915 , shortly after submitting his patent application for the doll 's design , Johnny Gruelle applied for a registered trademark for the Raggedy Ann name , which he created by combining words from two of James Whitcomb Riley poems , The Raggedy Man and Little Orphant Annie .", "prompt_labels": "On(O) June(O) 17(O) ,(O) 1915(O) ,(O) shortly(O) after(O) submitting(O) his(O) patent(O) application(O) for(O) the(O) doll(O) 's(O) design(O) ,(O) Johnny(B-person) Gruelle(I-person) applied(O) for(O) a(O) registered(O) trademark(O) for(O) the(O) Raggedy(O) Ann(O) name(O) ,(O) which(O) he(O) created(O) by(O) combining(O) words(O) from(O) two(O) of(O) James(B-writer) Whitcomb(I-writer) Riley(I-writer) poems(B-literary genre) ,(O) The(B-poem) Raggedy(I-poem) Man(I-poem) and(O) Little(B-poem) Orphant(I-poem) Annie(I-poem) .(O)"}}
{"id": "308", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "event", "book", "location", "writer", "country", "poem", "organization", "award", "person", "magazine"], "instance": {"id": "308", "words": ["The", "Anglo-Irish", "writer", "Jonathan", "Swift", "(", "1667-1745", ")", ",", "in", "his", "Discourse", "on", "the", "Contests", "and", "Dissentions", "in", "Athens", "and", "Rome", ",", "criticized", "Augustus", "for", "installing", "tyranny", "over", "Rome", ",", "and", "likened", "what", "he", "believed", "United", "Kingdom", "'", "s", "virtuous", "constitutional", "monarchy", "to", "Rome", "'s", "moral", "Republic", "of", "the", "2nd", "century", "BC", "."], "labels": ["O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "B-person", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, event, book, location, writer, country, poem, organization, award, person, magazine and O.\nSentence: The Anglo-Irish writer Jonathan Swift ( 1667-1745 ) , in his Discourse on the Contests and Dissentions in Athens and Rome , criticized Augustus for installing tyranny over Rome , and likened what he believed United Kingdom ' s virtuous constitutional monarchy to Rome 's moral Republic of the 2nd century BC .", "prompt_labels": "The(O) Anglo-Irish(O) writer(O) Jonathan(B-writer) Swift(I-writer) ((O) 1667-1745(O) )(O) ,(O) in(O) his(O) Discourse(B-book) on(I-book) the(I-book) Contests(I-book) and(I-book) Dissentions(I-book) in(I-book) Athens(I-book) and(I-book) Rome(I-book) ,(O) criticized(O) Augustus(B-person) for(O) installing(O) tyranny(O) over(O) Rome(B-location) ,(O) and(O) likened(O) what(O) he(O) believed(O) United(B-country) Kingdom(I-country) '(O) s(O) virtuous(O) constitutional(O) monarchy(O) to(O) Rome(B-location) 's(O) moral(O) Republic(O) of(O) the(O) 2nd(O) century(O) BC(O) .(O)"}}
{"id": "67", "dataset": "crossner_literature", "split": "dev", "label_list": ["book", "poem", "magazine", "organization", "location", "person", "event", "country", "writer", "literary genre", "award"], "instance": {"id": "67", "words": ["This", "aversion", "to", "war", "also", "led", "Einstein", "to", "befriend", "author", "Upton", "Sinclair", "and", "film", "star", "Charlie", "Chaplin", ",", "both", "noted", "for", "their", "pacifism", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, poem, magazine, organization, location, person, event, country, writer, literary genre, award and O.\nSentence: This aversion to war also led Einstein to befriend author Upton Sinclair and film star Charlie Chaplin , both noted for their pacifism .", "prompt_labels": "This(O) aversion(O) to(O) war(O) also(O) led(O) Einstein(B-person) to(O) befriend(O) author(O) Upton(B-writer) Sinclair(I-writer) and(O) film(O) star(O) Charlie(B-person) Chaplin(I-person) ,(O) both(O) noted(O) for(O) their(O) pacifism(O) .(O)"}}
{"id": "8", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "writer", "award", "magazine", "organization", "literary genre", "location", "country", "book", "event", "poem"], "instance": {"id": "8", "words": ["In", "1829", "he", "was", "publicly", "crowned", "with", "laurel", "as", "the", "king", "of", "Nordic", "countries", "poetry", "and", "the", "Scandinavian", "King", "of", "Song", "(", "by", "Bishop", "Esaias", "Tegnér", ",", "who", "would", "be", "his", "Swedish", "parallel", ")", "in", "the", "cathedral", "of", "Lund", ",", "Sweden", ",", "based", "on", "a", "vast", "production", "of", "poetry", ",", "theatre", "plays", "and", "prose", ",", "inspired", "by", "Johann", "Wolfgang", "von", "Goethe", ",", "Gottlieb", "Fichte", ",", "and", "Friedrich", "von", "Schelling", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "I-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "B-literary genre", "I-literary genre", "O", "B-literary genre", "O", "O", "O", "B-writer", "I-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "B-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, writer, award, magazine, organization, literary genre, location, country, book, event, poem and O.\nSentence: In 1829 he was publicly crowned with laurel as the king of Nordic countries poetry and the Scandinavian King of Song ( by Bishop Esaias Tegnér , who would be his Swedish parallel ) in the cathedral of Lund , Sweden , based on a vast production of poetry , theatre plays and prose , inspired by Johann Wolfgang von Goethe , Gottlieb Fichte , and Friedrich von Schelling .", "prompt_labels": "In(O) 1829(O) he(O) was(O) publicly(O) crowned(O) with(O) laurel(O) as(O) the(O) king(O) of(O) Nordic(B-literary genre) countries(I-literary genre) poetry(I-literary genre) and(O) the(O) Scandinavian(O) King(O) of(O) Song(O) ((O) by(O) Bishop(O) Esaias(B-writer) Tegnér(I-writer) ,(O) who(O) would(O) be(O) his(O) Swedish(O) parallel(O) )(O) in(O) the(O) cathedral(B-location) of(I-location) Lund(I-location) ,(O) Sweden(B-country) ,(O) based(O) on(O) a(O) vast(O) production(O) of(O) poetry(B-literary genre) ,(O) theatre(B-literary genre) plays(I-literary genre) and(O) prose(B-literary genre) ,(O) inspired(O) by(O) Johann(B-writer) Wolfgang(I-writer) von(I-writer) Goethe(I-writer) ,(O) Gottlieb(B-writer) Fichte(I-writer) ,(O) and(O) Friedrich(B-writer) von(I-writer) Schelling(I-writer) .(O)"}}
{"id": "51", "dataset": "crossner_literature", "split": "dev", "label_list": ["poem", "magazine", "event", "literary genre", "person", "location", "country", "award", "writer", "organization", "book"], "instance": {"id": "51", "words": ["Moreover", ",", "he", "planned", "the", "publication", "of", "the", "compilation", "Nietzsche", "contra", "Wagner", "and", "of", "the", "poems", "that", "made", "up", "his", "collection", "Dionysian-Dithyrambs", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, magazine, event, literary genre, person, location, country, award, writer, organization, book and O.\nSentence: Moreover , he planned the publication of the compilation Nietzsche contra Wagner and of the poems that made up his collection Dionysian-Dithyrambs .", "prompt_labels": "Moreover(O) ,(O) he(O) planned(O) the(O) publication(O) of(O) the(O) compilation(O) Nietzsche(B-book) contra(I-book) Wagner(I-book) and(O) of(O) the(O) poems(O) that(O) made(O) up(O) his(O) collection(O) Dionysian-Dithyrambs(B-book) .(O)"}}
{"id": "42", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "writer", "award", "event", "literary genre", "location", "poem", "person", "magazine", "country", "book"], "instance": {"id": "42", "words": ["She", "was", "appointed", "Burmese", "ambassador", "to", "India", "and", "Nepal", "in", "1960", ",", "and", "Aung", "San", "Suu", "Kyi", "followed", "her", "there", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "I-writer", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, writer, award, event, literary genre, location, poem, person, magazine, country, book and O.\nSentence: She was appointed Burmese ambassador to India and Nepal in 1960 , and Aung San Suu Kyi followed her there .", "prompt_labels": "She(O) was(O) appointed(O) Burmese(O) ambassador(O) to(O) India(B-country) and(O) Nepal(B-country) in(O) 1960(O) ,(O) and(O) Aung(B-writer) San(I-writer) Suu(I-writer) Kyi(I-writer) followed(O) her(O) there(O) .(O)"}}
{"id": "311", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "book", "person", "location", "literary genre", "event", "poem", "writer", "award", "magazine", "country"], "instance": {"id": "311", "words": ["He", "was", "a", "court", "poet", "of", "Udaya", "Varma", "(", "1446-1475", ")", "and", "the", "author", "of", "Krishnagatha", ",", "a", "poem", "which", "is", "considered", "a", "landmark", "in", "the", "development", "of", "Malayalam", "literature", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "B-poem", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, book, person, location, literary genre, event, poem, writer, award, magazine, country and O.\nSentence: He was a court poet of Udaya Varma ( 1446-1475 ) and the author of Krishnagatha , a poem which is considered a landmark in the development of Malayalam literature .", "prompt_labels": "He(O) was(O) a(O) court(O) poet(O) of(O) Udaya(B-person) Varma(I-person) ((O) 1446-1475(O) )(O) and(O) the(O) author(O) of(O) Krishnagatha(B-poem) ,(O) a(O) poem(B-literary genre) which(O) is(O) considered(O) a(O) landmark(O) in(O) the(O) development(O) of(O) Malayalam(B-literary genre) literature(I-literary genre) .(O)"}}
{"id": "81", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "literary genre", "writer", "book", "country", "award", "organization", "event", "poem", "person", "magazine"], "instance": {"id": "81", "words": ["It", "was", "adapted", "by", "Talbot", "Jennings", ",", "Tess", "Slesinger", ",", "and", "Claudine", "West", "from", "the", "play", "by", "Owen", "Davis", "and", "Donald", "Davis", ",", "which", "was", "in", "itself", "based", "on", "the", "1931", "The", "Good", "Earth", "by", "Nobel", "Prize", "-winning", "author", "Pearl", "S.", "Buck", "."], "labels": ["O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "B-award", "I-award", "O", "O", "B-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, literary genre, writer, book, country, award, organization, event, poem, person, magazine and O.\nSentence: It was adapted by Talbot Jennings , Tess Slesinger , and Claudine West from the play by Owen Davis and Donald Davis , which was in itself based on the 1931 The Good Earth by Nobel Prize -winning author Pearl S. Buck .", "prompt_labels": "It(O) was(O) adapted(O) by(O) Talbot(B-writer) Jennings(I-writer) ,(O) Tess(B-writer) Slesinger(I-writer) ,(O) and(O) Claudine(B-writer) West(I-writer) from(O) the(O) play(O) by(O) Owen(B-writer) Davis(I-writer) and(O) Donald(B-writer) Davis(I-writer) ,(O) which(O) was(O) in(O) itself(O) based(O) on(O) the(O) 1931(O) The(B-book) Good(I-book) Earth(I-book) by(O) Nobel(B-award) Prize(I-award) -winning(O) author(O) Pearl(B-writer) S.(I-writer) Buck(I-writer) .(O)"}}
{"id": "394", "dataset": "crossner_literature", "split": "dev", "label_list": ["book", "event", "organization", "writer", "magazine", "country", "poem", "location", "literary genre", "person", "award"], "instance": {"id": "394", "words": ["Nabokov", "'s", "Lolita", "(", "1955", ")", "was", "ranked", "fourth", "in", "the", "list", "of", "the", "Modern", "Library", "100", "Best", "Novels", "in", "2007", ";", "He", "was", "a", "finalist", "for", "the", "National", "Book", "Award", "for", "Fiction", "seven", "times", "."], "labels": ["B-writer", "O", "B-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, event, organization, writer, magazine, country, poem, location, literary genre, person, award and O.\nSentence: Nabokov 's Lolita ( 1955 ) was ranked fourth in the list of the Modern Library 100 Best Novels in 2007 ; He was a finalist for the National Book Award for Fiction seven times .", "prompt_labels": "Nabokov(B-writer) 's(O) Lolita(B-book) ((O) 1955(O) )(O) was(O) ranked(O) fourth(O) in(O) the(O) list(O) of(O) the(O) Modern(O) Library(O) 100(O) Best(O) Novels(O) in(O) 2007(O) ;(O) He(O) was(O) a(O) finalist(O) for(O) the(O) National(B-award) Book(I-award) Award(I-award) for(I-award) Fiction(I-award) seven(O) times(O) .(O)"}}
{"id": "305", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "magazine", "country", "location", "book", "writer", "organization", "poem", "literary genre", "award", "event"], "instance": {"id": "305", "words": ["Le", "Guin", "influenced", "many", "other", "authors", ",", "including", "Booker", "Prize", "winner", "Salman", "Rushdie", ",", "David", "Mitchell", ",", "Neil", "Gaiman", ",", "and", "Iain", "Banks", "."], "labels": ["B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, magazine, country, location, book, writer, organization, poem, literary genre, award, event and O.\nSentence: Le Guin influenced many other authors , including Booker Prize winner Salman Rushdie , David Mitchell , Neil Gaiman , and Iain Banks .", "prompt_labels": "Le(B-writer) Guin(I-writer) influenced(O) many(O) other(O) authors(O) ,(O) including(O) Booker(B-award) Prize(I-award) winner(O) Salman(B-writer) Rushdie(I-writer) ,(O) David(B-writer) Mitchell(I-writer) ,(O) Neil(B-writer) Gaiman(I-writer) ,(O) and(O) Iain(B-writer) Banks(I-writer) .(O)"}}
{"id": "203", "dataset": "crossner_literature", "split": "dev", "label_list": ["book", "event", "writer", "poem", "magazine", "award", "country", "literary genre", "location", "person", "organization"], "instance": {"id": "203", "words": ["The", "provinces", "ceded", "to", "Augustus", "for", "that", "ten-year", "period", "comprised", "much", "of", "the", "conquered", "Roman", "world", ",", "including", "all", "of", "Hispania", "and", "Gaul", ",", "Syria", ",", "Cilicia", ",", "Cyprus", ",", "and", "Egypt", "."], "labels": ["O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, event, writer, poem, magazine, award, country, literary genre, location, person, organization and O.\nSentence: The provinces ceded to Augustus for that ten-year period comprised much of the conquered Roman world , including all of Hispania and Gaul , Syria , Cilicia , Cyprus , and Egypt .", "prompt_labels": "The(O) provinces(O) ceded(O) to(O) Augustus(B-person) for(O) that(O) ten-year(O) period(O) comprised(O) much(O) of(O) the(O) conquered(O) Roman(O) world(O) ,(O) including(O) all(O) of(O) Hispania(B-location) and(O) Gaul(B-location) ,(O) Syria(B-location) ,(O) Cilicia(B-location) ,(O) Cyprus(B-location) ,(O) and(O) Egypt(B-location) .(O)"}}
{"id": "169", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "location", "book", "poem", "literary genre", "person", "organization", "country", "award", "event", "magazine"], "instance": {"id": "169", "words": ["It", "is", "based", "on", "H.", "P.", "Lovecraft", "'", "s", "Cthulhu", "Mythos", ",", "particularly", "At", "the", "Mountains", "of", "Madness", ",", "and", "is", "a", "follow-up", "to", "Infogrames", "'", "earlier", "Shadow", "of", "the", "Comet", "."], "labels": ["O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, location, book, poem, literary genre, person, organization, country, award, event, magazine and O.\nSentence: It is based on H. P. Lovecraft ' s Cthulhu Mythos , particularly At the Mountains of Madness , and is a follow-up to Infogrames ' earlier Shadow of the Comet .", "prompt_labels": "It(O) is(O) based(O) on(O) H.(B-writer) P.(I-writer) Lovecraft(I-writer) '(O) s(O) Cthulhu(O) Mythos(O) ,(O) particularly(O) At(B-book) the(I-book) Mountains(I-book) of(I-book) Madness(I-book) ,(O) and(O) is(O) a(O) follow-up(O) to(O) Infogrames(B-organization) '(O) earlier(O) Shadow(O) of(O) the(O) Comet(O) .(O)"}}
{"id": "130", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "writer", "location", "event", "country", "magazine", "poem", "award", "book", "literary genre", "organization"], "instance": {"id": "130", "words": ["At", "the", "National", "Book", "Award", "s", "ceremony", "in", "November", "2014", ",", "Handler", "made", "a", "controversial", "remark", "after", "author", "Jacqueline", "Woodson", "was", "presented", "with", "an", "award", "for", "young", "people", "'s", "literature", "."], "labels": ["O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, writer, location, event, country, magazine, poem, award, book, literary genre, organization and O.\nSentence: At the National Book Award s ceremony in November 2014 , Handler made a controversial remark after author Jacqueline Woodson was presented with an award for young people 's literature .", "prompt_labels": "At(O) the(O) National(B-award) Book(I-award) Award(I-award) s(O) ceremony(O) in(O) November(O) 2014(O) ,(O) Handler(B-writer) made(O) a(O) controversial(O) remark(O) after(O) author(O) Jacqueline(B-writer) Woodson(I-writer) was(O) presented(O) with(O) an(O) award(O) for(O) young(O) people(O) 's(O) literature(O) .(O)"}}
{"id": "18", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "organization", "award", "writer", "event", "literary genre", "location", "country", "poem", "book", "magazine"], "instance": {"id": "18", "words": ["His", "most", "famous", "work", "is", "The", "Chronicles", "of", "Prydain", ",", "a", "series", "of", "five", "high", "fantasy", "novels", "whose", "conclusion", ",", "The", "High", "King", ",", "was", "awarded", "the", "1969", "Newbery", "Medal", "for", "excellence", "in", "American", "children", "'s", "literature", "."], "labels": ["O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "I-literary genre", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "B-literary genre", "I-literary genre", "I-literary genre", "I-literary genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, award, writer, event, literary genre, location, country, poem, book, magazine and O.\nSentence: His most famous work is The Chronicles of Prydain , a series of five high fantasy novels whose conclusion , The High King , was awarded the 1969 Newbery Medal for excellence in American children 's literature .", "prompt_labels": "His(O) most(O) famous(O) work(O) is(O) The(B-book) Chronicles(I-book) of(I-book) Prydain(I-book) ,(O) a(O) series(O) of(O) five(O) high(B-literary genre) fantasy(I-literary genre) novels(I-literary genre) whose(O) conclusion(O) ,(O) The(B-book) High(I-book) King(I-book) ,(O) was(O) awarded(O) the(O) 1969(O) Newbery(B-award) Medal(I-award) for(O) excellence(O) in(O) American(B-literary genre) children(I-literary genre) 's(I-literary genre) literature(I-literary genre) .(O)"}}
{"id": "365", "dataset": "crossner_literature", "split": "dev", "label_list": ["poem", "writer", "location", "organization", "literary genre", "magazine", "event", "book", "person", "award", "country"], "instance": {"id": "365", "words": ["In", "2009", ",", "he", "was", "named", "the", "United", "Nations", "Special", "Envoy", "to", "Haiti", "and", "after", "the", "2010", "Haiti", "earthquake", ",", "he", "teamed", "up", "with", "George", "W.", "Bush", "to", "form", "the", "Clinton", "Bush", "Haiti", "Fund", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "B-country", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, writer, location, organization, literary genre, magazine, event, book, person, award, country and O.\nSentence: In 2009 , he was named the United Nations Special Envoy to Haiti and after the 2010 Haiti earthquake , he teamed up with George W. Bush to form the Clinton Bush Haiti Fund .", "prompt_labels": "In(O) 2009(O) ,(O) he(O) was(O) named(O) the(O) United(B-organization) Nations(I-organization) Special(O) Envoy(O) to(O) Haiti(B-country) and(O) after(O) the(O) 2010(B-event) Haiti(I-event) earthquake(I-event) ,(O) he(O) teamed(O) up(O) with(O) George(B-person) W.(I-person) Bush(I-person) to(O) form(O) the(O) Clinton(B-organization) Bush(I-organization) Haiti(I-organization) Fund(I-organization) .(O)"}}
{"id": "144", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "writer", "magazine", "literary genre", "location", "award", "event", "organization", "book", "person", "poem"], "instance": {"id": "144", "words": ["In", "a", "review", "in", "The", "Dial", ",", "T.", "S.", "Eliot", "said", "of", "Ulysses", ":", "I", "hold", "this", "book", "to", "be", "the", "most", "important", "expression", "which", "the", "present", "age", "has", "found", ";", "it", "is", "a", "book", "to", "which", "we", "are", "all", "indebted", ",", "and", "from", "which", "none", "of", "us", "can", "escape", "."], "labels": ["O", "O", "O", "O", "B-magazine", "I-magazine", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, writer, magazine, literary genre, location, award, event, organization, book, person, poem and O.\nSentence: In a review in The Dial , T. S. Eliot said of Ulysses : I hold this book to be the most important expression which the present age has found ; it is a book to which we are all indebted , and from which none of us can escape .", "prompt_labels": "In(O) a(O) review(O) in(O) The(B-magazine) Dial(I-magazine) ,(O) T.(B-writer) S.(I-writer) Eliot(I-writer) said(O) of(O) Ulysses(B-book) :(O) I(O) hold(O) this(O) book(O) to(O) be(O) the(O) most(O) important(O) expression(O) which(O) the(O) present(O) age(O) has(O) found(O) ;(O) it(O) is(O) a(O) book(O) to(O) which(O) we(O) are(O) all(O) indebted(O) ,(O) and(O) from(O) which(O) none(O) of(O) us(O) can(O) escape(O) .(O)"}}
{"id": "265", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "person", "book", "writer", "poem", "location", "award", "event", "organization", "magazine", "country"], "instance": {"id": "265", "words": ["Guido", "delle", "Colonne", "of", "Messina", ",", "one", "of", "the", "vernacular", "poets", "of", "the", "Sicilian", "school", ",", "composed", "the", "Historia", "destructionis", "Troiae", "."], "labels": ["B-writer", "I-writer", "I-writer", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, person, book, writer, poem, location, award, event, organization, magazine, country and O.\nSentence: Guido delle Colonne of Messina , one of the vernacular poets of the Sicilian school , composed the Historia destructionis Troiae .", "prompt_labels": "Guido(B-writer) delle(I-writer) Colonne(I-writer) of(O) Messina(B-location) ,(O) one(O) of(O) the(O) vernacular(O) poets(O) of(O) the(O) Sicilian(O) school(O) ,(O) composed(O) the(O) Historia(B-poem) destructionis(I-poem) Troiae(I-poem) .(O)"}}
{"id": "224", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "event", "magazine", "organization", "poem", "country", "book", "location", "literary genre", "person", "writer"], "instance": {"id": "224", "words": ["In", "1900", ",", "Potter", "revised", "her", "tale", "about", "the", "four", "little", "rabbits", ",", "and", "fashioned", "a", "dummy", "book", "of", "it", "-", "it", "has", "been", "suggested", ",", "in", "imitation", "of", "Helen", "Bannerman", "'", "s", "1899", "bestseller", "The", "Story", "of", "Little", "Black", "Sambo", ".Stevenson", ",", "Laura", "C.", "A", "Vogue", "for", "Small", "Books", ":", "The", "Tale", "of", "Peter", "Rabbit", "and", "its", "Contemporary", "Competitors", "to", "reproduce", "her", "watercolours", "."], "labels": ["O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "B-writer", "I-writer", "I-writer", "I-writer", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, magazine, organization, poem, country, book, location, literary genre, person, writer and O.\nSentence: In 1900 , Potter revised her tale about the four little rabbits , and fashioned a dummy book of it - it has been suggested , in imitation of Helen Bannerman ' s 1899 bestseller The Story of Little Black Sambo .Stevenson , Laura C. A Vogue for Small Books : The Tale of Peter Rabbit and its Contemporary Competitors to reproduce her watercolours .", "prompt_labels": "In(O) 1900(O) ,(O) Potter(B-writer) revised(O) her(O) tale(O) about(O) the(O) four(O) little(O) rabbits(O) ,(O) and(O) fashioned(O) a(O) dummy(O) book(O) of(O) it(O) -(O) it(O) has(O) been(O) suggested(O) ,(O) in(O) imitation(O) of(O) Helen(B-writer) Bannerman(I-writer) '(O) s(O) 1899(O) bestseller(O) The(B-book) Story(I-book) of(I-book) Little(I-book) Black(I-book) Sambo(I-book) .Stevenson(B-writer) ,(I-writer) Laura(I-writer) C.(I-writer) A(B-book) Vogue(I-book) for(I-book) Small(I-book) Books(I-book) :(I-book) The(I-book) Tale(I-book) of(I-book) Peter(I-book) Rabbit(I-book) and(O) its(O) Contemporary(O) Competitors(O) to(O) reproduce(O) her(O) watercolours(O) .(O)"}}
{"id": "389", "dataset": "crossner_literature", "split": "dev", "label_list": ["book", "writer", "award", "event", "magazine", "organization", "poem", "literary genre", "country", "person", "location"], "instance": {"id": "389", "words": ["Sabotage", "was", "loosely", "based", "on", "Joseph", "Conrad", "'", "s", "novel", ",", "The", "Secret", "Agent", "(", "1907", ")", ",", "about", "a", "woman", "who", "discovers", "that", "her", "husband", "is", "a", "terrorist", ",", "and", "Secret", "Agent", ",", "based", "on", "two", "stories", "in", "Ashenden", ":", "Or", "the", "British", "Agent", "(", "1928", ")", "by", "W.", "Somerset", "Maugham", "."], "labels": ["B-book", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, writer, award, event, magazine, organization, poem, literary genre, country, person, location and O.\nSentence: Sabotage was loosely based on Joseph Conrad ' s novel , The Secret Agent ( 1907 ) , about a woman who discovers that her husband is a terrorist , and Secret Agent , based on two stories in Ashenden : Or the British Agent ( 1928 ) by W. Somerset Maugham .", "prompt_labels": "Sabotage(B-book) was(O) loosely(O) based(O) on(O) Joseph(B-writer) Conrad(I-writer) '(O) s(O) novel(O) ,(O) The(B-book) Secret(I-book) Agent(I-book) ((O) 1907(O) )(O) ,(O) about(O) a(O) woman(O) who(O) discovers(O) that(O) her(O) husband(O) is(O) a(O) terrorist(O) ,(O) and(O) Secret(B-book) Agent(I-book) ,(O) based(O) on(O) two(O) stories(O) in(O) Ashenden(B-book) :(I-book) Or(I-book) the(I-book) British(I-book) Agent(I-book) ((O) 1928(O) )(O) by(O) W.(B-writer) Somerset(I-writer) Maugham(I-writer) .(O)"}}
{"id": "218", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "organization", "country", "book", "award", "person", "poem", "magazine", "event", "location", "writer"], "instance": {"id": "218", "words": ["Auden", "'s", "next", "large-scale", "work", "was", "The", "Orators", ":", "An", "English", "Study", "(", "1932", ";", "revised", "editions", ",", "1934", ",", "1966", ")", ",", "in", "verse", "and", "prose", ",", "largely", "about", "hero-worship", "in", "personal", "and", "political", "life", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, organization, country, book, award, person, poem, magazine, event, location, writer and O.\nSentence: Auden 's next large-scale work was The Orators : An English Study ( 1932 ; revised editions , 1934 , 1966 ) , in verse and prose , largely about hero-worship in personal and political life .", "prompt_labels": "Auden(B-writer) 's(O) next(O) large-scale(O) work(O) was(O) The(B-poem) Orators(I-poem) :(I-poem) An(I-poem) English(I-poem) Study(I-poem) ((O) 1932(O) ;(O) revised(O) editions(O) ,(O) 1934(O) ,(O) 1966(O) )(O) ,(O) in(O) verse(B-literary genre) and(O) prose(B-literary genre) ,(O) largely(O) about(O) hero-worship(O) in(O) personal(O) and(O) political(O) life(O) .(O)"}}
{"id": "359", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "poem", "book", "organization", "location", "award", "event", "country", "person", "literary genre", "writer"], "instance": {"id": "359", "words": ["Instead", "he", "wrote", "several", "fantasy", "novels", ",", "Elidor", "(", "1965", ")", ",", "The", "Owl", "Service", "(", "1967", ")", "and", "Red", "Shift", "(", "1973", ")", "."], "labels": ["O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "B-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, poem, book, organization, location, award, event, country, person, literary genre, writer and O.\nSentence: Instead he wrote several fantasy novels , Elidor ( 1965 ) , The Owl Service ( 1967 ) and Red Shift ( 1973 ) .", "prompt_labels": "Instead(O) he(O) wrote(O) several(O) fantasy(B-literary genre) novels(I-literary genre) ,(O) Elidor(B-book) ((O) 1965(O) )(O) ,(O) The(B-book) Owl(I-book) Service(I-book) ((O) 1967(O) )(O) and(O) Red(B-book) Shift(I-book) ((O) 1973(O) )(O) .(O)"}}
{"id": "107", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "location", "award", "event", "book", "literary genre", "writer", "magazine", "poem", "country", "person"], "instance": {"id": "107", "words": ["The", "work", "was", "such", "a", "popular", "success", "that", "the", "poet", "wrote", "a", "sequel", ",", "Remedia", "Amoris", "(", "Remedies", "for", "Love", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-poem", "I-poem", "O", "B-poem", "I-poem", "I-poem", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, award, event, book, literary genre, writer, magazine, poem, country, person and O.\nSentence: The work was such a popular success that the poet wrote a sequel , Remedia Amoris ( Remedies for Love ) .", "prompt_labels": "The(O) work(O) was(O) such(O) a(O) popular(O) success(O) that(O) the(O) poet(O) wrote(O) a(O) sequel(O) ,(O) Remedia(B-poem) Amoris(I-poem) ((O) Remedies(B-poem) for(I-poem) Love(I-poem) )(O) .(O)"}}
{"id": "307", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "book", "country", "location", "person", "organization", "magazine", "event", "writer", "poem", "award"], "instance": {"id": "307", "words": ["Also", ",", "In", "the", "short", "story", "La", "Santa", ",", "by", "Nobel", "Prize", "winner", "Gabriel", "García", "Márquez", "a", "character", "is", "named", "after", "Zavattini", "."], "labels": ["O", "O", "O", "O", "B-literary genre", "I-literary genre", "B-book", "I-book", "O", "O", "B-award", "I-award", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "B-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, book, country, location, person, organization, magazine, event, writer, poem, award and O.\nSentence: Also , In the short story La Santa , by Nobel Prize winner Gabriel García Márquez a character is named after Zavattini .", "prompt_labels": "Also(O) ,(O) In(O) the(O) short(B-literary genre) story(I-literary genre) La(B-book) Santa(I-book) ,(O) by(O) Nobel(B-award) Prize(I-award) winner(O) Gabriel(B-writer) García(I-writer) Márquez(I-writer) a(O) character(O) is(O) named(O) after(O) Zavattini(B-person) .(O)"}}
{"id": "155", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "literary genre", "location", "writer", "award", "magazine", "organization", "poem", "person", "country", "book"], "instance": {"id": "155", "words": ["He", "entered", "Yale", "in", "1903", "but", "did", "not", "receive", "his", "bachelor", "'s", "degree", "until", "1908", ",", "having", "taken", "time", "off", "to", "work", "at", "Helicon", "Home", "Colony", ",", "Upton", "Sinclair", "'", "s", "cooperative", "-living", "colony", "in", "Englewood", ",", "New", "Jersey", ",", "and", "to", "travel", "to", "Panama", "."], "labels": ["O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, literary genre, location, writer, award, magazine, organization, poem, person, country, book and O.\nSentence: He entered Yale in 1903 but did not receive his bachelor 's degree until 1908 , having taken time off to work at Helicon Home Colony , Upton Sinclair ' s cooperative -living colony in Englewood , New Jersey , and to travel to Panama .", "prompt_labels": "He(O) entered(O) Yale(B-organization) in(O) 1903(O) but(O) did(O) not(O) receive(O) his(O) bachelor(O) 's(O) degree(O) until(O) 1908(O) ,(O) having(O) taken(O) time(O) off(O) to(O) work(O) at(O) Helicon(B-organization) Home(I-organization) Colony(I-organization) ,(O) Upton(B-writer) Sinclair(I-writer) '(O) s(O) cooperative(O) -living(O) colony(O) in(O) Englewood(B-location) ,(O) New(B-location) Jersey(I-location) ,(O) and(O) to(O) travel(O) to(O) Panama(B-country) .(O)"}}
{"id": "375", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "poem", "location", "book", "event", "literary genre", "magazine", "organization", "person", "country", "writer"], "instance": {"id": "375", "words": ["He", "even", "writes", "a", "book", "about", "mystery", "fiction", "in", "which", "he", "deals", "sternly", "with", "Edgar", "Allan", "Poe", "and", "Wilkie", "Collins", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, poem, location, book, event, literary genre, magazine, organization, person, country, writer and O.\nSentence: He even writes a book about mystery fiction in which he deals sternly with Edgar Allan Poe and Wilkie Collins .", "prompt_labels": "He(O) even(O) writes(O) a(O) book(O) about(O) mystery(B-literary genre) fiction(I-literary genre) in(O) which(O) he(O) deals(O) sternly(O) with(O) Edgar(B-writer) Allan(I-writer) Poe(I-writer) and(O) Wilkie(B-writer) Collins(I-writer) .(O)"}}
{"id": "200", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "book", "organization", "writer", "person", "event", "literary genre", "location", "award", "poem", "magazine"], "instance": {"id": "200", "words": ["In", "1662", "he", "published", "The", "Day", "of", "Doom", "or", "a", "Poetical", "Description", "of", "the", "Great", "and", "Last", "Judgment", ",", "a", "doggerel", "epitome", "of", "Calvinistic", "theology", ",", "according", "to", "the", "anthology", ",", "Colonial", "Prose", "and", "Poetry", "(", "1903", ")", ",", "that", "attained", "immediately", "a", "phenomenal", "popularity", "."], "labels": ["O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, book, organization, writer, person, event, literary genre, location, award, poem, magazine and O.\nSentence: In 1662 he published The Day of Doom or a Poetical Description of the Great and Last Judgment , a doggerel epitome of Calvinistic theology , according to the anthology , Colonial Prose and Poetry ( 1903 ) , that attained immediately a phenomenal popularity .", "prompt_labels": "In(O) 1662(O) he(O) published(O) The(B-poem) Day(I-poem) of(I-poem) Doom(I-poem) or(I-poem) a(I-poem) Poetical(I-poem) Description(I-poem) of(I-poem) the(I-poem) Great(I-poem) and(I-poem) Last(I-poem) Judgment(I-poem) ,(O) a(O) doggerel(O) epitome(O) of(O) Calvinistic(O) theology(O) ,(O) according(O) to(O) the(O) anthology(O) ,(O) Colonial(B-book) Prose(I-book) and(I-book) Poetry(I-book) ((O) 1903(O) )(O) ,(O) that(O) attained(O) immediately(O) a(O) phenomenal(O) popularity(O) .(O)"}}
{"id": "2", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "location", "book", "person", "magazine", "country", "poem", "organization", "event", "award", "literary genre"], "instance": {"id": "2", "words": ["The", "fantasy", "critic", "L.", "Sprague", "de", "Camp", "said", "of", "him", "that", "nobody", "since", "Edgar", "Allan", "Poe", "has", "so", "loved", "a", "well-rotted", "corpse", "."], "labels": ["O", "O", "O", "B-writer", "I-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, location, book, person, magazine, country, poem, organization, event, award, literary genre and O.\nSentence: The fantasy critic L. Sprague de Camp said of him that nobody since Edgar Allan Poe has so loved a well-rotted corpse .", "prompt_labels": "The(O) fantasy(O) critic(O) L.(B-writer) Sprague(I-writer) de(I-writer) Camp(I-writer) said(O) of(O) him(O) that(O) nobody(O) since(O) Edgar(B-writer) Allan(I-writer) Poe(I-writer) has(O) so(O) loved(O) a(O) well-rotted(O) corpse(O) .(O)"}}
{"id": "76", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "poem", "country", "person", "award", "event", "organization", "book", "literary genre", "location", "magazine"], "instance": {"id": "76", "words": ["Jorge", "Luis", "Borges", "wrote", "a", "contemporary", "bestiary", "of", "sorts", ",", "the", "Book", "of", "Imaginary", "Beings", ",", "which", "collects", "imaginary", "beasts", "from", "bestiaries", "and", "fiction", "."], "labels": ["B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, poem, country, person, award, event, organization, book, literary genre, location, magazine and O.\nSentence: Jorge Luis Borges wrote a contemporary bestiary of sorts , the Book of Imaginary Beings , which collects imaginary beasts from bestiaries and fiction .", "prompt_labels": "Jorge(B-writer) Luis(I-writer) Borges(I-writer) wrote(O) a(O) contemporary(O) bestiary(O) of(O) sorts(O) ,(O) the(O) Book(B-book) of(I-book) Imaginary(I-book) Beings(I-book) ,(O) which(O) collects(O) imaginary(O) beasts(O) from(O) bestiaries(O) and(O) fiction(B-literary genre) .(O)"}}
{"id": "381", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "location", "magazine", "writer", "literary genre", "book", "event", "country", "person", "award", "poem"], "instance": {"id": "381", "words": ["Diaries", "had", "been", "written", "by", "men", "in", "Chinese", "for", "some", "time", ",", "but", "in", "the", "early", "tenth", "century", "Ki", "no", "Tsurayuki", "chose", "to", "write", "his", "Tosa", "Nikki", "from", "the", "standpoint", "of", "a", "woman", ",", "in", "kana", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "B-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, magazine, writer, literary genre, book, event, country, person, award, poem and O.\nSentence: Diaries had been written by men in Chinese for some time , but in the early tenth century Ki no Tsurayuki chose to write his Tosa Nikki from the standpoint of a woman , in kana .", "prompt_labels": "Diaries(O) had(O) been(O) written(O) by(O) men(O) in(O) Chinese(O) for(O) some(O) time(O) ,(O) but(O) in(O) the(O) early(O) tenth(O) century(O) Ki(B-writer) no(I-writer) Tsurayuki(I-writer) chose(O) to(O) write(O) his(O) Tosa(B-poem) Nikki(I-poem) from(O) the(O) standpoint(O) of(O) a(O) woman(O) ,(O) in(O) kana(O) .(O)"}}
{"id": "286", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "poem", "person", "event", "writer", "magazine", "literary genre", "award", "country", "book", "organization"], "instance": {"id": "286", "words": ["To", "earn", "money", ",", "he", "began", "writing", "short", "humorous", "articles", "for", "journals", "such", "as", "The", "Pall", "Mall", "Gazette", ",", "later", "collecting", "these", "in", "volume", "form", "as", "Select", "Conversations", "with", "an", "Uncle", "(", "1895", ")", "and", "Certain", "Personal", "Matters", "(", "1897", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "I-literary genre", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, poem, person, event, writer, magazine, literary genre, award, country, book, organization and O.\nSentence: To earn money , he began writing short humorous articles for journals such as The Pall Mall Gazette , later collecting these in volume form as Select Conversations with an Uncle ( 1895 ) and Certain Personal Matters ( 1897 ) .", "prompt_labels": "To(O) earn(O) money(O) ,(O) he(O) began(O) writing(O) short(B-literary genre) humorous(I-literary genre) articles(I-literary genre) for(O) journals(O) such(O) as(O) The(B-magazine) Pall(I-magazine) Mall(I-magazine) Gazette(I-magazine) ,(O) later(O) collecting(O) these(O) in(O) volume(O) form(O) as(O) Select(B-book) Conversations(I-book) with(I-book) an(I-book) Uncle(I-book) ((O) 1895(O) )(O) and(O) Certain(B-book) Personal(I-book) Matters(I-book) ((O) 1897(O) )(O) .(O)"}}
{"id": "143", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "writer", "organization", "literary genre", "location", "person", "award", "poem", "country", "book", "event"], "instance": {"id": "143", "words": ["For", "example", ",", "Susanna", "Moodie", "and", "Catharine", "Parr", "Traill", ",", "English", "sisters", "who", "adopted", "the", "country", "as", "their", "own", ",", "moved", "to", "Upper", "Canada", "in", "1832", "."], "labels": ["O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, writer, organization, literary genre, location, person, award, poem, country, book, event and O.\nSentence: For example , Susanna Moodie and Catharine Parr Traill , English sisters who adopted the country as their own , moved to Upper Canada in 1832 .", "prompt_labels": "For(O) example(O) ,(O) Susanna(B-writer) Moodie(I-writer) and(O) Catharine(B-writer) Parr(I-writer) Traill(I-writer) ,(O) English(O) sisters(O) who(O) adopted(O) the(O) country(O) as(O) their(O) own(O) ,(O) moved(O) to(O) Upper(B-country) Canada(I-country) in(O) 1832(O) .(O)"}}
{"id": "90", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "poem", "award", "writer", "person", "magazine", "event", "location", "organization", "literary genre", "book"], "instance": {"id": "90", "words": ["Confucianism", "reached", "its", "peak", "of", "influence", "during", "the", "Tang", "dynasty", "and", "Song", "dynasty", "Dynasties", "under", "a", "rebranded", "Confucianism", "called", "Neo-Confucianism", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, poem, award, writer, person, magazine, event, location, organization, literary genre, book and O.\nSentence: Confucianism reached its peak of influence during the Tang dynasty and Song dynasty Dynasties under a rebranded Confucianism called Neo-Confucianism .", "prompt_labels": "Confucianism(O) reached(O) its(O) peak(O) of(O) influence(O) during(O) the(O) Tang(B-country) dynasty(I-country) and(O) Song(B-country) dynasty(I-country) Dynasties(O) under(O) a(O) rebranded(O) Confucianism(O) called(O) Neo-Confucianism(O) .(O)"}}
{"id": "298", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "person", "magazine", "country", "poem", "writer", "organization", "book", "event", "location", "award"], "instance": {"id": "298", "words": ["In", "Vertigo", "(", "1958", ")", "and", "North", "by", "Northwest", "(", "1959", ")", "respectively", ",", "Kim", "Novak", "and", "Eva", "Marie", "Saint", "play", "the", "blonde", "heroines", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, person, magazine, country, poem, writer, organization, book, event, location, award and O.\nSentence: In Vertigo ( 1958 ) and North by Northwest ( 1959 ) respectively , Kim Novak and Eva Marie Saint play the blonde heroines .", "prompt_labels": "In(O) Vertigo(O) ((O) 1958(O) )(O) and(O) North(O) by(O) Northwest(O) ((O) 1959(O) )(O) respectively(O) ,(O) Kim(B-person) Novak(I-person) and(O) Eva(B-person) Marie(I-person) Saint(I-person) play(O) the(O) blonde(O) heroines(O) .(O)"}}
{"id": "26", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "poem", "person", "magazine", "book", "event", "location", "country", "writer", "literary genre", "award"], "instance": {"id": "26", "words": ["His", "father", "Noah", "Webster", "Sr.", "(", "1722-1813", ")", "was", "a", "descendant", "of", "Connecticut", "Governor", "John", "Webster", ";", "his", "mother", "Mercy", "(", "Steele", ")", "Webster", "(", "1727-1794", ")", "was", "a", "descendant", "of", "Governor", "William", "Bradford", "of", "Plymouth", "Colony", ".", "Noah", "had", "two", "brothers", ",", "Abraham", "(", "1751-1831", ")", "and", "Charles", "(", "b", "."], "labels": ["O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-writer", "I-writer", "O", "O", "O", "B-person", "I-person", "I-person", "I-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-country", "I-country", "O", "B-person", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "B-person", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, poem, person, magazine, book, event, location, country, writer, literary genre, award and O.\nSentence: His father Noah Webster Sr. ( 1722-1813 ) was a descendant of Connecticut Governor John Webster ; his mother Mercy ( Steele ) Webster ( 1727-1794 ) was a descendant of Governor William Bradford of Plymouth Colony . Noah had two brothers , Abraham ( 1751-1831 ) and Charles ( b .", "prompt_labels": "His(O) father(O) Noah(B-person) Webster(I-person) Sr.(I-person) ((O) 1722-1813(O) )(O) was(O) a(O) descendant(O) of(O) Connecticut(B-location) Governor(O) John(B-writer) Webster(I-writer) ;(O) his(O) mother(O) Mercy(B-person) ((I-person) Steele(I-person) )(I-person) Webster(I-person) ((O) 1727-1794(O) )(O) was(O) a(O) descendant(O) of(O) Governor(O) William(B-person) Bradford(I-person) of(O) Plymouth(B-country) Colony(I-country) .(O) Noah(B-person) had(O) two(O) brothers(O) ,(O) Abraham(B-person) ((O) 1751-1831(O) )(O) and(O) Charles(B-person) ((O) b(O) .(O)"}}
{"id": "385", "dataset": "crossner_literature", "split": "dev", "label_list": ["poem", "award", "writer", "location", "magazine", "event", "organization", "book", "country", "literary genre", "person"], "instance": {"id": "385", "words": ["In", "2018", ",", "Walker", "was", "asked", "by", "an", "interviewer", "from", "The", "New", "York", "Times", "Book", "Review", "What", "books", "are", "on", "your", "nightstand", "?", "She", "listed", "Icke", "'s", "And", "the", "Truth", "Shall", "Set", "You", "Free", ",", "a", "book", "promoting", "an", "antisemitic", "conspiracy", "theory", "which", "draws", "on", "The", "Protocols", "of", "the", "Elders", "of", "Zion", "and", "queries", "the", "Holocaust", "."], "labels": ["O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "B-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, award, writer, location, magazine, event, organization, book, country, literary genre, person and O.\nSentence: In 2018 , Walker was asked by an interviewer from The New York Times Book Review What books are on your nightstand ? She listed Icke 's And the Truth Shall Set You Free , a book promoting an antisemitic conspiracy theory which draws on The Protocols of the Elders of Zion and queries the Holocaust .", "prompt_labels": "In(O) 2018(O) ,(O) Walker(B-writer) was(O) asked(O) by(O) an(O) interviewer(O) from(O) The(B-magazine) New(I-magazine) York(I-magazine) Times(I-magazine) Book(I-magazine) Review(I-magazine) What(O) books(O) are(O) on(O) your(O) nightstand(O) ?(O) She(O) listed(O) Icke(B-writer) 's(O) And(B-book) the(I-book) Truth(I-book) Shall(I-book) Set(I-book) You(I-book) Free(I-book) ,(O) a(O) book(O) promoting(O) an(O) antisemitic(O) conspiracy(O) theory(O) which(O) draws(O) on(O) The(B-book) Protocols(I-book) of(I-book) the(I-book) Elders(I-book) of(I-book) Zion(I-book) and(O) queries(O) the(B-event) Holocaust(I-event) .(O)"}}
{"id": "153", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "literary genre", "award", "book", "organization", "country", "event", "person", "writer", "poem", "magazine"], "instance": {"id": "153", "words": ["In", "November", "1924", "he", "was", "awarded", "the", "Nobel", "Prize", "for", "Literature", "over", "rivals", "Thomas", "Mann", ",", "George", "Bernard", "Shaw", "and", "Thomas", "Hardy", ",", "after", "he", "had", "been", "nominated", "by", "Anders", "Österling", ",", "member", "of", "the", "Swedish", "Academy", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, literary genre, award, book, organization, country, event, person, writer, poem, magazine and O.\nSentence: In November 1924 he was awarded the Nobel Prize for Literature over rivals Thomas Mann , George Bernard Shaw and Thomas Hardy , after he had been nominated by Anders Österling , member of the Swedish Academy .", "prompt_labels": "In(O) November(O) 1924(O) he(O) was(O) awarded(O) the(O) Nobel(B-award) Prize(I-award) for(I-award) Literature(I-award) over(O) rivals(O) Thomas(B-writer) Mann(I-writer) ,(O) George(B-writer) Bernard(I-writer) Shaw(I-writer) and(O) Thomas(B-writer) Hardy(I-writer) ,(O) after(O) he(O) had(O) been(O) nominated(O) by(O) Anders(B-writer) Österling(I-writer) ,(O) member(O) of(O) the(O) Swedish(B-organization) Academy(I-organization) .(O)"}}
{"id": "379", "dataset": "crossner_literature", "split": "dev", "label_list": ["poem", "literary genre", "event", "location", "country", "magazine", "writer", "organization", "person", "award", "book"], "instance": {"id": "379", "words": ["When", "a", "little", "older", ",", "she", "moved", "on", "to", "reading", "the", "surreal", "verse", "of", "Edward", "Lear", "and", "Lewis", "Carroll", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, literary genre, event, location, country, magazine, writer, organization, person, award, book and O.\nSentence: When a little older , she moved on to reading the surreal verse of Edward Lear and Lewis Carroll .", "prompt_labels": "When(O) a(O) little(O) older(O) ,(O) she(O) moved(O) on(O) to(O) reading(O) the(O) surreal(B-literary genre) verse(I-literary genre) of(O) Edward(B-writer) Lear(I-writer) and(O) Lewis(B-writer) Carroll(I-writer) .(O)"}}
{"id": "383", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "event", "book", "magazine", "poem", "literary genre", "country", "writer", "award", "location", "person"], "instance": {"id": "383", "words": ["In", "2003", ",", "Van", "Sant", "'s", "film", "about", "the", "Columbine", "High", "School", "massacre", ",", "Elephant", ",", "won", "the", "Palme", "d", "'Or", "at", "the", "Cannes", "Film", "Festival", "."], "labels": ["O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, book, magazine, poem, literary genre, country, writer, award, location, person and O.\nSentence: In 2003 , Van Sant 's film about the Columbine High School massacre , Elephant , won the Palme d 'Or at the Cannes Film Festival .", "prompt_labels": "In(O) 2003(O) ,(O) Van(B-person) Sant(I-person) 's(O) film(O) about(O) the(O) Columbine(B-event) High(I-event) School(I-event) massacre(I-event) ,(O) Elephant(O) ,(O) won(O) the(O) Palme(B-award) d(I-award) 'Or(I-award) at(O) the(O) Cannes(B-event) Film(I-event) Festival(I-event) .(O)"}}
{"id": "393", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "writer", "person", "location", "magazine", "book", "country", "literary genre", "event", "award", "poem"], "instance": {"id": "393", "words": ["In", "the", "early", "6th", "century", "BC", ",", "the", "Kingdom", "of", "Judah", "rebelled", "against", "the", "Neo-Babylonian", "Empire", "and", "was", "destroyed", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "I-country", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, writer, person, location, magazine, book, country, literary genre, event, award, poem and O.\nSentence: In the early 6th century BC , the Kingdom of Judah rebelled against the Neo-Babylonian Empire and was destroyed .", "prompt_labels": "In(O) the(O) early(O) 6th(O) century(O) BC(O) ,(O) the(O) Kingdom(B-country) of(I-country) Judah(I-country) rebelled(O) against(O) the(O) Neo-Babylonian(B-country) Empire(I-country) and(O) was(O) destroyed(O) .(O)"}}
{"id": "121", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "writer", "location", "country", "organization", "award", "book", "magazine", "literary genre", "poem", "event"], "instance": {"id": "121", "words": ["Another", "historical", "novel", "by", "Graves", ",", "Count", "Belisarius", "(", "1938", ")", ",", "recounts", "the", "career", "of", "the", "Byzantine", "Empire", "general", "Belisarius", "."], "labels": ["O", "O", "B-literary genre", "O", "B-writer", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "B-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, writer, location, country, organization, award, book, magazine, literary genre, poem, event and O.\nSentence: Another historical novel by Graves , Count Belisarius ( 1938 ) , recounts the career of the Byzantine Empire general Belisarius .", "prompt_labels": "Another(O) historical(O) novel(B-literary genre) by(O) Graves(B-writer) ,(O) Count(B-book) Belisarius(I-book) ((O) 1938(O) )(O) ,(O) recounts(O) the(O) career(O) of(O) the(O) Byzantine(B-country) Empire(I-country) general(O) Belisarius(B-person) .(O)"}}
{"id": "371", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "person", "writer", "organization", "location", "magazine", "literary genre", "poem", "book", "event", "award"], "instance": {"id": "371", "words": ["He", "won", "the", "Nebula", "Award", "three", "times", "(", "out", "of", "14", "nominations", ")", "and", "the", "Hugo", "Award", "six", "times", "(", "also", "out", "of", "14", "nominations", ")", ",", "including", "two", "Hugos", "for", "novels", ":", "the", "serialized", "novel", "...", "And", "Call", "Me", "Conrad", "(", "1965", ")", ",", "subsequently", "published", "under", "the", "title", "This", "Immortal", "(", "1966", ")", "and", "then", "the", "novel", "Lord", "of", "Light", "(", "1967", ")", "."], "labels": ["O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "O", "B-literary genre", "O", "O", "O", "B-literary genre", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "B-literary genre", "B-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, writer, organization, location, magazine, literary genre, poem, book, event, award and O.\nSentence: He won the Nebula Award three times ( out of 14 nominations ) and the Hugo Award six times ( also out of 14 nominations ) , including two Hugos for novels : the serialized novel ... And Call Me Conrad ( 1965 ) , subsequently published under the title This Immortal ( 1966 ) and then the novel Lord of Light ( 1967 ) .", "prompt_labels": "He(O) won(O) the(O) Nebula(B-award) Award(I-award) three(O) times(O) ((O) out(O) of(O) 14(O) nominations(O) )(O) and(O) the(O) Hugo(B-award) Award(I-award) six(O) times(O) ((O) also(O) out(O) of(O) 14(O) nominations(O) )(O) ,(O) including(O) two(O) Hugos(B-award) for(O) novels(B-literary genre) :(O) the(O) serialized(O) novel(B-literary genre) ...(O) And(B-book) Call(I-book) Me(I-book) Conrad(I-book) ((O) 1965(O) )(O) ,(O) subsequently(O) published(O) under(O) the(O) title(O) This(B-book) Immortal(I-book) ((O) 1966(O) )(O) and(O) then(O) the(O) novel(B-literary genre) Lord(B-book) of(I-book) Light(I-book) ((O) 1967(O) )(O) .(O)"}}
{"id": "217", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "location", "person", "event", "literary genre", "poem", "book", "organization", "award", "writer", "country"], "instance": {"id": "217", "words": ["In", "1939", ",", "with", "the", "help", "of", "his", "agent", "Audrey", "Wood", ",", "Williams", "was", "awarded", "a", "$", "1,000", "grant", "from", "the", "Rockefeller", "Foundation", "in", "recognition", "of", "his", "play", "Battle", "of", "Angels", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, location, person, event, literary genre, poem, book, organization, award, writer, country and O.\nSentence: In 1939 , with the help of his agent Audrey Wood , Williams was awarded a $ 1,000 grant from the Rockefeller Foundation in recognition of his play Battle of Angels .", "prompt_labels": "In(O) 1939(O) ,(O) with(O) the(O) help(O) of(O) his(O) agent(O) Audrey(B-writer) Wood(I-writer) ,(O) Williams(B-writer) was(O) awarded(O) a(O) $(O) 1,000(O) grant(O) from(O) the(O) Rockefeller(B-organization) Foundation(I-organization) in(O) recognition(O) of(O) his(O) play(O) Battle(B-book) of(I-book) Angels(I-book) .(O)"}}
{"id": "13", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "award", "literary genre", "poem", "country", "person", "magazine", "book", "event", "writer", "organization"], "instance": {"id": "13", "words": ["During", "this", "period", ",", "he", "covered", "Timothy", "Leary", "and", "Richard", "Alpert", "'", "s", "Millbrook", ",", "New", "York", "-based", "Castalia", "Foundation", "at", "the", "instigation", "of", "Alan", "Watts", "in", "The", "Realist", ",", "cultivated", "important", "friendships", "with", "William", "S.", "Burroughs", "and", "Allen", "Ginsberg", ",", "and", "lectured", "at", "the", "Free", "University", "of", "New", "York", "on", "'", "Anarchist", "and", "Synergetic", "Politics", "'", "in", "1965", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-location", "O", "B-location", "I-location", "O", "B-organization", "I-organization", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-magazine", "I-magazine", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, literary genre, poem, country, person, magazine, book, event, writer, organization and O.\nSentence: During this period , he covered Timothy Leary and Richard Alpert ' s Millbrook , New York -based Castalia Foundation at the instigation of Alan Watts in The Realist , cultivated important friendships with William S. Burroughs and Allen Ginsberg , and lectured at the Free University of New York on ' Anarchist and Synergetic Politics ' in 1965 .", "prompt_labels": "During(O) this(O) period(O) ,(O) he(O) covered(O) Timothy(B-person) Leary(I-person) and(O) Richard(B-person) Alpert(I-person) '(O) s(O) Millbrook(B-location) ,(O) New(B-location) York(I-location) -based(O) Castalia(B-organization) Foundation(I-organization) at(O) the(O) instigation(O) of(O) Alan(B-writer) Watts(I-writer) in(O) The(B-magazine) Realist(I-magazine) ,(O) cultivated(O) important(O) friendships(O) with(O) William(B-writer) S.(I-writer) Burroughs(I-writer) and(O) Allen(B-writer) Ginsberg(I-writer) ,(O) and(O) lectured(O) at(O) the(O) Free(B-organization) University(I-organization) of(I-organization) New(I-organization) York(I-organization) on(O) '(O) Anarchist(O) and(O) Synergetic(O) Politics(O) '(O) in(O) 1965(O) .(O)"}}
{"id": "386", "dataset": "crossner_literature", "split": "dev", "label_list": ["book", "poem", "country", "writer", "event", "organization", "location", "award", "person", "literary genre", "magazine"], "instance": {"id": "386", "words": ["Thornton", "had", "his", "first", "break", "when", "he", "co-wrote", "and", "starred", "in", "the", "1992", "thriller", "One", "FALSE", "Move", ",", "and", "received", "international", "attention", "after", "writing", ",", "directing", ",", "and", "starring", "in", "the", "independent", "drama", "film", "Sling", "Blade", "(", "1996", ")", ",", "for", "which", "he", "won", "an", "Academy", "Award", "for", "Best", "Adapted", "Screenplay", "and", "was", "nominated", "for", "an", "Academy", "Award", "for", "Best", "Actor", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, poem, country, writer, event, organization, location, award, person, literary genre, magazine and O.\nSentence: Thornton had his first break when he co-wrote and starred in the 1992 thriller One FALSE Move , and received international attention after writing , directing , and starring in the independent drama film Sling Blade ( 1996 ) , for which he won an Academy Award for Best Adapted Screenplay and was nominated for an Academy Award for Best Actor .", "prompt_labels": "Thornton(B-writer) had(O) his(O) first(O) break(O) when(O) he(O) co-wrote(O) and(O) starred(O) in(O) the(O) 1992(O) thriller(O) One(O) FALSE(O) Move(O) ,(O) and(O) received(O) international(O) attention(O) after(O) writing(O) ,(O) directing(O) ,(O) and(O) starring(O) in(O) the(O) independent(O) drama(O) film(O) Sling(O) Blade(O) ((O) 1996(O) )(O) ,(O) for(O) which(O) he(O) won(O) an(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Adapted(I-award) Screenplay(I-award) and(O) was(O) nominated(O) for(O) an(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) .(O)"}}
{"id": "75", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "location", "person", "poem", "country", "event", "book", "writer", "magazine", "literary genre", "organization"], "instance": {"id": "75", "words": ["Amos", "prophesied", "during", "the", "reign", "of", "Jeroboam", "II", ",", "King", "of", "Israel", ",", "and", "of", "Uzziah", "of", "Kingdom", "of", "Judah", ",", "which", "places", "him", "in", "the", "first", "half", "of", "the", "8th", "century", "BC", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "B-country", "O", "O", "O", "B-person", "O", "B-country", "I-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, person, poem, country, event, book, writer, magazine, literary genre, organization and O.\nSentence: Amos prophesied during the reign of Jeroboam II , King of Israel , and of Uzziah of Kingdom of Judah , which places him in the first half of the 8th century BC .", "prompt_labels": "Amos(B-writer) prophesied(O) during(O) the(O) reign(O) of(O) Jeroboam(B-person) II(I-person) ,(O) King(O) of(O) Israel(B-country) ,(O) and(O) of(O) Uzziah(B-person) of(O) Kingdom(B-country) of(I-country) Judah(I-country) ,(O) which(O) places(O) him(O) in(O) the(O) first(O) half(O) of(O) the(O) 8th(O) century(O) BC(O) .(O)"}}
{"id": "85", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "award", "magazine", "book", "poem", "person", "literary genre", "event", "location", "country", "organization"], "instance": {"id": "85", "words": ["Wajda", "made", "two", "more", "increasingly", "accomplished", "films", ",", "which", "developed", "further", "the", "anti-war", "theme", "of", "A", "Generation", ":", "Kanał", "(", "1956", ")", "(", "Special", "Jury", "Prize", "at", "Cannes", "Film", "Festival", "in", "1957", ",", "shared", "with", "Bergman", "'s", "The", "Seventh", "Seal", ")", "and", "Ashes", "and", "Diamonds", "(", "1958", ")", "with", "Zbigniew", "Cybulski", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, award, magazine, book, poem, person, literary genre, event, location, country, organization and O.\nSentence: Wajda made two more increasingly accomplished films , which developed further the anti-war theme of A Generation : Kanał ( 1956 ) ( Special Jury Prize at Cannes Film Festival in 1957 , shared with Bergman 's The Seventh Seal ) and Ashes and Diamonds ( 1958 ) with Zbigniew Cybulski .", "prompt_labels": "Wajda(B-person) made(O) two(O) more(O) increasingly(O) accomplished(O) films(O) ,(O) which(O) developed(O) further(O) the(O) anti-war(O) theme(O) of(O) A(O) Generation(O) :(O) Kanał(O) ((O) 1956(O) )(O) ((O) Special(B-award) Jury(I-award) Prize(I-award) at(O) Cannes(B-event) Film(I-event) Festival(I-event) in(O) 1957(O) ,(O) shared(O) with(O) Bergman(B-person) 's(O) The(O) Seventh(O) Seal(O) )(O) and(O) Ashes(O) and(O) Diamonds(O) ((O) 1958(O) )(O) with(O) Zbigniew(B-person) Cybulski(I-person) .(O)"}}
{"id": "16", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "award", "book", "writer", "person", "location", "poem", "literary genre", "magazine", "event", "country"], "instance": {"id": "16", "words": ["His", "works", "include", "not", "only", "science", "fiction", ",", "but", "also", "articles", "for", "Playboy", "and", "Family", "Circle", "magazines", "and", "nonfiction", "books", "."], "labels": ["O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "O", "O", "O", "B-magazine", "O", "B-magazine", "I-magazine", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, book, writer, person, location, poem, literary genre, magazine, event, country and O.\nSentence: His works include not only science fiction , but also articles for Playboy and Family Circle magazines and nonfiction books .", "prompt_labels": "His(O) works(O) include(O) not(O) only(O) science(B-literary genre) fiction(I-literary genre) ,(O) but(O) also(O) articles(O) for(O) Playboy(B-magazine) and(O) Family(B-magazine) Circle(I-magazine) magazines(O) and(O) nonfiction(O) books(O) .(O)"}}
{"id": "353", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "magazine", "writer", "country", "poem", "award", "literary genre", "person", "event", "book", "location"], "instance": {"id": "353", "words": ["Formally", ",", "the", "use", "of", "sound", "to", "create", "atmosphere", ",", "and", "of", "symbols", "(", "images", "that", "take", "on", "an", "expanded", "function", "within", "the", "poem", ")", ",", "betray", "a", "move", "towards", "considering", "the", "poem", "as", "a", "self-referential", "object", ",", "an", "idea", "further", "developed", "by", "the", "Symbolists", "Paul", "Verlaine", "and", "Stéphane", "Mallarmé", ",", "who", "acknowledge", "Baudelaire", "as", "a", "pioneer", "in", "this", "regard", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, magazine, writer, country, poem, award, literary genre, person, event, book, location and O.\nSentence: Formally , the use of sound to create atmosphere , and of symbols ( images that take on an expanded function within the poem ) , betray a move towards considering the poem as a self-referential object , an idea further developed by the Symbolists Paul Verlaine and Stéphane Mallarmé , who acknowledge Baudelaire as a pioneer in this regard .", "prompt_labels": "Formally(O) ,(O) the(O) use(O) of(O) sound(O) to(O) create(O) atmosphere(O) ,(O) and(O) of(O) symbols(O) ((O) images(O) that(O) take(O) on(O) an(O) expanded(O) function(O) within(O) the(O) poem(B-literary genre) )(O) ,(O) betray(O) a(O) move(O) towards(O) considering(O) the(O) poem(B-literary genre) as(O) a(O) self-referential(O) object(O) ,(O) an(O) idea(O) further(O) developed(O) by(O) the(O) Symbolists(O) Paul(B-writer) Verlaine(I-writer) and(O) Stéphane(B-writer) Mallarmé(I-writer) ,(O) who(O) acknowledge(O) Baudelaire(B-writer) as(O) a(O) pioneer(O) in(O) this(O) regard(O) .(O)"}}
{"id": "179", "dataset": "crossner_literature", "split": "dev", "label_list": ["poem", "writer", "award", "literary genre", "organization", "magazine", "person", "book", "location", "event", "country"], "instance": {"id": "179", "words": ["It", "has", "been", "credited", "by", "American", "poets", "like", "W.", "S.", "Merwin", ",", "and", "American", "scholars", "like", "Clare", "Cavanagh", ",", "with", "having", "a", "profound", "impact", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, writer, award, literary genre, organization, magazine, person, book, location, event, country and O.\nSentence: It has been credited by American poets like W. S. Merwin , and American scholars like Clare Cavanagh , with having a profound impact .", "prompt_labels": "It(O) has(O) been(O) credited(O) by(O) American(O) poets(O) like(O) W.(B-writer) S.(I-writer) Merwin(I-writer) ,(O) and(O) American(O) scholars(O) like(O) Clare(B-writer) Cavanagh(I-writer) ,(O) with(O) having(O) a(O) profound(O) impact(O) .(O)"}}
{"id": "338", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "country", "literary genre", "magazine", "writer", "award", "location", "event", "book", "poem", "organization"], "instance": {"id": "338", "words": ["He", "was", "awarded", "the", "Pulitzer", "Prize", "for", "poetry", "in", "1954", "for", "his", "book", "The", "Waking", ",", "and", "he", "won", "the", "annual", "National", "Book", "Award", "for", "Poetry", "twice", ",", "in", "1959", "for", "Words", "for", "the", "Wind"], "labels": ["O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-poem", "I-poem", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "I-poem"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, literary genre, magazine, writer, award, location, event, book, poem, organization and O.\nSentence: He was awarded the Pulitzer Prize for poetry in 1954 for his book The Waking , and he won the annual National Book Award for Poetry twice , in 1959 for Words for the Wind", "prompt_labels": "He(O) was(O) awarded(O) the(O) Pulitzer(B-award) Prize(I-award) for(I-award) poetry(I-award) in(O) 1954(O) for(O) his(O) book(O) The(B-poem) Waking(I-poem) ,(O) and(O) he(O) won(O) the(O) annual(O) National(B-award) Book(I-award) Award(I-award) for(I-award) Poetry(I-award) twice(O) ,(O) in(O) 1959(O) for(O) Words(B-poem) for(I-poem) the(I-poem) Wind(I-poem)"}}
{"id": "131", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "location", "literary genre", "person", "award", "organization", "country", "writer", "poem", "book", "event"], "instance": {"id": "131", "words": ["Colin", "Macmillan", "Turnbull", "(", "November", "23", ",", "1924", "-", "July", "28", ",", "1994", ")", "was", "a", "British-American", "anthropologist", "who", "came", "to", "public", "attention", "with", "the", "popular", "books", "The", "Forest", "People", "(", "on", "the", "Mbuti", "Pygmies", "of", "Zaire", ")", "and", "The", "Mountain", "People", "(", "on", "the", "Ik", "people", "of", "Uganda", ")", ",", "and", "one", "of", "the", "first", "anthropologists", "to", "work", "in", "the", "field", "of", "ethnomusicology", "."], "labels": ["B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, location, literary genre, person, award, organization, country, writer, poem, book, event and O.\nSentence: Colin Macmillan Turnbull ( November 23 , 1924 - July 28 , 1994 ) was a British-American anthropologist who came to public attention with the popular books The Forest People ( on the Mbuti Pygmies of Zaire ) and The Mountain People ( on the Ik people of Uganda ) , and one of the first anthropologists to work in the field of ethnomusicology .", "prompt_labels": "Colin(B-writer) Macmillan(I-writer) Turnbull(I-writer) ((O) November(O) 23(O) ,(O) 1924(O) -(O) July(O) 28(O) ,(O) 1994(O) )(O) was(O) a(O) British-American(O) anthropologist(O) who(O) came(O) to(O) public(O) attention(O) with(O) the(O) popular(O) books(O) The(B-book) Forest(I-book) People(I-book) ((O) on(O) the(O) Mbuti(B-book) Pygmies(I-book) of(I-book) Zaire(I-book) )(O) and(O) The(B-book) Mountain(I-book) People(I-book) ((O) on(O) the(O) Ik(B-book) people(I-book) of(I-book) Uganda(I-book) )(O) ,(O) and(O) one(O) of(O) the(O) first(O) anthropologists(O) to(O) work(O) in(O) the(O) field(O) of(O) ethnomusicology(O) .(O)"}}
{"id": "333", "dataset": "crossner_literature", "split": "dev", "label_list": ["book", "event", "poem", "literary genre", "magazine", "person", "organization", "country", "location", "writer", "award"], "instance": {"id": "333", "words": ["Although", "first", "presented", "at", "the", "Venice", "Film", "Festival", "on", "August", "28", "the", "same", "year", ",", "Affliction", "did", "not", "see", "a", "theatrical", "release", "until", "some", "time", "later", "in", "most", "countries", "."], "labels": ["O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, event, poem, literary genre, magazine, person, organization, country, location, writer, award and O.\nSentence: Although first presented at the Venice Film Festival on August 28 the same year , Affliction did not see a theatrical release until some time later in most countries .", "prompt_labels": "Although(O) first(O) presented(O) at(O) the(O) Venice(B-event) Film(I-event) Festival(I-event) on(O) August(O) 28(O) the(O) same(O) year(O) ,(O) Affliction(O) did(O) not(O) see(O) a(O) theatrical(O) release(O) until(O) some(O) time(O) later(O) in(O) most(O) countries(O) .(O)"}}
{"id": "104", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "writer", "country", "magazine", "organization", "person", "award", "book", "literary genre", "poem", "location"], "instance": {"id": "104", "words": ["Examples", "include", "the", "chansons", "de", "geste", ",", "Macaire", ",", "the", "Entre", "en", "Espagne", "written", "by", "Niccola", "of", "Padua", ",", "the", "Prise", "de", "Pampelune", ",", "and", "others", "."], "labels": ["O", "O", "O", "B-poem", "I-poem", "I-poem", "O", "B-poem", "O", "O", "B-poem", "I-poem", "I-poem", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-poem", "I-poem", "I-poem", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, writer, country, magazine, organization, person, award, book, literary genre, poem, location and O.\nSentence: Examples include the chansons de geste , Macaire , the Entre en Espagne written by Niccola of Padua , the Prise de Pampelune , and others .", "prompt_labels": "Examples(O) include(O) the(O) chansons(B-poem) de(I-poem) geste(I-poem) ,(O) Macaire(B-poem) ,(O) the(O) Entre(B-poem) en(I-poem) Espagne(I-poem) written(O) by(O) Niccola(B-writer) of(I-writer) Padua(I-writer) ,(O) the(O) Prise(B-poem) de(I-poem) Pampelune(I-poem) ,(O) and(O) others(O) .(O)"}}
{"id": "120", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "award", "person", "location", "organization", "event", "literary genre", "writer", "country", "book", "poem"], "instance": {"id": "120", "words": ["The", "only", "completed", "screenplay", ",", "Heaven", ",", "was", "filmed", "by", "Tom", "Tykwer", "and", "premiered", "in", "2002", "at", "the", "Berlin", "International", "Film", "Festival", "."], "labels": ["O", "O", "O", "O", "O", "B-book", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, award, person, location, organization, event, literary genre, writer, country, book, poem and O.\nSentence: The only completed screenplay , Heaven , was filmed by Tom Tykwer and premiered in 2002 at the Berlin International Film Festival .", "prompt_labels": "The(O) only(O) completed(O) screenplay(O) ,(O) Heaven(B-book) ,(O) was(O) filmed(O) by(O) Tom(B-person) Tykwer(I-person) and(O) premiered(O) in(O) 2002(O) at(O) the(O) Berlin(B-event) International(I-event) Film(I-event) Festival(I-event) .(O)"}}
{"id": "191", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "writer", "poem", "organization", "book", "person", "literary genre", "event", "country", "award", "magazine"], "instance": {"id": "191", "words": ["Of", "Things", "to", "Come", ",", "The", "New", "York", "Times", "Book", "Review", ",", "October", "26", ",", "1975", "Theodore", "Sturgeon", "praised", "The", "Dispossessed", "as", "a", "beautifully", "written", ",", "beautifully", "composed", "book", ",", "saying", "it", "performs", "one", "of", "science", "fiction", "'s", "prime", "functions", ",", "which", "is", "to", "create", "another", "kind", "of", "social", "system", "to", "see", "how", "it", "would", "work", "."], "labels": ["O", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, writer, poem, organization, book, person, literary genre, event, country, award, magazine and O.\nSentence: Of Things to Come , The New York Times Book Review , October 26 , 1975 Theodore Sturgeon praised The Dispossessed as a beautifully written , beautifully composed book , saying it performs one of science fiction 's prime functions , which is to create another kind of social system to see how it would work .", "prompt_labels": "Of(O) Things(O) to(O) Come(O) ,(O) The(B-magazine) New(I-magazine) York(I-magazine) Times(I-magazine) Book(I-magazine) Review(I-magazine) ,(O) October(O) 26(O) ,(O) 1975(O) Theodore(B-writer) Sturgeon(I-writer) praised(O) The(B-book) Dispossessed(I-book) as(O) a(O) beautifully(O) written(O) ,(O) beautifully(O) composed(O) book(O) ,(O) saying(O) it(O) performs(O) one(O) of(O) science(B-literary genre) fiction(I-literary genre) 's(O) prime(O) functions(O) ,(O) which(O) is(O) to(O) create(O) another(O) kind(O) of(O) social(O) system(O) to(O) see(O) how(O) it(O) would(O) work(O) .(O)"}}
{"id": "362", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "person", "music genre", "event", "location", "band", "musical instrument", "country", "song", "musical artist", "award", "album"], "instance": {"id": "362", "words": ["In", "2013", "Blood", "Brothers", "was", "produced", "by", "the", "Harvest", "Rain", "Theatre", "Company", "of", "Brisbane", "playing", "the", "Cremorne", "Theatre", "August", "3-17", ":", "directed", "by", "Tim", "O", "'Connor", ",", "the", "production", "featured", "Amanda", "Muggleton", "in", "the", "role", "of", "Mrs.", "Johnstone.2011", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "B-location", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, music genre, event, location, band, musical instrument, country, song, musical artist, award, album and O.\nSentence: In 2013 Blood Brothers was produced by the Harvest Rain Theatre Company of Brisbane playing the Cremorne Theatre August 3-17 : directed by Tim O 'Connor , the production featured Amanda Muggleton in the role of Mrs. Johnstone.2011 .", "prompt_labels": "In(O) 2013(O) Blood(O) Brothers(O) was(O) produced(O) by(O) the(O) Harvest(B-location) Rain(I-location) Theatre(I-location) Company(I-location) of(O) Brisbane(B-location) playing(O) the(O) Cremorne(B-location) Theatre(I-location) August(O) 3-17(O) :(O) directed(O) by(O) Tim(B-person) O(I-person) 'Connor(I-person) ,(O) the(O) production(O) featured(O) Amanda(B-person) Muggleton(I-person) in(O) the(O) role(O) of(O) Mrs.(O) Johnstone.2011(O) .(O)"}}
{"id": "211", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "album", "location", "song", "musical instrument", "country", "musical artist", "band", "organization", "award", "person", "music genre"], "instance": {"id": "211", "words": ["Sometimes", "the", "most", "successful", "solo", "star", "from", "a", "band", "is", "not", "the", "most", "popular", "member", "such", "as", "Robbie", "Williams", "as", "opposed", "to", "lead", "singer", "Gary", "Barlow", "from", "Take", "That", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, album, location, song, musical instrument, country, musical artist, band, organization, award, person, music genre and O.\nSentence: Sometimes the most successful solo star from a band is not the most popular member such as Robbie Williams as opposed to lead singer Gary Barlow from Take That .", "prompt_labels": "Sometimes(O) the(O) most(O) successful(O) solo(O) star(O) from(O) a(O) band(O) is(O) not(O) the(O) most(O) popular(O) member(O) such(O) as(O) Robbie(B-musical artist) Williams(I-musical artist) as(O) opposed(O) to(O) lead(O) singer(O) Gary(B-musical artist) Barlow(I-musical artist) from(O) Take(B-band) That(I-band) .(O)"}}
{"id": "222", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "album", "organization", "award", "musical instrument", "person", "musical artist", "music genre", "event", "country", "band", "song"], "instance": {"id": "222", "words": ["Music", "critic", "Richie", "Unterberger", "has", "noted", "that", "the", "commercial", "success", "of", "the", "Byrds", "'", "cover", "version", "of", "Dylan", "'s", "Mr.", "Tambourine", "Man", ",", "along", "with", "Dylan", "'s", "own", "contributions", "to", "the", "genre", "on", "the", "albums", "Bringing", "It", "All", "Back", "Home", ",", "Highway", "61", "Revisited", ",", "and", "Blonde", "on", "Blonde", ",", "initiated", "an", "explosion", "of", "emulators", "and", "imitators", "."], "labels": ["O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "O", "O", "O", "O", "B-musical artist", "O", "B-song", "I-song", "I-song", "O", "O", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "O", "B-album", "I-album", "I-album", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, album, organization, award, musical instrument, person, musical artist, music genre, event, country, band, song and O.\nSentence: Music critic Richie Unterberger has noted that the commercial success of the Byrds ' cover version of Dylan 's Mr. Tambourine Man , along with Dylan 's own contributions to the genre on the albums Bringing It All Back Home , Highway 61 Revisited , and Blonde on Blonde , initiated an explosion of emulators and imitators .", "prompt_labels": "Music(O) critic(O) Richie(B-person) Unterberger(I-person) has(O) noted(O) that(O) the(O) commercial(O) success(O) of(O) the(O) Byrds(B-band) '(O) cover(O) version(O) of(O) Dylan(B-musical artist) 's(O) Mr.(B-song) Tambourine(I-song) Man(I-song) ,(O) along(O) with(O) Dylan(B-musical artist) 's(O) own(O) contributions(O) to(O) the(O) genre(O) on(O) the(O) albums(O) Bringing(B-album) It(I-album) All(I-album) Back(I-album) Home(I-album) ,(O) Highway(B-album) 61(I-album) Revisited(I-album) ,(O) and(O) Blonde(B-album) on(I-album) Blonde(I-album) ,(O) initiated(O) an(O) explosion(O) of(O) emulators(O) and(O) imitators(O) .(O)"}}
{"id": "110", "dataset": "crossner_music", "split": "dev", "label_list": ["person", "song", "country", "musical instrument", "award", "musical artist", "band", "album", "music genre", "location", "organization", "event"], "instance": {"id": "110", "words": ["Further", "nominations", "at", "the", "46th", "Academy", "Awards", "included", "Academy", "Award", "for", "Best", "Director", "(", "George", "Lucas", ")", ",", "Best", "Original", "Screenplay", "(", "Lucas", ",", "Willard", "Huyck", "and", "Gloria", "Katz", ")", ",", "Academy", "Award", "for", "Best", "Supporting", "Actress", "(", "Candy", "Clark", ")", "and", "Academy", "Award", "for", "Best", "Film", "Editing", "(", "Verna", "Fields", "and", "Marcia", "Lucas", ")", "."], "labels": ["O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "O", "O", "B-award", "I-award", "I-award", "O", "B-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, song, country, musical instrument, award, musical artist, band, album, music genre, location, organization, event and O.\nSentence: Further nominations at the 46th Academy Awards included Academy Award for Best Director ( George Lucas ) , Best Original Screenplay ( Lucas , Willard Huyck and Gloria Katz ) , Academy Award for Best Supporting Actress ( Candy Clark ) and Academy Award for Best Film Editing ( Verna Fields and Marcia Lucas ) .", "prompt_labels": "Further(O) nominations(O) at(O) the(O) 46th(B-award) Academy(I-award) Awards(I-award) included(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) ((O) George(B-person) Lucas(I-person) )(O) ,(O) Best(B-award) Original(I-award) Screenplay(I-award) ((O) Lucas(B-person) ,(O) Willard(B-person) Huyck(I-person) and(O) Gloria(B-person) Katz(I-person) )(O) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Supporting(I-award) Actress(I-award) ((O) Candy(B-person) Clark(I-person) )(O) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Film(I-award) Editing(I-award) ((O) Verna(B-person) Fields(I-person) and(O) Marcia(B-person) Lucas(I-person) )(O) .(O)"}}
{"id": "140", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "musical artist", "song", "location", "award", "musical instrument", "album", "organization", "country", "event", "band", "person"], "instance": {"id": "140", "words": ["Black", "metal", "album", "covers", "are", "typically", "dark", "and", "tend", "to", "be", "atmospheric", "or", "provocative", ";", "some", "feature", "natural", "or", "fantasy", "landscapes", "(", "for", "example", "Burzum", "'", "s", "Filosofem", "and", "Emperor", "'s", "In", "the", "Nightside", "Eclipse", ")", "while", "others", "are", "violent", ",", "sexually", "transgressive", ",", "sacrilegious", ",", "or", "iconoclastic", "(", "for", "example", "Marduk", "'", "s", "Fuck", "Me", "Jesus", "and", "Dimmu", "Borgir", "'", "s", "In", "Sorte", "Diaboli", ")", "."], "labels": ["B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "O", "O", "B-album", "O", "B-band", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "O", "O", "B-album", "I-album", "I-album", "O", "B-band", "I-band", "O", "O", "B-album", "I-album", "I-album", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, musical artist, song, location, award, musical instrument, album, organization, country, event, band, person and O.\nSentence: Black metal album covers are typically dark and tend to be atmospheric or provocative ; some feature natural or fantasy landscapes ( for example Burzum ' s Filosofem and Emperor 's In the Nightside Eclipse ) while others are violent , sexually transgressive , sacrilegious , or iconoclastic ( for example Marduk ' s Fuck Me Jesus and Dimmu Borgir ' s In Sorte Diaboli ) .", "prompt_labels": "Black(B-music genre) metal(I-music genre) album(O) covers(O) are(O) typically(O) dark(O) and(O) tend(O) to(O) be(O) atmospheric(O) or(O) provocative(O) ;(O) some(O) feature(O) natural(O) or(O) fantasy(O) landscapes(O) ((O) for(O) example(O) Burzum(B-band) '(O) s(O) Filosofem(B-album) and(O) Emperor(B-band) 's(O) In(B-album) the(I-album) Nightside(I-album) Eclipse(I-album) )(O) while(O) others(O) are(O) violent(O) ,(O) sexually(O) transgressive(O) ,(O) sacrilegious(O) ,(O) or(O) iconoclastic(O) ((O) for(O) example(O) Marduk(B-band) '(O) s(O) Fuck(B-album) Me(I-album) Jesus(I-album) and(O) Dimmu(B-band) Borgir(I-band) '(O) s(O) In(B-album) Sorte(I-album) Diaboli(I-album) )(O) .(O)"}}
{"id": "154", "dataset": "crossner_music", "split": "dev", "label_list": ["musical artist", "band", "award", "song", "country", "music genre", "album", "person", "organization", "musical instrument", "event", "location"], "instance": {"id": "154", "words": ["In", "recognition", "of", "her", "film", "career", ",", "she", "received", "BAFTA", "'s", "Lifetime", "Achievement", "Award", ",", "the", "Golden", "Globe", "Cecil", "B.", "DeMille", "Award", ",", "the", "Screen", "Actors", "Guild", "Life", "Achievement", "Award", ",", "and", "the", "Special", "Tony", "Award", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, band, award, song, country, music genre, album, person, organization, musical instrument, event, location and O.\nSentence: In recognition of her film career , she received BAFTA 's Lifetime Achievement Award , the Golden Globe Cecil B. DeMille Award , the Screen Actors Guild Life Achievement Award , and the Special Tony Award .", "prompt_labels": "In(O) recognition(O) of(O) her(O) film(O) career(O) ,(O) she(O) received(O) BAFTA(B-award) 's(I-award) Lifetime(I-award) Achievement(I-award) Award(I-award) ,(O) the(O) Golden(B-award) Globe(I-award) Cecil(I-award) B.(I-award) DeMille(I-award) Award(I-award) ,(O) the(O) Screen(B-award) Actors(I-award) Guild(I-award) Life(I-award) Achievement(I-award) Award(I-award) ,(O) and(O) the(O) Special(B-award) Tony(I-award) Award(I-award) .(O)"}}
{"id": "363", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "musical artist", "person", "song", "musical instrument", "album", "event", "music genre", "location", "country", "award", "band"], "instance": {"id": "363", "words": ["She", "rose", "to", "fame", "portraying", "Jamie", "Buchman", "in", "the", "sitcom", "Mad", "About", "You", "(", "1992-1999", ",", "2019-present", ")", ",", "for", "which", "she", "won", "three", "Golden", "Globe", "Awards", "for", "Best", "Actress", "in", "a", "Television", "Series", "-", "Musical", "or", "Comedy", "and", "four", "Emmy", "Award", "s", "Primetime", "Emmy", "Award", "for", "Outstanding", "Lead", "Actress", "in", "a", "Comedy", "Series", "."], "labels": ["O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, musical artist, person, song, musical instrument, album, event, music genre, location, country, award, band and O.\nSentence: She rose to fame portraying Jamie Buchman in the sitcom Mad About You ( 1992-1999 , 2019-present ) , for which she won three Golden Globe Awards for Best Actress in a Television Series - Musical or Comedy and four Emmy Award s Primetime Emmy Award for Outstanding Lead Actress in a Comedy Series .", "prompt_labels": "She(O) rose(O) to(O) fame(O) portraying(O) Jamie(B-person) Buchman(I-person) in(O) the(O) sitcom(O) Mad(O) About(O) You(O) ((O) 1992-1999(O) ,(O) 2019-present(O) )(O) ,(O) for(O) which(O) she(O) won(O) three(O) Golden(B-award) Globe(I-award) Awards(I-award) for(I-award) Best(I-award) Actress(I-award) in(I-award) a(I-award) Television(I-award) Series(I-award) -(I-award) Musical(I-award) or(I-award) Comedy(I-award) and(O) four(O) Emmy(B-award) Award(I-award) s(I-award) Primetime(B-award) Emmy(I-award) Award(I-award) for(I-award) Outstanding(I-award) Lead(I-award) Actress(I-award) in(I-award) a(I-award) Comedy(I-award) Series(I-award) .(O)"}}
{"id": "187", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "location", "musical artist", "song", "music genre", "musical instrument", "award", "band", "person", "event", "album", "country"], "instance": {"id": "187", "words": ["Fredriksson", "returned", "in", "2006", "with", "an", "album", "of", "Swedish", "cover", "songs", ",", "titled", "Min", "bäste", "vän", "(", "My", "Best", "Friend", ")", ",", "while", "Gessle", "recorded", "two", "more", "solo", "albums", ",", "En", "händig", "man", "(", "A", "Handy", "Man", ")", "(", "2007", ")", "and", "Party", "Crasher", "(", "2008", ")", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "B-album", "I-album", "I-album", "O", "O", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, musical artist, song, music genre, musical instrument, award, band, person, event, album, country and O.\nSentence: Fredriksson returned in 2006 with an album of Swedish cover songs , titled Min bäste vän ( My Best Friend ) , while Gessle recorded two more solo albums , En händig man ( A Handy Man ) ( 2007 ) and Party Crasher ( 2008 ) .", "prompt_labels": "Fredriksson(B-musical artist) returned(O) in(O) 2006(O) with(O) an(O) album(O) of(O) Swedish(O) cover(O) songs(O) ,(O) titled(O) Min(B-album) bäste(I-album) vän(I-album) ((O) My(B-album) Best(I-album) Friend(I-album) )(O) ,(O) while(O) Gessle(B-musical artist) recorded(O) two(O) more(O) solo(O) albums(O) ,(O) En(B-album) händig(I-album) man(I-album) ((O) A(B-album) Handy(I-album) Man(I-album) )(O) ((O) 2007(O) )(O) and(O) Party(B-album) Crasher(I-album) ((O) 2008(O) )(O) .(O)"}}
{"id": "107", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "musical instrument", "band", "location", "award", "event", "album", "organization", "musical artist", "music genre", "song", "person"], "instance": {"id": "107", "words": ["1995", "saw", "the", "release", "of", "Tri", "Repetae", ",", "their", "third", "album", ",", "as", "well", "as", "the", "EPs", "Anvil", "Vapre", "and", "Garbage", ",", "featuring", "a", "monochrome", "cover", "designed", "by", "The", "Designers", "Republic", ",", "with", "whom", "Autechre", "have", "long", "held", "a", "close", "association", "."], "labels": ["O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "B-album", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "B-band", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, musical instrument, band, location, award, event, album, organization, musical artist, music genre, song, person and O.\nSentence: 1995 saw the release of Tri Repetae , their third album , as well as the EPs Anvil Vapre and Garbage , featuring a monochrome cover designed by The Designers Republic , with whom Autechre have long held a close association .", "prompt_labels": "1995(O) saw(O) the(O) release(O) of(O) Tri(B-album) Repetae(I-album) ,(O) their(O) third(O) album(O) ,(O) as(O) well(O) as(O) the(O) EPs(O) Anvil(B-album) Vapre(I-album) and(O) Garbage(B-album) ,(O) featuring(O) a(O) monochrome(O) cover(O) designed(O) by(O) The(B-organization) Designers(I-organization) Republic(I-organization) ,(O) with(O) whom(O) Autechre(B-band) have(O) long(O) held(O) a(O) close(O) association(O) .(O)"}}
{"id": "236", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "musical instrument", "person", "music genre", "organization", "song", "event", "award", "musical artist", "country", "location", "band"], "instance": {"id": "236", "words": ["Country", "music", ",", "also", "known", "as", "country", "and", "western", "(", "or", "simply", "country", ")", ",", "and", "hillbilly", "music", ",", "is", "a", "genre", "of", "popular", "music", "that", "takes", "its", "roots", "from", "genres", "such", "as", "blues", "and", "old-time", "music", ",", "and", "various", "types", "of", "American", "folk", "music", "including", "Appalachian", "music", ",", "Cajun", "music", ",", "and", "the", "cowboy", "Western", "music", "styles", "of", "Red", "Dirt", ",", "New", "Mexico", "music", ",", "Texas", "country", "music", ",", "and", "Tejano", "music", "."], "labels": ["B-music genre", "I-music genre", "O", "O", "O", "O", "B-music genre", "O", "B-music genre", "O", "O", "O", "B-music genre", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, musical instrument, person, music genre, organization, song, event, award, musical artist, country, location, band and O.\nSentence: Country music , also known as country and western ( or simply country ) , and hillbilly music , is a genre of popular music that takes its roots from genres such as blues and old-time music , and various types of American folk music including Appalachian music , Cajun music , and the cowboy Western music styles of Red Dirt , New Mexico music , Texas country music , and Tejano music .", "prompt_labels": "Country(B-music genre) music(I-music genre) ,(O) also(O) known(O) as(O) country(B-music genre) and(O) western(B-music genre) ((O) or(O) simply(O) country(B-music genre) )(O) ,(O) and(O) hillbilly(B-music genre) music(I-music genre) ,(O) is(O) a(O) genre(O) of(O) popular(B-music genre) music(I-music genre) that(O) takes(O) its(O) roots(O) from(O) genres(O) such(O) as(O) blues(B-music genre) and(O) old-time(B-music genre) music(I-music genre) ,(O) and(O) various(O) types(O) of(O) American(B-music genre) folk(I-music genre) music(I-music genre) including(O) Appalachian(B-music genre) music(I-music genre) ,(O) Cajun(B-music genre) music(I-music genre) ,(O) and(O) the(O) cowboy(B-music genre) Western(I-music genre) music(I-music genre) styles(O) of(O) Red(B-music genre) Dirt(I-music genre) ,(O) New(B-music genre) Mexico(I-music genre) music(I-music genre) ,(O) Texas(B-music genre) country(I-music genre) music(I-music genre) ,(O) and(O) Tejano(B-music genre) music(I-music genre) .(O)"}}
{"id": "37", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "event", "person", "music genre", "album", "award", "song", "country", "location", "musical artist", "musical instrument", "band"], "instance": {"id": "37", "words": ["Several", "albums", "that", "continued", "this", "style", ",", "which", "had", "come", "to", "be", "known", "as", "technical", "thrash", "metal", ",", "were", "released", "in", "1991", ",", "such", "as", "Overkill", "'s", "Horrorscope", ",", "Heathen", "'", "s", "Victims", "of", "Deception", ",", "Dark", "Angel", "'", "s", "Time", "Does", "Not", "Heal", ",", "Sepultura", "'s", "Arise", ",", "and", "Coroner", "'s", "Mental", "Vortex", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "O", "B-album", "O", "B-band", "O", "O", "B-album", "I-album", "I-album", "O", "B-band", "I-band", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "B-band", "O", "B-album", "O", "O", "B-band", "O", "B-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, person, music genre, album, award, song, country, location, musical artist, musical instrument, band and O.\nSentence: Several albums that continued this style , which had come to be known as technical thrash metal , were released in 1991 , such as Overkill 's Horrorscope , Heathen ' s Victims of Deception , Dark Angel ' s Time Does Not Heal , Sepultura 's Arise , and Coroner 's Mental Vortex .", "prompt_labels": "Several(O) albums(O) that(O) continued(O) this(O) style(O) ,(O) which(O) had(O) come(O) to(O) be(O) known(O) as(O) technical(B-music genre) thrash(I-music genre) metal(I-music genre) ,(O) were(O) released(O) in(O) 1991(O) ,(O) such(O) as(O) Overkill(B-band) 's(O) Horrorscope(B-album) ,(O) Heathen(B-band) '(O) s(O) Victims(B-album) of(I-album) Deception(I-album) ,(O) Dark(B-band) Angel(I-band) '(O) s(O) Time(B-album) Does(I-album) Not(I-album) Heal(I-album) ,(O) Sepultura(B-band) 's(O) Arise(B-album) ,(O) and(O) Coroner(B-band) 's(O) Mental(B-album) Vortex(I-album) .(O)"}}
{"id": "77", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "award", "song", "musical artist", "musical instrument", "album", "band", "organization", "country", "person", "event", "music genre"], "instance": {"id": "77", "words": ["Schmidt", "was", "born", "in", "Pozsony", "(", "known", "in", "German", "as", "Pressburg", ")", ",", "in", "the", "Hungary", "part", "of", "the", "Austria-Hungary", "(", "the", "city", "is", "now", "Bratislava", ",", "capital", "of", "Slovakia", ")", "."], "labels": ["B-person", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "B-country", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "B-country", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, song, musical artist, musical instrument, album, band, organization, country, person, event, music genre and O.\nSentence: Schmidt was born in Pozsony ( known in German as Pressburg ) , in the Hungary part of the Austria-Hungary ( the city is now Bratislava , capital of Slovakia ) .", "prompt_labels": "Schmidt(B-person) was(O) born(O) in(O) Pozsony(B-location) ((O) known(O) in(O) German(O) as(O) Pressburg(B-location) )(O) ,(O) in(O) the(O) Hungary(B-country) part(O) of(O) the(O) Austria-Hungary(B-location) ((O) the(O) city(O) is(O) now(O) Bratislava(B-location) ,(O) capital(O) of(O) Slovakia(B-country) )(O) .(O)"}}
{"id": "70", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "band", "song", "event", "person", "music genre", "album", "location", "musical instrument", "country", "musical artist", "award"], "instance": {"id": "70", "words": ["Alexander", "has", "had", "an", "active", "career", "on", "stage", ",", "appearing", "in", "several", "Broadway", "musicals", "including", "Jerome", "Robbins", "'", "Broadway", "in", "1989", ",", "for", "which", "he", "won", "the", "Tony", "Award", "as", "Best", "Leading", "Actor", "in", "a", "Musical", "and", "a", "Grammy", "Award", "for", "Grammy", "Award", "for", "Best", "Musical", "Theater", "Album", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, band, song, event, person, music genre, album, location, musical instrument, country, musical artist, award and O.\nSentence: Alexander has had an active career on stage , appearing in several Broadway musicals including Jerome Robbins ' Broadway in 1989 , for which he won the Tony Award as Best Leading Actor in a Musical and a Grammy Award for Grammy Award for Best Musical Theater Album .", "prompt_labels": "Alexander(B-musical artist) has(O) had(O) an(O) active(O) career(O) on(O) stage(O) ,(O) appearing(O) in(O) several(O) Broadway(B-organization) musicals(O) including(O) Jerome(O) Robbins(O) '(O) Broadway(O) in(O) 1989(O) ,(O) for(O) which(O) he(O) won(O) the(O) Tony(B-award) Award(I-award) as(O) Best(B-award) Leading(I-award) Actor(I-award) in(I-award) a(I-award) Musical(I-award) and(O) a(O) Grammy(B-award) Award(I-award) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Musical(I-award) Theater(I-award) Album(I-award) .(O)"}}
{"id": "253", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "album", "country", "location", "person", "event", "musical artist", "song", "award", "music genre", "organization", "musical instrument"], "instance": {"id": "253", "words": ["Iommi", "called", "vocalist", "Dave", "Walker", ",", "a", "longtime", "friend", "of", "the", "band", ",", "who", "had", "previously", "been", "a", "member", "of", "Fleetwood", "Mac", "and", "Savoy", "Brown", ",", "and", "informed", "him", "that", "Osbourne", "had", "left", "the", "band", "."], "labels": ["B-musical artist", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, album, country, location, person, event, musical artist, song, award, music genre, organization, musical instrument and O.\nSentence: Iommi called vocalist Dave Walker , a longtime friend of the band , who had previously been a member of Fleetwood Mac and Savoy Brown , and informed him that Osbourne had left the band .", "prompt_labels": "Iommi(B-musical artist) called(O) vocalist(O) Dave(B-musical artist) Walker(I-musical artist) ,(O) a(O) longtime(O) friend(O) of(O) the(O) band(O) ,(O) who(O) had(O) previously(O) been(O) a(O) member(O) of(O) Fleetwood(B-band) Mac(I-band) and(O) Savoy(B-band) Brown(I-band) ,(O) and(O) informed(O) him(O) that(O) Osbourne(B-musical artist) had(O) left(O) the(O) band(O) .(O)"}}
{"id": "230", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "event", "country", "award", "organization", "musical instrument", "music genre", "location", "person", "band", "musical artist", "album"], "instance": {"id": "230", "words": ["Michael", "won", "numerous", "music", "awards", "throughout", "his", "30-year", "career", ",", "including", "three", "Brit", "Awards", "-", "winning", "Best", "British", "Male", "twice", ",", "four", "MTV", "Video", "Music", "Award", "s", ",", "four", "Ivor", "Novello", "Awards", ",", "three", "American", "Music", "Award", "s", "(", "including", "two", "in", "the", "traditionally-black", "Soul", "/", "R", "&", "B", "category"], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, event, country, award, organization, musical instrument, music genre, location, person, band, musical artist, album and O.\nSentence: Michael won numerous music awards throughout his 30-year career , including three Brit Awards - winning Best British Male twice , four MTV Video Music Award s , four Ivor Novello Awards , three American Music Award s ( including two in the traditionally-black Soul / R & B category", "prompt_labels": "Michael(B-musical artist) won(O) numerous(O) music(O) awards(O) throughout(O) his(O) 30-year(O) career(O) ,(O) including(O) three(O) Brit(B-award) Awards(I-award) -(O) winning(O) Best(B-award) British(I-award) Male(I-award) twice(O) ,(O) four(O) MTV(B-award) Video(I-award) Music(I-award) Award(I-award) s(I-award) ,(O) four(O) Ivor(B-award) Novello(I-award) Awards(I-award) ,(O) three(O) American(B-award) Music(I-award) Award(I-award) s(I-award) ((O) including(O) two(O) in(O) the(O) traditionally-black(O) Soul(O) /(O) R(B-music genre) &(I-music genre) B(I-music genre) category(O)"}}
{"id": "226", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "organization", "musical instrument", "album", "band", "award", "country", "music genre", "song", "person", "event", "musical artist"], "instance": {"id": "226", "words": ["The", "album", "features", "covers", "of", "songs", "originally", "performed", "by", "The", "Who", ",", "Liz", "Phair", ",", "Creedence", "Clearwater", "Revival", ",", "Ryan", "Adams", ",", "I", "Blame", "Coco", ",", "and", "Led", "Zeppelin", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "I-band", "O", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, musical instrument, album, band, award, country, music genre, song, person, event, musical artist and O.\nSentence: The album features covers of songs originally performed by The Who , Liz Phair , Creedence Clearwater Revival , Ryan Adams , I Blame Coco , and Led Zeppelin .", "prompt_labels": "The(O) album(O) features(O) covers(O) of(O) songs(O) originally(O) performed(O) by(O) The(B-band) Who(I-band) ,(O) Liz(B-musical artist) Phair(I-musical artist) ,(O) Creedence(B-band) Clearwater(I-band) Revival(I-band) ,(O) Ryan(B-musical artist) Adams(I-musical artist) ,(O) I(B-band) Blame(I-band) Coco(I-band) ,(O) and(O) Led(B-band) Zeppelin(I-band) .(O)"}}
{"id": "188", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "song", "music genre", "event", "musical artist", "award", "band", "organization", "person", "location", "musical instrument", "country"], "instance": {"id": "188", "words": ["They", "were", "inducted", "by", "Chuck", "D", "and", "LL", "Cool", "J", "on", "April", "14", ",", "2012", "therefore", "the", "group", "didn", "'t", "perform", ";", "instead", "Black", "Thought", ",", "Travie", "from", "Gym", "Class", "Heroes", "and", "Kid", "Rock", "performed", "a", "medley", "of", "their", "songs", "."], "labels": ["O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "O", "B-band", "I-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, song, music genre, event, musical artist, award, band, organization, person, location, musical instrument, country and O.\nSentence: They were inducted by Chuck D and LL Cool J on April 14 , 2012 therefore the group didn 't perform ; instead Black Thought , Travie from Gym Class Heroes and Kid Rock performed a medley of their songs .", "prompt_labels": "They(O) were(O) inducted(O) by(O) Chuck(B-musical artist) D(I-musical artist) and(O) LL(B-musical artist) Cool(I-musical artist) J(I-musical artist) on(O) April(O) 14(O) ,(O) 2012(O) therefore(O) the(O) group(O) didn(O) 't(O) perform(O) ;(O) instead(O) Black(B-musical artist) Thought(I-musical artist) ,(O) Travie(B-musical artist) from(O) Gym(B-band) Class(I-band) Heroes(I-band) and(O) Kid(B-musical artist) Rock(I-musical artist) performed(O) a(O) medley(O) of(O) their(O) songs(O) .(O)"}}
{"id": "79", "dataset": "crossner_music", "split": "dev", "label_list": ["musical artist", "person", "band", "musical instrument", "song", "location", "organization", "music genre", "event", "album", "award", "country"], "instance": {"id": "79", "words": ["Funds", "raised", "by", "the", "project", "will", "go", "to", "Amazon", "Watch", "and", "Extinction", "Rebellion", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, person, band, musical instrument, song, location, organization, music genre, event, album, award, country and O.\nSentence: Funds raised by the project will go to Amazon Watch and Extinction Rebellion .", "prompt_labels": "Funds(O) raised(O) by(O) the(O) project(O) will(O) go(O) to(O) Amazon(B-organization) Watch(I-organization) and(O) Extinction(B-organization) Rebellion(I-organization) .(O)"}}
{"id": "252", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "song", "country", "band", "event", "award", "person", "album", "location", "musical artist", "organization", "music genre"], "instance": {"id": "252", "words": ["He", "liked", "to", "surround", "himself", "with", "jazz", "musicians", "such", "as", "John", "McLaughlin", "and", "Binky", "McKenzie", "and", "often", "performed", "with", "a", "horn", "section", "drawn", "from", "a", "pool", "that", "included", ",", "among", "others", ",", "saxophone", "players", "Art", "Themen", ",", "Mel", "Collins", ",", "Dick", "Heckstall-Smith", ",", "Lol", "Coxhill", ",", "Dick", "Morrissey", ",", "John", "Surman", "and", "trombonist", "Mike", "Zwerin", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "B-musical instrument", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical instrument", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, song, country, band, event, award, person, album, location, musical artist, organization, music genre and O.\nSentence: He liked to surround himself with jazz musicians such as John McLaughlin and Binky McKenzie and often performed with a horn section drawn from a pool that included , among others , saxophone players Art Themen , Mel Collins , Dick Heckstall-Smith , Lol Coxhill , Dick Morrissey , John Surman and trombonist Mike Zwerin .", "prompt_labels": "He(O) liked(O) to(O) surround(O) himself(O) with(O) jazz(B-music genre) musicians(O) such(O) as(O) John(B-musical artist) McLaughlin(I-musical artist) and(O) Binky(B-musical artist) McKenzie(I-musical artist) and(O) often(O) performed(O) with(O) a(O) horn(B-musical instrument) section(O) drawn(O) from(O) a(O) pool(O) that(O) included(O) ,(O) among(O) others(O) ,(O) saxophone(B-musical instrument) players(O) Art(B-musical artist) Themen(I-musical artist) ,(O) Mel(B-musical artist) Collins(I-musical artist) ,(O) Dick(B-musical artist) Heckstall-Smith(I-musical artist) ,(O) Lol(B-musical artist) Coxhill(I-musical artist) ,(O) Dick(B-musical artist) Morrissey(I-musical artist) ,(O) John(B-musical artist) Surman(I-musical artist) and(O) trombonist(O) Mike(B-musical artist) Zwerin(I-musical artist) .(O)"}}
{"id": "288", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "person", "music genre", "award", "musical artist", "organization", "musical instrument", "album", "band", "country", "song", "event"], "instance": {"id": "288", "words": ["Underground", "scenes", "produced", "an", "array", "of", "more", "aggressive", "styles", ":", "thrash", "metal", "broke", "into", "the", "mainstream", "with", "bands", "such", "as", "Metallica", ",", "Slayer", ",", "Megadeth", ",", "and", "Anthrax", ",", "while", "other", "extreme", "subgenres", "of", "heavy", "metal", "such", "as", "death", "metal", "and", "black", "metal", "remain", "subcultural", "phenomena", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "O", "B-band", "O", "B-band", "O", "O", "B-band", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, music genre, award, musical artist, organization, musical instrument, album, band, country, song, event and O.\nSentence: Underground scenes produced an array of more aggressive styles : thrash metal broke into the mainstream with bands such as Metallica , Slayer , Megadeth , and Anthrax , while other extreme subgenres of heavy metal such as death metal and black metal remain subcultural phenomena .", "prompt_labels": "Underground(O) scenes(O) produced(O) an(O) array(O) of(O) more(O) aggressive(O) styles(O) :(O) thrash(B-music genre) metal(I-music genre) broke(O) into(O) the(O) mainstream(O) with(O) bands(O) such(O) as(O) Metallica(B-band) ,(O) Slayer(B-band) ,(O) Megadeth(B-band) ,(O) and(O) Anthrax(B-band) ,(O) while(O) other(O) extreme(O) subgenres(O) of(O) heavy(B-music genre) metal(I-music genre) such(O) as(O) death(B-music genre) metal(I-music genre) and(O) black(B-music genre) metal(I-music genre) remain(O) subcultural(O) phenomena(O) .(O)"}}
{"id": "68", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "person", "organization", "country", "song", "band", "album", "location", "award", "event", "music genre", "musical artist"], "instance": {"id": "68", "words": ["They", "also", "visited", "South", "America", "for", "the", "second", "time", "(", "the", "first", "time", "being", "in", "1999", ")", ",", "arriving", "at", "Chile", ",", "Argentina", ",", "and", "Brazil", "."], "labels": ["O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, person, organization, country, song, band, album, location, award, event, music genre, musical artist and O.\nSentence: They also visited South America for the second time ( the first time being in 1999 ) , arriving at Chile , Argentina , and Brazil .", "prompt_labels": "They(O) also(O) visited(O) South(B-location) America(I-location) for(O) the(O) second(O) time(O) ((O) the(O) first(O) time(O) being(O) in(O) 1999(O) )(O) ,(O) arriving(O) at(O) Chile(B-country) ,(O) Argentina(B-country) ,(O) and(O) Brazil(B-country) .(O)"}}
{"id": "86", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "band", "musical artist", "event", "award", "organization", "location", "person", "song", "album", "country", "music genre"], "instance": {"id": "86", "words": ["Other", "early", "examples", "include", "the", "Mercury", "Music", "Prize", "winning", "albums", "New", "Forms", "(", "1997", ")", "from", "Reprazent", "and", "OK", "(", "1998", ")", "from", "Talvin", "Singh", ",", "4hero", "'", "s", "Mercury-nominated", "Two", "Pages", "from", "1998", ",", "and", "Pendulum", "'", "s", "Hold", "Your", "Colour", "in", "2005", "(", "the", "best", "selling", "drum", "and", "bass", "album", "of", "all", "time", ")", "."], "labels": ["O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-band", "O", "B-band", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-band", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "B-musical instrument", "O", "B-musical instrument", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, band, musical artist, event, award, organization, location, person, song, album, country, music genre and O.\nSentence: Other early examples include the Mercury Music Prize winning albums New Forms ( 1997 ) from Reprazent and OK ( 1998 ) from Talvin Singh , 4hero ' s Mercury-nominated Two Pages from 1998 , and Pendulum ' s Hold Your Colour in 2005 ( the best selling drum and bass album of all time ) .", "prompt_labels": "Other(O) early(O) examples(O) include(O) the(O) Mercury(B-award) Music(I-award) Prize(I-award) winning(O) albums(O) New(B-album) Forms(I-album) ((O) 1997(O) )(O) from(O) Reprazent(B-band) and(O) OK(B-band) ((O) 1998(O) )(O) from(O) Talvin(B-musical artist) Singh(I-musical artist) ,(O) 4hero(B-band) '(O) s(O) Mercury-nominated(O) Two(B-album) Pages(I-album) from(O) 1998(O) ,(O) and(O) Pendulum(B-band) '(O) s(O) Hold(B-album) Your(I-album) Colour(I-album) in(O) 2005(O) ((O) the(O) best(O) selling(O) drum(B-musical instrument) and(O) bass(B-musical instrument) album(O) of(O) all(O) time(O) )(O) .(O)"}}
{"id": "50", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "award", "person", "album", "country", "song", "organization", "musical artist", "music genre", "musical instrument", "band", "location"], "instance": {"id": "50", "words": ["Castellano", "switched", "to", "rhythm", "guitar", "and", "keyboards", "(", "Castellano", "also", "filled", "in", "on", "lead", "guitar", "and", "vocals", "for", "an", "ailing", "Buck", "Dharma", "in", "two", "shows", "in", "2005", ")", ",", "and", "the", "position", "of", "bassist", "was", "taken", "up", "by", "Rudy", "Sarzo", "(", "previously", "a", "member", "of", "Quiet", "Riot", ",", "Whitesnake", ",", "Ozzy", "Osbourne", "and", "Dio", ")", ",", "with", "the", "band", "employing", "Danny", "Miranda", "and", "Jon", "Rogers", "as", "guest", "bassists", "to", "fill", "in", "when", "Sarzo", "was", "unavailable", "."], "labels": ["B-musical artist", "O", "O", "B-musical instrument", "I-musical instrument", "O", "B-musical instrument", "O", "B-musical artist", "O", "O", "O", "O", "O", "B-musical instrument", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, person, album, country, song, organization, musical artist, music genre, musical instrument, band, location and O.\nSentence: Castellano switched to rhythm guitar and keyboards ( Castellano also filled in on lead guitar and vocals for an ailing Buck Dharma in two shows in 2005 ) , and the position of bassist was taken up by Rudy Sarzo ( previously a member of Quiet Riot , Whitesnake , Ozzy Osbourne and Dio ) , with the band employing Danny Miranda and Jon Rogers as guest bassists to fill in when Sarzo was unavailable .", "prompt_labels": "Castellano(B-musical artist) switched(O) to(O) rhythm(B-musical instrument) guitar(I-musical instrument) and(O) keyboards(B-musical instrument) ((O) Castellano(B-musical artist) also(O) filled(O) in(O) on(O) lead(O) guitar(B-musical instrument) and(O) vocals(O) for(O) an(O) ailing(O) Buck(B-musical artist) Dharma(I-musical artist) in(O) two(O) shows(O) in(O) 2005(O) )(O) ,(O) and(O) the(O) position(O) of(O) bassist(O) was(O) taken(O) up(O) by(O) Rudy(B-musical artist) Sarzo(I-musical artist) ((O) previously(O) a(O) member(O) of(O) Quiet(B-band) Riot(I-band) ,(O) Whitesnake(B-band) ,(O) Ozzy(B-musical artist) Osbourne(I-musical artist) and(O) Dio(B-musical artist) )(O) ,(O) with(O) the(O) band(O) employing(O) Danny(B-musical artist) Miranda(I-musical artist) and(O) Jon(B-musical artist) Rogers(I-musical artist) as(O) guest(O) bassists(O) to(O) fill(O) in(O) when(O) Sarzo(B-musical artist) was(O) unavailable(O) .(O)"}}
{"id": "327", "dataset": "crossner_music", "split": "dev", "label_list": ["award", "person", "musical instrument", "song", "country", "organization", "album", "musical artist", "event", "band", "music genre", "location"], "instance": {"id": "327", "words": ["They", "take", "as", "an", "example", "Sabbath", "'s", "second", "album", "Paranoid", "(", "1970", ")", ",", "which", "included", "songs", "dealing", "with", "personal", "trauma", "-", "'", "Paranoid", "'", "and", "'", "Fairies", "Wear", "Boots", "'", "(", "which", "described", "the", "unsavoury", "side", "effects", "of", "drug-taking", ")", "-", "as", "well", "as", "those", "confronting", "wider", "issues", ",", "such", "as", "the", "self-explanatory", "'", "War", "Pigs", "'", "and", "'", "Hand", "of", "Doom", "'", "."], "labels": ["O", "O", "O", "O", "O", "B-musical artist", "O", "O", "O", "B-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "O", "O", "O", "B-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "O", "O", "O", "B-song", "I-song", "I-song", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, musical instrument, song, country, organization, album, musical artist, event, band, music genre, location and O.\nSentence: They take as an example Sabbath 's second album Paranoid ( 1970 ) , which included songs dealing with personal trauma - ' Paranoid ' and ' Fairies Wear Boots ' ( which described the unsavoury side effects of drug-taking ) - as well as those confronting wider issues , such as the self-explanatory ' War Pigs ' and ' Hand of Doom ' .", "prompt_labels": "They(O) take(O) as(O) an(O) example(O) Sabbath(B-musical artist) 's(O) second(O) album(O) Paranoid(B-album) ((O) 1970(O) )(O) ,(O) which(O) included(O) songs(O) dealing(O) with(O) personal(O) trauma(O) -(O) '(O) Paranoid(B-song) '(O) and(O) '(O) Fairies(B-song) Wear(I-song) Boots(I-song) '(O) ((O) which(O) described(O) the(O) unsavoury(O) side(O) effects(O) of(O) drug-taking(O) )(O) -(O) as(O) well(O) as(O) those(O) confronting(O) wider(O) issues(O) ,(O) such(O) as(O) the(O) self-explanatory(O) '(O) War(B-song) Pigs(I-song) '(O) and(O) '(O) Hand(B-song) of(I-song) Doom(I-song) '(O) .(O)"}}
{"id": "299", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "award", "event", "musical artist", "album", "person", "musical instrument", "song", "band", "country", "organization", "location"], "instance": {"id": "299", "words": ["Tim", "Burton", "appeared", "at", "the", "2009", "Comic-Con", "in", "San", "Diego", ",", "California", ",", "to", "promote", "both", "9", "and", "Alice", "in", "Wonderland", ",", "the", "latter", "won", "two", "Academy", "Awards", ",", "for", "Academy", "Award", "for", "Best", "Production", "Design", "and", "Academy", "Award", "for", "Best", "Costume", "Design", "."], "labels": ["B-person", "I-person", "O", "O", "O", "B-event", "I-event", "O", "B-location", "I-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, award, event, musical artist, album, person, musical instrument, song, band, country, organization, location and O.\nSentence: Tim Burton appeared at the 2009 Comic-Con in San Diego , California , to promote both 9 and Alice in Wonderland , the latter won two Academy Awards , for Academy Award for Best Production Design and Academy Award for Best Costume Design .", "prompt_labels": "Tim(B-person) Burton(I-person) appeared(O) at(O) the(O) 2009(B-event) Comic-Con(I-event) in(O) San(B-location) Diego(I-location) ,(O) California(B-location) ,(O) to(O) promote(O) both(O) 9(O) and(O) Alice(O) in(O) Wonderland(O) ,(O) the(O) latter(O) won(O) two(O) Academy(B-award) Awards(I-award) ,(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Production(I-award) Design(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Costume(I-award) Design(I-award) .(O)"}}
{"id": "304", "dataset": "crossner_music", "split": "dev", "label_list": ["musical artist", "band", "song", "organization", "country", "album", "event", "award", "location", "musical instrument", "music genre", "person"], "instance": {"id": "304", "words": ["Schayes", "played", "on", "the", "US", "basketball", "team", "that", "won", "a", "gold", "medal", "at", "the", "1977", "Maccabiah", "Games", "in", "Tel", "Aviv", ",", "Israel", ";", "he", "also", "played", "for", "Team", "USA", "at", "the", "1981", "Maccabiah", "Games", ",", "which", "won", "a", "gold", "medal", "."], "labels": ["B-person", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "B-location", "I-location", "O", "B-country", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, band, song, organization, country, album, event, award, location, musical instrument, music genre, person and O.\nSentence: Schayes played on the US basketball team that won a gold medal at the 1977 Maccabiah Games in Tel Aviv , Israel ; he also played for Team USA at the 1981 Maccabiah Games , which won a gold medal .", "prompt_labels": "Schayes(B-person) played(O) on(O) the(O) US(B-organization) basketball(I-organization) team(I-organization) that(O) won(O) a(O) gold(O) medal(O) at(O) the(O) 1977(B-event) Maccabiah(I-event) Games(I-event) in(O) Tel(B-location) Aviv(I-location) ,(O) Israel(B-country) ;(O) he(O) also(O) played(O) for(O) Team(B-organization) USA(I-organization) at(O) the(O) 1981(B-event) Maccabiah(I-event) Games(I-event) ,(O) which(O) won(O) a(O) gold(O) medal(O) .(O)"}}
{"id": "76", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "award", "country", "organization", "musical artist", "song", "musical instrument", "album", "music genre", "location", "band", "person"], "instance": {"id": "76", "words": ["It", "comprises", "the", "music", "of", "Bosnia", "and", "Herzegovina", ",", "Bulgaria", ",", "Croatia", ",", "Music", "of", "Greece", ",", "Montenegro", ",", "Serbia", ",", "Romania", ",", "Republic", "of", "Macedonia", ",", "Albania", ",", "some", "of", "the", "historical", "states", "of", "Yugoslavia", "or", "the", "State", "Union", "of", "Serbia", "and", "Montenegro", "and", "geographical", "regions", "such", "as", "Thrace", "."], "labels": ["O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "I-music genre", "I-music genre", "O", "B-country", "O", "B-country", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "I-country", "I-country", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "B-country", "I-country", "I-country", "I-country", "I-country", "I-country", "O", "O", "O", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, country, organization, musical artist, song, musical instrument, album, music genre, location, band, person and O.\nSentence: It comprises the music of Bosnia and Herzegovina , Bulgaria , Croatia , Music of Greece , Montenegro , Serbia , Romania , Republic of Macedonia , Albania , some of the historical states of Yugoslavia or the State Union of Serbia and Montenegro and geographical regions such as Thrace .", "prompt_labels": "It(O) comprises(O) the(O) music(B-music genre) of(I-music genre) Bosnia(I-music genre) and(I-music genre) Herzegovina(I-music genre) ,(O) Bulgaria(B-country) ,(O) Croatia(B-country) ,(O) Music(B-music genre) of(I-music genre) Greece(I-music genre) ,(O) Montenegro(B-country) ,(O) Serbia(B-country) ,(O) Romania(B-country) ,(O) Republic(B-country) of(I-country) Macedonia(I-country) ,(O) Albania(B-country) ,(O) some(O) of(O) the(O) historical(O) states(O) of(O) Yugoslavia(B-country) or(O) the(O) State(B-country) Union(I-country) of(I-country) Serbia(I-country) and(I-country) Montenegro(I-country) and(O) geographical(O) regions(O) such(O) as(O) Thrace(B-location) .(O)"}}
{"id": "31", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "award", "location", "album", "person", "musical artist", "country", "event", "song", "organization", "music genre", "musical instrument"], "instance": {"id": "31", "words": ["In", "the", "summer", "of", "2004", ",", "Diab", ",", "having", "left", "Alam", "El", "Phan", ",", "released", "his", "first", "album", "with", "Rotana", "Records", ",", "Leily", "Nahary", ",", "which", "he", "followed", "up", "with", "the", "hugely", "successful", "Kammel", "Kalamak", "(", "2005", ")", ",", "and", "El", "Lilady", "(", "2007", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, award, location, album, person, musical artist, country, event, song, organization, music genre, musical instrument and O.\nSentence: In the summer of 2004 , Diab , having left Alam El Phan , released his first album with Rotana Records , Leily Nahary , which he followed up with the hugely successful Kammel Kalamak ( 2005 ) , and El Lilady ( 2007 ) .", "prompt_labels": "In(O) the(O) summer(O) of(O) 2004(O) ,(O) Diab(B-musical artist) ,(O) having(O) left(O) Alam(B-organization) El(I-organization) Phan(I-organization) ,(O) released(O) his(O) first(O) album(O) with(O) Rotana(B-organization) Records(I-organization) ,(O) Leily(B-album) Nahary(I-album) ,(O) which(O) he(O) followed(O) up(O) with(O) the(O) hugely(O) successful(O) Kammel(B-album) Kalamak(I-album) ((O) 2005(O) )(O) ,(O) and(O) El(B-album) Lilady(I-album) ((O) 2007(O) )(O) .(O)"}}
{"id": "348", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "song", "location", "country", "person", "award", "musical instrument", "album", "band", "event", "musical artist", "organization"], "instance": {"id": "348", "words": ["Artists", "popularizing", "more", "traditional", "country", "music", "in", "Sweden", "have", "been", "Ann-Louise", "Hanson", ",", "Hasse", "Andersson", ",", "Kikki", "Danielsson", ",", "Elisabeth", "Andreassen", "and", "Jill", "Johnson", "."], "labels": ["O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-country", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, song, location, country, person, award, musical instrument, album, band, event, musical artist, organization and O.\nSentence: Artists popularizing more traditional country music in Sweden have been Ann-Louise Hanson , Hasse Andersson , Kikki Danielsson , Elisabeth Andreassen and Jill Johnson .", "prompt_labels": "Artists(O) popularizing(O) more(O) traditional(B-music genre) country(I-music genre) music(I-music genre) in(O) Sweden(B-country) have(O) been(O) Ann-Louise(B-musical artist) Hanson(I-musical artist) ,(O) Hasse(B-musical artist) Andersson(I-musical artist) ,(O) Kikki(B-musical artist) Danielsson(I-musical artist) ,(O) Elisabeth(B-musical artist) Andreassen(I-musical artist) and(O) Jill(B-musical artist) Johnson(I-musical artist) .(O)"}}
{"id": "78", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "person", "musical instrument", "musical artist", "music genre", "band", "organization", "album", "award", "song", "country", "event"], "instance": {"id": "78", "words": ["In", "2005", "Jamieson", "won", "Best", "Male", "Performer", "in", "the", "second", "annual", "Jack", "Awards", ",", "Jamieson", "showcased", "the", "sounds", "of", "Grinspoon", "to", "millions", "of", "viewers", "in", "March", "2006", ",", "playing", "live", "at", "Melbourne", "Cricket", "Ground", "as", "part", "of", "the", "closing", "ceremony", "of", "the", "2006", "Commonwealth", "Games", "."], "labels": ["O", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "B-musical artist", "O", "O", "O", "O", "B-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, musical instrument, musical artist, music genre, band, organization, album, award, song, country, event and O.\nSentence: In 2005 Jamieson won Best Male Performer in the second annual Jack Awards , Jamieson showcased the sounds of Grinspoon to millions of viewers in March 2006 , playing live at Melbourne Cricket Ground as part of the closing ceremony of the 2006 Commonwealth Games .", "prompt_labels": "In(O) 2005(O) Jamieson(B-musical artist) won(O) Best(O) Male(O) Performer(O) in(O) the(O) second(B-award) annual(I-award) Jack(I-award) Awards(I-award) ,(O) Jamieson(B-musical artist) showcased(O) the(O) sounds(O) of(O) Grinspoon(B-band) to(O) millions(O) of(O) viewers(O) in(O) March(O) 2006(O) ,(O) playing(O) live(O) at(O) Melbourne(B-location) Cricket(I-location) Ground(I-location) as(O) part(O) of(O) the(O) closing(O) ceremony(O) of(O) the(O) 2006(B-event) Commonwealth(I-event) Games(I-event) .(O)"}}
{"id": "265", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "musical artist", "organization", "album", "musical instrument", "song", "person", "music genre", "award", "country", "band", "location"], "instance": {"id": "265", "words": ["The", "game", "features", "the", "full", "albums", "of", "Dookie", ",", "American", "Idiot", ",", "and", "21st", "Century", "Breakdown", "as", "well", "as", "select", "songs", "from", "the", "rest", "of", "Green", "Day", "'s", "discography", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-album", "O", "B-album", "I-album", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical artist, organization, album, musical instrument, song, person, music genre, award, country, band, location and O.\nSentence: The game features the full albums of Dookie , American Idiot , and 21st Century Breakdown as well as select songs from the rest of Green Day 's discography .", "prompt_labels": "The(O) game(O) features(O) the(O) full(O) albums(O) of(O) Dookie(B-album) ,(O) American(B-album) Idiot(I-album) ,(O) and(O) 21st(B-album) Century(I-album) Breakdown(I-album) as(O) well(O) as(O) select(O) songs(O) from(O) the(O) rest(O) of(O) Green(B-band) Day(I-band) 's(O) discography(O) .(O)"}}
{"id": "305", "dataset": "crossner_music", "split": "dev", "label_list": ["person", "song", "country", "musical artist", "band", "location", "award", "event", "organization", "music genre", "album", "musical instrument"], "instance": {"id": "305", "words": ["Touring", "in", "support", "of", "Cross", "Purposes", "began", "in", "February", "with", "Morbid", "Angel", "and", "Motörhead", "in", "the", "U.S.", "The", "band", "filmed", "a", "live", "performance", "at", "the", "Hammersmith", "Apollo", "on", "13", "April", "1994", ",", "which", "was", "released", "on", "VHS", "accompanied", "by", "a", "CD", ",", "titled", "Cross", "Purposes", "Live", "."], "labels": ["O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, song, country, musical artist, band, location, award, event, organization, music genre, album, musical instrument and O.\nSentence: Touring in support of Cross Purposes began in February with Morbid Angel and Motörhead in the U.S. The band filmed a live performance at the Hammersmith Apollo on 13 April 1994 , which was released on VHS accompanied by a CD , titled Cross Purposes Live .", "prompt_labels": "Touring(O) in(O) support(O) of(O) Cross(B-album) Purposes(I-album) began(O) in(O) February(O) with(O) Morbid(B-band) Angel(I-band) and(O) Motörhead(B-band) in(O) the(O) U.S.(B-country) The(O) band(O) filmed(O) a(O) live(O) performance(O) at(O) the(O) Hammersmith(B-location) Apollo(I-location) on(O) 13(O) April(O) 1994(O) ,(O) which(O) was(O) released(O) on(O) VHS(O) accompanied(O) by(O) a(O) CD(O) ,(O) titled(O) Cross(B-album) Purposes(I-album) Live(I-album) .(O)"}}
{"id": "294", "dataset": "crossner_music", "split": "dev", "label_list": ["person", "song", "music genre", "organization", "musical artist", "musical instrument", "event", "award", "band", "country", "album", "location"], "instance": {"id": "294", "words": ["In", "April", "2018", ",", "Roach", "performed", "at", "the", "2018", "Commonwealth", "Games", "Closing", "Ceremony", "on", "the", "Gold", "Coast", "with", "Amy", "Shark", "."], "labels": ["O", "O", "O", "O", "B-musical artist", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "B-location", "I-location", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, song, music genre, organization, musical artist, musical instrument, event, award, band, country, album, location and O.\nSentence: In April 2018 , Roach performed at the 2018 Commonwealth Games Closing Ceremony on the Gold Coast with Amy Shark .", "prompt_labels": "In(O) April(O) 2018(O) ,(O) Roach(B-musical artist) performed(O) at(O) the(O) 2018(B-event) Commonwealth(I-event) Games(I-event) Closing(I-event) Ceremony(I-event) on(O) the(O) Gold(B-location) Coast(I-location) with(O) Amy(B-musical artist) Shark(I-musical artist) .(O)"}}
{"id": "354", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "band", "person", "event", "musical artist", "organization", "music genre", "location", "album", "award", "country", "musical instrument"], "instance": {"id": "354", "words": ["Holly", "was", "unhappy", "with", "the", "results", "of", "his", "time", "with", "Decca", ";", "he", "was", "inspired", "by", "the", "success", "of", "Buddy", "Knox", "'", "s", "Party", "Doll", "and", "Jimmy", "Bowen", "'", "s", "I", "'m", "Stickin", "'", "with", "You", ",", "and", "visited", "Norman", "Petty", ",", "who", "had", "produced", "and", "promoted", "both", "records", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "B-song", "I-song", "O", "B-musical artist", "I-musical artist", "O", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "I-song", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, band, person, event, musical artist, organization, music genre, location, album, award, country, musical instrument and O.\nSentence: Holly was unhappy with the results of his time with Decca ; he was inspired by the success of Buddy Knox ' s Party Doll and Jimmy Bowen ' s I 'm Stickin ' with You , and visited Norman Petty , who had produced and promoted both records .", "prompt_labels": "Holly(B-musical artist) was(O) unhappy(O) with(O) the(O) results(O) of(O) his(O) time(O) with(O) Decca(B-organization) ;(O) he(O) was(O) inspired(O) by(O) the(O) success(O) of(O) Buddy(B-musical artist) Knox(I-musical artist) '(O) s(O) Party(B-song) Doll(I-song) and(O) Jimmy(B-musical artist) Bowen(I-musical artist) '(O) s(O) I(B-song) 'm(I-song) Stickin(I-song) '(I-song) with(I-song) You(I-song) ,(O) and(O) visited(O) Norman(B-musical artist) Petty(I-musical artist) ,(O) who(O) had(O) produced(O) and(O) promoted(O) both(O) records(O) .(O)"}}
{"id": "344", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "band", "organization", "country", "person", "album", "music genre", "award", "musical instrument", "location", "musical artist", "song"], "instance": {"id": "344", "words": ["The", "genre", "has", "influenced", "many", "other", "genres", "like", "hip", "hop", ",", "big", "beat", ",", "house", "music", ",", "trip", "hop", ",", "ambient", "music", ",", "techno", ",", "rock", "and", "pop", ",", "with", "artists", "such", "as", "Bill", "Laswell", ",", "Incubus", ",", "Pitchshifter", ",", "Linkin", "Park", ",", "The", "Roots", ",", "Talvin", "Singh", ",", "MIDIval", "Punditz", ",", "Missy", "Elliott", ",", "The", "Freestylers", ",", "Bowery", "Electric", ",", "Nine", "Inch", "Nails", ",", "David", "Bowie", "(", "the", "last", "two", "both", "using", "elements", "of", "Goldie", "'s", "Timeless", ")"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "O", "B-music genre", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "O", "B-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, band, organization, country, person, album, music genre, award, musical instrument, location, musical artist, song and O.\nSentence: The genre has influenced many other genres like hip hop , big beat , house music , trip hop , ambient music , techno , rock and pop , with artists such as Bill Laswell , Incubus , Pitchshifter , Linkin Park , The Roots , Talvin Singh , MIDIval Punditz , Missy Elliott , The Freestylers , Bowery Electric , Nine Inch Nails , David Bowie ( the last two both using elements of Goldie 's Timeless )", "prompt_labels": "The(O) genre(O) has(O) influenced(O) many(O) other(O) genres(O) like(O) hip(B-music genre) hop(I-music genre) ,(O) big(B-music genre) beat(I-music genre) ,(O) house(B-music genre) music(I-music genre) ,(O) trip(B-music genre) hop(I-music genre) ,(O) ambient(B-music genre) music(I-music genre) ,(O) techno(B-music genre) ,(O) rock(B-music genre) and(O) pop(B-music genre) ,(O) with(O) artists(O) such(O) as(O) Bill(B-musical artist) Laswell(I-musical artist) ,(O) Incubus(B-band) ,(O) Pitchshifter(B-band) ,(O) Linkin(B-band) Park(I-band) ,(O) The(B-band) Roots(I-band) ,(O) Talvin(B-musical artist) Singh(I-musical artist) ,(O) MIDIval(B-band) Punditz(I-band) ,(O) Missy(B-musical artist) Elliott(I-musical artist) ,(O) The(B-band) Freestylers(I-band) ,(O) Bowery(B-band) Electric(I-band) ,(O) Nine(B-band) Inch(I-band) Nails(I-band) ,(O) David(B-musical artist) Bowie(I-musical artist) ((O) the(O) last(O) two(O) both(O) using(O) elements(O) of(O) Goldie(B-musical artist) 's(O) Timeless(B-album) )(O)"}}
{"id": "89", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "musical instrument", "organization", "event", "country", "band", "musical artist", "album", "location", "music genre", "award", "person"], "instance": {"id": "89", "words": ["Ullman", "and", "the", "show", "went", "on", "to", "receive", "a", "slew", "of", "awards", "including", "six", "Emmy", "Awards", ",", "two", "CableAce", "Awards", ",", "three", "American", "Comedy", "Awards", ",", "two", "GLAAD", "Media", "Award", "s", ",", "as", "well", "as", "a", "Screen", "Actors", "Guild", "Awards", "in", "1999", "for", "Screen", "Actors", "Guild", "Award", "for", "Outstanding", "Performance", "by", "a", "Female", "Actor", "in", "a", "Comedy", "Series", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, musical instrument, organization, event, country, band, musical artist, album, location, music genre, award, person and O.\nSentence: Ullman and the show went on to receive a slew of awards including six Emmy Awards , two CableAce Awards , three American Comedy Awards , two GLAAD Media Award s , as well as a Screen Actors Guild Awards in 1999 for Screen Actors Guild Award for Outstanding Performance by a Female Actor in a Comedy Series .", "prompt_labels": "Ullman(B-musical artist) and(O) the(O) show(O) went(O) on(O) to(O) receive(O) a(O) slew(O) of(O) awards(O) including(O) six(O) Emmy(B-award) Awards(I-award) ,(O) two(O) CableAce(B-award) Awards(I-award) ,(O) three(O) American(B-award) Comedy(I-award) Awards(I-award) ,(O) two(O) GLAAD(B-award) Media(I-award) Award(I-award) s(O) ,(O) as(O) well(O) as(O) a(O) Screen(B-award) Actors(I-award) Guild(I-award) Awards(I-award) in(O) 1999(O) for(O) Screen(B-award) Actors(I-award) Guild(I-award) Award(I-award) for(I-award) Outstanding(I-award) Performance(I-award) by(I-award) a(I-award) Female(I-award) Actor(I-award) in(I-award) a(I-award) Comedy(I-award) Series(I-award) .(O)"}}
{"id": "130", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "band", "event", "musical artist", "location", "person", "music genre", "organization", "musical instrument", "country", "song", "award"], "instance": {"id": "130", "words": ["Following", "the", "number-one", "success", "of", "MARRS", "'", "Pump", "Up", "The", "Volume", "in", "October", ",", "in", "1987", "to", "1989", ",", "UK", "acts", "such", "as", "The", "Beatmasters", ",", "Krush", ",", "Coldcut", ",", "Yazz", ",", "Bomb", "The", "Bass", ",", "S-Express", ",", "and", "Italy", "'s", "Black", "Box", "opened", "the", "doors", "to", "house", "music", "success", "on", "the", "UK", "charts", "."], "labels": ["O", "O", "O", "O", "O", "B-band", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "B-band", "I-band", "O", "B-band", "O", "B-band", "O", "B-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "O", "O", "B-country", "O", "B-band", "I-band", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "B-country", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, band, event, musical artist, location, person, music genre, organization, musical instrument, country, song, award and O.\nSentence: Following the number-one success of MARRS ' Pump Up The Volume in October , in 1987 to 1989 , UK acts such as The Beatmasters , Krush , Coldcut , Yazz , Bomb The Bass , S-Express , and Italy 's Black Box opened the doors to house music success on the UK charts .", "prompt_labels": "Following(O) the(O) number-one(O) success(O) of(O) MARRS(B-band) '(O) Pump(B-song) Up(I-song) The(I-song) Volume(I-song) in(O) October(O) ,(O) in(O) 1987(O) to(O) 1989(O) ,(O) UK(B-country) acts(O) such(O) as(O) The(B-band) Beatmasters(I-band) ,(O) Krush(B-band) ,(O) Coldcut(B-band) ,(O) Yazz(B-band) ,(O) Bomb(B-band) The(I-band) Bass(I-band) ,(O) S-Express(B-band) ,(O) and(O) Italy(B-country) 's(O) Black(B-band) Box(I-band) opened(O) the(O) doors(O) to(O) house(B-music genre) music(I-music genre) success(O) on(O) the(O) UK(B-country) charts(O) .(O)"}}
{"id": "218", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "album", "organization", "event", "music genre", "award", "location", "country", "song", "musical artist", "person", "musical instrument"], "instance": {"id": "218", "words": ["139", "with", "Curley", "Weaver", ",", "Tampa", "Red", ",", "Barbecue", "Bob", "and", "Kokomo", "Arnold", "as", "representatives", "of", "this", "style", "."], "labels": ["O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, album, organization, event, music genre, award, location, country, song, musical artist, person, musical instrument and O.\nSentence: 139 with Curley Weaver , Tampa Red , Barbecue Bob and Kokomo Arnold as representatives of this style .", "prompt_labels": "139(O) with(O) Curley(B-musical artist) Weaver(I-musical artist) ,(O) Tampa(B-musical artist) Red(I-musical artist) ,(O) Barbecue(B-musical artist) Bob(I-musical artist) and(O) Kokomo(B-musical artist) Arnold(I-musical artist) as(O) representatives(O) of(O) this(O) style(O) .(O)"}}
{"id": "319", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "music genre", "musical artist", "band", "musical instrument", "location", "award", "organization", "country", "song", "album", "person"], "instance": {"id": "319", "words": ["Three", "more", "albums", "followed", ":", "1985", "'s", "Little", "Creatures", "(", "which", "featured", "the", "hit", "singles", "And", "She", "Was", "and", "Road", "to", "Nowhere", ")", ",", "During", "that", "time", ",", "the", "group", "was", "falling", "increasingly", "under", "David", "Byrne", "'s", "control", "and", ",", "after", "Naked", ",", "the", "band", "went", "on", "hiatus", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "B-album", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, music genre, musical artist, band, musical instrument, location, award, organization, country, song, album, person and O.\nSentence: Three more albums followed : 1985 's Little Creatures ( which featured the hit singles And She Was and Road to Nowhere ) , During that time , the group was falling increasingly under David Byrne 's control and , after Naked , the band went on hiatus .", "prompt_labels": "Three(O) more(O) albums(O) followed(O) :(O) 1985(O) 's(O) Little(B-album) Creatures(I-album) ((O) which(O) featured(O) the(O) hit(O) singles(O) And(B-song) She(I-song) Was(I-song) and(O) Road(B-song) to(I-song) Nowhere(I-song) )(O) ,(O) During(O) that(O) time(O) ,(O) the(O) group(O) was(O) falling(O) increasingly(O) under(O) David(B-musical artist) Byrne(I-musical artist) 's(O) control(O) and(O) ,(O) after(O) Naked(B-album) ,(O) the(O) band(O) went(O) on(O) hiatus(O) .(O)"}}
{"id": "131", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "organization", "musical artist", "musical instrument", "person", "album", "award", "song", "country", "event", "location", "music genre"], "instance": {"id": "131", "words": ["Organised", "by", "the", "European", "Broadcasting", "Union", "(", "EBU", ")", "and", "host", "broadcaster", "Eesti", "Televisioon", "(", "ETV", ")", ",", "the", "contest", "was", "held", "at", "the", "Saku", "Suurhall", ",", "with", "the", "final", "on", "25", "May", "2002", "."], "labels": ["O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, organization, musical artist, musical instrument, person, album, award, song, country, event, location, music genre and O.\nSentence: Organised by the European Broadcasting Union ( EBU ) and host broadcaster Eesti Televisioon ( ETV ) , the contest was held at the Saku Suurhall , with the final on 25 May 2002 .", "prompt_labels": "Organised(O) by(O) the(O) European(B-organization) Broadcasting(I-organization) Union(I-organization) ((O) EBU(B-organization) )(O) and(O) host(O) broadcaster(O) Eesti(B-organization) Televisioon(I-organization) ((O) ETV(B-organization) )(O) ,(O) the(O) contest(O) was(O) held(O) at(O) the(O) Saku(B-location) Suurhall(I-location) ,(O) with(O) the(O) final(O) on(O) 25(O) May(O) 2002(O) .(O)"}}
{"id": "40", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "musical artist", "band", "person", "musical instrument", "award", "location", "album", "event", "organization", "music genre", "country"], "instance": {"id": "40", "words": ["Notable", "rockabilly", "revivalists", "and", "psychobilly", "performers", "from", "the", "1990s", "and", "first", "decade", "of", "the", "21st", "century", "include", "Scott", "Owen", "(", "from", "the", "Australian", "band", "The", "Living", "End", ")", ",", "Jimbo", "Wallace", "(", "from", "the", "US", "band", "Reverend", "Horton", "Heat", ")", ",", "Kim", "Nekroman", "(", "Nekromantix", ")", ",", "Patricia", "Day", "(", "HorrorPops", ")", ",", "Geoff", "Kresge", "(", "Tiger", "Army", ",", "ex-", "AFI", ")", "."], "labels": ["O", "B-music genre", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "B-country", "O", "B-band", "I-band", "I-band", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O", "O", "B-band", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, musical artist, band, person, musical instrument, award, location, album, event, organization, music genre, country and O.\nSentence: Notable rockabilly revivalists and psychobilly performers from the 1990s and first decade of the 21st century include Scott Owen ( from the Australian band The Living End ) , Jimbo Wallace ( from the US band Reverend Horton Heat ) , Kim Nekroman ( Nekromantix ) , Patricia Day ( HorrorPops ) , Geoff Kresge ( Tiger Army , ex- AFI ) .", "prompt_labels": "Notable(O) rockabilly(B-music genre) revivalists(O) and(O) psychobilly(B-music genre) performers(O) from(O) the(O) 1990s(O) and(O) first(O) decade(O) of(O) the(O) 21st(O) century(O) include(O) Scott(B-musical artist) Owen(I-musical artist) ((O) from(O) the(O) Australian(O) band(O) The(B-band) Living(I-band) End(I-band) )(O) ,(O) Jimbo(B-musical artist) Wallace(I-musical artist) ((O) from(O) the(O) US(B-country) band(O) Reverend(B-band) Horton(I-band) Heat(I-band) )(O) ,(O) Kim(B-musical artist) Nekroman(I-musical artist) ((O) Nekromantix(B-band) )(O) ,(O) Patricia(B-musical artist) Day(I-musical artist) ((O) HorrorPops(B-band) )(O) ,(O) Geoff(B-musical artist) Kresge(I-musical artist) ((O) Tiger(B-band) Army(I-band) ,(O) ex-(O) AFI(B-band) )(O) .(O)"}}
{"id": "242", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "location", "person", "musical instrument", "event", "song", "country", "musical artist", "organization", "award", "band", "album"], "instance": {"id": "242", "words": ["Bedrock", "Records", "is", "an", "English", "record", "label", "for", "trance", ",", "house", "and", "techno", "started", "by", "Nick", "Muir", "and", "John", "Digweed", "."], "labels": ["B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "B-music genre", "O", "B-music genre", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, location, person, musical instrument, event, song, country, musical artist, organization, award, band, album and O.\nSentence: Bedrock Records is an English record label for trance , house and techno started by Nick Muir and John Digweed .", "prompt_labels": "Bedrock(B-organization) Records(I-organization) is(O) an(O) English(O) record(O) label(O) for(O) trance(B-music genre) ,(O) house(B-music genre) and(O) techno(B-music genre) started(O) by(O) Nick(B-musical artist) Muir(I-musical artist) and(O) John(B-musical artist) Digweed(I-musical artist) .(O)"}}
{"id": "201", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "country", "song", "album", "music genre", "organization", "award", "musical instrument", "event", "musical artist", "person", "band"], "instance": {"id": "201", "words": ["They", "collaborated", "with", "producer", "Brian", "Eno", "on", "a", "trio", "of", "experimental", "and", "critically", "acclaimed", "releases", ":", "More", "Songs", "About", "Buildings", "and", "Food", "(", "1978", ")", ",", "Fear", "of", "Music", "(", "1979", ")", ",", "and", "Remain", "in", "Light", "(", "1980", ")", "."], "labels": ["O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, song, album, music genre, organization, award, musical instrument, event, musical artist, person, band and O.\nSentence: They collaborated with producer Brian Eno on a trio of experimental and critically acclaimed releases : More Songs About Buildings and Food ( 1978 ) , Fear of Music ( 1979 ) , and Remain in Light ( 1980 ) .", "prompt_labels": "They(O) collaborated(O) with(O) producer(O) Brian(B-musical artist) Eno(I-musical artist) on(O) a(O) trio(O) of(O) experimental(O) and(O) critically(O) acclaimed(O) releases(O) :(O) More(B-album) Songs(I-album) About(I-album) Buildings(I-album) and(I-album) Food(I-album) ((O) 1978(O) )(O) ,(O) Fear(B-album) of(I-album) Music(I-album) ((O) 1979(O) )(O) ,(O) and(O) Remain(B-album) in(I-album) Light(I-album) ((O) 1980(O) )(O) .(O)"}}
{"id": "62", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "music genre", "event", "album", "person", "song", "organization", "musical instrument", "musical artist", "location", "award", "band"], "instance": {"id": "62", "words": ["142", "He", "returned", "to", "the", "stage", "on", "8", "September", "2005", ",", "appearing", "with", "Arcade", "Fire", "for", "the", "US", "nationally", "televised", "event", "Fashion", "Rocks", ",", "and", "performed", "with", "the", "Canadian", "band", "for", "the", "second", "time", "a", "week", "later", "during", "the", "CMJ", "Music", "Marathon", ".", "Thompson", "(", "2006", ")", ":", "pp.", "291-92", "He", "contributed", "backing", "vocals", "on", "TV", "on", "the", "Radio", "'", "s", "song", "Province", "for", "their", "album", "Return", "to", "Cookie", "Mountain", ","], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "O", "B-country", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "I-band", "O", "O", "O", "B-song", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, music genre, event, album, person, song, organization, musical instrument, musical artist, location, award, band and O.\nSentence: 142 He returned to the stage on 8 September 2005 , appearing with Arcade Fire for the US nationally televised event Fashion Rocks , and performed with the Canadian band for the second time a week later during the CMJ Music Marathon . Thompson ( 2006 ) : pp. 291-92 He contributed backing vocals on TV on the Radio ' s song Province for their album Return to Cookie Mountain ,", "prompt_labels": "142(O) He(O) returned(O) to(O) the(O) stage(O) on(O) 8(O) September(O) 2005(O) ,(O) appearing(O) with(O) Arcade(B-band) Fire(I-band) for(O) the(O) US(B-country) nationally(O) televised(O) event(O) Fashion(B-event) Rocks(I-event) ,(O) and(O) performed(O) with(O) the(O) Canadian(O) band(O) for(O) the(O) second(O) time(O) a(O) week(O) later(O) during(O) the(O) CMJ(B-event) Music(I-event) Marathon(I-event) .(O) Thompson(B-musical artist) ((O) 2006(O) )(O) :(O) pp.(O) 291-92(O) He(O) contributed(O) backing(O) vocals(O) on(O) TV(B-band) on(I-band) the(I-band) Radio(I-band) '(O) s(O) song(O) Province(B-song) for(O) their(O) album(O) Return(B-album) to(I-album) Cookie(I-album) Mountain(I-album) ,(O)"}}
{"id": "274", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "album", "musical artist", "music genre", "award", "organization", "musical instrument", "event", "person", "location", "song", "band"], "instance": {"id": "274", "words": ["Comprising", "25", "albums", ",", "the", "series", "includes", "music", "from", "regions", "that", "span", "the", "globe", ",", "including", "the", "Sudan", ",", "Nigeria", ",", "Tibet", ",", "Indonesia", ",", "Latvia", ",", "and", "Brazil", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "B-location", "O", "B-country", "O", "B-country", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, album, musical artist, music genre, award, organization, musical instrument, event, person, location, song, band and O.\nSentence: Comprising 25 albums , the series includes music from regions that span the globe , including the Sudan , Nigeria , Tibet , Indonesia , Latvia , and Brazil .", "prompt_labels": "Comprising(O) 25(O) albums(O) ,(O) the(O) series(O) includes(O) music(O) from(O) regions(O) that(O) span(O) the(O) globe(O) ,(O) including(O) the(O) Sudan(B-country) ,(O) Nigeria(B-country) ,(O) Tibet(B-location) ,(O) Indonesia(B-country) ,(O) Latvia(B-country) ,(O) and(O) Brazil(B-country) .(O)"}}
{"id": "192", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "musical instrument", "album", "band", "song", "event", "award", "location", "music genre", "musical artist", "country", "person"], "instance": {"id": "192", "words": ["Grunge", "began", "as", "a", "mixture", "of", "Heavy", "metal", "music", ",", "punk", "rock", "and", "indie", "rock", "in", "the", "1980s", "and", "gained", "mainstream", "prominence", "in", "the", "early", "1990s", "."], "labels": ["B-music genre", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, musical instrument, album, band, song, event, award, location, music genre, musical artist, country, person and O.\nSentence: Grunge began as a mixture of Heavy metal music , punk rock and indie rock in the 1980s and gained mainstream prominence in the early 1990s .", "prompt_labels": "Grunge(B-music genre) began(O) as(O) a(O) mixture(O) of(O) Heavy(B-music genre) metal(I-music genre) music(I-music genre) ,(O) punk(B-music genre) rock(I-music genre) and(O) indie(B-music genre) rock(I-music genre) in(O) the(O) 1980s(O) and(O) gained(O) mainstream(O) prominence(O) in(O) the(O) early(O) 1990s(O) .(O)"}}
{"id": "26", "dataset": "crossner_music", "split": "dev", "label_list": ["musical artist", "event", "musical instrument", "organization", "song", "music genre", "award", "country", "band", "person", "location", "album"], "instance": {"id": "26", "words": ["Special", "guests", "were", "Pete", "Seeger", ",", "Bonnie", "Raitt", ",", "David", "Bromberg", "and", "Jerry", "Jeff", "Walker", "."], "labels": ["O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, event, musical instrument, organization, song, music genre, award, country, band, person, location, album and O.\nSentence: Special guests were Pete Seeger , Bonnie Raitt , David Bromberg and Jerry Jeff Walker .", "prompt_labels": "Special(O) guests(O) were(O) Pete(B-musical artist) Seeger(I-musical artist) ,(O) Bonnie(B-musical artist) Raitt(I-musical artist) ,(O) David(B-musical artist) Bromberg(I-musical artist) and(O) Jerry(B-musical artist) Jeff(I-musical artist) Walker(I-musical artist) .(O)"}}
{"id": "308", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "song", "musical artist", "award", "person", "event", "location", "band", "album", "musical instrument", "music genre", "country"], "instance": {"id": "308", "words": ["Singer", "Marcia", "Barrett", "(", "from", "Jamaica", ")", "joined", "the", "group", ",", "who", "brought", "in", "Liz", "Mitchell", ",", "a", "former", "member", "of", "the", "Les", "Humphries", "Singers", "and", "Boney", "M.", "was", "finalised", "."], "labels": ["O", "B-musical artist", "I-musical artist", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, song, musical artist, award, person, event, location, band, album, musical instrument, music genre, country and O.\nSentence: Singer Marcia Barrett ( from Jamaica ) joined the group , who brought in Liz Mitchell , a former member of the Les Humphries Singers and Boney M. was finalised .", "prompt_labels": "Singer(O) Marcia(B-musical artist) Barrett(I-musical artist) ((O) from(O) Jamaica(B-country) )(O) joined(O) the(O) group(O) ,(O) who(O) brought(O) in(O) Liz(B-musical artist) Mitchell(I-musical artist) ,(O) a(O) former(O) member(O) of(O) the(O) Les(B-band) Humphries(I-band) Singers(I-band) and(O) Boney(B-band) M.(I-band) was(O) finalised(O) .(O)"}}
{"id": "257", "dataset": "crossner_music", "split": "dev", "label_list": ["person", "music genre", "event", "album", "musical artist", "band", "song", "country", "location", "award", "musical instrument", "organization"], "instance": {"id": "257", "words": ["Schoolly", "D", "'s", "debut", "album", ",", "Schoolly", "D", ",", "and", "especially", "the", "song", "P.S.K.", "What", "Does", "It", "Mean", "?", ",", "would", "heavily", "influence", "not", "only", "Ice-T", ",", "but", "also", "Eazy-E", "and", "N.W.A", "(", "most", "notably", "in", "the", "song", "Boyz-n-the-Hood", ")", "as", "well", "as", "the", "Beastie", "Boys", "on", "their", "seminal", "hardcore", "hip", "hop", "inspired", "album", "Licensed", "to", "Ill", "(", "1986", ")", "."], "labels": ["B-musical artist", "I-musical artist", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "O", "B-musical artist", "O", "B-band", "O", "O", "O", "O", "O", "O", "B-song", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, music genre, event, album, musical artist, band, song, country, location, award, musical instrument, organization and O.\nSentence: Schoolly D 's debut album , Schoolly D , and especially the song P.S.K. What Does It Mean ? , would heavily influence not only Ice-T , but also Eazy-E and N.W.A ( most notably in the song Boyz-n-the-Hood ) as well as the Beastie Boys on their seminal hardcore hip hop inspired album Licensed to Ill ( 1986 ) .", "prompt_labels": "Schoolly(B-musical artist) D(I-musical artist) 's(O) debut(O) album(O) ,(O) Schoolly(B-album) D(I-album) ,(O) and(O) especially(O) the(O) song(O) P.S.K.(B-song) What(I-song) Does(I-song) It(I-song) Mean(I-song) ?(I-song) ,(O) would(O) heavily(O) influence(O) not(O) only(O) Ice-T(B-musical artist) ,(O) but(O) also(O) Eazy-E(B-musical artist) and(O) N.W.A(B-band) ((O) most(O) notably(O) in(O) the(O) song(O) Boyz-n-the-Hood(B-song) )(O) as(O) well(O) as(O) the(O) Beastie(B-band) Boys(I-band) on(O) their(O) seminal(O) hardcore(O) hip(B-music genre) hop(I-music genre) inspired(O) album(O) Licensed(B-album) to(I-album) Ill(I-album) ((O) 1986(O) )(O) .(O)"}}
{"id": "5", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "band", "person", "event", "music genre", "country", "location", "musical artist", "musical instrument", "organization", "song", "award"], "instance": {"id": "5", "words": ["His", "style", "incorporates", "elements", "of", "Rock", "music", ",", "blues", ",", "Soul", "music", ",", "R", "&", "B", ",", "funk", ",", "jazz", ",", "reggae", ",", "hard", "rock", ",", "Psychedelic", "rock", ",", "Pop", "music", ",", "Folk", "music", ",", "and", "ballads", "."], "labels": ["O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, band, person, event, music genre, country, location, musical artist, musical instrument, organization, song, award and O.\nSentence: His style incorporates elements of Rock music , blues , Soul music , R & B , funk , jazz , reggae , hard rock , Psychedelic rock , Pop music , Folk music , and ballads .", "prompt_labels": "His(O) style(O) incorporates(O) elements(O) of(O) Rock(B-music genre) music(I-music genre) ,(O) blues(B-music genre) ,(O) Soul(B-music genre) music(I-music genre) ,(O) R(B-music genre) &(I-music genre) B(I-music genre) ,(O) funk(B-music genre) ,(O) jazz(B-music genre) ,(O) reggae(B-music genre) ,(O) hard(B-music genre) rock(I-music genre) ,(O) Psychedelic(B-music genre) rock(I-music genre) ,(O) Pop(B-music genre) music(I-music genre) ,(O) Folk(B-music genre) music(I-music genre) ,(O) and(O) ballads(B-music genre) .(O)"}}
{"id": "356", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "award", "musical artist", "album", "country", "person", "song", "music genre", "organization", "band", "event", "location"], "instance": {"id": "356", "words": ["In", "Ireland", ",", "The", "Clancy", "Brothers", "and", "Tommy", "Makem", "(", "although", "its", "members", "were", "all", "Irish-born", ",", "the", "group", "became", "famous", "while", "based", "in", "New", "York", "'s", "Greenwich", "Village", ")", ",", "The", "Dubliners", ",", "Clannad", ",", "Planxty", ",", "The", "Chieftains", ",", "The", "Pogues", ",", "The", "Corrs", ",", "The", "Irish", "Rovers", ",", "and", "a", "variety", "of", "other", "folk", "bands", "have", "done", "much", "over", "the", "past", "few", "decades", "to", "revitalise", "and", "re-popularise", "Irish", "traditional", "music", "."], "labels": ["O", "B-country", "O", "B-band", "I-band", "I-band", "I-band", "I-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "I-location", "O", "O", "B-band", "I-band", "O", "B-band", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, award, musical artist, album, country, person, song, music genre, organization, band, event, location and O.\nSentence: In Ireland , The Clancy Brothers and Tommy Makem ( although its members were all Irish-born , the group became famous while based in New York 's Greenwich Village ) , The Dubliners , Clannad , Planxty , The Chieftains , The Pogues , The Corrs , The Irish Rovers , and a variety of other folk bands have done much over the past few decades to revitalise and re-popularise Irish traditional music .", "prompt_labels": "In(O) Ireland(B-country) ,(O) The(B-band) Clancy(I-band) Brothers(I-band) and(I-band) Tommy(I-band) Makem(I-band) ((O) although(O) its(O) members(O) were(O) all(O) Irish-born(O) ,(O) the(O) group(O) became(O) famous(O) while(O) based(O) in(O) New(B-location) York(I-location) 's(I-location) Greenwich(I-location) Village(I-location) )(O) ,(O) The(B-band) Dubliners(I-band) ,(O) Clannad(B-band) ,(O) Planxty(B-band) ,(O) The(B-band) Chieftains(I-band) ,(O) The(B-band) Pogues(I-band) ,(O) The(B-band) Corrs(I-band) ,(O) The(B-band) Irish(I-band) Rovers(I-band) ,(O) and(O) a(O) variety(O) of(O) other(O) folk(O) bands(O) have(O) done(O) much(O) over(O) the(O) past(O) few(O) decades(O) to(O) revitalise(O) and(O) re-popularise(O) Irish(B-music genre) traditional(I-music genre) music(I-music genre) .(O)"}}
{"id": "169", "dataset": "crossner_music", "split": "dev", "label_list": ["person", "musical instrument", "event", "award", "band", "country", "song", "album", "organization", "musical artist", "music genre", "location"], "instance": {"id": "169", "words": ["The", "total", "playing", "time", "covered", "three", "sides", "of", "an", "LP", ",", "so", "they", "decided", "to", "expand", "it", "into", "a", "double", "by", "including", "previously", "unreleased", "tracks", "from", "the", "sessions", "for", "the", "earlier", "albums", "Led", "Zeppelin", "III", ",", "Led", "Zeppelin", "IV", "and", "Houses", "of", "the", "Holy", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "B-album", "I-album", "I-album", "O", "B-album", "I-album", "I-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, musical instrument, event, award, band, country, song, album, organization, musical artist, music genre, location and O.\nSentence: The total playing time covered three sides of an LP , so they decided to expand it into a double by including previously unreleased tracks from the sessions for the earlier albums Led Zeppelin III , Led Zeppelin IV and Houses of the Holy .", "prompt_labels": "The(O) total(O) playing(O) time(O) covered(O) three(O) sides(O) of(O) an(O) LP(O) ,(O) so(O) they(O) decided(O) to(O) expand(O) it(O) into(O) a(O) double(O) by(O) including(O) previously(O) unreleased(O) tracks(O) from(O) the(O) sessions(O) for(O) the(O) earlier(O) albums(O) Led(B-album) Zeppelin(I-album) III(I-album) ,(O) Led(B-album) Zeppelin(I-album) IV(I-album) and(O) Houses(B-album) of(I-album) the(I-album) Holy(I-album) .(O)"}}
{"id": "200", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "musical artist", "country", "person", "award", "album", "band", "location", "music genre", "musical instrument", "organization", "song"], "instance": {"id": "200", "words": ["Brooks", "and", "Dunn", "have", "more", "Country", "Music", "Association", "awards", "and", "Academy", "of", "Country", "Music", "awards", "than", "any", "act", "in", "the", "history", "of", "country", "music", "."], "labels": ["B-band", "I-band", "I-band", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical artist, country, person, award, album, band, location, music genre, musical instrument, organization, song and O.\nSentence: Brooks and Dunn have more Country Music Association awards and Academy of Country Music awards than any act in the history of country music .", "prompt_labels": "Brooks(B-band) and(I-band) Dunn(I-band) have(O) more(O) Country(B-award) Music(I-award) Association(I-award) awards(I-award) and(O) Academy(B-award) of(I-award) Country(I-award) Music(I-award) awards(I-award) than(O) any(O) act(O) in(O) the(O) history(O) of(O) country(B-music genre) music(I-music genre) .(O)"}}
{"id": "83", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "person", "award", "location", "country", "organization", "musical artist", "song", "band", "album", "event", "musical instrument"], "instance": {"id": "83", "words": ["The", "film", "drew", "many", "of", "the", "biggest", "living", "influencers", "of", "the", "rhythm", "and", "blues", "genre", "together", ",", "such", "as", "Ray", "Charles", ",", "James", "Brown", ",", "Cab", "Calloway", ",", "Aretha", "Franklin", ",", "and", "John", "Lee", "Hooker", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, person, award, location, country, organization, musical artist, song, band, album, event, musical instrument and O.\nSentence: The film drew many of the biggest living influencers of the rhythm and blues genre together , such as Ray Charles , James Brown , Cab Calloway , Aretha Franklin , and John Lee Hooker .", "prompt_labels": "The(O) film(O) drew(O) many(O) of(O) the(O) biggest(O) living(O) influencers(O) of(O) the(O) rhythm(B-music genre) and(I-music genre) blues(I-music genre) genre(O) together(O) ,(O) such(O) as(O) Ray(B-musical artist) Charles(I-musical artist) ,(O) James(B-musical artist) Brown(I-musical artist) ,(O) Cab(B-musical artist) Calloway(I-musical artist) ,(O) Aretha(B-musical artist) Franklin(I-musical artist) ,(O) and(O) John(B-musical artist) Lee(I-musical artist) Hooker(I-musical artist) .(O)"}}
{"id": "231", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "album", "musical artist", "award", "event", "person", "song", "band", "location", "country", "organization", "musical instrument"], "instance": {"id": "231", "words": ["Soca", "is", "an", "offshoot", "of", "kaiso", "/", "Calypso", "music", ",", "with", "influences", "from", "Music", "of", "Latin", "America", ",", "Cadence", "rampa", ",", "funk", "and", "Soul", "music", "."], "labels": ["B-music genre", "O", "O", "O", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, album, musical artist, award, event, person, song, band, location, country, organization, musical instrument and O.\nSentence: Soca is an offshoot of kaiso / Calypso music , with influences from Music of Latin America , Cadence rampa , funk and Soul music .", "prompt_labels": "Soca(B-music genre) is(O) an(O) offshoot(O) of(O) kaiso(B-music genre) /(O) Calypso(B-music genre) music(I-music genre) ,(O) with(O) influences(O) from(O) Music(B-music genre) of(I-music genre) Latin(I-music genre) America(I-music genre) ,(O) Cadence(B-music genre) rampa(I-music genre) ,(O) funk(B-music genre) and(O) Soul(B-music genre) music(I-music genre) .(O)"}}
{"id": "243", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "musical instrument", "music genre", "country", "award", "organization", "location", "event", "band", "musical artist", "song", "person"], "instance": {"id": "243", "words": ["Gilmour", "has", "supported", "charities", "including", "Oxfam", ",", "the", "European", "Union", "Mental", "Health", "and", "Illness", "Association", ",", "Greenpeace", ",", "Amnesty", "International", ","], "labels": ["B-musical artist", "O", "O", "O", "O", "B-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, musical instrument, music genre, country, award, organization, location, event, band, musical artist, song, person and O.\nSentence: Gilmour has supported charities including Oxfam , the European Union Mental Health and Illness Association , Greenpeace , Amnesty International ,", "prompt_labels": "Gilmour(B-musical artist) has(O) supported(O) charities(O) including(O) Oxfam(B-organization) ,(O) the(O) European(B-organization) Union(I-organization) Mental(I-organization) Health(I-organization) and(I-organization) Illness(I-organization) Association(I-organization) ,(O) Greenpeace(B-organization) ,(O) Amnesty(B-organization) International(I-organization) ,(O)"}}
{"id": "23", "dataset": "crossner_music", "split": "dev", "label_list": ["person", "location", "album", "song", "band", "country", "award", "organization", "musical artist", "event", "musical instrument", "music genre"], "instance": {"id": "23", "words": ["She", "rose", "to", "stardom", "in", "the", "romantic", "comedy", "Roman", "Holiday", "(", "1953", ")", ",", "alongside", "Gregory", "Peck", ",", "for", "which", "she", "was", "the", "first", "actress", "to", "win", "an", "Academy", "Awards", ",", "a", "Golden", "Globe", "Awards", ",", "and", "a", "British", "Academy", "Film", "Awards", "for", "a", "single", "performance", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, album, song, band, country, award, organization, musical artist, event, musical instrument, music genre and O.\nSentence: She rose to stardom in the romantic comedy Roman Holiday ( 1953 ) , alongside Gregory Peck , for which she was the first actress to win an Academy Awards , a Golden Globe Awards , and a British Academy Film Awards for a single performance .", "prompt_labels": "She(O) rose(O) to(O) stardom(O) in(O) the(O) romantic(O) comedy(O) Roman(O) Holiday(O) ((O) 1953(O) )(O) ,(O) alongside(O) Gregory(B-person) Peck(I-person) ,(O) for(O) which(O) she(O) was(O) the(O) first(O) actress(O) to(O) win(O) an(O) Academy(B-award) Awards(I-award) ,(O) a(O) Golden(B-award) Globe(I-award) Awards(I-award) ,(O) and(O) a(O) British(B-award) Academy(I-award) Film(I-award) Awards(I-award) for(O) a(O) single(O) performance(O) .(O)"}}
{"id": "102", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "musical instrument", "song", "musical artist", "country", "event", "band", "music genre", "person", "award", "location", "organization"], "instance": {"id": "102", "words": ["Steve", "Young", ",", "David", "Allan", "Coe", ",", "John", "Prine", ",", "Billy", "Joe", "Shaver", ",", "Gary", "Stewart", ",", "Townes", "Van", "Zandt", ",", "Kris", "Kristofferson", ",", "Michael", "Martin", "Murphey", ",", "Tompall", "Glaser", ",", "Steve", "Earle", ",", "and", "the", "later", "career", "renaissance", "of", "Johnny", "Cash", ",", "along", "with", "a", "few", "female", "vocalists", "such", "as", "Jessi", "Colter", ",", "Sammi", "Smith", ",", "Tanya", "Tucker", ",", "and", "Rosanne", "Cash", "."], "labels": ["B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, musical instrument, song, musical artist, country, event, band, music genre, person, award, location, organization and O.\nSentence: Steve Young , David Allan Coe , John Prine , Billy Joe Shaver , Gary Stewart , Townes Van Zandt , Kris Kristofferson , Michael Martin Murphey , Tompall Glaser , Steve Earle , and the later career renaissance of Johnny Cash , along with a few female vocalists such as Jessi Colter , Sammi Smith , Tanya Tucker , and Rosanne Cash .", "prompt_labels": "Steve(B-musical artist) Young(I-musical artist) ,(O) David(B-musical artist) Allan(I-musical artist) Coe(I-musical artist) ,(O) John(B-musical artist) Prine(I-musical artist) ,(O) Billy(B-musical artist) Joe(I-musical artist) Shaver(I-musical artist) ,(O) Gary(B-musical artist) Stewart(I-musical artist) ,(O) Townes(B-musical artist) Van(I-musical artist) Zandt(I-musical artist) ,(O) Kris(B-musical artist) Kristofferson(I-musical artist) ,(O) Michael(B-musical artist) Martin(I-musical artist) Murphey(I-musical artist) ,(O) Tompall(B-musical artist) Glaser(I-musical artist) ,(O) Steve(B-musical artist) Earle(I-musical artist) ,(O) and(O) the(O) later(O) career(O) renaissance(O) of(O) Johnny(B-musical artist) Cash(I-musical artist) ,(O) along(O) with(O) a(O) few(O) female(O) vocalists(O) such(O) as(O) Jessi(B-musical artist) Colter(I-musical artist) ,(O) Sammi(B-musical artist) Smith(I-musical artist) ,(O) Tanya(B-musical artist) Tucker(I-musical artist) ,(O) and(O) Rosanne(B-musical artist) Cash(I-musical artist) .(O)"}}
{"id": "185", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "country", "person", "musical instrument", "song", "band", "organization", "location", "event", "album", "award", "musical artist"], "instance": {"id": "185", "words": ["Switchfoot", "has", "been", "involved", "in", "a", "number", "of", "humanitarian", "causes", ",", "including", "DATA", ",", "the", "ONE", "Campaign", ",", "the", "Keep", "A", "Breast", "Foundation", ",", "Habitat", "for", "Humanity", ",", "Invisible", "Children", ",", "and", "To", "Write", "Love", "on", "Her", "Arms", "."], "labels": ["B-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, country, person, musical instrument, song, band, organization, location, event, album, award, musical artist and O.\nSentence: Switchfoot has been involved in a number of humanitarian causes , including DATA , the ONE Campaign , the Keep A Breast Foundation , Habitat for Humanity , Invisible Children , and To Write Love on Her Arms .", "prompt_labels": "Switchfoot(B-band) has(O) been(O) involved(O) in(O) a(O) number(O) of(O) humanitarian(O) causes(O) ,(O) including(O) DATA(B-organization) ,(O) the(O) ONE(B-organization) Campaign(I-organization) ,(O) the(O) Keep(B-organization) A(I-organization) Breast(I-organization) Foundation(I-organization) ,(O) Habitat(B-organization) for(I-organization) Humanity(I-organization) ,(O) Invisible(B-organization) Children(I-organization) ,(O) and(O) To(B-organization) Write(I-organization) Love(I-organization) on(I-organization) Her(I-organization) Arms(I-organization) .(O)"}}
{"id": "166", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "country", "song", "award", "event", "organization", "location", "musical instrument", "musical artist", "band", "person", "music genre"], "instance": {"id": "166", "words": ["A", "critical", "and", "commercial", "success", "upon", "release", "and", "nominated", "for", "four", "Academy", "Awards", ",", "including", "for", "Academy", "Award", "for", "Best", "Picture", ",", "Academy", "Award", "for", "Best", "Actor", "(", "for", "De", "Niro", ")", "and", "Academy", "Award", "for", "Best", "Supporting", "Actress", "(", "for", "Foster", ")", ",", "Taxi", "Driver", "won", "the", "Palme", "d", "'Or", "at", "the", "1976", "Cannes", "Film", "Festival", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-person", "I-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, country, song, award, event, organization, location, musical instrument, musical artist, band, person, music genre and O.\nSentence: A critical and commercial success upon release and nominated for four Academy Awards , including for Academy Award for Best Picture , Academy Award for Best Actor ( for De Niro ) and Academy Award for Best Supporting Actress ( for Foster ) , Taxi Driver won the Palme d 'Or at the 1976 Cannes Film Festival .", "prompt_labels": "A(O) critical(O) and(O) commercial(O) success(O) upon(O) release(O) and(O) nominated(O) for(O) four(O) Academy(B-award) Awards(I-award) ,(O) including(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) ((O) for(O) De(B-person) Niro(I-person) )(O) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Supporting(I-award) Actress(I-award) ((O) for(O) Foster(B-person) )(O) ,(O) Taxi(O) Driver(O) won(O) the(O) Palme(B-award) d(I-award) 'Or(I-award) at(O) the(O) 1976(B-event) Cannes(I-event) Film(I-event) Festival(I-event) .(O)"}}
{"id": "180", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "album", "band", "award", "musical instrument", "organization", "person", "country", "event", "location", "song", "musical artist"], "instance": {"id": "180", "words": ["Alexander", "von", "Meilenwald", "from", "German", "band", "Nagelfar", "considers", "Ungod", "'", "s", "1993", "debut", "Circle", "of", "the", "Seven", "Infernal", "Pacts", ",", "Desaster", "'", "s", "1994", "demo", "Lost", "in", "the", "Ages", ",", "Tha-Norr", "'", "s", "1995", "album", "Wolfenzeitalter", ",", "Lunar", "Aurora", "'", "s", "1996", "debut", "Weltengänger", "and", "Katharsis", "'", "s", "2000", "debut", "666", "Alexander", "von", "Meilenwald", ":", "5", "Klassiker", "."], "labels": ["B-musical artist", "I-musical artist", "I-musical artist", "O", "B-country", "O", "B-band", "O", "B-band", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O", "B-band", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "B-band", "O", "O", "O", "O", "B-album", "O", "B-band", "I-band", "O", "O", "O", "O", "B-album", "O", "B-band", "O", "O", "O", "O", "B-album", "B-musical artist", "I-musical artist", "I-musical artist", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, album, band, award, musical instrument, organization, person, country, event, location, song, musical artist and O.\nSentence: Alexander von Meilenwald from German band Nagelfar considers Ungod ' s 1993 debut Circle of the Seven Infernal Pacts , Desaster ' s 1994 demo Lost in the Ages , Tha-Norr ' s 1995 album Wolfenzeitalter , Lunar Aurora ' s 1996 debut Weltengänger and Katharsis ' s 2000 debut 666 Alexander von Meilenwald : 5 Klassiker .", "prompt_labels": "Alexander(B-musical artist) von(I-musical artist) Meilenwald(I-musical artist) from(O) German(B-country) band(O) Nagelfar(B-band) considers(O) Ungod(B-band) '(O) s(O) 1993(O) debut(O) Circle(B-album) of(I-album) the(I-album) Seven(I-album) Infernal(I-album) Pacts(I-album) ,(O) Desaster(B-band) '(O) s(O) 1994(O) demo(O) Lost(B-album) in(I-album) the(I-album) Ages(I-album) ,(O) Tha-Norr(B-band) '(O) s(O) 1995(O) album(O) Wolfenzeitalter(B-album) ,(O) Lunar(B-band) Aurora(I-band) '(O) s(O) 1996(O) debut(O) Weltengänger(B-album) and(O) Katharsis(B-band) '(O) s(O) 2000(O) debut(O) 666(B-album) Alexander(B-musical artist) von(I-musical artist) Meilenwald(I-musical artist) :(O) 5(O) Klassiker(O) .(O)"}}
{"id": "38", "dataset": "crossner_music", "split": "dev", "label_list": ["person", "award", "band", "organization", "country", "event", "musical artist", "song", "music genre", "location", "album", "musical instrument"], "instance": {"id": "38", "words": ["Burton", "'s", "work", "on", "Sweeney", "Todd", "won", "the", "National", "Board", "of", "Review", "Award", "for", "Best", "Director", ",", "and", "won", "an", "Academy", "Awards", "for", "Academy", "Award", "for", "Best", "Production", "Design", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, band, organization, country, event, musical artist, song, music genre, location, album, musical instrument and O.\nSentence: Burton 's work on Sweeney Todd won the National Board of Review Award for Best Director , and won an Academy Awards for Academy Award for Best Production Design .", "prompt_labels": "Burton(B-person) 's(O) work(O) on(O) Sweeney(O) Todd(O) won(O) the(O) National(B-award) Board(I-award) of(I-award) Review(I-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) ,(O) and(O) won(O) an(O) Academy(B-award) Awards(I-award) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Production(I-award) Design(I-award) .(O)"}}
{"id": "35", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "song", "award", "musical instrument", "band", "music genre", "country", "musical artist", "event", "organization", "location", "person"], "instance": {"id": "35", "words": ["Construction", "commenced", "in", "1975", "and", "the", "venue", "opened", "ahead", "of", "the", "1978", "Commonwealth", "Games", "(", "hence", "its", "name", ")", ",", "replacing", "the", "adjacent", "Clarke", "Stadium", "as", "the", "Eskimos", "home", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, song, award, musical instrument, band, music genre, country, musical artist, event, organization, location, person and O.\nSentence: Construction commenced in 1975 and the venue opened ahead of the 1978 Commonwealth Games ( hence its name ) , replacing the adjacent Clarke Stadium as the Eskimos home .", "prompt_labels": "Construction(O) commenced(O) in(O) 1975(O) and(O) the(O) venue(O) opened(O) ahead(O) of(O) the(O) 1978(B-event) Commonwealth(I-event) Games(I-event) ((O) hence(O) its(O) name(O) )(O) ,(O) replacing(O) the(O) adjacent(O) Clarke(B-location) Stadium(I-location) as(O) the(O) Eskimos(O) home(O) .(O)"}}
{"id": "16", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "musical instrument", "country", "album", "song", "organization", "musical artist", "band", "person", "music genre", "award", "event"], "instance": {"id": "16", "words": ["Since", "the", "introduction", "of", "the", "50", "/", "50", "voting", "system", "in", "2009", ",", "the", "juries", "and", "the", "voters", "have", "disagreed", "on", "the", "winner", "on", "five", "occasions", ",", "in", "Eurovision", "Song", "Contest", "2011", ",", "Eurovision", "Song", "Contest", "2015", ",", "Eurovision", "Song", "Contest", "2016", ",", "Eurovision", "Song", "Contest", "2018", "and", "Eurovision", "Song", "Contest", "2019", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, musical instrument, country, album, song, organization, musical artist, band, person, music genre, award, event and O.\nSentence: Since the introduction of the 50 / 50 voting system in 2009 , the juries and the voters have disagreed on the winner on five occasions , in Eurovision Song Contest 2011 , Eurovision Song Contest 2015 , Eurovision Song Contest 2016 , Eurovision Song Contest 2018 and Eurovision Song Contest 2019 .", "prompt_labels": "Since(O) the(O) introduction(O) of(O) the(O) 50(O) /(O) 50(O) voting(O) system(O) in(O) 2009(O) ,(O) the(O) juries(O) and(O) the(O) voters(O) have(O) disagreed(O) on(O) the(O) winner(O) on(O) five(O) occasions(O) ,(O) in(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2011(I-event) ,(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2015(I-event) ,(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2016(I-event) ,(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2018(I-event) and(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2019(I-event) .(O)"}}
{"id": "3", "dataset": "crossner_music", "split": "dev", "label_list": ["award", "musical instrument", "event", "location", "country", "musical artist", "music genre", "organization", "song", "album", "band", "person"], "instance": {"id": "3", "words": ["He", "has", "also", "won", "three", "Grammy", "Awards", ",", "14", "Academy", "of", "Country", "Music", "awards", ",", "11", "Country", "Music", "Association", "(", "CMA", ")", "awards", ",", "10", "American", "Music", "Awards", ",", "and", "three", "People", "'s", "Choice", "Awards", "."], "labels": ["O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, musical instrument, event, location, country, musical artist, music genre, organization, song, album, band, person and O.\nSentence: He has also won three Grammy Awards , 14 Academy of Country Music awards , 11 Country Music Association ( CMA ) awards , 10 American Music Awards , and three People 's Choice Awards .", "prompt_labels": "He(O) has(O) also(O) won(O) three(O) Grammy(B-award) Awards(I-award) ,(O) 14(O) Academy(B-award) of(I-award) Country(I-award) Music(I-award) awards(I-award) ,(O) 11(O) Country(B-award) Music(I-award) Association(I-award) ((I-award) CMA(I-award) )(I-award) awards(I-award) ,(O) 10(O) American(B-award) Music(I-award) Awards(I-award) ,(O) and(O) three(O) People(B-award) 's(I-award) Choice(I-award) Awards(I-award) .(O)"}}
{"id": "302", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "organization", "album", "song", "location", "music genre", "award", "person", "musical artist", "band", "country", "musical instrument"], "instance": {"id": "302", "words": ["Notably", ",", "American", "figure", "skater", "Evan", "Lysacek", "used", "Scheherazade", "in", "his", "free", "skate", "and", "won", "the", "gold", "medal", "at", "Figure", "skating", "at", "the", "2010", "Winter", "Olympics", "in", "Vancouver", "."], "labels": ["O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-song", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, album, song, location, music genre, award, person, musical artist, band, country, musical instrument and O.\nSentence: Notably , American figure skater Evan Lysacek used Scheherazade in his free skate and won the gold medal at Figure skating at the 2010 Winter Olympics in Vancouver .", "prompt_labels": "Notably(O) ,(O) American(O) figure(O) skater(O) Evan(B-person) Lysacek(I-person) used(O) Scheherazade(B-song) in(O) his(O) free(O) skate(O) and(O) won(O) the(O) gold(O) medal(O) at(O) Figure(O) skating(O) at(O) the(O) 2010(B-event) Winter(I-event) Olympics(I-event) in(O) Vancouver(B-location) .(O)"}}
{"id": "312", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "organization", "event", "person", "country", "music genre", "song", "musical instrument", "award", "band", "musical artist", "location"], "instance": {"id": "312", "words": ["Pioneered", "by", "black-doom", "bands", "like", "Ophthalamia", ",", "Katatonia", ",", "Bethlehem", ",", "Forgotten", "Tomb", "and", "Shining", ",", "depressive", "suicidal", "black", "metal", ",", "also", "known", "as", "suicidal", "black", "metal", ",", "depressive", "black", "metal", "or", "DSBM", ",", "is", "a", "style", "that", "melds", "the", "second", "wave", "-style", "of", "black", "metal", "with", "doom", "metal", ","], "labels": ["O", "O", "B-music genre", "O", "O", "B-band", "O", "B-band", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "O", "B-music genre", "I-music genre", "I-music genre", "I-music genre", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, organization, event, person, country, music genre, song, musical instrument, award, band, musical artist, location and O.\nSentence: Pioneered by black-doom bands like Ophthalamia , Katatonia , Bethlehem , Forgotten Tomb and Shining , depressive suicidal black metal , also known as suicidal black metal , depressive black metal or DSBM , is a style that melds the second wave -style of black metal with doom metal ,", "prompt_labels": "Pioneered(O) by(O) black-doom(B-music genre) bands(O) like(O) Ophthalamia(B-band) ,(O) Katatonia(B-band) ,(O) Bethlehem(B-band) ,(O) Forgotten(B-band) Tomb(I-band) and(O) Shining(B-band) ,(O) depressive(B-music genre) suicidal(I-music genre) black(I-music genre) metal(I-music genre) ,(O) also(O) known(O) as(O) suicidal(B-music genre) black(I-music genre) metal(I-music genre) ,(O) depressive(B-music genre) black(I-music genre) metal(I-music genre) or(O) DSBM(B-music genre) ,(O) is(O) a(O) style(O) that(O) melds(O) the(O) second(O) wave(O) -style(O) of(O) black(B-music genre) metal(I-music genre) with(O) doom(B-music genre) metal(I-music genre) ,(O)"}}
{"id": "48", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "event", "location", "country", "music genre", "song", "person", "musical instrument", "musical artist", "organization", "award", "album"], "instance": {"id": "48", "words": ["All", "ten", "of", "Cannibal", "Corpse", "'s", "albums", ",", "the", "live", "album", "Live", "Cannibalism", ",", "the", "boxed", "set", "15", "Year", "Killing", "Spree", ",", "the", "EP", "Worm", "Infested", ",", "and", "the", "single", "Hammer", "Smashed", "Face", "were", "re-released", "in", "Australia", "between", "2006", "and", "2007", ",", "finally", "classified", "by", "ARIA", "and", "allowed", "for", "sale", "in", "Australia", "."], "labels": ["O", "O", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-song", "I-song", "I-song", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, event, location, country, music genre, song, person, musical instrument, musical artist, organization, award, album and O.\nSentence: All ten of Cannibal Corpse 's albums , the live album Live Cannibalism , the boxed set 15 Year Killing Spree , the EP Worm Infested , and the single Hammer Smashed Face were re-released in Australia between 2006 and 2007 , finally classified by ARIA and allowed for sale in Australia .", "prompt_labels": "All(O) ten(O) of(O) Cannibal(B-band) Corpse(I-band) 's(O) albums(O) ,(O) the(O) live(O) album(O) Live(B-album) Cannibalism(I-album) ,(O) the(O) boxed(O) set(O) 15(B-album) Year(I-album) Killing(I-album) Spree(I-album) ,(O) the(O) EP(O) Worm(B-album) Infested(I-album) ,(O) and(O) the(O) single(O) Hammer(B-song) Smashed(I-song) Face(I-song) were(O) re-released(O) in(O) Australia(B-country) between(O) 2006(O) and(O) 2007(O) ,(O) finally(O) classified(O) by(O) ARIA(B-organization) and(O) allowed(O) for(O) sale(O) in(O) Australia(B-country) .(O)"}}
{"id": "96", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "song", "person", "organization", "musical instrument", "musical artist", "band", "award", "album", "country", "event", "music genre"], "instance": {"id": "96", "words": ["These", "were", "finished", "by", "early", "1984", ",", "with", "all", "night", "games", "in", "Adelaide", "moving", "from", "the", "suburban", "grounds", "(", "Norwood", "Oval", "and", "Thebarton", "Oval", ")", "to", "league", "headquarters", "for", "the", "next", "16", "years", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, song, person, organization, musical instrument, musical artist, band, award, album, country, event, music genre and O.\nSentence: These were finished by early 1984 , with all night games in Adelaide moving from the suburban grounds ( Norwood Oval and Thebarton Oval ) to league headquarters for the next 16 years .", "prompt_labels": "These(O) were(O) finished(O) by(O) early(O) 1984(O) ,(O) with(O) all(O) night(O) games(O) in(O) Adelaide(B-location) moving(O) from(O) the(O) suburban(O) grounds(O) ((O) Norwood(B-location) Oval(I-location) and(O) Thebarton(B-location) Oval(I-location) )(O) to(O) league(O) headquarters(O) for(O) the(O) next(O) 16(O) years(O) .(O)"}}
{"id": "220", "dataset": "crossner_music", "split": "dev", "label_list": ["award", "country", "musical artist", "album", "location", "music genre", "organization", "band", "song", "musical instrument", "event", "person"], "instance": {"id": "220", "words": ["The", "instrument", "is", "played", "in", "Guinea", ",", "Guinea-Bissau", ",", "Mali", ",", "Senegal", ",", "Burkina", "Faso", "and", "the", "Gambia", "."], "labels": ["O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "I-country", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, country, musical artist, album, location, music genre, organization, band, song, musical instrument, event, person and O.\nSentence: The instrument is played in Guinea , Guinea-Bissau , Mali , Senegal , Burkina Faso and the Gambia .", "prompt_labels": "The(O) instrument(O) is(O) played(O) in(O) Guinea(B-country) ,(O) Guinea-Bissau(B-country) ,(O) Mali(B-country) ,(O) Senegal(B-country) ,(O) Burkina(B-country) Faso(I-country) and(O) the(B-country) Gambia(I-country) .(O)"}}
{"id": "168", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "location", "award", "band", "musical instrument", "organization", "person", "song", "musical artist", "album", "event", "music genre"], "instance": {"id": "168", "words": ["The", "album", "was", "certified", "gold", "by", "both", "British", "Phonographic", "Industry", "and", "the", "Recording", "Industry", "Association", "of", "America", "in", "December", "2007", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, award, band, musical instrument, organization, person, song, musical artist, album, event, music genre and O.\nSentence: The album was certified gold by both British Phonographic Industry and the Recording Industry Association of America in December 2007 .", "prompt_labels": "The(O) album(O) was(O) certified(O) gold(O) by(O) both(O) British(B-organization) Phonographic(I-organization) Industry(I-organization) and(O) the(O) Recording(B-organization) Industry(I-organization) Association(I-organization) of(I-organization) America(I-organization) in(O) December(O) 2007(O) .(O)"}}
{"id": "92", "dataset": "crossner_music", "split": "dev", "label_list": ["person", "music genre", "location", "musical artist", "country", "song", "band", "musical instrument", "album", "award", "event", "organization"], "instance": {"id": "92", "words": ["The", "5th", "Dimension", "is", "an", "United", "States", "popular", "music", "vocal", "group", ",", "whose", "repertoire", "includes", "Pop", "music", ",", "Rhythm", "and", "blues", ",", "Soul", "music", ",", "jazz", ",", "light", "opera", ",", "and", "Broadway", "-", "the", "melange", "was", "coined", "as", "Champagne", "Soul", "."], "labels": ["B-band", "I-band", "I-band", "O", "O", "B-country", "I-country", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, music genre, location, musical artist, country, song, band, musical instrument, album, award, event, organization and O.\nSentence: The 5th Dimension is an United States popular music vocal group , whose repertoire includes Pop music , Rhythm and blues , Soul music , jazz , light opera , and Broadway - the melange was coined as Champagne Soul .", "prompt_labels": "The(B-band) 5th(I-band) Dimension(I-band) is(O) an(O) United(B-country) States(I-country) popular(B-music genre) music(I-music genre) vocal(O) group(O) ,(O) whose(O) repertoire(O) includes(O) Pop(B-music genre) music(I-music genre) ,(O) Rhythm(B-music genre) and(I-music genre) blues(I-music genre) ,(O) Soul(B-music genre) music(I-music genre) ,(O) jazz(B-music genre) ,(O) light(B-music genre) opera(I-music genre) ,(O) and(O) Broadway(B-music genre) -(O) the(O) melange(O) was(O) coined(O) as(O) Champagne(B-music genre) Soul(I-music genre) .(O)"}}
{"id": "27", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "music genre", "song", "country", "organization", "award", "event", "musical artist", "person", "album", "musical instrument", "band"], "instance": {"id": "27", "words": ["Under", "the", "current", "voting", "system", ",", "in", "place", "since", "2016", ",", "the", "highest-scoring", "winner", "is", "Salvador", "Sobral", "of", "Portugal", "who", "won", "the", "Eurovision", "Song", "Contest", "2017", "in", "Kiev", ",", "Ukraine", ",", "with", "758", "points", ";", "under", "the", "previous", "system", ",", "the", "highest-scoring", "winner", "was", "Alexander", "Rybak", "of", "Norway", "with", "387", "points", "in", "Eurovision", "Song", "Contest", "2009", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-country", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "B-location", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-country", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, music genre, song, country, organization, award, event, musical artist, person, album, musical instrument, band and O.\nSentence: Under the current voting system , in place since 2016 , the highest-scoring winner is Salvador Sobral of Portugal who won the Eurovision Song Contest 2017 in Kiev , Ukraine , with 758 points ; under the previous system , the highest-scoring winner was Alexander Rybak of Norway with 387 points in Eurovision Song Contest 2009 .", "prompt_labels": "Under(O) the(O) current(O) voting(O) system(O) ,(O) in(O) place(O) since(O) 2016(O) ,(O) the(O) highest-scoring(O) winner(O) is(O) Salvador(B-musical artist) Sobral(I-musical artist) of(O) Portugal(B-country) who(O) won(O) the(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2017(I-event) in(O) Kiev(B-location) ,(O) Ukraine(B-country) ,(O) with(O) 758(O) points(O) ;(O) under(O) the(O) previous(O) system(O) ,(O) the(O) highest-scoring(O) winner(O) was(O) Alexander(B-musical artist) Rybak(I-musical artist) of(O) Norway(B-country) with(O) 387(O) points(O) in(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2009(I-event) .(O)"}}
{"id": "151", "dataset": "crossner_music", "split": "dev", "label_list": ["musical artist", "song", "country", "musical instrument", "person", "music genre", "event", "award", "band", "organization", "location", "album"], "instance": {"id": "151", "words": ["Drawing", "influences", "from", "hip", "hop", ",", "dub", ",", "pop", ",", "Gorillaz", "released", "their", "self-titled", "debut", "album", "in", "2001", "to", "worldwide", "success", ",", "spawning", "successful", "follow-ups", "Demon", "Days", "(", "2005", ")", ",", "Plastic", "Beach", ",", "The", "Fall", "(", "both", "released", "in", "2010", ")", ",", "Humanz", "(", "2017", ")", ",", "and", "The", "Now", "Now", "(", "2018", ")", "."], "labels": ["O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "O", "B-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "B-album", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, song, country, musical instrument, person, music genre, event, award, band, organization, location, album and O.\nSentence: Drawing influences from hip hop , dub , pop , Gorillaz released their self-titled debut album in 2001 to worldwide success , spawning successful follow-ups Demon Days ( 2005 ) , Plastic Beach , The Fall ( both released in 2010 ) , Humanz ( 2017 ) , and The Now Now ( 2018 ) .", "prompt_labels": "Drawing(O) influences(O) from(O) hip(B-music genre) hop(I-music genre) ,(O) dub(B-music genre) ,(O) pop(B-music genre) ,(O) Gorillaz(B-band) released(O) their(O) self-titled(O) debut(O) album(O) in(O) 2001(O) to(O) worldwide(O) success(O) ,(O) spawning(O) successful(O) follow-ups(O) Demon(B-album) Days(I-album) ((O) 2005(O) )(O) ,(O) Plastic(B-album) Beach(I-album) ,(O) The(B-album) Fall(I-album) ((O) both(O) released(O) in(O) 2010(O) )(O) ,(O) Humanz(B-album) ((O) 2017(O) )(O) ,(O) and(O) The(B-album) Now(I-album) Now(I-album) ((O) 2018(O) )(O) .(O)"}}
{"id": "329", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "band", "location", "person", "event", "organization", "country", "musical instrument", "album", "musical artist", "music genre", "award"], "instance": {"id": "329", "words": ["Jimmie", "Rodgers", ",", "Moon", "Mullican", ",", "Bob", "Wills", ",", "Bill", "Monroe", "and", "Hank", "Williams", "have", "all", "described", "themselves", "as", "blues", "singers", "and", "their", "music", "has", "a", "blues", "feel", "that", "is", "different", ",", "at", "first", "glance", "at", "least", ",", "from", "the", "later", "country", "pop", "of", "artists", "like", "Eddy", "Arnold", "."], "labels": ["B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, band, location, person, event, organization, country, musical instrument, album, musical artist, music genre, award and O.\nSentence: Jimmie Rodgers , Moon Mullican , Bob Wills , Bill Monroe and Hank Williams have all described themselves as blues singers and their music has a blues feel that is different , at first glance at least , from the later country pop of artists like Eddy Arnold .", "prompt_labels": "Jimmie(B-musical artist) Rodgers(I-musical artist) ,(O) Moon(B-musical artist) Mullican(I-musical artist) ,(O) Bob(B-musical artist) Wills(I-musical artist) ,(O) Bill(B-musical artist) Monroe(I-musical artist) and(O) Hank(B-musical artist) Williams(I-musical artist) have(O) all(O) described(O) themselves(O) as(O) blues(B-music genre) singers(O) and(O) their(O) music(O) has(O) a(O) blues(O) feel(O) that(O) is(O) different(O) ,(O) at(O) first(O) glance(O) at(O) least(O) ,(O) from(O) the(O) later(O) country(B-music genre) pop(I-music genre) of(O) artists(O) like(O) Eddy(B-musical artist) Arnold(I-musical artist) .(O)"}}
{"id": "233", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "organization", "location", "musical artist", "person", "band", "musical instrument", "event", "song", "award", "music genre", "album"], "instance": {"id": "233", "words": ["Imported", "styles", "of", "popular", "music", "with", "a", "distinctively", "Latin", "flavor", "include", "Latin", "jazz", ",", "Argentine", "and", "Uruguayan", "rock", "and", "Cuban", "and", "Mexican", "hip", "hop", ",", "all", "influenced", "by", "styles", "from", "the", "United", "States", "(", "jazz", ",", "rock", "and", "roll", "and", "hip", "hop", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "I-country", "O", "B-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, location, musical artist, person, band, musical instrument, event, song, award, music genre, album and O.\nSentence: Imported styles of popular music with a distinctively Latin flavor include Latin jazz , Argentine and Uruguayan rock and Cuban and Mexican hip hop , all influenced by styles from the United States ( jazz , rock and roll and hip hop ) .", "prompt_labels": "Imported(O) styles(O) of(O) popular(O) music(O) with(O) a(O) distinctively(O) Latin(B-location) flavor(O) include(O) Latin(B-music genre) jazz(I-music genre) ,(O) Argentine(O) and(O) Uruguayan(O) rock(B-music genre) and(O) Cuban(O) and(O) Mexican(O) hip(B-music genre) hop(I-music genre) ,(O) all(O) influenced(O) by(O) styles(O) from(O) the(B-country) United(I-country) States(I-country) ((O) jazz(B-music genre) ,(O) rock(B-music genre) and(I-music genre) roll(I-music genre) and(O) hip(B-music genre) hop(I-music genre) )(O) .(O)"}}
{"id": "214", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "location", "song", "person", "musical instrument", "musical artist", "award", "event", "music genre", "organization", "country", "band"], "instance": {"id": "214", "words": ["British", "psychedelic", "/", "progressive", "rock", "band", "Knifeworld", "features", "the", "bassoon", "playing", "of", "Chloe", "Herrington", ",", "who", "also", "plays", "for", "experimental", "Baroque", "pop", "orchestra", "Chrome", "Hoof", "."], "labels": ["B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-band", "O", "O", "B-musical instrument", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, location, song, person, musical instrument, musical artist, award, event, music genre, organization, country, band and O.\nSentence: British psychedelic / progressive rock band Knifeworld features the bassoon playing of Chloe Herrington , who also plays for experimental Baroque pop orchestra Chrome Hoof .", "prompt_labels": "British(B-music genre) psychedelic(I-music genre) /(O) progressive(B-music genre) rock(I-music genre) band(O) Knifeworld(B-band) features(O) the(O) bassoon(B-musical instrument) playing(O) of(O) Chloe(B-musical artist) Herrington(I-musical artist) ,(O) who(O) also(O) plays(O) for(O) experimental(O) Baroque(B-music genre) pop(I-music genre) orchestra(O) Chrome(B-band) Hoof(I-band) .(O)"}}
{"id": "52", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "event", "musical artist", "person", "organization", "location", "musical instrument", "award", "album", "band", "song", "country"], "instance": {"id": "52", "words": ["Originally", "based", "in", "West", "Germany", ",", "the", "four", "original", "members", "of", "the", "group", "'s", "official", "line-up", "were", "Liz", "Mitchell", "and", "Marcia", "Barrett", "from", "Jamaica", ",", "Maizie", "Williams", "from", "Montserrat", "and", "Bobby", "Farrell", ",", "a", "performing", "artist", "from", "Aruba", "."], "labels": ["O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-country", "O", "B-musical artist", "I-musical artist", "O", "B-location", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, event, musical artist, person, organization, location, musical instrument, award, album, band, song, country and O.\nSentence: Originally based in West Germany , the four original members of the group 's official line-up were Liz Mitchell and Marcia Barrett from Jamaica , Maizie Williams from Montserrat and Bobby Farrell , a performing artist from Aruba .", "prompt_labels": "Originally(O) based(O) in(O) West(B-country) Germany(I-country) ,(O) the(O) four(O) original(O) members(O) of(O) the(O) group(O) 's(O) official(O) line-up(O) were(O) Liz(B-musical artist) Mitchell(I-musical artist) and(O) Marcia(B-musical artist) Barrett(I-musical artist) from(O) Jamaica(B-country) ,(O) Maizie(B-musical artist) Williams(I-musical artist) from(O) Montserrat(B-location) and(O) Bobby(B-musical artist) Farrell(I-musical artist) ,(O) a(O) performing(O) artist(O) from(O) Aruba(B-country) .(O)"}}
{"id": "32", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "country", "award", "song", "organization", "location", "musical instrument", "event", "person", "album", "musical artist", "music genre"], "instance": {"id": "32", "words": ["Four", "singles", "-", "Until", "It", "Sleeps", ",", "Hero", "of", "the", "Day", ",", "Mama", "Said", ",", "and", "King", "Nothing", "-", "were", "released", "as", "part", "of", "the", "marketing", "campaign", "for", "the", "album", "."], "labels": ["O", "O", "O", "B-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "O", "O", "B-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, country, award, song, organization, location, musical instrument, event, person, album, musical artist, music genre and O.\nSentence: Four singles - Until It Sleeps , Hero of the Day , Mama Said , and King Nothing - were released as part of the marketing campaign for the album .", "prompt_labels": "Four(O) singles(O) -(O) Until(B-song) It(I-song) Sleeps(I-song) ,(O) Hero(B-song) of(I-song) the(I-song) Day(I-song) ,(O) Mama(B-song) Said(I-song) ,(O) and(O) King(B-song) Nothing(I-song) -(O) were(O) released(O) as(O) part(O) of(O) the(O) marketing(O) campaign(O) for(O) the(O) album(O) .(O)"}}
{"id": "349", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "event", "musical artist", "album", "organization", "location", "music genre", "band", "musical instrument", "person", "award", "country"], "instance": {"id": "349", "words": ["In", "the", "days", "following", "his", "death", ",", "tributes", "were", "paid", "by", "then-President", "George", "W.", "Bush", ",", "the", "United", "States", "House", "of", "Representatives", ",", "and", "many", "musicians", "and", "performers", ",", "including", "B.", "B.", "King", ",", "Ronnie", "Hawkins", ",", "Mick", "Jagger", ",", "Ronnie", "Wood", ",", "George", "Thorogood", ",", "Eric", "Clapton", ",", "Tom", "Petty", ",", "Robert", "Plant", ",", "Elvis", "Costello", ",", "Bonnie", "Raitt", ",", "Robert", "Randolph", "and", "the", "Family", "Band", "and", "Eric", "Burdon", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "I-band", "I-band", "I-band", "I-band", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, event, musical artist, album, organization, location, music genre, band, musical instrument, person, award, country and O.\nSentence: In the days following his death , tributes were paid by then-President George W. Bush , the United States House of Representatives , and many musicians and performers , including B. B. King , Ronnie Hawkins , Mick Jagger , Ronnie Wood , George Thorogood , Eric Clapton , Tom Petty , Robert Plant , Elvis Costello , Bonnie Raitt , Robert Randolph and the Family Band and Eric Burdon .", "prompt_labels": "In(O) the(O) days(O) following(O) his(O) death(O) ,(O) tributes(O) were(O) paid(O) by(O) then-President(O) George(B-person) W.(I-person) Bush(I-person) ,(O) the(O) United(O) States(O) House(O) of(O) Representatives(O) ,(O) and(O) many(O) musicians(O) and(O) performers(O) ,(O) including(O) B.(B-musical artist) B.(I-musical artist) King(I-musical artist) ,(O) Ronnie(B-musical artist) Hawkins(I-musical artist) ,(O) Mick(B-musical artist) Jagger(I-musical artist) ,(O) Ronnie(B-musical artist) Wood(I-musical artist) ,(O) George(B-musical artist) Thorogood(I-musical artist) ,(O) Eric(B-musical artist) Clapton(I-musical artist) ,(O) Tom(B-musical artist) Petty(I-musical artist) ,(O) Robert(B-musical artist) Plant(I-musical artist) ,(O) Elvis(B-musical artist) Costello(I-musical artist) ,(O) Bonnie(B-musical artist) Raitt(I-musical artist) ,(O) Robert(B-band) Randolph(I-band) and(I-band) the(I-band) Family(I-band) Band(I-band) and(O) Eric(B-musical artist) Burdon(I-musical artist) .(O)"}}
{"id": "333", "dataset": "crossner_music", "split": "dev", "label_list": ["award", "band", "musical instrument", "musical artist", "music genre", "album", "country", "organization", "event", "person", "location", "song"], "instance": {"id": "333", "words": ["It", "won", "Gabriel", "a", "Grammy", "Award", "for", "Grammy", "Award", "for", "Best", "New", "Age", "Album", "and", "a", "nomination", "for", "a", "Golden", "Globe", "for", "Golden", "Globe", "Award", "for", "Best", "Original", "Score", "."], "labels": ["O", "O", "B-musical artist", "O", "B-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, band, musical instrument, musical artist, music genre, album, country, organization, event, person, location, song and O.\nSentence: It won Gabriel a Grammy Award for Grammy Award for Best New Age Album and a nomination for a Golden Globe for Golden Globe Award for Best Original Score .", "prompt_labels": "It(O) won(O) Gabriel(B-musical artist) a(O) Grammy(B-award) Award(I-award) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) New(I-award) Age(I-award) Album(I-award) and(O) a(O) nomination(O) for(O) a(O) Golden(B-award) Globe(I-award) for(O) Golden(B-award) Globe(I-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Score(I-award) .(O)"}}
{"id": "306", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "organization", "musical instrument", "country", "award", "album", "event", "location", "music genre", "musical artist", "band", "person"], "instance": {"id": "306", "words": ["Gurney", "spent", "the", "last", "15", "years", "of", "his", "life", "in", "psychiatric", "hospital", "s", ",", "first", "for", "a", "short", "period", "at", "Barnwood", "House", "Hospital", "in", "Gloucester", ",", "and", "then", "at", "the", "Stone", "House", "Hospital", ",", "Dartford", ",", "where", "he", "was", "diagnosed", "as", "suffering", "from", "delusional", "insanity", "(", "systematised", ")", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, organization, musical instrument, country, award, album, event, location, music genre, musical artist, band, person and O.\nSentence: Gurney spent the last 15 years of his life in psychiatric hospital s , first for a short period at Barnwood House Hospital in Gloucester , and then at the Stone House Hospital , Dartford , where he was diagnosed as suffering from delusional insanity ( systematised ) .", "prompt_labels": "Gurney(B-musical artist) spent(O) the(O) last(O) 15(O) years(O) of(O) his(O) life(O) in(O) psychiatric(O) hospital(O) s(O) ,(O) first(O) for(O) a(O) short(O) period(O) at(O) Barnwood(B-location) House(I-location) Hospital(I-location) in(O) Gloucester(B-location) ,(O) and(O) then(O) at(O) the(O) Stone(B-location) House(I-location) Hospital(I-location) ,(O) Dartford(B-location) ,(O) where(O) he(O) was(O) diagnosed(O) as(O) suffering(O) from(O) delusional(O) insanity(O) ((O) systematised(O) )(O) .(O)"}}
{"id": "378", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "music genre", "album", "event", "organization", "person", "award", "country", "song", "band", "musical artist", "musical instrument"], "instance": {"id": "378", "words": ["The", "duo", "released", "the", "double", "platinum", ",", "The", "Jerky", "Boys", "2", "in", "1994", ",", "followed", "by", "The", "Jerky", "Boys", "3", "in", "1996", ",", "The", "Jerky", "Boys", "4", "in", "1997", ",", "Stop", "Staring", "at", "Me", "!", "in", "1999", ",", "and", "The", "Jerky", "Tapes", "in", "2001", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, music genre, album, event, organization, person, award, country, song, band, musical artist, musical instrument and O.\nSentence: The duo released the double platinum , The Jerky Boys 2 in 1994 , followed by The Jerky Boys 3 in 1996 , The Jerky Boys 4 in 1997 , Stop Staring at Me ! in 1999 , and The Jerky Tapes in 2001 .", "prompt_labels": "The(O) duo(O) released(O) the(O) double(O) platinum(O) ,(O) The(B-album) Jerky(I-album) Boys(I-album) 2(I-album) in(O) 1994(O) ,(O) followed(O) by(O) The(B-album) Jerky(I-album) Boys(I-album) 3(I-album) in(O) 1996(O) ,(O) The(B-album) Jerky(I-album) Boys(I-album) 4(I-album) in(O) 1997(O) ,(O) Stop(B-album) Staring(I-album) at(I-album) Me(I-album) !(I-album) in(O) 1999(O) ,(O) and(O) The(B-album) Jerky(I-album) Tapes(I-album) in(O) 2001(O) .(O)"}}
{"id": "266", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "musical instrument", "person", "musical artist", "organization", "location", "country", "music genre", "band", "award", "album", "song"], "instance": {"id": "266", "words": ["Country", "influences", "combined", "with", "Punk", "rock", "and", "alternative", "rock", "to", "forge", "the", "cowpunk", "scene", "in", "Southern", "California", "during", "the", "1980s", ",", "which", "included", "bands", "such", "as", "The", "Long", "Ryders", ",", "Lone", "Justice", "and", "The", "Beat", "Farmers", ",", "as", "well", "as", "the", "established", "punk", "group", "X", ",", "whose", "music", "had", "begun", "to", "include", "country", "and", "rockabilly", "influences", "."], "labels": ["B-music genre", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O", "B-music genre", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "B-band", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "B-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical instrument, person, musical artist, organization, location, country, music genre, band, award, album, song and O.\nSentence: Country influences combined with Punk rock and alternative rock to forge the cowpunk scene in Southern California during the 1980s , which included bands such as The Long Ryders , Lone Justice and The Beat Farmers , as well as the established punk group X , whose music had begun to include country and rockabilly influences .", "prompt_labels": "Country(B-music genre) influences(O) combined(O) with(O) Punk(B-music genre) rock(I-music genre) and(O) alternative(B-music genre) rock(I-music genre) to(O) forge(O) the(O) cowpunk(B-music genre) scene(O) in(O) Southern(B-location) California(I-location) during(O) the(O) 1980s(O) ,(O) which(O) included(O) bands(O) such(O) as(O) The(B-band) Long(I-band) Ryders(I-band) ,(O) Lone(B-band) Justice(I-band) and(O) The(B-band) Beat(I-band) Farmers(I-band) ,(O) as(O) well(O) as(O) the(O) established(O) punk(B-music genre) group(O) X(B-band) ,(O) whose(O) music(O) had(O) begun(O) to(O) include(O) country(B-music genre) and(O) rockabilly(B-music genre) influences(O) .(O)"}}
{"id": "297", "dataset": "crossner_music", "split": "dev", "label_list": ["award", "location", "album", "organization", "band", "musical instrument", "person", "musical artist", "song", "music genre", "country", "event"], "instance": {"id": "297", "words": ["With", "the", "continued", "success", "of", "Backstreet", "Boys", "and", "NSYNC", ",", "American", "and", "British", "groups", "like", "98", "Degrees", ",", "Westlife", ",", "O-Town", ",", "A1", ",", "Blue", ",", "and", "Busted", "gained", "quick", "popularity", "both", "domestically", "and", "internationally", "."], "labels": ["O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "O", "B-band", "O", "B-band", "O", "B-band", "O", "O", "B-band", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, album, organization, band, musical instrument, person, musical artist, song, music genre, country, event and O.\nSentence: With the continued success of Backstreet Boys and NSYNC , American and British groups like 98 Degrees , Westlife , O-Town , A1 , Blue , and Busted gained quick popularity both domestically and internationally .", "prompt_labels": "With(O) the(O) continued(O) success(O) of(O) Backstreet(B-band) Boys(I-band) and(O) NSYNC(B-band) ,(O) American(O) and(O) British(O) groups(O) like(O) 98(B-band) Degrees(I-band) ,(O) Westlife(B-band) ,(O) O-Town(B-band) ,(O) A1(B-band) ,(O) Blue(B-band) ,(O) and(O) Busted(B-band) gained(O) quick(O) popularity(O) both(O) domestically(O) and(O) internationally(O) .(O)"}}
{"id": "205", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "country", "band", "song", "music genre", "musical instrument", "album", "location", "musical artist", "award", "person", "organization"], "instance": {"id": "205", "words": ["The", "elements", "of", "Cali-Style", "Salsa", "were", "strongly", "influenced", "by", "dances", "to", "Caribbean", "rhythms", "which", "preceded", "salsa", ",", "such", "as", "Pachanga", "and", "Boogaloo", "."], "labels": ["O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "O", "O", "O", "B-music genre", "O", "B-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, band, song, music genre, musical instrument, album, location, musical artist, award, person, organization and O.\nSentence: The elements of Cali-Style Salsa were strongly influenced by dances to Caribbean rhythms which preceded salsa , such as Pachanga and Boogaloo .", "prompt_labels": "The(O) elements(O) of(O) Cali-Style(B-music genre) Salsa(I-music genre) were(O) strongly(O) influenced(O) by(O) dances(O) to(O) Caribbean(B-music genre) rhythms(I-music genre) which(O) preceded(O) salsa(B-music genre) ,(O) such(O) as(O) Pachanga(B-music genre) and(O) Boogaloo(B-music genre) .(O)"}}
{"id": "45", "dataset": "crossner_music", "split": "dev", "label_list": ["person", "music genre", "album", "band", "location", "event", "country", "award", "musical artist", "musical instrument", "song", "organization"], "instance": {"id": "45", "words": ["The", "venue", "hosted", "Badminton", "at", "the", "2012", "Summer", "Olympics", "and", "Gymnastics", "at", "the", "2012", "Summer", "Olympics", "at", "the", "2012", "Summer", "Olympics", "."], "labels": ["O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, music genre, album, band, location, event, country, award, musical artist, musical instrument, song, organization and O.\nSentence: The venue hosted Badminton at the 2012 Summer Olympics and Gymnastics at the 2012 Summer Olympics at the 2012 Summer Olympics .", "prompt_labels": "The(O) venue(O) hosted(O) Badminton(B-event) at(I-event) the(I-event) 2012(I-event) Summer(I-event) Olympics(I-event) and(O) Gymnastics(B-event) at(I-event) the(I-event) 2012(I-event) Summer(I-event) Olympics(I-event) at(O) the(O) 2012(B-event) Summer(I-event) Olympics(I-event) .(O)"}}
{"id": "182", "dataset": "crossner_music", "split": "dev", "label_list": ["person", "music genre", "organization", "event", "musical instrument", "album", "song", "band", "musical artist", "award", "location", "country"], "instance": {"id": "182", "words": ["In", "the", "early", "1990s", ",", "the", "rise", "of", "melodic", "death", "metal", "was", "recognized", ",", "with", "Swedish", "bands", "such", "as", "Dark", "Tranquillity", ",", "At", "the", "Gates", ",", "and", "In", "Flames", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, music genre, organization, event, musical instrument, album, song, band, musical artist, award, location, country and O.\nSentence: In the early 1990s , the rise of melodic death metal was recognized , with Swedish bands such as Dark Tranquillity , At the Gates , and In Flames .", "prompt_labels": "In(O) the(O) early(O) 1990s(O) ,(O) the(O) rise(O) of(O) melodic(B-music genre) death(I-music genre) metal(I-music genre) was(O) recognized(O) ,(O) with(O) Swedish(O) bands(O) such(O) as(O) Dark(B-band) Tranquillity(I-band) ,(O) At(B-band) the(I-band) Gates(I-band) ,(O) and(O) In(B-band) Flames(I-band) .(O)"}}
{"id": "44", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "musical instrument", "musical artist", "band", "country", "award", "person", "event", "organization", "song", "location", "music genre"], "instance": {"id": "44", "words": ["There", "are", "many", "kinds", "of", "bubens", ",", "including", "def", ",", "daf", ",", "or", "qaval", "(", "Azerbaijan", ")", ",", "daf", "or", "khaval", "(", "Armenia", ")", ",", "daira", "(", "Georgia", ")", ",", "doira", "(", "Uzbekistan", "and", "Tajikistan", ")", ",", "daire", "or", "def", "(", "Iran", ")", ",", "bendeir", "(", "Arab", "countries", ")", ",", "pandero", "(", "Spain", ")", "."], "labels": ["O", "O", "O", "O", "O", "B-musical instrument", "O", "O", "B-musical instrument", "O", "B-musical instrument", "O", "O", "B-musical instrument", "O", "B-country", "O", "O", "B-musical instrument", "O", "B-musical instrument", "O", "B-country", "O", "O", "B-musical instrument", "O", "B-country", "O", "O", "B-musical instrument", "O", "B-country", "O", "B-country", "O", "O", "B-musical instrument", "O", "B-musical instrument", "O", "B-country", "O", "O", "B-musical instrument", "O", "O", "O", "O", "O", "B-musical instrument", "O", "B-country", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, musical instrument, musical artist, band, country, award, person, event, organization, song, location, music genre and O.\nSentence: There are many kinds of bubens , including def , daf , or qaval ( Azerbaijan ) , daf or khaval ( Armenia ) , daira ( Georgia ) , doira ( Uzbekistan and Tajikistan ) , daire or def ( Iran ) , bendeir ( Arab countries ) , pandero ( Spain ) .", "prompt_labels": "There(O) are(O) many(O) kinds(O) of(O) bubens(B-musical instrument) ,(O) including(O) def(B-musical instrument) ,(O) daf(B-musical instrument) ,(O) or(O) qaval(B-musical instrument) ((O) Azerbaijan(B-country) )(O) ,(O) daf(B-musical instrument) or(O) khaval(B-musical instrument) ((O) Armenia(B-country) )(O) ,(O) daira(B-musical instrument) ((O) Georgia(B-country) )(O) ,(O) doira(B-musical instrument) ((O) Uzbekistan(B-country) and(O) Tajikistan(B-country) )(O) ,(O) daire(B-musical instrument) or(O) def(B-musical instrument) ((O) Iran(B-country) )(O) ,(O) bendeir(B-musical instrument) ((O) Arab(O) countries(O) )(O) ,(O) pandero(B-musical instrument) ((O) Spain(B-country) )(O) .(O)"}}
{"id": "328", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "country", "musical instrument", "location", "award", "organization", "band", "music genre", "song", "person", "album", "musical artist"], "instance": {"id": "328", "words": ["The", "album", "downplayed", "the", "disco", "style", "and", "was", "inspired", "by", "1980s", "artists", "such", "as", "Scritti", "Politti", ",", "The", "Human", "League", ",", "Adam", "and", "the", "Ants", "and", "Prince", ",", "blending", "their", "styles", "with", "elements", "of", "hip", "hop", "."], "labels": ["O", "O", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-band", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, musical instrument, location, award, organization, band, music genre, song, person, album, musical artist and O.\nSentence: The album downplayed the disco style and was inspired by 1980s artists such as Scritti Politti , The Human League , Adam and the Ants and Prince , blending their styles with elements of hip hop .", "prompt_labels": "The(O) album(O) downplayed(O) the(O) disco(B-music genre) style(O) and(O) was(O) inspired(O) by(O) 1980s(O) artists(O) such(O) as(O) Scritti(B-band) Politti(I-band) ,(O) The(B-band) Human(I-band) League(I-band) ,(O) Adam(B-band) and(I-band) the(I-band) Ants(I-band) and(O) Prince(B-band) ,(O) blending(O) their(O) styles(O) with(O) elements(O) of(O) hip(B-music genre) hop(I-music genre) .(O)"}}
{"id": "156", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "event", "album", "organization", "music genre", "location", "musical artist", "person", "musical instrument", "country", "band", "award"], "instance": {"id": "156", "words": ["His", "best-known", "songs", "include", "I", "Ain", "'t", "Marching", "Anymore", ",", "Changes", ",", "Crucifixion", ",", "Draft", "Dodger", "Rag", ",", "Love", "Me", ",", "I", "'m", "a", "Liberal", ",", "Outside", "of", "a", "Small", "Circle", "of", "Friends", ",", "Power", "and", "the", "Glory", ",", "There", "but", "for", "Fortune", ",", "and", "The", "War", "Is", "Over", "."], "labels": ["O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O", "B-song", "O", "B-song", "O", "B-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "B-song", "I-song", "I-song", "I-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, event, album, organization, music genre, location, musical artist, person, musical instrument, country, band, award and O.\nSentence: His best-known songs include I Ain 't Marching Anymore , Changes , Crucifixion , Draft Dodger Rag , Love Me , I 'm a Liberal , Outside of a Small Circle of Friends , Power and the Glory , There but for Fortune , and The War Is Over .", "prompt_labels": "His(O) best-known(O) songs(O) include(O) I(B-song) Ain(I-song) 't(I-song) Marching(I-song) Anymore(I-song) ,(O) Changes(B-song) ,(O) Crucifixion(B-song) ,(O) Draft(B-song) Dodger(I-song) Rag(I-song) ,(O) Love(B-song) Me(I-song) ,(I-song) I(I-song) 'm(I-song) a(I-song) Liberal(I-song) ,(O) Outside(B-song) of(I-song) a(I-song) Small(I-song) Circle(I-song) of(I-song) Friends(I-song) ,(O) Power(B-song) and(I-song) the(I-song) Glory(I-song) ,(O) There(B-song) but(I-song) for(I-song) Fortune(I-song) ,(O) and(O) The(B-song) War(I-song) Is(I-song) Over(I-song) .(O)"}}
{"id": "358", "dataset": "crossner_music", "split": "dev", "label_list": ["musical artist", "musical instrument", "person", "event", "band", "music genre", "country", "award", "location", "song", "album", "organization"], "instance": {"id": "358", "words": ["The", "Elephant", "6", "Recording", "Company", "(", "or", "simply", "Elephant", "6", ")", "is", "a", "collective", "of", "American", "musicians", "that", "spawned", "many", "notable", "independent", "bands", "of", "the", "1990s", ",", "including", "the", "Apples", "in", "Stereo", ",", "the", "Olivia", "Tremor", "Control", ",", "Neutral", "Milk", "Hotel", ",", "Beulah", ",", "Elf", "Power", ",", "of", "Montreal", ",", "The", "Minders", "and", "Circulatory", "System", "."], "labels": ["B-band", "I-band", "I-band", "I-band", "I-band", "O", "O", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, musical instrument, person, event, band, music genre, country, award, location, song, album, organization and O.\nSentence: The Elephant 6 Recording Company ( or simply Elephant 6 ) is a collective of American musicians that spawned many notable independent bands of the 1990s , including the Apples in Stereo , the Olivia Tremor Control , Neutral Milk Hotel , Beulah , Elf Power , of Montreal , The Minders and Circulatory System .", "prompt_labels": "The(B-band) Elephant(I-band) 6(I-band) Recording(I-band) Company(I-band) ((O) or(O) simply(O) Elephant(B-band) 6(I-band) )(O) is(O) a(O) collective(O) of(O) American(O) musicians(O) that(O) spawned(O) many(O) notable(O) independent(O) bands(O) of(O) the(O) 1990s(O) ,(O) including(O) the(B-band) Apples(I-band) in(I-band) Stereo(I-band) ,(O) the(B-band) Olivia(I-band) Tremor(I-band) Control(I-band) ,(O) Neutral(B-band) Milk(I-band) Hotel(I-band) ,(O) Beulah(B-band) ,(O) Elf(B-band) Power(I-band) ,(O) of(B-band) Montreal(I-band) ,(O) The(B-band) Minders(I-band) and(O) Circulatory(B-band) System(I-band) .(O)"}}
{"id": "21", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "album", "music genre", "location", "musical artist", "song", "person", "country", "event", "organization", "musical instrument", "award"], "instance": {"id": "21", "words": ["Western", "music", "artists", "such", "as", "Michael", "Martin", "Murphey", ",", "and", "artists", "within", "the", "aforementioned", "styles", "and", "genres", ",", "have", "seen", "continued", "success", "throughout", "their", "respective", "fields", ",", "including", "the", "likes", "of", "The", "Great", "Divide", ",", "Lorenzo", "Antonio", ",", "Sparx", ",", "Pat", "Green", ",", "and", "Jack", "Ingram", "."], "labels": ["B-music genre", "I-music genre", "O", "O", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "O", "B-musical artist", "I-musical artist", "O", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, album, music genre, location, musical artist, song, person, country, event, organization, musical instrument, award and O.\nSentence: Western music artists such as Michael Martin Murphey , and artists within the aforementioned styles and genres , have seen continued success throughout their respective fields , including the likes of The Great Divide , Lorenzo Antonio , Sparx , Pat Green , and Jack Ingram .", "prompt_labels": "Western(B-music genre) music(I-music genre) artists(O) such(O) as(O) Michael(B-musical artist) Martin(I-musical artist) Murphey(I-musical artist) ,(O) and(O) artists(O) within(O) the(O) aforementioned(O) styles(O) and(O) genres(O) ,(O) have(O) seen(O) continued(O) success(O) throughout(O) their(O) respective(O) fields(O) ,(O) including(O) the(O) likes(O) of(O) The(B-band) Great(I-band) Divide(I-band) ,(O) Lorenzo(B-musical artist) Antonio(I-musical artist) ,(O) Sparx(B-band) ,(O) Pat(B-musical artist) Green(I-musical artist) ,(O) and(O) Jack(B-musical artist) Ingram(I-musical artist) .(O)"}}
{"id": "325", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "award", "musical instrument", "band", "person", "song", "musical artist", "music genre", "album", "country", "location", "event"], "instance": {"id": "325", "words": ["Stock", "Aitken", "Waterman", "(", "SAW", ")", "expensively-produced", "productions", "for", "Mel", "and", "Kim", ",", "including", "the", "number-one", "hit", "Respectable", ",", "added", "elements", "of", "house", "to", "their", "previous", "Europop", "sound", "."], "labels": ["B-band", "I-band", "I-band", "O", "B-band", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "O", "O", "O", "O", "B-song", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "B-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, musical instrument, band, person, song, musical artist, music genre, album, country, location, event and O.\nSentence: Stock Aitken Waterman ( SAW ) expensively-produced productions for Mel and Kim , including the number-one hit Respectable , added elements of house to their previous Europop sound .", "prompt_labels": "Stock(B-band) Aitken(I-band) Waterman(I-band) ((O) SAW(B-band) )(O) expensively-produced(O) productions(O) for(O) Mel(B-band) and(I-band) Kim(I-band) ,(O) including(O) the(O) number-one(O) hit(O) Respectable(B-song) ,(O) added(O) elements(O) of(O) house(B-music genre) to(O) their(O) previous(O) Europop(B-music genre) sound(O) .(O)"}}
{"id": "204", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "person", "song", "award", "country", "event", "musical artist", "organization", "location", "musical instrument", "music genre", "album"], "instance": {"id": "204", "words": ["The", "music", "of", "Honduras", "varies", "from", "Punta", "and", "Paranda", "(", "the", "local", "genre", "of", "the", "Garifunas", ")", "to", "Caribbean", "music", "such", "as", "Salsa", "music", ",", "Merengue", "music", ",", "reggae", "and", "reggaeton", "(", "all", "widely", "heard", ",", "especially", "in", "the", "north", ")", "."], "labels": ["O", "O", "O", "B-country", "O", "O", "B-music genre", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, person, song, award, country, event, musical artist, organization, location, musical instrument, music genre, album and O.\nSentence: The music of Honduras varies from Punta and Paranda ( the local genre of the Garifunas ) to Caribbean music such as Salsa music , Merengue music , reggae and reggaeton ( all widely heard , especially in the north ) .", "prompt_labels": "The(O) music(O) of(O) Honduras(B-country) varies(O) from(O) Punta(B-music genre) and(O) Paranda(B-music genre) ((O) the(O) local(O) genre(O) of(O) the(O) Garifunas(O) )(O) to(O) Caribbean(B-music genre) music(I-music genre) such(O) as(O) Salsa(B-music genre) music(I-music genre) ,(O) Merengue(B-music genre) music(I-music genre) ,(O) reggae(B-music genre) and(O) reggaeton(B-music genre) ((O) all(O) widely(O) heard(O) ,(O) especially(O) in(O) the(O) north(O) )(O) .(O)"}}
{"id": "282", "dataset": "crossner_music", "split": "dev", "label_list": ["award", "band", "music genre", "musical instrument", "country", "event", "organization", "musical artist", "person", "song", "album", "location"], "instance": {"id": "282", "words": ["Beginning", "after", "World", "War", "I", "through", "the", "1970s", ",", "corps", "and", "competitions", "were", "often", "sponsored", "by", "the", "Veterans", "of", "Foreign", "Wars", ",", "Scout", "troop", "s", ",", "churches", ",", "the", "Royal", "Canadian", "Legion", ",", "and", "the", "American", "Legion", "."], "labels": ["O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, band, music genre, musical instrument, country, event, organization, musical artist, person, song, album, location and O.\nSentence: Beginning after World War I through the 1970s , corps and competitions were often sponsored by the Veterans of Foreign Wars , Scout troop s , churches , the Royal Canadian Legion , and the American Legion .", "prompt_labels": "Beginning(O) after(O) World(B-event) War(I-event) I(I-event) through(O) the(O) 1970s(O) ,(O) corps(O) and(O) competitions(O) were(O) often(O) sponsored(O) by(O) the(O) Veterans(B-organization) of(I-organization) Foreign(I-organization) Wars(I-organization) ,(O) Scout(B-organization) troop(I-organization) s(O) ,(O) churches(O) ,(O) the(O) Royal(B-organization) Canadian(I-organization) Legion(I-organization) ,(O) and(O) the(O) American(B-organization) Legion(I-organization) .(O)"}}
{"id": "196", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "award", "musical instrument", "album", "musical artist", "country", "event", "song", "location", "music genre", "band", "person"], "instance": {"id": "196", "words": ["In", "July", "2010", ",", "Dayne", "released", "Facing", "a", "Miracle", ",", "the", "official", "theme", "song", "to", "the", "2010", "Gay", "Games", "."], "labels": ["O", "O", "O", "O", "B-musical artist", "O", "B-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, musical instrument, album, musical artist, country, event, song, location, music genre, band, person and O.\nSentence: In July 2010 , Dayne released Facing a Miracle , the official theme song to the 2010 Gay Games .", "prompt_labels": "In(O) July(O) 2010(O) ,(O) Dayne(B-musical artist) released(O) Facing(B-song) a(I-song) Miracle(I-song) ,(O) the(O) official(O) theme(O) song(O) to(O) the(O) 2010(B-event) Gay(I-event) Games(I-event) .(O)"}}
{"id": "194", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "country", "location", "musical artist", "album", "song", "event", "award", "band", "person", "musical instrument", "music genre"], "instance": {"id": "194", "words": ["He", "was", "the", "guitarist", "for", "the", "1980s", "Hi-NRG", ",", "Synth-pop", "band", ",", "Frankie", "Goes", "to", "Hollywood", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "B-music genre", "O", "O", "B-band", "I-band", "I-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, location, musical artist, album, song, event, award, band, person, musical instrument, music genre and O.\nSentence: He was the guitarist for the 1980s Hi-NRG , Synth-pop band , Frankie Goes to Hollywood .", "prompt_labels": "He(O) was(O) the(O) guitarist(O) for(O) the(O) 1980s(O) Hi-NRG(B-music genre) ,(O) Synth-pop(B-music genre) band(O) ,(O) Frankie(B-band) Goes(I-band) to(I-band) Hollywood(I-band) .(O)"}}
{"id": "146", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "music genre", "organization", "location", "award", "band", "album", "song", "musical instrument", "person", "country", "musical artist"], "instance": {"id": "146", "words": ["Major", "artists", "who", "have", "been", "influenced", "by", "Costello", "include", "Green", "Day", ",", "Prince", ",", "Billy", "Bragg", ",", "Goldfinger", ",", "the", "Pogues", ",", "Radiohead", ",", "Dexys", "Midnight", "Runners", ",", "Pulp", ",", "Crowded", "House", ",", "James", ",", "Suzanne", "Vega", ",", "Less", "than", "Jake", ",", "and", "Foo", "Fighters", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-musical artist", "O", "B-band", "I-band", "O", "B-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "I-band", "O", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, music genre, organization, location, award, band, album, song, musical instrument, person, country, musical artist and O.\nSentence: Major artists who have been influenced by Costello include Green Day , Prince , Billy Bragg , Goldfinger , the Pogues , Radiohead , Dexys Midnight Runners , Pulp , Crowded House , James , Suzanne Vega , Less than Jake , and Foo Fighters .", "prompt_labels": "Major(O) artists(O) who(O) have(O) been(O) influenced(O) by(O) Costello(B-musical artist) include(O) Green(B-band) Day(I-band) ,(O) Prince(B-band) ,(O) Billy(B-musical artist) Bragg(I-musical artist) ,(O) Goldfinger(B-band) ,(O) the(B-band) Pogues(I-band) ,(O) Radiohead(B-band) ,(O) Dexys(B-band) Midnight(I-band) Runners(I-band) ,(O) Pulp(B-band) ,(O) Crowded(B-band) House(I-band) ,(O) James(B-band) ,(O) Suzanne(B-musical artist) Vega(I-musical artist) ,(O) Less(B-band) than(I-band) Jake(I-band) ,(O) and(O) Foo(B-band) Fighters(I-band) .(O)"}}
{"id": "51", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "song", "band", "organization", "country", "music genre", "album", "musical artist", "award", "location", "event", "person"], "instance": {"id": "51", "words": ["This", "album", "featured", "vocal", "contributions", "by", "Vicotnik", "of", "Ved", "Buens", "Ende", "and", "Dødheimsgard", "and", "Aldrahn", "of", "Dødheimsgard", "and", "Zyklon-B", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-musical artist", "O", "B-band", "I-band", "I-band", "O", "B-band", "O", "B-musical artist", "O", "B-band", "O", "B-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, song, band, organization, country, music genre, album, musical artist, award, location, event, person and O.\nSentence: This album featured vocal contributions by Vicotnik of Ved Buens Ende and Dødheimsgard and Aldrahn of Dødheimsgard and Zyklon-B .", "prompt_labels": "This(O) album(O) featured(O) vocal(O) contributions(O) by(O) Vicotnik(B-musical artist) of(O) Ved(B-band) Buens(I-band) Ende(I-band) and(O) Dødheimsgard(B-band) and(O) Aldrahn(B-musical artist) of(O) Dødheimsgard(B-band) and(O) Zyklon-B(B-band) .(O)"}}
{"id": "473", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "country", "election", "person", "politician", "organization", "political party", "event"], "instance": {"id": "473", "words": ["During", "the", "Western", "Desert", "Campaign", "of", "World", "War", "II", ",", "two", "Allies", "of", "World", "War", "II", "officers", "in", "Egypt", "are", "interviewed", "to", "lead", "a", "dangerous", "commando", "mission", "far", "behind", "enemy", "lines", "in", "Benghazi", "."], "labels": ["O", "O", "B-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "O", "O", "B-country", "O", "B-event", "I-event", "I-event", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, election, person, politician, organization, political party, event and O.\nSentence: During the Western Desert Campaign of World War II , two Allies of World War II officers in Egypt are interviewed to lead a dangerous commando mission far behind enemy lines in Benghazi .", "prompt_labels": "During(O) the(O) Western(B-event) Desert(I-event) Campaign(I-event) of(O) World(B-event) War(I-event) II(I-event) ,(O) two(O) Allies(B-country) of(O) World(B-event) War(I-event) II(I-event) officers(O) in(O) Egypt(B-country) are(O) interviewed(O) to(O) lead(O) a(O) dangerous(O) commando(O) mission(O) far(O) behind(O) enemy(O) lines(O) in(O) Benghazi(B-location) .(O)"}}
{"id": "428", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "person", "location", "event", "election", "country", "organization", "politician"], "instance": {"id": "428", "words": ["On", "17", "October", ",", "the", "National", "Court", "ordered", "Jordi", "Sànchez", "i", "Picanyol", "and", "Jordi", "Cuixart", "-", "leaders", "of", "pro-independence", "groups", "Assemblea", "Nacional", "Catalana", "(", "ANC", ")", "and", "Òmnium", "Cultural", "-", "to", "be", "preventively", "put", "into", "jail", "without", "bail", "pending", "an", "investigation", "into", "alleged", "sedition", "for", "having", "played", "central", "roles", "in", "orchestrating", "massive", "protests", "aimed", "at", "hindering", "Civil", "Guard", "activity", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-politician", "I-politician", "I-politician", "I-politician", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, person, location, event, election, country, organization, politician and O.\nSentence: On 17 October , the National Court ordered Jordi Sànchez i Picanyol and Jordi Cuixart - leaders of pro-independence groups Assemblea Nacional Catalana ( ANC ) and Òmnium Cultural - to be preventively put into jail without bail pending an investigation into alleged sedition for having played central roles in orchestrating massive protests aimed at hindering Civil Guard activity .", "prompt_labels": "On(O) 17(O) October(O) ,(O) the(O) National(B-organization) Court(I-organization) ordered(O) Jordi(B-politician) Sànchez(I-politician) i(I-politician) Picanyol(I-politician) and(O) Jordi(B-person) Cuixart(I-person) -(O) leaders(O) of(O) pro-independence(O) groups(O) Assemblea(B-organization) Nacional(I-organization) Catalana(I-organization) ((O) ANC(B-organization) )(O) and(O) Òmnium(B-organization) Cultural(I-organization) -(O) to(O) be(O) preventively(O) put(O) into(O) jail(O) without(O) bail(O) pending(O) an(O) investigation(O) into(O) alleged(O) sedition(O) for(O) having(O) played(O) central(O) roles(O) in(O) orchestrating(O) massive(O) protests(O) aimed(O) at(O) hindering(O) Civil(O) Guard(O) activity(O) .(O)"}}
{"id": "452", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "person", "country", "election", "politician", "political party", "event", "organization"], "instance": {"id": "452", "words": ["The", "CEI", "itself", "was", "devoid", "of", "a", "central", "purpose", "as", "its", "four", "largest", "members", "-", "the", "Institution", "of", "Civil", "Engineers", "(", "ICE", ")", ",", "Institution", "of", "Mechanical", "Engineers", "(", "IMechE", ")", ",", "Institution", "of", "Chemical", "Engineers", "(", "IChemE", ")", "and", "the", "Institution", "of", "Electrical", "Engineers", "(", "IEE", ")", "-", "could", "not", "agree", "on", "a", "driving", "policy", "."], "labels": ["O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, country, election, politician, political party, event, organization and O.\nSentence: The CEI itself was devoid of a central purpose as its four largest members - the Institution of Civil Engineers ( ICE ) , Institution of Mechanical Engineers ( IMechE ) , Institution of Chemical Engineers ( IChemE ) and the Institution of Electrical Engineers ( IEE ) - could not agree on a driving policy .", "prompt_labels": "The(O) CEI(B-organization) itself(O) was(O) devoid(O) of(O) a(O) central(O) purpose(O) as(O) its(O) four(O) largest(O) members(O) -(O) the(O) Institution(B-organization) of(I-organization) Civil(I-organization) Engineers(I-organization) ((O) ICE(B-organization) )(O) ,(O) Institution(B-organization) of(I-organization) Mechanical(I-organization) Engineers(I-organization) ((O) IMechE(B-organization) )(O) ,(O) Institution(B-organization) of(I-organization) Chemical(I-organization) Engineers(I-organization) ((O) IChemE(B-organization) )(O) and(O) the(O) Institution(B-organization) of(I-organization) Electrical(I-organization) Engineers(I-organization) ((O) IEE(B-organization) )(O) -(O) could(O) not(O) agree(O) on(O) a(O) driving(O) policy(O) .(O)"}}
{"id": "408", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "event", "political party", "person", "location", "country", "election", "politician"], "instance": {"id": "408", "words": ["The", "election", "also", "produced", "other", "close", "results", ";", "Milton", "Young", "(", "R-ND", ")", "won", "reelection", "against", "Democrat", "William", "L.", "Guy", "by", "only", "186", "votes", "and", "Henry", "Bellmon", "(", "R-OK", ")", "won", "reelection", "against", "Democrat", "Ed", "Edmondson", "by", "half", "a", "percent", "of", "the", "vote", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, political party, person, location, country, election, politician and O.\nSentence: The election also produced other close results ; Milton Young ( R-ND ) won reelection against Democrat William L. Guy by only 186 votes and Henry Bellmon ( R-OK ) won reelection against Democrat Ed Edmondson by half a percent of the vote .", "prompt_labels": "The(O) election(O) also(O) produced(O) other(O) close(O) results(O) ;(O) Milton(B-politician) Young(I-politician) ((O) R-ND(O) )(O) won(O) reelection(O) against(O) Democrat(O) William(B-politician) L.(I-politician) Guy(I-politician) by(O) only(O) 186(O) votes(O) and(O) Henry(B-politician) Bellmon(I-politician) ((O) R-OK(O) )(O) won(O) reelection(O) against(O) Democrat(O) Ed(B-politician) Edmondson(I-politician) by(O) half(O) a(O) percent(O) of(O) the(O) vote(O) .(O)"}}
{"id": "359", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "politician", "event", "country", "political party", "organization", "person", "location"], "instance": {"id": "359", "words": ["Tassi", "'s", "first", "run", "for", "elective", "office", "was", "as", "a", "candidate", "for", "the", "Ontario", "Liberal", "Party", "in", "the", "1995", "Ontario", "general", "election", ",", "where", "she", "finished", "a", "narrow", "second", "to", "Ontario", "New", "Democratic", "Party", "incumbent", "David", "Christopherson", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, event, country, political party, organization, person, location and O.\nSentence: Tassi 's first run for elective office was as a candidate for the Ontario Liberal Party in the 1995 Ontario general election , where she finished a narrow second to Ontario New Democratic Party incumbent David Christopherson .", "prompt_labels": "Tassi(B-politician) 's(O) first(O) run(O) for(O) elective(O) office(O) was(O) as(O) a(O) candidate(O) for(O) the(O) Ontario(B-political party) Liberal(I-political party) Party(I-political party) in(O) the(O) 1995(B-election) Ontario(I-election) general(I-election) election(I-election) ,(O) where(O) she(O) finished(O) a(O) narrow(O) second(O) to(O) Ontario(B-political party) New(I-political party) Democratic(I-political party) Party(I-political party) incumbent(O) David(B-politician) Christopherson(I-politician) .(O)"}}
{"id": "41", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "election", "event", "organization", "location", "country", "person", "political party"], "instance": {"id": "41", "words": ["In", "1990", "Australian", "federal", "election", "Caldicott", "unsuccessfully", "contested", "the", "House", "of", "Representatives", "New", "South", "Wales", "seat", "of", "Richmond", ",", "a", "seat", "held", "since", "the", "inaugural", "1901", "Australian", "federal", "election", "by", "conservatives", ",", "and", "by", "the", "National", "Party", "of", "Australia", "since", "it", "first", "contested", "elections", "at", "the", "1922", "Australian", "federal", "election", "."], "labels": ["O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, event, organization, location, country, person, political party and O.\nSentence: In 1990 Australian federal election Caldicott unsuccessfully contested the House of Representatives New South Wales seat of Richmond , a seat held since the inaugural 1901 Australian federal election by conservatives , and by the National Party of Australia since it first contested elections at the 1922 Australian federal election .", "prompt_labels": "In(O) 1990(B-election) Australian(I-election) federal(I-election) election(I-election) Caldicott(O) unsuccessfully(O) contested(O) the(O) House(B-organization) of(I-organization) Representatives(I-organization) New(I-organization) South(I-organization) Wales(I-organization) seat(O) of(O) Richmond(B-location) ,(O) a(O) seat(O) held(O) since(O) the(O) inaugural(O) 1901(B-election) Australian(I-election) federal(I-election) election(I-election) by(O) conservatives(O) ,(O) and(O) by(O) the(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) since(O) it(O) first(O) contested(O) elections(O) at(O) the(O) 1922(B-election) Australian(I-election) federal(I-election) election(I-election) .(O)"}}
{"id": "373", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "country", "organization", "location", "politician", "election", "event", "political party"], "instance": {"id": "373", "words": ["In", "1999", "New", "Zealand", "general", "election", "when", "Jeanette", "Fitzsimons", "contested", "the", "(", "usually", "New", "Zealand", "National", "Party", ")", "seat", "of", "Coromandel", "for", "the", "Green", "Party", "of", "Aotearoa", "New", "Zealand", ",", "it", "seemed", "that", "the", "Greens", "'", "chances", "of", "entering", "parliament", "were", "dependent", "on", "Fitzsimons", "'", "performance", "in", "Coromandel", ";", "in", "order", "to", "receive", "proportional", "representation", ",", "the", "party", "needed", "to", "either", "gain", "five", "percent", "of", "the", "national", "vote", "or", "win", "an", "electorate", "seat", ",", "and", "it", "appeared", "that", "the", "former", "option", "was", "unlikely", "."], "labels": ["O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-politician", "I-politician", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "B-location", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, organization, location, politician, election, event, political party and O.\nSentence: In 1999 New Zealand general election when Jeanette Fitzsimons contested the ( usually New Zealand National Party ) seat of Coromandel for the Green Party of Aotearoa New Zealand , it seemed that the Greens ' chances of entering parliament were dependent on Fitzsimons ' performance in Coromandel ; in order to receive proportional representation , the party needed to either gain five percent of the national vote or win an electorate seat , and it appeared that the former option was unlikely .", "prompt_labels": "In(O) 1999(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) when(O) Jeanette(B-politician) Fitzsimons(I-politician) contested(O) the(O) ((O) usually(O) New(B-political party) Zealand(I-political party) National(I-political party) Party(I-political party) )(O) seat(O) of(O) Coromandel(B-location) for(O) the(O) Green(B-political party) Party(I-political party) of(I-political party) Aotearoa(I-political party) New(I-political party) Zealand(I-political party) ,(O) it(O) seemed(O) that(O) the(O) Greens(B-political party) '(O) chances(O) of(O) entering(O) parliament(O) were(O) dependent(O) on(O) Fitzsimons(B-politician) '(O) performance(O) in(O) Coromandel(B-location) ;(O) in(O) order(O) to(O) receive(O) proportional(O) representation(O) ,(O) the(O) party(O) needed(O) to(O) either(O) gain(O) five(O) percent(O) of(O) the(O) national(O) vote(O) or(O) win(O) an(O) electorate(O) seat(O) ,(O) and(O) it(O) appeared(O) that(O) the(O) former(O) option(O) was(O) unlikely(O) .(O)"}}
{"id": "149", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "politician", "election", "organization", "event", "person", "political party", "country"], "instance": {"id": "149", "words": ["Patteson", "was", "a", "member", "of", "the", "Board", "of", "Trustees", "of", "West", "Virginia", "Wesleyan", ",", "and", "of", "a", "number", "of", "societies", ":", "Free", "mason", "s", ",", "Knights", "Templar", ",", "Moose", "International", ",", "Lions", "Clubs", "International", ",", "Chamber", "of", "Commerce", ",", "American", "Legion", ",", "Sons", "of", "the", "American", "Revolution", "and", "Benevolent", "and", "Protective", "Order", "of", "Elks", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, politician, election, organization, event, person, political party, country and O.\nSentence: Patteson was a member of the Board of Trustees of West Virginia Wesleyan , and of a number of societies : Free mason s , Knights Templar , Moose International , Lions Clubs International , Chamber of Commerce , American Legion , Sons of the American Revolution and Benevolent and Protective Order of Elks .", "prompt_labels": "Patteson(B-person) was(O) a(O) member(O) of(O) the(O) Board(O) of(O) Trustees(O) of(O) West(B-organization) Virginia(I-organization) Wesleyan(I-organization) ,(O) and(O) of(O) a(O) number(O) of(O) societies(O) :(O) Free(B-organization) mason(I-organization) s(O) ,(O) Knights(B-organization) Templar(I-organization) ,(O) Moose(B-organization) International(I-organization) ,(O) Lions(B-organization) Clubs(I-organization) International(I-organization) ,(O) Chamber(B-organization) of(I-organization) Commerce(I-organization) ,(O) American(B-organization) Legion(I-organization) ,(O) Sons(B-organization) of(I-organization) the(I-organization) American(I-organization) Revolution(I-organization) and(O) Benevolent(B-organization) and(I-organization) Protective(I-organization) Order(I-organization) of(I-organization) Elks(I-organization) .(O)"}}
{"id": "48", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "location", "person", "organization", "political party", "country", "politician", "election"], "instance": {"id": "48", "words": ["In", "the", "February", "1974", "United", "Kingdom", "general", "election", "the", "seat", "was", "won", "by", "John", "Carson", "of", "the", "Ulster", "Unionist", "Party", "with", "backing", "by", "the", "Vanguard", "Progressive", "Unionist", "Party", "and", "the", "Democratic", "Unionist", "Party", "on", "a", "united", "slate", "in", "opposition", "to", "the", "Sunningdale", "Agreement", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, person, organization, political party, country, politician, election and O.\nSentence: In the February 1974 United Kingdom general election the seat was won by John Carson of the Ulster Unionist Party with backing by the Vanguard Progressive Unionist Party and the Democratic Unionist Party on a united slate in opposition to the Sunningdale Agreement .", "prompt_labels": "In(O) the(O) February(B-election) 1974(I-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) the(O) seat(O) was(O) won(O) by(O) John(B-politician) Carson(I-politician) of(O) the(O) Ulster(B-political party) Unionist(I-political party) Party(I-political party) with(O) backing(O) by(O) the(O) Vanguard(B-political party) Progressive(I-political party) Unionist(I-political party) Party(I-political party) and(O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) on(O) a(O) united(O) slate(O) in(O) opposition(O) to(O) the(O) Sunningdale(B-organization) Agreement(I-organization) .(O)"}}
{"id": "92", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "location", "politician", "country", "person", "organization", "election", "political party"], "instance": {"id": "92", "words": ["Supporters", "of", "the", "campaign", "included", "Margo", "Kingston", "(", "journalist", ")", ",", "John", "Valder", "(", "previous", "president", "of", "Howard", "'s", "Liberal", "Party", "of", "Australia", ")", ",", "Brian", "Deegan", "(", "former", "magistrate", ",", "who", "stood", "against", "Alexander", "Downer", ")", ",", "Andrew", "Wilkie", "(", "Australian", "Greens", "candidate", ")", ",", "Alex", "Broun", "playwright", "and", "Nicole", "Campbell", "(", "Australian", "Labor", "Party", "candidate", ")", "."], "labels": ["O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-politician", "I-politician", "O", "B-political party", "I-political party", "O", "O", "O", "B-person", "I-person", "O", "O", "B-politician", "I-politician", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, politician, country, person, organization, election, political party and O.\nSentence: Supporters of the campaign included Margo Kingston ( journalist ) , John Valder ( previous president of Howard 's Liberal Party of Australia ) , Brian Deegan ( former magistrate , who stood against Alexander Downer ) , Andrew Wilkie ( Australian Greens candidate ) , Alex Broun playwright and Nicole Campbell ( Australian Labor Party candidate ) .", "prompt_labels": "Supporters(O) of(O) the(O) campaign(O) included(O) Margo(B-person) Kingston(I-person) ((O) journalist(O) )(O) ,(O) John(B-politician) Valder(I-politician) ((O) previous(O) president(O) of(O) Howard(O) 's(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) )(O) ,(O) Brian(B-person) Deegan(I-person) ((O) former(O) magistrate(O) ,(O) who(O) stood(O) against(O) Alexander(B-politician) Downer(I-politician) )(O) ,(O) Andrew(B-politician) Wilkie(I-politician) ((O) Australian(B-political party) Greens(I-political party) candidate(O) )(O) ,(O) Alex(B-person) Broun(I-person) playwright(O) and(O) Nicole(B-politician) Campbell(I-politician) ((O) Australian(B-political party) Labor(I-political party) Party(I-political party) candidate(O) )(O) .(O)"}}
{"id": "416", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "country", "politician", "event", "organization", "election", "person", "political party"], "instance": {"id": "416", "words": ["The", "call", "for", "creation", "of", "the", "Akhand", "Bharat", "or", "Akhand", "Hindusthan", "has", "on", "occasions", "been", "raised", "by", "Hindu", "nationalist", "organisations", "such", "as", "the", "Hindu", "Mahasabha", ",", "Kakbhusundi", "Revolutionary", "Forum", "(", "KRF", ")", ",", "Rashtriya", "Swayamsevak", "Sangh", "(", "RSS", ")", ",", "Vishwa", "Hindu", "Parishad", ",", "Shiv", "Sena", ",", "Hindu", "Sena", ",", "Hindu", "Janajagruti", "Samiti", "and", "Bharatiya", "Janata", "Party", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "O", "B-organization", "I-organization", "I-organization", "O", "B-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, politician, event, organization, election, person, political party and O.\nSentence: The call for creation of the Akhand Bharat or Akhand Hindusthan has on occasions been raised by Hindu nationalist organisations such as the Hindu Mahasabha , Kakbhusundi Revolutionary Forum ( KRF ) , Rashtriya Swayamsevak Sangh ( RSS ) , Vishwa Hindu Parishad , Shiv Sena , Hindu Sena , Hindu Janajagruti Samiti and Bharatiya Janata Party .", "prompt_labels": "The(O) call(O) for(O) creation(O) of(O) the(O) Akhand(O) Bharat(O) or(O) Akhand(O) Hindusthan(O) has(O) on(O) occasions(O) been(O) raised(O) by(O) Hindu(O) nationalist(O) organisations(O) such(O) as(O) the(O) Hindu(B-political party) Mahasabha(I-political party) ,(O) Kakbhusundi(B-organization) Revolutionary(I-organization) Forum(I-organization) ((O) KRF(B-organization) )(O) ,(O) Rashtriya(B-organization) Swayamsevak(I-organization) Sangh(I-organization) ((O) RSS(B-organization) )(O) ,(O) Vishwa(B-organization) Hindu(I-organization) Parishad(I-organization) ,(O) Shiv(B-political party) Sena(I-political party) ,(O) Hindu(B-political party) Sena(I-political party) ,(O) Hindu(B-organization) Janajagruti(I-organization) Samiti(I-organization) and(O) Bharatiya(B-political party) Janata(I-political party) Party(I-political party) .(O)"}}
{"id": "158", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "election", "country", "location", "political party", "event", "person", "organization"], "instance": {"id": "158", "words": ["In", "British", "Columbia", ",", "the", "British", "Columbia", "Social", "Credit", "Party", "was", "replaced", "as", "the", "party", "of", "the", "centre-right", "by", "the", "British", "Columbia", "Liberal", "Party", ",", "and", "in", "Alberta", "the", "Alberta", "Social", "Credit", "Party", "were", "completely", "annihilated", "by", "the", "more", "moderate", "Alberta", "Progressive", "Conservative", "Party", ",", "leaving", "both", "parties", "as", "marginal", "political", "forces", "."], "labels": ["O", "B-location", "I-location", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "B-location", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, country, location, political party, event, person, organization and O.\nSentence: In British Columbia , the British Columbia Social Credit Party was replaced as the party of the centre-right by the British Columbia Liberal Party , and in Alberta the Alberta Social Credit Party were completely annihilated by the more moderate Alberta Progressive Conservative Party , leaving both parties as marginal political forces .", "prompt_labels": "In(O) British(B-location) Columbia(I-location) ,(O) the(O) British(B-political party) Columbia(I-political party) Social(I-political party) Credit(I-political party) Party(I-political party) was(O) replaced(O) as(O) the(O) party(O) of(O) the(O) centre-right(O) by(O) the(O) British(B-political party) Columbia(I-political party) Liberal(I-political party) Party(I-political party) ,(O) and(O) in(O) Alberta(B-location) the(O) Alberta(B-political party) Social(I-political party) Credit(I-political party) Party(I-political party) were(O) completely(O) annihilated(O) by(O) the(O) more(O) moderate(O) Alberta(B-political party) Progressive(I-political party) Conservative(I-political party) Party(I-political party) ,(O) leaving(O) both(O) parties(O) as(O) marginal(O) political(O) forces(O) .(O)"}}
{"id": "421", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "event", "politician", "election", "country", "person", "location", "political party"], "instance": {"id": "421", "words": ["The", "six-part", "series", "features", "Richard", "Madden", "and", "Keeley", "Hawes", ",", "and", "includes", "Gina", "McKee", ",", "Sophie", "Rundle", ",", "Vincent", "Franklin", ",", "Pippa", "Haywood", ",", "Paul", "Ready", ",", "Tom", "Brooke", ",", "Nicholas", "Gleaves", ",", "Stuart", "Bowman", ",", "Stephanie", "Hyam", ",", "David", "Westhead", ",", "Matt", "Stokoe", ",", "Nina", "Toussaint-White", ",", "Ash", "Tandon", ",", "and", "Anjli", "Mohindra", "."], "labels": ["O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, politician, election, country, person, location, political party and O.\nSentence: The six-part series features Richard Madden and Keeley Hawes , and includes Gina McKee , Sophie Rundle , Vincent Franklin , Pippa Haywood , Paul Ready , Tom Brooke , Nicholas Gleaves , Stuart Bowman , Stephanie Hyam , David Westhead , Matt Stokoe , Nina Toussaint-White , Ash Tandon , and Anjli Mohindra .", "prompt_labels": "The(O) six-part(O) series(O) features(O) Richard(B-person) Madden(I-person) and(O) Keeley(B-person) Hawes(I-person) ,(O) and(O) includes(O) Gina(B-person) McKee(I-person) ,(O) Sophie(B-person) Rundle(I-person) ,(O) Vincent(B-person) Franklin(I-person) ,(O) Pippa(B-person) Haywood(I-person) ,(O) Paul(B-person) Ready(I-person) ,(O) Tom(B-person) Brooke(I-person) ,(O) Nicholas(B-person) Gleaves(I-person) ,(O) Stuart(B-person) Bowman(I-person) ,(O) Stephanie(B-person) Hyam(I-person) ,(O) David(B-person) Westhead(I-person) ,(O) Matt(B-person) Stokoe(I-person) ,(O) Nina(B-person) Toussaint-White(I-person) ,(O) Ash(B-person) Tandon(I-person) ,(O) and(O) Anjli(B-person) Mohindra(I-person) .(O)"}}
{"id": "328", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "location", "election", "person", "political party", "country", "politician", "event"], "instance": {"id": "328", "words": ["The", "California", "Republican", "Party", "nominee", ",", "Senator", "Pete", "Wilson", ",", "defeated", "the", "California", "Democratic", "Party", "nominee", ",", "former", "San", "Francisco", "Mayor", "Dianne", "Feinstein", ",", "who", "would", "later", "go", "on", "to", "1992", "United", "States", "Senate", "special", "election", "in", "California", "."], "labels": ["O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "B-location", "I-location", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, election, person, political party, country, politician, event and O.\nSentence: The California Republican Party nominee , Senator Pete Wilson , defeated the California Democratic Party nominee , former San Francisco Mayor Dianne Feinstein , who would later go on to 1992 United States Senate special election in California .", "prompt_labels": "The(O) California(B-political party) Republican(I-political party) Party(I-political party) nominee(O) ,(O) Senator(O) Pete(B-politician) Wilson(I-politician) ,(O) defeated(O) the(O) California(B-political party) Democratic(I-political party) Party(I-political party) nominee(O) ,(O) former(O) San(B-location) Francisco(I-location) Mayor(O) Dianne(B-politician) Feinstein(I-politician) ,(O) who(O) would(O) later(O) go(O) on(O) to(O) 1992(B-election) United(I-election) States(I-election) Senate(I-election) special(I-election) election(I-election) in(I-election) California(I-election) .(O)"}}
{"id": "23", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "country", "organization", "event", "person", "politician", "political party", "location"], "instance": {"id": "23", "words": ["It", "was", "succeeded", "in", "the", "Flemish", "Community", "of", "Belgium", "by", "the", "Open", "Vlaamse", "Liberalen", "en", "Democraten", "(", "VLD", ")", "and", "in", "the", "French", "Community", "by", "the", "Liberal", "Reformist", "Party", ",", "Parti", "des", "Réformes", "et", "des", "Libertés", "de", "Wallonie", "and", "the", "current-day", "Mouvement", "Réformateur", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-location", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, country, organization, event, person, politician, political party, location and O.\nSentence: It was succeeded in the Flemish Community of Belgium by the Open Vlaamse Liberalen en Democraten ( VLD ) and in the French Community by the Liberal Reformist Party , Parti des Réformes et des Libertés de Wallonie and the current-day Mouvement Réformateur .", "prompt_labels": "It(O) was(O) succeeded(O) in(O) the(O) Flemish(B-organization) Community(I-organization) of(O) Belgium(B-location) by(O) the(O) Open(B-political party) Vlaamse(I-political party) Liberalen(I-political party) en(I-political party) Democraten(I-political party) ((O) VLD(B-political party) )(O) and(O) in(O) the(O) French(B-country) Community(I-country) by(O) the(O) Liberal(B-political party) Reformist(I-political party) Party(I-political party) ,(O) Parti(B-political party) des(I-political party) Réformes(I-political party) et(I-political party) des(I-political party) Libertés(I-political party) de(I-political party) Wallonie(I-political party) and(O) the(O) current-day(O) Mouvement(B-political party) Réformateur(I-political party) .(O)"}}
{"id": "304", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "election", "country", "organization", "location", "person", "political party", "politician"], "instance": {"id": "304", "words": ["The", "dissolution", "of", "Jeune", "Nation", "in", "1958", "and", "of", "the", "Organisation", "Armée", "Secrète", "in", "1962", ",", "as", "well", "as", "the", "failures", "of", "far-right", "candidate", "Jean-Louis", "Tixier-Vignancour", "in", "the", "1965", "French", "presidential", "election", "and", "of", "the", "European", "Rally", "for", "Liberty", "(", "REL", ")", "in", "the", "1967", "French", "legislative", "election", ",", "are", "cited", "as", "events", "conducive", "to", "the", "foundation", "of", "GRECE", "and", "the", "development", "of", "its", "meta-political", "strategy", "."], "labels": ["O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, country, organization, location, person, political party, politician and O.\nSentence: The dissolution of Jeune Nation in 1958 and of the Organisation Armée Secrète in 1962 , as well as the failures of far-right candidate Jean-Louis Tixier-Vignancour in the 1965 French presidential election and of the European Rally for Liberty ( REL ) in the 1967 French legislative election , are cited as events conducive to the foundation of GRECE and the development of its meta-political strategy .", "prompt_labels": "The(O) dissolution(O) of(O) Jeune(B-political party) Nation(I-political party) in(O) 1958(O) and(O) of(O) the(O) Organisation(B-organization) Armée(I-organization) Secrète(I-organization) in(O) 1962(O) ,(O) as(O) well(O) as(O) the(O) failures(O) of(O) far-right(O) candidate(O) Jean-Louis(B-politician) Tixier-Vignancour(I-politician) in(O) the(O) 1965(B-election) French(I-election) presidential(I-election) election(I-election) and(O) of(O) the(O) European(B-political party) Rally(I-political party) for(I-political party) Liberty(I-political party) ((O) REL(B-political party) )(O) in(O) the(O) 1967(B-election) French(I-election) legislative(I-election) election(I-election) ,(O) are(O) cited(O) as(O) events(O) conducive(O) to(O) the(O) foundation(O) of(O) GRECE(B-country) and(O) the(O) development(O) of(O) its(O) meta-political(O) strategy(O) .(O)"}}
{"id": "164", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "event", "location", "election", "country", "political party", "person", "politician"], "instance": {"id": "164", "words": ["The", "Socialist", "Party", "of", "the", "United", "States", "(", "SPUS", ")", "-", "its", "name", "inspired", "by", "co-thinkers", "in", "the", "Socialist", "Party", "of", "Great", "Britain", "(", "SPGB", ")", "and", "the", "original", "(", "non-WSM", ")", "Socialist", "Party", "of", "Canada", "(", "SPC", ")", "-", "was", "established", "on", "July", "7", ",", "1916", "by", "42", "defecting", "members", "of", "Local", "Detroit", "of", "the", "Socialist", "Party", "of", "America", "(", "SPA", ")", "."], "labels": ["O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, location, election, country, political party, person, politician and O.\nSentence: The Socialist Party of the United States ( SPUS ) - its name inspired by co-thinkers in the Socialist Party of Great Britain ( SPGB ) and the original ( non-WSM ) Socialist Party of Canada ( SPC ) - was established on July 7 , 1916 by 42 defecting members of Local Detroit of the Socialist Party of America ( SPA ) .", "prompt_labels": "The(O) Socialist(B-political party) Party(I-political party) of(I-political party) the(I-political party) United(I-political party) States(I-political party) ((O) SPUS(B-political party) )(O) -(O) its(O) name(O) inspired(O) by(O) co-thinkers(O) in(O) the(O) Socialist(B-political party) Party(I-political party) of(I-political party) Great(I-political party) Britain(I-political party) ((O) SPGB(B-political party) )(O) and(O) the(O) original(O) ((O) non-WSM(O) )(O) Socialist(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) ((O) SPC(B-political party) )(O) -(O) was(O) established(O) on(O) July(O) 7(O) ,(O) 1916(O) by(O) 42(O) defecting(O) members(O) of(O) Local(B-location) Detroit(I-location) of(O) the(O) Socialist(B-political party) Party(I-political party) of(I-political party) America(I-political party) ((O) SPA(B-political party) )(O) .(O)"}}
{"id": "224", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "organization", "election", "country", "person", "event", "politician", "political party"], "instance": {"id": "224", "words": ["Fletcher", "later", "joined", "the", "Progressive", "Conservative", "Party", "of", "Manitoba", ",", "and", "lost", "to", "Molgat", "under", "this", "party", "'s", "banner", "in", "the", "provincial", "elections", "of", "1959", "Manitoba", "general", "election", "and", "1962", "Manitoba", "general", "election", "."], "labels": ["B-politician", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, election, country, person, event, politician, political party and O.\nSentence: Fletcher later joined the Progressive Conservative Party of Manitoba , and lost to Molgat under this party 's banner in the provincial elections of 1959 Manitoba general election and 1962 Manitoba general election .", "prompt_labels": "Fletcher(B-politician) later(O) joined(O) the(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Manitoba(I-political party) ,(O) and(O) lost(O) to(O) Molgat(B-politician) under(O) this(O) party(O) 's(O) banner(O) in(O) the(O) provincial(O) elections(O) of(O) 1959(B-election) Manitoba(I-election) general(I-election) election(I-election) and(O) 1962(B-election) Manitoba(I-election) general(I-election) election(I-election) .(O)"}}
{"id": "394", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "political party", "organization", "election", "person", "politician", "country", "event"], "instance": {"id": "394", "words": ["Incumbent", "Democrat", "Carl", "Levin", "won", "re-election", "to", "a", "fourth", "term", "over", "Ronna", "Romney", "radio", "talk", "show", "host", "and", "former", "daughter-in-law", "of", "Michigan", "governor", "George", "W.", "Romney", "."], "labels": ["O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-politician", "I-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, political party, organization, election, person, politician, country, event and O.\nSentence: Incumbent Democrat Carl Levin won re-election to a fourth term over Ronna Romney radio talk show host and former daughter-in-law of Michigan governor George W. Romney .", "prompt_labels": "Incumbent(O) Democrat(O) Carl(B-politician) Levin(I-politician) won(O) re-election(O) to(O) a(O) fourth(O) term(O) over(O) Ronna(B-politician) Romney(I-politician) radio(O) talk(O) show(O) host(O) and(O) former(O) daughter-in-law(O) of(O) Michigan(B-location) governor(O) George(B-politician) W.(I-politician) Romney(I-politician) .(O)"}}
{"id": "18", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "political party", "location", "country", "organization", "event", "election", "person"], "instance": {"id": "18", "words": ["In", "the", "2012", "Ukrainian", "parliamentary", "election", ",", "2014", "Ukrainian", "parliamentary", "election", "and", "2019", "Ukrainian", "parliamentary", "election", "elections"], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, political party, location, country, organization, event, election, person and O.\nSentence: In the 2012 Ukrainian parliamentary election , 2014 Ukrainian parliamentary election and 2019 Ukrainian parliamentary election elections", "prompt_labels": "In(O) the(O) 2012(B-election) Ukrainian(I-election) parliamentary(I-election) election(I-election) ,(O) 2014(B-election) Ukrainian(I-election) parliamentary(I-election) election(I-election) and(O) 2019(B-election) Ukrainian(I-election) parliamentary(I-election) election(I-election) elections(O)"}}
{"id": "530", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "political party", "politician", "person", "event", "location", "country", "organization"], "instance": {"id": "530", "words": ["The", "2017", "BRICS", "summit", "was", "the", "ninth", "annual", "BRICS", "summit", ",", "an", "international", "relations", "conference", "attended", "by", "the", "heads", "of", "state", "or", "heads", "of", "government", "of", "the", "five", "member", "states", "Brazil", ",", "Russia", ",", "India", ",", "China", "and", "South", "Africa", "."], "labels": ["O", "B-event", "I-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, politician, person, event, location, country, organization and O.\nSentence: The 2017 BRICS summit was the ninth annual BRICS summit , an international relations conference attended by the heads of state or heads of government of the five member states Brazil , Russia , India , China and South Africa .", "prompt_labels": "The(O) 2017(B-event) BRICS(I-event) summit(I-event) was(O) the(O) ninth(B-event) annual(I-event) BRICS(I-event) summit(I-event) ,(O) an(O) international(O) relations(O) conference(O) attended(O) by(O) the(O) heads(O) of(O) state(O) or(O) heads(O) of(O) government(O) of(O) the(O) five(O) member(O) states(O) Brazil(B-country) ,(O) Russia(B-country) ,(O) India(B-country) ,(O) China(B-country) and(O) South(B-country) Africa(I-country) .(O)"}}
{"id": "438", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "location", "election", "person", "organization", "political party", "politician", "country"], "instance": {"id": "438", "words": ["The", "film", "stars", "Vijay", ",", "Keerthy", "Suresh", ",", "and", "Varalaxmi", "Sarathkumar", "while", "Yogi", "Babu", ",", "Radha", "Ravi", ",", "and", "."], "labels": ["O", "O", "O", "B-person", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, election, person, organization, political party, politician, country and O.\nSentence: The film stars Vijay , Keerthy Suresh , and Varalaxmi Sarathkumar while Yogi Babu , Radha Ravi , and .", "prompt_labels": "The(O) film(O) stars(O) Vijay(B-person) ,(O) Keerthy(B-person) Suresh(I-person) ,(O) and(O) Varalaxmi(B-person) Sarathkumar(I-person) while(O) Yogi(B-person) Babu(I-person) ,(O) Radha(B-person) Ravi(I-person) ,(O) and(O) .(O)"}}
{"id": "235", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "country", "politician", "event", "election", "organization", "political party", "location"], "instance": {"id": "235", "words": ["In", "the", "2009", "Kurdistan", "Region", "parliamentary", "election", "they", "formed", "a", "coalition", "with", "the", "Kurdistan", "Islamic", "Union", ",", "the", "Kurdistan", "Socialist", "Democratic", "Party", "and", "the", "Future", "Party", ",", "called", "the", "Service", "and", "Reform", "List", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, politician, event, election, organization, political party, location and O.\nSentence: In the 2009 Kurdistan Region parliamentary election they formed a coalition with the Kurdistan Islamic Union , the Kurdistan Socialist Democratic Party and the Future Party , called the Service and Reform List .", "prompt_labels": "In(O) the(O) 2009(B-election) Kurdistan(I-election) Region(I-election) parliamentary(I-election) election(I-election) they(O) formed(O) a(O) coalition(O) with(O) the(O) Kurdistan(B-political party) Islamic(I-political party) Union(I-political party) ,(O) the(O) Kurdistan(B-political party) Socialist(I-political party) Democratic(I-political party) Party(I-political party) and(O) the(O) Future(B-political party) Party(I-political party) ,(O) called(O) the(O) Service(B-political party) and(I-political party) Reform(I-political party) List(I-political party) .(O)"}}
{"id": "46", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "politician", "organization", "political party", "person", "location", "event", "election"], "instance": {"id": "46", "words": ["Of", "the", "five", "main", "political", "parties", "in", "Northern", "Ireland", ",", "four", "(", "the", "Ulster", "Unionist", "Party", ",", "the", "Democratic", "Unionist", "Party", ",", "the", "Social", "Democratic", "and", "Labour", "Party", "and", "Sinn", "Féin", ")", "all", "have", "relatively", "strong", "support", "bases", "and", "routinely", "poll", "similar", "results", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, politician, organization, political party, person, location, event, election and O.\nSentence: Of the five main political parties in Northern Ireland , four ( the Ulster Unionist Party , the Democratic Unionist Party , the Social Democratic and Labour Party and Sinn Féin ) all have relatively strong support bases and routinely poll similar results .", "prompt_labels": "Of(O) the(O) five(O) main(O) political(O) parties(O) in(O) Northern(B-country) Ireland(I-country) ,(O) four(O) ((O) the(O) Ulster(B-political party) Unionist(I-political party) Party(I-political party) ,(O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) ,(O) the(O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) and(O) Sinn(B-political party) Féin(I-political party) )(O) all(O) have(O) relatively(O) strong(O) support(O) bases(O) and(O) routinely(O) poll(O) similar(O) results(O) .(O)"}}
{"id": "525", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "location", "politician", "organization", "political party", "person", "election", "country"], "instance": {"id": "525", "words": ["When", "war", "broke", "out", "in", "1914", "many", "Social", "Democratic", "Party", "of", "Germany", "opposed", "the", "leadership", "decision", "to", "operate", "a", "political", "truce", ",", "which", "in", "effect", "meant", "voting", "in", "the", "Reichstag", "in", "support", "of", "war", "credits", "to", "fund", "the", "war", ",", "and", "refraining", "from", "criticism", "of", "the", "government", "for", "its", "duration", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, politician, organization, political party, person, election, country and O.\nSentence: When war broke out in 1914 many Social Democratic Party of Germany opposed the leadership decision to operate a political truce , which in effect meant voting in the Reichstag in support of war credits to fund the war , and refraining from criticism of the government for its duration .", "prompt_labels": "When(O) war(O) broke(O) out(O) in(O) 1914(O) many(O) Social(B-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Germany(I-political party) opposed(O) the(O) leadership(O) decision(O) to(O) operate(O) a(O) political(O) truce(O) ,(O) which(O) in(O) effect(O) meant(O) voting(O) in(O) the(O) Reichstag(B-location) in(O) support(O) of(O) war(O) credits(O) to(O) fund(O) the(O) war(O) ,(O) and(O) refraining(O) from(O) criticism(O) of(O) the(O) government(O) for(O) its(O) duration(O) .(O)"}}
{"id": "115", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "event", "person", "country", "organization", "politician", "election", "political party"], "instance": {"id": "115", "words": ["In", "1463", ",", "Bagrat", "allied", "himself", "with", "other", "oppositionist", "royal", "subjects", ",", "dukes", "(", "eristavi", ")", "of", "Principality", "of", "Mingrelia", ",", "Principality", "of", "Guria", ",", "Principality", "of", "Svaneti", "and", "Principality", "of", "Abkhazia", "."], "labels": ["O", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "I-country", "O", "B-country", "I-country", "I-country", "O", "B-country", "I-country", "I-country", "O", "B-country", "I-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, person, country, organization, politician, election, political party and O.\nSentence: In 1463 , Bagrat allied himself with other oppositionist royal subjects , dukes ( eristavi ) of Principality of Mingrelia , Principality of Guria , Principality of Svaneti and Principality of Abkhazia .", "prompt_labels": "In(O) 1463(O) ,(O) Bagrat(B-politician) allied(O) himself(O) with(O) other(O) oppositionist(O) royal(O) subjects(O) ,(O) dukes(O) ((O) eristavi(O) )(O) of(O) Principality(B-country) of(I-country) Mingrelia(I-country) ,(O) Principality(B-country) of(I-country) Guria(I-country) ,(O) Principality(B-country) of(I-country) Svaneti(I-country) and(O) Principality(B-country) of(I-country) Abkhazia(I-country) .(O)"}}
{"id": "257", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "election", "organization", "country", "location", "political party", "event", "politician"], "instance": {"id": "257", "words": ["A", "new", "Republican", "Party", "President", "was", "only", "elected", "later", "in", "the", "following", "decade", "of", "the", "early", "1950s", "with", "the", "losses", "by", "two-time", "nominee", ",", "the", "Governor", "of", "Illinois", "Adlai", "Stevenson", "II", "(", "grandson", "of", "the", "Adlai", "Stevenson", "I", "of", "the", "1892", "United", "States", "presidential", "election", ")", "to", "the", "very", "popular", "war", "hero", "and", "commanding", "general", "in", "World", "War", "II", ",", "General", "Dwight", "D.", "Eisenhower", "(", "in", "1952", "United", "States", "presidential", "election", "and", "1956", "United", "States", "presidential", "election", ")", "."], "labels": ["O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "B-politician", "I-politician", "I-politician", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "B-politician", "I-politician", "I-politician", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, election, organization, country, location, political party, event, politician and O.\nSentence: A new Republican Party President was only elected later in the following decade of the early 1950s with the losses by two-time nominee , the Governor of Illinois Adlai Stevenson II ( grandson of the Adlai Stevenson I of the 1892 United States presidential election ) to the very popular war hero and commanding general in World War II , General Dwight D. Eisenhower ( in 1952 United States presidential election and 1956 United States presidential election ) .", "prompt_labels": "A(O) new(O) Republican(B-political party) Party(I-political party) President(O) was(O) only(O) elected(O) later(O) in(O) the(O) following(O) decade(O) of(O) the(O) early(O) 1950s(O) with(O) the(O) losses(O) by(O) two-time(O) nominee(O) ,(O) the(O) Governor(O) of(O) Illinois(B-location) Adlai(B-politician) Stevenson(I-politician) II(I-politician) ((O) grandson(O) of(O) the(O) Adlai(B-politician) Stevenson(I-politician) I(I-politician) of(O) the(O) 1892(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) )(O) to(O) the(O) very(O) popular(O) war(O) hero(O) and(O) commanding(O) general(O) in(O) World(B-event) War(I-event) II(I-event) ,(O) General(O) Dwight(B-politician) D.(I-politician) Eisenhower(I-politician) ((O) in(O) 1952(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) and(O) 1956(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) )(O) .(O)"}}
{"id": "167", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "country", "person", "organization", "political party", "event", "election", "politician"], "instance": {"id": "167", "words": ["During", "the", "2005", "German", "federal", "election", "campaign", ",", "Angela", "Merkel", ",", "leader", "of", "the", "Christian", "Democratic", "Union", "of", "Germany", "/", "Christian", "Social", "Union", "in", "Bavaria", ",", "announced", "that", "Kirchhof", "would", "serve", "as", "minister", "of", "finance", "if", "she", "formed", "a", "government", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, person, organization, political party, event, election, politician and O.\nSentence: During the 2005 German federal election campaign , Angela Merkel , leader of the Christian Democratic Union of Germany / Christian Social Union in Bavaria , announced that Kirchhof would serve as minister of finance if she formed a government .", "prompt_labels": "During(O) the(O) 2005(B-election) German(I-election) federal(I-election) election(I-election) campaign(O) ,(O) Angela(B-politician) Merkel(I-politician) ,(O) leader(O) of(O) the(O) Christian(B-political party) Democratic(I-political party) Union(I-political party) of(I-political party) Germany(I-political party) /(O) Christian(B-political party) Social(I-political party) Union(I-political party) in(I-political party) Bavaria(I-political party) ,(O) announced(O) that(O) Kirchhof(B-politician) would(O) serve(O) as(O) minister(O) of(O) finance(O) if(O) she(O) formed(O) a(O) government(O) .(O)"}}
{"id": "142", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "country", "election", "political party", "event", "politician", "organization", "person"], "instance": {"id": "142", "words": ["He", "had", "been", "targeted", "by", "the", "radical", "democrats", ",", "including", "Albert", "Chan", "of", "the", "People", "Power", "in", "the", "2011", "Hong", "Kong", "local", "elections", "who", "opposed", "Democrats", "'", "compromise", "with", "the", "Beijing", "officials", "on", "the", "2012", "constitutional", "reform", "proposals", "and", "in", "the", "2015", "Hong", "Kong", "local", "elections", "by", "Civic", "Passion", "'", "s", "Cheng", "Chung-tai", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-political party", "I-political party", "O", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, election, political party, event, politician, organization, person and O.\nSentence: He had been targeted by the radical democrats , including Albert Chan of the People Power in the 2011 Hong Kong local elections who opposed Democrats ' compromise with the Beijing officials on the 2012 constitutional reform proposals and in the 2015 Hong Kong local elections by Civic Passion ' s Cheng Chung-tai .", "prompt_labels": "He(O) had(O) been(O) targeted(O) by(O) the(O) radical(O) democrats(O) ,(O) including(O) Albert(B-politician) Chan(I-politician) of(O) the(O) People(B-political party) Power(I-political party) in(O) the(O) 2011(B-election) Hong(I-election) Kong(I-election) local(I-election) elections(I-election) who(O) opposed(O) Democrats(O) '(O) compromise(O) with(O) the(O) Beijing(B-location) officials(O) on(O) the(O) 2012(O) constitutional(O) reform(O) proposals(O) and(O) in(O) the(O) 2015(B-election) Hong(I-election) Kong(I-election) local(I-election) elections(I-election) by(O) Civic(B-political party) Passion(I-political party) '(O) s(O) Cheng(B-politician) Chung-tai(I-politician) .(O)"}}
{"id": "119", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "political party", "election", "event", "person", "organization", "country", "location"], "instance": {"id": "119", "words": ["Peninsular", "Malaysia", "shares", "a", "land", "and", "maritime", "border", "with", "Thailand", "and", "maritime", "borders", "with", "Singapore", ",", "Vietnam", ",", "and", "Indonesia", "."], "labels": ["B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, political party, election, event, person, organization, country, location and O.\nSentence: Peninsular Malaysia shares a land and maritime border with Thailand and maritime borders with Singapore , Vietnam , and Indonesia .", "prompt_labels": "Peninsular(B-location) Malaysia(I-location) shares(O) a(O) land(O) and(O) maritime(O) border(O) with(O) Thailand(B-country) and(O) maritime(O) borders(O) with(O) Singapore(B-country) ,(O) Vietnam(B-country) ,(O) and(O) Indonesia(B-country) .(O)"}}
{"id": "407", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "political party", "location", "politician", "country", "event", "election", "organization"], "instance": {"id": "407", "words": ["He", "had", "run", "in", "the", "Republican", "primary", "in", "the", "1976", "United", "States", "Senate", "election", "in", "Pennsylvania", ",", "but", "was", "defeated", "by", "John", "Heinz", "and", "also", "ran", "in", "the", "1978", "Pennsylvania", "gubernatorial", "election", ",", "but", "was", "defeated", "by", "Dick", "Thornburgh", "in", "the", "primary", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, location, politician, country, event, election, organization and O.\nSentence: He had run in the Republican primary in the 1976 United States Senate election in Pennsylvania , but was defeated by John Heinz and also ran in the 1978 Pennsylvania gubernatorial election , but was defeated by Dick Thornburgh in the primary .", "prompt_labels": "He(O) had(O) run(O) in(O) the(O) Republican(O) primary(O) in(O) the(O) 1976(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Pennsylvania(I-election) ,(O) but(O) was(O) defeated(O) by(O) John(B-politician) Heinz(I-politician) and(O) also(O) ran(O) in(O) the(O) 1978(B-election) Pennsylvania(I-election) gubernatorial(I-election) election(I-election) ,(O) but(O) was(O) defeated(O) by(O) Dick(B-politician) Thornburgh(I-politician) in(O) the(O) primary(O) .(O)"}}
{"id": "482", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "country", "person", "politician", "political party", "event", "location", "election"], "instance": {"id": "482", "words": ["He", "spurred", "the", "ongoing", "revolt", "in", "Duchy", "of", "Saxony", ",", "which", "had", "forced", "Henry", "IV", "to", "retreat", "from", "that", "region", "(", "he", "crushed", "the", "revolt", "at", "the", "Battle", "of", "Langensalza", "soon", "thereafter", ")", ";", "the", "Polish", "king", "seized", "the", "occasion", "to", "launch", "an", "invasion", "against", "Henry", "IV", "'s", "vassal", ",", "Vratislaus", "II", "of", "Bohemia", ",", "alongside", "an", "ally", "from", "Grand", "Prince", "Vladimir", "II", "Monomakh", "of", "Kiev", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-country", "I-country", "I-country", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "B-person", "I-person", "I-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, person, politician, political party, event, location, election and O.\nSentence: He spurred the ongoing revolt in Duchy of Saxony , which had forced Henry IV to retreat from that region ( he crushed the revolt at the Battle of Langensalza soon thereafter ) ; the Polish king seized the occasion to launch an invasion against Henry IV 's vassal , Vratislaus II of Bohemia , alongside an ally from Grand Prince Vladimir II Monomakh of Kiev .", "prompt_labels": "He(O) spurred(O) the(O) ongoing(O) revolt(O) in(O) Duchy(B-country) of(I-country) Saxony(I-country) ,(O) which(O) had(O) forced(O) Henry(B-person) IV(I-person) to(O) retreat(O) from(O) that(O) region(O) ((O) he(O) crushed(O) the(O) revolt(O) at(O) the(O) Battle(B-event) of(I-event) Langensalza(I-event) soon(O) thereafter(O) )(O) ;(O) the(O) Polish(O) king(O) seized(O) the(O) occasion(O) to(O) launch(O) an(O) invasion(O) against(O) Henry(B-person) IV(I-person) 's(O) vassal(O) ,(O) Vratislaus(B-person) II(I-person) of(I-person) Bohemia(I-person) ,(O) alongside(O) an(O) ally(O) from(O) Grand(O) Prince(O) Vladimir(B-person) II(I-person) Monomakh(I-person) of(O) Kiev(B-location) .(O)"}}
{"id": "42", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "election", "person", "country", "event", "politician", "political party", "location"], "instance": {"id": "42", "words": ["In", "the", "February", "1974", "United", "Kingdom", "general", "election", ",", "however", ",", "the", "Social", "Democratic", "and", "Labour", "Party", "(", "SDLP", ")", "contested", "the", "seat", ",", "dividing", "the", "nationalist", "vote", "and", "allowing", "Harry", "West", "of", "the", "UUP", "to", "win", "with", "the", "support", "of", "the", "Vanguard", "Progressive", "Unionist", "Party", "and", "the", "Democratic", "Unionist", "Party", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, election, person, country, event, politician, political party, location and O.\nSentence: In the February 1974 United Kingdom general election , however , the Social Democratic and Labour Party ( SDLP ) contested the seat , dividing the nationalist vote and allowing Harry West of the UUP to win with the support of the Vanguard Progressive Unionist Party and the Democratic Unionist Party .", "prompt_labels": "In(O) the(O) February(B-election) 1974(I-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) however(O) ,(O) the(O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) ((O) SDLP(B-political party) )(O) contested(O) the(O) seat(O) ,(O) dividing(O) the(O) nationalist(O) vote(O) and(O) allowing(O) Harry(B-politician) West(I-politician) of(O) the(O) UUP(B-political party) to(O) win(O) with(O) the(O) support(O) of(O) the(O) Vanguard(B-political party) Progressive(I-political party) Unionist(I-political party) Party(I-political party) and(O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) .(O)"}}
{"id": "265", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "location", "country", "political party", "person", "organization", "election", "event"], "instance": {"id": "265", "words": ["DeFeria", "also", "ran", "in", "the", "2000", "Canadian", "federal", "election", "in", "the", "riding", "of", "Mississauga", "East", "for", "the", "Progressive", "Conservative", "Party", "of", "Canada", "finishing", "a", "close", "third", "behind", "Jainstien", "Dookie", "of", "the", "Canadian", "Alliance", "and", "the", "winner", "Albina", "Guarnieri", "of", "the", "Liberal", "Party", "of", "Canada", "."], "labels": ["B-politician", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, country, political party, person, organization, election, event and O.\nSentence: DeFeria also ran in the 2000 Canadian federal election in the riding of Mississauga East for the Progressive Conservative Party of Canada finishing a close third behind Jainstien Dookie of the Canadian Alliance and the winner Albina Guarnieri of the Liberal Party of Canada .", "prompt_labels": "DeFeria(B-politician) also(O) ran(O) in(O) the(O) 2000(B-election) Canadian(I-election) federal(I-election) election(I-election) in(O) the(O) riding(O) of(O) Mississauga(B-location) East(I-location) for(O) the(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) finishing(O) a(O) close(O) third(O) behind(O) Jainstien(B-politician) Dookie(I-politician) of(O) the(O) Canadian(B-political party) Alliance(I-political party) and(O) the(O) winner(O) Albina(B-politician) Guarnieri(I-politician) of(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) .(O)"}}
{"id": "151", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "location", "person", "election", "politician", "organization", "event", "political party"], "instance": {"id": "151", "words": ["Before", "the", "2000", "United", "States", "presidential", "election", ",", "West", "Virginia", "had", "been", "won", "by", "the", "Democratic", "nominee", "every", "time", "since", "1932", "(", "except", "for", "the", "Republican", "landslides", "of", "1956", "United", "States", "presidential", "election", ",", "1972", "United", "States", "presidential", "election", ",", "and", "1984", "United", "States", "presidential", "election", ")", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, person, election, politician, organization, event, political party and O.\nSentence: Before the 2000 United States presidential election , West Virginia had been won by the Democratic nominee every time since 1932 ( except for the Republican landslides of 1956 United States presidential election , 1972 United States presidential election , and 1984 United States presidential election ) .", "prompt_labels": "Before(O) the(O) 2000(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) West(B-location) Virginia(I-location) had(O) been(O) won(O) by(O) the(O) Democratic(O) nominee(O) every(O) time(O) since(O) 1932(O) ((O) except(O) for(O) the(O) Republican(O) landslides(O) of(O) 1956(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) 1972(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) and(O) 1984(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) )(O) .(O)"}}
{"id": "15", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "organization", "election", "country", "person", "political party", "politician", "event"], "instance": {"id": "15", "words": ["The", "decline", "was", "evident", "even", "before", "the", "1981", "Northern", "Ireland", "local", "elections", "as", "4", "of", "the", "12", "UUUP", "councillors", "elected", "in", "1977", "Northern", "Ireland", "local", "elections", "had", "defected", "to", "other", "Unionist", "parties", "(", "2", "to", "UUP", ",", "1", "to", "DUP", "and", "1", "to", "the", "Ulster", "Popular", "Unionist", "Party", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-political party", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "O", "O", "B-political party", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, election, country, person, political party, politician, event and O.\nSentence: The decline was evident even before the 1981 Northern Ireland local elections as 4 of the 12 UUUP councillors elected in 1977 Northern Ireland local elections had defected to other Unionist parties ( 2 to UUP , 1 to DUP and 1 to the Ulster Popular Unionist Party ) .", "prompt_labels": "The(O) decline(O) was(O) evident(O) even(O) before(O) the(O) 1981(B-election) Northern(I-election) Ireland(I-election) local(I-election) elections(I-election) as(O) 4(O) of(O) the(O) 12(O) UUUP(B-political party) councillors(O) elected(O) in(O) 1977(B-election) Northern(I-election) Ireland(I-election) local(I-election) elections(I-election) had(O) defected(O) to(O) other(O) Unionist(O) parties(O) ((O) 2(O) to(O) UUP(B-political party) ,(O) 1(O) to(O) DUP(B-political party) and(B-political party) 1(O) to(O) the(O) Ulster(B-political party) Popular(I-political party) Unionist(I-political party) Party(I-political party) )(O) .(O)"}}
{"id": "116", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "person", "country", "political party", "event", "location", "organization", "election"], "instance": {"id": "116", "words": ["A", "Central", "Committee", "was", "elected", ",", "composed", "of", "nine", "members", ":", "Giacomo", "Montalto", "for", "the", "province", "of", "Trapani", ",", "Nicola", "Petrina", "for", "the", "province", "of", "Messina", ",", "Giuseppe", "De", "Felice", "Giuffrida", "for", "the", "province", "of", "Catania", ",", "Luigi", "Leone", "for", "the", "province", "of", "Siracusa", ",", "Antonio", "Licata", "for", "the", "province", "of", "Agrigento", ",", "Agostino", "Lo", "Piano", "Pomar", "for", "the", "province", "of", "Caltanissetta", ",", "Rosario", "Garibaldi", "Bosco", ",", "Nicola", "Barbato", "and", "Bernardino", "Verro", "for", "the", "province", "of", "Palermo", "."], "labels": ["O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-location", "I-location", "I-location", "O", "B-politician", "I-politician", "O", "O", "B-location", "I-location", "I-location", "O", "B-politician", "I-politician", "I-politician", "I-politician", "O", "O", "B-location", "I-location", "I-location", "O", "B-politician", "I-politician", "O", "O", "B-location", "I-location", "I-location", "O", "B-politician", "I-politician", "O", "O", "B-location", "I-location", "I-location", "O", "B-politician", "I-politician", "I-politician", "I-politician", "O", "O", "B-location", "I-location", "I-location", "O", "B-politician", "I-politician", "I-politician", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "O", "B-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, country, political party, event, location, organization, election and O.\nSentence: A Central Committee was elected , composed of nine members : Giacomo Montalto for the province of Trapani , Nicola Petrina for the province of Messina , Giuseppe De Felice Giuffrida for the province of Catania , Luigi Leone for the province of Siracusa , Antonio Licata for the province of Agrigento , Agostino Lo Piano Pomar for the province of Caltanissetta , Rosario Garibaldi Bosco , Nicola Barbato and Bernardino Verro for the province of Palermo .", "prompt_labels": "A(O) Central(B-organization) Committee(I-organization) was(O) elected(O) ,(O) composed(O) of(O) nine(O) members(O) :(O) Giacomo(B-politician) Montalto(I-politician) for(O) the(O) province(B-location) of(I-location) Trapani(I-location) ,(O) Nicola(B-politician) Petrina(I-politician) for(O) the(O) province(B-location) of(I-location) Messina(I-location) ,(O) Giuseppe(B-politician) De(I-politician) Felice(I-politician) Giuffrida(I-politician) for(O) the(O) province(B-location) of(I-location) Catania(I-location) ,(O) Luigi(B-politician) Leone(I-politician) for(O) the(O) province(B-location) of(I-location) Siracusa(I-location) ,(O) Antonio(B-politician) Licata(I-politician) for(O) the(O) province(B-location) of(I-location) Agrigento(I-location) ,(O) Agostino(B-politician) Lo(I-politician) Piano(I-politician) Pomar(I-politician) for(O) the(O) province(B-location) of(I-location) Caltanissetta(I-location) ,(O) Rosario(B-politician) Garibaldi(I-politician) Bosco(I-politician) ,(O) Nicola(B-politician) Barbato(I-politician) and(O) Bernardino(B-politician) Verro(I-politician) for(O) the(O) province(B-location) of(I-location) Palermo(I-location) .(O)"}}
{"id": "232", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "location", "event", "person", "political party", "country", "politician", "election"], "instance": {"id": "232", "words": ["Samis", "was", "re-elected", "for", "the", "redistributed", "constituency", "of", "Cornwall", "in", "1975", "Ontario", "general", "election", ",", "1977", "Ontario", "general", "election", "and", "1981", "Ontario", "general", "election", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, event, person, political party, country, politician, election and O.\nSentence: Samis was re-elected for the redistributed constituency of Cornwall in 1975 Ontario general election , 1977 Ontario general election and 1981 Ontario general election .", "prompt_labels": "Samis(B-politician) was(O) re-elected(O) for(O) the(O) redistributed(O) constituency(O) of(O) Cornwall(B-location) in(O) 1975(B-election) Ontario(I-election) general(I-election) election(I-election) ,(O) 1977(B-election) Ontario(I-election) general(I-election) election(I-election) and(O) 1981(B-election) Ontario(I-election) general(I-election) election(I-election) .(O)"}}
{"id": "353", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "country", "politician", "event", "political party", "location", "election", "person"], "instance": {"id": "353", "words": ["Campbell", "'s", "Progressive", "Conservatives", "and", "McLaughlin", "'s", "New", "Democratic", "Party", "were", "decimated", "in", "1993", ",", "both", "failing", "to", "reach", "official", "party", "status", ",", "and", "Lyn", "McLeod", "'", "s", "Ontario", "Liberal", "Party", "lost", "the", "1995", "Ontario", "general", "election", "despite", "having", "more", "than", "a", "10", "per", "cent", "lead", "in", "the", "polls", "when", "the", "election", "was", "called", "."], "labels": ["B-politician", "O", "B-political party", "I-political party", "O", "B-politician", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, politician, event, political party, location, election, person and O.\nSentence: Campbell 's Progressive Conservatives and McLaughlin 's New Democratic Party were decimated in 1993 , both failing to reach official party status , and Lyn McLeod ' s Ontario Liberal Party lost the 1995 Ontario general election despite having more than a 10 per cent lead in the polls when the election was called .", "prompt_labels": "Campbell(B-politician) 's(O) Progressive(B-political party) Conservatives(I-political party) and(O) McLaughlin(B-politician) 's(O) New(B-political party) Democratic(I-political party) Party(I-political party) were(O) decimated(O) in(O) 1993(O) ,(O) both(O) failing(O) to(O) reach(O) official(O) party(O) status(O) ,(O) and(O) Lyn(B-politician) McLeod(I-politician) '(O) s(O) Ontario(B-political party) Liberal(I-political party) Party(I-political party) lost(O) the(O) 1995(B-election) Ontario(I-election) general(I-election) election(I-election) despite(O) having(O) more(O) than(O) a(O) 10(O) per(O) cent(O) lead(O) in(O) the(O) polls(O) when(O) the(O) election(O) was(O) called(O) .(O)"}}
{"id": "227", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "election", "person", "organization", "politician", "location", "country", "political party"], "instance": {"id": "227", "words": ["Hanni", "served", "as", "leader", "of", "the", "Reform", "Party", "of", "British", "Columbia", "from", "August", "30", ",", "1997", "to", "June", "1998", ",", "and", "later", "as", "leader", "of", "the", "British", "Columbia", "Party", ",", "and", "the", "British", "Columbia", "Conservative", "Party", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, person, organization, politician, location, country, political party and O.\nSentence: Hanni served as leader of the Reform Party of British Columbia from August 30 , 1997 to June 1998 , and later as leader of the British Columbia Party , and the British Columbia Conservative Party .", "prompt_labels": "Hanni(B-politician) served(O) as(O) leader(O) of(O) the(O) Reform(B-political party) Party(I-political party) of(I-political party) British(I-political party) Columbia(I-political party) from(O) August(O) 30(O) ,(O) 1997(O) to(O) June(O) 1998(O) ,(O) and(O) later(O) as(O) leader(O) of(O) the(O) British(B-political party) Columbia(I-political party) Party(I-political party) ,(O) and(O) the(O) British(B-political party) Columbia(I-political party) Conservative(I-political party) Party(I-political party) .(O)"}}
{"id": "480", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "location", "country", "politician", "organization", "person", "political party", "election"], "instance": {"id": "480", "words": ["The", "Greek", "Civil", "War", "(", "o", "Emfýlios", ",", "the", "Civil", "War", ")", "was", "a", "civil", "war", "in", "Greece", "fought", "between", "the", "Greek", "government", "army", "(", "supported", "by", "the", "United", "Kingdom", "and", "the", "United", "States", ")", "and", "the", "Democratic", "Army", "of", "Greece", "(", "DSE", ")", "-", "the", "military", "branch", "of", "the", "Communist", "Party", "of", "Greece", "(", "supported", "by", "Yugoslavia", ",", "Albania", "and", "Bulgaria", ")", "from", "1946", "to", "1949", "."], "labels": ["O", "B-event", "I-event", "I-event", "O", "B-event", "I-event", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, country, politician, organization, person, political party, election and O.\nSentence: The Greek Civil War ( o Emfýlios , the Civil War ) was a civil war in Greece fought between the Greek government army ( supported by the United Kingdom and the United States ) and the Democratic Army of Greece ( DSE ) - the military branch of the Communist Party of Greece ( supported by Yugoslavia , Albania and Bulgaria ) from 1946 to 1949 .", "prompt_labels": "The(O) Greek(B-event) Civil(I-event) War(I-event) ((O) o(B-event) Emfýlios(I-event) ,(O) the(O) Civil(B-event) War(I-event) )(O) was(O) a(O) civil(O) war(O) in(O) Greece(B-country) fought(O) between(O) the(O) Greek(O) government(O) army(O) ((O) supported(O) by(O) the(O) United(B-country) Kingdom(I-country) and(O) the(O) United(B-country) States(I-country) )(O) and(O) the(O) Democratic(O) Army(O) of(O) Greece(O) ((O) DSE(O) )(O) -(O) the(O) military(O) branch(O) of(O) the(O) Communist(B-political party) Party(I-political party) of(I-political party) Greece(I-political party) ((O) supported(O) by(O) Yugoslavia(B-country) ,(O) Albania(B-country) and(O) Bulgaria(B-country) )(O) from(O) 1946(O) to(O) 1949(O) .(O)"}}
{"id": "208", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "event", "person", "politician", "location", "election", "organization", "country"], "instance": {"id": "208", "words": ["In", "the", "negotiations", "to", "form", "a", "coalition", "government", "with", "the", "Christian", "Democrats", "-", "both", "the", "Christian", "Democratic", "Union", "of", "Germany", "(", "CDU", ")", "and", "the", "Christian", "Social", "Union", "in", "Bavaria", "(", "CSU", ")", "-", "and", "the", "Free", "Democratic", "Party", "(", "FDP", ")", "following", "the", "2017", "German", "federal", "election", ",", "Bütikofer", "is", "currently", "part", "of", "the", "14-member", "delegation", "of", "the", "Green", "Party", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, person, politician, location, election, organization, country and O.\nSentence: In the negotiations to form a coalition government with the Christian Democrats - both the Christian Democratic Union of Germany ( CDU ) and the Christian Social Union in Bavaria ( CSU ) - and the Free Democratic Party ( FDP ) following the 2017 German federal election , Bütikofer is currently part of the 14-member delegation of the Green Party .", "prompt_labels": "In(O) the(O) negotiations(O) to(O) form(O) a(O) coalition(O) government(O) with(O) the(O) Christian(O) Democrats(O) -(O) both(O) the(O) Christian(B-political party) Democratic(I-political party) Union(I-political party) of(I-political party) Germany(I-political party) ((O) CDU(B-political party) )(O) and(O) the(O) Christian(B-political party) Social(I-political party) Union(I-political party) in(I-political party) Bavaria(I-political party) ((O) CSU(B-political party) )(O) -(O) and(O) the(O) Free(B-political party) Democratic(I-political party) Party(I-political party) ((O) FDP(B-political party) )(O) following(O) the(O) 2017(B-election) German(I-election) federal(I-election) election(I-election) ,(O) Bütikofer(B-politician) is(O) currently(O) part(O) of(O) the(O) 14-member(O) delegation(O) of(O) the(O) Green(B-political party) Party(I-political party) .(O)"}}
{"id": "311", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "person", "political party", "organization", "event", "location", "election", "country"], "instance": {"id": "311", "words": ["In", "2009", "Queensland", "state", "election", ",", "she", "became", "the", "first", "woman", "in", "Australia", "to", "be", "elected", "Premier", ",", "though", "she", "subsequently", "suffered", "a", "landslide", "loss", "to", "Campbell", "Newman", "'", "s", "Liberal", "National", "Party", "of", "Queensland", "in", "2012", "Queensland", "state", "election", "."], "labels": ["O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, political party, organization, event, location, election, country and O.\nSentence: In 2009 Queensland state election , she became the first woman in Australia to be elected Premier , though she subsequently suffered a landslide loss to Campbell Newman ' s Liberal National Party of Queensland in 2012 Queensland state election .", "prompt_labels": "In(O) 2009(B-election) Queensland(I-election) state(I-election) election(I-election) ,(O) she(O) became(O) the(O) first(O) woman(O) in(O) Australia(B-country) to(O) be(O) elected(O) Premier(O) ,(O) though(O) she(O) subsequently(O) suffered(O) a(O) landslide(O) loss(O) to(O) Campbell(B-politician) Newman(I-politician) '(O) s(O) Liberal(B-political party) National(I-political party) Party(I-political party) of(I-political party) Queensland(I-political party) in(O) 2012(B-election) Queensland(I-election) state(I-election) election(I-election) .(O)"}}
{"id": "231", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "location", "person", "organization", "politician", "election", "country", "event"], "instance": {"id": "231", "words": ["(", "Derby-Lewis", "later", "served", "a", "life", "sentence", "for", "conspiracy", "to", "murder", "Chris", "Hani", ",", "a", "leader", "of", "the", "South", "African", "Communist", "Party", "and", "of", "the", "African", "National", "Congress", "s", "Umkhonto", "we", "Sizwe", ",", "the", "armed", "wing", "of", "the", "African", "National", "Congress", ",", "who", "was", "assassinated", "in", "1993", ")", "."], "labels": ["O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, location, person, organization, politician, election, country, event and O.\nSentence: ( Derby-Lewis later served a life sentence for conspiracy to murder Chris Hani , a leader of the South African Communist Party and of the African National Congress s Umkhonto we Sizwe , the armed wing of the African National Congress , who was assassinated in 1993 ) .", "prompt_labels": "((O) Derby-Lewis(B-person) later(O) served(O) a(O) life(O) sentence(O) for(O) conspiracy(O) to(O) murder(O) Chris(B-politician) Hani(I-politician) ,(O) a(O) leader(O) of(O) the(O) South(B-political party) African(I-political party) Communist(I-political party) Party(I-political party) and(O) of(O) the(O) African(B-political party) National(I-political party) Congress(I-political party) s(O) Umkhonto(B-organization) we(I-organization) Sizwe(I-organization) ,(O) the(O) armed(O) wing(O) of(O) the(O) African(B-political party) National(I-political party) Congress(I-political party) ,(O) who(O) was(O) assassinated(O) in(O) 1993(O) )(O) .(O)"}}
{"id": "268", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "election", "person", "politician", "organization", "event", "country", "political party"], "instance": {"id": "268", "words": ["Michael", "Bloomberg", ",", "formerly", "a", "Democrat", ",", "was", "elected", "as", "a", "Republican", "in", "2001", "New", "York", "City", "mayoral", "election", "and", "2005", "New", "York", "City", "mayoral", "election", ",", "succeeding", "another", "Republican", "mayor", ",", "Rudy", "Giuliani", ",", "elected", "in", "1993", "and", "1997", "New", "York", "City", "mayoral", "election", "."], "labels": ["B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, election, person, politician, organization, event, country, political party and O.\nSentence: Michael Bloomberg , formerly a Democrat , was elected as a Republican in 2001 New York City mayoral election and 2005 New York City mayoral election , succeeding another Republican mayor , Rudy Giuliani , elected in 1993 and 1997 New York City mayoral election .", "prompt_labels": "Michael(B-politician) Bloomberg(I-politician) ,(O) formerly(O) a(O) Democrat(O) ,(O) was(O) elected(O) as(O) a(O) Republican(O) in(O) 2001(B-election) New(I-election) York(I-election) City(I-election) mayoral(I-election) election(I-election) and(O) 2005(B-election) New(I-election) York(I-election) City(I-election) mayoral(I-election) election(I-election) ,(O) succeeding(O) another(O) Republican(O) mayor(O) ,(O) Rudy(B-politician) Giuliani(I-politician) ,(O) elected(O) in(O) 1993(B-election) and(I-election) 1997(I-election) New(I-election) York(I-election) City(I-election) mayoral(I-election) election(I-election) .(O)"}}
{"id": "152", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "country", "event", "organization", "political party", "politician", "person", "location"], "instance": {"id": "152", "words": ["Molloy", "ran", "for", "the", "Canadian", "House", "of", "Commons", "in", "the", "1921", "Canadian", "federal", "election", "as", "a", "candidate", "of", "the", "Liberal", "Party", "of", "Canada", ",", "and", "lost", "to", "Progressive", "Party", "of", "Canada", "candidate", "Robert", "Alexander", "Hoey", "by", "1,397", "votes", "in", "the", "riding", "of", "Springfield", "."], "labels": ["B-politician", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, country, event, organization, political party, politician, person, location and O.\nSentence: Molloy ran for the Canadian House of Commons in the 1921 Canadian federal election as a candidate of the Liberal Party of Canada , and lost to Progressive Party of Canada candidate Robert Alexander Hoey by 1,397 votes in the riding of Springfield .", "prompt_labels": "Molloy(B-politician) ran(O) for(O) the(O) Canadian(B-organization) House(I-organization) of(I-organization) Commons(I-organization) in(O) the(O) 1921(B-election) Canadian(I-election) federal(I-election) election(I-election) as(O) a(O) candidate(O) of(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) ,(O) and(O) lost(O) to(O) Progressive(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) candidate(O) Robert(B-politician) Alexander(I-politician) Hoey(I-politician) by(O) 1,397(O) votes(O) in(O) the(O) riding(O) of(O) Springfield(B-location) .(O)"}}
{"id": "368", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "organization", "politician", "person", "country", "event", "location", "political party"], "instance": {"id": "368", "words": ["Bernier", "was", "born", "in", "Saint-Georges", ",", "Quebec", ",", "the", "son", "of", "Doris", "(", "Rodrigue", ")", "and", "Gilles", "Bernier", ",", "a", "well", "known", "radio", "host", ",", "who", "represented", "the", "riding", "of", "Beauce", "from", "1984", "Canadian", "federal", "election", "to", "1997", "Canadian", "federal", "election", ",", "first", "as", "a", "Progressive", "Conservative", "Party", "of", "Canada", "and", "then", "as", "an", "independent", "."], "labels": ["B-person", "O", "O", "O", "B-location", "O", "B-location", "O", "O", "O", "O", "B-person", "O", "B-person", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, politician, person, country, event, location, political party and O.\nSentence: Bernier was born in Saint-Georges , Quebec , the son of Doris ( Rodrigue ) and Gilles Bernier , a well known radio host , who represented the riding of Beauce from 1984 Canadian federal election to 1997 Canadian federal election , first as a Progressive Conservative Party of Canada and then as an independent .", "prompt_labels": "Bernier(B-person) was(O) born(O) in(O) Saint-Georges(B-location) ,(O) Quebec(B-location) ,(O) the(O) son(O) of(O) Doris(B-person) ((O) Rodrigue(B-person) )(O) and(O) Gilles(B-person) Bernier(I-person) ,(O) a(O) well(O) known(O) radio(O) host(O) ,(O) who(O) represented(O) the(O) riding(O) of(O) Beauce(B-location) from(O) 1984(B-election) Canadian(I-election) federal(I-election) election(I-election) to(O) 1997(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) first(O) as(O) a(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) and(O) then(O) as(O) an(O) independent(O) .(O)"}}
{"id": "519", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "political party", "country", "politician", "election", "person", "location", "organization"], "instance": {"id": "519", "words": ["U.S.", "President", "George", "W.", "Bush", "also", "set", "his", "hopes", "on", "leaders", "of", "the", "summit", "to", "back", "the", "2008", "G-20", "Washington", "summit", "and", "declaration", "to", "the", "financial", "crisis", "of", "2007-2008", "."], "labels": ["B-country", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, country, politician, election, person, location, organization and O.\nSentence: U.S. President George W. Bush also set his hopes on leaders of the summit to back the 2008 G-20 Washington summit and declaration to the financial crisis of 2007-2008 .", "prompt_labels": "U.S.(B-country) President(O) George(B-politician) W.(I-politician) Bush(I-politician) also(O) set(O) his(O) hopes(O) on(O) leaders(O) of(O) the(O) summit(O) to(O) back(O) the(O) 2008(B-event) G-20(I-event) Washington(I-event) summit(I-event) and(O) declaration(O) to(O) the(O) financial(B-event) crisis(I-event) of(I-event) 2007-2008(I-event) .(O)"}}
{"id": "357", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "country", "election", "politician", "person", "organization", "political party", "event"], "instance": {"id": "357", "words": ["At", "the", "time", "of", "its", "operations", ",", "the", "group", "was", "composed", "of", "businessmen", "and", "political", "organizers", "of", "three", "federalist", "political", "parties", "-", "the", "Liberal", "Party", "of", "Canada", ",", "the", "Quebec", "Liberal", "Party", "and", "the", "Progressive", "Conservative", "Party", "of", "Canada", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, election, politician, person, organization, political party, event and O.\nSentence: At the time of its operations , the group was composed of businessmen and political organizers of three federalist political parties - the Liberal Party of Canada , the Quebec Liberal Party and the Progressive Conservative Party of Canada .", "prompt_labels": "At(O) the(O) time(O) of(O) its(O) operations(O) ,(O) the(O) group(O) was(O) composed(O) of(O) businessmen(O) and(O) political(O) organizers(O) of(O) three(O) federalist(O) political(O) parties(O) -(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) ,(O) the(O) Quebec(B-political party) Liberal(I-political party) Party(I-political party) and(O) the(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) .(O)"}}
{"id": "147", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "election", "event", "person", "location", "country", "organization", "politician"], "instance": {"id": "147", "words": ["In", "the", "1969", "Malaysian", "general", "election", ",", "MCA", "lost", "more", "than", "half", "its", "seats", "to", "the", "new", ",", "mainly", "Chinese", "Malaysian", ",", "opposition", "parties", "Democratic", "Action", "Party", "(", "DAP", ")", "and", "Parti", "Gerakan", "Rakyat", "Malaysia", "(", "Gerakan", ")", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, election, event, person, location, country, organization, politician and O.\nSentence: In the 1969 Malaysian general election , MCA lost more than half its seats to the new , mainly Chinese Malaysian , opposition parties Democratic Action Party ( DAP ) and Parti Gerakan Rakyat Malaysia ( Gerakan ) .", "prompt_labels": "In(O) the(O) 1969(B-election) Malaysian(I-election) general(I-election) election(I-election) ,(O) MCA(B-political party) lost(O) more(O) than(O) half(O) its(O) seats(O) to(O) the(O) new(O) ,(O) mainly(O) Chinese(O) Malaysian(O) ,(O) opposition(O) parties(O) Democratic(B-political party) Action(I-political party) Party(I-political party) ((O) DAP(B-political party) )(O) and(O) Parti(B-political party) Gerakan(I-political party) Rakyat(I-political party) Malaysia(I-political party) ((O) Gerakan(B-political party) )(O) .(O)"}}
{"id": "141", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "organization", "political party", "event", "politician", "location", "person", "election"], "instance": {"id": "141", "words": ["During", "her", "visit", "she", "met", "representatives", "from", "the", "National", "Center", "for", "Transgender", "Equality", ",", "the", "National", "Association", "of", "LGBT", "Community", "Centers", ",", "the", "National", "Gay", "and", "Lesbian", "Task", "Force", ",", "Freedom", "to", "Marry", "and", "the", "Stonewall", "Democrats", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, political party, event, politician, location, person, election and O.\nSentence: During her visit she met representatives from the National Center for Transgender Equality , the National Association of LGBT Community Centers , the National Gay and Lesbian Task Force , Freedom to Marry and the Stonewall Democrats .", "prompt_labels": "During(O) her(O) visit(O) she(O) met(O) representatives(O) from(O) the(O) National(B-organization) Center(I-organization) for(I-organization) Transgender(I-organization) Equality(I-organization) ,(O) the(O) National(B-organization) Association(I-organization) of(I-organization) LGBT(I-organization) Community(I-organization) Centers(I-organization) ,(O) the(O) National(B-organization) Gay(I-organization) and(I-organization) Lesbian(I-organization) Task(I-organization) Force(I-organization) ,(O) Freedom(B-organization) to(I-organization) Marry(I-organization) and(O) the(O) Stonewall(B-organization) Democrats(I-organization) .(O)"}}
{"id": "358", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "politician", "country", "person", "organization", "location", "election", "event"], "instance": {"id": "358", "words": ["No", "attempts", "were", "made", "during", "the", "1903", "Australian", "federal", "election", ",", "owing", "to", "the", "House", "seats", "split", "almost", "evenly", "between", "the", "Protectionist", "Party", ",", "the", "Free", "Trade", "Party", "and", "the", "new", "Labour", "Party", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, country, person, organization, location, election, event and O.\nSentence: No attempts were made during the 1903 Australian federal election , owing to the House seats split almost evenly between the Protectionist Party , the Free Trade Party and the new Labour Party .", "prompt_labels": "No(O) attempts(O) were(O) made(O) during(O) the(O) 1903(B-election) Australian(I-election) federal(I-election) election(I-election) ,(O) owing(O) to(O) the(O) House(O) seats(O) split(O) almost(O) evenly(O) between(O) the(O) Protectionist(B-political party) Party(I-political party) ,(O) the(O) Free(B-political party) Trade(I-political party) Party(I-political party) and(O) the(O) new(O) Labour(B-political party) Party(I-political party) .(O)"}}
{"id": "211", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "person", "event", "political party", "country", "organization", "election", "location"], "instance": {"id": "211", "words": ["He", "campaigned", "for", "the", "House", "of", "Commons", "of", "Canada", "in", "the", "1949", "Manitoba", "general", "election", "as", "a", "candidate", "of", "the", "Progressive", "Conservative", "Party", "of", "Canada", ",", "and", "finished", "second", "against", "Liberal", "Party", "of", "Canada", "candidate", "William", "Gilbert", "Weir", "in", "Portage", "-", "Neepawa", "."], "labels": ["O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "I-politician", "O", "B-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, event, political party, country, organization, election, location and O.\nSentence: He campaigned for the House of Commons of Canada in the 1949 Manitoba general election as a candidate of the Progressive Conservative Party of Canada , and finished second against Liberal Party of Canada candidate William Gilbert Weir in Portage - Neepawa .", "prompt_labels": "He(O) campaigned(O) for(O) the(O) House(B-organization) of(I-organization) Commons(I-organization) of(I-organization) Canada(I-organization) in(O) the(O) 1949(B-election) Manitoba(I-election) general(I-election) election(I-election) as(O) a(O) candidate(O) of(O) the(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) ,(O) and(O) finished(O) second(O) against(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) candidate(O) William(B-politician) Gilbert(I-politician) Weir(I-politician) in(O) Portage(B-location) -(I-location) Neepawa(I-location) .(O)"}}
{"id": "70", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "location", "political party", "politician", "country", "event", "person", "election"], "instance": {"id": "70", "words": ["President", "Kim", "Dae-jung", "'", "s", "National", "Congress", "for", "New", "Politics", "(", "NCNP", ")", "re-branded", "itself", "to", "Millennium", "Democratic", "Party", "(", "MDP", ")", "in", "2000", ",", "but", "was", "struggling", "as", "it", "had", "defeated", "by", "the", "Liberty", "Korea", "Party", "(", "GNP", ")", "both", "the", "2000", "South", "Korean", "legislative", "election", "and", "2002", "South", "Korean", "local", "elections", "."], "labels": ["O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, political party, politician, country, event, person, election and O.\nSentence: President Kim Dae-jung ' s National Congress for New Politics ( NCNP ) re-branded itself to Millennium Democratic Party ( MDP ) in 2000 , but was struggling as it had defeated by the Liberty Korea Party ( GNP ) both the 2000 South Korean legislative election and 2002 South Korean local elections .", "prompt_labels": "President(O) Kim(B-politician) Dae-jung(I-politician) '(O) s(O) National(B-political party) Congress(I-political party) for(I-political party) New(I-political party) Politics(I-political party) ((O) NCNP(B-political party) )(O) re-branded(O) itself(O) to(O) Millennium(B-political party) Democratic(I-political party) Party(I-political party) ((O) MDP(B-political party) )(O) in(O) 2000(O) ,(O) but(O) was(O) struggling(O) as(O) it(O) had(O) defeated(O) by(O) the(O) Liberty(B-political party) Korea(I-political party) Party(I-political party) ((O) GNP(B-political party) )(O) both(O) the(O) 2000(B-election) South(I-election) Korean(I-election) legislative(I-election) election(I-election) and(O) 2002(B-election) South(I-election) Korean(I-election) local(I-election) elections(I-election) .(O)"}}
{"id": "337", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "politician", "political party", "organization", "election", "country", "location", "person"], "instance": {"id": "337", "words": ["He", "ran", "in", "the", "2012", "Quebec", "general", "election", "for", "the", "Parti", "Québécois", "in", "Verdun", ",", "losing", "to", "incumbent", "Henri-François", "Gautrin", "of", "the", "Quebec", "Liberal", "Party", "."], "labels": ["O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "B-political party", "I-political party", "O", "B-location", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, politician, political party, organization, election, country, location, person and O.\nSentence: He ran in the 2012 Quebec general election for the Parti Québécois in Verdun , losing to incumbent Henri-François Gautrin of the Quebec Liberal Party .", "prompt_labels": "He(O) ran(O) in(O) the(O) 2012(B-election) Quebec(I-election) general(I-election) election(I-election) for(O) the(O) Parti(B-political party) Québécois(I-political party) in(O) Verdun(B-location) ,(O) losing(O) to(O) incumbent(O) Henri-François(B-politician) Gautrin(I-politician) of(O) the(O) Quebec(B-political party) Liberal(I-political party) Party(I-political party) .(O)"}}
{"id": "202", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "politician", "election", "organization", "event", "political party", "location", "country"], "instance": {"id": "202", "words": ["She", "was", "re-elected", "to", "additional", "terms", "in", "1974", "Australian", "federal", "election", ",", "1975", "Australian", "federal", "election", ",", "and", "1980", "Australian", "federal", "election", ",", "retiring", "on", "5", "June", "1987", "at", "the", "end", "of", "her", "final", "term", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, election, organization, event, political party, location, country and O.\nSentence: She was re-elected to additional terms in 1974 Australian federal election , 1975 Australian federal election , and 1980 Australian federal election , retiring on 5 June 1987 at the end of her final term .", "prompt_labels": "She(O) was(O) re-elected(O) to(O) additional(O) terms(O) in(O) 1974(B-election) Australian(I-election) federal(I-election) election(I-election) ,(O) 1975(B-election) Australian(I-election) federal(I-election) election(I-election) ,(O) and(O) 1980(B-election) Australian(I-election) federal(I-election) election(I-election) ,(O) retiring(O) on(O) 5(O) June(O) 1987(O) at(O) the(O) end(O) of(O) her(O) final(O) term(O) .(O)"}}
{"id": "261", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "political party", "organization", "location", "event", "politician", "election", "person"], "instance": {"id": "261", "words": ["Since", "2008", "it", "was", "part", "of", "the", "Sammarinese", "Union", "of", "Moderates", "together", "with", "Sammarinese", "Populars", "and", "stood", "in", "opposition", "to", "the", "2006-2008", "coalition", "government", "consisting", "of", "the", "Party", "of", "Socialists", "and", "Democrats", ",", "the", "Popular", "Alliance", "and", "the", "United", "Left", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, organization, location, event, politician, election, person and O.\nSentence: Since 2008 it was part of the Sammarinese Union of Moderates together with Sammarinese Populars and stood in opposition to the 2006-2008 coalition government consisting of the Party of Socialists and Democrats , the Popular Alliance and the United Left .", "prompt_labels": "Since(O) 2008(O) it(O) was(O) part(O) of(O) the(O) Sammarinese(B-political party) Union(I-political party) of(I-political party) Moderates(I-political party) together(O) with(O) Sammarinese(B-political party) Populars(I-political party) and(O) stood(O) in(O) opposition(O) to(O) the(O) 2006-2008(O) coalition(O) government(O) consisting(O) of(O) the(O) Party(B-political party) of(I-political party) Socialists(I-political party) and(I-political party) Democrats(I-political party) ,(O) the(O) Popular(B-political party) Alliance(I-political party) and(O) the(O) United(B-organization) Left(I-organization) .(O)"}}
{"id": "451", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "politician", "election", "political party", "organization", "country", "location", "person"], "instance": {"id": "451", "words": ["Alongside", "the", "initial", "series", "announcement", ",", "it", "was", "confirmed", "that", "Ben", "Platt", "would", "star", "in", "the", "series", "and", "that", "Barbra", "Streisand", "and", "Gwyneth", "Paltrow", "were", "in", "talks", "to", "join", "as", "series", "regulars", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, politician, election, political party, organization, country, location, person and O.\nSentence: Alongside the initial series announcement , it was confirmed that Ben Platt would star in the series and that Barbra Streisand and Gwyneth Paltrow were in talks to join as series regulars .", "prompt_labels": "Alongside(O) the(O) initial(O) series(O) announcement(O) ,(O) it(O) was(O) confirmed(O) that(O) Ben(B-person) Platt(I-person) would(O) star(O) in(O) the(O) series(O) and(O) that(O) Barbra(B-person) Streisand(I-person) and(O) Gwyneth(B-person) Paltrow(I-person) were(O) in(O) talks(O) to(O) join(O) as(O) series(O) regulars(O) .(O)"}}
{"id": "462", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "politician", "person", "political party", "country", "event", "election", "location"], "instance": {"id": "462", "words": ["The", "audiobook", "version", "features", "Jim", "Parsons", ",", "Jesse", "Tyler", "Ferguson", ",", "Jeff", "Garlin", ",", "Ellie", "Kemper", ",", "John", "Lithgow", ",", "Jack", "McBrayer", ",", "and", "RuPaul", "."], "labels": ["O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, person, political party, country, event, election, location and O.\nSentence: The audiobook version features Jim Parsons , Jesse Tyler Ferguson , Jeff Garlin , Ellie Kemper , John Lithgow , Jack McBrayer , and RuPaul .", "prompt_labels": "The(O) audiobook(O) version(O) features(O) Jim(B-person) Parsons(I-person) ,(O) Jesse(B-person) Tyler(I-person) Ferguson(I-person) ,(O) Jeff(B-person) Garlin(I-person) ,(O) Ellie(B-person) Kemper(I-person) ,(O) John(B-person) Lithgow(I-person) ,(O) Jack(B-person) McBrayer(I-person) ,(O) and(O) RuPaul(B-person) .(O)"}}
{"id": "400", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "person", "organization", "event", "location", "country", "political party", "election"], "instance": {"id": "400", "words": ["The", "tablets", "quote", "the", "Vermont", "Constitution", ",", "Ethan", "Allen", ",", "Calvin", "Coolidge", ",", "George", "Aiken", ",", "Warren", "Austin", ",", "and", "Dorothy", "Canfield", "Fisher", "among", "others", "."], "labels": ["O", "O", "O", "O", "B-location", "O", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, organization, event, location, country, political party, election and O.\nSentence: The tablets quote the Vermont Constitution , Ethan Allen , Calvin Coolidge , George Aiken , Warren Austin , and Dorothy Canfield Fisher among others .", "prompt_labels": "The(O) tablets(O) quote(O) the(O) Vermont(B-location) Constitution(O) ,(O) Ethan(B-politician) Allen(I-politician) ,(O) Calvin(B-politician) Coolidge(I-politician) ,(O) George(B-politician) Aiken(I-politician) ,(O) Warren(B-politician) Austin(I-politician) ,(O) and(O) Dorothy(B-person) Canfield(I-person) Fisher(I-person) among(O) others(O) .(O)"}}
{"id": "526", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "event", "person", "location", "organization", "politician", "political party", "country"], "instance": {"id": "526", "words": ["The", "National", "War", "Labor", "Board", ",", "commonly", "the", "War", "Labor", "Board", "(", "NWLB", "or", "WLB", ")", "was", "an", "agency", "of", "the", "United", "States", "government", "established", "January", "12", ",", "1942", "by", "executive", "order", "to", "mediate", "labor", "disputes", "during", "World", "War", "II", "."], "labels": ["O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, person, location, organization, politician, political party, country and O.\nSentence: The National War Labor Board , commonly the War Labor Board ( NWLB or WLB ) was an agency of the United States government established January 12 , 1942 by executive order to mediate labor disputes during World War II .", "prompt_labels": "The(O) National(B-organization) War(I-organization) Labor(I-organization) Board(I-organization) ,(O) commonly(O) the(O) War(B-organization) Labor(I-organization) Board(I-organization) ((O) NWLB(B-organization) or(O) WLB(B-organization) )(O) was(O) an(O) agency(O) of(O) the(O) United(B-country) States(I-country) government(O) established(O) January(O) 12(O) ,(O) 1942(O) by(O) executive(O) order(O) to(O) mediate(O) labor(O) disputes(O) during(O) World(B-event) War(I-event) II(I-event) .(O)"}}
{"id": "126", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "election", "politician", "country", "organization", "event", "person", "location"], "instance": {"id": "126", "words": ["For", "the", "2018", "Pakistani", "general", "election", ",", "PML-F", "lead", "a", "new", "coalition", "named", "Grand", "Democratic", "Alliance", "with", "Awami", "Tahreek", ",", "National", "Peoples", "Party", ",", "Pakistan", "Peoples", "Party", "Workers", "and", "Pakistan", "Peoples", "Muslim", "League", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-political party", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, election, politician, country, organization, event, person, location and O.\nSentence: For the 2018 Pakistani general election , PML-F lead a new coalition named Grand Democratic Alliance with Awami Tahreek , National Peoples Party , Pakistan Peoples Party Workers and Pakistan Peoples Muslim League .", "prompt_labels": "For(O) the(O) 2018(B-election) Pakistani(I-election) general(I-election) election(I-election) ,(O) PML-F(B-political party) lead(O) a(O) new(O) coalition(O) named(O) Grand(B-political party) Democratic(I-political party) Alliance(I-political party) with(O) Awami(B-political party) Tahreek(I-political party) ,(O) National(B-political party) Peoples(I-political party) Party(I-political party) ,(O) Pakistan(B-political party) Peoples(I-political party) Party(I-political party) Workers(I-political party) and(O) Pakistan(B-political party) Peoples(I-political party) Muslim(I-political party) League(I-political party) .(O)"}}
{"id": "315", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "political party", "organization", "event", "person", "politician", "location", "country"], "instance": {"id": "315", "words": ["He", "won", "election", "again", "in", "2000", "Canadian", "federal", "election", "as", "a", "Canadian", "Alliance", "MP", "with", "60", "%", "of", "the", "vote", "and", "won", "his", "fourth", "straight", "victory", "in", "the", "2004", "Canadian", "federal", "election", ",", "this", "time", "as", "a", "Conservative", "Party", "of", "Canada", "."], "labels": ["O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, organization, event, person, politician, location, country and O.\nSentence: He won election again in 2000 Canadian federal election as a Canadian Alliance MP with 60 % of the vote and won his fourth straight victory in the 2004 Canadian federal election , this time as a Conservative Party of Canada .", "prompt_labels": "He(O) won(O) election(O) again(O) in(O) 2000(B-election) Canadian(I-election) federal(I-election) election(I-election) as(O) a(O) Canadian(B-political party) Alliance(I-political party) MP(O) with(O) 60(O) %(O) of(O) the(O) vote(O) and(O) won(O) his(O) fourth(O) straight(O) victory(O) in(O) the(O) 2004(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) this(O) time(O) as(O) a(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) .(O)"}}
{"id": "493", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "election", "person", "location", "country", "organization", "event", "political party"], "instance": {"id": "493", "words": ["He", "has", "also", "seen", "success", "as", "a", "political", "strategist", ",", "acting", "as", "campaign", "director", "for", "the", "successful", "NOtoAV", "campaign", "in", "the", "2011", "Alternative", "Vote", "referendum", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, person, location, country, organization, event, political party and O.\nSentence: He has also seen success as a political strategist , acting as campaign director for the successful NOtoAV campaign in the 2011 Alternative Vote referendum .", "prompt_labels": "He(O) has(O) also(O) seen(O) success(O) as(O) a(O) political(O) strategist(O) ,(O) acting(O) as(O) campaign(O) director(O) for(O) the(O) successful(O) NOtoAV(B-organization) campaign(O) in(O) the(O) 2011(B-event) Alternative(I-event) Vote(I-event) referendum(I-event) .(O)"}}
{"id": "84", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "election", "organization", "country", "event", "location", "politician", "person"], "instance": {"id": "84", "words": ["Under", "Terre", "'Blanche", ",", "the", "AWB", "swore", "to", "use", "violence", "to", "preserve", "minority", "rule", ",", "opposing", "any", "concessions", "offered", "to", "the", "African", "National", "Congress", "-", "an", "organisation", "AWB", "supporters", "repeatedly", "branded", "as", "Marxist", "terrorist", "s", "Immediately", "prior", "to", "South", "Africa", "'s", "1994", "South", "African", "general", "election", ",", "Terre", "'Blanche", "'s", "followers", "were", "linked", "to", "a", "number", "of", "bombings", "and", "assassinations", "targeting", "the", "South", "African", "Communist", "Party", ";", "armed", "AWB", "commandos", "participated", "in", "the", "crisis", "in", "Bophuthatswana", "in", "1994", "."], "labels": ["O", "B-politician", "I-politician", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, election, organization, country, event, location, politician, person and O.\nSentence: Under Terre 'Blanche , the AWB swore to use violence to preserve minority rule , opposing any concessions offered to the African National Congress - an organisation AWB supporters repeatedly branded as Marxist terrorist s Immediately prior to South Africa 's 1994 South African general election , Terre 'Blanche 's followers were linked to a number of bombings and assassinations targeting the South African Communist Party ; armed AWB commandos participated in the crisis in Bophuthatswana in 1994 .", "prompt_labels": "Under(O) Terre(B-politician) 'Blanche(I-politician) ,(O) the(O) AWB(B-organization) swore(O) to(O) use(O) violence(O) to(O) preserve(O) minority(O) rule(O) ,(O) opposing(O) any(O) concessions(O) offered(O) to(O) the(O) African(B-political party) National(I-political party) Congress(I-political party) -(O) an(O) organisation(O) AWB(B-organization) supporters(O) repeatedly(O) branded(O) as(O) Marxist(O) terrorist(O) s(O) Immediately(O) prior(O) to(O) South(B-country) Africa(I-country) 's(O) 1994(B-election) South(I-election) African(I-election) general(I-election) election(I-election) ,(O) Terre(B-politician) 'Blanche(I-politician) 's(O) followers(O) were(O) linked(O) to(O) a(O) number(O) of(O) bombings(O) and(O) assassinations(O) targeting(O) the(O) South(B-political party) African(I-political party) Communist(I-political party) Party(I-political party) ;(O) armed(O) AWB(B-organization) commandos(O) participated(O) in(O) the(O) crisis(O) in(O) Bophuthatswana(B-country) in(O) 1994(O) .(O)"}}
{"id": "466", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "person", "politician", "event", "location", "country", "election", "organization"], "instance": {"id": "466", "words": ["The", "U-boat", "Campaign", "from", "1914", "to", "1918", "was", "the", "World", "War", "I", "naval", "campaign", "fought", "by", "German", "U-boat", "s", "against", "the", "trade", "routes", "of", "the", "Allies", "of", "World", "War", "I", "."], "labels": ["O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, person, politician, event, location, country, election, organization and O.\nSentence: The U-boat Campaign from 1914 to 1918 was the World War I naval campaign fought by German U-boat s against the trade routes of the Allies of World War I .", "prompt_labels": "The(O) U-boat(B-event) Campaign(I-event) from(O) 1914(O) to(O) 1918(O) was(O) the(O) World(B-event) War(I-event) I(I-event) naval(O) campaign(O) fought(O) by(O) German(O) U-boat(O) s(O) against(O) the(O) trade(O) routes(O) of(O) the(O) Allies(B-country) of(O) World(B-event) War(I-event) I(I-event) .(O)"}}
{"id": "349", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "political party", "organization", "election", "person", "event", "country", "location"], "instance": {"id": "349", "words": ["Criticism", "on", "Meshadi", "Azizbekov", "'s", "political", "views", "by", "Maharram", "Zulfuqarli", "-", "SalamInfo", "Though", "Azerbaijan", "Communist", "Party", "and", "many", "other", "local", "left-wing", "politicians", "and", "the", "people", "sees", "Azizbekov", "as", "an", "important", ",", "notable", "and", "positive", "figure", "in", "the", "history", "of", "Azerbaijan", "."], "labels": ["O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, political party, organization, election, person, event, country, location and O.\nSentence: Criticism on Meshadi Azizbekov 's political views by Maharram Zulfuqarli - SalamInfo Though Azerbaijan Communist Party and many other local left-wing politicians and the people sees Azizbekov as an important , notable and positive figure in the history of Azerbaijan .", "prompt_labels": "Criticism(O) on(O) Meshadi(B-politician) Azizbekov(I-politician) 's(O) political(O) views(O) by(O) Maharram(B-politician) Zulfuqarli(I-politician) -(O) SalamInfo(O) Though(O) Azerbaijan(B-political party) Communist(I-political party) Party(I-political party) and(O) many(O) other(O) local(O) left-wing(O) politicians(O) and(O) the(O) people(O) sees(O) Azizbekov(B-politician) as(O) an(O) important(O) ,(O) notable(O) and(O) positive(O) figure(O) in(O) the(O) history(O) of(O) Azerbaijan(B-country) .(O)"}}
{"id": "62", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "organization", "country", "politician", "event", "political party", "person", "election"], "instance": {"id": "62", "words": ["The", "Member", "of", "Parliament", "since", "1997", "United", "Kingdom", "general", "election", "is", "Sir", "Jeffrey", "Donaldson", "who", "was", "elected", "as", "a", "member", "of", "the", "Ulster", "Unionist", "Party", "but", "switched", "to", "the", "Democratic", "Unionist", "Party", "in", "2004", "."], "labels": ["O", "O", "O", "B-organization", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, country, politician, event, political party, person, election and O.\nSentence: The Member of Parliament since 1997 United Kingdom general election is Sir Jeffrey Donaldson who was elected as a member of the Ulster Unionist Party but switched to the Democratic Unionist Party in 2004 .", "prompt_labels": "The(O) Member(O) of(O) Parliament(B-organization) since(O) 1997(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) is(O) Sir(B-politician) Jeffrey(I-politician) Donaldson(I-politician) who(O) was(O) elected(O) as(O) a(O) member(O) of(O) the(O) Ulster(B-political party) Unionist(I-political party) Party(I-political party) but(O) switched(O) to(O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) in(O) 2004(O) .(O)"}}
{"id": "279", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "event", "person", "election", "political party", "organization", "location", "politician"], "instance": {"id": "279", "words": ["The", "Russian", "election", "law", "stipulates", "that", "parties", "with", "representatives", "in", "the", "State", "Duma", "(", "currently", "United", "Russia", ",", "Communist", "Party", "of", "the", "Russian", "Federation", ",", "Liberal", "Democratic", "Party", "of", "Russia", "and", "A", "Just", "Russia", ")", "are", "free", "to", "put", "forward", "a", "list", "of", "candidates", "for", "the", "Duma", "elections", ",", "while", "parties", "with", "no", "current", "representation", "need", "first", "to", "collect", "signatures", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, person, election, political party, organization, location, politician and O.\nSentence: The Russian election law stipulates that parties with representatives in the State Duma ( currently United Russia , Communist Party of the Russian Federation , Liberal Democratic Party of Russia and A Just Russia ) are free to put forward a list of candidates for the Duma elections , while parties with no current representation need first to collect signatures .", "prompt_labels": "The(O) Russian(O) election(O) law(O) stipulates(O) that(O) parties(O) with(O) representatives(O) in(O) the(O) State(B-organization) Duma(I-organization) ((O) currently(O) United(B-political party) Russia(I-political party) ,(O) Communist(B-political party) Party(I-political party) of(I-political party) the(I-political party) Russian(I-political party) Federation(I-political party) ,(O) Liberal(B-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Russia(I-political party) and(O) A(B-political party) Just(I-political party) Russia(I-political party) )(O) are(O) free(O) to(O) put(O) forward(O) a(O) list(O) of(O) candidates(O) for(O) the(O) Duma(O) elections(O) ,(O) while(O) parties(O) with(O) no(O) current(O) representation(O) need(O) first(O) to(O) collect(O) signatures(O) .(O)"}}
{"id": "439", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "country", "politician", "person", "political party", "location", "election", "organization"], "instance": {"id": "439", "words": ["According", "to", "journalist", "Robert", "Fisk", ",", "Hariri", "could", "not", "have", "resigned", "on", "his", "own", ",", "as", "he", "had", "already", "scheduled", "visits", "with", "International", "Monetary", "Fund", "and", "the", "World", "Bank", "for", "the", "following", "Monday", "."], "labels": ["O", "O", "O", "B-person", "I-person", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, politician, person, political party, location, election, organization and O.\nSentence: According to journalist Robert Fisk , Hariri could not have resigned on his own , as he had already scheduled visits with International Monetary Fund and the World Bank for the following Monday .", "prompt_labels": "According(O) to(O) journalist(O) Robert(B-person) Fisk(I-person) ,(O) Hariri(B-person) could(O) not(O) have(O) resigned(O) on(O) his(O) own(O) ,(O) as(O) he(O) had(O) already(O) scheduled(O) visits(O) with(O) International(B-organization) Monetary(I-organization) Fund(I-organization) and(O) the(O) World(B-organization) Bank(I-organization) for(O) the(O) following(O) Monday(O) .(O)"}}
{"id": "339", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "organization", "event", "political party", "location", "election", "country", "politician"], "instance": {"id": "339", "words": ["He", "was", "president", "of", "the", "International", "Political", "Science", "Association", "from", "1970", "to", "1973", ",", "president", "of", "the", "International", "Social", "Science", "Council", "(", "ISSC", ")", ",", "which", "was", "founded", "by", "UNESCO", ",", "from", "1973", "to", "1977", ",", "vice-president", "of", "the", "International", "Sociological", "Association", "from", "1966", "to", "1970", ",", "and", "chairman", "(", "from", "1970", "to", "1976", ")", "and", "co-founder", "of", "the", "European", "Consortium", "for", "Political", "Research", "(", "ECPR", ")", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, event, political party, location, election, country, politician and O.\nSentence: He was president of the International Political Science Association from 1970 to 1973 , president of the International Social Science Council ( ISSC ) , which was founded by UNESCO , from 1973 to 1977 , vice-president of the International Sociological Association from 1966 to 1970 , and chairman ( from 1970 to 1976 ) and co-founder of the European Consortium for Political Research ( ECPR ) .", "prompt_labels": "He(O) was(O) president(O) of(O) the(O) International(B-organization) Political(I-organization) Science(I-organization) Association(I-organization) from(O) 1970(O) to(O) 1973(O) ,(O) president(O) of(O) the(O) International(B-organization) Social(I-organization) Science(I-organization) Council(I-organization) ((O) ISSC(B-organization) )(O) ,(O) which(O) was(O) founded(O) by(O) UNESCO(B-organization) ,(O) from(O) 1973(O) to(O) 1977(O) ,(O) vice-president(O) of(O) the(O) International(B-organization) Sociological(I-organization) Association(I-organization) from(O) 1966(O) to(O) 1970(O) ,(O) and(O) chairman(O) ((O) from(O) 1970(O) to(O) 1976(O) )(O) and(O) co-founder(O) of(O) the(O) European(B-organization) Consortium(I-organization) for(I-organization) Political(I-organization) Research(I-organization) ((O) ECPR(B-organization) )(O) .(O)"}}
{"id": "338", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "person", "location", "event", "politician", "country", "election", "political party"], "instance": {"id": "338", "words": ["The", "Party", "stood", "one", "candidate", ",", "John", "Morris", ",", "in", "the", "1997", "United", "Kingdom", "general", "election", "and", "2001", "United", "Kingdom", "general", "election", "in", "the", "Guildford", "constituency", ",", "and", "two", "in", "the", "2005", "United", "Kingdom", "general", "election", "with", "Caroline", "O", "'Reilly", "also", "standing", "in", "Brighton", "Kemptown", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, location, event, politician, country, election, political party and O.\nSentence: The Party stood one candidate , John Morris , in the 1997 United Kingdom general election and 2001 United Kingdom general election in the Guildford constituency , and two in the 2005 United Kingdom general election with Caroline O 'Reilly also standing in Brighton Kemptown .", "prompt_labels": "The(O) Party(O) stood(O) one(O) candidate(O) ,(O) John(B-politician) Morris(I-politician) ,(O) in(O) the(O) 1997(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 2001(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) in(O) the(O) Guildford(B-location) constituency(O) ,(O) and(O) two(O) in(O) the(O) 2005(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) with(O) Caroline(O) O(O) 'Reilly(O) also(O) standing(O) in(O) Brighton(B-location) Kemptown(I-location) .(O)"}}
{"id": "118", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "politician", "political party", "organization", "election", "person", "country", "location"], "instance": {"id": "118", "words": ["Vice", "presidents", "Chen", "Cheng", ",", "Yen", "Chia-kan", ",", "and", "Lien", "Chan", "all", "served", "as", "premier", "concurrently", "as", "vice", "president", "during", "part", "of", "their", "terms", ",", "and", "vice", "president", "Annette", "Lu", "has", "at", "times", "been", "mentioned", "as", "a", "possible", "candidate", "for", "premiership", "."], "labels": ["O", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, politician, political party, organization, election, person, country, location and O.\nSentence: Vice presidents Chen Cheng , Yen Chia-kan , and Lien Chan all served as premier concurrently as vice president during part of their terms , and vice president Annette Lu has at times been mentioned as a possible candidate for premiership .", "prompt_labels": "Vice(O) presidents(O) Chen(B-politician) Cheng(I-politician) ,(O) Yen(B-politician) Chia-kan(I-politician) ,(O) and(O) Lien(B-politician) Chan(I-politician) all(O) served(O) as(O) premier(O) concurrently(O) as(O) vice(O) president(O) during(O) part(O) of(O) their(O) terms(O) ,(O) and(O) vice(O) president(O) Annette(B-politician) Lu(I-politician) has(O) at(O) times(O) been(O) mentioned(O) as(O) a(O) possible(O) candidate(O) for(O) premiership(O) .(O)"}}
{"id": "479", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "politician", "political party", "election", "location", "person", "country", "event"], "instance": {"id": "479", "words": ["The", "38th", "G8", "summit", "was", "the", "first", "summit", "for", "French", "President", "François", "Hollande", "and", "was", "the", "last", "summit", "for", "Dmitry", "Medvedev", "as", "the", "leader", "of", "Russia", "."], "labels": ["O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, political party, election, location, person, country, event and O.\nSentence: The 38th G8 summit was the first summit for French President François Hollande and was the last summit for Dmitry Medvedev as the leader of Russia .", "prompt_labels": "The(O) 38th(B-event) G8(I-event) summit(I-event) was(O) the(O) first(O) summit(O) for(O) French(O) President(O) François(B-politician) Hollande(I-politician) and(O) was(O) the(O) last(O) summit(O) for(O) Dmitry(B-politician) Medvedev(I-politician) as(O) the(O) leader(O) of(O) Russia(B-country) .(O)"}}
{"id": "101", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "country", "political party", "election", "location", "organization", "politician", "event"], "instance": {"id": "101", "words": ["The", "five", "largest", "parties", "in", "the", "election", "were", "the", "National", "Party", "of", "Indonesia", "(", "Partai", "Nasional", "Indonesia", ")", ",", "Masyumi", ",", "Nahdlatul", "Ulama", ",", "the", "Communist", "Party", "of", "Indonesia", "(", "Partai", "Komunis", "Indonesia", ",", "PKI", ")", ",", "and", "the", "Indonesian", "Islamic", "Union", "Party", "(", "Partai", "Sarekat", "Islam", "Indonesia", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, political party, election, location, organization, politician, event and O.\nSentence: The five largest parties in the election were the National Party of Indonesia ( Partai Nasional Indonesia ) , Masyumi , Nahdlatul Ulama , the Communist Party of Indonesia ( Partai Komunis Indonesia , PKI ) , and the Indonesian Islamic Union Party ( Partai Sarekat Islam Indonesia ) .", "prompt_labels": "The(O) five(O) largest(O) parties(O) in(O) the(O) election(O) were(O) the(O) National(B-political party) Party(I-political party) of(I-political party) Indonesia(I-political party) ((O) Partai(B-political party) Nasional(I-political party) Indonesia(I-political party) )(O) ,(O) Masyumi(B-political party) ,(O) Nahdlatul(B-political party) Ulama(I-political party) ,(O) the(O) Communist(B-political party) Party(I-political party) of(I-political party) Indonesia(I-political party) ((O) Partai(B-political party) Komunis(I-political party) Indonesia(I-political party) ,(O) PKI(B-political party) )(O) ,(O) and(O) the(O) Indonesian(B-political party) Islamic(I-political party) Union(I-political party) Party(I-political party) ((O) Partai(B-political party) Sarekat(I-political party) Islam(I-political party) Indonesia(I-political party) )(O) .(O)"}}
{"id": "386", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "country", "event", "election", "political party", "location", "politician", "person"], "instance": {"id": "386", "words": ["Incumbent", "Republican", "Mike", "DeWine", "won", "re-election", "to", "a", "second", "term", ",", "beating", "Democrat", "Ted", "Celeste", ",", "real", "estate", "developer", "and", "brother", "of", "former", "Ohio", "Governor", "Dick", "Celeste", "."], "labels": ["O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, event, election, political party, location, politician, person and O.\nSentence: Incumbent Republican Mike DeWine won re-election to a second term , beating Democrat Ted Celeste , real estate developer and brother of former Ohio Governor Dick Celeste .", "prompt_labels": "Incumbent(O) Republican(O) Mike(B-politician) DeWine(I-politician) won(O) re-election(O) to(O) a(O) second(O) term(O) ,(O) beating(O) Democrat(O) Ted(B-politician) Celeste(I-politician) ,(O) real(O) estate(O) developer(O) and(O) brother(O) of(O) former(O) Ohio(B-location) Governor(O) Dick(B-politician) Celeste(I-politician) .(O)"}}
{"id": "165", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "event", "person", "politician", "election", "location", "country", "organization"], "instance": {"id": "165", "words": ["He", "was", "elected", "as", "a", "Social", "Credit", "MLA", "in", "Vancouver", "South", "in", "1975", "British", "Columbia", "general", "election", ",", "1979", "British", "Columbia", "general", "election", ",", "1983", "British", "Columbia", "general", "election", "and", "1986", "British", "Columbia", "general", "election", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-location", "I-location", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, person, politician, election, location, country, organization and O.\nSentence: He was elected as a Social Credit MLA in Vancouver South in 1975 British Columbia general election , 1979 British Columbia general election , 1983 British Columbia general election and 1986 British Columbia general election .", "prompt_labels": "He(O) was(O) elected(O) as(O) a(O) Social(B-organization) Credit(I-organization) MLA(I-organization) in(O) Vancouver(B-location) South(I-location) in(O) 1975(B-election) British(I-election) Columbia(I-election) general(I-election) election(I-election) ,(O) 1979(B-election) British(I-election) Columbia(I-election) general(I-election) election(I-election) ,(O) 1983(B-election) British(I-election) Columbia(I-election) general(I-election) election(I-election) and(O) 1986(B-election) British(I-election) Columbia(I-election) general(I-election) election(I-election) .(O)"}}
{"id": "404", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "location", "election", "political party", "country", "person", "politician", "organization"], "instance": {"id": "404", "words": ["Incumbent", "Democrat", "John", "C.", "Stennis", "won", "re-election", "to", "his", "seventh", "term", "over", "Republican", "Haley", "Barbour", ",", "a", "political", "operative", "who", "campaigned", "for", "U.S.", "Presidents", "Richard", "Nixon", "and", "Gerald", "Ford", "."], "labels": ["O", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, election, political party, country, person, politician, organization and O.\nSentence: Incumbent Democrat John C. Stennis won re-election to his seventh term over Republican Haley Barbour , a political operative who campaigned for U.S. Presidents Richard Nixon and Gerald Ford .", "prompt_labels": "Incumbent(O) Democrat(O) John(B-politician) C.(I-politician) Stennis(I-politician) won(O) re-election(O) to(O) his(O) seventh(O) term(O) over(O) Republican(O) Haley(B-politician) Barbour(I-politician) ,(O) a(O) political(O) operative(O) who(O) campaigned(O) for(O) U.S.(B-country) Presidents(O) Richard(B-politician) Nixon(I-politician) and(O) Gerald(B-politician) Ford(I-politician) .(O)"}}
{"id": "534", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "country", "politician", "location", "event", "person", "organization", "election"], "instance": {"id": "534", "words": ["The", "Yangtze", "River", "Crossing", "Campaign", "(", ")", "was", "a", "military", "campaign", "launched", "by", "the", "People", "'s", "Liberation", "Army", "to", "cross", "the", "Yangtze", "River", "and", "capture", "Nanjing", ",", "the", "capital", "of", "the", "Nationalist", "government", "of", "the", "Kuomintang", ",", "in", "the", "final", "stage", "of", "the", "Chinese", "Civil", "War", "."], "labels": ["O", "B-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, country, politician, location, event, person, organization, election and O.\nSentence: The Yangtze River Crossing Campaign ( ) was a military campaign launched by the People 's Liberation Army to cross the Yangtze River and capture Nanjing , the capital of the Nationalist government of the Kuomintang , in the final stage of the Chinese Civil War .", "prompt_labels": "The(O) Yangtze(B-event) River(I-event) Crossing(I-event) Campaign(I-event) ((O) )(O) was(O) a(O) military(O) campaign(O) launched(O) by(O) the(O) People(O) 's(O) Liberation(O) Army(O) to(O) cross(O) the(O) Yangtze(B-location) River(I-location) and(O) capture(O) Nanjing(B-location) ,(O) the(O) capital(O) of(O) the(O) Nationalist(O) government(O) of(O) the(O) Kuomintang(B-political party) ,(O) in(O) the(O) final(O) stage(O) of(O) the(O) Chinese(B-event) Civil(I-event) War(I-event) .(O)"}}
{"id": "78", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "election", "politician", "political party", "person", "location", "event", "organization"], "instance": {"id": "78", "words": ["The", "organization", "was", "founded", "in", "October", "2002", "during", "the", "build-up", "to", "the", "United", "States", "'", "2003", "invasion", "of", "Iraq", "by", "dozens", "of", "groups", "including", "the", "National", "Organization", "for", "Women", ",", "National", "Council", "of", "Churches", ",", "Peace", "Action", ",", "the", "American", "Friends", "Service", "Committee", ",", "Black", "Voices", "for", "Peace", ",", "Not", "In", "Our", "Name", ",", "September", "Eleventh", "Families", "for", "Peaceful", "Tomorrows", ",", "and", "Veterans", "for", "Peace", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, politician, political party, person, location, event, organization and O.\nSentence: The organization was founded in October 2002 during the build-up to the United States ' 2003 invasion of Iraq by dozens of groups including the National Organization for Women , National Council of Churches , Peace Action , the American Friends Service Committee , Black Voices for Peace , Not In Our Name , September Eleventh Families for Peaceful Tomorrows , and Veterans for Peace .", "prompt_labels": "The(O) organization(O) was(O) founded(O) in(O) October(O) 2002(O) during(O) the(O) build-up(O) to(O) the(O) United(B-country) States(I-country) '(O) 2003(B-event) invasion(I-event) of(I-event) Iraq(I-event) by(O) dozens(O) of(O) groups(O) including(O) the(O) National(B-organization) Organization(I-organization) for(I-organization) Women(I-organization) ,(O) National(B-organization) Council(I-organization) of(I-organization) Churches(I-organization) ,(O) Peace(B-organization) Action(I-organization) ,(O) the(O) American(B-organization) Friends(I-organization) Service(I-organization) Committee(I-organization) ,(O) Black(B-organization) Voices(I-organization) for(I-organization) Peace(I-organization) ,(O) Not(B-organization) In(I-organization) Our(I-organization) Name(I-organization) ,(O) September(B-organization) Eleventh(I-organization) Families(I-organization) for(I-organization) Peaceful(I-organization) Tomorrows(I-organization) ,(O) and(O) Veterans(B-organization) for(I-organization) Peace(I-organization) .(O)"}}
{"id": "414", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "event", "political party", "organization", "person", "election", "country", "politician"], "instance": {"id": "414", "words": ["The", "principal", "candidates", "were", "Mayor", "Michael", "Bloomberg", ",", "an", "independent", "running", "for", "the", "third", "time", "on", "the", "Republican", "and", "Independence", "Party", "of", "New", "York", "lines", ",", "and", "New", "York", "City", "Comptroller", "Bill", "Thompson", ",", "running", "for", "the", "Democratic", "and", "Working", "Families", "Party", "Parties", "."], "labels": ["O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, political party, organization, person, election, country, politician and O.\nSentence: The principal candidates were Mayor Michael Bloomberg , an independent running for the third time on the Republican and Independence Party of New York lines , and New York City Comptroller Bill Thompson , running for the Democratic and Working Families Party Parties .", "prompt_labels": "The(O) principal(O) candidates(O) were(O) Mayor(O) Michael(B-politician) Bloomberg(I-politician) ,(O) an(O) independent(O) running(O) for(O) the(O) third(O) time(O) on(O) the(O) Republican(O) and(O) Independence(B-political party) Party(I-political party) of(I-political party) New(I-political party) York(I-political party) lines(O) ,(O) and(O) New(B-location) York(I-location) City(I-location) Comptroller(O) Bill(B-person) Thompson(I-person) ,(O) running(O) for(O) the(O) Democratic(O) and(O) Working(B-political party) Families(I-political party) Party(I-political party) Parties(O) .(O)"}}
{"id": "38", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "event", "country", "political party", "organization", "location", "election", "person"], "instance": {"id": "38", "words": ["Before", "the", "merger", "of", "the", "Queensland", "branches", "of", "the", "Liberal", "Party", "of", "Australia", "and", "Nationals", "as", "the", "Liberal", "National", "Party", "of", "Queensland", ",", "the", "National", "Party", "had", "been", "the", "senior", "partner", "in", "the", "non-", "Australian", "Labor", "Party", "Coalition", "since", "1924", "."], "labels": ["O", "O", "O", "O", "O", "B-location", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, event, country, political party, organization, location, election, person and O.\nSentence: Before the merger of the Queensland branches of the Liberal Party of Australia and Nationals as the Liberal National Party of Queensland , the National Party had been the senior partner in the non- Australian Labor Party Coalition since 1924 .", "prompt_labels": "Before(O) the(O) merger(O) of(O) the(O) Queensland(B-location) branches(O) of(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) and(O) Nationals(O) as(O) the(O) Liberal(B-political party) National(I-political party) Party(I-political party) of(I-political party) Queensland(I-political party) ,(O) the(O) National(O) Party(O) had(O) been(O) the(O) senior(O) partner(O) in(O) the(O) non-(O) Australian(B-political party) Labor(I-political party) Party(I-political party) Coalition(O) since(O) 1924(O) .(O)"}}
{"id": "515", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "country", "political party", "person", "event", "organization", "politician", "location"], "instance": {"id": "515", "words": ["6", "June", "1920", ")", "was", "a", "senior", "British", "Army", "officer", "during", "the", "First", "World", "War", "who", "served", "at", "the", "Royal", "Military", "College", ",", "Sandhurst", "and", "saw", "action", "on", "the", "North-West", "Frontier", "of", "India", ",", "in", "South", "Africa", "during", "the", "Second", "Boer", "War", "and", "in", "France", "and", "Greece", "during", "the", "First", "World", "War", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-country", "O", "O", "B-country", "I-country", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "B-country", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, country, political party, person, event, organization, politician, location and O.\nSentence: 6 June 1920 ) was a senior British Army officer during the First World War who served at the Royal Military College , Sandhurst and saw action on the North-West Frontier of India , in South Africa during the Second Boer War and in France and Greece during the First World War .", "prompt_labels": "6(O) June(O) 1920(O) )(O) was(O) a(O) senior(O) British(B-organization) Army(I-organization) officer(O) during(O) the(O) First(B-event) World(I-event) War(I-event) who(O) served(O) at(O) the(O) Royal(B-event) Military(I-event) College(I-event) ,(I-event) Sandhurst(I-event) and(O) saw(O) action(O) on(O) the(O) North-West(B-location) Frontier(I-location) of(O) India(B-country) ,(O) in(O) South(B-country) Africa(I-country) during(O) the(O) Second(B-event) Boer(I-event) War(I-event) and(O) in(O) France(O) and(O) Greece(B-country) during(O) the(O) First(B-event) World(I-event) War(I-event) .(O)"}}
{"id": "243", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "person", "event", "politician", "election", "political party", "organization", "location"], "instance": {"id": "243", "words": ["Dickson", "campaigned", "for", "the", "federal", "New", "Democratic", "Party", "in", "the", "1974", "Canadian", "federal", "election", ",", "and", "finished", "third", "in", "St.", "Catharines", "against", "Liberal", "Party", "of", "Canada", "candidate", "Gilbert", "Parent", "."], "labels": ["B-politician", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, event, politician, election, political party, organization, location and O.\nSentence: Dickson campaigned for the federal New Democratic Party in the 1974 Canadian federal election , and finished third in St. Catharines against Liberal Party of Canada candidate Gilbert Parent .", "prompt_labels": "Dickson(B-politician) campaigned(O) for(O) the(O) federal(O) New(B-political party) Democratic(I-political party) Party(I-political party) in(O) the(O) 1974(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) and(O) finished(O) third(O) in(O) St.(B-location) Catharines(I-location) against(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) candidate(O) Gilbert(B-politician) Parent(I-politician) .(O)"}}
{"id": "365", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "politician", "political party", "election", "location", "country", "event", "person"], "instance": {"id": "365", "words": ["On", "the", "other", "hand", ",", "after", "the", "acceptance", "of", "the", "'", "third", "Memorandum", "'", "on", "2015", "organizations", "like", "Internationalist", "Workers", "'", "Left", ",", "Active", "Citizens", ",", "New", "Fighter", ",", "Democratic", "Social", "Movement", ",", "Anticapitalist", "Political", "Group", "and", "the", "Communist", "Tendency", "(", "section", "of", "IMT", ")", "joined", "the", "Left", "Platform", "to", "create", "Popular", "Unity", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, political party, election, location, country, event, person and O.\nSentence: On the other hand , after the acceptance of the ' third Memorandum ' on 2015 organizations like Internationalist Workers ' Left , Active Citizens , New Fighter , Democratic Social Movement , Anticapitalist Political Group and the Communist Tendency ( section of IMT ) joined the Left Platform to create Popular Unity .", "prompt_labels": "On(O) the(O) other(O) hand(O) ,(O) after(O) the(O) acceptance(O) of(O) the(O) '(O) third(O) Memorandum(O) '(O) on(O) 2015(O) organizations(O) like(O) Internationalist(B-political party) Workers(I-political party) '(I-political party) Left(I-political party) ,(O) Active(B-political party) Citizens(I-political party) ,(O) New(B-political party) Fighter(I-political party) ,(O) Democratic(B-political party) Social(I-political party) Movement(I-political party) ,(O) Anticapitalist(O) Political(O) Group(O) and(O) the(O) Communist(O) Tendency(O) ((O) section(O) of(O) IMT(O) )(O) joined(O) the(O) Left(O) Platform(O) to(O) create(O) Popular(B-organization) Unity(I-organization) .(O)"}}
{"id": "64", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "political party", "person", "event", "organization", "location", "politician", "election"], "instance": {"id": "64", "words": ["Whereas", "elsewhere", "there", "are", "effectively", "three", "fundamental", "battles", "fought", "in", "elections", "-", "between", "the", "Ulster", "Unionist", "Party", "and", "the", "Democratic", "Unionist", "Party", "to", "be", "the", "leading", "unionist", "party", ",", "between", "the", "Social", "Democratic", "and", "Labour", "Party", "and", "Sinn", "Féin", "to", "be", "the", "leading", "nationalist", "party", ",", "and", "between", "unionism", "and", "nationalism", "as", "a", "whole", ",", "North", "Down", "is", "different", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, person, event, organization, location, politician, election and O.\nSentence: Whereas elsewhere there are effectively three fundamental battles fought in elections - between the Ulster Unionist Party and the Democratic Unionist Party to be the leading unionist party , between the Social Democratic and Labour Party and Sinn Féin to be the leading nationalist party , and between unionism and nationalism as a whole , North Down is different .", "prompt_labels": "Whereas(O) elsewhere(O) there(O) are(O) effectively(O) three(O) fundamental(O) battles(O) fought(O) in(O) elections(O) -(O) between(O) the(O) Ulster(B-political party) Unionist(I-political party) Party(I-political party) and(O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) to(O) be(O) the(O) leading(O) unionist(B-political party) party(I-political party) ,(O) between(O) the(O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) and(O) Sinn(B-political party) Féin(I-political party) to(O) be(O) the(O) leading(O) nationalist(B-political party) party(I-political party) ,(O) and(O) between(O) unionism(O) and(O) nationalism(O) as(O) a(O) whole(O) ,(O) North(B-location) Down(I-location) is(O) different(O) .(O)"}}
{"id": "74", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "event", "politician", "location", "person", "election", "organization", "country"], "instance": {"id": "74", "words": ["The", "main", "line", "of", "conflict", "in", "France", "during", "the", "19th", "century", "was", "between", "monarchists", "(", "mainly", "Legitimists", "and", "Orléanist", "s", ",", "but", "also", "Bonapartism", ")", "and", "republicans", "(", "Radical-Socialists", ",", "Opportunist", "Republicans", ",", "and", "later", "socialists", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, politician, location, person, election, organization, country and O.\nSentence: The main line of conflict in France during the 19th century was between monarchists ( mainly Legitimists and Orléanist s , but also Bonapartism ) and republicans ( Radical-Socialists , Opportunist Republicans , and later socialists ) .", "prompt_labels": "The(O) main(O) line(O) of(O) conflict(O) in(O) France(B-country) during(O) the(O) 19th(O) century(O) was(O) between(O) monarchists(O) ((O) mainly(O) Legitimists(O) and(O) Orléanist(O) s(O) ,(O) but(O) also(O) Bonapartism(O) )(O) and(O) republicans(O) ((O) Radical-Socialists(O) ,(O) Opportunist(B-organization) Republicans(I-organization) ,(O) and(O) later(O) socialists(O) )(O) .(O)"}}
{"id": "183", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "election", "person", "location", "event", "political party", "organization", "country"], "instance": {"id": "183", "words": ["She", "was", "the", "lead", "Senate", "candidate", "at", "the", "2007", "Australian", "federal", "election", ",", "again", "at", "the", "2010", "Australian", "federal", "election", ",", "in", "which", "she", "became", "the", "first", "Greens", "candidate", "elected", "in", "Queensland", ",", "and", "the", "2019", "Australian", "federal", "election", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "O", "O", "B-location", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, person, location, event, political party, organization, country and O.\nSentence: She was the lead Senate candidate at the 2007 Australian federal election , again at the 2010 Australian federal election , in which she became the first Greens candidate elected in Queensland , and the 2019 Australian federal election .", "prompt_labels": "She(O) was(O) the(O) lead(O) Senate(O) candidate(O) at(O) the(O) 2007(B-election) Australian(I-election) federal(I-election) election(I-election) ,(O) again(O) at(O) the(O) 2010(B-election) Australian(I-election) federal(I-election) election(I-election) ,(O) in(O) which(O) she(O) became(O) the(O) first(O) Greens(B-political party) candidate(O) elected(O) in(O) Queensland(B-location) ,(O) and(O) the(O) 2019(B-election) Australian(I-election) federal(I-election) election(I-election) .(O)"}}
{"id": "450", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "organization", "person", "event", "election", "political party", "politician", "country"], "instance": {"id": "450", "words": ["Other", "talents", "include", "Omar", "Chaparro", ",", "who", "previously", "collaborated", "with", "Huevocartoon", "for", "Un", "gallo", "con", "muchos", "huevos", ",", "Eduardo", "Manzano", ",", "and", "ufologist", "Jaime", "Maussan", ",", "among", "others", "."], "labels": ["O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, person, event, election, political party, politician, country and O.\nSentence: Other talents include Omar Chaparro , who previously collaborated with Huevocartoon for Un gallo con muchos huevos , Eduardo Manzano , and ufologist Jaime Maussan , among others .", "prompt_labels": "Other(O) talents(O) include(O) Omar(B-person) Chaparro(I-person) ,(O) who(O) previously(O) collaborated(O) with(O) Huevocartoon(B-person) for(O) Un(O) gallo(O) con(O) muchos(O) huevos(O) ,(O) Eduardo(B-person) Manzano(I-person) ,(O) and(O) ufologist(O) Jaime(B-person) Maussan(I-person) ,(O) among(O) others(O) .(O)"}}
{"id": "495", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "country", "organization", "person", "political party", "event", "election", "location"], "instance": {"id": "495", "words": ["Kamerun", "were", "carried", "out", "by", "German", "Empire", "and", "Allies", "of", "World", "War", "I", "forces", "during", "the", "Kamerun", "Campaign", "of", "the", "First", "World", "War", "from", "August", "to", "September", "1914", "."], "labels": ["B-country", "O", "O", "O", "O", "B-country", "I-country", "O", "B-country", "O", "B-event", "I-event", "I-event", "O", "O", "O", "B-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, organization, person, political party, event, election, location and O.\nSentence: Kamerun were carried out by German Empire and Allies of World War I forces during the Kamerun Campaign of the First World War from August to September 1914 .", "prompt_labels": "Kamerun(B-country) were(O) carried(O) out(O) by(O) German(B-country) Empire(I-country) and(O) Allies(B-country) of(O) World(B-event) War(I-event) I(I-event) forces(O) during(O) the(O) Kamerun(B-event) Campaign(I-event) of(O) the(O) First(B-event) World(I-event) War(I-event) from(O) August(O) to(O) September(O) 1914(O) .(O)"}}
{"id": "155", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "organization", "politician", "event", "location", "political party", "election", "country"], "instance": {"id": "155", "words": ["He", "was", "elected", "to", "the", "Parliament", "of", "Norway", "from", "Vest-Agder", "in", "1993", "Norwegian", "parliamentary", "election", ",", "and", "was", "re-elected", "on", "the", "two", "following", "occasions", "in", "1997", "Norwegian", "parliamentary", "election", "and", "2001", "Norwegian", "parliamentary", "election", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-location", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, politician, event, location, political party, election, country and O.\nSentence: He was elected to the Parliament of Norway from Vest-Agder in 1993 Norwegian parliamentary election , and was re-elected on the two following occasions in 1997 Norwegian parliamentary election and 2001 Norwegian parliamentary election .", "prompt_labels": "He(O) was(O) elected(O) to(O) the(O) Parliament(B-organization) of(I-organization) Norway(I-organization) from(O) Vest-Agder(B-location) in(O) 1993(B-election) Norwegian(I-election) parliamentary(I-election) election(I-election) ,(O) and(O) was(O) re-elected(O) on(O) the(O) two(O) following(O) occasions(O) in(O) 1997(B-election) Norwegian(I-election) parliamentary(I-election) election(I-election) and(O) 2001(B-election) Norwegian(I-election) parliamentary(I-election) election(I-election) .(O)"}}
{"id": "53", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "event", "country", "person", "organization", "election", "politician", "location"], "instance": {"id": "53", "words": ["In", "the", "February", "1974", "United", "Kingdom", "general", "election", "the", "seat", "was", "won", "by", "Robert", "Bradford", "of", "the", "Vanguard", "Unionist", "Progressive", "Party", "on", "a", "united", "anti-", "Sunningdale", "Agreement", "slate", "with", "the", "Ulster", "Unionist", "Party", "and", "the", "Democratic", "Unionist", "Party", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, country, person, organization, election, politician, location and O.\nSentence: In the February 1974 United Kingdom general election the seat was won by Robert Bradford of the Vanguard Unionist Progressive Party on a united anti- Sunningdale Agreement slate with the Ulster Unionist Party and the Democratic Unionist Party .", "prompt_labels": "In(O) the(O) February(B-election) 1974(I-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) the(O) seat(O) was(O) won(O) by(O) Robert(B-politician) Bradford(I-politician) of(O) the(O) Vanguard(B-political party) Unionist(I-political party) Progressive(I-political party) Party(I-political party) on(O) a(O) united(O) anti-(O) Sunningdale(O) Agreement(O) slate(O) with(O) the(O) Ulster(B-political party) Unionist(I-political party) Party(I-political party) and(O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) .(O)"}}
{"id": "226", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "political party", "organization", "election", "location", "event", "country", "politician"], "instance": {"id": "226", "words": ["If", "you", "stand", "for", "compulsory", "student", "unionism", ",", "drug-injecting", "rooms", "and", "lowering", "the", "homosexual", "age", "of", "consent", ",", "you", "can", "choose", "the", "Australian", "Greens", ",", "Australian", "Labor", "Party", "or", "the", "Australian", "Democrats", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, organization, election, location, event, country, politician and O.\nSentence: If you stand for compulsory student unionism , drug-injecting rooms and lowering the homosexual age of consent , you can choose the Australian Greens , Australian Labor Party or the Australian Democrats .", "prompt_labels": "If(O) you(O) stand(O) for(O) compulsory(O) student(O) unionism(O) ,(O) drug-injecting(O) rooms(O) and(O) lowering(O) the(O) homosexual(O) age(O) of(O) consent(O) ,(O) you(O) can(O) choose(O) the(O) Australian(B-political party) Greens(I-political party) ,(O) Australian(B-political party) Labor(I-political party) Party(I-political party) or(O) the(O) Australian(B-political party) Democrats(I-political party) .(O)"}}
{"id": "133", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "person", "election", "location", "political party", "country", "politician", "organization"], "instance": {"id": "133", "words": ["Since", "1952", ",", "control", "of", "the", "House", "has", "changed", "hands", "five", "times", ",", "all", "of", "which", "were", "in", "midterm", "elections", "(", "1954", "United", "States", "House", "of", "Representatives", "elections", ",", "1994", "United", "States", "House", "of", "Representatives", "elections", ",", "2006", "United", "States", "House", "of", "Representatives", "elections", ",", "2010", "United", "States", "House", "of", "Representatives", "elections", "and", "2018", "United", "States", "House", "of", "Representatives", "elections", ")", "and", "all", "of", "which", "were", "at", "the", "expense", "of", "the", "incumbent", "President", "'s", "party", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, election, location, political party, country, politician, organization and O.\nSentence: Since 1952 , control of the House has changed hands five times , all of which were in midterm elections ( 1954 United States House of Representatives elections , 1994 United States House of Representatives elections , 2006 United States House of Representatives elections , 2010 United States House of Representatives elections and 2018 United States House of Representatives elections ) and all of which were at the expense of the incumbent President 's party .", "prompt_labels": "Since(O) 1952(O) ,(O) control(O) of(O) the(O) House(O) has(O) changed(O) hands(O) five(O) times(O) ,(O) all(O) of(O) which(O) were(O) in(O) midterm(O) elections(O) ((O) 1954(B-election) United(I-election) States(I-election) House(I-election) of(I-election) Representatives(I-election) elections(I-election) ,(O) 1994(B-election) United(I-election) States(I-election) House(I-election) of(I-election) Representatives(I-election) elections(I-election) ,(O) 2006(B-election) United(I-election) States(I-election) House(I-election) of(I-election) Representatives(I-election) elections(I-election) ,(O) 2010(B-election) United(I-election) States(I-election) House(I-election) of(I-election) Representatives(I-election) elections(I-election) and(O) 2018(B-election) United(I-election) States(I-election) House(I-election) of(I-election) Representatives(I-election) elections(I-election) )(O) and(O) all(O) of(O) which(O) were(O) at(O) the(O) expense(O) of(O) the(O) incumbent(O) President(O) 's(O) party(O) .(O)"}}
{"id": "148", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "country", "politician", "event", "election", "location", "organization", "political party"], "instance": {"id": "148", "words": ["Political", "scientists", "see", "European", "political", "parties", "such", "as", "Ecolo", "and", "Groen", "in", "Belgium", ",", "Alliance", "90", "/", "The", "Greens", "in", "Germany", ",", "or", "the", "Green", "Progressive", "Accord", "and", "GroenLinks", "in", "the", "Netherlands", "as", "coming", "out", "of", "the", "New", "Left", "and", "emphasizing", "spontaneous", "self-organization", ",", "participatory", "democracy", ",", "decentralization", "and", "voluntarism", ",", "being", "contrasted", "to", "the", "bureaucratic", "or", "statist", "approach", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "B-political party", "O", "B-country", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "O", "B-country", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "B-country", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, politician, event, election, location, organization, political party and O.\nSentence: Political scientists see European political parties such as Ecolo and Groen in Belgium , Alliance 90 / The Greens in Germany , or the Green Progressive Accord and GroenLinks in the Netherlands as coming out of the New Left and emphasizing spontaneous self-organization , participatory democracy , decentralization and voluntarism , being contrasted to the bureaucratic or statist approach .", "prompt_labels": "Political(O) scientists(O) see(O) European(O) political(O) parties(O) such(O) as(O) Ecolo(B-political party) and(O) Groen(B-political party) in(O) Belgium(B-country) ,(O) Alliance(B-political party) 90(I-political party) /(O) The(B-political party) Greens(I-political party) in(O) Germany(B-country) ,(O) or(O) the(O) Green(B-political party) Progressive(I-political party) Accord(I-political party) and(O) GroenLinks(B-political party) in(O) the(O) Netherlands(B-country) as(O) coming(O) out(O) of(O) the(O) New(B-event) Left(I-event) and(O) emphasizing(O) spontaneous(O) self-organization(O) ,(O) participatory(O) democracy(O) ,(O) decentralization(O) and(O) voluntarism(O) ,(O) being(O) contrasted(O) to(O) the(O) bureaucratic(O) or(O) statist(O) approach(O) .(O)"}}
{"id": "21", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "person", "country", "election", "political party", "event", "politician", "organization"], "instance": {"id": "21", "words": ["A", "Blue", "Tory", "is", ",", "in", "Canada", "politics", ",", "a", "conservative", "who", "advocates", "free-market", "or", "economically", "liberal", "policies", "..", "The", "term", "has", "been", "applied", "to", "members", "of", "the", "modern", "Conservative", "Party", "of", "Canada", "and", "provincial", "Progressive", "Conservative", "parties", ",", "as", "well", "as", "the", "historical", "Progressive", "Conservative", "Party", "of", "Canada", ",", "Reform", "Party", "of", "Canada", "and", "Canadian", "Alliance", "."], "labels": ["O", "B-political party", "I-political party", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, country, election, political party, event, politician, organization and O.\nSentence: A Blue Tory is , in Canada politics , a conservative who advocates free-market or economically liberal policies .. The term has been applied to members of the modern Conservative Party of Canada and provincial Progressive Conservative parties , as well as the historical Progressive Conservative Party of Canada , Reform Party of Canada and Canadian Alliance .", "prompt_labels": "A(O) Blue(B-political party) Tory(I-political party) is(O) ,(O) in(O) Canada(B-country) politics(O) ,(O) a(O) conservative(O) who(O) advocates(O) free-market(O) or(O) economically(O) liberal(O) policies(O) ..(O) The(O) term(O) has(O) been(O) applied(O) to(O) members(O) of(O) the(O) modern(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) and(O) provincial(O) Progressive(O) Conservative(O) parties(O) ,(O) as(O) well(O) as(O) the(O) historical(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) ,(O) Reform(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) and(O) Canadian(B-political party) Alliance(I-political party) .(O)"}}
{"id": "14", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "person", "event", "politician", "political party", "location", "organization", "country"], "instance": {"id": "14", "words": ["In", "the", "United", "States", ",", "such", "voter", "turnout", "organizations", "include", "the", "League", "of", "Women", "Voters", ",", "Rock", "the", "Vote", ",", "The", "Voter", "Participation", "Center", "and", "Vote.org", ",", "which", "attempt", "to", "motivate", "potential", "voters", "to", "register", "and", "to", "vote", "in", "the", "belief", "that", "failure", "of", "any", "eligible", "voter", "to", "vote", "in", "any", "election", "is", "a", "loss", "to", "society", "."], "labels": ["O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, person, event, politician, political party, location, organization, country and O.\nSentence: In the United States , such voter turnout organizations include the League of Women Voters , Rock the Vote , The Voter Participation Center and Vote.org , which attempt to motivate potential voters to register and to vote in the belief that failure of any eligible voter to vote in any election is a loss to society .", "prompt_labels": "In(O) the(O) United(B-country) States(I-country) ,(O) such(O) voter(O) turnout(O) organizations(O) include(O) the(O) League(B-organization) of(I-organization) Women(I-organization) Voters(I-organization) ,(O) Rock(B-organization) the(I-organization) Vote(I-organization) ,(O) The(B-organization) Voter(I-organization) Participation(I-organization) Center(I-organization) and(O) Vote.org(O) ,(O) which(O) attempt(O) to(O) motivate(O) potential(O) voters(O) to(O) register(O) and(O) to(O) vote(O) in(O) the(O) belief(O) that(O) failure(O) of(O) any(O) eligible(O) voter(O) to(O) vote(O) in(O) any(O) election(O) is(O) a(O) loss(O) to(O) society(O) .(O)"}}
{"id": "346", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "political party", "location", "country", "person", "election", "organization", "politician"], "instance": {"id": "346", "words": ["He", "was", "re-elected", "in", "the", "1991", "New", "Brunswick", "general", "election", "and", "1995", "New", "Brunswick", "general", "election", "but", "was", "defeated", "in", "1999", "New", "Brunswick", "general", "election", "."], "labels": ["O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, location, country, person, election, organization, politician and O.\nSentence: He was re-elected in the 1991 New Brunswick general election and 1995 New Brunswick general election but was defeated in 1999 New Brunswick general election .", "prompt_labels": "He(O) was(O) re-elected(O) in(O) the(O) 1991(B-election) New(I-election) Brunswick(I-election) general(I-election) election(I-election) and(O) 1995(B-election) New(I-election) Brunswick(I-election) general(I-election) election(I-election) but(O) was(O) defeated(O) in(O) 1999(B-election) New(I-election) Brunswick(I-election) general(I-election) election(I-election) .(O)"}}
{"id": "34", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "political party", "election", "organization", "person", "event", "politician", "location"], "instance": {"id": "34", "words": ["In", "December", "2017", "IdV", "was", "a", "founding", "member", "of", "the", "Popular", "Civic", "List", "(", "CP", ")", ",", "a", "centrist", "electoral", "list", "within", "the", "centre-left", "coalition", ",", "along", "with", "Popular", "Alternative", "(", "AP", ")", ",", "the", "Centrists", "for", "Europe", "(", "CpE", ")", ",", "Solidary", "Democracy", "(", "DemoS", ")", ",", "the", "Union", "for", "Trentino", "(", "UpT", ")", ",", "Italy", "is", "Popular", "(", "IP", ")", "and", "minor", "parties", "/", "groups", "."], "labels": ["O", "O", "O", "B-political party", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, election, organization, person, event, politician, location and O.\nSentence: In December 2017 IdV was a founding member of the Popular Civic List ( CP ) , a centrist electoral list within the centre-left coalition , along with Popular Alternative ( AP ) , the Centrists for Europe ( CpE ) , Solidary Democracy ( DemoS ) , the Union for Trentino ( UpT ) , Italy is Popular ( IP ) and minor parties / groups .", "prompt_labels": "In(O) December(O) 2017(O) IdV(B-political party) was(O) a(O) founding(O) member(O) of(O) the(O) Popular(B-political party) Civic(I-political party) List(I-political party) ((O) CP(B-political party) )(O) ,(O) a(O) centrist(O) electoral(O) list(O) within(O) the(O) centre-left(O) coalition(O) ,(O) along(O) with(O) Popular(B-political party) Alternative(I-political party) ((O) AP(B-political party) )(O) ,(O) the(O) Centrists(B-political party) for(I-political party) Europe(I-political party) ((O) CpE(B-political party) )(O) ,(O) Solidary(B-political party) Democracy(I-political party) ((O) DemoS(B-political party) )(O) ,(O) the(O) Union(B-political party) for(I-political party) Trentino(I-political party) ((O) UpT(B-political party) )(O) ,(O) Italy(B-political party) is(I-political party) Popular(I-political party) ((O) IP(B-political party) )(O) and(O) minor(O) parties(O) /(O) groups(O) .(O)"}}
{"id": "274", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "politician", "organization", "event", "country", "political party", "location", "election"], "instance": {"id": "274", "words": ["In", "Great", "Britain", ",", "the", "major", "parties", "are", "considered", "to", "be", "the", "Conservative", "and", "Unionist", "Party", ",", "the", "Labour", "Party", ",", "the", "Liberal", "Democrats", "and", "its", "forerunners", ",", "the", "Liberal", "Unionist", "Party", ",", "the", "various", "National", "Liberal", "parties", ",", "National", "Labour", "Organisation", ",", "the", "Scottish", "National", "Party", "and", "Plaid", "Cymru", "."], "labels": ["O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, organization, event, country, political party, location, election and O.\nSentence: In Great Britain , the major parties are considered to be the Conservative and Unionist Party , the Labour Party , the Liberal Democrats and its forerunners , the Liberal Unionist Party , the various National Liberal parties , National Labour Organisation , the Scottish National Party and Plaid Cymru .", "prompt_labels": "In(O) Great(B-country) Britain(I-country) ,(O) the(O) major(O) parties(O) are(O) considered(O) to(O) be(O) the(O) Conservative(B-political party) and(I-political party) Unionist(I-political party) Party(I-political party) ,(O) the(O) Labour(B-political party) Party(I-political party) ,(O) the(O) Liberal(B-political party) Democrats(I-political party) and(O) its(O) forerunners(O) ,(O) the(O) Liberal(B-political party) Unionist(I-political party) Party(I-political party) ,(O) the(O) various(O) National(B-political party) Liberal(I-political party) parties(O) ,(O) National(B-political party) Labour(I-political party) Organisation(I-political party) ,(O) the(O) Scottish(B-political party) National(I-political party) Party(I-political party) and(O) Plaid(B-political party) Cymru(I-political party) .(O)"}}
{"id": "155", "dataset": "crossner_science", "split": "dev", "label_list": ["theory", "organization", "award", "scientist", "enzyme", "event", "discipline", "country", "location", "astronomical object", "university", "person", "chemical element", "chemical compound", "protein", "academic journal"], "instance": {"id": "155", "words": ["Known", "for", "his", "research", "on", "Mitogen-activated", "protein", "kinase", "(", "MAPK", ")", "cascade", "in", "plants", ",", "he", "is", "a", "three-time", "Alexander", "von", "Humboldt", "Fellow", "and", "an", "elected", "fellow", "of", "the", "National", "Academy", "of", "Sciences", ",", "India", "."], "labels": ["O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "I-enzyme", "O", "B-enzyme", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, organization, award, scientist, enzyme, event, discipline, country, location, astronomical object, university, person, chemical element, chemical compound, protein, academic journal and O.\nSentence: Known for his research on Mitogen-activated protein kinase ( MAPK ) cascade in plants , he is a three-time Alexander von Humboldt Fellow and an elected fellow of the National Academy of Sciences , India .", "prompt_labels": "Known(O) for(O) his(O) research(O) on(O) Mitogen-activated(B-enzyme) protein(I-enzyme) kinase(I-enzyme) ((O) MAPK(B-enzyme) )(O) cascade(O) in(O) plants(O) ,(O) he(O) is(O) a(O) three-time(O) Alexander(B-award) von(I-award) Humboldt(I-award) Fellow(I-award) and(O) an(O) elected(O) fellow(B-award) of(I-award) the(I-award) National(I-award) Academy(I-award) of(I-award) Sciences(I-award) ,(O) India(B-country) .(O)"}}
{"id": "414", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical compound", "enzyme", "location", "protein", "chemical element", "organization", "scientist", "theory", "discipline", "event", "person", "country", "academic journal", "award", "university", "astronomical object"], "instance": {"id": "414", "words": ["The", "area", "include", "the", "Northern", "Rocky", "Mountains", "to", "the", "north", "of", "Lake", "Williston", "and", "the", "Rocky", "Mountain", "Foothills", "north", "of", "the", "Peace", "River", "and", "much", "of", "the", "southeastern", "Cassiar", "Mountains", "and", "a", "small", "portion", "of", "the", "northeastern", "Omineca", "Mountains", "."], "labels": ["O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, enzyme, location, protein, chemical element, organization, scientist, theory, discipline, event, person, country, academic journal, award, university, astronomical object and O.\nSentence: The area include the Northern Rocky Mountains to the north of Lake Williston and the Rocky Mountain Foothills north of the Peace River and much of the southeastern Cassiar Mountains and a small portion of the northeastern Omineca Mountains .", "prompt_labels": "The(O) area(O) include(O) the(O) Northern(B-location) Rocky(I-location) Mountains(I-location) to(O) the(O) north(O) of(O) Lake(B-location) Williston(I-location) and(O) the(O) Rocky(B-location) Mountain(I-location) Foothills(I-location) north(O) of(O) the(O) Peace(B-location) River(I-location) and(O) much(O) of(O) the(O) southeastern(O) Cassiar(B-location) Mountains(I-location) and(O) a(O) small(O) portion(O) of(O) the(O) northeastern(O) Omineca(B-location) Mountains(I-location) .(O)"}}
{"id": "83", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "scientist", "theory", "academic journal", "person", "organization", "chemical compound", "event", "award", "enzyme", "university", "country", "protein", "discipline", "chemical element", "location"], "instance": {"id": "83", "words": ["Other", "higher", "education", "organizations", "present", "in", "the", "community", ",", "but", "not", "offering", "classes", "locally", ",", "include", "the", "Oak", "Ridge", "Institute", "for", "Science", "and", "Education", ",", "Oak", "Ridge", "Associated", "Universities", ",", "and", "the", "University", "of", "Tennessee", "Forestry", "Stations", "and", "Arboretum", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-university", "I-university", "I-university", "I-university", "O", "O", "O", "B-university", "I-university", "I-university", "B-organization", "I-organization", "O", "B-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, scientist, theory, academic journal, person, organization, chemical compound, event, award, enzyme, university, country, protein, discipline, chemical element, location and O.\nSentence: Other higher education organizations present in the community , but not offering classes locally , include the Oak Ridge Institute for Science and Education , Oak Ridge Associated Universities , and the University of Tennessee Forestry Stations and Arboretum .", "prompt_labels": "Other(O) higher(O) education(O) organizations(O) present(O) in(O) the(O) community(O) ,(O) but(O) not(O) offering(O) classes(O) locally(O) ,(O) include(O) the(O) Oak(B-organization) Ridge(I-organization) Institute(I-organization) for(I-organization) Science(I-organization) and(I-organization) Education(I-organization) ,(O) Oak(B-university) Ridge(I-university) Associated(I-university) Universities(I-university) ,(O) and(O) the(O) University(B-university) of(I-university) Tennessee(I-university) Forestry(B-organization) Stations(I-organization) and(O) Arboretum(B-organization) .(O)"}}
{"id": "314", "dataset": "crossner_science", "split": "dev", "label_list": ["location", "scientist", "astronomical object", "discipline", "academic journal", "person", "protein", "enzyme", "chemical compound", "country", "event", "award", "university", "chemical element", "organization", "theory"], "instance": {"id": "314", "words": ["Ting", "was", "awarded", "Ernest", "Orlando", "Lawrence", "Award", "(", "in", "1976", ")", ",", "Nobel", "Prize", "in", "Physics", "(", "in", "1976", ")", ",", "Eringen", "Medal", "(", "in", "1977", ")", ",", "DeGaspari", "Award", "in", "Science", "from", "the", "Government", "of", "Italy", "(", "in", "1988", ")", ",", "Gold", "Medal", "for", "Science", "from", "Brescia", ",", "Italy", "(", "in", "1988", ")", ",", "and", "NASA", "Public", "Service", "Medal", "(", "in", "2001", ")", "."], "labels": ["B-scientist", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "B-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-organization", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, scientist, astronomical object, discipline, academic journal, person, protein, enzyme, chemical compound, country, event, award, university, chemical element, organization, theory and O.\nSentence: Ting was awarded Ernest Orlando Lawrence Award ( in 1976 ) , Nobel Prize in Physics ( in 1976 ) , Eringen Medal ( in 1977 ) , DeGaspari Award in Science from the Government of Italy ( in 1988 ) , Gold Medal for Science from Brescia , Italy ( in 1988 ) , and NASA Public Service Medal ( in 2001 ) .", "prompt_labels": "Ting(B-scientist) was(O) awarded(O) Ernest(B-award) Orlando(I-award) Lawrence(I-award) Award(I-award) ((O) in(O) 1976(O) )(O) ,(O) Nobel(B-award) Prize(I-award) in(I-award) Physics(I-award) ((O) in(O) 1976(O) )(O) ,(O) Eringen(B-award) Medal(I-award) ((O) in(O) 1977(O) )(O) ,(O) DeGaspari(B-award) Award(I-award) in(I-award) Science(I-award) from(O) the(O) Government(O) of(O) Italy(B-country) ((O) in(O) 1988(O) )(O) ,(O) Gold(B-award) Medal(I-award) for(I-award) Science(I-award) from(O) Brescia(B-location) ,(O) Italy(B-location) ((O) in(O) 1988(O) )(O) ,(O) and(O) NASA(B-organization) Public(B-award) Service(I-award) Medal(I-award) ((O) in(O) 2001(O) )(O) .(O)"}}
{"id": "378", "dataset": "crossner_science", "split": "dev", "label_list": ["academic journal", "person", "chemical element", "location", "astronomical object", "theory", "enzyme", "award", "scientist", "protein", "university", "organization", "event", "discipline", "chemical compound", "country"], "instance": {"id": "378", "words": ["The", "town", "was", "frequented", "by", "notable", "Old", "West", "personalities", ",", "including", "Dave", "Rudabaugh", ",", "Billy", "the", "Kid", ",", "Pat", "Garrett", ",", "and", "Shotgun", "John", "Collins", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, person, chemical element, location, astronomical object, theory, enzyme, award, scientist, protein, university, organization, event, discipline, chemical compound, country and O.\nSentence: The town was frequented by notable Old West personalities , including Dave Rudabaugh , Billy the Kid , Pat Garrett , and Shotgun John Collins .", "prompt_labels": "The(O) town(O) was(O) frequented(O) by(O) notable(O) Old(O) West(O) personalities(O) ,(O) including(O) Dave(B-person) Rudabaugh(I-person) ,(O) Billy(B-person) the(I-person) Kid(I-person) ,(O) Pat(B-person) Garrett(I-person) ,(O) and(O) Shotgun(B-person) John(I-person) Collins(I-person) .(O)"}}
{"id": "88", "dataset": "crossner_science", "split": "dev", "label_list": ["location", "country", "enzyme", "scientist", "theory", "award", "academic journal", "chemical compound", "person", "event", "discipline", "university", "chemical element", "organization", "protein", "astronomical object"], "instance": {"id": "88", "words": ["The", "name", "was", "suggested", "by", "John", "Herschel", "(", "son", "of", "William", "Herschel", ",", "discoverer", "of", "Mimas", "and", "Enceladus", ")", "in", "his", "1847", "publication", "Results", "of", "Astronomical", "Observations", "made", "at", "the", "Cape", "of", "Good", "Hope", ",", "in", "which", "he", "advocated", "naming", "the", "moons", "of", "Saturn", "after", "the", "Titans", ",", "brothers", "and", "sisters", "of", "the", "Titan", "Cronus", "(", "whom", "the", "Romans", "equated", "with", "their", "god", "Saturn", ")", "."], "labels": ["O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, enzyme, scientist, theory, award, academic journal, chemical compound, person, event, discipline, university, chemical element, organization, protein, astronomical object and O.\nSentence: The name was suggested by John Herschel ( son of William Herschel , discoverer of Mimas and Enceladus ) in his 1847 publication Results of Astronomical Observations made at the Cape of Good Hope , in which he advocated naming the moons of Saturn after the Titans , brothers and sisters of the Titan Cronus ( whom the Romans equated with their god Saturn ) .", "prompt_labels": "The(O) name(O) was(O) suggested(O) by(O) John(B-scientist) Herschel(I-scientist) ((O) son(O) of(O) William(B-scientist) Herschel(I-scientist) ,(O) discoverer(O) of(O) Mimas(B-astronomical object) and(O) Enceladus(B-astronomical object) )(O) in(O) his(O) 1847(O) publication(O) Results(O) of(O) Astronomical(O) Observations(O) made(O) at(O) the(O) Cape(B-location) of(I-location) Good(I-location) Hope(I-location) ,(O) in(O) which(O) he(O) advocated(O) naming(O) the(O) moons(O) of(O) Saturn(B-astronomical object) after(O) the(O) Titans(B-astronomical object) ,(O) brothers(O) and(O) sisters(O) of(O) the(O) Titan(B-person) Cronus(I-person) ((O) whom(O) the(O) Romans(O) equated(O) with(O) their(O) god(O) Saturn(B-astronomical object) )(O) .(O)"}}
{"id": "429", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "academic journal", "award", "event", "chemical element", "astronomical object", "person", "country", "university", "scientist", "chemical compound", "discipline", "theory", "location", "enzyme", "organization"], "instance": {"id": "429", "words": ["Her", "work", "on", "ATP-binding", "cassette", "transporter", "includes", "investigating", "their", "role", "in", "resistance", "to", "chemotherapy", "drugs", ";", "antigen", "presentation", "in", "adaptive", "immunity", "and", "viral", "infection", ";", "cystic", "fibrosis", ";", "and", "bacterial", "nutrition", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, academic journal, award, event, chemical element, astronomical object, person, country, university, scientist, chemical compound, discipline, theory, location, enzyme, organization and O.\nSentence: Her work on ATP-binding cassette transporter includes investigating their role in resistance to chemotherapy drugs ; antigen presentation in adaptive immunity and viral infection ; cystic fibrosis ; and bacterial nutrition .", "prompt_labels": "Her(O) work(O) on(O) ATP-binding(O) cassette(O) transporter(O) includes(O) investigating(O) their(O) role(O) in(O) resistance(O) to(O) chemotherapy(O) drugs(O) ;(O) antigen(O) presentation(O) in(O) adaptive(O) immunity(O) and(O) viral(O) infection(O) ;(O) cystic(O) fibrosis(O) ;(O) and(O) bacterial(O) nutrition(O) .(O)"}}
{"id": "405", "dataset": "crossner_science", "split": "dev", "label_list": ["academic journal", "university", "organization", "award", "event", "scientist", "enzyme", "theory", "location", "country", "discipline", "person", "chemical compound", "protein", "chemical element", "astronomical object"], "instance": {"id": "405", "words": ["The", "journals", "Museum", "Anthropology", ",", "Journal", "of", "Museum", "Ethnography", ",", "Gradhiva", ",", "and", "Museum", "Anthropology", "Review", "are", "closely", "identified", "with", "museum", "anthropology", "as", "a", "field", ".."], "labels": ["O", "O", "B-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, university, organization, award, event, scientist, enzyme, theory, location, country, discipline, person, chemical compound, protein, chemical element, astronomical object and O.\nSentence: The journals Museum Anthropology , Journal of Museum Ethnography , Gradhiva , and Museum Anthropology Review are closely identified with museum anthropology as a field ..", "prompt_labels": "The(O) journals(O) Museum(B-academic journal) Anthropology(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Museum(I-academic journal) Ethnography(I-academic journal) ,(O) Gradhiva(B-academic journal) ,(O) and(O) Museum(B-academic journal) Anthropology(I-academic journal) Review(I-academic journal) are(O) closely(O) identified(O) with(O) museum(O) anthropology(O) as(O) a(O) field(O) ..(O)"}}
{"id": "310", "dataset": "crossner_science", "split": "dev", "label_list": ["person", "enzyme", "organization", "chemical compound", "astronomical object", "location", "country", "university", "protein", "event", "theory", "chemical element", "award", "academic journal", "scientist", "discipline"], "instance": {"id": "310", "words": ["Herschbach", "is", "a", "Fellow", "of", "the", "American", "Academy", "of", "Arts", "and", "Sciences", ",", "the", "National", "Academy", "of", "Sciences", ",", "the", "American", "Philosophical", "Society", "and", "the", "Royal", "Chemical", "Society", "of", "Great", "Britain", "."], "labels": ["B-scientist", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, enzyme, organization, chemical compound, astronomical object, location, country, university, protein, event, theory, chemical element, award, academic journal, scientist, discipline and O.\nSentence: Herschbach is a Fellow of the American Academy of Arts and Sciences , the National Academy of Sciences , the American Philosophical Society and the Royal Chemical Society of Great Britain .", "prompt_labels": "Herschbach(B-scientist) is(O) a(O) Fellow(B-award) of(I-award) the(I-award) American(I-award) Academy(I-award) of(I-award) Arts(I-award) and(I-award) Sciences(I-award) ,(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) the(O) American(B-organization) Philosophical(I-organization) Society(I-organization) and(O) the(O) Royal(B-organization) Chemical(I-organization) Society(I-organization) of(I-organization) Great(I-organization) Britain(I-organization) .(O)"}}
{"id": "131", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "theory", "person", "enzyme", "chemical element", "event", "protein", "organization", "academic journal", "astronomical object", "award", "chemical compound", "scientist", "country", "university", "location"], "instance": {"id": "131", "words": ["Wasserburg", "completed", "his", "Ph.D.", "from", "the", "University", "of", "Chicago", "in", "1954", ",", "with", "a", "thesis", "on", "the", "development", "of", "K-Ar", "dating", ",", "done", "under", "the", "sponsorship", "of", "Harold", "Urey", "and", "Mark", "Inghram", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-theory", "I-theory", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, theory, person, enzyme, chemical element, event, protein, organization, academic journal, astronomical object, award, chemical compound, scientist, country, university, location and O.\nSentence: Wasserburg completed his Ph.D. from the University of Chicago in 1954 , with a thesis on the development of K-Ar dating , done under the sponsorship of Harold Urey and Mark Inghram .", "prompt_labels": "Wasserburg(B-scientist) completed(O) his(O) Ph.D.(O) from(O) the(O) University(B-university) of(I-university) Chicago(I-university) in(O) 1954(O) ,(O) with(O) a(O) thesis(O) on(O) the(O) development(O) of(O) K-Ar(B-theory) dating(I-theory) ,(O) done(O) under(O) the(O) sponsorship(O) of(O) Harold(B-scientist) Urey(I-scientist) and(O) Mark(B-scientist) Inghram(I-scientist) .(O)"}}
{"id": "427", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "organization", "enzyme", "protein", "astronomical object", "university", "theory", "academic journal", "event", "location", "discipline", "scientist", "chemical compound", "award", "country", "person"], "instance": {"id": "427", "words": ["The", "study", "was", "conducted", "in", "human", "retinal", "pigment", "epithelial", "cells", ",", "and", "the", "use", "of", "CRISPR", "led", "to", "a", "selection", "against", "cells", "with", "a", "functional", "p53", "pathway", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, organization, enzyme, protein, astronomical object, university, theory, academic journal, event, location, discipline, scientist, chemical compound, award, country, person and O.\nSentence: The study was conducted in human retinal pigment epithelial cells , and the use of CRISPR led to a selection against cells with a functional p53 pathway .", "prompt_labels": "The(O) study(O) was(O) conducted(O) in(O) human(O) retinal(O) pigment(O) epithelial(O) cells(O) ,(O) and(O) the(O) use(O) of(O) CRISPR(O) led(O) to(O) a(O) selection(O) against(O) cells(O) with(O) a(O) functional(O) p53(B-protein) pathway(O) .(O)"}}
{"id": "171", "dataset": "crossner_science", "split": "dev", "label_list": ["person", "theory", "protein", "university", "organization", "academic journal", "discipline", "country", "scientist", "award", "chemical compound", "chemical element", "astronomical object", "enzyme", "location", "event"], "instance": {"id": "171", "words": ["The", "discipline", "emerged", "after", "2010", "following", "the", "development", "of", "genome", "editing", "technology", "including", "TALENS", "and", "CRISPR", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, theory, protein, university, organization, academic journal, discipline, country, scientist, award, chemical compound, chemical element, astronomical object, enzyme, location, event and O.\nSentence: The discipline emerged after 2010 following the development of genome editing technology including TALENS and CRISPR .", "prompt_labels": "The(O) discipline(O) emerged(O) after(O) 2010(O) following(O) the(O) development(O) of(O) genome(O) editing(O) technology(O) including(O) TALENS(O) and(O) CRISPR(O) .(O)"}}
{"id": "316", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "chemical compound", "academic journal", "enzyme", "location", "protein", "theory", "discipline", "organization", "person", "event", "astronomical object", "scientist", "country", "university", "award"], "instance": {"id": "316", "words": ["Some", "idea", "of", "his", "activity", "as", "a", "writer", "on", "mathematical", "and", "physical", "subjects", "during", "these", "early", "years", "may", "be", "gathered", "from", "the", "fact", "that", "previous", "to", "this", "appointment", "he", "had", "contributed", "no", "less", "than", "three", "important", "memoirs", "to", "the", "Philosophical", "Transactions", "of", "the", "Royal", "Society", ",", "and", "eight", "to", "the", "Cambridge", "Philosophical", "Society", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, chemical compound, academic journal, enzyme, location, protein, theory, discipline, organization, person, event, astronomical object, scientist, country, university, award and O.\nSentence: Some idea of his activity as a writer on mathematical and physical subjects during these early years may be gathered from the fact that previous to this appointment he had contributed no less than three important memoirs to the Philosophical Transactions of the Royal Society , and eight to the Cambridge Philosophical Society .", "prompt_labels": "Some(O) idea(O) of(O) his(O) activity(O) as(O) a(O) writer(O) on(O) mathematical(O) and(O) physical(O) subjects(O) during(O) these(O) early(O) years(O) may(O) be(O) gathered(O) from(O) the(O) fact(O) that(O) previous(O) to(O) this(O) appointment(O) he(O) had(O) contributed(O) no(O) less(O) than(O) three(O) important(O) memoirs(O) to(O) the(O) Philosophical(B-academic journal) Transactions(I-academic journal) of(I-academic journal) the(I-academic journal) Royal(I-academic journal) Society(I-academic journal) ,(O) and(O) eight(O) to(O) the(O) Cambridge(B-organization) Philosophical(I-organization) Society(I-organization) .(O)"}}
{"id": "140", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "country", "organization", "chemical compound", "discipline", "university", "event", "academic journal", "enzyme", "chemical element", "award", "astronomical object", "person", "location", "scientist", "theory"], "instance": {"id": "140", "words": ["Later", ",", "scientists", "such", "as", "Ludwig", "Boltzmann", ",", "Josiah", "Willard", "Gibbs", ",", "and", "James", "Clerk", "Maxwell", "gave", "entropy", "a", "statistical", "basis", "."], "labels": ["O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, country, organization, chemical compound, discipline, university, event, academic journal, enzyme, chemical element, award, astronomical object, person, location, scientist, theory and O.\nSentence: Later , scientists such as Ludwig Boltzmann , Josiah Willard Gibbs , and James Clerk Maxwell gave entropy a statistical basis .", "prompt_labels": "Later(O) ,(O) scientists(O) such(O) as(O) Ludwig(B-scientist) Boltzmann(I-scientist) ,(O) Josiah(B-scientist) Willard(I-scientist) Gibbs(I-scientist) ,(O) and(O) James(B-scientist) Clerk(I-scientist) Maxwell(I-scientist) gave(O) entropy(O) a(O) statistical(O) basis(O) .(O)"}}
{"id": "50", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "enzyme", "scientist", "academic journal", "theory", "person", "location", "chemical compound", "country", "organization", "astronomical object", "event", "university", "award", "discipline", "chemical element"], "instance": {"id": "50", "words": ["He", "then", "worked", "at", "the", "Theoretical", "Physics", "Institute", "of", "the", "University", "of", "Copenhagen", "from", "1928", "to", "1931", ",", "with", "a", "break", "to", "work", "with", "Ernest", "Rutherford", "at", "the", "Cavendish", "Laboratory", "in", "Cambridge", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "B-organization", "I-organization", "O", "B-university", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, enzyme, scientist, academic journal, theory, person, location, chemical compound, country, organization, astronomical object, event, university, award, discipline, chemical element and O.\nSentence: He then worked at the Theoretical Physics Institute of the University of Copenhagen from 1928 to 1931 , with a break to work with Ernest Rutherford at the Cavendish Laboratory in Cambridge .", "prompt_labels": "He(O) then(O) worked(O) at(O) the(O) Theoretical(B-organization) Physics(I-organization) Institute(I-organization) of(O) the(O) University(B-university) of(I-university) Copenhagen(I-university) from(O) 1928(O) to(O) 1931(O) ,(O) with(O) a(O) break(O) to(O) work(O) with(O) Ernest(B-scientist) Rutherford(I-scientist) at(O) the(O) Cavendish(B-organization) Laboratory(I-organization) in(O) Cambridge(B-university) .(O)"}}
{"id": "382", "dataset": "crossner_science", "split": "dev", "label_list": ["person", "scientist", "chemical compound", "organization", "chemical element", "academic journal", "country", "location", "event", "university", "astronomical object", "award", "discipline", "theory", "enzyme", "protein"], "instance": {"id": "382", "words": ["G.", "Evelyn", "Hutchinson", ",", "a", "limnologist", "who", "was", "a", "contemporary", "of", "Tansley", "'s", ",", "combined", "Charles", "Sutherland", "Elton", "'", "s", "ideas", "about", "trophic", "ecology", "with", "those", "of", "Russian", "geochemist", "Vladimir", "Vernadsky", "."], "labels": ["B-scientist", "I-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "O", "O", "O", "B-discipline", "I-discipline", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, scientist, chemical compound, organization, chemical element, academic journal, country, location, event, university, astronomical object, award, discipline, theory, enzyme, protein and O.\nSentence: G. Evelyn Hutchinson , a limnologist who was a contemporary of Tansley 's , combined Charles Sutherland Elton ' s ideas about trophic ecology with those of Russian geochemist Vladimir Vernadsky .", "prompt_labels": "G.(B-scientist) Evelyn(I-scientist) Hutchinson(I-scientist) ,(O) a(O) limnologist(O) who(O) was(O) a(O) contemporary(O) of(O) Tansley(B-scientist) 's(O) ,(O) combined(O) Charles(B-scientist) Sutherland(I-scientist) Elton(I-scientist) '(O) s(O) ideas(O) about(O) trophic(B-discipline) ecology(I-discipline) with(O) those(O) of(O) Russian(O) geochemist(O) Vladimir(B-scientist) Vernadsky(I-scientist) .(O)"}}
{"id": "94", "dataset": "crossner_science", "split": "dev", "label_list": ["country", "theory", "organization", "university", "protein", "chemical compound", "scientist", "person", "academic journal", "astronomical object", "event", "chemical element", "enzyme", "discipline", "award", "location"], "instance": {"id": "94", "words": ["Some", "ligninolytic", "enzymes", "include", "Haem", "peroxidase", "such", "as", "lignin", "peroxidase", "s", ",", "manganese", "peroxidase", "s", ",", "versatile", "peroxidase", "s", ",", "and", "Dye", "decolorizing", "peroxidase", "as", "well", "as", "copper-based", "laccase", "s", "."], "labels": ["O", "O", "O", "O", "B-enzyme", "I-enzyme", "O", "O", "B-enzyme", "I-enzyme", "O", "O", "B-enzyme", "I-enzyme", "O", "O", "B-enzyme", "I-enzyme", "O", "O", "O", "B-enzyme", "I-enzyme", "I-enzyme", "O", "O", "O", "O", "B-enzyme", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, theory, organization, university, protein, chemical compound, scientist, person, academic journal, astronomical object, event, chemical element, enzyme, discipline, award, location and O.\nSentence: Some ligninolytic enzymes include Haem peroxidase such as lignin peroxidase s , manganese peroxidase s , versatile peroxidase s , and Dye decolorizing peroxidase as well as copper-based laccase s .", "prompt_labels": "Some(O) ligninolytic(O) enzymes(O) include(O) Haem(B-enzyme) peroxidase(I-enzyme) such(O) as(O) lignin(B-enzyme) peroxidase(I-enzyme) s(O) ,(O) manganese(B-enzyme) peroxidase(I-enzyme) s(O) ,(O) versatile(B-enzyme) peroxidase(I-enzyme) s(O) ,(O) and(O) Dye(B-enzyme) decolorizing(I-enzyme) peroxidase(I-enzyme) as(O) well(O) as(O) copper-based(O) laccase(B-enzyme) s(O) .(O)"}}
{"id": "312", "dataset": "crossner_science", "split": "dev", "label_list": ["theory", "chemical element", "university", "scientist", "protein", "person", "chemical compound", "enzyme", "event", "astronomical object", "award", "location", "organization", "discipline", "country", "academic journal"], "instance": {"id": "312", "words": ["Some", "asteroid", "s", ",", "also", "named", "after", "the", "same", "Shakespearean", "characters", ",", "share", "names", "with", "moons", "of", "Uranus", ":", "171", "Ophelia", ",", "218", "Bianca", ",", "593", "Titania", ",", "666", "Desdemona", ",", "763", "Cupido", ",", "and", "2758", "Cordelia", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "O", "B-astronomical object", "I-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, chemical element, university, scientist, protein, person, chemical compound, enzyme, event, astronomical object, award, location, organization, discipline, country, academic journal and O.\nSentence: Some asteroid s , also named after the same Shakespearean characters , share names with moons of Uranus : 171 Ophelia , 218 Bianca , 593 Titania , 666 Desdemona , 763 Cupido , and 2758 Cordelia .", "prompt_labels": "Some(O) asteroid(O) s(O) ,(O) also(O) named(O) after(O) the(O) same(O) Shakespearean(B-person) characters(O) ,(O) share(O) names(O) with(O) moons(O) of(O) Uranus(B-astronomical object) :(O) 171(B-astronomical object) Ophelia(I-astronomical object) ,(O) 218(B-astronomical object) Bianca(I-astronomical object) ,(O) 593(B-astronomical object) Titania(I-astronomical object) ,(O) 666(B-astronomical object) Desdemona(I-astronomical object) ,(O) 763(B-astronomical object) Cupido(I-astronomical object) ,(O) and(O) 2758(B-astronomical object) Cordelia(I-astronomical object) .(O)"}}
{"id": "57", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "location", "scientist", "academic journal", "event", "university", "chemical compound", "chemical element", "award", "enzyme", "theory", "protein", "country", "organization", "astronomical object", "person"], "instance": {"id": "57", "words": ["He", "also", "included", "perturbations", "due", "to", "the", "other", "planets", "(", "principally", "Jupiter", "and", "Venus", ")", "and", "also", "accounted", "for", "the", "more", "difficult", "problem", "of", "the", "non-spherical", "nature", "of", "the", "Earth", "and", "Moon", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, location, scientist, academic journal, event, university, chemical compound, chemical element, award, enzyme, theory, protein, country, organization, astronomical object, person and O.\nSentence: He also included perturbations due to the other planets ( principally Jupiter and Venus ) and also accounted for the more difficult problem of the non-spherical nature of the Earth and Moon .", "prompt_labels": "He(O) also(O) included(O) perturbations(O) due(O) to(O) the(O) other(O) planets(O) ((O) principally(O) Jupiter(B-astronomical object) and(O) Venus(B-astronomical object) )(O) and(O) also(O) accounted(O) for(O) the(O) more(O) difficult(O) problem(O) of(O) the(O) non-spherical(O) nature(O) of(O) the(O) Earth(B-astronomical object) and(O) Moon(B-astronomical object) .(O)"}}
{"id": "135", "dataset": "crossner_science", "split": "dev", "label_list": ["award", "protein", "event", "enzyme", "discipline", "scientist", "astronomical object", "university", "chemical element", "country", "organization", "academic journal", "person", "chemical compound", "location", "theory"], "instance": {"id": "135", "words": ["In", "Oct", "2002", "and", "after", ",", "Science", ",", "Physical", "Review", ",", "and", "Applied", "Physics", "Letters", "withdrew", "more", "than", "a", "dozen", "papers", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-academic journal", "O", "B-academic journal", "I-academic journal", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, protein, event, enzyme, discipline, scientist, astronomical object, university, chemical element, country, organization, academic journal, person, chemical compound, location, theory and O.\nSentence: In Oct 2002 and after , Science , Physical Review , and Applied Physics Letters withdrew more than a dozen papers .", "prompt_labels": "In(O) Oct(O) 2002(O) and(O) after(O) ,(O) Science(B-academic journal) ,(O) Physical(B-academic journal) Review(I-academic journal) ,(O) and(O) Applied(B-academic journal) Physics(I-academic journal) Letters(I-academic journal) withdrew(O) more(O) than(O) a(O) dozen(O) papers(O) .(O)"}}
{"id": "403", "dataset": "crossner_science", "split": "dev", "label_list": ["award", "enzyme", "theory", "university", "chemical compound", "person", "location", "organization", "chemical element", "country", "academic journal", "discipline", "event", "protein", "astronomical object", "scientist"], "instance": {"id": "403", "words": ["In", "Costa", "Rica", "he", "interacted", "with", "many", "field", "biologists", ",", "including", "Daniel", "Janzen", ",", "Stephen", "Hubbell", ",", "Gary", "Stiles", ",", "Luis", "Diego", "Gómez", ",", "Isidro", "A.", "Chacón", ",", "Gordon", "B.", "Small", ",", "Alwyn", "H.", "Gentry", ",", "Robin", "Foster", "."], "labels": ["O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, enzyme, theory, university, chemical compound, person, location, organization, chemical element, country, academic journal, discipline, event, protein, astronomical object, scientist and O.\nSentence: In Costa Rica he interacted with many field biologists , including Daniel Janzen , Stephen Hubbell , Gary Stiles , Luis Diego Gómez , Isidro A. Chacón , Gordon B. Small , Alwyn H. Gentry , Robin Foster .", "prompt_labels": "In(O) Costa(B-country) Rica(I-country) he(O) interacted(O) with(O) many(O) field(O) biologists(O) ,(O) including(O) Daniel(B-scientist) Janzen(I-scientist) ,(O) Stephen(B-scientist) Hubbell(I-scientist) ,(O) Gary(B-scientist) Stiles(I-scientist) ,(O) Luis(B-scientist) Diego(I-scientist) Gómez(I-scientist) ,(O) Isidro(B-scientist) A.(I-scientist) Chacón(I-scientist) ,(O) Gordon(B-scientist) B.(I-scientist) Small(I-scientist) ,(O) Alwyn(B-scientist) H.(I-scientist) Gentry(I-scientist) ,(O) Robin(B-scientist) Foster(I-scientist) .(O)"}}
{"id": "354", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "academic journal", "enzyme", "location", "person", "country", "theory", "discipline", "chemical compound", "chemical element", "event", "award", "university", "astronomical object", "organization", "scientist"], "instance": {"id": "354", "words": ["Ceres", ",", "2", "Pallas", ",", "3", "Juno", "and", "4", "Vesta", "lost", "their", "planet", "status", "after", "the", "discovery", "of", "many", "other", "asteroid", "s", "."], "labels": ["B-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, academic journal, enzyme, location, person, country, theory, discipline, chemical compound, chemical element, event, award, university, astronomical object, organization, scientist and O.\nSentence: Ceres , 2 Pallas , 3 Juno and 4 Vesta lost their planet status after the discovery of many other asteroid s .", "prompt_labels": "Ceres(B-astronomical object) ,(O) 2(B-astronomical object) Pallas(I-astronomical object) ,(O) 3(B-astronomical object) Juno(I-astronomical object) and(O) 4(B-astronomical object) Vesta(I-astronomical object) lost(O) their(O) planet(O) status(O) after(O) the(O) discovery(O) of(O) many(O) other(O) asteroid(O) s(O) .(O)"}}
{"id": "13", "dataset": "crossner_science", "split": "dev", "label_list": ["university", "chemical element", "event", "academic journal", "country", "scientist", "location", "protein", "person", "astronomical object", "award", "organization", "discipline", "chemical compound", "enzyme", "theory"], "instance": {"id": "13", "words": ["In", "addition", "to", "his", "steady", "research", "output", ",", "Naqvi", "has", "manifested", "his", "commitment", "to", "teaching", "by", "contributing", "to", "journals", "devoted", "to", "didactical", "aspects", "of", "science", "(", "American", "Journal", "of", "Physics", ",", "European", "Journal", "of", "Physics", ",", "Journal", "of", "Chemical", "Education", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, chemical element, event, academic journal, country, scientist, location, protein, person, astronomical object, award, organization, discipline, chemical compound, enzyme, theory and O.\nSentence: In addition to his steady research output , Naqvi has manifested his commitment to teaching by contributing to journals devoted to didactical aspects of science ( American Journal of Physics , European Journal of Physics , Journal of Chemical Education ) .", "prompt_labels": "In(O) addition(O) to(O) his(O) steady(O) research(O) output(O) ,(O) Naqvi(B-scientist) has(O) manifested(O) his(O) commitment(O) to(O) teaching(O) by(O) contributing(O) to(O) journals(O) devoted(O) to(O) didactical(O) aspects(O) of(O) science(O) ((O) American(B-academic journal) Journal(I-academic journal) of(I-academic journal) Physics(I-academic journal) ,(O) European(B-academic journal) Journal(I-academic journal) of(I-academic journal) Physics(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Chemical(I-academic journal) Education(I-academic journal) )(O) .(O)"}}
{"id": "138", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "academic journal", "astronomical object", "university", "protein", "theory", "country", "location", "event", "chemical compound", "person", "enzyme", "organization", "chemical element", "award", "scientist"], "instance": {"id": "138", "words": ["catalyzed", "by", "Phosphoenolpyruvate", "carboxylase", "(", "PEPC", ")", ",", "to", "carboxylate", "phosphoenolpyruvate", "(", "PEP", ")", "to", "oxaloacetate", "(", "OAA", ")", "which", "is", "a", "Csub4", "/", "sub", "dicarboxylic", "acid", "."], "labels": ["O", "O", "B-enzyme", "I-enzyme", "O", "B-enzyme", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "O", "O", "B-chemical compound", "O", "B-chemical compound", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "I-chemical compound", "I-chemical compound", "I-chemical compound", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, academic journal, astronomical object, university, protein, theory, country, location, event, chemical compound, person, enzyme, organization, chemical element, award, scientist and O.\nSentence: catalyzed by Phosphoenolpyruvate carboxylase ( PEPC ) , to carboxylate phosphoenolpyruvate ( PEP ) to oxaloacetate ( OAA ) which is a Csub4 / sub dicarboxylic acid .", "prompt_labels": "catalyzed(O) by(O) Phosphoenolpyruvate(B-enzyme) carboxylase(I-enzyme) ((O) PEPC(B-enzyme) )(O) ,(O) to(O) carboxylate(B-chemical compound) phosphoenolpyruvate(I-chemical compound) ((O) PEP(B-chemical compound) )(O) to(O) oxaloacetate(B-chemical compound) ((O) OAA(B-chemical compound) )(O) which(O) is(O) a(O) Csub4(B-chemical compound) /(I-chemical compound) sub(I-chemical compound) dicarboxylic(I-chemical compound) acid(I-chemical compound) .(O)"}}
{"id": "184", "dataset": "crossner_science", "split": "dev", "label_list": ["scientist", "discipline", "university", "astronomical object", "location", "chemical element", "chemical compound", "academic journal", "enzyme", "person", "organization", "country", "award", "theory", "event", "protein"], "instance": {"id": "184", "words": ["Starting", "with", "1972", ",", "the", "Review", "no", "longer", "appear", "exclusively", "in", "Reviews", "of", "Modern", "Physics", ",", "but", "also", "in", "Physics", "Letters", "B", ",", "European", "Physical", "Journal", "C", ",", "Journal", "of", "Physics", "G", ",", "Physical", "Review", "D", ",", "and", "Chinese", "Physics", "C", "(", "depending", "on", "the", "year", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, discipline, university, astronomical object, location, chemical element, chemical compound, academic journal, enzyme, person, organization, country, award, theory, event, protein and O.\nSentence: Starting with 1972 , the Review no longer appear exclusively in Reviews of Modern Physics , but also in Physics Letters B , European Physical Journal C , Journal of Physics G , Physical Review D , and Chinese Physics C ( depending on the year ) .", "prompt_labels": "Starting(O) with(O) 1972(O) ,(O) the(O) Review(O) no(O) longer(O) appear(O) exclusively(O) in(O) Reviews(B-academic journal) of(I-academic journal) Modern(I-academic journal) Physics(I-academic journal) ,(O) but(O) also(O) in(O) Physics(B-academic journal) Letters(I-academic journal) B(I-academic journal) ,(O) European(B-academic journal) Physical(I-academic journal) Journal(I-academic journal) C(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Physics(I-academic journal) G(I-academic journal) ,(O) Physical(B-academic journal) Review(I-academic journal) D(I-academic journal) ,(O) and(O) Chinese(B-academic journal) Physics(I-academic journal) C(I-academic journal) ((O) depending(O) on(O) the(O) year(O) )(O) .(O)"}}
{"id": "192", "dataset": "crossner_science", "split": "dev", "label_list": ["theory", "academic journal", "award", "event", "person", "discipline", "enzyme", "location", "country", "scientist", "university", "chemical compound", "organization", "protein", "chemical element", "astronomical object"], "instance": {"id": "192", "words": ["LH", "is", "released", "from", "the", "pituitary", "gland", "along", "with", "Follicle-stimulating", "hormone", "in", "response", "to", "GnRH", "release", "into", "the", "hypophyseal", "portal", "system", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, academic journal, award, event, person, discipline, enzyme, location, country, scientist, university, chemical compound, organization, protein, chemical element, astronomical object and O.\nSentence: LH is released from the pituitary gland along with Follicle-stimulating hormone in response to GnRH release into the hypophyseal portal system .", "prompt_labels": "LH(O) is(O) released(O) from(O) the(O) pituitary(O) gland(O) along(O) with(O) Follicle-stimulating(O) hormone(O) in(O) response(O) to(O) GnRH(O) release(O) into(O) the(O) hypophyseal(O) portal(O) system(O) .(O)"}}
{"id": "377", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "astronomical object", "location", "chemical element", "university", "organization", "theory", "country", "event", "protein", "chemical compound", "scientist", "enzyme", "person", "award", "academic journal"], "instance": {"id": "377", "words": ["That", "said", ",", "three", "of", "the", "four", "major", "international", "charities", "in", "the", "region", "are", "religious", "in", "nature", ":", "World", "Concern", ",", "World", "Vision", "International", ",", "and", "Mercy", "Corps", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, astronomical object, location, chemical element, university, organization, theory, country, event, protein, chemical compound, scientist, enzyme, person, award, academic journal and O.\nSentence: That said , three of the four major international charities in the region are religious in nature : World Concern , World Vision International , and Mercy Corps .", "prompt_labels": "That(O) said(O) ,(O) three(O) of(O) the(O) four(O) major(O) international(O) charities(O) in(O) the(O) region(O) are(O) religious(O) in(O) nature(O) :(O) World(B-organization) Concern(I-organization) ,(O) World(B-organization) Vision(I-organization) International(I-organization) ,(O) and(O) Mercy(B-organization) Corps(I-organization) .(O)"}}
{"id": "365", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "location", "scientist", "chemical element", "enzyme", "discipline", "university", "academic journal", "award", "event", "chemical compound", "country", "organization", "astronomical object", "person", "theory"], "instance": {"id": "365", "words": ["All", "three", "have", "streets", "named", "in", "their", "honor", "as", "does", "President", "Gerald", "Ford", ",", "a", "longtime", "Rancho", "Mirage", "resident", "and", "benefactor", "of", "the", "substance", "abuse", "center", "that", "bears", "his", "wife", "'s", "name", ",", "the", "Betty", "Ford", "Center", "on", "the", "campus", "of", "the", "Eisenhower", "Medical", "Center", ",", "named", "for", "general", ",", "U.S.", "president", "and", "part-time", "resident", "Dwight", "Eisenhower", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "B-location", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, location, scientist, chemical element, enzyme, discipline, university, academic journal, award, event, chemical compound, country, organization, astronomical object, person, theory and O.\nSentence: All three have streets named in their honor as does President Gerald Ford , a longtime Rancho Mirage resident and benefactor of the substance abuse center that bears his wife 's name , the Betty Ford Center on the campus of the Eisenhower Medical Center , named for general , U.S. president and part-time resident Dwight Eisenhower .", "prompt_labels": "All(O) three(O) have(O) streets(O) named(O) in(O) their(O) honor(O) as(O) does(O) President(O) Gerald(B-person) Ford(I-person) ,(O) a(O) longtime(O) Rancho(B-location) Mirage(B-location) resident(O) and(O) benefactor(O) of(O) the(O) substance(O) abuse(O) center(O) that(O) bears(O) his(O) wife(O) 's(O) name(O) ,(O) the(O) Betty(B-location) Ford(I-location) Center(I-location) on(O) the(O) campus(O) of(O) the(O) Eisenhower(B-location) Medical(I-location) Center(I-location) ,(O) named(O) for(O) general(O) ,(O) U.S.(B-country) president(O) and(O) part-time(O) resident(O) Dwight(B-person) Eisenhower(I-person) .(O)"}}
{"id": "273", "dataset": "crossner_science", "split": "dev", "label_list": ["award", "person", "protein", "university", "astronomical object", "scientist", "organization", "chemical element", "discipline", "theory", "country", "location", "chemical compound", "event", "academic journal", "enzyme"], "instance": {"id": "273", "words": ["In", "1986", ",", "he", "settled", "in", "France", ",", "where", "he", "first", "taught", "cognitive", "science", "and", "epistemology", "at", "the", "École", "Polytechnique", ",", "and", "later", "neuroscience", "at", "the", "University", "of", "Paris", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "B-discipline", "I-discipline", "O", "B-discipline", "O", "O", "B-university", "I-university", "O", "O", "O", "B-discipline", "O", "O", "B-university", "I-university", "I-university", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, protein, university, astronomical object, scientist, organization, chemical element, discipline, theory, country, location, chemical compound, event, academic journal, enzyme and O.\nSentence: In 1986 , he settled in France , where he first taught cognitive science and epistemology at the École Polytechnique , and later neuroscience at the University of Paris .", "prompt_labels": "In(O) 1986(O) ,(O) he(O) settled(O) in(O) France(B-country) ,(O) where(O) he(O) first(O) taught(O) cognitive(B-discipline) science(I-discipline) and(O) epistemology(B-discipline) at(O) the(O) École(B-university) Polytechnique(I-university) ,(O) and(O) later(O) neuroscience(B-discipline) at(O) the(O) University(B-university) of(I-university) Paris(I-university) .(O)"}}
{"id": "207", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "astronomical object", "organization", "protein", "university", "scientist", "country", "chemical compound", "award", "theory", "academic journal", "chemical element", "event", "location", "person", "enzyme"], "instance": {"id": "207", "words": ["Kobe", "is", "home", "to", "eighteen", "public", "and", "private", "universities", ",", "including", "Kobe", "University", ",", "Kobe", "Institute", "of", "Computing", "and", "Konan", "University", ",", "and", "eight", "junior", "colleges", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "O", "B-university", "I-university", "I-university", "I-university", "O", "B-university", "I-university", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, astronomical object, organization, protein, university, scientist, country, chemical compound, award, theory, academic journal, chemical element, event, location, person, enzyme and O.\nSentence: Kobe is home to eighteen public and private universities , including Kobe University , Kobe Institute of Computing and Konan University , and eight junior colleges .", "prompt_labels": "Kobe(B-person) is(O) home(O) to(O) eighteen(O) public(O) and(O) private(O) universities(O) ,(O) including(O) Kobe(B-university) University(I-university) ,(O) Kobe(B-university) Institute(I-university) of(I-university) Computing(I-university) and(O) Konan(B-university) University(I-university) ,(O) and(O) eight(O) junior(O) colleges(O) .(O)"}}
{"id": "391", "dataset": "crossner_science", "split": "dev", "label_list": ["event", "discipline", "university", "academic journal", "chemical compound", "person", "enzyme", "astronomical object", "protein", "location", "award", "scientist", "country", "theory", "organization", "chemical element"], "instance": {"id": "391", "words": ["Besides", "Kaiser", "Permanente", "and", "Providence", "hospitals", ",", "most", "of", "the", "valley", "is", "served", "by", "non-profit", "hospitals", "such", "as", ":", "Valley", "Presbyterian", "Hospital", "in", "Van", "Nuys", ",", "Northridge", "Hospital", "Medical", "Center", "in", "Northridge", ",", "Olive", "View", "-", "UCLA", "Medical", "Center", "in", "Sylmar", ",", "Encino", "Hospital", "Medical", "Center", "in", "Encino", ",", "and", "Sherman", "Oaks", "Hospital", "in", "Sherman", "Oaks", "."], "labels": ["O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-location", "I-location", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-location", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-location", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-location", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, discipline, university, academic journal, chemical compound, person, enzyme, astronomical object, protein, location, award, scientist, country, theory, organization, chemical element and O.\nSentence: Besides Kaiser Permanente and Providence hospitals , most of the valley is served by non-profit hospitals such as : Valley Presbyterian Hospital in Van Nuys , Northridge Hospital Medical Center in Northridge , Olive View - UCLA Medical Center in Sylmar , Encino Hospital Medical Center in Encino , and Sherman Oaks Hospital in Sherman Oaks .", "prompt_labels": "Besides(O) Kaiser(B-organization) Permanente(I-organization) and(O) Providence(B-organization) hospitals(I-organization) ,(O) most(O) of(O) the(O) valley(O) is(O) served(O) by(O) non-profit(O) hospitals(O) such(O) as(O) :(O) Valley(B-organization) Presbyterian(I-organization) Hospital(I-organization) in(O) Van(B-location) Nuys(I-location) ,(O) Northridge(B-organization) Hospital(I-organization) Medical(I-organization) Center(I-organization) in(O) Northridge(B-location) ,(O) Olive(B-organization) View(I-organization) -(I-organization) UCLA(I-organization) Medical(I-organization) Center(I-organization) in(O) Sylmar(B-location) ,(O) Encino(B-organization) Hospital(I-organization) Medical(I-organization) Center(I-organization) in(O) Encino(B-location) ,(O) and(O) Sherman(B-organization) Oaks(I-organization) Hospital(I-organization) in(O) Sherman(B-location) Oaks(I-location) .(O)"}}
{"id": "194", "dataset": "crossner_science", "split": "dev", "label_list": ["enzyme", "chemical element", "protein", "scientist", "country", "university", "location", "organization", "academic journal", "astronomical object", "person", "chemical compound", "discipline", "theory", "event", "award"], "instance": {"id": "194", "words": ["The", "manufacturing", "process", "of", "Technora", "reacts", "P-Phenylenediamine", "and", "3,4", "'", "-diaminodiphenylether", "(", "3,4", "'", "-ODA", ")", "with", "Terephthaloyl", "chloride", "."], "labels": ["O", "O", "O", "O", "B-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "I-chemical compound", "I-chemical compound", "O", "B-chemical compound", "I-chemical compound", "I-chemical compound", "O", "O", "B-chemical compound", "I-chemical compound", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, chemical element, protein, scientist, country, university, location, organization, academic journal, astronomical object, person, chemical compound, discipline, theory, event, award and O.\nSentence: The manufacturing process of Technora reacts P-Phenylenediamine and 3,4 ' -diaminodiphenylether ( 3,4 ' -ODA ) with Terephthaloyl chloride .", "prompt_labels": "The(O) manufacturing(O) process(O) of(O) Technora(B-chemical compound) reacts(O) P-Phenylenediamine(B-chemical compound) and(O) 3,4(B-chemical compound) '(I-chemical compound) -diaminodiphenylether(I-chemical compound) ((O) 3,4(B-chemical compound) '(I-chemical compound) -ODA(I-chemical compound) )(O) with(O) Terephthaloyl(B-chemical compound) chloride(I-chemical compound) .(O)"}}
{"id": "234", "dataset": "crossner_science", "split": "dev", "label_list": ["location", "protein", "person", "theory", "discipline", "astronomical object", "university", "chemical element", "event", "country", "organization", "enzyme", "award", "academic journal", "scientist", "chemical compound"], "instance": {"id": "234", "words": ["In", "November", "2006", ",", "the", "Seventh", "World", "Summit", "of", "Nobel", "Peace", "Prize", "in", "Rome", "presented", "Gabriel", "with", "the", "Man", "of", "Peace", "award", "."], "labels": ["O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "B-location", "O", "B-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, protein, person, theory, discipline, astronomical object, university, chemical element, event, country, organization, enzyme, award, academic journal, scientist, chemical compound and O.\nSentence: In November 2006 , the Seventh World Summit of Nobel Peace Prize in Rome presented Gabriel with the Man of Peace award .", "prompt_labels": "In(O) November(O) 2006(O) ,(O) the(O) Seventh(B-event) World(I-event) Summit(I-event) of(I-event) Nobel(I-event) Peace(I-event) Prize(I-event) in(O) Rome(B-location) presented(O) Gabriel(B-person) with(O) the(O) Man(B-award) of(I-award) Peace(I-award) award(I-award) .(O)"}}
{"id": "333", "dataset": "crossner_science", "split": "dev", "label_list": ["event", "organization", "award", "enzyme", "person", "chemical compound", "astronomical object", "academic journal", "scientist", "location", "protein", "chemical element", "theory", "university", "discipline", "country"], "instance": {"id": "333", "words": ["EF-Tu", "receptor", ",", "abbreviated", "as", "EFR", ",", "is", "a", "pattern-recognition", "receptor", "(", "PRR", ")", "that", "binds", "to", "the", "prokaryotic", "protein", "EF-Tu", "(", "elongation", "factor", "thermo", "unstable", ")", "in", "Arabidopsis", "thaliana", "."], "labels": ["B-protein", "I-protein", "O", "O", "O", "B-protein", "O", "O", "O", "B-protein", "I-protein", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, award, enzyme, person, chemical compound, astronomical object, academic journal, scientist, location, protein, chemical element, theory, university, discipline, country and O.\nSentence: EF-Tu receptor , abbreviated as EFR , is a pattern-recognition receptor ( PRR ) that binds to the prokaryotic protein EF-Tu ( elongation factor thermo unstable ) in Arabidopsis thaliana .", "prompt_labels": "EF-Tu(B-protein) receptor(I-protein) ,(O) abbreviated(O) as(O) EFR(B-protein) ,(O) is(O) a(O) pattern-recognition(B-protein) receptor(I-protein) ((O) PRR(B-protein) )(O) that(O) binds(O) to(O) the(O) prokaryotic(O) protein(O) EF-Tu(B-protein) ((O) elongation(O) factor(O) thermo(O) unstable(O) )(O) in(O) Arabidopsis(O) thaliana(O) .(O)"}}
{"id": "398", "dataset": "crossner_science", "split": "dev", "label_list": ["academic journal", "event", "scientist", "discipline", "astronomical object", "location", "theory", "award", "enzyme", "chemical element", "person", "organization", "chemical compound", "university", "country", "protein"], "instance": {"id": "398", "words": ["CoRoT", "discovered", "its", "first", "two", "planets", "in", "2007", ":", "the", "hot", "Jupiter", "s", "CoRoT-1b", "and", "CoRoT-2b", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, event, scientist, discipline, astronomical object, location, theory, award, enzyme, chemical element, person, organization, chemical compound, university, country, protein and O.\nSentence: CoRoT discovered its first two planets in 2007 : the hot Jupiter s CoRoT-1b and CoRoT-2b .", "prompt_labels": "CoRoT(O) discovered(O) its(O) first(O) two(O) planets(O) in(O) 2007(O) :(O) the(O) hot(B-astronomical object) Jupiter(I-astronomical object) s(O) CoRoT-1b(B-astronomical object) and(O) CoRoT-2b(B-astronomical object) .(O)"}}
{"id": "111", "dataset": "crossner_science", "split": "dev", "label_list": ["country", "chemical element", "scientist", "enzyme", "astronomical object", "university", "discipline", "award", "protein", "academic journal", "chemical compound", "location", "theory", "person", "event", "organization"], "instance": {"id": "111", "words": ["He", "was", "elected", "to", "the", "National", "Academy", "of", "Sciences", "in", "1879", "and", "received", "the", "1880", "Rumford", "Prize", "from", "the", "American", "Academy", "of", "Arts", "and", "Sciences", "for", "his", "work", "on", "chemical", "thermodynamics", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-discipline", "I-discipline", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, chemical element, scientist, enzyme, astronomical object, university, discipline, award, protein, academic journal, chemical compound, location, theory, person, event, organization and O.\nSentence: He was elected to the National Academy of Sciences in 1879 and received the 1880 Rumford Prize from the American Academy of Arts and Sciences for his work on chemical thermodynamics .", "prompt_labels": "He(O) was(O) elected(O) to(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) in(O) 1879(O) and(O) received(O) the(O) 1880(O) Rumford(B-award) Prize(I-award) from(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) for(O) his(O) work(O) on(O) chemical(B-discipline) thermodynamics(I-discipline) .(O)"}}
{"id": "262", "dataset": "crossner_science", "split": "dev", "label_list": ["academic journal", "enzyme", "organization", "person", "protein", "chemical compound", "theory", "scientist", "country", "astronomical object", "event", "discipline", "award", "chemical element", "university", "location"], "instance": {"id": "262", "words": ["In", "2011", ",", "the", "Joint", "Working", "Party", "of", "international", "scientific", "bodies", "International", "Union", "of", "Pure", "and", "Applied", "Chemistry", "(", "IUPAC", ")", "and", "International", "Union", "of", "Pure", "and", "Applied", "Physics", "(", "IUPAP", ")", "evaluated", "the", "2004", "and", "2007", "Dubna", "experiments", ",", "and", "concluded", "that", "they", "did", "not", "meet", "the", "criteria", "for", "discovery", "."], "labels": ["O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, enzyme, organization, person, protein, chemical compound, theory, scientist, country, astronomical object, event, discipline, award, chemical element, university, location and O.\nSentence: In 2011 , the Joint Working Party of international scientific bodies International Union of Pure and Applied Chemistry ( IUPAC ) and International Union of Pure and Applied Physics ( IUPAP ) evaluated the 2004 and 2007 Dubna experiments , and concluded that they did not meet the criteria for discovery .", "prompt_labels": "In(O) 2011(O) ,(O) the(O) Joint(B-organization) Working(I-organization) Party(I-organization) of(O) international(O) scientific(O) bodies(O) International(B-organization) Union(I-organization) of(I-organization) Pure(I-organization) and(I-organization) Applied(I-organization) Chemistry(I-organization) ((O) IUPAC(B-organization) )(O) and(O) International(B-organization) Union(I-organization) of(I-organization) Pure(I-organization) and(I-organization) Applied(I-organization) Physics(I-organization) ((O) IUPAP(B-organization) )(O) evaluated(O) the(O) 2004(O) and(O) 2007(O) Dubna(O) experiments(O) ,(O) and(O) concluded(O) that(O) they(O) did(O) not(O) meet(O) the(O) criteria(O) for(O) discovery(O) .(O)"}}
{"id": "154", "dataset": "crossner_science", "split": "dev", "label_list": ["person", "discipline", "country", "academic journal", "theory", "organization", "chemical element", "astronomical object", "event", "university", "award", "protein", "scientist", "chemical compound", "enzyme", "location"], "instance": {"id": "154", "words": ["His", "work", "has", "been", "published", "in", "international", "refereed", "journals", ",", "including", "American", "Economic", "Review", ",", "Journal", "of", "European", "Economic", "Association", ",", "Journal", "of", "Economic", "Perspectives", ",", "Economic", "Journal", "and", "American", "Political", "Science", "Review", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, discipline, country, academic journal, theory, organization, chemical element, astronomical object, event, university, award, protein, scientist, chemical compound, enzyme, location and O.\nSentence: His work has been published in international refereed journals , including American Economic Review , Journal of European Economic Association , Journal of Economic Perspectives , Economic Journal and American Political Science Review .", "prompt_labels": "His(O) work(O) has(O) been(O) published(O) in(O) international(O) refereed(O) journals(O) ,(O) including(O) American(B-academic journal) Economic(I-academic journal) Review(I-academic journal) ,(O) Journal(O) of(O) European(B-academic journal) Economic(I-academic journal) Association(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Economic(I-academic journal) Perspectives(I-academic journal) ,(O) Economic(B-academic journal) Journal(I-academic journal) and(O) American(B-academic journal) Political(I-academic journal) Science(I-academic journal) Review(I-academic journal) .(O)"}}
{"id": "286", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "scientist", "event", "theory", "country", "academic journal", "person", "location", "chemical element", "award", "chemical compound", "university", "organization", "protein", "enzyme", "discipline"], "instance": {"id": "286", "words": ["This", "receptor", "has", "several", "CC", "chemokine", "ligands", "including", "CCL2", ",", "CCL3", ",", "CCL4", ",", "CCL5", ",", "CCL11", ",", "CCL13", ",", "CCL14", "and", "CCL16", ".ref", "name", "=", "nomiyama", "/", "ref", "name", "=", "ogilvie", "/"], "labels": ["O", "O", "O", "O", "B-protein", "I-protein", "I-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "O", "O", "B-protein", "O", "O", "O", "O", "B-protein", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, scientist, event, theory, country, academic journal, person, location, chemical element, award, chemical compound, university, organization, protein, enzyme, discipline and O.\nSentence: This receptor has several CC chemokine ligands including CCL2 , CCL3 , CCL4 , CCL5 , CCL11 , CCL13 , CCL14 and CCL16 .ref name = nomiyama / ref name = ogilvie /", "prompt_labels": "This(O) receptor(O) has(O) several(O) CC(B-protein) chemokine(I-protein) ligands(I-protein) including(O) CCL2(B-protein) ,(O) CCL3(B-protein) ,(O) CCL4(B-protein) ,(O) CCL5(B-protein) ,(O) CCL11(B-protein) ,(O) CCL13(B-protein) ,(O) CCL14(B-protein) and(O) CCL16(B-protein) .ref(O) name(O) =(O) nomiyama(B-protein) /(O) ref(O) name(O) =(O) ogilvie(B-protein) /(O)"}}
{"id": "295", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical compound", "theory", "scientist", "person", "protein", "university", "award", "location", "country", "discipline", "organization", "academic journal", "enzyme", "astronomical object", "chemical element", "event"], "instance": {"id": "295", "words": ["He", "competed", "in", "Swimming", "at", "the", "1980", "Summer", "Olympics", "at", "the", "1980", "Summer", "Olympics", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, theory, scientist, person, protein, university, award, location, country, discipline, organization, academic journal, enzyme, astronomical object, chemical element, event and O.\nSentence: He competed in Swimming at the 1980 Summer Olympics at the 1980 Summer Olympics .", "prompt_labels": "He(O) competed(O) in(O) Swimming(O) at(O) the(O) 1980(B-event) Summer(I-event) Olympics(I-event) at(O) the(O) 1980(B-event) Summer(I-event) Olympics(I-event) .(O)"}}
{"id": "350", "dataset": "crossner_science", "split": "dev", "label_list": ["country", "university", "chemical compound", "protein", "enzyme", "theory", "organization", "discipline", "event", "chemical element", "person", "astronomical object", "academic journal", "award", "location", "scientist"], "instance": {"id": "350", "words": ["A", "second", "meeting", "was", "held", "soon", "thereafter", "and", "included", "Klaus", "Clusius", ",", "Robert", "Döpel", ",", "Werner", "Heisenberg", ",", "and", "Carl", "Friedrich", "von", "Weizsäcker", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, university, chemical compound, protein, enzyme, theory, organization, discipline, event, chemical element, person, astronomical object, academic journal, award, location, scientist and O.\nSentence: A second meeting was held soon thereafter and included Klaus Clusius , Robert Döpel , Werner Heisenberg , and Carl Friedrich von Weizsäcker .", "prompt_labels": "A(O) second(O) meeting(O) was(O) held(O) soon(O) thereafter(O) and(O) included(O) Klaus(B-scientist) Clusius(I-scientist) ,(O) Robert(B-scientist) Döpel(I-scientist) ,(O) Werner(B-scientist) Heisenberg(I-scientist) ,(O) and(O) Carl(B-scientist) Friedrich(I-scientist) von(I-scientist) Weizsäcker(I-scientist) .(O)"}}
{"id": "309", "dataset": "crossner_science", "split": "dev", "label_list": ["person", "chemical element", "university", "location", "astronomical object", "enzyme", "country", "academic journal", "chemical compound", "organization", "award", "scientist", "theory", "protein", "event", "discipline"], "instance": {"id": "309", "words": ["The", "Patriot", "was", "nominated", "for", "three", "Academy", "Awards", ":", "Academy", "Award", "for", "Best", "Sound", "Mixing", "(", "Kevin", "O", "'Connell", ",", "Greg", "P.", "Russell", "and", "Lee", "Orloff", ")", ",", "Academy", "Award", "for", "Best", "Cinematography", ",", "and", "Academy", "Award", "for", "Best", "Original", "Score", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, chemical element, university, location, astronomical object, enzyme, country, academic journal, chemical compound, organization, award, scientist, theory, protein, event, discipline and O.\nSentence: The Patriot was nominated for three Academy Awards : Academy Award for Best Sound Mixing ( Kevin O 'Connell , Greg P. Russell and Lee Orloff ) , Academy Award for Best Cinematography , and Academy Award for Best Original Score .", "prompt_labels": "The(O) Patriot(O) was(O) nominated(O) for(O) three(O) Academy(O) Awards(O) :(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Sound(I-award) Mixing(I-award) ((O) Kevin(B-person) O(I-person) 'Connell(I-person) ,(O) Greg(B-person) P.(I-person) Russell(I-person) and(O) Lee(B-person) Orloff(I-person) )(O) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Cinematography(I-award) ,(O) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Score(I-award) .(O)"}}
{"id": "245", "dataset": "crossner_science", "split": "dev", "label_list": ["university", "scientist", "academic journal", "location", "enzyme", "award", "organization", "astronomical object", "theory", "country", "event", "person", "discipline", "chemical element", "protein", "chemical compound"], "instance": {"id": "245", "words": ["There", "are", "global", "climate", "simulation", "models", "that", "have", "been", "written", "for", "Jupiter", ",", "Saturn", ",", "Neptune", "and", "Venus", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, scientist, academic journal, location, enzyme, award, organization, astronomical object, theory, country, event, person, discipline, chemical element, protein, chemical compound and O.\nSentence: There are global climate simulation models that have been written for Jupiter , Saturn , Neptune and Venus .", "prompt_labels": "There(O) are(O) global(O) climate(O) simulation(O) models(O) that(O) have(O) been(O) written(O) for(O) Jupiter(B-astronomical object) ,(O) Saturn(B-astronomical object) ,(O) Neptune(B-astronomical object) and(O) Venus(B-astronomical object) .(O)"}}
{"id": "374", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "award", "country", "academic journal", "discipline", "organization", "chemical element", "astronomical object", "person", "location", "chemical compound", "enzyme", "scientist", "university", "event", "theory"], "instance": {"id": "374", "words": ["Sinope", "was", "the", "outermost", "known", "moon", "of", "Jupiter", "until", "the", "discovery", "of", "Megaclite", "in", "2000", "."], "labels": ["B-astronomical object", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, award, country, academic journal, discipline, organization, chemical element, astronomical object, person, location, chemical compound, enzyme, scientist, university, event, theory and O.\nSentence: Sinope was the outermost known moon of Jupiter until the discovery of Megaclite in 2000 .", "prompt_labels": "Sinope(B-astronomical object) was(O) the(O) outermost(O) known(O) moon(B-astronomical object) of(O) Jupiter(B-astronomical object) until(O) the(O) discovery(O) of(O) Megaclite(B-astronomical object) in(O) 2000(O) .(O)"}}
{"id": "321", "dataset": "crossner_science", "split": "dev", "label_list": ["event", "person", "chemical element", "university", "award", "chemical compound", "discipline", "theory", "organization", "academic journal", "location", "astronomical object", "scientist", "protein", "country", "enzyme"], "instance": {"id": "321", "words": ["The", "song", "was", "nominated", "for", "various", "Grammy", "Award", ",", "winning", "Best", "Pop", "Duo", "/", "Group", "Performance", "and", "Grammy", "Award", "for", "Best", "Music", "Video", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, chemical element, university, award, chemical compound, discipline, theory, organization, academic journal, location, astronomical object, scientist, protein, country, enzyme and O.\nSentence: The song was nominated for various Grammy Award , winning Best Pop Duo / Group Performance and Grammy Award for Best Music Video .", "prompt_labels": "The(O) song(O) was(O) nominated(O) for(O) various(O) Grammy(B-award) Award(I-award) ,(O) winning(O) Best(B-award) Pop(I-award) Duo(I-award) /(I-award) Group(I-award) Performance(I-award) and(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Music(I-award) Video(I-award) .(O)"}}
{"id": "200", "dataset": "crossner_science", "split": "dev", "label_list": ["enzyme", "protein", "award", "location", "country", "chemical compound", "scientist", "academic journal", "chemical element", "event", "discipline", "organization", "astronomical object", "person", "theory", "university"], "instance": {"id": "200", "words": ["For", "example", ",", "Venus", "'", "s", "year", "(", "sidereal", "period", ")", "is", "225", "days", ",", "and", "Earth", "'", "s", "is", "365", "days", "."], "labels": ["O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, protein, award, location, country, chemical compound, scientist, academic journal, chemical element, event, discipline, organization, astronomical object, person, theory, university and O.\nSentence: For example , Venus ' s year ( sidereal period ) is 225 days , and Earth ' s is 365 days .", "prompt_labels": "For(O) example(O) ,(O) Venus(B-astronomical object) '(O) s(O) year(O) ((O) sidereal(O) period(O) )(O) is(O) 225(O) days(O) ,(O) and(O) Earth(B-astronomical object) '(O) s(O) is(O) 365(O) days(O) .(O)"}}
{"id": "204", "dataset": "crossner_science", "split": "dev", "label_list": ["organization", "enzyme", "location", "astronomical object", "event", "chemical compound", "country", "protein", "person", "scientist", "award", "discipline", "chemical element", "university", "academic journal", "theory"], "instance": {"id": "204", "words": ["The", "group", "had", "a", "close", "working", "relationship", "with", "Otto", "Heckmann", "and", "his", "student", "Engelbert", "Schücking", "at", "Hamburger", "Sternwarte", ",", "the", "city", "'s", "observatory", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, enzyme, location, astronomical object, event, chemical compound, country, protein, person, scientist, award, discipline, chemical element, university, academic journal, theory and O.\nSentence: The group had a close working relationship with Otto Heckmann and his student Engelbert Schücking at Hamburger Sternwarte , the city 's observatory .", "prompt_labels": "The(O) group(O) had(O) a(O) close(O) working(O) relationship(O) with(O) Otto(B-scientist) Heckmann(I-scientist) and(O) his(O) student(O) Engelbert(B-scientist) Schücking(I-scientist) at(O) Hamburger(B-location) Sternwarte(I-location) ,(O) the(O) city(O) 's(O) observatory(O) .(O)"}}
{"id": "123", "dataset": "crossner_science", "split": "dev", "label_list": ["enzyme", "theory", "astronomical object", "chemical element", "protein", "university", "scientist", "award", "location", "country", "organization", "chemical compound", "discipline", "person", "academic journal", "event"], "instance": {"id": "123", "words": ["A", "super-Earth", "is", "an", "extrasolar", "planet", "with", "a", "mass", "higher", "than", "Earth", "'", "s", ",", "but", "substantially", "below", "those", "of", "the", "Solar", "System", "'s", "ice", "giant", "s", ",", "Uranus", "and", "Neptune", ",", "which", "are", "14.5", "and", "17", "times", "Earth", "'s", ",", "respectively", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, theory, astronomical object, chemical element, protein, university, scientist, award, location, country, organization, chemical compound, discipline, person, academic journal, event and O.\nSentence: A super-Earth is an extrasolar planet with a mass higher than Earth ' s , but substantially below those of the Solar System 's ice giant s , Uranus and Neptune , which are 14.5 and 17 times Earth 's , respectively .", "prompt_labels": "A(O) super-Earth(O) is(O) an(O) extrasolar(O) planet(O) with(O) a(O) mass(O) higher(O) than(O) Earth(B-astronomical object) '(O) s(O) ,(O) but(O) substantially(O) below(O) those(O) of(O) the(O) Solar(O) System(O) 's(O) ice(O) giant(O) s(O) ,(O) Uranus(B-astronomical object) and(O) Neptune(B-astronomical object) ,(O) which(O) are(O) 14.5(O) and(O) 17(O) times(O) Earth(B-astronomical object) 's(O) ,(O) respectively(O) .(O)"}}
{"id": "267", "dataset": "crossner_science", "split": "dev", "label_list": ["academic journal", "university", "chemical element", "event", "organization", "scientist", "location", "protein", "enzyme", "chemical compound", "award", "theory", "person", "astronomical object", "discipline", "country"], "instance": {"id": "267", "words": ["In", "1970", "Sakharov", "was", "among", "the", "three", "founding", "members", "of", "the", "Committee", "on", "Human", "Rights", "in", "the", "USSR", "along", "with", "Valery", "Chalidze", "and", "Andrei", "Tverdokhlebov", "."], "labels": ["O", "O", "B-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, university, chemical element, event, organization, scientist, location, protein, enzyme, chemical compound, award, theory, person, astronomical object, discipline, country and O.\nSentence: In 1970 Sakharov was among the three founding members of the Committee on Human Rights in the USSR along with Valery Chalidze and Andrei Tverdokhlebov .", "prompt_labels": "In(O) 1970(O) Sakharov(B-scientist) was(O) among(O) the(O) three(O) founding(O) members(O) of(O) the(O) Committee(B-organization) on(I-organization) Human(I-organization) Rights(I-organization) in(I-organization) the(I-organization) USSR(I-organization) along(O) with(O) Valery(B-scientist) Chalidze(I-scientist) and(O) Andrei(B-scientist) Tverdokhlebov(I-scientist) .(O)"}}
{"id": "172", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "location", "enzyme", "organization", "country", "person", "discipline", "university", "protein", "academic journal", "chemical compound", "scientist", "award", "chemical element", "event", "theory"], "instance": {"id": "172", "words": ["Instead", ",", "for", "the", "UEFA", "Euro", "2008", "articles", "(", "UEFA", "Euro", "2008", ",", "UEFA", "Euro", "2008", "Group", "A", ",", "UEFA", "Euro", "2008", "Group", "B", ",", "UEFA", "Euro", "2008", "Group", "C", ",", "UEFA", "Euro", "2008", "Group", "D", "and", "UEFA", "Euro", "2008", "knockout", "stage", ")", ",", "the", "old", "partial", "URL", "string", "ones", ":"], "labels": ["O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, location, enzyme, organization, country, person, discipline, university, protein, academic journal, chemical compound, scientist, award, chemical element, event, theory and O.\nSentence: Instead , for the UEFA Euro 2008 articles ( UEFA Euro 2008 , UEFA Euro 2008 Group A , UEFA Euro 2008 Group B , UEFA Euro 2008 Group C , UEFA Euro 2008 Group D and UEFA Euro 2008 knockout stage ) , the old partial URL string ones :", "prompt_labels": "Instead(O) ,(O) for(O) the(O) UEFA(B-event) Euro(I-event) 2008(I-event) articles(O) ((O) UEFA(B-event) Euro(I-event) 2008(I-event) ,(O) UEFA(B-event) Euro(I-event) 2008(I-event) Group(O) A(O) ,(O) UEFA(B-event) Euro(I-event) 2008(I-event) Group(O) B(O) ,(O) UEFA(B-event) Euro(I-event) 2008(I-event) Group(O) C(O) ,(O) UEFA(B-event) Euro(I-event) 2008(I-event) Group(O) D(O) and(O) UEFA(B-event) Euro(I-event) 2008(I-event) knockout(O) stage(O) )(O) ,(O) the(O) old(O) partial(O) URL(O) string(O) ones(O) :(O)"}}
{"id": "188", "dataset": "crossner_science", "split": "dev", "label_list": ["academic journal", "astronomical object", "event", "country", "theory", "discipline", "chemical compound", "protein", "chemical element", "award", "enzyme", "university", "location", "organization", "scientist", "person"], "instance": {"id": "188", "words": ["Michel", "Adanson", "(", "1763", ")", ",", "Antoine", "Laurent", "de", "Jussieu", "(", "1789", ")", ",", "and", "Augustin", "Pyramus", "de", "Candolle", "(", "1819", ")", "all", "proposed", "various", "alternative", "natural", "systems", "of", "classification", "that", "grouped", "plants", "using", "a", "wider", "range", "of", "shared", "characters", "and", "were", "widely", "followed", "."], "labels": ["B-scientist", "I-scientist", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, astronomical object, event, country, theory, discipline, chemical compound, protein, chemical element, award, enzyme, university, location, organization, scientist, person and O.\nSentence: Michel Adanson ( 1763 ) , Antoine Laurent de Jussieu ( 1789 ) , and Augustin Pyramus de Candolle ( 1819 ) all proposed various alternative natural systems of classification that grouped plants using a wider range of shared characters and were widely followed .", "prompt_labels": "Michel(B-scientist) Adanson(I-scientist) ((O) 1763(O) )(O) ,(O) Antoine(B-scientist) Laurent(I-scientist) de(I-scientist) Jussieu(I-scientist) ((O) 1789(O) )(O) ,(O) and(O) Augustin(B-scientist) Pyramus(I-scientist) de(I-scientist) Candolle(I-scientist) ((O) 1819(O) )(O) all(O) proposed(O) various(O) alternative(O) natural(O) systems(O) of(O) classification(O) that(O) grouped(O) plants(O) using(O) a(O) wider(O) range(O) of(O) shared(O) characters(O) and(O) were(O) widely(O) followed(O) .(O)"}}
{"id": "196", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "award", "person", "scientist", "university", "location", "chemical element", "chemical compound", "country", "enzyme", "protein", "theory", "organization", "event", "academic journal", "astronomical object"], "instance": {"id": "196", "words": ["Two", "further", "super-Earths", "were", "discovered", "in", "2006", ":", "OGLE-2005-BLG-390Lb", "with", "a", "mass", "of", "5.5", "Earth", "masses", ",", "which", "was", "found", "by", "gravitational", "microlensing", ",", "and", "HD", "69830", "b", "with", "a", "mass", "of", "10", "Earth", "masses", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, award, person, scientist, university, location, chemical element, chemical compound, country, enzyme, protein, theory, organization, event, academic journal, astronomical object and O.\nSentence: Two further super-Earths were discovered in 2006 : OGLE-2005-BLG-390Lb with a mass of 5.5 Earth masses , which was found by gravitational microlensing , and HD 69830 b with a mass of 10 Earth masses .", "prompt_labels": "Two(O) further(O) super-Earths(O) were(O) discovered(O) in(O) 2006(O) :(O) OGLE-2005-BLG-390Lb(B-astronomical object) with(O) a(O) mass(O) of(O) 5.5(O) Earth(B-astronomical object) masses(O) ,(O) which(O) was(O) found(O) by(O) gravitational(O) microlensing(O) ,(O) and(O) HD(B-astronomical object) 69830(I-astronomical object) b(I-astronomical object) with(O) a(O) mass(O) of(O) 10(O) Earth(B-astronomical object) masses(O) .(O)"}}
{"id": "220", "dataset": "crossner_science", "split": "dev", "label_list": ["theory", "person", "award", "enzyme", "academic journal", "country", "protein", "university", "chemical element", "location", "discipline", "organization", "astronomical object", "event", "chemical compound", "scientist"], "instance": {"id": "220", "words": ["Pupin", "was", "a", "founding", "member", "of", "National", "Advisory", "Committee", "for", "Aeronautics", "(", "NACA", ")", "on", "3", "March", "1915", ",", "which", "later", "became", "NASA", ",", "and", "he", "participated", "in", "the", "founding", "of", "American", "Mathematical", "Society", "and", "American", "Physical", "Society", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, person, award, enzyme, academic journal, country, protein, university, chemical element, location, discipline, organization, astronomical object, event, chemical compound, scientist and O.\nSentence: Pupin was a founding member of National Advisory Committee for Aeronautics ( NACA ) on 3 March 1915 , which later became NASA , and he participated in the founding of American Mathematical Society and American Physical Society .", "prompt_labels": "Pupin(B-scientist) was(O) a(O) founding(O) member(O) of(O) National(B-organization) Advisory(I-organization) Committee(I-organization) for(I-organization) Aeronautics(I-organization) ((O) NACA(B-organization) )(O) on(O) 3(O) March(O) 1915(O) ,(O) which(O) later(O) became(O) NASA(B-organization) ,(O) and(O) he(O) participated(O) in(O) the(O) founding(O) of(O) American(B-organization) Mathematical(I-organization) Society(I-organization) and(O) American(B-organization) Physical(I-organization) Society(I-organization) .(O)"}}
{"id": "143", "dataset": "crossner_science", "split": "dev", "label_list": ["university", "academic journal", "event", "scientist", "discipline", "chemical compound", "location", "astronomical object", "chemical element", "organization", "award", "enzyme", "country", "theory", "protein", "person"], "instance": {"id": "143", "words": ["Although", "the", "notion", "of", "higher", "dimensions", "goes", "back", "to", "René", "Descartes", ",", "substantial", "development", "of", "a", "higher-dimensional", "geometry", "only", "began", "in", "the", "19th", "century", ",", "via", "the", "work", "of", "Arthur", "Cayley", ",", "William", "Rowan", "Hamilton", ",", "Ludwig", "Schläfli", "and", "Bernhard", "Riemann", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "B-discipline", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, academic journal, event, scientist, discipline, chemical compound, location, astronomical object, chemical element, organization, award, enzyme, country, theory, protein, person and O.\nSentence: Although the notion of higher dimensions goes back to René Descartes , substantial development of a higher-dimensional geometry only began in the 19th century , via the work of Arthur Cayley , William Rowan Hamilton , Ludwig Schläfli and Bernhard Riemann .", "prompt_labels": "Although(O) the(O) notion(O) of(O) higher(O) dimensions(O) goes(O) back(O) to(O) René(B-scientist) Descartes(I-scientist) ,(O) substantial(O) development(O) of(O) a(O) higher-dimensional(O) geometry(B-discipline) only(O) began(O) in(O) the(O) 19th(O) century(O) ,(O) via(O) the(O) work(O) of(O) Arthur(B-scientist) Cayley(I-scientist) ,(O) William(B-scientist) Rowan(I-scientist) Hamilton(I-scientist) ,(O) Ludwig(B-scientist) Schläfli(I-scientist) and(O) Bernhard(B-scientist) Riemann(I-scientist) .(O)"}}
{"id": "276", "dataset": "crossner_science", "split": "dev", "label_list": ["scientist", "person", "theory", "chemical element", "chemical compound", "academic journal", "discipline", "country", "enzyme", "organization", "award", "location", "protein", "event", "university", "astronomical object"], "instance": {"id": "276", "words": ["Writers", "Terry", "Nation", "and", "Dennis", "Spooner", ",", "Director", "Douglas", "Camfield", ",", "Producer", "John", "Wiles", "."], "labels": ["O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, person, theory, chemical element, chemical compound, academic journal, discipline, country, enzyme, organization, award, location, protein, event, university, astronomical object and O.\nSentence: Writers Terry Nation and Dennis Spooner , Director Douglas Camfield , Producer John Wiles .", "prompt_labels": "Writers(O) Terry(B-person) Nation(I-person) and(O) Dennis(B-person) Spooner(I-person) ,(O) Director(O) Douglas(B-person) Camfield(I-person) ,(O) Producer(O) John(B-person) Wiles(I-person) .(O)"}}
{"id": "401", "dataset": "crossner_science", "split": "dev", "label_list": ["organization", "person", "university", "event", "astronomical object", "protein", "enzyme", "chemical element", "discipline", "award", "country", "scientist", "location", "academic journal", "chemical compound", "theory"], "instance": {"id": "401", "words": ["In", "2009", ",", "Welch", "founded", "the", "Jack", "Welch", "Management", "Institute", "(", "JWMI", ")", ",", "a", "program", "at", "Chancellor", "University", "that", "offered", "an", "online", "executive", "Master", "of", "Business", "Administration", "."], "labels": ["O", "O", "O", "B-scientist", "O", "O", "B-university", "I-university", "I-university", "I-university", "O", "B-university", "O", "O", "O", "O", "O", "B-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, university, event, astronomical object, protein, enzyme, chemical element, discipline, award, country, scientist, location, academic journal, chemical compound, theory and O.\nSentence: In 2009 , Welch founded the Jack Welch Management Institute ( JWMI ) , a program at Chancellor University that offered an online executive Master of Business Administration .", "prompt_labels": "In(O) 2009(O) ,(O) Welch(B-scientist) founded(O) the(O) Jack(B-university) Welch(I-university) Management(I-university) Institute(I-university) ((O) JWMI(B-university) )(O) ,(O) a(O) program(O) at(O) Chancellor(B-university) University(I-university) that(O) offered(O) an(O) online(O) executive(O) Master(O) of(O) Business(O) Administration(O) .(O)"}}
{"id": "139", "dataset": "crossner_science", "split": "dev", "label_list": ["event", "person", "award", "country", "astronomical object", "enzyme", "organization", "chemical element", "academic journal", "scientist", "university", "chemical compound", "discipline", "protein", "theory", "location"], "instance": {"id": "139", "words": ["Some", "of", "these", "mechanisms", "include", "ATP-dependent", "chromatin", "remodeling", ",", "LINE1", ",", "and", "prion", "protein-based", "modifications", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "O", "B-protein", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, award, country, astronomical object, enzyme, organization, chemical element, academic journal, scientist, university, chemical compound, discipline, protein, theory, location and O.\nSentence: Some of these mechanisms include ATP-dependent chromatin remodeling , LINE1 , and prion protein-based modifications .", "prompt_labels": "Some(O) of(O) these(O) mechanisms(O) include(O) ATP-dependent(O) chromatin(O) remodeling(O) ,(O) LINE1(B-protein) ,(O) and(O) prion(B-protein) protein-based(O) modifications(O) .(O)"}}
{"id": "144", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "country", "award", "protein", "organization", "chemical element", "university", "event", "theory", "astronomical object", "academic journal", "chemical compound", "scientist", "enzyme", "location", "person"], "instance": {"id": "144", "words": ["DNA", "cytosine", "methylation", "is", "catalyzed", "by", "DNA", "methyltransferase", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, country, award, protein, organization, chemical element, university, event, theory, astronomical object, academic journal, chemical compound, scientist, enzyme, location, person and O.\nSentence: DNA cytosine methylation is catalyzed by DNA methyltransferase .", "prompt_labels": "DNA(O) cytosine(O) methylation(O) is(O) catalyzed(O) by(O) DNA(B-enzyme) methyltransferase(I-enzyme) .(O)"}}
{"id": "296", "dataset": "crossner_science", "split": "dev", "label_list": ["award", "scientist", "university", "chemical compound", "discipline", "person", "event", "theory", "country", "protein", "astronomical object", "academic journal", "location", "chemical element", "enzyme", "organization"], "instance": {"id": "296", "words": ["Gilbert", "Gil", "Jerome", "Perlow", "(", "10", "February", "1916", "-", "17", "February", "2007", ")", ",", "was", "an", "American", "physicist", "famous", "for", "his", "work", "related", "to", "the", "Mössbauer", "effect", ",", "and", "an", "editor", "of", "the", "Journal", "of", "Applied", "Physics", "and", "Applied", "Physics", "Letters", "."], "labels": ["B-scientist", "I-scientist", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-theory", "I-theory", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, scientist, university, chemical compound, discipline, person, event, theory, country, protein, astronomical object, academic journal, location, chemical element, enzyme, organization and O.\nSentence: Gilbert Gil Jerome Perlow ( 10 February 1916 - 17 February 2007 ) , was an American physicist famous for his work related to the Mössbauer effect , and an editor of the Journal of Applied Physics and Applied Physics Letters .", "prompt_labels": "Gilbert(B-scientist) Gil(I-scientist) Jerome(B-scientist) Perlow(I-scientist) ((O) 10(O) February(O) 1916(O) -(O) 17(O) February(O) 2007(O) )(O) ,(O) was(O) an(O) American(O) physicist(O) famous(O) for(O) his(O) work(O) related(O) to(O) the(O) Mössbauer(B-theory) effect(I-theory) ,(O) and(O) an(O) editor(O) of(O) the(O) Journal(B-academic journal) of(I-academic journal) Applied(I-academic journal) Physics(I-academic journal) and(O) Applied(B-academic journal) Physics(I-academic journal) Letters(I-academic journal) .(O)"}}
{"id": "218", "dataset": "crossner_science", "split": "dev", "label_list": ["location", "enzyme", "university", "award", "protein", "chemical compound", "academic journal", "person", "event", "astronomical object", "country", "chemical element", "theory", "organization", "discipline", "scientist"], "instance": {"id": "218", "words": ["The", "cut", "site", "is", "then", "radiolabeled", "with", "phosphorus-32", "and", "splint-ligate", "d", "to", "a", "116nt", "ssDNA", "oligonucleotide", "using", "DNA", "ligase", ".ref", "name", "=", "probing", "RNase", "T1", "/", "A", "is", "introduced", "to", "the", "sample", "to", "digest", "all", "RNA", ",", "except", "for", "the", "RNA", "molecules", "with", "the", "116-mers", "DNA", "attached", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-chemical element", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, enzyme, university, award, protein, chemical compound, academic journal, person, event, astronomical object, country, chemical element, theory, organization, discipline, scientist and O.\nSentence: The cut site is then radiolabeled with phosphorus-32 and splint-ligate d to a 116nt ssDNA oligonucleotide using DNA ligase .ref name = probing RNase T1 / A is introduced to the sample to digest all RNA , except for the RNA molecules with the 116-mers DNA attached .", "prompt_labels": "The(O) cut(O) site(O) is(O) then(O) radiolabeled(O) with(O) phosphorus-32(B-chemical element) and(O) splint-ligate(O) d(O) to(O) a(O) 116nt(O) ssDNA(O) oligonucleotide(O) using(O) DNA(B-enzyme) ligase(I-enzyme) .ref(O) name(O) =(O) probing(O) RNase(B-enzyme) T1(I-enzyme) /(O) A(O) is(O) introduced(O) to(O) the(O) sample(O) to(O) digest(O) all(O) RNA(O) ,(O) except(O) for(O) the(O) RNA(O) molecules(O) with(O) the(O) 116-mers(O) DNA(O) attached(O) .(O)"}}
{"id": "240", "dataset": "crossner_science", "split": "dev", "label_list": ["person", "scientist", "chemical element", "protein", "organization", "astronomical object", "event", "academic journal", "theory", "chemical compound", "country", "discipline", "enzyme", "university", "location", "award"], "instance": {"id": "240", "words": ["Stanley", "N.", "Cohen", "(", "November", "17", ",", "1922", "-", "February", "5", ",", "2020", ")", "was", "an", "American", "biochemist", "who", ",", "along", "with", "Rita", "Levi-Montalcini", ",", "was", "awarded", "the", "Nobel", "Prize", "in", "Physiology", "or", "Medicine", "in", "1986", "for", "the", "isolation", "of", "nerve", "growth", "factor", "and", "the", "discovery", "of", "epidermal", "growth", "factor", "."], "labels": ["B-scientist", "I-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, scientist, chemical element, protein, organization, astronomical object, event, academic journal, theory, chemical compound, country, discipline, enzyme, university, location, award and O.\nSentence: Stanley N. Cohen ( November 17 , 1922 - February 5 , 2020 ) was an American biochemist who , along with Rita Levi-Montalcini , was awarded the Nobel Prize in Physiology or Medicine in 1986 for the isolation of nerve growth factor and the discovery of epidermal growth factor .", "prompt_labels": "Stanley(B-scientist) N.(I-scientist) Cohen(I-scientist) ((O) November(O) 17(O) ,(O) 1922(O) -(O) February(O) 5(O) ,(O) 2020(O) )(O) was(O) an(O) American(O) biochemist(O) who(O) ,(O) along(O) with(O) Rita(B-scientist) Levi-Montalcini(I-scientist) ,(O) was(O) awarded(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Physiology(I-award) or(I-award) Medicine(I-award) in(O) 1986(O) for(O) the(O) isolation(O) of(O) nerve(O) growth(O) factor(O) and(O) the(O) discovery(O) of(O) epidermal(O) growth(O) factor(O) .(O)"}}
{"id": "235", "dataset": "crossner_science", "split": "dev", "label_list": ["location", "award", "discipline", "country", "enzyme", "event", "theory", "academic journal", "chemical element", "chemical compound", "organization", "university", "scientist", "protein", "person", "astronomical object"], "instance": {"id": "235", "words": ["It", "was", "nominated", "for", "the", "2002", "Hugo", "Award", "for", "Best", "Novel", "and", "tied", "for", "the", "2002", "John", "W.", "Campbell", "Memorial", "Award", "for", "Best", "Science", "Fiction", "Novel", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, discipline, country, enzyme, event, theory, academic journal, chemical element, chemical compound, organization, university, scientist, protein, person, astronomical object and O.\nSentence: It was nominated for the 2002 Hugo Award for Best Novel and tied for the 2002 John W. Campbell Memorial Award for Best Science Fiction Novel .", "prompt_labels": "It(O) was(O) nominated(O) for(O) the(O) 2002(O) Hugo(B-award) Award(I-award) for(I-award) Best(I-award) Novel(I-award) and(O) tied(O) for(O) the(O) 2002(O) John(B-award) W.(I-award) Campbell(I-award) Memorial(I-award) Award(I-award) for(I-award) Best(I-award) Science(I-award) Fiction(I-award) Novel(I-award) .(O)"}}
{"id": "215", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "award", "protein", "event", "organization", "astronomical object", "university", "enzyme", "country", "person", "academic journal", "scientist", "location", "theory", "discipline", "chemical compound"], "instance": {"id": "215", "words": ["Through", "the", "1980s", "three", "dedicated", "journals", "appeared", "in", "the", "field", ":", "Journal", "of", "Chemometrics", ",", "Chemometrics", "and", "Intelligent", "Laboratory", "Systems", ",", "and", "Journal", "of", "Chemical", "Information", "and", "Modeling", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, award, protein, event, organization, astronomical object, university, enzyme, country, person, academic journal, scientist, location, theory, discipline, chemical compound and O.\nSentence: Through the 1980s three dedicated journals appeared in the field : Journal of Chemometrics , Chemometrics and Intelligent Laboratory Systems , and Journal of Chemical Information and Modeling .", "prompt_labels": "Through(O) the(O) 1980s(O) three(O) dedicated(O) journals(O) appeared(O) in(O) the(O) field(O) :(O) Journal(B-academic journal) of(I-academic journal) Chemometrics(I-academic journal) ,(O) Chemometrics(B-academic journal) and(I-academic journal) Intelligent(I-academic journal) Laboratory(I-academic journal) Systems(I-academic journal) ,(O) and(O) Journal(B-academic journal) of(I-academic journal) Chemical(I-academic journal) Information(I-academic journal) and(I-academic journal) Modeling(I-academic journal) .(O)"}}
{"id": "368", "dataset": "crossner_science", "split": "dev", "label_list": ["event", "scientist", "theory", "location", "chemical compound", "person", "discipline", "protein", "astronomical object", "country", "award", "chemical element", "enzyme", "academic journal", "university", "organization"], "instance": {"id": "368", "words": ["The", "minor", "planets", "8749", "Beatles", ",", "4147", "Lennon", ",", "4148", "McCartney", "and", "4149", "Harrison", "were", "named", "after", "the", "band", "and", "its", "three", "other", "members", "."], "labels": ["O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, scientist, theory, location, chemical compound, person, discipline, protein, astronomical object, country, award, chemical element, enzyme, academic journal, university, organization and O.\nSentence: The minor planets 8749 Beatles , 4147 Lennon , 4148 McCartney and 4149 Harrison were named after the band and its three other members .", "prompt_labels": "The(O) minor(O) planets(O) 8749(B-astronomical object) Beatles(I-astronomical object) ,(O) 4147(B-astronomical object) Lennon(I-astronomical object) ,(O) 4148(B-astronomical object) McCartney(I-astronomical object) and(O) 4149(B-astronomical object) Harrison(I-astronomical object) were(O) named(O) after(O) the(O) band(O) and(O) its(O) three(O) other(O) members(O) .(O)"}}
{"id": "258", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "event", "university", "location", "protein", "country", "award", "organization", "academic journal", "theory", "astronomical object", "chemical compound", "enzyme", "scientist", "person", "discipline"], "instance": {"id": "258", "words": ["During", "the", "mid", "1970s", ",", "he", "spent", "considerable", "time", "in", "Japan", "at", "the", "University", "of", "Tokyo", "and", "also", "at", "Kwame", "Nkrumah", "University", "of", "Science", "and", "Technology", "in", "Kumasi", ",", "Ghana", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "I-university", "I-university", "O", "B-location", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, event, university, location, protein, country, award, organization, academic journal, theory, astronomical object, chemical compound, enzyme, scientist, person, discipline and O.\nSentence: During the mid 1970s , he spent considerable time in Japan at the University of Tokyo and also at Kwame Nkrumah University of Science and Technology in Kumasi , Ghana .", "prompt_labels": "During(O) the(O) mid(O) 1970s(O) ,(O) he(O) spent(O) considerable(O) time(O) in(O) Japan(B-country) at(O) the(O) University(B-university) of(I-university) Tokyo(I-university) and(O) also(O) at(O) Kwame(B-university) Nkrumah(I-university) University(I-university) of(I-university) Science(I-university) and(I-university) Technology(I-university) in(O) Kumasi(B-location) ,(O) Ghana(B-country) .(O)"}}
{"id": "415", "dataset": "crossner_science", "split": "dev", "label_list": ["location", "protein", "theory", "enzyme", "country", "astronomical object", "university", "organization", "academic journal", "event", "award", "person", "scientist", "chemical element", "discipline", "chemical compound"], "instance": {"id": "415", "words": ["ref", "name", "=", "necrop", "/", "Other", "groups", "first", "observed", "that", "the", "stimulation", "of", "Fas", "/", "TNFR", "family", "of", "Death", "domain", "receptors", "(", "DR", ")", "activated", "a", "canonical", "apoptotic", "pathway", ";", "however", ",", "in", "many", "cell", "types", ",", "not", "only", "did", "caspase", "inhibition", "fail", "to", "inhibit", "cell", "death", ",", "as", "would", "be", "expected", "of", "canonical", "apoptosis", ",", "but", "stimulated", "cells", "experienced", "a", "form", "of", "cell", "death", "that", "more", "closely", "resembled", "necrosis", "than", "apoptosis", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, protein, theory, enzyme, country, astronomical object, university, organization, academic journal, event, award, person, scientist, chemical element, discipline, chemical compound and O.\nSentence: ref name = necrop / Other groups first observed that the stimulation of Fas / TNFR family of Death domain receptors ( DR ) activated a canonical apoptotic pathway ; however , in many cell types , not only did caspase inhibition fail to inhibit cell death , as would be expected of canonical apoptosis , but stimulated cells experienced a form of cell death that more closely resembled necrosis than apoptosis .", "prompt_labels": "ref(O) name(O) =(O) necrop(O) /(O) Other(O) groups(O) first(O) observed(O) that(O) the(O) stimulation(O) of(O) Fas(O) /(O) TNFR(O) family(O) of(O) Death(O) domain(O) receptors(O) ((O) DR(O) )(O) activated(O) a(O) canonical(O) apoptotic(O) pathway(O) ;(O) however(O) ,(O) in(O) many(O) cell(O) types(O) ,(O) not(O) only(O) did(O) caspase(O) inhibition(O) fail(O) to(O) inhibit(O) cell(O) death(O) ,(O) as(O) would(O) be(O) expected(O) of(O) canonical(O) apoptosis(O) ,(O) but(O) stimulated(O) cells(O) experienced(O) a(O) form(O) of(O) cell(O) death(O) that(O) more(O) closely(O) resembled(O) necrosis(O) than(O) apoptosis(O) .(O)"}}
{"id": "437", "dataset": "crossner_science", "split": "dev", "label_list": ["organization", "academic journal", "astronomical object", "discipline", "chemical compound", "country", "person", "chemical element", "event", "location", "scientist", "enzyme", "award", "university", "protein", "theory"], "instance": {"id": "437", "words": ["It", "was", "discovered", "on", "26", "September", "1960", ",", "by", "Ingrid", "van", "Houten-Groeneveld", "and", "Cornelis", "van", "Houten", "at", "Leiden", ",", "and", "Tom", "Gehrels", "at", "Palomar", "Observatory", "in", "California", ",", "United", "States", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-location", "O", "O", "B-scientist", "I-scientist", "O", "B-location", "I-location", "O", "B-location", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, academic journal, astronomical object, discipline, chemical compound, country, person, chemical element, event, location, scientist, enzyme, award, university, protein, theory and O.\nSentence: It was discovered on 26 September 1960 , by Ingrid van Houten-Groeneveld and Cornelis van Houten at Leiden , and Tom Gehrels at Palomar Observatory in California , United States .", "prompt_labels": "It(O) was(O) discovered(O) on(O) 26(O) September(O) 1960(O) ,(O) by(O) Ingrid(B-scientist) van(I-scientist) Houten-Groeneveld(I-scientist) and(O) Cornelis(B-scientist) van(I-scientist) Houten(I-scientist) at(O) Leiden(B-location) ,(O) and(O) Tom(B-scientist) Gehrels(I-scientist) at(O) Palomar(B-location) Observatory(I-location) in(O) California(B-location) ,(O) United(B-country) States(I-country) .(O)"}}
{"id": "38", "dataset": "crossner_science", "split": "dev", "label_list": ["scientist", "astronomical object", "university", "country", "chemical element", "location", "protein", "award", "event", "person", "discipline", "academic journal", "theory", "chemical compound", "enzyme", "organization"], "instance": {"id": "38", "words": ["One", "aspect", ",", "the", "idea", "of", "modelling", "atomic", "behaviour", "under", "incident", "electromagnetic", "radiation", "using", "virtual", "oscillators", "at", "the", "absorption", "and", "emission", "frequencies", ",", "rather", "than", "the", "(", "different", ")", "apparent", "frequencies", "of", "the", "Bohr", "orbits", ",", "significantly", "led", "Max", "Born", ",", "Werner", "Heisenberg", "and", "Kramers", "to", "explore", "mathematics", "that", "strongly", "inspired", "the", "subsequent", "development", "of", "matrix", "mechanics", ",", "the", "first", "form", "of", "modern", "quantum", "mechanics", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-theory", "I-theory", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "O", "O", "B-discipline", "O", "O", "O", "O", "O", "O", "O", "B-discipline", "I-discipline", "O", "O", "O", "O", "O", "O", "B-discipline", "I-discipline", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, astronomical object, university, country, chemical element, location, protein, award, event, person, discipline, academic journal, theory, chemical compound, enzyme, organization and O.\nSentence: One aspect , the idea of modelling atomic behaviour under incident electromagnetic radiation using virtual oscillators at the absorption and emission frequencies , rather than the ( different ) apparent frequencies of the Bohr orbits , significantly led Max Born , Werner Heisenberg and Kramers to explore mathematics that strongly inspired the subsequent development of matrix mechanics , the first form of modern quantum mechanics .", "prompt_labels": "One(O) aspect(O) ,(O) the(O) idea(O) of(O) modelling(O) atomic(O) behaviour(O) under(O) incident(O) electromagnetic(O) radiation(O) using(O) virtual(O) oscillators(O) at(O) the(O) absorption(O) and(O) emission(O) frequencies(O) ,(O) rather(O) than(O) the(O) ((O) different(O) )(O) apparent(O) frequencies(O) of(O) the(O) Bohr(B-theory) orbits(I-theory) ,(O) significantly(O) led(O) Max(B-scientist) Born(I-scientist) ,(O) Werner(B-scientist) Heisenberg(I-scientist) and(O) Kramers(B-scientist) to(O) explore(O) mathematics(B-discipline) that(O) strongly(O) inspired(O) the(O) subsequent(O) development(O) of(O) matrix(B-discipline) mechanics(I-discipline) ,(O) the(O) first(O) form(O) of(O) modern(O) quantum(B-discipline) mechanics(I-discipline) .(O)"}}
{"id": "74", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "country", "location", "chemical element", "theory", "event", "discipline", "university", "astronomical object", "organization", "scientist", "chemical compound", "award", "academic journal", "person", "enzyme"], "instance": {"id": "74", "words": ["1879", "saw", "the", "establishment", "of", "the", "Anthropological", "Society", "of", "Washington", "(", "which", "first", "published", "the", "journal", "American", "Anthropologist", ",", "before", "it", "became", "a", "national", "journal", ")", ",", "and", "1882", "saw", "the", "American", "Association", "for", "the", "Advancement", "of", "Science", "established", "an", "anthropological", "section", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, country, location, chemical element, theory, event, discipline, university, astronomical object, organization, scientist, chemical compound, award, academic journal, person, enzyme and O.\nSentence: 1879 saw the establishment of the Anthropological Society of Washington ( which first published the journal American Anthropologist , before it became a national journal ) , and 1882 saw the American Association for the Advancement of Science established an anthropological section .", "prompt_labels": "1879(O) saw(O) the(O) establishment(O) of(O) the(O) Anthropological(B-organization) Society(I-organization) of(I-organization) Washington(I-organization) ((O) which(O) first(O) published(O) the(O) journal(O) American(B-academic journal) Anthropologist(I-academic journal) ,(O) before(O) it(O) became(O) a(O) national(O) journal(O) )(O) ,(O) and(O) 1882(O) saw(O) the(O) American(B-organization) Association(I-organization) for(I-organization) the(I-organization) Advancement(I-organization) of(I-organization) Science(I-organization) established(O) an(O) anthropological(O) section(O) .(O)"}}
{"id": "156", "dataset": "crossner_science", "split": "dev", "label_list": ["university", "chemical element", "scientist", "academic journal", "enzyme", "location", "award", "chemical compound", "discipline", "organization", "protein", "theory", "person", "event", "astronomical object", "country"], "instance": {"id": "156", "words": ["An", "approved", "residency", "program", "and", "certification", "(", "in", "the", "U.S.", ",", "the", "American", "Board", "of", "Pathology", "or", "the", "American", "Osteopathic", "Board", "of", "Pathology", ")", "is", "usually", "required", "to", "obtain", "employment", "or", "hospital", "privileges", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, chemical element, scientist, academic journal, enzyme, location, award, chemical compound, discipline, organization, protein, theory, person, event, astronomical object, country and O.\nSentence: An approved residency program and certification ( in the U.S. , the American Board of Pathology or the American Osteopathic Board of Pathology ) is usually required to obtain employment or hospital privileges .", "prompt_labels": "An(O) approved(O) residency(O) program(O) and(O) certification(O) ((O) in(O) the(O) U.S.(B-country) ,(O) the(O) American(B-organization) Board(I-organization) of(I-organization) Pathology(I-organization) or(O) the(O) American(B-organization) Osteopathic(I-organization) Board(I-organization) of(I-organization) Pathology(I-organization) )(O) is(O) usually(O) required(O) to(O) obtain(O) employment(O) or(O) hospital(O) privileges(O) .(O)"}}
{"id": "352", "dataset": "crossner_science", "split": "dev", "label_list": ["academic journal", "location", "scientist", "discipline", "organization", "country", "event", "university", "award", "chemical compound", "person", "protein", "astronomical object", "chemical element", "enzyme", "theory"], "instance": {"id": "352", "words": ["PKA", "affects", "both", "the", "6,7-dihydropteridine", "reductase", "and", "RyR", ",", "increasing", "the", "rise", "in", "Ca", "sup2", "+", "/", "sup", "within", "the", "contractile", "cells", "and", "therefore", "increasing", "rate", "of", "muscle", "contraction", "."], "labels": ["O", "O", "O", "O", "B-enzyme", "I-enzyme", "O", "B-protein", "O", "O", "O", "O", "O", "B-chemical element", "I-chemical element", "I-chemical element", "I-chemical element", "I-chemical element", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, location, scientist, discipline, organization, country, event, university, award, chemical compound, person, protein, astronomical object, chemical element, enzyme, theory and O.\nSentence: PKA affects both the 6,7-dihydropteridine reductase and RyR , increasing the rise in Ca sup2 + / sup within the contractile cells and therefore increasing rate of muscle contraction .", "prompt_labels": "PKA(O) affects(O) both(O) the(O) 6,7-dihydropteridine(B-enzyme) reductase(I-enzyme) and(O) RyR(B-protein) ,(O) increasing(O) the(O) rise(O) in(O) Ca(B-chemical element) sup2(I-chemical element) +(I-chemical element) /(I-chemical element) sup(I-chemical element) within(O) the(O) contractile(O) cells(O) and(O) therefore(O) increasing(O) rate(O) of(O) muscle(O) contraction(O) .(O)"}}
{"id": "42", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "discipline", "theory", "country", "chemical element", "academic journal", "chemical compound", "location", "award", "event", "person", "organization", "enzyme", "university", "scientist", "protein"], "instance": {"id": "42", "words": ["Michelson", "was", "a", "member", "of", "the", "Royal", "Society", ",", "the", "National", "Academy", "of", "Sciences", ",", "the", "American", "Physical", "Society", "and", "the", "American", "Association", "for", "the", "Advancement", "of", "Science", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, discipline, theory, country, chemical element, academic journal, chemical compound, location, award, event, person, organization, enzyme, university, scientist, protein and O.\nSentence: Michelson was a member of the Royal Society , the National Academy of Sciences , the American Physical Society and the American Association for the Advancement of Science .", "prompt_labels": "Michelson(B-scientist) was(O) a(O) member(O) of(O) the(O) Royal(B-organization) Society(I-organization) ,(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) the(O) American(B-organization) Physical(I-organization) Society(I-organization) and(O) the(O) American(B-organization) Association(I-organization) for(I-organization) the(I-organization) Advancement(I-organization) of(I-organization) Science(I-organization) .(O)"}}
{"id": "256", "dataset": "crossner_science", "split": "dev", "label_list": ["enzyme", "award", "person", "event", "location", "astronomical object", "chemical compound", "chemical element", "scientist", "discipline", "protein", "academic journal", "organization", "country", "university", "theory"], "instance": {"id": "256", "words": ["The", "corresponding", "extension", "problem", "for", "diffeomorphisms", "of", "higher-dimensional", "spheres", "Ssup", "n", "−", "1", "/", "sup", "was", "much", "studied", "in", "the", "1950s", "and", "1960s", ",", "with", "notable", "contributions", "from", "René", "Thom", ",", "John", "Milnor", "and", "Stephen", "Smale", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, award, person, event, location, astronomical object, chemical compound, chemical element, scientist, discipline, protein, academic journal, organization, country, university, theory and O.\nSentence: The corresponding extension problem for diffeomorphisms of higher-dimensional spheres Ssup n − 1 / sup was much studied in the 1950s and 1960s , with notable contributions from René Thom , John Milnor and Stephen Smale .", "prompt_labels": "The(O) corresponding(O) extension(O) problem(O) for(O) diffeomorphisms(O) of(O) higher-dimensional(O) spheres(O) Ssup(O) n(O) −(O) 1(O) /(O) sup(O) was(O) much(O) studied(O) in(O) the(O) 1950s(O) and(O) 1960s(O) ,(O) with(O) notable(O) contributions(O) from(O) René(B-scientist) Thom(I-scientist) ,(O) John(B-scientist) Milnor(I-scientist) and(O) Stephen(B-scientist) Smale(I-scientist) .(O)"}}
{"id": "112", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "scientist", "chemical element", "event", "academic journal", "person", "protein", "country", "chemical compound", "organization", "theory", "astronomical object", "award", "enzyme", "location", "university"], "instance": {"id": "112", "words": ["It", "was", "discovered", "on", "24", "September", "1960", ",", "by", "astronomers", "Cornelis", "Johannes", "van", "Houten", ",", "Ingrid", "van", "Houten-Groeneveld", "and", "Tom", "Gehrels", "at", "Palomar", "Observatory", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, scientist, chemical element, event, academic journal, person, protein, country, chemical compound, organization, theory, astronomical object, award, enzyme, location, university and O.\nSentence: It was discovered on 24 September 1960 , by astronomers Cornelis Johannes van Houten , Ingrid van Houten-Groeneveld and Tom Gehrels at Palomar Observatory .", "prompt_labels": "It(O) was(O) discovered(O) on(O) 24(O) September(O) 1960(O) ,(O) by(O) astronomers(O) Cornelis(B-scientist) Johannes(I-scientist) van(I-scientist) Houten(I-scientist) ,(O) Ingrid(B-scientist) van(I-scientist) Houten-Groeneveld(I-scientist) and(O) Tom(B-scientist) Gehrels(I-scientist) at(O) Palomar(B-location) Observatory(I-location) .(O)"}}
{"id": "31", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "person", "university", "event", "enzyme", "astronomical object", "scientist", "academic journal", "chemical element", "organization", "award", "location", "country", "theory", "protein", "chemical compound"], "instance": {"id": "31", "words": ["Usually", ",", "in", "the", "presence", "of", "NADPH", "dehydrogenase", "or", "NADH", "dehydrogenase", "as", "the", "enzyme", ",", "NADPH", "or", "NADH", "is", "the", "reductant", "that", "converts", "resazurin", "to", "resorufin", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "O", "B-enzyme", "I-enzyme", "O", "O", "O", "O", "B-chemical compound", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, person, university, event, enzyme, astronomical object, scientist, academic journal, chemical element, organization, award, location, country, theory, protein, chemical compound and O.\nSentence: Usually , in the presence of NADPH dehydrogenase or NADH dehydrogenase as the enzyme , NADPH or NADH is the reductant that converts resazurin to resorufin .", "prompt_labels": "Usually(O) ,(O) in(O) the(O) presence(O) of(O) NADPH(B-enzyme) dehydrogenase(I-enzyme) or(O) NADH(B-enzyme) dehydrogenase(I-enzyme) as(O) the(O) enzyme(O) ,(O) NADPH(B-chemical compound) or(O) NADH(B-chemical compound) is(O) the(O) reductant(O) that(O) converts(O) resazurin(O) to(O) resorufin(O) .(O)"}}
{"id": "96", "dataset": "crossner_science", "split": "dev", "label_list": ["enzyme", "theory", "chemical element", "astronomical object", "academic journal", "award", "location", "discipline", "event", "chemical compound", "person", "organization", "university", "country", "protein", "scientist"], "instance": {"id": "96", "words": ["Gas", "giants", "with", "a", "large", "radius", "and", "very", "low", "density", "are", "sometimes", "called", "puffy", "planets", "COROT-1b", ",", "TrES-4", ",", "WASP-12b", ",", "WASP-17b", ",", "and", "Kepler-7b", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, theory, chemical element, astronomical object, academic journal, award, location, discipline, event, chemical compound, person, organization, university, country, protein, scientist and O.\nSentence: Gas giants with a large radius and very low density are sometimes called puffy planets COROT-1b , TrES-4 , WASP-12b , WASP-17b , and Kepler-7b .", "prompt_labels": "Gas(O) giants(O) with(O) a(O) large(O) radius(O) and(O) very(O) low(O) density(O) are(O) sometimes(O) called(O) puffy(O) planets(O) COROT-1b(B-astronomical object) ,(O) TrES-4(B-astronomical object) ,(O) WASP-12b(B-astronomical object) ,(O) WASP-17b(B-astronomical object) ,(O) and(O) Kepler-7b(B-astronomical object) .(O)"}}
{"id": "187", "dataset": "crossner_science", "split": "dev", "label_list": ["enzyme", "country", "event", "chemical compound", "scientist", "award", "location", "protein", "chemical element", "astronomical object", "university", "academic journal", "discipline", "person", "organization", "theory"], "instance": {"id": "187", "words": ["The", "orbit", "of", "53", "Kalypso", "places", "it", "in", "a", "mean", "motion", "resonance", "with", "the", "planets", "Jupiter", "and", "Saturn", "."], "labels": ["O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, country, event, chemical compound, scientist, award, location, protein, chemical element, astronomical object, university, academic journal, discipline, person, organization, theory and O.\nSentence: The orbit of 53 Kalypso places it in a mean motion resonance with the planets Jupiter and Saturn .", "prompt_labels": "The(O) orbit(O) of(O) 53(B-astronomical object) Kalypso(I-astronomical object) places(O) it(O) in(O) a(O) mean(O) motion(O) resonance(O) with(O) the(O) planets(O) Jupiter(B-astronomical object) and(O) Saturn(B-astronomical object) .(O)"}}
{"id": "281", "dataset": "crossner_science", "split": "dev", "label_list": ["university", "country", "protein", "chemical element", "astronomical object", "enzyme", "person", "organization", "theory", "chemical compound", "location", "scientist", "award", "event", "academic journal", "discipline"], "instance": {"id": "281", "words": ["He", "was", "a", "visiting", "Scientist", "at", "Massachusetts", "General", "Hospital", ",", "Cambridge", ",", "MA", "&", "amp", ";", "Research", "Fellow", ",", "Shriners", "Hospitals", "for", "Children", ",", "Cambridge", ",", "MA", "from", "2006-2012", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-university", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-university", "O", "B-location", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, country, protein, chemical element, astronomical object, enzyme, person, organization, theory, chemical compound, location, scientist, award, event, academic journal, discipline and O.\nSentence: He was a visiting Scientist at Massachusetts General Hospital , Cambridge , MA & amp ; Research Fellow , Shriners Hospitals for Children , Cambridge , MA from 2006-2012 .", "prompt_labels": "He(O) was(O) a(O) visiting(O) Scientist(O) at(O) Massachusetts(B-organization) General(I-organization) Hospital(I-organization) ,(O) Cambridge(B-university) ,(O) MA(B-location) &(O) amp(O) ;(O) Research(O) Fellow(O) ,(O) Shriners(B-organization) Hospitals(I-organization) for(I-organization) Children(I-organization) ,(O) Cambridge(B-university) ,(O) MA(B-location) from(O) 2006-2012(O) .(O)"}}
{"id": "430", "dataset": "crossner_science", "split": "dev", "label_list": ["enzyme", "astronomical object", "discipline", "event", "award", "chemical element", "country", "protein", "person", "academic journal", "location", "scientist", "chemical compound", "theory", "university", "organization"], "instance": {"id": "430", "words": ["The", "invitees", "included", "Walther", "Bothe", ",", "Siegfried", "Flügge", ",", "Hans", "Geiger", ",", "Otto", "Hahn", ",", "Paul", "Harteck", ",", "Gerhard", "Hoffmann", ",", "Josef", "Mattauch", ",", "and", "Georg", "Stetter", "."], "labels": ["O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, astronomical object, discipline, event, award, chemical element, country, protein, person, academic journal, location, scientist, chemical compound, theory, university, organization and O.\nSentence: The invitees included Walther Bothe , Siegfried Flügge , Hans Geiger , Otto Hahn , Paul Harteck , Gerhard Hoffmann , Josef Mattauch , and Georg Stetter .", "prompt_labels": "The(O) invitees(O) included(O) Walther(B-scientist) Bothe(I-scientist) ,(O) Siegfried(B-scientist) Flügge(I-scientist) ,(O) Hans(B-scientist) Geiger(I-scientist) ,(O) Otto(B-scientist) Hahn(I-scientist) ,(O) Paul(B-scientist) Harteck(I-scientist) ,(O) Gerhard(B-scientist) Hoffmann(I-scientist) ,(O) Josef(B-scientist) Mattauch(I-scientist) ,(O) and(O) Georg(B-scientist) Stetter(I-scientist) .(O)"}}
{"id": "113", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "protein", "discipline", "award", "location", "event", "academic journal", "astronomical object", "person", "chemical compound", "scientist", "country", "enzyme", "university", "theory", "organization"], "instance": {"id": "113", "words": ["Major", "species", "assessors", "include", "BirdLife", "International", ",", "the", "Institute", "of", "Zoology", "(", "the", "research", "division", "of", "the", "Zoological", "Society", "of", "London", ")", ",", "the", "World", "Conservation", "Monitoring", "Centre", ",", "and", "many", "Specialist", "Groups", "within", "the", "IUCN", "Species", "Survival", "Commission", "(", "SSC", ")", "."], "labels": ["O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, protein, discipline, award, location, event, academic journal, astronomical object, person, chemical compound, scientist, country, enzyme, university, theory, organization and O.\nSentence: Major species assessors include BirdLife International , the Institute of Zoology ( the research division of the Zoological Society of London ) , the World Conservation Monitoring Centre , and many Specialist Groups within the IUCN Species Survival Commission ( SSC ) .", "prompt_labels": "Major(O) species(O) assessors(O) include(O) BirdLife(B-organization) International(I-organization) ,(O) the(O) Institute(B-organization) of(I-organization) Zoology(I-organization) ((O) the(O) research(O) division(O) of(O) the(O) Zoological(B-organization) Society(I-organization) of(I-organization) London(I-organization) )(O) ,(O) the(O) World(B-organization) Conservation(I-organization) Monitoring(I-organization) Centre(I-organization) ,(O) and(O) many(O) Specialist(O) Groups(O) within(O) the(O) IUCN(B-organization) Species(I-organization) Survival(I-organization) Commission(I-organization) ((O) SSC(B-organization) )(O) .(O)"}}
{"id": "10", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical compound", "location", "protein", "event", "organization", "theory", "country", "scientist", "award", "enzyme", "discipline", "chemical element", "person", "university", "academic journal", "astronomical object"], "instance": {"id": "10", "words": ["He", "is", "currently", "Director", "of", "the", "Yale", "Center", "for", "the", "Study", "of", "Globalization", "at", "Yale", "University", ",", "is", "the", "Latin", "American", "co-chair", "of", "the", "Inter-American", "Dialogue", ",", "and", "is", "on", "the", "board", "of", "directors", "of", "Citigroup", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, location, protein, event, organization, theory, country, scientist, award, enzyme, discipline, chemical element, person, university, academic journal, astronomical object and O.\nSentence: He is currently Director of the Yale Center for the Study of Globalization at Yale University , is the Latin American co-chair of the Inter-American Dialogue , and is on the board of directors of Citigroup .", "prompt_labels": "He(O) is(O) currently(O) Director(O) of(O) the(O) Yale(B-organization) Center(I-organization) for(I-organization) the(I-organization) Study(I-organization) of(I-organization) Globalization(I-organization) at(O) Yale(B-university) University(I-university) ,(O) is(O) the(O) Latin(O) American(O) co-chair(O) of(O) the(O) Inter-American(B-organization) Dialogue(I-organization) ,(O) and(O) is(O) on(O) the(O) board(O) of(O) directors(O) of(O) Citigroup(B-organization) .(O)"}}
{"id": "147", "dataset": "crossner_science", "split": "dev", "label_list": ["scientist", "organization", "theory", "event", "astronomical object", "chemical element", "location", "chemical compound", "enzyme", "country", "person", "award", "university", "protein", "discipline", "academic journal"], "instance": {"id": "147", "words": ["In", "fact", ",", "it", "is", "the", "third", "dimmest", "of", "the", "first", "twenty-three", "asteroids", "discovered", ",", "with", "only", "13", "Egeria", "and", "17", "Thetis", "having", "lower", "mean", "opposition", "magnitude", "s", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, organization, theory, event, astronomical object, chemical element, location, chemical compound, enzyme, country, person, award, university, protein, discipline, academic journal and O.\nSentence: In fact , it is the third dimmest of the first twenty-three asteroids discovered , with only 13 Egeria and 17 Thetis having lower mean opposition magnitude s .", "prompt_labels": "In(O) fact(O) ,(O) it(O) is(O) the(O) third(O) dimmest(O) of(O) the(O) first(O) twenty-three(O) asteroids(O) discovered(O) ,(O) with(O) only(O) 13(B-astronomical object) Egeria(I-astronomical object) and(O) 17(B-astronomical object) Thetis(I-astronomical object) having(O) lower(O) mean(O) opposition(O) magnitude(O) s(O) .(O)"}}
{"id": "136", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "chemical compound", "award", "astronomical object", "location", "person", "event", "enzyme", "organization", "discipline", "scientist", "theory", "chemical element", "academic journal", "country", "university"], "instance": {"id": "136", "words": ["Gas", "giants", "with", "a", "large", "radius", "and", "very", "low", "density", "are", "sometimes", "called", "puffy", "planets", "COROT-1b", ",", "TrES-4", ",", "WASP-12b", ",", "WASP-17b", ",", "and", "Kepler-7b", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, chemical compound, award, astronomical object, location, person, event, enzyme, organization, discipline, scientist, theory, chemical element, academic journal, country, university and O.\nSentence: Gas giants with a large radius and very low density are sometimes called puffy planets COROT-1b , TrES-4 , WASP-12b , WASP-17b , and Kepler-7b .", "prompt_labels": "Gas(O) giants(O) with(O) a(O) large(O) radius(O) and(O) very(O) low(O) density(O) are(O) sometimes(O) called(O) puffy(O) planets(O) COROT-1b(B-astronomical object) ,(O) TrES-4(B-astronomical object) ,(O) WASP-12b(B-astronomical object) ,(O) WASP-17b(B-astronomical object) ,(O) and(O) Kepler-7b(B-astronomical object) .(O)"}}
{"id": "343", "dataset": "crossner_science", "split": "dev", "label_list": ["country", "discipline", "theory", "enzyme", "event", "organization", "academic journal", "astronomical object", "scientist", "person", "chemical compound", "chemical element", "location", "university", "protein", "award"], "instance": {"id": "343", "words": ["Ethylmercury", "is", "a", "breakdown", "product", "of", "the", "antibacteriological", "agent", "ethylmercurithiosalicylate", ",", "which", "has", "been", "used", "as", "a", "topical", "antiseptic", "and", "a", "vaccine", "preservative", "(", "further", "discussed", "under", "Thiomersal", "below", ")", "."], "labels": ["B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, discipline, theory, enzyme, event, organization, academic journal, astronomical object, scientist, person, chemical compound, chemical element, location, university, protein, award and O.\nSentence: Ethylmercury is a breakdown product of the antibacteriological agent ethylmercurithiosalicylate , which has been used as a topical antiseptic and a vaccine preservative ( further discussed under Thiomersal below ) .", "prompt_labels": "Ethylmercury(B-chemical compound) is(O) a(O) breakdown(O) product(O) of(O) the(O) antibacteriological(O) agent(O) ethylmercurithiosalicylate(B-chemical compound) ,(O) which(O) has(O) been(O) used(O) as(O) a(O) topical(O) antiseptic(O) and(O) a(O) vaccine(O) preservative(O) ((O) further(O) discussed(O) under(O) Thiomersal(B-chemical compound) below(O) )(O) .(O)"}}
{"id": "164", "dataset": "crossner_science", "split": "dev", "label_list": ["university", "enzyme", "event", "discipline", "award", "chemical compound", "astronomical object", "chemical element", "protein", "scientist", "country", "theory", "organization", "location", "academic journal", "person"], "instance": {"id": "164", "words": ["Published", "in", "1993", ",", "it", "won", "the", "1994", "Nebula", "Award", "for", "Best", "Novel", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, enzyme, event, discipline, award, chemical compound, astronomical object, chemical element, protein, scientist, country, theory, organization, location, academic journal, person and O.\nSentence: Published in 1993 , it won the 1994 Nebula Award for Best Novel .", "prompt_labels": "Published(O) in(O) 1993(O) ,(O) it(O) won(O) the(O) 1994(O) Nebula(B-award) Award(I-award) for(I-award) Best(I-award) Novel(I-award) .(O)"}}
{"id": "237", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "organization", "location", "discipline", "enzyme", "person", "protein", "chemical element", "event", "scientist", "country", "academic journal", "award", "university", "theory", "chemical compound"], "instance": {"id": "237", "words": ["The", "Institute", "is", "a", "partnership", "among", "Harvard", "University", ",", "its", "major", "affiliated", "hospitals", "(", "Beth", "Israel", "Deaconess", "Medical", "Center", ",", "Brigham", "and", "Women", "'s", "Hospital", ",", "Boston", "Children", "'s", "Hospital", ",", "Dana", "Farber", "Cancer", "Institute", ",", "Massachusetts", "General", "Hospital", ",", "Spaulding", "Rehabilitation", "Hospital", ")", ",", "Boston", "University", ",", "Massachusetts", "Institute", "of", "Technology", ",", "Tufts", "University", ",", "University", "of", "Massachusetts", "Medical", "School", ",", "Charité", "-", "Universitätsmedizin", "Berlin", ",", "and", "University", "of", "Zurich", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-university", "I-university", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-university", "I-university", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-university", "I-university", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "O", "B-university", "I-university", "I-university", "I-university", "O", "O", "B-university", "I-university", "I-university", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, organization, location, discipline, enzyme, person, protein, chemical element, event, scientist, country, academic journal, award, university, theory, chemical compound and O.\nSentence: The Institute is a partnership among Harvard University , its major affiliated hospitals ( Beth Israel Deaconess Medical Center , Brigham and Women 's Hospital , Boston Children 's Hospital , Dana Farber Cancer Institute , Massachusetts General Hospital , Spaulding Rehabilitation Hospital ) , Boston University , Massachusetts Institute of Technology , Tufts University , University of Massachusetts Medical School , Charité - Universitätsmedizin Berlin , and University of Zurich .", "prompt_labels": "The(O) Institute(O) is(O) a(O) partnership(O) among(O) Harvard(B-university) University(I-university) ,(O) its(O) major(O) affiliated(O) hospitals(O) ((O) Beth(B-organization) Israel(I-organization) Deaconess(I-organization) Medical(I-organization) Center(I-organization) ,(O) Brigham(B-organization) and(I-organization) Women(I-organization) 's(I-organization) Hospital(I-organization) ,(O) Boston(B-organization) Children(I-organization) 's(I-organization) Hospital(I-organization) ,(O) Dana(B-organization) Farber(I-organization) Cancer(I-organization) Institute(I-organization) ,(O) Massachusetts(B-organization) General(I-organization) Hospital(I-organization) ,(O) Spaulding(B-organization) Rehabilitation(I-organization) Hospital(I-organization) )(O) ,(O) Boston(B-university) University(I-university) ,(O) Massachusetts(B-organization) Institute(I-organization) of(I-organization) Technology(I-organization) ,(O) Tufts(B-university) University(I-university) ,(O) University(B-university) of(I-university) Massachusetts(I-university) Medical(I-university) School(I-university) ,(O) Charité(B-university) -(I-university) Universitätsmedizin(I-university) Berlin(I-university) ,(O) and(O) University(B-university) of(I-university) Zurich(I-university) .(O)"}}
{"id": "167", "dataset": "crossner_science", "split": "dev", "label_list": ["academic journal", "protein", "scientist", "award", "theory", "chemical element", "university", "location", "person", "astronomical object", "event", "country", "organization", "enzyme", "discipline", "chemical compound"], "instance": {"id": "167", "words": ["She", "also", "played", "at", "1986", ",", "1989", ",", "1991", ",", "1993", ",", "1995", "AFC", "Championship", ",", "1990", "and", "Football", "at", "the", "1994", "Asian", "Games", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, protein, scientist, award, theory, chemical element, university, location, person, astronomical object, event, country, organization, enzyme, discipline, chemical compound and O.\nSentence: She also played at 1986 , 1989 , 1991 , 1993 , 1995 AFC Championship , 1990 and Football at the 1994 Asian Games .", "prompt_labels": "She(O) also(O) played(O) at(O) 1986(O) ,(O) 1989(O) ,(O) 1991(O) ,(O) 1993(O) ,(O) 1995(O) AFC(B-event) Championship(I-event) ,(O) 1990(O) and(O) Football(O) at(O) the(O) 1994(O) Asian(B-event) Games(I-event) .(O)"}}
{"id": "370", "dataset": "crossner_science", "split": "dev", "label_list": ["academic journal", "chemical element", "astronomical object", "chemical compound", "person", "scientist", "enzyme", "protein", "organization", "theory", "country", "location", "event", "award", "university", "discipline"], "instance": {"id": "370", "words": ["Silindo", "may", "be", "Jupiter", ";", "Carnil", ",", "Mars", ";", "Elemmire", ",", "Mercury", ";", "Luinil", ",", "Uranus", ";", "Lumbar", ",", "Saturn", ";", "and", "Nenar", ",", "Neptune", "."], "labels": ["O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, chemical element, astronomical object, chemical compound, person, scientist, enzyme, protein, organization, theory, country, location, event, award, university, discipline and O.\nSentence: Silindo may be Jupiter ; Carnil , Mars ; Elemmire , Mercury ; Luinil , Uranus ; Lumbar , Saturn ; and Nenar , Neptune .", "prompt_labels": "Silindo(O) may(O) be(O) Jupiter(B-astronomical object) ;(O) Carnil(B-astronomical object) ,(O) Mars(B-astronomical object) ;(O) Elemmire(B-astronomical object) ,(O) Mercury(B-astronomical object) ;(O) Luinil(B-astronomical object) ,(O) Uranus(B-astronomical object) ;(O) Lumbar(B-astronomical object) ,(O) Saturn(B-astronomical object) ;(O) and(O) Nenar(B-astronomical object) ,(O) Neptune(B-astronomical object) .(O)"}}
{"id": "327", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "person", "academic journal", "protein", "discipline", "country", "enzyme", "organization", "university", "event", "scientist", "location", "astronomical object", "theory", "award", "chemical compound"], "instance": {"id": "327", "words": ["Kandler", "was", "the", "founder", "and", "editor", "of", "Systematic", "and", "Applied", "Microbiology", ",", "co-editor", "of", "the", "Archives", "of", "Microbiology", "and", "of", "Zeitschrift", "für", "Pflanzenphysiologie", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, person, academic journal, protein, discipline, country, enzyme, organization, university, event, scientist, location, astronomical object, theory, award, chemical compound and O.\nSentence: Kandler was the founder and editor of Systematic and Applied Microbiology , co-editor of the Archives of Microbiology and of Zeitschrift für Pflanzenphysiologie .", "prompt_labels": "Kandler(B-scientist) was(O) the(O) founder(O) and(O) editor(O) of(O) Systematic(B-academic journal) and(I-academic journal) Applied(I-academic journal) Microbiology(I-academic journal) ,(O) co-editor(O) of(O) the(O) Archives(B-academic journal) of(I-academic journal) Microbiology(I-academic journal) and(O) of(O) Zeitschrift(B-academic journal) für(I-academic journal) Pflanzenphysiologie(I-academic journal) .(O)"}}
{"id": "434", "dataset": "crossner_science", "split": "dev", "label_list": ["university", "protein", "astronomical object", "scientist", "organization", "discipline", "country", "award", "theory", "academic journal", "chemical element", "location", "enzyme", "person", "chemical compound", "event"], "instance": {"id": "434", "words": ["4", "Vesta", "is", "the", "largest", "asteroid", "in", "the", "inner", "zone", ",", "1", "Ceres", "and", "2", "Pallas", "in", "the", "middle", "zone", ",", "and", "10", "Hygiea", "in", "the", "outer", "zone", "."], "labels": ["B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, protein, astronomical object, scientist, organization, discipline, country, award, theory, academic journal, chemical element, location, enzyme, person, chemical compound, event and O.\nSentence: 4 Vesta is the largest asteroid in the inner zone , 1 Ceres and 2 Pallas in the middle zone , and 10 Hygiea in the outer zone .", "prompt_labels": "4(B-astronomical object) Vesta(I-astronomical object) is(O) the(O) largest(O) asteroid(O) in(O) the(O) inner(O) zone(O) ,(O) 1(B-astronomical object) Ceres(I-astronomical object) and(O) 2(B-astronomical object) Pallas(I-astronomical object) in(O) the(O) middle(O) zone(O) ,(O) and(O) 10(B-astronomical object) Hygiea(I-astronomical object) in(O) the(O) outer(O) zone(O) .(O)"}}
{"id": "159", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "chemical compound", "theory", "chemical element", "academic journal", "enzyme", "country", "discipline", "scientist", "location", "university", "person", "event", "award", "organization", "protein"], "instance": {"id": "159", "words": ["Bennett", "was", "the", "flight", "safety", "manager", "for", "the", "radioisotope", "power", "sources", "currently", "in", "use", "on", "the", "Voyager", "1", "and", "Voyager", "2", "spacecraft", "(", "which", "went", "to", "Jupiter", ",", "Saturn", ",", "Uranus", ",", "Neptune", "and", "beyond", ")", "and", "on", "Lincoln", "Laboratory", "'", "s", "LES", "8", "and", "LES", "9", "communications", "satellites", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, chemical compound, theory, chemical element, academic journal, enzyme, country, discipline, scientist, location, university, person, event, award, organization, protein and O.\nSentence: Bennett was the flight safety manager for the radioisotope power sources currently in use on the Voyager 1 and Voyager 2 spacecraft ( which went to Jupiter , Saturn , Uranus , Neptune and beyond ) and on Lincoln Laboratory ' s LES 8 and LES 9 communications satellites .", "prompt_labels": "Bennett(B-person) was(O) the(O) flight(O) safety(O) manager(O) for(O) the(O) radioisotope(O) power(O) sources(O) currently(O) in(O) use(O) on(O) the(O) Voyager(O) 1(O) and(O) Voyager(O) 2(O) spacecraft(O) ((O) which(O) went(O) to(O) Jupiter(B-astronomical object) ,(O) Saturn(B-astronomical object) ,(O) Uranus(B-astronomical object) ,(O) Neptune(B-astronomical object) and(O) beyond(O) )(O) and(O) on(O) Lincoln(B-organization) Laboratory(I-organization) '(O) s(O) LES(O) 8(O) and(O) LES(O) 9(O) communications(O) satellites(O) .(O)"}}
{"id": "92", "dataset": "crossner_science", "split": "dev", "label_list": ["scientist", "chemical compound", "enzyme", "organization", "theory", "chemical element", "country", "location", "award", "discipline", "astronomical object", "person", "university", "academic journal", "event", "protein"], "instance": {"id": "92", "words": ["Of", "the", "eight", "solar", "planet", "s", ",", "all", "but", "Venus", "and", "Uranus", "have", "prograde", "rotation", "-", "that", "is", ",", "they", "rotate", "more", "than", "once", "per", "year", "in", "the", "same", "direction", "as", "they", "orbit", "the", "Sun", ",", "so", "the", "Sun", "rises", "in", "the", "east", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, chemical compound, enzyme, organization, theory, chemical element, country, location, award, discipline, astronomical object, person, university, academic journal, event, protein and O.\nSentence: Of the eight solar planet s , all but Venus and Uranus have prograde rotation - that is , they rotate more than once per year in the same direction as they orbit the Sun , so the Sun rises in the east .", "prompt_labels": "Of(O) the(O) eight(O) solar(O) planet(O) s(O) ,(O) all(O) but(O) Venus(B-astronomical object) and(O) Uranus(B-astronomical object) have(O) prograde(O) rotation(O) -(O) that(O) is(O) ,(O) they(O) rotate(O) more(O) than(O) once(O) per(O) year(O) in(O) the(O) same(O) direction(O) as(O) they(O) orbit(O) the(O) Sun(B-astronomical object) ,(O) so(O) the(O) Sun(B-astronomical object) rises(O) in(O) the(O) east(O) .(O)"}}
{"id": "223", "dataset": "crossner_science", "split": "dev", "label_list": ["university", "protein", "event", "organization", "country", "chemical compound", "discipline", "enzyme", "scientist", "location", "person", "academic journal", "award", "astronomical object", "theory", "chemical element"], "instance": {"id": "223", "words": ["During", "the", "special", "the", "Doctor", "is", "forced", "to", "regenerate", "several", "times", ",", "with", "his", "subsequent", "incarnations", "played", "by", ",", "in", "order", ",", "Richard", "E.", "Grant", ",", "Jim", "Broadbent", ",", "Hugh", "Grant", "and", "Joanna", "Lumley", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, protein, event, organization, country, chemical compound, discipline, enzyme, scientist, location, person, academic journal, award, astronomical object, theory, chemical element and O.\nSentence: During the special the Doctor is forced to regenerate several times , with his subsequent incarnations played by , in order , Richard E. Grant , Jim Broadbent , Hugh Grant and Joanna Lumley .", "prompt_labels": "During(O) the(O) special(O) the(O) Doctor(O) is(O) forced(O) to(O) regenerate(O) several(O) times(O) ,(O) with(O) his(O) subsequent(O) incarnations(O) played(O) by(O) ,(O) in(O) order(O) ,(O) Richard(B-person) E.(I-person) Grant(I-person) ,(O) Jim(B-person) Broadbent(I-person) ,(O) Hugh(B-person) Grant(I-person) and(O) Joanna(B-person) Lumley(I-person) .(O)"}}
{"id": "128", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "chemical element", "university", "person", "event", "astronomical object", "scientist", "academic journal", "location", "organization", "chemical compound", "award", "protein", "theory", "enzyme", "country"], "instance": {"id": "128", "words": ["This", "range", ",", "as", "well", "as", "the", "relative", "speeds", "between", "the", "planets", ",", "led", "Kepler", "to", "conclude", "that", "the", "Solar", "System", "was", "composed", "of", "two", "basses", "(", "Saturn", "and", "Jupiter", ")", ",", "a", "tenor", "(", "Mars", ")", ",", "two", "altos", "(", "Venus", "and", "Earth", ")", ",", "and", "a", "soprano", "(", "Mercury", ")", ",", "which", "had", "sung", "in", "perfect", "concord", ",", "at", "the", "beginning", "of", "time", ",", "and", "could", "potentially", "arrange", "themselves", "to", "do", "so", "again", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, chemical element, university, person, event, astronomical object, scientist, academic journal, location, organization, chemical compound, award, protein, theory, enzyme, country and O.\nSentence: This range , as well as the relative speeds between the planets , led Kepler to conclude that the Solar System was composed of two basses ( Saturn and Jupiter ) , a tenor ( Mars ) , two altos ( Venus and Earth ) , and a soprano ( Mercury ) , which had sung in perfect concord , at the beginning of time , and could potentially arrange themselves to do so again .", "prompt_labels": "This(O) range(O) ,(O) as(O) well(O) as(O) the(O) relative(O) speeds(O) between(O) the(O) planets(O) ,(O) led(O) Kepler(B-scientist) to(O) conclude(O) that(O) the(O) Solar(O) System(O) was(O) composed(O) of(O) two(O) basses(O) ((O) Saturn(B-astronomical object) and(O) Jupiter(B-astronomical object) )(O) ,(O) a(O) tenor(O) ((O) Mars(B-astronomical object) )(O) ,(O) two(O) altos(O) ((O) Venus(B-astronomical object) and(O) Earth(B-astronomical object) )(O) ,(O) and(O) a(O) soprano(O) ((O) Mercury(B-astronomical object) )(O) ,(O) which(O) had(O) sung(O) in(O) perfect(O) concord(O) ,(O) at(O) the(O) beginning(O) of(O) time(O) ,(O) and(O) could(O) potentially(O) arrange(O) themselves(O) to(O) do(O) so(O) again(O) .(O)"}}
{"id": "157", "dataset": "crossner_science", "split": "dev", "label_list": ["university", "chemical compound", "theory", "enzyme", "country", "astronomical object", "chemical element", "organization", "protein", "discipline", "academic journal", "person", "scientist", "award", "event", "location"], "instance": {"id": "157", "words": ["Singer", "is", "a", "member", "of", "the", "National", "Academy", "of", "Sciences", "and", "the", "American", "Academy", "of", "Arts", "and", "Sciences", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, chemical compound, theory, enzyme, country, astronomical object, chemical element, organization, protein, discipline, academic journal, person, scientist, award, event, location and O.\nSentence: Singer is a member of the National Academy of Sciences and the American Academy of Arts and Sciences .", "prompt_labels": "Singer(B-scientist) is(O) a(O) member(O) of(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) and(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) .(O)"}}
{"id": "348", "dataset": "crossner_science", "split": "dev", "label_list": ["theory", "astronomical object", "location", "organization", "enzyme", "academic journal", "award", "scientist", "country", "university", "chemical compound", "discipline", "person", "event", "protein", "chemical element"], "instance": {"id": "348", "words": ["The", "northwestern", "through", "northeastern", "parts", "of", "the", "metropolitan", "area", "are", "served", "by", "various", "campuses", "of", "the", "Lone", "Star", "College", "System", ",", "while", "the", "southeastern", "portion", "of", "the", "city", "and", "some", "surrounding", "areas", "are", "served", "by", "San", "Jacinto", "College", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, astronomical object, location, organization, enzyme, academic journal, award, scientist, country, university, chemical compound, discipline, person, event, protein, chemical element and O.\nSentence: The northwestern through northeastern parts of the metropolitan area are served by various campuses of the Lone Star College System , while the southeastern portion of the city and some surrounding areas are served by San Jacinto College .", "prompt_labels": "The(O) northwestern(O) through(O) northeastern(O) parts(O) of(O) the(O) metropolitan(O) area(O) are(O) served(O) by(O) various(O) campuses(O) of(O) the(O) Lone(B-university) Star(I-university) College(I-university) System(I-university) ,(O) while(O) the(O) southeastern(O) portion(O) of(O) the(O) city(O) and(O) some(O) surrounding(O) areas(O) are(O) served(O) by(O) San(B-university) Jacinto(I-university) College(I-university) .(O)"}}
{"id": "213", "dataset": "crossner_science", "split": "dev", "label_list": ["scientist", "university", "country", "theory", "person", "enzyme", "chemical element", "organization", "event", "award", "chemical compound", "discipline", "astronomical object", "academic journal", "protein", "location"], "instance": {"id": "213", "words": ["In", "1919", "he", "was", "ordinary", "professor", "of", "systematic", "philosophy", "at", "Cologne", "and", "in", "1921", "professor", "of", "philosophy", "at", "Leipzig", ",", "though", "he", "was", "a", "visiting", "professor", "in", "Nanjing", "and", "Beijing", "during", "1922-23", ",", "and", "in", "1923", "he", "received", "honorable", "doctor", "'s", "degree", "from", "National", "Southeastern", "University", "(", "later", "renamed", "National", "Central", "University", "and", "Nanjing", "University", ")", "where", "he", "taught", "for", "a", "semester", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-discipline", "I-discipline", "O", "B-location", "O", "O", "O", "O", "O", "B-discipline", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "B-university", "I-university", "I-university", "O", "B-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, university, country, theory, person, enzyme, chemical element, organization, event, award, chemical compound, discipline, astronomical object, academic journal, protein, location and O.\nSentence: In 1919 he was ordinary professor of systematic philosophy at Cologne and in 1921 professor of philosophy at Leipzig , though he was a visiting professor in Nanjing and Beijing during 1922-23 , and in 1923 he received honorable doctor 's degree from National Southeastern University ( later renamed National Central University and Nanjing University ) where he taught for a semester .", "prompt_labels": "In(O) 1919(O) he(O) was(O) ordinary(O) professor(O) of(O) systematic(B-discipline) philosophy(I-discipline) at(O) Cologne(B-location) and(O) in(O) 1921(O) professor(O) of(O) philosophy(B-discipline) at(O) Leipzig(B-location) ,(O) though(O) he(O) was(O) a(O) visiting(O) professor(O) in(O) Nanjing(B-location) and(O) Beijing(B-location) during(O) 1922-23(O) ,(O) and(O) in(O) 1923(O) he(O) received(O) honorable(O) doctor(O) 's(O) degree(O) from(O) National(B-university) Southeastern(I-university) University(I-university) ((O) later(O) renamed(O) National(B-university) Central(I-university) University(I-university) and(O) Nanjing(B-university) University(I-university) )(O) where(O) he(O) taught(O) for(O) a(O) semester(O) .(O)"}}
{"id": "436", "dataset": "crossner_science", "split": "dev", "label_list": ["enzyme", "protein", "country", "chemical compound", "discipline", "academic journal", "event", "university", "scientist", "chemical element", "theory", "award", "organization", "person", "astronomical object", "location"], "instance": {"id": "436", "words": ["The", "film", "was", "also", "nominated", "for", "Academy", "Award", "for", "Best", "Supporting", "Actor", "(", "Sam", "Shepard", ")", ",", "Academy", "Award", "for", "Best", "Production", "Design", "(", "Art", "Direction", ":", "Geoffrey", "Kirkland", ",", "Richard", "Lawrence", ",", "W.", "Stewart", "Campbell", "and", "Peter", "R.", "Romero", ";", "Set", "Decoration", ":", "George", "R.", "Nelson", ")", ",", "Academy", "Award", "for", "Best", "Cinematography", "(", "Caleb", "Deschanel", ")", "and", "Academy", "Award", "for", "Best", "Picture", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, protein, country, chemical compound, discipline, academic journal, event, university, scientist, chemical element, theory, award, organization, person, astronomical object, location and O.\nSentence: The film was also nominated for Academy Award for Best Supporting Actor ( Sam Shepard ) , Academy Award for Best Production Design ( Art Direction : Geoffrey Kirkland , Richard Lawrence , W. Stewart Campbell and Peter R. Romero ; Set Decoration : George R. Nelson ) , Academy Award for Best Cinematography ( Caleb Deschanel ) and Academy Award for Best Picture .", "prompt_labels": "The(O) film(O) was(O) also(O) nominated(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Supporting(I-award) Actor(I-award) ((O) Sam(B-person) Shepard(I-person) )(O) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Production(I-award) Design(I-award) ((O) Art(B-person) Direction(I-person) :(O) Geoffrey(B-person) Kirkland(I-person) ,(O) Richard(B-person) Lawrence(I-person) ,(O) W.(B-person) Stewart(I-person) Campbell(I-person) and(O) Peter(B-person) R.(I-person) Romero(I-person) ;(O) Set(B-person) Decoration(I-person) :(O) George(B-person) R.(I-person) Nelson(I-person) )(O) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Cinematography(I-award) ((O) Caleb(B-person) Deschanel(I-person) )(O) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) .(O)"}}
{"id": "120", "dataset": "crossner_science", "split": "dev", "label_list": ["scientist", "person", "country", "chemical compound", "enzyme", "theory", "organization", "university", "protein", "event", "academic journal", "award", "discipline", "astronomical object", "location", "chemical element"], "instance": {"id": "120", "words": ["GSK-3", "has", "been", "implicated", "in", "bipolar", "disorder", ",", "as", "bipolar", "medications", "lithium", "and", "valproate", "have", "been", "shown", "to", "increase", "its", "phosphorylation", ",", "thereby", "inhibiting", "it", "."], "labels": ["B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical element", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, person, country, chemical compound, enzyme, theory, organization, university, protein, event, academic journal, award, discipline, astronomical object, location, chemical element and O.\nSentence: GSK-3 has been implicated in bipolar disorder , as bipolar medications lithium and valproate have been shown to increase its phosphorylation , thereby inhibiting it .", "prompt_labels": "GSK-3(B-protein) has(O) been(O) implicated(O) in(O) bipolar(O) disorder(O) ,(O) as(O) bipolar(O) medications(O) lithium(B-chemical element) and(O) valproate(B-chemical compound) have(O) been(O) shown(O) to(O) increase(O) its(O) phosphorylation(O) ,(O) thereby(O) inhibiting(O) it(O) .(O)"}}
{"id": "198", "dataset": "crossner_science", "split": "dev", "label_list": ["award", "scientist", "enzyme", "protein", "person", "theory", "university", "discipline", "chemical compound", "country", "astronomical object", "event", "organization", "location", "chemical element", "academic journal"], "instance": {"id": "198", "words": ["Phthalocyanine", "s", ",", "which", ",", "like", "Phthalocyanine", "Blue", "BN", "and", "Phthalocyanine", "Green", "G", ",", "often", "contain", "a", "transition", "metal", "ion", ",", "exchange", "an", "electron", "with", "the", "complexed", "transition", "metal", "ion", "that", "easily", "changes", "its", "oxidation", "state", "."], "labels": ["B-chemical compound", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "I-chemical compound", "O", "B-chemical compound", "I-chemical compound", "I-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, scientist, enzyme, protein, person, theory, university, discipline, chemical compound, country, astronomical object, event, organization, location, chemical element, academic journal and O.\nSentence: Phthalocyanine s , which , like Phthalocyanine Blue BN and Phthalocyanine Green G , often contain a transition metal ion , exchange an electron with the complexed transition metal ion that easily changes its oxidation state .", "prompt_labels": "Phthalocyanine(B-chemical compound) s(O) ,(O) which(O) ,(O) like(O) Phthalocyanine(B-chemical compound) Blue(I-chemical compound) BN(I-chemical compound) and(O) Phthalocyanine(B-chemical compound) Green(I-chemical compound) G(I-chemical compound) ,(O) often(O) contain(O) a(O) transition(O) metal(O) ion(O) ,(O) exchange(O) an(O) electron(O) with(O) the(O) complexed(O) transition(O) metal(O) ion(O) that(O) easily(O) changes(O) its(O) oxidation(O) state(O) .(O)"}}
{"id": "1343", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "genre", "song", "average ratings", "plot", "rating", "review", "character", "year", "director", "trailer", "actor"], "instance": {"id": "1343", "words": ["i", "would", "love", "to", "see", "a", "1960", "s", "melodrama", "made", "by", "melville", "shavelson", "with", "a", "nine", "rating"], "labels": ["O", "O", "O", "O", "O", "O", "B-year", "I-year", "B-genre", "O", "O", "B-director", "I-director", "O", "O", "B-average ratings", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, genre, song, average ratings, plot, rating, review, character, year, director, trailer, actor and O.\nSentence: i would love to see a 1960 s melodrama made by melville shavelson with a nine rating", "prompt_labels": "i(O) would(O) love(O) to(O) see(O) a(O) 1960(B-year) s(I-year) melodrama(B-genre) made(O) by(O) melville(B-director) shavelson(I-director) with(O) a(O) nine(B-average ratings) rating(O)"}}
{"id": "1437", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "review", "rating", "year", "director", "trailer", "average ratings", "song", "character", "genre", "plot", "actor"], "instance": {"id": "1437", "words": ["is", "heath", "ledger", "in", "and", "rated", "g", "film", "noir", "movies", "that", "got", "two", "thumbs", "up"], "labels": ["O", "B-actor", "I-actor", "O", "O", "O", "B-rating", "B-genre", "I-genre", "O", "O", "O", "B-average ratings", "I-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, review, rating, year, director, trailer, average ratings, song, character, genre, plot, actor and O.\nSentence: is heath ledger in and rated g film noir movies that got two thumbs up", "prompt_labels": "is(O) heath(B-actor) ledger(I-actor) in(O) and(O) rated(O) g(B-rating) film(B-genre) noir(I-genre) movies(O) that(O) got(O) two(B-average ratings) thumbs(I-average ratings) up(I-average ratings)"}}
{"id": "1190", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "actor", "plot", "song", "title", "average ratings", "character", "genre", "review", "trailer", "rating", "year"], "instance": {"id": "1190", "words": ["do", "you", "have", "a", "debra", "winger", "adventure", "movie", "with", "an", "average", "ratings", "of", "a", "six", "from", "the", "last", "seven", "decades", "about", "degradation"], "labels": ["O", "O", "O", "O", "B-actor", "I-actor", "B-genre", "O", "O", "O", "O", "O", "O", "O", "B-average ratings", "O", "O", "B-year", "I-year", "I-year", "O", "B-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, actor, plot, song, title, average ratings, character, genre, review, trailer, rating, year and O.\nSentence: do you have a debra winger adventure movie with an average ratings of a six from the last seven decades about degradation", "prompt_labels": "do(O) you(O) have(O) a(O) debra(B-actor) winger(I-actor) adventure(B-genre) movie(O) with(O) an(O) average(O) ratings(O) of(O) a(O) six(B-average ratings) from(O) the(O) last(B-year) seven(I-year) decades(I-year) about(O) degradation(B-plot)"}}
{"id": "2110", "dataset": "mit-movie", "split": "dev", "label_list": ["plot", "year", "genre", "song", "rating", "character", "title", "actor", "review", "average ratings", "trailer", "director"], "instance": {"id": "2110", "words": ["what", "is", "the", "plot", "of", "the", "movie", "us", "now"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, year, genre, song, rating, character, title, actor, review, average ratings, trailer, director and O.\nSentence: what is the plot of the movie us now", "prompt_labels": "what(O) is(O) the(O) plot(O) of(O) the(O) movie(O) us(B-title) now(I-title)"}}
{"id": "2342", "dataset": "mit-movie", "split": "dev", "label_list": ["plot", "trailer", "average ratings", "song", "director", "rating", "title", "character", "review", "genre", "actor", "year"], "instance": {"id": "2342", "words": ["would", "you", "be", "able", "to", "help", "me", "find", "the", "cheetah", "girls", "2", "movie"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-title", "I-title", "I-title", "I-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, trailer, average ratings, song, director, rating, title, character, review, genre, actor, year and O.\nSentence: would you be able to help me find the cheetah girls 2 movie", "prompt_labels": "would(O) you(O) be(O) able(O) to(O) help(O) me(O) find(O) the(B-title) cheetah(I-title) girls(I-title) 2(I-title) movie(O)"}}
{"id": "321", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "year", "actor", "review", "trailer", "title", "rating", "character", "average ratings", "plot", "song", "director"], "instance": {"id": "321", "words": ["name", "a", "movie", "with", "joseph", "cotton", "and", "orson", "welles"], "labels": ["O", "O", "O", "O", "B-actor", "I-actor", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, year, actor, review, trailer, title, rating, character, average ratings, plot, song, director and O.\nSentence: name a movie with joseph cotton and orson welles", "prompt_labels": "name(O) a(O) movie(O) with(O) joseph(B-actor) cotton(I-actor) and(O) orson(B-actor) welles(I-actor)"}}
{"id": "2341", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "trailer", "director", "genre", "actor", "character", "song", "review", "rating", "year", "plot", "title"], "instance": {"id": "2341", "words": ["would", "you", "be", "able", "to", "find", "me", "a", "pg", "13", "movie", "from", "the", "1980", "s", "starring", "lara", "fabian"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-rating", "I-rating", "O", "O", "O", "B-year", "I-year", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, trailer, director, genre, actor, character, song, review, rating, year, plot, title and O.\nSentence: would you be able to find me a pg 13 movie from the 1980 s starring lara fabian", "prompt_labels": "would(O) you(O) be(O) able(O) to(O) find(O) me(O) a(O) pg(B-rating) 13(I-rating) movie(O) from(O) the(O) 1980(B-year) s(I-year) starring(O) lara(B-actor) fabian(I-actor)"}}
{"id": "2289", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "genre", "song", "character", "trailer", "plot", "average ratings", "year", "director", "rating", "title", "review"], "instance": {"id": "2289", "words": ["which", "animation", "film", "stars", "dustin", "diamond"], "labels": ["O", "B-genre", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, genre, song, character, trailer, plot, average ratings, year, director, rating, title, review and O.\nSentence: which animation film stars dustin diamond", "prompt_labels": "which(O) animation(B-genre) film(O) stars(O) dustin(B-actor) diamond(I-actor)"}}
{"id": "2262", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "rating", "title", "character", "year", "trailer", "genre", "average ratings", "song", "plot", "actor", "review"], "instance": {"id": "2262", "words": ["when", "did", "the", "darwin", "awards", "come", "out"], "labels": ["O", "O", "B-title", "I-title", "I-title", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, rating, title, character, year, trailer, genre, average ratings, song, plot, actor, review and O.\nSentence: when did the darwin awards come out", "prompt_labels": "when(O) did(O) the(B-title) darwin(I-title) awards(I-title) come(O) out(O)"}}
{"id": "1035", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "trailer", "average ratings", "year", "genre", "review", "director", "character", "rating", "plot", "song", "title"], "instance": {"id": "1035", "words": ["the", "song", "hold", "on", "by", "90s", "band", "wilson", "phillips", "is", "used", "in", "which", "film", "soundtracks"], "labels": ["O", "O", "B-song", "I-song", "O", "B-year", "O", "B-song", "I-song", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, trailer, average ratings, year, genre, review, director, character, rating, plot, song, title and O.\nSentence: the song hold on by 90s band wilson phillips is used in which film soundtracks", "prompt_labels": "the(O) song(O) hold(B-song) on(I-song) by(O) 90s(B-year) band(O) wilson(B-song) phillips(I-song) is(O) used(O) in(O) which(O) film(O) soundtracks(O)"}}
{"id": "208", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "genre", "trailer", "director", "review", "year", "title", "average ratings", "song", "actor", "character", "plot"], "instance": {"id": "208", "words": ["find", "me", "a", "movie", "with", "the", "song", "over", "the", "rainbow", "in", "it"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, genre, trailer, director, review, year, title, average ratings, song, actor, character, plot and O.\nSentence: find me a movie with the song over the rainbow in it", "prompt_labels": "find(O) me(O) a(O) movie(O) with(O) the(O) song(O) over(B-song) the(I-song) rainbow(I-song) in(O) it(O)"}}
{"id": "315", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "title", "plot", "song", "character", "genre", "trailer", "year", "actor", "director", "rating", "review"], "instance": {"id": "315", "words": ["name", "the", "theme", "song", "for", "2001", "a", "space", "odyssey"], "labels": ["O", "O", "B-song", "I-song", "O", "B-title", "I-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, title, plot, song, character, genre, trailer, year, actor, director, rating, review and O.\nSentence: name the theme song for 2001 a space odyssey", "prompt_labels": "name(O) the(O) theme(B-song) song(I-song) for(O) 2001(B-title) a(I-title) space(I-title) odyssey(I-title)"}}
{"id": "74", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "review", "song", "plot", "actor", "genre", "rating", "director", "title", "average ratings", "character", "trailer"], "instance": {"id": "74", "words": ["who", "was", "the", "actress", "in", "the", "goodbye", "girl", "with", "richard", "dreyfuss"], "labels": ["O", "O", "O", "O", "O", "B-title", "I-title", "I-title", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, review, song, plot, actor, genre, rating, director, title, average ratings, character, trailer and O.\nSentence: who was the actress in the goodbye girl with richard dreyfuss", "prompt_labels": "who(O) was(O) the(O) actress(O) in(O) the(B-title) goodbye(I-title) girl(I-title) with(O) richard(B-actor) dreyfuss(I-actor)"}}
{"id": "1961", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "review", "genre", "average ratings", "rating", "trailer", "song", "title", "character", "year", "actor", "plot"], "instance": {"id": "1961", "words": ["what", "good", "crime", "movie", "was", "made", "in", "the", "1990", "s"], "labels": ["O", "O", "B-genre", "O", "O", "O", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, review, genre, average ratings, rating, trailer, song, title, character, year, actor, plot and O.\nSentence: what good crime movie was made in the 1990 s", "prompt_labels": "what(O) good(O) crime(B-genre) movie(O) was(O) made(O) in(O) the(O) 1990(B-year) s(I-year)"}}
{"id": "1600", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "year", "rating", "average ratings", "trailer", "title", "genre", "character", "actor", "plot", "review", "song"], "instance": {"id": "1600", "words": ["list", "a", "pg", "13", "film", "from", "the", "past", "three", "decades", "starring", "chase", "masterson"], "labels": ["O", "O", "B-rating", "I-rating", "O", "O", "O", "B-year", "I-year", "I-year", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, year, rating, average ratings, trailer, title, genre, character, actor, plot, review, song and O.\nSentence: list a pg 13 film from the past three decades starring chase masterson", "prompt_labels": "list(O) a(O) pg(B-rating) 13(I-rating) film(O) from(O) the(O) past(B-year) three(I-year) decades(I-year) starring(O) chase(B-actor) masterson(I-actor)"}}
{"id": "1123", "dataset": "mit-movie", "split": "dev", "label_list": ["trailer", "plot", "year", "character", "average ratings", "review", "rating", "title", "director", "genre", "song", "actor"], "instance": {"id": "1123", "words": ["dating", "1980", "s", "movie", "that", "was", "rated", "pg", "13", "and", "was", "rated", "four", "stars"], "labels": ["B-plot", "B-year", "I-year", "O", "O", "O", "O", "B-rating", "I-rating", "O", "O", "O", "B-average ratings", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, plot, year, character, average ratings, review, rating, title, director, genre, song, actor and O.\nSentence: dating 1980 s movie that was rated pg 13 and was rated four stars", "prompt_labels": "dating(B-plot) 1980(B-year) s(I-year) movie(O) that(O) was(O) rated(O) pg(B-rating) 13(I-rating) and(O) was(O) rated(O) four(B-average ratings) stars(O)"}}
{"id": "291", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "year", "review", "plot", "song", "rating", "average ratings", "trailer", "actor", "director", "character", "title"], "instance": {"id": "291", "words": ["find", "a", "trailer", "for", "space", "2010"], "labels": ["O", "O", "B-trailer", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, year, review, plot, song, rating, average ratings, trailer, actor, director, character, title and O.\nSentence: find a trailer for space 2010", "prompt_labels": "find(O) a(O) trailer(B-trailer) for(O) space(B-title) 2010(I-title)"}}
{"id": "1527", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "trailer", "title", "average ratings", "character", "review", "actor", "director", "plot", "genre", "song", "rating"], "instance": {"id": "1527", "words": ["is", "there", "a", "movie", "which", "director", "was", "mel", "brooks"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, trailer, title, average ratings, character, review, actor, director, plot, genre, song, rating and O.\nSentence: is there a movie which director was mel brooks", "prompt_labels": "is(O) there(O) a(O) movie(O) which(O) director(O) was(O) mel(B-director) brooks(I-director)"}}
{"id": "1357", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "genre", "plot", "trailer", "song", "director", "actor", "character", "year", "average ratings", "rating", "review"], "instance": {"id": "1357", "words": ["im", "looking", "for", "a", "western"], "labels": ["O", "O", "O", "O", "B-genre"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, genre, plot, trailer, song, director, actor, character, year, average ratings, rating, review and O.\nSentence: im looking for a western", "prompt_labels": "im(O) looking(O) for(O) a(O) western(B-genre)"}}
{"id": "831", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "title", "year", "review", "actor", "rating", "character", "genre", "song", "director", "plot", "trailer"], "instance": {"id": "831", "words": ["show", "me", "a", "movie", "with", "the", "song", "somewhere", "over", "the", "rainbow"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, title, year, review, actor, rating, character, genre, song, director, plot, trailer and O.\nSentence: show me a movie with the song somewhere over the rainbow", "prompt_labels": "show(O) me(O) a(O) movie(O) with(O) the(O) song(O) somewhere(B-song) over(I-song) the(I-song) rainbow(I-song)"}}
{"id": "491", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "title", "review", "rating", "actor", "plot", "trailer", "year", "genre", "director", "character", "song"], "instance": {"id": "491", "words": ["who", "directed", "space", "balls"], "labels": ["O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, title, review, rating, actor, plot, trailer, year, genre, director, character, song and O.\nSentence: who directed space balls", "prompt_labels": "who(O) directed(O) space(B-title) balls(I-title)"}}
{"id": "379", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "year", "character", "genre", "plot", "trailer", "title", "rating", "review", "average ratings", "actor", "song"], "instance": {"id": "379", "words": ["list", "all", "science", "fiction", "movies", "playing", "the", "next", "12", "hours"], "labels": ["O", "O", "B-genre", "I-genre", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, year, character, genre, plot, trailer, title, rating, review, average ratings, actor, song and O.\nSentence: list all science fiction movies playing the next 12 hours", "prompt_labels": "list(O) all(O) science(B-genre) fiction(I-genre) movies(O) playing(O) the(O) next(O) 12(O) hours(O)"}}
{"id": "947", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "director", "plot", "song", "average ratings", "trailer", "title", "character", "review", "genre", "year", "actor"], "instance": {"id": "947", "words": ["what", "film", "had", "the", "song", "the", "power", "of", "love"], "labels": ["O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, director, plot, song, average ratings, trailer, title, character, review, genre, year, actor and O.\nSentence: what film had the song the power of love", "prompt_labels": "what(O) film(O) had(O) the(O) song(O) the(B-song) power(I-song) of(I-song) love(I-song)"}}
{"id": "2159", "dataset": "mit-movie", "split": "dev", "label_list": ["character", "plot", "actor", "trailer", "rating", "review", "genre", "director", "average ratings", "year", "song", "title"], "instance": {"id": "2159", "words": ["what", "movies", "with", "danielle", "mccormack", "were", "highly", "recommended"], "labels": ["O", "O", "O", "B-actor", "I-actor", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, plot, actor, trailer, rating, review, genre, director, average ratings, year, song, title and O.\nSentence: what movies with danielle mccormack were highly recommended", "prompt_labels": "what(O) movies(O) with(O) danielle(B-actor) mccormack(I-actor) were(O) highly(B-average ratings) recommended(I-average ratings)"}}
{"id": "688", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "average ratings", "year", "trailer", "plot", "title", "character", "director", "review", "actor", "song", "rating"], "instance": {"id": "688", "words": ["looking", "for", "the", "movie", "with", "elizabeth", "tayor", "rock", "hudson", "and", "james", "dean"], "labels": ["O", "O", "O", "O", "O", "B-actor", "I-actor", "I-actor", "I-actor", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, average ratings, year, trailer, plot, title, character, director, review, actor, song, rating and O.\nSentence: looking for the movie with elizabeth tayor rock hudson and james dean", "prompt_labels": "looking(O) for(O) the(O) movie(O) with(O) elizabeth(B-actor) tayor(I-actor) rock(I-actor) hudson(I-actor) and(O) james(B-actor) dean(I-actor)"}}
{"id": "1133", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "title", "average ratings", "review", "genre", "year", "song", "trailer", "plot", "character", "rating", "director"], "instance": {"id": "1133", "words": ["did", "david", "fincher", "direct", "a", "movie", "about", "war", "in", "the", "1990", "s"], "labels": ["O", "B-director", "I-director", "O", "O", "O", "O", "B-genre", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, title, average ratings, review, genre, year, song, trailer, plot, character, rating, director and O.\nSentence: did david fincher direct a movie about war in the 1990 s", "prompt_labels": "did(O) david(B-director) fincher(I-director) direct(O) a(O) movie(O) about(O) war(B-genre) in(O) the(O) 1990(B-year) s(I-year)"}}
{"id": "861", "dataset": "mit-movie", "split": "dev", "label_list": ["character", "year", "trailer", "review", "average ratings", "rating", "actor", "title", "director", "plot", "song", "genre"], "instance": {"id": "861", "words": ["find", "meg", "ryan", "films", "from", "the", "1990s", "with", "angels"], "labels": ["O", "O", "O", "O", "O", "O", "B-year", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, year, trailer, review, average ratings, rating, actor, title, director, plot, song, genre and O.\nSentence: find meg ryan films from the 1990s with angels", "prompt_labels": "find(O) meg(O) ryan(O) films(O) from(O) the(O) 1990s(B-year) with(O) angels(O)"}}
{"id": "2255", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "trailer", "character", "director", "year", "title", "plot", "actor", "song", "average ratings", "rating", "review"], "instance": {"id": "2255", "words": ["whats", "the", "name", "of", "the", "nc", "17", "romance", "movie", "directed", "by", "lev", "l", "spiro", "that", "came", "out", "this", "year"], "labels": ["O", "O", "O", "O", "O", "B-rating", "I-rating", "B-genre", "O", "O", "O", "B-director", "I-director", "I-director", "O", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, trailer, character, director, year, title, plot, actor, song, average ratings, rating, review and O.\nSentence: whats the name of the nc 17 romance movie directed by lev l spiro that came out this year", "prompt_labels": "whats(O) the(O) name(O) of(O) the(O) nc(B-rating) 17(I-rating) romance(B-genre) movie(O) directed(O) by(O) lev(B-director) l(I-director) spiro(I-director) that(O) came(O) out(O) this(B-year) year(I-year)"}}
{"id": "1578", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "plot", "character", "review", "average ratings", "actor", "title", "year", "director", "genre", "trailer", "song"], "instance": {"id": "1578", "words": ["list", "home", "alone", "4"], "labels": ["O", "B-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, plot, character, review, average ratings, actor, title, year, director, genre, trailer, song and O.\nSentence: list home alone 4", "prompt_labels": "list(O) home(B-title) alone(I-title) 4(I-title)"}}
{"id": "1051", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "review", "genre", "title", "plot", "song", "character", "average ratings", "rating", "trailer", "year", "director"], "instance": {"id": "1051", "words": ["which", "was", "the", "first", "harry", "potter", "movie", "in", "the", "series"], "labels": ["O", "O", "O", "B-year", "B-title", "I-title", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, review, genre, title, plot, song, character, average ratings, rating, trailer, year, director and O.\nSentence: which was the first harry potter movie in the series", "prompt_labels": "which(O) was(O) the(O) first(B-year) harry(B-title) potter(I-title) movie(O) in(O) the(O) series(O)"}}
{"id": "828", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "trailer", "year", "plot", "character", "average ratings", "rating", "genre", "song", "title", "review", "director"], "instance": {"id": "828", "words": ["is", "there", "a", "movie", "where", "a", "little", "girl", "enters", "a", "beauty", "contest"], "labels": ["B-plot", "I-plot", "I-plot", "I-plot", "I-plot", "I-plot", "I-plot", "I-plot", "I-plot", "I-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, trailer, year, plot, character, average ratings, rating, genre, song, title, review, director and O.\nSentence: is there a movie where a little girl enters a beauty contest", "prompt_labels": "is(B-plot) there(I-plot) a(I-plot) movie(I-plot) where(I-plot) a(I-plot) little(I-plot) girl(I-plot) enters(I-plot) a(I-plot) beauty(I-plot) contest(I-plot)"}}
{"id": "520", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "year", "character", "genre", "title", "trailer", "rating", "average ratings", "song", "plot", "actor", "review"], "instance": {"id": "520", "words": ["what", "are", "some", "funny", "arnold", "schwarzenegger", "comedies"], "labels": ["O", "O", "O", "O", "B-actor", "I-actor", "B-genre"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, year, character, genre, title, trailer, rating, average ratings, song, plot, actor, review and O.\nSentence: what are some funny arnold schwarzenegger comedies", "prompt_labels": "what(O) are(O) some(O) funny(O) arnold(B-actor) schwarzenegger(I-actor) comedies(B-genre)"}}
{"id": "2046", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "year", "director", "plot", "average ratings", "review", "trailer", "actor", "song", "character", "genre", "title"], "instance": {"id": "2046", "words": ["what", "is", "a", "well", "rated", "action", "movie", "about", "prisoners"], "labels": ["O", "O", "O", "B-average ratings", "I-average ratings", "B-genre", "O", "O", "B-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, year, director, plot, average ratings, review, trailer, actor, song, character, genre, title and O.\nSentence: what is a well rated action movie about prisoners", "prompt_labels": "what(O) is(O) a(O) well(B-average ratings) rated(I-average ratings) action(B-genre) movie(O) about(O) prisoners(B-plot)"}}
{"id": "99", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "rating", "review", "actor", "plot", "director", "year", "genre", "song", "trailer", "average ratings", "character"], "instance": {"id": "99", "words": ["find", "a", "john", "malcovich", "thriller"], "labels": ["O", "O", "B-actor", "I-actor", "B-genre"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, rating, review, actor, plot, director, year, genre, song, trailer, average ratings, character and O.\nSentence: find a john malcovich thriller", "prompt_labels": "find(O) a(O) john(B-actor) malcovich(I-actor) thriller(B-genre)"}}
{"id": "766", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "title", "director", "character", "rating", "trailer", "genre", "plot", "year", "song", "review", "average ratings"], "instance": {"id": "766", "words": ["where", "is", "the", "quote", "heres", "looking", "at", "you", "kid", "from"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, title, director, character, rating, trailer, genre, plot, year, song, review, average ratings and O.\nSentence: where is the quote heres looking at you kid from", "prompt_labels": "where(O) is(O) the(O) quote(O) heres(O) looking(O) at(O) you(O) kid(O) from(O)"}}
{"id": "1442", "dataset": "mit-movie", "split": "dev", "label_list": ["trailer", "title", "genre", "review", "average ratings", "rating", "character", "actor", "year", "plot", "director", "song"], "instance": {"id": "1442", "words": ["is", "robert", "de", "niro", "in", "any", "disaster", "movies"], "labels": ["O", "B-actor", "I-actor", "I-actor", "O", "O", "B-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, title, genre, review, average ratings, rating, character, actor, year, plot, director, song and O.\nSentence: is robert de niro in any disaster movies", "prompt_labels": "is(O) robert(B-actor) de(I-actor) niro(I-actor) in(O) any(O) disaster(B-genre) movies(O)"}}
{"id": "2202", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "trailer", "genre", "rating", "title", "character", "director", "average ratings", "plot", "song", "review", "actor"], "instance": {"id": "2202", "words": ["what", "was", "planet", "of", "the", "vampires", "about"], "labels": ["O", "O", "B-title", "I-title", "I-title", "I-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, trailer, genre, rating, title, character, director, average ratings, plot, song, review, actor and O.\nSentence: what was planet of the vampires about", "prompt_labels": "what(O) was(O) planet(B-title) of(I-title) the(I-title) vampires(I-title) about(O)"}}
{"id": "459", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "title", "review", "director", "plot", "year", "song", "average ratings", "character", "actor", "trailer", "rating"], "instance": {"id": "459", "words": ["name", "a", "movie", "where", "beer", "is", "important", "to", "the", "plot"], "labels": ["O", "O", "O", "O", "B-plot", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, title, review, director, plot, year, song, average ratings, character, actor, trailer, rating and O.\nSentence: name a movie where beer is important to the plot", "prompt_labels": "name(O) a(O) movie(O) where(O) beer(B-plot) is(O) important(O) to(O) the(O) plot(O)"}}
{"id": "1010", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "trailer", "plot", "director", "title", "genre", "character", "year", "review", "actor", "song", "rating"], "instance": {"id": "1010", "words": ["who", "is", "the", "director", "of", "the", "breakfast", "club"], "labels": ["B-director", "I-director", "I-director", "I-director", "I-director", "I-director", "I-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, trailer, plot, director, title, genre, character, year, review, actor, song, rating and O.\nSentence: who is the director of the breakfast club", "prompt_labels": "who(B-director) is(I-director) the(I-director) director(I-director) of(I-director) the(I-director) breakfast(I-director) club(I-director)"}}
{"id": "249", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "genre", "character", "review", "title", "trailer", "director", "average ratings", "actor", "year", "song", "plot"], "instance": {"id": "249", "words": ["give", "me", "a", "list", "of", "legal", "dramas", "from", "the", "70s"], "labels": ["O", "O", "O", "O", "O", "B-plot", "B-genre", "O", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, genre, character, review, title, trailer, director, average ratings, actor, year, song, plot and O.\nSentence: give me a list of legal dramas from the 70s", "prompt_labels": "give(O) me(O) a(O) list(O) of(O) legal(B-plot) dramas(B-genre) from(O) the(O) 70s(B-year)"}}
{"id": "2270", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "plot", "title", "year", "average ratings", "rating", "actor", "trailer", "genre", "character", "song", "director"], "instance": {"id": "2270", "words": ["when", "was", "the", "movie", "stake", "land", "made"], "labels": ["O", "O", "O", "O", "B-title", "I-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, plot, title, year, average ratings, rating, actor, trailer, genre, character, song, director and O.\nSentence: when was the movie stake land made", "prompt_labels": "when(O) was(O) the(O) movie(O) stake(B-title) land(I-title) made(O)"}}
{"id": "2326", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "title", "trailer", "character", "genre", "year", "song", "plot", "rating", "review", "average ratings", "director"], "instance": {"id": "2326", "words": ["who", "starred", "in", "the", "movie", "dirty", "love"], "labels": ["O", "O", "O", "O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, title, trailer, character, genre, year, song, plot, rating, review, average ratings, director and O.\nSentence: who starred in the movie dirty love", "prompt_labels": "who(O) starred(O) in(O) the(O) movie(O) dirty(B-title) love(I-title)"}}
{"id": "1879", "dataset": "mit-movie", "split": "dev", "label_list": ["character", "average ratings", "review", "genre", "title", "song", "director", "rating", "plot", "actor", "year", "trailer"], "instance": {"id": "1879", "words": ["what", "actor", "stars", "in", "a", "pg", "13", "action", "movie", "set", "in", "the", "year", "1940", "that", "was", "received", "well"], "labels": ["O", "O", "O", "O", "O", "B-rating", "I-rating", "B-genre", "O", "O", "O", "O", "O", "B-year", "O", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, average ratings, review, genre, title, song, director, rating, plot, actor, year, trailer and O.\nSentence: what actor stars in a pg 13 action movie set in the year 1940 that was received well", "prompt_labels": "what(O) actor(O) stars(O) in(O) a(O) pg(B-rating) 13(I-rating) action(B-genre) movie(O) set(O) in(O) the(O) year(O) 1940(B-year) that(O) was(O) received(B-average ratings) well(I-average ratings)"}}
{"id": "1410", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "review", "rating", "title", "actor", "director", "song", "year", "average ratings", "trailer", "plot", "character"], "instance": {"id": "1410", "words": ["in", "the", "2010", "s", "was", "albert", "finney", "in", "an", "r", "rated", "spaghetti", "western"], "labels": ["O", "O", "B-year", "I-year", "O", "B-actor", "I-actor", "O", "O", "B-rating", "O", "B-genre", "I-genre"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, review, rating, title, actor, director, song, year, average ratings, trailer, plot, character and O.\nSentence: in the 2010 s was albert finney in an r rated spaghetti western", "prompt_labels": "in(O) the(O) 2010(B-year) s(I-year) was(O) albert(B-actor) finney(I-actor) in(O) an(O) r(B-rating) rated(O) spaghetti(B-genre) western(I-genre)"}}
{"id": "852", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "rating", "director", "song", "actor", "review", "character", "average ratings", "year", "title", "plot", "trailer"], "instance": {"id": "852", "words": ["whats", "the", "james", "cameron", "movie", "with", "the", "blue", "aliens"], "labels": ["O", "O", "B-director", "I-director", "O", "O", "B-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, rating, director, song, actor, review, character, average ratings, year, title, plot, trailer and O.\nSentence: whats the james cameron movie with the blue aliens", "prompt_labels": "whats(O) the(O) james(B-director) cameron(I-director) movie(O) with(O) the(B-plot) blue(I-plot) aliens(I-plot)"}}
{"id": "1489", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "character", "genre", "actor", "rating", "review", "average ratings", "song", "trailer", "director", "plot", "year"], "instance": {"id": "1489", "words": ["is", "there", "a", "good", "horror", "movie", "about", "a", "curse"], "labels": ["O", "O", "O", "O", "B-genre", "O", "O", "O", "B-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, character, genre, actor, rating, review, average ratings, song, trailer, director, plot, year and O.\nSentence: is there a good horror movie about a curse", "prompt_labels": "is(O) there(O) a(O) good(O) horror(B-genre) movie(O) about(O) a(O) curse(B-plot)"}}
{"id": "2134", "dataset": "mit-movie", "split": "dev", "label_list": ["character", "plot", "director", "song", "trailer", "average ratings", "genre", "rating", "year", "actor", "title", "review"], "instance": {"id": "2134", "words": ["what", "movie", "could", "be", "classified", "as", "a", "mystery"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-genre"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, plot, director, song, trailer, average ratings, genre, rating, year, actor, title, review and O.\nSentence: what movie could be classified as a mystery", "prompt_labels": "what(O) movie(O) could(O) be(O) classified(O) as(O) a(O) mystery(B-genre)"}}
{"id": "1824", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "director", "character", "actor", "plot", "rating", "genre", "title", "review", "trailer", "song", "year"], "instance": {"id": "1824", "words": ["what", "1960", "s", "drama", "received", "good", "ratings"], "labels": ["O", "B-year", "I-year", "B-genre", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, director, character, actor, plot, rating, genre, title, review, trailer, song, year and O.\nSentence: what 1960 s drama received good ratings", "prompt_labels": "what(O) 1960(B-year) s(I-year) drama(B-genre) received(O) good(B-average ratings) ratings(I-average ratings)"}}
{"id": "2138", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "year", "song", "average ratings", "director", "title", "character", "trailer", "plot", "actor", "genre", "rating"], "instance": {"id": "2138", "words": ["what", "movie", "has", "cicely", "tyson", "been", "in", "the", "past", "five", "years"], "labels": ["O", "O", "O", "B-actor", "I-actor", "O", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, year, song, average ratings, director, title, character, trailer, plot, actor, genre, rating and O.\nSentence: what movie has cicely tyson been in the past five years", "prompt_labels": "what(O) movie(O) has(O) cicely(B-actor) tyson(I-actor) been(O) in(O) the(O) past(B-year) five(I-year) years(I-year)"}}
{"id": "1207", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "review", "year", "song", "character", "title", "plot", "actor", "rating", "director", "genre", "trailer"], "instance": {"id": "1207", "words": ["do", "you", "have", "the", "film", "the", "loss", "of", "sexual", "innocence"], "labels": ["O", "O", "O", "O", "O", "B-title", "I-title", "I-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, review, year, song, character, title, plot, actor, rating, director, genre, trailer and O.\nSentence: do you have the film the loss of sexual innocence", "prompt_labels": "do(O) you(O) have(O) the(O) film(O) the(B-title) loss(I-title) of(I-title) sexual(I-title) innocence(I-title)"}}
{"id": "2264", "dataset": "mit-movie", "split": "dev", "label_list": ["trailer", "song", "director", "rating", "title", "year", "review", "plot", "average ratings", "genre", "actor", "character"], "instance": {"id": "2264", "words": ["when", "did", "war", "gods", "of", "the", "deep", "come", "out"], "labels": ["O", "O", "B-title", "I-title", "I-title", "I-title", "I-title", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, song, director, rating, title, year, review, plot, average ratings, genre, actor, character and O.\nSentence: when did war gods of the deep come out", "prompt_labels": "when(O) did(O) war(B-title) gods(I-title) of(I-title) the(I-title) deep(I-title) come(O) out(O)"}}
{"id": "1552", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "song", "director", "review", "actor", "rating", "average ratings", "character", "trailer", "genre", "plot", "year"], "instance": {"id": "1552", "words": ["is", "there", "an", "animated", "movie", "that", "is", "directed", "by", "christopher", "nolan"], "labels": ["O", "O", "O", "B-genre", "O", "O", "O", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, song, director, review, actor, rating, average ratings, character, trailer, genre, plot, year and O.\nSentence: is there an animated movie that is directed by christopher nolan", "prompt_labels": "is(O) there(O) an(O) animated(B-genre) movie(O) that(O) is(O) directed(O) by(O) christopher(B-director) nolan(I-director)"}}
{"id": "1152", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "review", "trailer", "rating", "song", "plot", "actor", "character", "genre", "title", "average ratings", "year"], "instance": {"id": "1152", "words": ["did", "lucille", "ball", "appear", "in", "any", "pg", "13", "movies", "in", "the", "1990", "s"], "labels": ["O", "B-actor", "I-actor", "O", "O", "O", "B-rating", "I-rating", "O", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, review, trailer, rating, song, plot, actor, character, genre, title, average ratings, year and O.\nSentence: did lucille ball appear in any pg 13 movies in the 1990 s", "prompt_labels": "did(O) lucille(B-actor) ball(I-actor) appear(O) in(O) any(O) pg(B-rating) 13(I-rating) movies(O) in(O) the(O) 1990(B-year) s(I-year)"}}
{"id": "1460", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "director", "year", "song", "plot", "character", "actor", "average ratings", "trailer", "rating", "review", "title"], "instance": {"id": "1460", "words": ["is", "there", "a", "pg", "documentary", "in", "the", "past", "nine", "decades", "with", "tim", "allen"], "labels": ["O", "O", "O", "B-rating", "B-genre", "O", "O", "B-year", "I-year", "I-year", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, director, year, song, plot, character, actor, average ratings, trailer, rating, review, title and O.\nSentence: is there a pg documentary in the past nine decades with tim allen", "prompt_labels": "is(O) there(O) a(O) pg(B-rating) documentary(B-genre) in(O) the(O) past(B-year) nine(I-year) decades(I-year) with(O) tim(B-actor) allen(I-actor)"}}
{"id": "2345", "dataset": "mit-movie", "split": "dev", "label_list": ["trailer", "genre", "character", "review", "year", "song", "rating", "plot", "actor", "title", "average ratings", "director"], "instance": {"id": "2345", "words": ["did", "christian", "bale", "star", "in", "a", "science", "fiction", "in", "the", "past", "four", "years"], "labels": ["O", "B-actor", "I-actor", "O", "O", "O", "B-genre", "I-genre", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, genre, character, review, year, song, rating, plot, actor, title, average ratings, director and O.\nSentence: did christian bale star in a science fiction in the past four years", "prompt_labels": "did(O) christian(B-actor) bale(I-actor) star(O) in(O) a(O) science(B-genre) fiction(I-genre) in(O) the(O) past(B-year) four(I-year) years(I-year)"}}
{"id": "1680", "dataset": "mit-movie", "split": "dev", "label_list": ["trailer", "character", "plot", "year", "title", "actor", "genre", "average ratings", "song", "director", "rating", "review"], "instance": {"id": "1680", "words": ["list", "crime", "movies", "from", "the", "2000", "s", "that", "were", "rated", "nc", "17"], "labels": ["O", "B-genre", "O", "O", "O", "B-year", "I-year", "O", "O", "O", "B-rating", "I-rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, character, plot, year, title, actor, genre, average ratings, song, director, rating, review and O.\nSentence: list crime movies from the 2000 s that were rated nc 17", "prompt_labels": "list(O) crime(B-genre) movies(O) from(O) the(O) 2000(B-year) s(I-year) that(O) were(O) rated(O) nc(B-rating) 17(I-rating)"}}
{"id": "2254", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "rating", "character", "actor", "average ratings", "director", "genre", "trailer", "song", "title", "year", "plot"], "instance": {"id": "2254", "words": ["whats", "the", "name", "of", "a", "scary", "movie", "with", "michael", "dudikoff", "in", "it"], "labels": ["O", "O", "O", "O", "O", "B-genre", "O", "O", "B-actor", "I-actor", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, rating, character, actor, average ratings, director, genre, trailer, song, title, year, plot and O.\nSentence: whats the name of a scary movie with michael dudikoff in it", "prompt_labels": "whats(O) the(O) name(O) of(O) a(O) scary(B-genre) movie(O) with(O) michael(B-actor) dudikoff(I-actor) in(O) it(O)"}}
{"id": "1438", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "actor", "genre", "average ratings", "year", "director", "trailer", "song", "rating", "plot", "review", "character"], "instance": {"id": "1438", "words": ["is", "james", "cagney", "in", "the", "shining"], "labels": ["O", "B-actor", "I-actor", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, actor, genre, average ratings, year, director, trailer, song, rating, plot, review, character and O.\nSentence: is james cagney in the shining", "prompt_labels": "is(O) james(B-actor) cagney(I-actor) in(O) the(B-title) shining(I-title)"}}
{"id": "2080", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "genre", "character", "song", "rating", "trailer", "average ratings", "plot", "year", "director", "title", "actor"], "instance": {"id": "2080", "words": ["what", "is", "the", "movie", "angel", "camouflaged", "about"], "labels": ["O", "O", "O", "O", "B-title", "I-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, genre, character, song, rating, trailer, average ratings, plot, year, director, title, actor and O.\nSentence: what is the movie angel camouflaged about", "prompt_labels": "what(O) is(O) the(O) movie(O) angel(B-title) camouflaged(I-title) about(O)"}}
{"id": "506", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "trailer", "rating", "plot", "genre", "character", "year", "director", "average ratings", "actor", "song", "title"], "instance": {"id": "506", "words": ["i", "want", "to", "find", "the", "movie", "starring", "meryl", "streep", "and", "anne", "hathaway"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-actor", "I-actor", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, trailer, rating, plot, genre, character, year, director, average ratings, actor, song, title and O.\nSentence: i want to find the movie starring meryl streep and anne hathaway", "prompt_labels": "i(O) want(O) to(O) find(O) the(O) movie(O) starring(O) meryl(B-actor) streep(I-actor) and(O) anne(B-actor) hathaway(I-actor)"}}
{"id": "1424", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "year", "genre", "review", "rating", "actor", "song", "character", "plot", "director", "trailer", "average ratings"], "instance": {"id": "1424", "words": ["in", "the", "past", "nine", "decades", "what", "movies", "have", "come", "out", "for", "children"], "labels": ["O", "O", "B-year", "I-year", "I-year", "O", "O", "O", "O", "O", "O", "B-genre"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, year, genre, review, rating, actor, song, character, plot, director, trailer, average ratings and O.\nSentence: in the past nine decades what movies have come out for children", "prompt_labels": "in(O) the(O) past(B-year) nine(I-year) decades(I-year) what(O) movies(O) have(O) come(O) out(O) for(O) children(B-genre)"}}
{"id": "1633", "dataset": "mit-movie", "split": "dev", "label_list": ["song", "title", "genre", "year", "director", "plot", "rating", "average ratings", "review", "trailer", "actor", "character"], "instance": {"id": "1633", "words": ["list", "a", "psychological", "drama", "movie", "that", "centers", "on", "lost", "experiences"], "labels": ["O", "O", "B-genre", "I-genre", "O", "O", "O", "O", "B-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, title, genre, year, director, plot, rating, average ratings, review, trailer, actor, character and O.\nSentence: list a psychological drama movie that centers on lost experiences", "prompt_labels": "list(O) a(O) psychological(B-genre) drama(I-genre) movie(O) that(O) centers(O) on(O) lost(B-plot) experiences(I-plot)"}}
{"id": "1348", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "review", "song", "trailer", "genre", "title", "director", "plot", "rating", "average ratings", "actor", "character"], "instance": {"id": "1348", "words": ["id", "like", "to", "find", "a", "pg", "13", "movie", "about", "promises", "that", "was", "directed", "by", "steven", "silver"], "labels": ["O", "O", "O", "O", "O", "B-rating", "I-rating", "O", "O", "B-plot", "O", "O", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, review, song, trailer, genre, title, director, plot, rating, average ratings, actor, character and O.\nSentence: id like to find a pg 13 movie about promises that was directed by steven silver", "prompt_labels": "id(O) like(O) to(O) find(O) a(O) pg(B-rating) 13(I-rating) movie(O) about(O) promises(B-plot) that(O) was(O) directed(O) by(O) steven(B-director) silver(I-director)"}}
{"id": "1608", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "actor", "plot", "genre", "song", "title", "review", "year", "rating", "character", "average ratings", "trailer"], "instance": {"id": "1608", "words": ["list", "a", "crime", "film", "directed", "by", "jonathan", "demme"], "labels": ["O", "O", "B-genre", "O", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, actor, plot, genre, song, title, review, year, rating, character, average ratings, trailer and O.\nSentence: list a crime film directed by jonathan demme", "prompt_labels": "list(O) a(O) crime(B-genre) film(O) directed(O) by(O) jonathan(B-director) demme(I-director)"}}
{"id": "678", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "trailer", "song", "average ratings", "rating", "year", "genre", "director", "character", "title", "plot", "actor"], "instance": {"id": "678", "words": ["what", "is", "a", "comedy", "from", "the", "1990s", "with", "a", "car", "chase"], "labels": ["O", "O", "O", "O", "O", "O", "B-year", "O", "O", "B-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, trailer, song, average ratings, rating, year, genre, director, character, title, plot, actor and O.\nSentence: what is a comedy from the 1990s with a car chase", "prompt_labels": "what(O) is(O) a(O) comedy(O) from(O) the(O) 1990s(B-year) with(O) a(O) car(B-plot) chase(I-plot)"}}
{"id": "1564", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "year", "plot", "genre", "trailer", "rating", "title", "average ratings", "review", "character", "song", "director"], "instance": {"id": "1564", "words": ["is", "there", "any", "good", "teen", "movies", "that", "are", "supposed", "to", "be", "coming", "out", "this", "year"], "labels": ["O", "O", "O", "O", "B-genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, year, plot, genre, trailer, rating, title, average ratings, review, character, song, director and O.\nSentence: is there any good teen movies that are supposed to be coming out this year", "prompt_labels": "is(O) there(O) any(O) good(O) teen(B-genre) movies(O) that(O) are(O) supposed(O) to(O) be(O) coming(O) out(O) this(O) year(O)"}}
{"id": "2297", "dataset": "mit-movie", "split": "dev", "label_list": ["song", "character", "review", "director", "rating", "plot", "genre", "title", "year", "trailer", "average ratings", "actor"], "instance": {"id": "2297", "words": ["which", "unrated", "documentary", "starring", "jason", "priestley", "was", "rated", "as", "all", "right"], "labels": ["O", "B-rating", "B-genre", "O", "B-actor", "I-actor", "O", "O", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, character, review, director, rating, plot, genre, title, year, trailer, average ratings, actor and O.\nSentence: which unrated documentary starring jason priestley was rated as all right", "prompt_labels": "which(O) unrated(B-rating) documentary(B-genre) starring(O) jason(B-actor) priestley(I-actor) was(O) rated(O) as(O) all(B-average ratings) right(I-average ratings)"}}
{"id": "1766", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "director", "trailer", "plot", "actor", "song", "review", "year", "genre", "character", "average ratings", "title"], "instance": {"id": "1766", "words": ["tell", "me", "about", "a", "highly", "rated", "animation", "film", "starring", "theresa", "randall"], "labels": ["O", "O", "O", "O", "B-average ratings", "I-average ratings", "B-genre", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, director, trailer, plot, actor, song, review, year, genre, character, average ratings, title and O.\nSentence: tell me about a highly rated animation film starring theresa randall", "prompt_labels": "tell(O) me(O) about(O) a(O) highly(B-average ratings) rated(I-average ratings) animation(B-genre) film(O) starring(O) theresa(B-actor) randall(I-actor)"}}
{"id": "245", "dataset": "mit-movie", "split": "dev", "label_list": ["plot", "song", "actor", "year", "rating", "trailer", "review", "title", "genre", "director", "character", "average ratings"], "instance": {"id": "245", "words": ["has", "john", "candy", "made", "any", "must", "see", "movies"], "labels": ["O", "B-actor", "I-actor", "O", "O", "B-review", "I-review", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, song, actor, year, rating, trailer, review, title, genre, director, character, average ratings and O.\nSentence: has john candy made any must see movies", "prompt_labels": "has(O) john(B-actor) candy(I-actor) made(O) any(O) must(B-review) see(I-review) movies(O)"}}
{"id": "532", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "actor", "review", "song", "character", "trailer", "director", "title", "rating", "plot", "year", "average ratings"], "instance": {"id": "532", "words": ["what", "is", "a", "fun", "action", "movie"], "labels": ["O", "O", "O", "B-review", "B-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, actor, review, song, character, trailer, director, title, rating, plot, year, average ratings and O.\nSentence: what is a fun action movie", "prompt_labels": "what(O) is(O) a(O) fun(B-review) action(B-genre) movie(O)"}}
{"id": "320", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "title", "plot", "rating", "average ratings", "character", "director", "review", "song", "year", "genre", "trailer"], "instance": {"id": "320", "words": ["show", "me", "a", "list", "of", "comedies", "from", "the", "1970s"], "labels": ["O", "O", "O", "O", "O", "B-genre", "O", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, title, plot, rating, average ratings, character, director, review, song, year, genre, trailer and O.\nSentence: show me a list of comedies from the 1970s", "prompt_labels": "show(O) me(O) a(O) list(O) of(O) comedies(B-genre) from(O) the(O) 1970s(B-year)"}}
{"id": "770", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "plot", "character", "song", "rating", "director", "year", "review", "genre", "trailer", "title", "actor"], "instance": {"id": "770", "words": ["what", "movies", "feature", "the", "character", "jeeves"], "labels": ["O", "O", "O", "O", "O", "B-character"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, plot, character, song, rating, director, year, review, genre, trailer, title, actor and O.\nSentence: what movies feature the character jeeves", "prompt_labels": "what(O) movies(O) feature(O) the(O) character(O) jeeves(B-character)"}}
{"id": "1676", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "average ratings", "actor", "song", "title", "genre", "review", "character", "rating", "plot", "trailer", "year"], "instance": {"id": "1676", "words": ["list", "an", "independent", "movie"], "labels": ["O", "O", "B-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, average ratings, actor, song, title, genre, review, character, rating, plot, trailer, year and O.\nSentence: list an independent movie", "prompt_labels": "list(O) an(O) independent(B-genre) movie(O)"}}
{"id": "983", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "genre", "character", "average ratings", "review", "plot", "year", "song", "title", "trailer", "director", "actor"], "instance": {"id": "983", "words": ["what", "year", "was", "jaws", "released"], "labels": ["O", "O", "O", "B-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, genre, character, average ratings, review, plot, year, song, title, trailer, director, actor and O.\nSentence: what year was jaws released", "prompt_labels": "what(O) year(O) was(O) jaws(B-title) released(O)"}}
{"id": "990", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "average ratings", "song", "trailer", "title", "character", "year", "director", "rating", "plot", "actor", "review"], "instance": {"id": "990", "words": ["who", "directed", "the", "bedford", "incident"], "labels": ["O", "O", "B-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, average ratings, song, trailer, title, character, year, director, rating, plot, actor, review and O.\nSentence: who directed the bedford incident", "prompt_labels": "who(O) directed(O) the(B-title) bedford(I-title) incident(I-title)"}}
{"id": "1923", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "review", "rating", "average ratings", "character", "genre", "title", "plot", "song", "year", "trailer", "director"], "instance": {"id": "1923", "words": ["what", "are", "the", "best", "disney", "movies"], "labels": ["O", "O", "O", "O", "B-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, review, rating, average ratings, character, genre, title, plot, song, year, trailer, director and O.\nSentence: what are the best disney movies", "prompt_labels": "what(O) are(O) the(O) best(O) disney(B-genre) movies(O)"}}
{"id": "1920", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "review", "song", "trailer", "character", "plot", "title", "rating", "genre", "year", "director", "actor"], "instance": {"id": "1920", "words": ["what", "are", "some", "very", "popular", "documentary", "movies", "rated", "r", "from", "the", "2010", "s"], "labels": ["O", "O", "O", "B-average ratings", "I-average ratings", "B-genre", "O", "O", "B-rating", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, review, song, trailer, character, plot, title, rating, genre, year, director, actor and O.\nSentence: what are some very popular documentary movies rated r from the 2010 s", "prompt_labels": "what(O) are(O) some(O) very(B-average ratings) popular(I-average ratings) documentary(B-genre) movies(O) rated(O) r(B-rating) from(O) the(O) 2010(B-year) s(I-year)"}}
{"id": "217", "dataset": "mit-movie", "split": "dev", "label_list": ["song", "character", "genre", "plot", "actor", "title", "trailer", "review", "average ratings", "rating", "director", "year"], "instance": {"id": "217", "words": ["has", "samuel", "jackson", "been", "in", "any", "thriller", "flicks"], "labels": ["O", "B-actor", "I-actor", "O", "O", "O", "B-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, character, genre, plot, actor, title, trailer, review, average ratings, rating, director, year and O.\nSentence: has samuel jackson been in any thriller flicks", "prompt_labels": "has(O) samuel(B-actor) jackson(I-actor) been(O) in(O) any(O) thriller(B-genre) flicks(O)"}}
{"id": "1778", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "title", "rating", "actor", "song", "character", "plot", "trailer", "review", "genre", "average ratings", "year"], "instance": {"id": "1778", "words": ["the", "movie", "the", "wind", "that", "shakes", "the", "barley", "what", "is", "it", "about"], "labels": ["O", "O", "B-title", "I-title", "I-title", "I-title", "I-title", "I-title", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, title, rating, actor, song, character, plot, trailer, review, genre, average ratings, year and O.\nSentence: the movie the wind that shakes the barley what is it about", "prompt_labels": "the(O) movie(O) the(B-title) wind(I-title) that(I-title) shakes(I-title) the(I-title) barley(I-title) what(O) is(O) it(O) about(O)"}}
{"id": "2155", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "plot", "actor", "genre", "song", "rating", "year", "trailer", "average ratings", "character", "title", "director"], "instance": {"id": "2155", "words": ["what", "movies", "star", "sarah", "brown", "that", "have", "good", "ratings"], "labels": ["O", "O", "O", "B-actor", "I-actor", "O", "O", "B-average ratings", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, plot, actor, genre, song, rating, year, trailer, average ratings, character, title, director and O.\nSentence: what movies star sarah brown that have good ratings", "prompt_labels": "what(O) movies(O) star(O) sarah(B-actor) brown(I-actor) that(O) have(O) good(B-average ratings) ratings(O)"}}
{"id": "843", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "average ratings", "trailer", "title", "director", "year", "review", "actor", "genre", "character", "song", "plot"], "instance": {"id": "843", "words": ["how", "many", "movies", "has", "al", "pacino", "been", "in"], "labels": ["O", "O", "O", "O", "B-actor", "I-actor", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, average ratings, trailer, title, director, year, review, actor, genre, character, song, plot and O.\nSentence: how many movies has al pacino been in", "prompt_labels": "how(O) many(O) movies(O) has(O) al(B-actor) pacino(I-actor) been(O) in(O)"}}
{"id": "62", "dataset": "mit-movie", "split": "dev", "label_list": ["trailer", "title", "genre", "average ratings", "actor", "review", "director", "plot", "song", "rating", "year", "character"], "instance": {"id": "62", "words": ["how", "many", "movies", "have", "tim", "burton", "and", "johnny", "depp", "done", "together"], "labels": ["O", "O", "O", "O", "B-actor", "I-actor", "O", "B-actor", "I-actor", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, title, genre, average ratings, actor, review, director, plot, song, rating, year, character and O.\nSentence: how many movies have tim burton and johnny depp done together", "prompt_labels": "how(O) many(O) movies(O) have(O) tim(B-actor) burton(I-actor) and(O) johnny(B-actor) depp(I-actor) done(O) together(O)"}}
{"id": "734", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "average ratings", "plot", "year", "genre", "rating", "character", "director", "trailer", "actor", "song", "title"], "instance": {"id": "734", "words": ["show", "me", "a", "movie", "starring", "meryl", "streep", "and", "robert", "deniro"], "labels": ["O", "O", "O", "O", "O", "B-actor", "I-actor", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, average ratings, plot, year, genre, rating, character, director, trailer, actor, song, title and O.\nSentence: show me a movie starring meryl streep and robert deniro", "prompt_labels": "show(O) me(O) a(O) movie(O) starring(O) meryl(B-actor) streep(I-actor) and(O) robert(B-actor) deniro(I-actor)"}}
{"id": "169", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "trailer", "review", "year", "plot", "title", "genre", "song", "average ratings", "rating", "actor", "character"], "instance": {"id": "169", "words": ["did", "joe", "pesci", "direct", "any", "films"], "labels": ["O", "B-director", "I-director", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, trailer, review, year, plot, title, genre, song, average ratings, rating, actor, character and O.\nSentence: did joe pesci direct any films", "prompt_labels": "did(O) joe(B-director) pesci(I-director) direct(O) any(O) films(O)"}}
{"id": "1671", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "trailer", "director", "plot", "character", "actor", "song", "title", "genre", "review", "year", "average ratings"], "instance": {"id": "1671", "words": ["list", "an", "animation", "from", "the", "past", "three", "years", "that", "has", "excellent", "ratings", "and", "andrew", "douglas", "is", "the", "director"], "labels": ["O", "O", "B-genre", "O", "O", "B-year", "I-year", "I-year", "O", "O", "B-average ratings", "I-average ratings", "O", "B-director", "I-director", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, trailer, director, plot, character, actor, song, title, genre, review, year, average ratings and O.\nSentence: list an animation from the past three years that has excellent ratings and andrew douglas is the director", "prompt_labels": "list(O) an(O) animation(B-genre) from(O) the(O) past(B-year) three(I-year) years(I-year) that(O) has(O) excellent(B-average ratings) ratings(I-average ratings) and(O) andrew(B-director) douglas(I-director) is(O) the(O) director(O)"}}
{"id": "894", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "director", "genre", "trailer", "average ratings", "plot", "song", "actor", "year", "character", "rating", "title"], "instance": {"id": "894", "words": ["show", "me", "the", "spike", "lee", "movie", "about", "a", "serial", "killer"], "labels": ["O", "O", "O", "B-director", "I-director", "O", "B-plot", "I-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, director, genre, trailer, average ratings, plot, song, actor, year, character, rating, title and O.\nSentence: show me the spike lee movie about a serial killer", "prompt_labels": "show(O) me(O) the(O) spike(B-director) lee(I-director) movie(O) about(B-plot) a(I-plot) serial(I-plot) killer(I-plot)"}}
{"id": "1047", "dataset": "mit-movie", "split": "dev", "label_list": ["character", "review", "trailer", "plot", "actor", "rating", "title", "director", "average ratings", "year", "song", "genre"], "instance": {"id": "1047", "words": ["in", "what", "year", "was", "kid", "galahad", "released"], "labels": ["O", "O", "O", "O", "B-title", "I-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, review, trailer, plot, actor, rating, title, director, average ratings, year, song, genre and O.\nSentence: in what year was kid galahad released", "prompt_labels": "in(O) what(O) year(O) was(O) kid(B-title) galahad(I-title) released(O)"}}
{"id": "1686", "dataset": "mit-movie", "split": "dev", "label_list": ["trailer", "director", "average ratings", "character", "actor", "review", "plot", "rating", "year", "genre", "song", "title"], "instance": {"id": "1686", "words": ["list", "movies", "that", "starred", "charlton", "heston"], "labels": ["O", "O", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, director, average ratings, character, actor, review, plot, rating, year, genre, song, title and O.\nSentence: list movies that starred charlton heston", "prompt_labels": "list(O) movies(O) that(O) starred(O) charlton(B-actor) heston(I-actor)"}}
{"id": "2220", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "actor", "rating", "year", "genre", "review", "trailer", "director", "average ratings", "song", "plot", "character"], "instance": {"id": "2220", "words": ["what", "was", "the", "last", "fantasy", "film", "that", "tim", "burton", "directed", "that", "was", "rated", "r"], "labels": ["O", "O", "O", "O", "B-genre", "O", "O", "B-director", "I-director", "O", "O", "O", "O", "B-rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, actor, rating, year, genre, review, trailer, director, average ratings, song, plot, character and O.\nSentence: what was the last fantasy film that tim burton directed that was rated r", "prompt_labels": "what(O) was(O) the(O) last(O) fantasy(B-genre) film(O) that(O) tim(B-director) burton(I-director) directed(O) that(O) was(O) rated(O) r(B-rating)"}}
{"id": "38", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "average ratings", "character", "actor", "year", "trailer", "review", "genre", "rating", "director", "plot", "song"], "instance": {"id": "38", "words": ["are", "there", "any", "drama", "movies", "with", "seth", "green"], "labels": ["O", "O", "O", "B-genre", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, average ratings, character, actor, year, trailer, review, genre, rating, director, plot, song and O.\nSentence: are there any drama movies with seth green", "prompt_labels": "are(O) there(O) any(O) drama(B-genre) movies(O) with(O) seth(B-actor) green(I-actor)"}}
{"id": "1817", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "director", "genre", "review", "actor", "title", "plot", "trailer", "song", "character", "average ratings", "year"], "instance": {"id": "1817", "words": ["were", "there", "many", "family", "movies", "liked", "by", "many", "in", "1940"], "labels": ["O", "O", "O", "B-genre", "O", "B-average ratings", "I-average ratings", "I-average ratings", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, director, genre, review, actor, title, plot, trailer, song, character, average ratings, year and O.\nSentence: were there many family movies liked by many in 1940", "prompt_labels": "were(O) there(O) many(O) family(B-genre) movies(O) liked(B-average ratings) by(I-average ratings) many(I-average ratings) in(O) 1940(B-year)"}}
{"id": "926", "dataset": "mit-movie", "split": "dev", "label_list": ["trailer", "actor", "year", "title", "director", "character", "plot", "review", "rating", "song", "average ratings", "genre"], "instance": {"id": "926", "words": ["what", "m", "night", "shyamalan", "movie", "got", "the", "best", "reviews"], "labels": ["O", "B-director", "I-director", "I-director", "O", "O", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, actor, year, title, director, character, plot, review, rating, song, average ratings, genre and O.\nSentence: what m night shyamalan movie got the best reviews", "prompt_labels": "what(O) m(B-director) night(I-director) shyamalan(I-director) movie(O) got(O) the(O) best(B-average ratings) reviews(I-average ratings)"}}
{"id": "667", "dataset": "mit-movie", "split": "dev", "label_list": ["character", "title", "genre", "song", "director", "plot", "average ratings", "year", "review", "actor", "rating", "trailer"], "instance": {"id": "667", "words": ["find", "meryl", "streep", "dramas", "set", "in", "africa"], "labels": ["O", "B-actor", "I-actor", "B-genre", "B-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, title, genre, song, director, plot, average ratings, year, review, actor, rating, trailer and O.\nSentence: find meryl streep dramas set in africa", "prompt_labels": "find(O) meryl(B-actor) streep(I-actor) dramas(B-genre) set(B-plot) in(I-plot) africa(I-plot)"}}
{"id": "2059", "dataset": "mit-movie", "split": "dev", "label_list": ["character", "average ratings", "year", "plot", "song", "actor", "rating", "trailer", "director", "review", "genre", "title"], "instance": {"id": "2059", "words": ["what", "is", "the", "domino", "effect", "about"], "labels": ["O", "O", "O", "B-title", "I-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, average ratings, year, plot, song, actor, rating, trailer, director, review, genre, title and O.\nSentence: what is the domino effect about", "prompt_labels": "what(O) is(O) the(O) domino(B-title) effect(I-title) about(O)"}}
{"id": "353", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "review", "plot", "trailer", "rating", "actor", "director", "year", "genre", "title", "song", "character"], "instance": {"id": "353", "words": ["how", "many", "pirates", "of", "the", "caribbean", "movies", "is", "johnny", "depp", "in"], "labels": ["O", "O", "B-title", "I-title", "I-title", "I-title", "O", "O", "B-actor", "I-actor", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, review, plot, trailer, rating, actor, director, year, genre, title, song, character and O.\nSentence: how many pirates of the caribbean movies is johnny depp in", "prompt_labels": "how(O) many(O) pirates(B-title) of(I-title) the(I-title) caribbean(I-title) movies(O) is(O) johnny(B-actor) depp(I-actor) in(O)"}}
{"id": "1773", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "review", "song", "trailer", "director", "year", "actor", "average ratings", "rating", "plot", "character", "title"], "instance": {"id": "1773", "words": ["that", "comedy", "was", "so", "funny", "that", "id", "give", "it", "ten", "stars"], "labels": ["O", "O", "O", "O", "B-genre", "O", "O", "O", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, review, song, trailer, director, year, actor, average ratings, rating, plot, character, title and O.\nSentence: that comedy was so funny that id give it ten stars", "prompt_labels": "that(O) comedy(O) was(O) so(O) funny(B-genre) that(O) id(O) give(O) it(O) ten(B-average ratings) stars(I-average ratings)"}}
{"id": "1425", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "rating", "review", "director", "title", "trailer", "genre", "character", "average ratings", "song", "plot", "actor"], "instance": {"id": "1425", "words": ["in", "the", "past", "seven", "decades", "was", "mimi", "rogers", "an", "many", "adventure", "films"], "labels": ["O", "O", "B-year", "I-year", "I-year", "O", "B-actor", "I-actor", "O", "O", "B-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, rating, review, director, title, trailer, genre, character, average ratings, song, plot, actor and O.\nSentence: in the past seven decades was mimi rogers an many adventure films", "prompt_labels": "in(O) the(O) past(B-year) seven(I-year) decades(I-year) was(O) mimi(B-actor) rogers(I-actor) an(O) many(O) adventure(B-genre) films(O)"}}
{"id": "445", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "average ratings", "review", "rating", "year", "plot", "director", "trailer", "genre", "title", "character", "song"], "instance": {"id": "445", "words": ["show", "movies", "from", "the", "1960s", "directed", "by", "stanley", "kubrick"], "labels": ["O", "O", "O", "O", "B-year", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, average ratings, review, rating, year, plot, director, trailer, genre, title, character, song and O.\nSentence: show movies from the 1960s directed by stanley kubrick", "prompt_labels": "show(O) movies(O) from(O) the(O) 1960s(B-year) directed(O) by(O) stanley(B-director) kubrick(I-director)"}}
{"id": "2247", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "year", "average ratings", "plot", "character", "genre", "actor", "title", "song", "rating", "review", "trailer"], "instance": {"id": "2247", "words": ["whats", "a", "pg", "13", "movie", "about", "a", "pianist", "and", "starring", "katherine", "kelly", "that", "got", "seven", "stars"], "labels": ["O", "O", "B-rating", "I-rating", "O", "O", "O", "B-plot", "O", "O", "B-actor", "I-actor", "O", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, year, average ratings, plot, character, genre, actor, title, song, rating, review, trailer and O.\nSentence: whats a pg 13 movie about a pianist and starring katherine kelly that got seven stars", "prompt_labels": "whats(O) a(O) pg(B-rating) 13(I-rating) movie(O) about(O) a(O) pianist(B-plot) and(O) starring(O) katherine(B-actor) kelly(I-actor) that(O) got(O) seven(B-average ratings) stars(I-average ratings)"}}
{"id": "605", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "average ratings", "review", "character", "rating", "song", "trailer", "year", "title", "actor", "plot", "director"], "instance": {"id": "605", "words": ["find", "all", "movies", "directed", "by", "steven", "speilburg"], "labels": ["O", "O", "O", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, average ratings, review, character, rating, song, trailer, year, title, actor, plot, director and O.\nSentence: find all movies directed by steven speilburg", "prompt_labels": "find(O) all(O) movies(O) directed(O) by(O) steven(B-director) speilburg(I-director)"}}
{"id": "1406", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Restaurant Name", "Price", "Hours", "Dish", "Cuisine", "Location", "Amenity"], "instance": {"id": "1406", "words": ["where", "is", "the", "best", "reviewed", "place", "to", "get", "a", "burger", "and", "fries", "near", "beacon", "hill"], "labels": ["O", "O", "O", "B-Rating", "I-Rating", "O", "O", "O", "O", "B-Dish", "I-Dish", "I-Dish", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Price, Hours, Dish, Cuisine, Location, Amenity and O.\nSentence: where is the best reviewed place to get a burger and fries near beacon hill", "prompt_labels": "where(O) is(O) the(O) best(B-Rating) reviewed(I-Rating) place(O) to(O) get(O) a(O) burger(B-Dish) and(I-Dish) fries(I-Dish) near(B-Location) beacon(I-Location) hill(I-Location)"}}
{"id": "1006", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Rating", "Hours", "Restaurant Name", "Location", "Dish", "Amenity", "Cuisine"], "instance": {"id": "1006", "words": ["my", "kids", "want", "to", "sit", "outside", "and", "eat", "cheeseburgers"], "labels": ["O", "O", "O", "O", "B-Amenity", "I-Amenity", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Rating, Hours, Restaurant Name, Location, Dish, Amenity, Cuisine and O.\nSentence: my kids want to sit outside and eat cheeseburgers", "prompt_labels": "my(O) kids(O) want(O) to(O) sit(B-Amenity) outside(I-Amenity) and(O) eat(O) cheeseburgers(B-Dish)"}}
{"id": "1136", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Amenity", "Rating", "Location", "Hours", "Cuisine", "Restaurant Name", "Price"], "instance": {"id": "1136", "words": ["what", "is", "the", "most", "closest", "eat", "in", "restaurant", "in", "the", "area"], "labels": ["O", "O", "O", "O", "B-Location", "B-Amenity", "I-Amenity", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Amenity, Rating, Location, Hours, Cuisine, Restaurant Name, Price and O.\nSentence: what is the most closest eat in restaurant in the area", "prompt_labels": "what(O) is(O) the(O) most(O) closest(B-Location) eat(B-Amenity) in(I-Amenity) restaurant(O) in(B-Location) the(I-Location) area(I-Location)"}}
{"id": "1515", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Cuisine", "Location", "Price", "Hours", "Rating", "Amenity", "Dish"], "instance": {"id": "1515", "words": ["will", "i", "be", "able", "to", "find", "a", "romantic", "restaurant", "for", "my", "date", "tonight"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Amenity", "O", "O", "O", "O", "B-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Cuisine, Location, Price, Hours, Rating, Amenity, Dish and O.\nSentence: will i be able to find a romantic restaurant for my date tonight", "prompt_labels": "will(O) i(O) be(O) able(O) to(O) find(O) a(O) romantic(B-Amenity) restaurant(O) for(O) my(O) date(O) tonight(B-Hours)"}}
{"id": "280", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Dish", "Cuisine", "Price", "Restaurant Name", "Location", "Amenity", "Rating"], "instance": {"id": "280", "words": ["does", "chuck", "e", "cheeses", "have", "drive", "thru"], "labels": ["O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Dish, Cuisine, Price, Restaurant Name, Location, Amenity, Rating and O.\nSentence: does chuck e cheeses have drive thru", "prompt_labels": "does(O) chuck(B-Restaurant Name) e(I-Restaurant Name) cheeses(I-Restaurant Name) have(O) drive(B-Amenity) thru(I-Amenity)"}}
{"id": "1396", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Rating", "Dish", "Location", "Restaurant Name", "Cuisine", "Hours", "Price"], "instance": {"id": "1396", "words": ["where", "is", "some", "good", "outdoor", "dining"], "labels": ["O", "O", "O", "B-Rating", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Dish, Location, Restaurant Name, Cuisine, Hours, Price and O.\nSentence: where is some good outdoor dining", "prompt_labels": "where(O) is(O) some(O) good(B-Rating) outdoor(B-Amenity) dining(I-Amenity)"}}
{"id": "766", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Dish", "Restaurant Name", "Price", "Location", "Amenity", "Hours", "Cuisine"], "instance": {"id": "766", "words": ["is", "dave", "and", "busters", "a", "good", "lunch", "spot"], "labels": ["O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "O", "B-Rating", "B-Hours", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Dish, Restaurant Name, Price, Location, Amenity, Hours, Cuisine and O.\nSentence: is dave and busters a good lunch spot", "prompt_labels": "is(O) dave(B-Restaurant Name) and(I-Restaurant Name) busters(I-Restaurant Name) a(O) good(B-Rating) lunch(B-Hours) spot(O)"}}
{"id": "1284", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Price", "Restaurant Name", "Hours", "Location", "Rating", "Amenity", "Cuisine"], "instance": {"id": "1284", "words": ["where", "can", "i", "find", "a", "rainforest", "cafe"], "labels": ["O", "O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Price, Restaurant Name, Hours, Location, Rating, Amenity, Cuisine and O.\nSentence: where can i find a rainforest cafe", "prompt_labels": "where(O) can(O) i(O) find(O) a(O) rainforest(B-Restaurant Name) cafe(I-Restaurant Name)"}}
{"id": "124", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Price", "Restaurant Name", "Cuisine", "Dish", "Location", "Hours", "Rating"], "instance": {"id": "124", "words": ["can", "i", "get", "gluten", "free", "pizza", "within", "10", "miles", "of", "here"], "labels": ["O", "O", "O", "B-Dish", "I-Dish", "I-Dish", "B-Location", "I-Location", "I-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Price, Restaurant Name, Cuisine, Dish, Location, Hours, Rating and O.\nSentence: can i get gluten free pizza within 10 miles of here", "prompt_labels": "can(O) i(O) get(O) gluten(B-Dish) free(I-Dish) pizza(I-Dish) within(B-Location) 10(I-Location) miles(I-Location) of(I-Location) here(I-Location)"}}
{"id": "1371", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Restaurant Name", "Cuisine", "Price", "Dish", "Rating", "Location", "Amenity"], "instance": {"id": "1371", "words": ["where", "in", "the", "theater", "district", "is", "there", "a", "restaurant", "with", "portions", "that", "are", "a", "bit", "small"], "labels": ["O", "O", "O", "B-Location", "I-Location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Cuisine, Price, Dish, Rating, Location, Amenity and O.\nSentence: where in the theater district is there a restaurant with portions that are a bit small", "prompt_labels": "where(O) in(O) the(O) theater(B-Location) district(I-Location) is(O) there(O) a(O) restaurant(O) with(O) portions(O) that(O) are(O) a(O) bit(O) small(O)"}}
{"id": "1232", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Dish", "Restaurant Name", "Cuisine", "Amenity", "Location", "Price", "Hours"], "instance": {"id": "1232", "words": ["whats", "the", "best", "place", "around", "here", "to", "get", "cheese"], "labels": ["O", "O", "B-Rating", "O", "O", "O", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Dish, Restaurant Name, Cuisine, Amenity, Location, Price, Hours and O.\nSentence: whats the best place around here to get cheese", "prompt_labels": "whats(O) the(O) best(B-Rating) place(O) around(O) here(O) to(O) get(O) cheese(B-Dish)"}}
{"id": "402", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Restaurant Name", "Hours", "Dish", "Location", "Price", "Amenity", "Cuisine"], "instance": {"id": "402", "words": ["find", "me", "a", "romantic", "restaurant", "in", "7", "hills"], "labels": ["O", "O", "O", "B-Amenity", "B-Location", "I-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Hours, Dish, Location, Price, Amenity, Cuisine and O.\nSentence: find me a romantic restaurant in 7 hills", "prompt_labels": "find(O) me(O) a(O) romantic(B-Amenity) restaurant(B-Location) in(I-Location) 7(I-Location) hills(I-Location)"}}
{"id": "24", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Rating", "Hours", "Amenity", "Restaurant Name", "Dish", "Price", "Location"], "instance": {"id": "24", "words": ["are", "there", "any", "brewpubs", "downtown"], "labels": ["O", "O", "O", "B-Cuisine", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Hours, Amenity, Restaurant Name, Dish, Price, Location and O.\nSentence: are there any brewpubs downtown", "prompt_labels": "are(O) there(O) any(O) brewpubs(B-Cuisine) downtown(B-Location)"}}
{"id": "513", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Hours", "Location", "Restaurant Name", "Amenity", "Cuisine", "Dish", "Rating"], "instance": {"id": "513", "words": ["how", "far", "for", "a", "burger", "place"], "labels": ["O", "O", "O", "O", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Hours, Location, Restaurant Name, Amenity, Cuisine, Dish, Rating and O.\nSentence: how far for a burger place", "prompt_labels": "how(O) far(O) for(O) a(O) burger(B-Cuisine) place(O)"}}
{"id": "1168", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Location", "Amenity", "Price", "Rating", "Cuisine", "Dish", "Hours"], "instance": {"id": "1168", "words": ["what", "pizza", "place", "has", "the", "best", "toppings", "and", "is", "no", "farther", "than", "4", "miles"], "labels": ["O", "B-Dish", "O", "O", "O", "B-Rating", "I-Rating", "O", "O", "B-Location", "I-Location", "I-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Location, Amenity, Price, Rating, Cuisine, Dish, Hours and O.\nSentence: what pizza place has the best toppings and is no farther than 4 miles", "prompt_labels": "what(O) pizza(B-Dish) place(O) has(O) the(O) best(B-Rating) toppings(I-Rating) and(O) is(O) no(B-Location) farther(I-Location) than(I-Location) 4(I-Location) miles(I-Location)"}}
{"id": "37", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Rating", "Hours", "Dish", "Location", "Restaurant Name", "Price", "Cuisine"], "instance": {"id": "37", "words": ["are", "there", "any", "fast", "food", "joints", "east", "of", "here"], "labels": ["O", "O", "O", "B-Cuisine", "I-Cuisine", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Hours, Dish, Location, Restaurant Name, Price, Cuisine and O.\nSentence: are there any fast food joints east of here", "prompt_labels": "are(O) there(O) any(O) fast(B-Cuisine) food(I-Cuisine) joints(O) east(B-Location) of(I-Location) here(I-Location)"}}
{"id": "461", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Amenity", "Price", "Cuisine", "Dish", "Location", "Hours", "Rating"], "instance": {"id": "461", "words": ["get", "me", "to", "a", "mexican", "place"], "labels": ["O", "O", "O", "O", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Price, Cuisine, Dish, Location, Hours, Rating and O.\nSentence: get me to a mexican place", "prompt_labels": "get(O) me(O) to(O) a(O) mexican(B-Cuisine) place(O)"}}
{"id": "73", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Price", "Dish", "Rating", "Hours", "Amenity", "Location", "Cuisine"], "instance": {"id": "73", "words": ["are", "there", "any", "restaurants", "for", "diabetics", "that", "serve", "sugar", "free", "desserts"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-Dish", "I-Dish", "I-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Price, Dish, Rating, Hours, Amenity, Location, Cuisine and O.\nSentence: are there any restaurants for diabetics that serve sugar free desserts", "prompt_labels": "are(O) there(O) any(O) restaurants(O) for(O) diabetics(O) that(O) serve(O) sugar(B-Dish) free(I-Dish) desserts(I-Dish)"}}
{"id": "77", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Hours", "Restaurant Name", "Price", "Rating", "Dish", "Amenity", "Location"], "instance": {"id": "77", "words": ["are", "there", "any", "restaurants", "on", "the", "way", "that", "serve", "hamburgers", "and", "are", "open", "after", "1", "am"], "labels": ["O", "O", "O", "O", "O", "O", "B-Location", "O", "O", "B-Dish", "O", "O", "B-Hours", "I-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Hours, Restaurant Name, Price, Rating, Dish, Amenity, Location and O.\nSentence: are there any restaurants on the way that serve hamburgers and are open after 1 am", "prompt_labels": "are(O) there(O) any(O) restaurants(O) on(O) the(O) way(B-Location) that(O) serve(O) hamburgers(B-Dish) and(O) are(O) open(B-Hours) after(I-Hours) 1(I-Hours) am(I-Hours)"}}
{"id": "322", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Dish", "Cuisine", "Price", "Location", "Rating", "Amenity", "Hours"], "instance": {"id": "322", "words": ["does", "the", "pho", "2000", "on", "state", "park", "rd", "have", "online", "reservation", "options"], "labels": ["O", "O", "B-Restaurant Name", "I-Restaurant Name", "O", "B-Location", "I-Location", "I-Location", "O", "B-Amenity", "I-Amenity", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Dish, Cuisine, Price, Location, Rating, Amenity, Hours and O.\nSentence: does the pho 2000 on state park rd have online reservation options", "prompt_labels": "does(O) the(O) pho(B-Restaurant Name) 2000(I-Restaurant Name) on(O) state(B-Location) park(I-Location) rd(I-Location) have(O) online(B-Amenity) reservation(I-Amenity) options(O)"}}
{"id": "564", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Dish", "Amenity", "Cuisine", "Location", "Hours", "Price", "Restaurant Name"], "instance": {"id": "564", "words": ["i", "have", "alcohol", "where", "can", "i", "find", "a", "good", "appetizer"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-Rating", "B-Cuisine"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Dish, Amenity, Cuisine, Location, Hours, Price, Restaurant Name and O.\nSentence: i have alcohol where can i find a good appetizer", "prompt_labels": "i(O) have(O) alcohol(O) where(O) can(O) i(O) find(O) a(O) good(B-Rating) appetizer(B-Cuisine)"}}
{"id": "1084", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Price", "Rating", "Hours", "Cuisine", "Amenity", "Dish", "Location"], "instance": {"id": "1084", "words": ["today", "is", "a", "good", "day", "for", "some", "tacos", "off", "to", "taco", "bell"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Dish", "O", "O", "B-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Price, Rating, Hours, Cuisine, Amenity, Dish, Location and O.\nSentence: today is a good day for some tacos off to taco bell", "prompt_labels": "today(O) is(O) a(O) good(O) day(O) for(O) some(O) tacos(B-Dish) off(O) to(O) taco(B-Restaurant Name) bell(I-Restaurant Name)"}}
{"id": "1286", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Amenity", "Dish", "Restaurant Name", "Hours", "Price", "Location", "Cuisine"], "instance": {"id": "1286", "words": ["where", "can", "i", "find", "a", "restaurant", "that", "has", "good", "portions", "and", "is", "open", "at", "9", "p"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "O", "O", "B-Hours", "I-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Dish, Restaurant Name, Hours, Price, Location, Cuisine and O.\nSentence: where can i find a restaurant that has good portions and is open at 9 p", "prompt_labels": "where(O) can(O) i(O) find(O) a(O) restaurant(O) that(O) has(O) good(B-Amenity) portions(I-Amenity) and(O) is(O) open(B-Hours) at(I-Hours) 9(I-Hours) p(I-Hours)"}}
{"id": "853", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Location", "Price", "Restaurant Name", "Rating", "Amenity", "Dish", "Cuisine"], "instance": {"id": "853", "words": ["is", "there", "a", "rancho", "veo", "restaurant", "in", "north", "memphis"], "labels": ["O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "O", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Location, Price, Restaurant Name, Rating, Amenity, Dish, Cuisine and O.\nSentence: is there a rancho veo restaurant in north memphis", "prompt_labels": "is(O) there(O) a(O) rancho(B-Restaurant Name) veo(I-Restaurant Name) restaurant(O) in(O) north(B-Location) memphis(I-Location)"}}
{"id": "800", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Hours", "Location", "Rating", "Restaurant Name", "Price", "Cuisine", "Dish"], "instance": {"id": "800", "words": ["is", "there", "a", "bojangles", "in", "laurel"], "labels": ["O", "O", "O", "B-Restaurant Name", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Location, Rating, Restaurant Name, Price, Cuisine, Dish and O.\nSentence: is there a bojangles in laurel", "prompt_labels": "is(O) there(O) a(O) bojangles(B-Restaurant Name) in(O) laurel(B-Location)"}}
{"id": "344", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Cuisine", "Location", "Amenity", "Restaurant Name", "Dish", "Rating", "Price"], "instance": {"id": "344", "words": ["find", "a", "mexican", "food", "restaurant", "that", "has", "a", "very", "good", "rating"], "labels": ["O", "O", "B-Cuisine", "O", "O", "O", "O", "O", "B-Rating", "I-Rating", "I-Rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Location, Amenity, Restaurant Name, Dish, Rating, Price and O.\nSentence: find a mexican food restaurant that has a very good rating", "prompt_labels": "find(O) a(O) mexican(B-Cuisine) food(O) restaurant(O) that(O) has(O) a(O) very(B-Rating) good(I-Rating) rating(I-Rating)"}}
{"id": "517", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Dish", "Restaurant Name", "Price", "Location", "Cuisine", "Hours", "Amenity"], "instance": {"id": "517", "words": ["how", "far", "is", "the", "nearest", "applebeas"], "labels": ["O", "O", "O", "O", "B-Location", "B-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Dish, Restaurant Name, Price, Location, Cuisine, Hours, Amenity and O.\nSentence: how far is the nearest applebeas", "prompt_labels": "how(O) far(O) is(O) the(O) nearest(B-Location) applebeas(B-Restaurant Name)"}}
{"id": "869", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Restaurant Name", "Cuisine", "Rating", "Hours", "Dish", "Price", "Location"], "instance": {"id": "869", "words": ["is", "there", "a", "sclafani", "italian", "bakery", "nearby", "with", "a", "view"], "labels": ["O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "B-Location", "O", "O", "B-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Restaurant Name, Cuisine, Rating, Hours, Dish, Price, Location and O.\nSentence: is there a sclafani italian bakery nearby with a view", "prompt_labels": "is(O) there(O) a(O) sclafani(B-Restaurant Name) italian(I-Restaurant Name) bakery(I-Restaurant Name) nearby(B-Location) with(O) a(O) view(B-Amenity)"}}
{"id": "592", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Restaurant Name", "Rating", "Dish", "Location", "Cuisine", "Hours", "Price"], "instance": {"id": "592", "words": ["i", "need", "an", "italian", "restaurant", "with", "a", "kids", "menu"], "labels": ["O", "O", "O", "B-Cuisine", "O", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Restaurant Name, Rating, Dish, Location, Cuisine, Hours, Price and O.\nSentence: i need an italian restaurant with a kids menu", "prompt_labels": "i(O) need(O) an(O) italian(B-Cuisine) restaurant(O) with(O) a(O) kids(B-Amenity) menu(I-Amenity)"}}
{"id": "403", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Restaurant Name", "Amenity", "Rating", "Hours", "Cuisine", "Dish", "Location"], "instance": {"id": "403", "words": ["find", "me", "a", "romantic", "restaurant", "that", "has", "an", "open", "table"], "labels": ["O", "O", "O", "B-Amenity", "O", "O", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Restaurant Name, Amenity, Rating, Hours, Cuisine, Dish, Location and O.\nSentence: find me a romantic restaurant that has an open table", "prompt_labels": "find(O) me(O) a(O) romantic(B-Amenity) restaurant(O) that(O) has(O) an(O) open(B-Amenity) table(I-Amenity)"}}
{"id": "716", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Price", "Amenity", "Hours", "Rating", "Dish", "Restaurant Name", "Location"], "instance": {"id": "716", "words": ["if", "want", "only", "organic", "vegetables", "and", "fruits", "in", "my", "dishes", "which", "restaurant", "is", "best"], "labels": ["O", "O", "O", "B-Cuisine", "I-Cuisine", "I-Cuisine", "I-Cuisine", "O", "O", "O", "O", "O", "O", "B-Rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Amenity, Hours, Rating, Dish, Restaurant Name, Location and O.\nSentence: if want only organic vegetables and fruits in my dishes which restaurant is best", "prompt_labels": "if(O) want(O) only(O) organic(B-Cuisine) vegetables(I-Cuisine) and(I-Cuisine) fruits(I-Cuisine) in(O) my(O) dishes(O) which(O) restaurant(O) is(O) best(B-Rating)"}}
{"id": "423", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Rating", "Restaurant Name", "Hours", "Price", "Dish", "Cuisine", "Amenity"], "instance": {"id": "423", "words": ["find", "me", "chicken", "places", "that", "accept", "discover", "card"], "labels": ["O", "O", "B-Dish", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Rating, Restaurant Name, Hours, Price, Dish, Cuisine, Amenity and O.\nSentence: find me chicken places that accept discover card", "prompt_labels": "find(O) me(O) chicken(B-Dish) places(O) that(O) accept(B-Amenity) discover(I-Amenity) card(I-Amenity)"}}
{"id": "851", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Hours", "Location", "Cuisine", "Amenity", "Dish", "Rating", "Restaurant Name"], "instance": {"id": "851", "words": ["is", "there", "a", "pool", "side", "night", "club", "nearby"], "labels": ["O", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity", "I-Amenity", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Hours, Location, Cuisine, Amenity, Dish, Rating, Restaurant Name and O.\nSentence: is there a pool side night club nearby", "prompt_labels": "is(O) there(O) a(O) pool(B-Amenity) side(I-Amenity) night(I-Amenity) club(I-Amenity) nearby(B-Location)"}}
{"id": "606", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Location", "Cuisine", "Amenity", "Price", "Restaurant Name", "Dish", "Rating"], "instance": {"id": "606", "words": ["i", "need", "the", "closest", "chic", "fil", "a", "that", "is", "still", "serving", "the", "peach", "shake"], "labels": ["O", "O", "O", "B-Location", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "O", "O", "O", "O", "O", "B-Dish", "I-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Location, Cuisine, Amenity, Price, Restaurant Name, Dish, Rating and O.\nSentence: i need the closest chic fil a that is still serving the peach shake", "prompt_labels": "i(O) need(O) the(O) closest(B-Location) chic(B-Restaurant Name) fil(I-Restaurant Name) a(I-Restaurant Name) that(O) is(O) still(O) serving(O) the(O) peach(B-Dish) shake(I-Dish)"}}
{"id": "1339", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Restaurant Name", "Rating", "Amenity", "Cuisine", "Location", "Price", "Dish"], "instance": {"id": "1339", "words": ["where", "can", "i", "get", "some", "shrimp"], "labels": ["O", "O", "O", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Rating, Amenity, Cuisine, Location, Price, Dish and O.\nSentence: where can i get some shrimp", "prompt_labels": "where(O) can(O) i(O) get(O) some(O) shrimp(B-Dish)"}}
{"id": "373", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Price", "Amenity", "Restaurant Name", "Rating", "Dish", "Hours", "Location"], "instance": {"id": "373", "words": ["find", "me", "a", "good", "deli", "in", "manhattan"], "labels": ["O", "O", "O", "B-Rating", "B-Cuisine", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Amenity, Restaurant Name, Rating, Dish, Hours, Location and O.\nSentence: find me a good deli in manhattan", "prompt_labels": "find(O) me(O) a(O) good(B-Rating) deli(B-Cuisine) in(O) manhattan(B-Location)"}}
{"id": "1188", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Hours", "Restaurant Name", "Dish", "Amenity", "Location", "Rating", "Price"], "instance": {"id": "1188", "words": ["what", "restaurants", "are", "nearby"], "labels": ["O", "O", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Hours, Restaurant Name, Dish, Amenity, Location, Rating, Price and O.\nSentence: what restaurants are nearby", "prompt_labels": "what(O) restaurants(O) are(O) nearby(B-Location)"}}
{"id": "1207", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Rating", "Amenity", "Location", "Hours", "Dish", "Restaurant Name", "Price"], "instance": {"id": "1207", "words": ["what", "time", "does", "on", "the", "rocks", "on", "seminary", "stop", "serving", "food"], "labels": ["O", "B-Hours", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "O", "B-Location", "B-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Amenity, Location, Hours, Dish, Restaurant Name, Price and O.\nSentence: what time does on the rocks on seminary stop serving food", "prompt_labels": "what(O) time(B-Hours) does(O) on(B-Restaurant Name) the(I-Restaurant Name) rocks(I-Restaurant Name) on(O) seminary(B-Location) stop(B-Hours) serving(I-Hours) food(I-Hours)"}}
{"id": "903", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Rating", "Location", "Hours", "Restaurant Name", "Dish", "Cuisine", "Amenity"], "instance": {"id": "903", "words": ["is", "there", "any", "good", "pizza", "in", "the", "area"], "labels": ["O", "O", "O", "B-Rating", "B-Dish", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Rating, Location, Hours, Restaurant Name, Dish, Cuisine, Amenity and O.\nSentence: is there any good pizza in the area", "prompt_labels": "is(O) there(O) any(O) good(B-Rating) pizza(B-Dish) in(B-Location) the(I-Location) area(I-Location)"}}
{"id": "1093", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Amenity", "Price", "Restaurant Name", "Rating", "Hours", "Location", "Cuisine"], "instance": {"id": "1093", "words": ["what", "are", "some", "restaurants", "that", "serve", "american", "food", "that", "are", "with", "in", "one", "mile"], "labels": ["O", "O", "O", "O", "O", "O", "B-Cuisine", "O", "O", "O", "B-Location", "I-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Amenity, Price, Restaurant Name, Rating, Hours, Location, Cuisine and O.\nSentence: what are some restaurants that serve american food that are with in one mile", "prompt_labels": "what(O) are(O) some(O) restaurants(O) that(O) serve(O) american(B-Cuisine) food(O) that(O) are(O) with(B-Location) in(I-Location) one(I-Location) mile(I-Location)"}}
{"id": "66", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Location", "Rating", "Dish", "Amenity", "Cuisine", "Hours", "Restaurant Name"], "instance": {"id": "66", "words": ["are", "there", "any", "places", "near", "by", "that", "sell", "hamburgers", "and", "pizza"], "labels": ["O", "O", "O", "O", "B-Location", "I-Location", "O", "O", "B-Dish", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Location, Rating, Dish, Amenity, Cuisine, Hours, Restaurant Name and O.\nSentence: are there any places near by that sell hamburgers and pizza", "prompt_labels": "are(O) there(O) any(O) places(O) near(B-Location) by(I-Location) that(O) sell(O) hamburgers(B-Dish) and(O) pizza(B-Dish)"}}
{"id": "546", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Restaurant Name", "Cuisine", "Hours", "Amenity", "Dish", "Location", "Price"], "instance": {"id": "546", "words": ["i", "am", "looking", "for", "a", "good", "place", "to", "eat"], "labels": ["O", "O", "O", "O", "O", "B-Rating", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Cuisine, Hours, Amenity, Dish, Location, Price and O.\nSentence: i am looking for a good place to eat", "prompt_labels": "i(O) am(O) looking(O) for(O) a(O) good(B-Rating) place(O) to(O) eat(O)"}}
{"id": "852", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Dish", "Hours", "Cuisine", "Rating", "Restaurant Name", "Amenity", "Location"], "instance": {"id": "852", "words": ["is", "there", "a", "portuguese", "restaurant", "nearby"], "labels": ["O", "O", "O", "B-Cuisine", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Dish, Hours, Cuisine, Rating, Restaurant Name, Amenity, Location and O.\nSentence: is there a portuguese restaurant nearby", "prompt_labels": "is(O) there(O) a(O) portuguese(B-Cuisine) restaurant(O) nearby(B-Location)"}}
{"id": "824", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Dish", "Restaurant Name", "Cuisine", "Location", "Rating", "Price", "Hours"], "instance": {"id": "824", "words": ["is", "there", "a", "japanese", "restraunt", "near", "by"], "labels": ["O", "O", "O", "B-Cuisine", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Restaurant Name, Cuisine, Location, Rating, Price, Hours and O.\nSentence: is there a japanese restraunt near by", "prompt_labels": "is(O) there(O) a(O) japanese(B-Cuisine) restraunt(O) near(B-Location) by(I-Location)"}}
{"id": "841", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Restaurant Name", "Location", "Cuisine", "Rating", "Amenity", "Price", "Hours"], "instance": {"id": "841", "words": ["is", "there", "a", "place", "near", "by", "that", "serves", "tapas"], "labels": ["O", "O", "O", "O", "B-Location", "I-Location", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Restaurant Name, Location, Cuisine, Rating, Amenity, Price, Hours and O.\nSentence: is there a place near by that serves tapas", "prompt_labels": "is(O) there(O) a(O) place(O) near(B-Location) by(I-Location) that(O) serves(O) tapas(B-Dish)"}}
{"id": "382", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Dish", "Location", "Hours", "Amenity", "Cuisine", "Price", "Rating"], "instance": {"id": "382", "words": ["find", "me", "a", "pizza", "parlour", "please"], "labels": ["O", "O", "O", "B-Cuisine", "I-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Dish, Location, Hours, Amenity, Cuisine, Price, Rating and O.\nSentence: find me a pizza parlour please", "prompt_labels": "find(O) me(O) a(O) pizza(B-Cuisine) parlour(I-Cuisine) please(O)"}}
{"id": "660", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Location", "Price", "Rating", "Amenity", "Hours", "Restaurant Name", "Dish"], "instance": {"id": "660", "words": ["i", "want", "to", "find", "a", "place", "with", "spaghetti", "and", "meatballs"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Dish", "I-Dish", "I-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Location, Price, Rating, Amenity, Hours, Restaurant Name, Dish and O.\nSentence: i want to find a place with spaghetti and meatballs", "prompt_labels": "i(O) want(O) to(O) find(O) a(O) place(O) with(O) spaghetti(B-Dish) and(I-Dish) meatballs(I-Dish)"}}
{"id": "12", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Location", "Dish", "Rating", "Amenity", "Cuisine", "Restaurant Name", "Price"], "instance": {"id": "12", "words": ["any", "restaurants", "open", "right", "now"], "labels": ["O", "O", "B-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Location, Dish, Rating, Amenity, Cuisine, Restaurant Name, Price and O.\nSentence: any restaurants open right now", "prompt_labels": "any(O) restaurants(O) open(B-Hours) right(I-Hours) now(I-Hours)"}}
{"id": "128", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Restaurant Name", "Hours", "Dish", "Cuisine", "Amenity", "Rating", "Location"], "instance": {"id": "128", "words": ["can", "i", "have", "the", "phone", "number", "for", "kfc", "in", "los", "angeles", "ca"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Restaurant Name", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Restaurant Name, Hours, Dish, Cuisine, Amenity, Rating, Location and O.\nSentence: can i have the phone number for kfc in los angeles ca", "prompt_labels": "can(O) i(O) have(O) the(O) phone(O) number(O) for(O) kfc(B-Restaurant Name) in(O) los(B-Location) angeles(I-Location) ca(I-Location)"}}
{"id": "220", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Rating", "Dish", "Hours", "Location", "Restaurant Name", "Cuisine", "Amenity"], "instance": {"id": "220", "words": ["could", "you", "help", "me", "locate", "a", "place", "for", "lunch"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Rating, Dish, Hours, Location, Restaurant Name, Cuisine, Amenity and O.\nSentence: could you help me locate a place for lunch", "prompt_labels": "could(O) you(O) help(O) me(O) locate(O) a(O) place(O) for(O) lunch(B-Hours)"}}
{"id": "709", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Dish", "Cuisine", "Amenity", "Rating", "Location", "Hours", "Restaurant Name"], "instance": {"id": "709", "words": ["id", "like", "to", "find", "a", "cheap", "pub", "with", "internet", "access"], "labels": ["O", "O", "O", "O", "O", "B-Price", "B-Cuisine", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Dish, Cuisine, Amenity, Rating, Location, Hours, Restaurant Name and O.\nSentence: id like to find a cheap pub with internet access", "prompt_labels": "id(O) like(O) to(O) find(O) a(O) cheap(B-Price) pub(B-Cuisine) with(O) internet(B-Amenity) access(I-Amenity)"}}
{"id": "1330", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Cuisine", "Amenity", "Restaurant Name", "Hours", "Rating", "Dish", "Price"], "instance": {"id": "1330", "words": ["where", "can", "i", "get", "nachos", "within", "1", "mile", "of", "me", "that", "is", "open", "late"], "labels": ["O", "O", "O", "O", "B-Dish", "B-Location", "I-Location", "I-Location", "O", "O", "O", "O", "B-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Cuisine, Amenity, Restaurant Name, Hours, Rating, Dish, Price and O.\nSentence: where can i get nachos within 1 mile of me that is open late", "prompt_labels": "where(O) can(O) i(O) get(O) nachos(B-Dish) within(B-Location) 1(I-Location) mile(I-Location) of(O) me(O) that(O) is(O) open(B-Hours) late(I-Hours)"}}
{"id": "775", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Dish", "Restaurant Name", "Price", "Hours", "Location", "Rating", "Cuisine"], "instance": {"id": "775", "words": ["is", "pasquales", "still", "located", "on", "dayton", "street"], "labels": ["O", "B-Restaurant Name", "O", "O", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Restaurant Name, Price, Hours, Location, Rating, Cuisine and O.\nSentence: is pasquales still located on dayton street", "prompt_labels": "is(O) pasquales(B-Restaurant Name) still(O) located(O) on(O) dayton(B-Location) street(I-Location)"}}
{"id": "1262", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Restaurant Name", "Dish", "Hours", "Rating", "Amenity", "Price", "Location"], "instance": {"id": "1262", "words": ["where", "can", "i", "eat", "pita", "bread", "late", "at", "night"], "labels": ["O", "O", "O", "O", "B-Dish", "I-Dish", "B-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Restaurant Name, Dish, Hours, Rating, Amenity, Price, Location and O.\nSentence: where can i eat pita bread late at night", "prompt_labels": "where(O) can(O) i(O) eat(O) pita(B-Dish) bread(I-Dish) late(B-Hours) at(I-Hours) night(I-Hours)"}}
{"id": "889", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Amenity", "Hours", "Rating", "Location", "Dish", "Price", "Restaurant Name"], "instance": {"id": "889", "words": ["is", "there", "an", "environmentally", "friendly", "restaurant", "nearby"], "labels": ["O", "O", "O", "B-Amenity", "I-Amenity", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Amenity, Hours, Rating, Location, Dish, Price, Restaurant Name and O.\nSentence: is there an environmentally friendly restaurant nearby", "prompt_labels": "is(O) there(O) an(O) environmentally(B-Amenity) friendly(I-Amenity) restaurant(O) nearby(B-Location)"}}
{"id": "82", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Rating", "Hours", "Dish", "Location", "Restaurant Name", "Cuisine", "Price"], "instance": {"id": "82", "words": ["are", "there", "any", "restaurants", "that", "serve", "raw", "and", "organic", "food", "nearby"], "labels": ["O", "O", "O", "O", "O", "O", "B-Cuisine", "I-Cuisine", "I-Cuisine", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Hours, Dish, Location, Restaurant Name, Cuisine, Price and O.\nSentence: are there any restaurants that serve raw and organic food nearby", "prompt_labels": "are(O) there(O) any(O) restaurants(O) that(O) serve(O) raw(B-Cuisine) and(I-Cuisine) organic(I-Cuisine) food(O) nearby(B-Location)"}}
{"id": "1034", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Hours", "Restaurant Name", "Dish", "Cuisine", "Amenity", "Location", "Rating"], "instance": {"id": "1034", "words": ["please", "locate", "a", "mcdonalds"], "labels": ["O", "O", "O", "B-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Hours, Restaurant Name, Dish, Cuisine, Amenity, Location, Rating and O.\nSentence: please locate a mcdonalds", "prompt_labels": "please(O) locate(O) a(O) mcdonalds(B-Restaurant Name)"}}
{"id": "765", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Cuisine", "Amenity", "Dish", "Price", "Location", "Restaurant Name", "Hours"], "instance": {"id": "765", "words": ["is", "chickfila", "open", "today"], "labels": ["O", "B-Restaurant Name", "B-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Cuisine, Amenity, Dish, Price, Location, Restaurant Name, Hours and O.\nSentence: is chickfila open today", "prompt_labels": "is(O) chickfila(B-Restaurant Name) open(B-Hours) today(I-Hours)"}}
{"id": "989", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Amenity", "Rating", "Restaurant Name", "Location", "Price", "Hours", "Cuisine"], "instance": {"id": "989", "words": ["looking", "for", "urban", "gourmet", "on", "bay", "road", "with", "great", "wine", "lists"], "labels": ["O", "O", "B-Cuisine", "I-Cuisine", "O", "B-Location", "I-Location", "O", "B-Rating", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Amenity, Rating, Restaurant Name, Location, Price, Hours, Cuisine and O.\nSentence: looking for urban gourmet on bay road with great wine lists", "prompt_labels": "looking(O) for(O) urban(B-Cuisine) gourmet(I-Cuisine) on(O) bay(B-Location) road(I-Location) with(O) great(B-Rating) wine(O) lists(O)"}}
{"id": "1116", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Restaurant Name", "Rating", "Cuisine", "Location", "Amenity", "Dish", "Hours"], "instance": {"id": "1116", "words": ["what", "is", "the", "average", "price", "for", "lunch", "at", "olive", "garden"], "labels": ["O", "O", "O", "O", "O", "O", "B-Hours", "O", "B-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Restaurant Name, Rating, Cuisine, Location, Amenity, Dish, Hours and O.\nSentence: what is the average price for lunch at olive garden", "prompt_labels": "what(O) is(O) the(O) average(O) price(O) for(O) lunch(B-Hours) at(O) olive(B-Restaurant Name) garden(I-Restaurant Name)"}}
{"id": "1158", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Hours", "Cuisine", "Rating", "Restaurant Name", "Location", "Dish", "Amenity"], "instance": {"id": "1158", "words": ["what", "kind", "of", "food", "does", "abc", "cafe", "serve"], "labels": ["O", "O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Hours, Cuisine, Rating, Restaurant Name, Location, Dish, Amenity and O.\nSentence: what kind of food does abc cafe serve", "prompt_labels": "what(O) kind(O) of(O) food(O) does(O) abc(B-Restaurant Name) cafe(I-Restaurant Name) serve(O)"}}
{"id": "102", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Location", "Rating", "Hours", "Amenity", "Restaurant Name", "Cuisine", "Price"], "instance": {"id": "102", "words": ["asian", "cuisine", "in", "my", "zip", "code"], "labels": ["B-Cuisine", "O", "B-Location", "I-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Rating, Hours, Amenity, Restaurant Name, Cuisine, Price and O.\nSentence: asian cuisine in my zip code", "prompt_labels": "asian(B-Cuisine) cuisine(O) in(B-Location) my(I-Location) zip(I-Location) code(I-Location)"}}
{"id": "994", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Dish", "Amenity", "Restaurant Name", "Price", "Hours", "Location", "Rating"], "instance": {"id": "994", "words": ["make", "a", "reservation", "for", "us", "at", "the", "french", "restaurant", "for", "tonight", "at", "8", "pm", "also", "give", "us", "the", "dress", "code", "for", "the", "restaurant"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Cuisine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Dish, Amenity, Restaurant Name, Price, Hours, Location, Rating and O.\nSentence: make a reservation for us at the french restaurant for tonight at 8 pm also give us the dress code for the restaurant", "prompt_labels": "make(O) a(O) reservation(O) for(O) us(O) at(O) the(O) french(B-Cuisine) restaurant(O) for(O) tonight(O) at(O) 8(O) pm(O) also(O) give(O) us(O) the(O) dress(B-Amenity) code(I-Amenity) for(O) the(O) restaurant(O)"}}
{"id": "1412", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Cuisine", "Hours", "Rating", "Restaurant Name", "Location", "Dish", "Price"], "instance": {"id": "1412", "words": ["where", "is", "the", "closest", "happy", "hour"], "labels": ["O", "O", "O", "B-Location", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Cuisine, Hours, Rating, Restaurant Name, Location, Dish, Price and O.\nSentence: where is the closest happy hour", "prompt_labels": "where(O) is(O) the(O) closest(B-Location) happy(B-Amenity) hour(I-Amenity)"}}
{"id": "1036", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Amenity", "Restaurant Name", "Cuisine", "Price", "Location", "Dish", "Hours"], "instance": {"id": "1036", "words": ["please", "name", "all", "restaurants", "that", "offer", "curb", "side", "pick", "up", "on", "highway", "43", "south"], "labels": ["O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity", "I-Amenity", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Restaurant Name, Cuisine, Price, Location, Dish, Hours and O.\nSentence: please name all restaurants that offer curb side pick up on highway 43 south", "prompt_labels": "please(O) name(O) all(O) restaurants(O) that(O) offer(O) curb(B-Amenity) side(I-Amenity) pick(I-Amenity) up(I-Amenity) on(O) highway(B-Location) 43(I-Location) south(I-Location)"}}
{"id": "178", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Location", "Hours", "Cuisine", "Restaurant Name", "Rating", "Price", "Dish"], "instance": {"id": "178", "words": ["can", "you", "get", "pork", "in", "chinatown"], "labels": ["O", "O", "O", "B-Dish", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Location, Hours, Cuisine, Restaurant Name, Rating, Price, Dish and O.\nSentence: can you get pork in chinatown", "prompt_labels": "can(O) you(O) get(O) pork(B-Dish) in(O) chinatown(B-Location)"}}
{"id": "1487", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Rating", "Price", "Amenity", "Hours", "Restaurant Name", "Cuisine", "Location"], "instance": {"id": "1487", "words": ["which", "pizza", "places", "will", "deliver", "to", "me"], "labels": ["O", "B-Cuisine", "O", "O", "B-Amenity", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Rating, Price, Amenity, Hours, Restaurant Name, Cuisine, Location and O.\nSentence: which pizza places will deliver to me", "prompt_labels": "which(O) pizza(B-Cuisine) places(O) will(O) deliver(B-Amenity) to(O) me(O)"}}
{"id": "736", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Cuisine", "Rating", "Hours", "Price", "Amenity", "Restaurant Name", "Dish"], "instance": {"id": "736", "words": ["im", "looking", "for", "a", "nice", "place", "to", "eat", "for", "me", "and", "my", "girlfriends", "one", "year", "anniversary", "where", "i", "can", "schedule", "romantic", "candles", "and", "flowers", "for", "her"], "labels": ["O", "O", "O", "O", "B-Rating", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "O", "B-Amenity", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Cuisine, Rating, Hours, Price, Amenity, Restaurant Name, Dish and O.\nSentence: im looking for a nice place to eat for me and my girlfriends one year anniversary where i can schedule romantic candles and flowers for her", "prompt_labels": "im(O) looking(O) for(O) a(O) nice(B-Rating) place(O) to(O) eat(O) for(O) me(O) and(O) my(O) girlfriends(O) one(O) year(O) anniversary(O) where(O) i(O) can(O) schedule(O) romantic(B-Amenity) candles(I-Amenity) and(O) flowers(B-Amenity) for(O) her(O)"}}
{"id": "1357", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Price", "Location", "Restaurant Name", "Rating", "Dish", "Cuisine", "Amenity"], "instance": {"id": "1357", "words": ["where", "can", "i", "take", "my", "5", "year", "old", "nephew", "to", "eat"], "labels": ["O", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity", "I-Amenity", "I-Amenity", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Price, Location, Restaurant Name, Rating, Dish, Cuisine, Amenity and O.\nSentence: where can i take my 5 year old nephew to eat", "prompt_labels": "where(O) can(O) i(O) take(B-Amenity) my(I-Amenity) 5(I-Amenity) year(I-Amenity) old(I-Amenity) nephew(O) to(O) eat(O)"}}
{"id": "1430", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Amenity", "Rating", "Dish", "Hours", "Location", "Restaurant Name", "Price"], "instance": {"id": "1430", "words": ["where", "is", "the", "nearest", "burger", "place"], "labels": ["O", "O", "O", "B-Location", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Amenity, Rating, Dish, Hours, Location, Restaurant Name, Price and O.\nSentence: where is the nearest burger place", "prompt_labels": "where(O) is(O) the(O) nearest(B-Location) burger(B-Cuisine) place(O)"}}
{"id": "946", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Amenity", "Cuisine", "Location", "Hours", "Dish", "Price", "Restaurant Name"], "instance": {"id": "946", "words": ["local", "mcdonalds", "please"], "labels": ["B-Location", "B-Restaurant Name", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Cuisine, Location, Hours, Dish, Price, Restaurant Name and O.\nSentence: local mcdonalds please", "prompt_labels": "local(B-Location) mcdonalds(B-Restaurant Name) please(O)"}}
{"id": "453", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Rating", "Restaurant Name", "Dish", "Price", "Amenity", "Hours", "Cuisine"], "instance": {"id": "453", "words": ["find", "the", "nearest", "chicken", "stand"], "labels": ["O", "O", "B-Location", "B-Cuisine", "I-Cuisine"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Rating, Restaurant Name, Dish, Price, Amenity, Hours, Cuisine and O.\nSentence: find the nearest chicken stand", "prompt_labels": "find(O) the(O) nearest(B-Location) chicken(B-Cuisine) stand(I-Cuisine)"}}
{"id": "998", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Price", "Restaurant Name", "Location", "Rating", "Cuisine", "Amenity", "Hours"], "instance": {"id": "998", "words": ["may", "i", "have", "the", "business", "hours", "for", "the", "nearest", "red", "lobster"], "labels": ["O", "O", "O", "O", "B-Hours", "O", "O", "O", "B-Location", "B-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Price, Restaurant Name, Location, Rating, Cuisine, Amenity, Hours and O.\nSentence: may i have the business hours for the nearest red lobster", "prompt_labels": "may(O) i(O) have(O) the(O) business(B-Hours) hours(O) for(O) the(O) nearest(B-Location) red(B-Restaurant Name) lobster(I-Restaurant Name)"}}
{"id": "1321", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Price", "Dish", "Restaurant Name", "Amenity", "Location", "Cuisine", "Rating"], "instance": {"id": "1321", "words": ["where", "can", "i", "get", "bagels"], "labels": ["O", "O", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Price, Dish, Restaurant Name, Amenity, Location, Cuisine, Rating and O.\nSentence: where can i get bagels", "prompt_labels": "where(O) can(O) i(O) get(O) bagels(B-Dish)"}}
{"id": "301", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Amenity", "Hours", "Price", "Rating", "Dish", "Cuisine", "Restaurant Name"], "instance": {"id": "301", "words": ["does", "ricatonis", "offer", "a", "lunch", "portion", "option"], "labels": ["O", "B-Restaurant Name", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Amenity, Hours, Price, Rating, Dish, Cuisine, Restaurant Name and O.\nSentence: does ricatonis offer a lunch portion option", "prompt_labels": "does(O) ricatonis(B-Restaurant Name) offer(O) a(O) lunch(B-Amenity) portion(I-Amenity) option(I-Amenity)"}}
{"id": "1432", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Rating", "Amenity", "Location", "Dish", "Restaurant Name", "Price", "Hours"], "instance": {"id": "1432", "words": ["where", "is", "the", "nearest", "chinese", "restaurant", "with", "more", "than", "3", "stars", "that", "is", "under", "10", "an", "entree"], "labels": ["O", "O", "O", "B-Location", "B-Cuisine", "O", "O", "B-Rating", "I-Rating", "I-Rating", "I-Rating", "O", "O", "B-Price", "I-Price", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Amenity, Location, Dish, Restaurant Name, Price, Hours and O.\nSentence: where is the nearest chinese restaurant with more than 3 stars that is under 10 an entree", "prompt_labels": "where(O) is(O) the(O) nearest(B-Location) chinese(B-Cuisine) restaurant(O) with(O) more(B-Rating) than(I-Rating) 3(I-Rating) stars(I-Rating) that(O) is(O) under(B-Price) 10(I-Price) an(O) entree(O)"}}
{"id": "540", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Price", "Rating", "Location", "Hours", "Dish", "Cuisine", "Amenity"], "instance": {"id": "540", "words": ["i", "am", "diabetic", "and", "need", "to", "know", "if", "there", "are", "any", "health", "stores", "in", "the", "area"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-Cuisine", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Price, Rating, Location, Hours, Dish, Cuisine, Amenity and O.\nSentence: i am diabetic and need to know if there are any health stores in the area", "prompt_labels": "i(O) am(O) diabetic(O) and(O) need(O) to(O) know(O) if(O) there(O) are(O) any(O) health(B-Cuisine) stores(O) in(B-Location) the(I-Location) area(I-Location)"}}
{"id": "815", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Restaurant Name", "Price", "Hours", "Dish", "Rating", "Amenity", "Cuisine"], "instance": {"id": "815", "words": ["is", "there", "a", "dress", "code", "and", "yuris", "dine", "in", "restaurant"], "labels": ["O", "O", "O", "B-Amenity", "I-Amenity", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Restaurant Name, Price, Hours, Dish, Rating, Amenity, Cuisine and O.\nSentence: is there a dress code and yuris dine in restaurant", "prompt_labels": "is(O) there(O) a(O) dress(B-Amenity) code(I-Amenity) and(O) yuris(B-Restaurant Name) dine(I-Restaurant Name) in(I-Restaurant Name) restaurant(I-Restaurant Name)"}}
{"id": "397", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Dish", "Amenity", "Cuisine", "Location", "Hours", "Restaurant Name", "Rating"], "instance": {"id": "397", "words": ["find", "me", "a", "restaurant", "with", "burgers", "on", "the", "menu", "and", "large", "portions", "that", "offers", "business", "dining"], "labels": ["O", "O", "O", "O", "O", "B-Dish", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Dish, Amenity, Cuisine, Location, Hours, Restaurant Name, Rating and O.\nSentence: find me a restaurant with burgers on the menu and large portions that offers business dining", "prompt_labels": "find(O) me(O) a(O) restaurant(O) with(O) burgers(B-Dish) on(O) the(O) menu(O) and(O) large(B-Amenity) portions(I-Amenity) that(O) offers(O) business(B-Amenity) dining(I-Amenity)"}}
{"id": "1312", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Hours", "Amenity", "Restaurant Name", "Rating", "Dish", "Location", "Price"], "instance": {"id": "1312", "words": ["where", "can", "i", "get", "a", "great", "tofu", "omelette", "in", "wayland"], "labels": ["O", "O", "O", "O", "O", "O", "B-Dish", "I-Dish", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Hours, Amenity, Restaurant Name, Rating, Dish, Location, Price and O.\nSentence: where can i get a great tofu omelette in wayland", "prompt_labels": "where(O) can(O) i(O) get(O) a(O) great(O) tofu(B-Dish) omelette(I-Dish) in(O) wayland(B-Location)"}}
{"id": "986", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Hours", "Price", "Cuisine", "Rating", "Dish", "Location", "Restaurant Name"], "instance": {"id": "986", "words": ["looking", "for", "the", "cheapest", "place", "to", "eat", "im", "on", "a", "budget"], "labels": ["O", "O", "O", "B-Price", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Price, Cuisine, Rating, Dish, Location, Restaurant Name and O.\nSentence: looking for the cheapest place to eat im on a budget", "prompt_labels": "looking(O) for(O) the(O) cheapest(B-Price) place(O) to(O) eat(O) im(O) on(O) a(O) budget(O)"}}
{"id": "1147", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Rating", "Dish", "Location", "Hours", "Restaurant Name", "Amenity", "Price"], "instance": {"id": "1147", "words": ["what", "is", "the", "phone", "number", "to", "alans", "on", "oak", "street"], "labels": ["O", "O", "O", "O", "O", "O", "B-Restaurant Name", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Dish, Location, Hours, Restaurant Name, Amenity, Price and O.\nSentence: what is the phone number to alans on oak street", "prompt_labels": "what(O) is(O) the(O) phone(O) number(O) to(O) alans(B-Restaurant Name) on(O) oak(B-Location) street(I-Location)"}}
{"id": "376", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Price", "Rating", "Location", "Amenity", "Restaurant Name", "Cuisine", "Dish"], "instance": {"id": "376", "words": ["find", "me", "a", "good", "vegetarian", "restaurant"], "labels": ["O", "O", "O", "B-Rating", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Price, Rating, Location, Amenity, Restaurant Name, Cuisine, Dish and O.\nSentence: find me a good vegetarian restaurant", "prompt_labels": "find(O) me(O) a(O) good(B-Rating) vegetarian(B-Cuisine) restaurant(O)"}}
{"id": "209", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Dish", "Price", "Hours", "Amenity", "Location", "Restaurant Name", "Cuisine"], "instance": {"id": "209", "words": ["can", "you", "tell", "me", "where", "the", "nearest", "sushi", "restaurant", "is"], "labels": ["O", "O", "O", "O", "O", "O", "B-Location", "B-Cuisine", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Dish, Price, Hours, Amenity, Location, Restaurant Name, Cuisine and O.\nSentence: can you tell me where the nearest sushi restaurant is", "prompt_labels": "can(O) you(O) tell(O) me(O) where(O) the(O) nearest(B-Location) sushi(B-Cuisine) restaurant(O) is(O)"}}
{"id": "512", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Location", "Hours", "Cuisine", "Rating", "Price", "Dish", "Restaurant Name"], "instance": {"id": "512", "words": ["how", "far", "away", "is", "the", "nearest", "steak", "house"], "labels": ["O", "O", "O", "O", "O", "B-Location", "B-Cuisine", "I-Cuisine"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Location, Hours, Cuisine, Rating, Price, Dish, Restaurant Name and O.\nSentence: how far away is the nearest steak house", "prompt_labels": "how(O) far(O) away(O) is(O) the(O) nearest(B-Location) steak(B-Cuisine) house(I-Cuisine)"}}
{"id": "883", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Hours", "Rating", "Location", "Cuisine", "Dish", "Price", "Restaurant Name"], "instance": {"id": "883", "words": ["is", "there", "a", "wangs", "fast", "food", "near", "providence", "highway"], "labels": ["O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Rating, Location, Cuisine, Dish, Price, Restaurant Name and O.\nSentence: is there a wangs fast food near providence highway", "prompt_labels": "is(O) there(O) a(O) wangs(B-Restaurant Name) fast(I-Restaurant Name) food(I-Restaurant Name) near(O) providence(B-Location) highway(I-Location)"}}
{"id": "929", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Hours", "Restaurant Name", "Price", "Cuisine", "Rating", "Amenity", "Dish"], "instance": {"id": "929", "words": ["is", "this", "restaurant", "a", "local", "favorite"], "labels": ["O", "O", "O", "O", "B-Location", "B-Rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Hours, Restaurant Name, Price, Cuisine, Rating, Amenity, Dish and O.\nSentence: is this restaurant a local favorite", "prompt_labels": "is(O) this(O) restaurant(O) a(O) local(B-Location) favorite(B-Rating)"}}
{"id": "1489", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Dish", "Price", "Amenity", "Hours", "Cuisine", "Rating", "Restaurant Name"], "instance": {"id": "1489", "words": ["which", "restaurant", "can", "i", "get", "to", "within", "5", "minutes", "which", "serves", "healthy", "portions"], "labels": ["O", "O", "O", "O", "O", "O", "B-Location", "I-Location", "I-Location", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Dish, Price, Amenity, Hours, Cuisine, Rating, Restaurant Name and O.\nSentence: which restaurant can i get to within 5 minutes which serves healthy portions", "prompt_labels": "which(O) restaurant(O) can(O) i(O) get(O) to(O) within(B-Location) 5(I-Location) minutes(I-Location) which(O) serves(O) healthy(B-Amenity) portions(I-Amenity)"}}
{"id": "665", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Hours", "Rating", "Amenity", "Restaurant Name", "Location", "Dish", "Price"], "instance": {"id": "665", "words": ["i", "want", "to", "get", "a", "reservation", "at", "the", "best", "michelin", "rated", "french", "restaurant"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-Rating", "I-Rating", "I-Rating", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Hours, Rating, Amenity, Restaurant Name, Location, Dish, Price and O.\nSentence: i want to get a reservation at the best michelin rated french restaurant", "prompt_labels": "i(O) want(O) to(O) get(O) a(O) reservation(O) at(O) the(O) best(B-Rating) michelin(I-Rating) rated(I-Rating) french(B-Cuisine) restaurant(O)"}}
{"id": "1192", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Rating", "Dish", "Restaurant Name", "Location", "Hours", "Amenity", "Cuisine"], "instance": {"id": "1192", "words": ["what", "restaurants", "are", "within", "1", "mile", "and", "have", "good", "prices"], "labels": ["O", "O", "O", "B-Location", "I-Location", "I-Location", "O", "O", "B-Price", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Rating, Dish, Restaurant Name, Location, Hours, Amenity, Cuisine and O.\nSentence: what restaurants are within 1 mile and have good prices", "prompt_labels": "what(O) restaurants(O) are(O) within(B-Location) 1(I-Location) mile(I-Location) and(O) have(O) good(B-Price) prices(O)"}}
{"id": "1141", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Price", "Dish", "Location", "Rating", "Restaurant Name", "Amenity", "Hours"], "instance": {"id": "1141", "words": ["what", "is", "the", "name", "or", "phone", "number", "of", "the", "coffee", "shop", "on", "seminary", "street"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-Cuisine", "O", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Dish, Location, Rating, Restaurant Name, Amenity, Hours and O.\nSentence: what is the name or phone number of the coffee shop on seminary street", "prompt_labels": "what(O) is(O) the(O) name(O) or(O) phone(O) number(O) of(O) the(O) coffee(B-Cuisine) shop(O) on(O) seminary(B-Location) street(I-Location)"}}
{"id": "857", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Restaurant Name", "Rating", "Price", "Amenity", "Location", "Hours", "Dish"], "instance": {"id": "857", "words": ["is", "there", "a", "restaurant", "around", "here", "that", "serves", "chicken", "wings"], "labels": ["O", "O", "O", "O", "B-Location", "I-Location", "O", "O", "B-Dish", "I-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Restaurant Name, Rating, Price, Amenity, Location, Hours, Dish and O.\nSentence: is there a restaurant around here that serves chicken wings", "prompt_labels": "is(O) there(O) a(O) restaurant(O) around(B-Location) here(I-Location) that(O) serves(O) chicken(B-Dish) wings(I-Dish)"}}
{"id": "218", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Price", "Amenity", "Rating", "Dish", "Location", "Cuisine", "Hours"], "instance": {"id": "218", "words": ["could", "you", "find", "me", "a", "place", "thats", "open", "every", "day"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Price, Amenity, Rating, Dish, Location, Cuisine, Hours and O.\nSentence: could you find me a place thats open every day", "prompt_labels": "could(O) you(O) find(O) me(O) a(O) place(O) thats(O) open(B-Hours) every(I-Hours) day(I-Hours)"}}
{"id": "558", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Dish", "Price", "Hours", "Cuisine", "Restaurant Name", "Amenity", "Location"], "instance": {"id": "558", "words": ["i", "feel", "in", "the", "mood", "for", "spicy", "food", "what", "can", "you", "do", "for", "me"], "labels": ["O", "O", "O", "O", "O", "O", "B-Cuisine", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Dish, Price, Hours, Cuisine, Restaurant Name, Amenity, Location and O.\nSentence: i feel in the mood for spicy food what can you do for me", "prompt_labels": "i(O) feel(O) in(O) the(O) mood(O) for(O) spicy(B-Cuisine) food(O) what(O) can(O) you(O) do(O) for(O) me(O)"}}
{"id": "1259", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Rating", "Dish", "Price", "Cuisine", "Location", "Hours", "Restaurant Name"], "instance": {"id": "1259", "words": ["where", "can", "i", "eat", "african", "food", "for", "cheap"], "labels": ["O", "O", "O", "O", "B-Cuisine", "O", "O", "B-Price"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Dish, Price, Cuisine, Location, Hours, Restaurant Name and O.\nSentence: where can i eat african food for cheap", "prompt_labels": "where(O) can(O) i(O) eat(O) african(B-Cuisine) food(O) for(O) cheap(B-Price)"}}
{"id": "625", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Price", "Location", "Dish", "Rating", "Cuisine", "Hours", "Amenity"], "instance": {"id": "625", "words": ["i", "want", "a", "place", "that", "allows", "smoking", "and", "serves", "health", "food"], "labels": ["O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "O", "B-Amenity", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Price, Location, Dish, Rating, Cuisine, Hours, Amenity and O.\nSentence: i want a place that allows smoking and serves health food", "prompt_labels": "i(O) want(O) a(O) place(O) that(O) allows(B-Amenity) smoking(I-Amenity) and(O) serves(B-Amenity) health(B-Cuisine) food(O)"}}
{"id": "201", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Price", "Rating", "Amenity", "Location", "Cuisine", "Restaurant Name", "Hours"], "instance": {"id": "201", "words": ["can", "you", "please", "direct", "me", "to", "the", "nearest", "chinese", "restaurant"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Location", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Price, Rating, Amenity, Location, Cuisine, Restaurant Name, Hours and O.\nSentence: can you please direct me to the nearest chinese restaurant", "prompt_labels": "can(O) you(O) please(O) direct(O) me(O) to(O) the(O) nearest(B-Location) chinese(B-Cuisine) restaurant(O)"}}
{"id": "390", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Hours", "Amenity", "Restaurant Name", "Dish", "Cuisine", "Rating", "Price"], "instance": {"id": "390", "words": ["find", "me", "a", "ranch", "style", "barbecue", "that", "serves", "lunch", "at", "3", "pm", "about", "one", "mile", "from", "my", "current", "position"], "labels": ["O", "O", "O", "B-Cuisine", "I-Cuisine", "I-Cuisine", "O", "B-Hours", "I-Hours", "I-Hours", "I-Hours", "I-Hours", "B-Location", "I-Location", "I-Location", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Hours, Amenity, Restaurant Name, Dish, Cuisine, Rating, Price and O.\nSentence: find me a ranch style barbecue that serves lunch at 3 pm about one mile from my current position", "prompt_labels": "find(O) me(O) a(O) ranch(B-Cuisine) style(I-Cuisine) barbecue(I-Cuisine) that(O) serves(B-Hours) lunch(I-Hours) at(I-Hours) 3(I-Hours) pm(I-Hours) about(B-Location) one(I-Location) mile(I-Location) from(O) my(O) current(O) position(O)"}}
{"id": "374", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Hours", "Restaurant Name", "Cuisine", "Rating", "Amenity", "Price", "Location"], "instance": {"id": "374", "words": ["find", "me", "a", "good", "pho", "restaurant", "in", "portland", "or"], "labels": ["O", "O", "O", "B-Rating", "B-Cuisine", "O", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Hours, Restaurant Name, Cuisine, Rating, Amenity, Price, Location and O.\nSentence: find me a good pho restaurant in portland or", "prompt_labels": "find(O) me(O) a(O) good(B-Rating) pho(B-Cuisine) restaurant(O) in(O) portland(B-Location) or(I-Location)"}}
{"id": "1014", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Cuisine", "Dish", "Price", "Rating", "Amenity", "Location", "Hours"], "instance": {"id": "1014", "words": ["nyc", "5", "star", "pizza", "parlors"], "labels": ["B-Location", "B-Rating", "I-Rating", "B-Dish", "B-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Cuisine, Dish, Price, Rating, Amenity, Location, Hours and O.\nSentence: nyc 5 star pizza parlors", "prompt_labels": "nyc(B-Location) 5(B-Rating) star(I-Rating) pizza(B-Dish) parlors(B-Restaurant Name)"}}
