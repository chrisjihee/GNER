{"id":"0","dataset":"mit-movie","split":"dev","instance":{"id":"0","prompt_labels":"are(O) there(O) any(O) good(O) romantic(B-genre) comedies(I-genre) out(O) right(B-year) now(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, trailer, average ratings, director, rating, actor, review, year, genre, song, plot, title and O.\nSentence: are there any good romantic comedies out right now","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","good","romantic","comedies","out","right","now"],"labels":["O","O","O","O","B-genre","I-genre","O","B-year","I-year"],"target_index":null,"target_label":null},"label_list":["character","trailer","average_ratings","director","rating","actor","review","year","genre","song","plot","title"]}
{"id":"1","dataset":"mit-movie","split":"dev","instance":{"id":"1","prompt_labels":"show(O) me(O) a(O) movie(O) about(O) cars(B-plot) that(I-plot) talk(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, rating, average ratings, actor, title, song, plot, review, trailer, character, director, genre and O.\nSentence: show me a movie about cars that talk","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","a","movie","about","cars","that","talk"],"labels":["O","O","O","O","O","B-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["year","rating","average_ratings","actor","title","song","plot","review","trailer","character","director","genre"]}
{"id":"2","dataset":"mit-movie","split":"dev","instance":{"id":"2","prompt_labels":"list(O) the(O) five(B-average ratings) star(I-average ratings) rated(O) movies(O) starring(O) mel(B-actor) gibson(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, average ratings, song, review, trailer, plot, title, genre, actor, rating, character, year and O.\nSentence: list the five star rated movies starring mel gibson","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","the","five","star","rated","movies","starring","mel","gibson"],"labels":["O","O","B-average ratings","I-average ratings","O","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["director","average_ratings","song","review","trailer","plot","title","genre","actor","rating","character","year"]}
{"id":"3","dataset":"mit-movie","split":"dev","instance":{"id":"3","prompt_labels":"what(O) science(B-genre) fiction(I-genre) films(O) have(O) come(O) out(O) recently(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, plot, review, genre, year, director, rating, character, title, trailer, song, actor and O.\nSentence: what science fiction films have come out recently","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","science","fiction","films","have","come","out","recently"],"labels":["O","B-genre","I-genre","O","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["average_ratings","plot","review","genre","year","director","rating","character","title","trailer","song","actor"]}
{"id":"4","dataset":"mit-movie","split":"dev","instance":{"id":"4","prompt_labels":"did(O) the(O) same(O) director(O) make(O) all(O) of(O) the(O) harry(B-title) potter(I-title) movies(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, title, review, character, rating, year, average ratings, director, genre, song, trailer, plot and O.\nSentence: did the same director make all of the harry potter movies","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","the","same","director","make","all","of","the","harry","potter","movies"],"labels":["O","O","O","O","O","O","O","O","B-title","I-title","O"],"target_index":null,"target_label":null},"label_list":["actor","title","review","character","rating","year","average_ratings","director","genre","song","trailer","plot"]}
{"id":"5","dataset":"mit-movie","split":"dev","instance":{"id":"5","prompt_labels":"show(O) me(O) 1980s(B-year) action(B-genre) movies(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, actor, title, review, song, rating, character, year, average ratings, genre, trailer, director and O.\nSentence: show me 1980s action movies","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","1980s","action","movies"],"labels":["O","O","B-year","B-genre","O"],"target_index":null,"target_label":null},"label_list":["plot","actor","title","review","song","rating","character","year","average_ratings","genre","trailer","director"]}
{"id":"6","dataset":"mit-movie","split":"dev","instance":{"id":"6","prompt_labels":"what(O) is(O) the(O) name(O) of(O) the(O) third(O) movie(O) in(O) the(O) star(B-title) trek(I-title) series(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, plot, average ratings, character, trailer, rating, director, year, song, actor, review, title and O.\nSentence: what is the name of the third movie in the star trek series","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","name","of","the","third","movie","in","the","star","trek","series"],"labels":["O","O","O","O","O","O","O","O","O","O","B-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["genre","plot","average_ratings","character","trailer","rating","director","year","song","actor","review","title"]}
{"id":"7","dataset":"mit-movie","split":"dev","instance":{"id":"7","prompt_labels":"can(O) you(O) get(O) a(O) soundtrac(B-song) for(O) the(O) harry(B-title) potter(I-title) films(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, trailer, review, genre, year, character, title, rating, average ratings, director, song, plot and O.\nSentence: can you get a soundtrac for the harry potter films","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","get","a","soundtrac","for","the","harry","potter","films"],"labels":["O","O","O","O","B-song","O","O","B-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["actor","trailer","review","genre","year","character","title","rating","average_ratings","director","song","plot"]}
{"id":"8","dataset":"mit-movie","split":"dev","instance":{"id":"8","prompt_labels":"find(O) me(O) science(B-genre) fiction(I-genre) movies(O) since(B-year) 2005(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, review, actor, rating, title, genre, year, plot, song, character, director, average ratings and O.\nSentence: find me science fiction movies since 2005","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","science","fiction","movies","since","2005"],"labels":["O","O","B-genre","I-genre","O","B-year","I-year"],"target_index":null,"target_label":null},"label_list":["trailer","review","actor","rating","title","genre","year","plot","song","character","director","average_ratings"]}
{"id":"9","dataset":"mit-movie","split":"dev","instance":{"id":"9","prompt_labels":"what(O) is(O) the(O) most(O) current(B-year) movie(O) featuring(O) mat(B-actor) damon(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, character, year, actor, rating, director, genre, average ratings, trailer, review, plot, title and O.\nSentence: what is the most current movie featuring mat damon","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","most","current","movie","featuring","mat","damon"],"labels":["O","O","O","O","B-year","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["song","character","year","actor","rating","director","genre","average_ratings","trailer","review","plot","title"]}
{"id":"15","dataset":"mit-movie","split":"dev","instance":{"id":"15","prompt_labels":"find(O) rated(O) g(B-rating) films(O) with(O) flying(B-plot) cars(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, average ratings, title, genre, character, song, rating, director, actor, review, year, trailer and O.\nSentence: find rated g films with flying cars","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","rated","g","films","with","flying","cars"],"labels":["O","O","B-rating","O","O","B-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["plot","average_ratings","title","genre","character","song","rating","director","actor","review","year","trailer"]}
{"id":"16","dataset":"mit-movie","split":"dev","instance":{"id":"16","prompt_labels":"what(O) was(O) the(O) best(B-review) rated(O) stanley(B-director) kubrick(I-director) film(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, character, actor, song, title, trailer, review, director, year, average ratings, genre, plot and O.\nSentence: what was the best rated stanley kubrick film","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","best","rated","stanley","kubrick","film"],"labels":["O","O","O","B-review","O","B-director","I-director","O"],"target_index":null,"target_label":null},"label_list":["rating","character","actor","song","title","trailer","review","director","year","average_ratings","genre","plot"]}
{"id":"17","dataset":"mit-movie","split":"dev","instance":{"id":"17","prompt_labels":"are(O) there(O) any(O) films(O) directed(O) by(O) shawn(B-director) levy(I-director) about(O) large(B-plot) families(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, genre, plot, director, rating, title, trailer, character, year, actor, average ratings and O.\nSentence: are there any films directed by shawn levy about large families","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","films","directed","by","shawn","levy","about","large","families"],"labels":["O","O","O","O","O","O","B-director","I-director","O","B-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["review","song","genre","plot","director","rating","title","trailer","character","year","actor","average_ratings"]}
{"id":"18","dataset":"mit-movie","split":"dev","instance":{"id":"18","prompt_labels":"list(O) pg(B-rating) rated(O) movies(O) about(O) cars(B-plot) released(O) in(O) the(O) 1990s(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, average ratings, song, title, trailer, review, plot, character, rating, director, actor, genre and O.\nSentence: list pg rated movies about cars released in the 1990s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","pg","rated","movies","about","cars","released","in","the","1990s"],"labels":["O","B-rating","O","O","O","B-plot","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["year","average_ratings","song","title","trailer","review","plot","character","rating","director","actor","genre"]}
{"id":"19","dataset":"mit-movie","split":"dev","instance":{"id":"19","prompt_labels":"what(O) movie(O) won(O) best(B-average ratings) picure(I-average ratings) at(O) the(O) 2012(B-average ratings) oscars(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, year, actor, title, plot, rating, review, director, song, average ratings, trailer, character and O.\nSentence: what movie won best picure at the 2012 oscars","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","movie","won","best","picure","at","the","2012","oscars"],"labels":["O","O","O","B-average ratings","I-average ratings","O","O","B-average ratings","O"],"target_index":null,"target_label":null},"label_list":["genre","year","actor","title","plot","rating","review","director","song","average_ratings","trailer","character"]}
{"id":"23","dataset":"mit-movie","split":"dev","instance":{"id":"23","prompt_labels":"who(O) directed(B-director) the(O) film(O) pulp(B-title) fiction(I-title) that(O) starred(O) john(B-actor) travolta(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, title, average ratings, review, actor, song, year, trailer, rating, genre, director, character and O.\nSentence: who directed the film pulp fiction that starred john travolta","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","directed","the","film","pulp","fiction","that","starred","john","travolta"],"labels":["O","B-director","O","O","B-title","I-title","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["plot","title","average_ratings","review","actor","song","year","trailer","rating","genre","director","character"]}
{"id":"29","dataset":"mit-movie","split":"dev","instance":{"id":"29","prompt_labels":"is(O) there(O) a(O) pg(B-rating) 13(I-rating) movie(O) thats(O) scary(B-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, character, rating, review, year, genre, title, actor, average ratings, song, director, trailer and O.\nSentence: is there a pg 13 movie thats scary","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","pg","13","movie","thats","scary"],"labels":["O","O","O","B-rating","I-rating","O","O","B-genre"],"target_index":null,"target_label":null},"label_list":["plot","character","rating","review","year","genre","title","actor","average_ratings","song","director","trailer"]}
{"id":"30","dataset":"mit-movie","split":"dev","instance":{"id":"30","prompt_labels":"what(O) movies(O) made(O) in(O) 2004(B-year) were(O) pg(B-rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, plot, rating, genre, character, year, trailer, title, average ratings, song, director, actor and O.\nSentence: what movies made in 2004 were pg","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","movies","made","in","2004","were","pg"],"labels":["O","O","O","O","B-year","O","B-rating"],"target_index":null,"target_label":null},"label_list":["review","plot","rating","genre","character","year","trailer","title","average_ratings","song","director","actor"]}
{"id":"31","dataset":"mit-movie","split":"dev","instance":{"id":"31","prompt_labels":"find(O) movies(O) with(O) robert(B-actor) diniero(I-actor) in(O) it(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, character, actor, average ratings, genre, trailer, plot, director, review, song, rating, title and O.\nSentence: find movies with robert diniero in it","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","movies","with","robert","diniero","in","it"],"labels":["O","O","O","B-actor","I-actor","O","O"],"target_index":null,"target_label":null},"label_list":["year","character","actor","average_ratings","genre","trailer","plot","director","review","song","rating","title"]}
{"id":"32","dataset":"mit-movie","split":"dev","instance":{"id":"32","prompt_labels":"have(O) pg(B-rating) 13(I-rating) movies(O) for(O) the(O) kidz(B-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, actor, average ratings, title, genre, year, plot, director, song, review, rating, trailer and O.\nSentence: have pg 13 movies for the kidz","prediction_output":null,"prediction_outputs":null,"group":null,"words":["have","pg","13","movies","for","the","kidz"],"labels":["O","B-rating","I-rating","O","O","O","B-genre"],"target_index":null,"target_label":null},"label_list":["character","actor","average_ratings","title","genre","year","plot","director","song","review","rating","trailer"]}
{"id":"35","dataset":"mit-movie","split":"dev","instance":{"id":"35","prompt_labels":"did(O) george(B-director) clooney(I-director) direct(O) any(O) comedy(B-genre) films(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, trailer, genre, song, rating, year, review, director, average ratings, character, title, actor and O.\nSentence: did george clooney direct any comedy films","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","george","clooney","direct","any","comedy","films"],"labels":["O","B-director","I-director","O","O","B-genre","O"],"target_index":null,"target_label":null},"label_list":["plot","trailer","genre","song","rating","year","review","director","average_ratings","character","title","actor"]}
{"id":"37","dataset":"mit-movie","split":"dev","instance":{"id":"37","prompt_labels":"find(O) me(O) the(O) movie(O) that(O) has(O) a(O) aerosmith(B-song) song(I-song)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, title, actor, director, trailer, review, rating, song, year, plot, average ratings, genre and O.\nSentence: find me the movie that has a aerosmith song","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","the","movie","that","has","a","aerosmith","song"],"labels":["O","O","O","O","O","O","O","B-song","I-song"],"target_index":null,"target_label":null},"label_list":["character","title","actor","director","trailer","review","rating","song","year","plot","average_ratings","genre"]}
{"id":"40","dataset":"mit-movie","split":"dev","instance":{"id":"40","prompt_labels":"what(O) is(O) the(O) highest(B-average ratings) rated(I-average ratings) kids(B-genre) new(B-year) release(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, actor, character, trailer, song, title, plot, genre, rating, average ratings, review, year and O.\nSentence: what is the highest rated kids new release","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","highest","rated","kids","new","release"],"labels":["O","O","O","B-average ratings","I-average ratings","B-genre","B-year","I-year"],"target_index":null,"target_label":null},"label_list":["director","actor","character","trailer","song","title","plot","genre","rating","average_ratings","review","year"]}
{"id":"41","dataset":"mit-movie","split":"dev","instance":{"id":"41","prompt_labels":"worst(B-average ratings) review(I-average ratings) for(O) 2011(B-year) movies(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, trailer, year, average ratings, director, rating, title, genre, character, plot, song, review and O.\nSentence: worst review for 2011 movies","prediction_output":null,"prediction_outputs":null,"group":null,"words":["worst","review","for","2011","movies"],"labels":["B-average ratings","I-average ratings","O","B-year","O"],"target_index":null,"target_label":null},"label_list":["actor","trailer","year","average_ratings","director","rating","title","genre","character","plot","song","review"]}
{"id":"43","dataset":"mit-movie","split":"dev","instance":{"id":"43","prompt_labels":"lets(O) find(O) an(O) independent(B-genre) film(I-genre) company(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, genre, trailer, director, title, character, plot, review, year, rating, song, average ratings and O.\nSentence: lets find an independent film company","prediction_output":null,"prediction_outputs":null,"group":null,"words":["lets","find","an","independent","film","company"],"labels":["O","O","O","B-genre","I-genre","O"],"target_index":null,"target_label":null},"label_list":["actor","genre","trailer","director","title","character","plot","review","year","rating","song","average_ratings"]}
{"id":"44","dataset":"mit-movie","split":"dev","instance":{"id":"44","prompt_labels":"which(O) movies(O) were(O) based(B-plot) off(I-plot) of(I-plot) video(I-plot) games(I-plot) besides(O) resident(B-title) evil(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, rating, director, plot, character, review, average ratings, song, actor, title, trailer, genre and O.\nSentence: which movies were based off of video games besides resident evil","prediction_output":null,"prediction_outputs":null,"group":null,"words":["which","movies","were","based","off","of","video","games","besides","resident","evil"],"labels":["O","O","O","B-plot","I-plot","I-plot","I-plot","I-plot","O","B-title","I-title"],"target_index":null,"target_label":null},"label_list":["year","rating","director","plot","character","review","average_ratings","song","actor","title","trailer","genre"]}
{"id":"45","dataset":"mit-movie","split":"dev","instance":{"id":"45","prompt_labels":"did(O) dame(B-actor) judy(I-actor) dench(I-actor) star(O) in(O) a(O) british(B-plot) film(O) about(O) queen(B-character) elizabeth(I-character)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, genre, plot, song, title, year, average ratings, review, character, director, trailer, actor and O.\nSentence: did dame judy dench star in a british film about queen elizabeth","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","dame","judy","dench","star","in","a","british","film","about","queen","elizabeth"],"labels":["O","B-actor","I-actor","I-actor","O","O","O","B-plot","O","O","B-character","I-character"],"target_index":null,"target_label":null},"label_list":["rating","genre","plot","song","title","year","average_ratings","review","character","director","trailer","actor"]}
{"id":"46","dataset":"mit-movie","split":"dev","instance":{"id":"46","prompt_labels":"present(O) list(O) of(O) family(B-genre) movies(I-genre) that(O) chris(B-director) columbus(I-director) directed(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, average ratings, plot, year, rating, director, genre, review, song, trailer, character, actor and O.\nSentence: present list of family movies that chris columbus directed","prediction_output":null,"prediction_outputs":null,"group":null,"words":["present","list","of","family","movies","that","chris","columbus","directed"],"labels":["O","O","O","B-genre","I-genre","O","B-director","I-director","O"],"target_index":null,"target_label":null},"label_list":["title","average_ratings","plot","year","rating","director","genre","review","song","trailer","character","actor"]}
{"id":"47","dataset":"mit-movie","split":"dev","instance":{"id":"47","prompt_labels":"find(O) a(O) movie(O) with(O) dogs(B-plot) as(O) the(O) main(O) character(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, director, review, trailer, character, plot, average ratings, year, rating, actor, genre, song and O.\nSentence: find a movie with dogs as the main character","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","a","movie","with","dogs","as","the","main","character"],"labels":["O","O","O","O","B-plot","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["title","director","review","trailer","character","plot","average_ratings","year","rating","actor","genre","song"]}
{"id":"50","dataset":"mit-movie","split":"dev","instance":{"id":"50","prompt_labels":"find(O) me(O) the(O) movies(O) that(O) starred(O) anne(B-actor) hathaway(I-actor) and(O) julie(B-actor) andrews(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, character, average ratings, rating, director, genre, year, actor, title, plot, trailer and O.\nSentence: find me the movies that starred anne hathaway and julie andrews","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","the","movies","that","starred","anne","hathaway","and","julie","andrews"],"labels":["O","O","O","O","O","O","B-actor","I-actor","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["review","song","character","average_ratings","rating","director","genre","year","actor","title","plot","trailer"]}
{"id":"52","dataset":"mit-movie","split":"dev","instance":{"id":"52","prompt_labels":"list(O) the(O) dirty(B-title) harry(I-title) films(O) from(O) the(O) 1980s(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, song, genre, rating, plot, average ratings, director, character, title, review, actor, trailer and O.\nSentence: list the dirty harry films from the 1980s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","the","dirty","harry","films","from","the","1980s"],"labels":["O","O","B-title","I-title","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["year","song","genre","rating","plot","average_ratings","director","character","title","review","actor","trailer"]}
{"id":"54","dataset":"mit-movie","split":"dev","instance":{"id":"54","prompt_labels":"are(O) there(O) any(O) meg(B-actor) ryan(I-actor) romantic(B-genre) comedy(I-genre) movies(O) that(O) are(O) considered(O) must(B-review) see(I-review)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, genre, title, trailer, rating, director, average ratings, year, actor, song, plot, review and O.\nSentence: are there any meg ryan romantic comedy movies that are considered must see","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","meg","ryan","romantic","comedy","movies","that","are","considered","must","see"],"labels":["O","O","O","B-actor","I-actor","B-genre","I-genre","O","O","O","O","B-review","I-review"],"target_index":null,"target_label":null},"label_list":["character","genre","title","trailer","rating","director","average_ratings","year","actor","song","plot","review"]}
{"id":"55","dataset":"mit-movie","split":"dev","instance":{"id":"55","prompt_labels":"what(O) was(O) james(B-director) camerons(I-director) directorial(O) debut(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, year, title, genre, director, average ratings, plot, trailer, character, song, review, actor and O.\nSentence: what was james camerons directorial debut","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","james","camerons","directorial","debut"],"labels":["O","O","B-director","I-director","O","O"],"target_index":null,"target_label":null},"label_list":["rating","year","title","genre","director","average_ratings","plot","trailer","character","song","review","actor"]}
{"id":"56","dataset":"mit-movie","split":"dev","instance":{"id":"56","prompt_labels":"what(O) are(O) top(B-average ratings) 50(I-average ratings) movies(O) of(O) all(B-average ratings) time(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, title, genre, actor, director, song, average ratings, rating, character, trailer, plot, year and O.\nSentence: what are top 50 movies of all time","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","top","50","movies","of","all","time"],"labels":["O","O","B-average ratings","I-average ratings","O","O","B-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["review","title","genre","actor","director","song","average_ratings","rating","character","trailer","plot","year"]}
{"id":"57","dataset":"mit-movie","split":"dev","instance":{"id":"57","prompt_labels":"find(O) me(O) comedy(B-genre) movies(O) with(O) liam(B-actor) hemsworth(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, average ratings, plot, actor, year, director, song, trailer, character, title, rating, review and O.\nSentence: find me comedy movies with liam hemsworth","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","comedy","movies","with","liam","hemsworth"],"labels":["O","O","B-genre","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["genre","average_ratings","plot","actor","year","director","song","trailer","character","title","rating","review"]}
{"id":"58","dataset":"mit-movie","split":"dev","instance":{"id":"58","prompt_labels":"i(O) want(O) to(O) see(O) cradle(B-title) 2(I-title) the(I-title) grave(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, average ratings, plot, director, genre, song, character, trailer, title, rating, actor, year and O.\nSentence: i want to see cradle 2 the grave","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","to","see","cradle","2","the","grave"],"labels":["O","O","O","O","B-title","I-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["review","average_ratings","plot","director","genre","song","character","trailer","title","rating","actor","year"]}
{"id":"59","dataset":"mit-movie","split":"dev","instance":{"id":"59","prompt_labels":"what(O) was(O) the(O) most(B-review) popular(I-review) movie(I-review) from(O) 2004(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, actor, year, title, song, character, trailer, plot, average ratings, director, rating, review and O.\nSentence: what was the most popular movie from 2004","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","most","popular","movie","from","2004"],"labels":["O","O","O","B-review","I-review","I-review","O","B-year"],"target_index":null,"target_label":null},"label_list":["genre","actor","year","title","song","character","trailer","plot","average_ratings","director","rating","review"]}
{"id":"60","dataset":"mit-movie","split":"dev","instance":{"id":"60","prompt_labels":"what(O) is(O) the(O) g(B-rating) rated(I-rating) movie(O) about(O) rabbits(B-plot) looking(I-plot) for(I-plot) a(I-plot) new(I-plot) home(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, trailer, average ratings, director, year, rating, character, title, actor, song, plot, genre and O.\nSentence: what is the g rated movie about rabbits looking for a new home","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","g","rated","movie","about","rabbits","looking","for","a","new","home"],"labels":["O","O","O","B-rating","I-rating","O","O","B-plot","I-plot","I-plot","I-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["review","trailer","average_ratings","director","year","rating","character","title","actor","song","plot","genre"]}
{"id":"63","dataset":"mit-movie","split":"dev","instance":{"id":"63","prompt_labels":"how(O) many(O) times(O) has(O) matt(B-actor) damon(I-actor) been(O) jason(B-character) bourne(I-character)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, average ratings, song, title, year, director, plot, actor, review, character, trailer, rating and O.\nSentence: how many times has matt damon been jason bourne","prediction_output":null,"prediction_outputs":null,"group":null,"words":["how","many","times","has","matt","damon","been","jason","bourne"],"labels":["O","O","O","O","B-actor","I-actor","O","B-character","I-character"],"target_index":null,"target_label":null},"label_list":["genre","average_ratings","song","title","year","director","plot","actor","review","character","trailer","rating"]}
{"id":"64","dataset":"mit-movie","split":"dev","instance":{"id":"64","prompt_labels":"whats(O) the(O) latetest(B-year) foreign(B-genre) romantic(I-genre) movie(O) with(O) lots(O) of(O) sex(B-plot) and(O) sadness(B-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, review, year, character, rating, title, director, actor, trailer, song, genre, plot and O.\nSentence: whats the latetest foreign romantic movie with lots of sex and sadness","prediction_output":null,"prediction_outputs":null,"group":null,"words":["whats","the","latetest","foreign","romantic","movie","with","lots","of","sex","and","sadness"],"labels":["O","O","B-year","B-genre","I-genre","O","O","O","O","B-plot","O","B-plot"],"target_index":null,"target_label":null},"label_list":["average_ratings","review","year","character","rating","title","director","actor","trailer","song","genre","plot"]}
{"id":"65","dataset":"mit-movie","split":"dev","instance":{"id":"65","prompt_labels":"list(O) movies(O) with(O) jeremy(B-actor) piven(I-actor) released(O) in(O) the(O) 1990s(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, character, actor, trailer, average ratings, year, song, review, genre, plot, title, director and O.\nSentence: list movies with jeremy piven released in the 1990s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","movies","with","jeremy","piven","released","in","the","1990s"],"labels":["O","O","O","B-actor","I-actor","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["rating","character","actor","trailer","average_ratings","year","song","review","genre","plot","title","director"]}
{"id":"66","dataset":"mit-movie","split":"dev","instance":{"id":"66","prompt_labels":"show(O) me(O) the(O) collection(O) of(O) action(B-genre) movies(I-genre) of(O) arnold(B-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, year, song, review, title, plot, actor, genre, average ratings, rating, trailer, character and O.\nSentence: show me the collection of action movies of arnold","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","the","collection","of","action","movies","of","arnold"],"labels":["O","O","O","O","O","B-genre","I-genre","O","B-actor"],"target_index":null,"target_label":null},"label_list":["director","year","song","review","title","plot","actor","genre","average_ratings","rating","trailer","character"]}
{"id":"67","dataset":"mit-movie","split":"dev","instance":{"id":"67","prompt_labels":"are(O) there(O) comic(B-genre) book(I-genre) movies(I-genre) that(O) are(O) over(O) pg(B-rating) 13(I-rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, review, character, year, song, trailer, average ratings, rating, director, title, actor, plot and O.\nSentence: are there comic book movies that are over pg 13","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","comic","book","movies","that","are","over","pg","13"],"labels":["O","O","B-genre","I-genre","I-genre","O","O","O","B-rating","I-rating"],"target_index":null,"target_label":null},"label_list":["genre","review","character","year","song","trailer","average_ratings","rating","director","title","actor","plot"]}
{"id":"68","dataset":"mit-movie","split":"dev","instance":{"id":"68","prompt_labels":"the(O) new(O) batman(B-title) movie(O) looks(O) epic(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, character, rating, genre, actor, plot, title, year, trailer, director, average ratings and O.\nSentence: the new batman movie looks epic","prediction_output":null,"prediction_outputs":null,"group":null,"words":["the","new","batman","movie","looks","epic"],"labels":["O","O","B-title","O","O","O"],"target_index":null,"target_label":null},"label_list":["review","song","character","rating","genre","actor","plot","title","year","trailer","director","average_ratings"]}
{"id":"69","dataset":"mit-movie","split":"dev","instance":{"id":"69","prompt_labels":"show(O) me(O) movies(O) who(O) won(O) awards(B-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, year, trailer, rating, character, director, genre, title, plot, review, song, average ratings and O.\nSentence: show me movies who won awards","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","movies","who","won","awards"],"labels":["O","O","O","O","O","B-average ratings"],"target_index":null,"target_label":null},"label_list":["actor","year","trailer","rating","character","director","genre","title","plot","review","song","average_ratings"]}
{"id":"70","dataset":"mit-movie","split":"dev","instance":{"id":"70","prompt_labels":"what(O) is(O) the(O) year(B-year) that(O) dirty(B-title) dancing(I-title) was(O) released(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, actor, song, rating, plot, review, average ratings, character, director, genre, title, trailer and O.\nSentence: what is the year that dirty dancing was released","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","year","that","dirty","dancing","was","released"],"labels":["O","O","O","B-year","O","B-title","I-title","O","O"],"target_index":null,"target_label":null},"label_list":["year","actor","song","rating","plot","review","average_ratings","character","director","genre","title","trailer"]}
{"id":"73","dataset":"mit-movie","split":"dev","instance":{"id":"73","prompt_labels":"are(O) there(O) any(O) silent(B-genre) movies(O) made(O) after(B-year) 1930(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, plot, title, character, director, average ratings, actor, year, rating, review, trailer, song and O.\nSentence: are there any silent movies made after 1930","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","silent","movies","made","after","1930"],"labels":["O","O","O","B-genre","O","O","B-year","I-year"],"target_index":null,"target_label":null},"label_list":["genre","plot","title","character","director","average_ratings","actor","year","rating","review","trailer","song"]}
{"id":"74","dataset":"mit-movie","split":"dev","instance":{"id":"74","prompt_labels":"who(O) was(O) the(O) actress(O) in(O) the(B-title) goodbye(I-title) girl(I-title) with(O) richard(B-actor) dreyfuss(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, average ratings, plot, review, genre, year, title, trailer, character, rating, song, actor and O.\nSentence: who was the actress in the goodbye girl with richard dreyfuss","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","was","the","actress","in","the","goodbye","girl","with","richard","dreyfuss"],"labels":["O","O","O","O","O","B-title","I-title","I-title","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["director","average_ratings","plot","review","genre","year","title","trailer","character","rating","song","actor"]}
{"id":"78","dataset":"mit-movie","split":"dev","instance":{"id":"78","prompt_labels":"what(O) movie(O) has(O) the(O) most(B-review) remakes(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, rating, actor, director, year, title, character, average ratings, review, song, genre, trailer and O.\nSentence: what movie has the most remakes","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","movie","has","the","most","remakes"],"labels":["O","O","O","O","B-review","O"],"target_index":null,"target_label":null},"label_list":["plot","rating","actor","director","year","title","character","average_ratings","review","song","genre","trailer"]}
{"id":"81","dataset":"mit-movie","split":"dev","instance":{"id":"81","prompt_labels":"i(O) would(O) like(O) a(O) list(O) of(O) movies(O) about(O) dancing(B-plot) from(O) the(O) past(B-year) 10(I-year) years(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, year, rating, genre, director, title, actor, song, average ratings, plot, trailer, review and O.\nSentence: i would like a list of movies about dancing from the past 10 years","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","would","like","a","list","of","movies","about","dancing","from","the","past","10","years"],"labels":["O","O","O","O","O","O","O","O","B-plot","O","O","B-year","I-year","I-year"],"target_index":null,"target_label":null},"label_list":["character","year","rating","genre","director","title","actor","song","average_ratings","plot","trailer","review"]}
{"id":"82","dataset":"mit-movie","split":"dev","instance":{"id":"82","prompt_labels":"who(O) stars(O) in(O) project(B-title) x(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, plot, character, rating, genre, title, average ratings, director, year, actor, trailer, song and O.\nSentence: who stars in project x","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","stars","in","project","x"],"labels":["O","O","O","B-title","I-title"],"target_index":null,"target_label":null},"label_list":["review","plot","character","rating","genre","title","average_ratings","director","year","actor","trailer","song"]}
{"id":"83","dataset":"mit-movie","split":"dev","instance":{"id":"83","prompt_labels":"find(O) action(B-genre) movies(O) featuring(O) comic(B-plot) book(I-plot) characters(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, director, rating, year, character, song, plot, actor, trailer, average ratings, title, review and O.\nSentence: find action movies featuring comic book characters","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","action","movies","featuring","comic","book","characters"],"labels":["O","B-genre","O","O","B-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["genre","director","rating","year","character","song","plot","actor","trailer","average_ratings","title","review"]}
{"id":"84","dataset":"mit-movie","split":"dev","instance":{"id":"84","prompt_labels":"what(O) are(O) some(O) g(B-rating) rated(O) movies(O) with(O) fairies(B-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, plot, title, genre, song, rating, director, actor, average ratings, review, character, year and O.\nSentence: what are some g rated movies with fairies","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","some","g","rated","movies","with","fairies"],"labels":["O","O","O","B-rating","O","O","O","B-plot"],"target_index":null,"target_label":null},"label_list":["trailer","plot","title","genre","song","rating","director","actor","average_ratings","review","character","year"]}
{"id":"86","dataset":"mit-movie","split":"dev","instance":{"id":"86","prompt_labels":"what(O) movie(O) did(O) rod(B-director) serling(I-director) write(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, actor, review, average ratings, director, trailer, title, rating, year, genre, character, plot and O.\nSentence: what movie did rod serling write","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","movie","did","rod","serling","write"],"labels":["O","O","O","B-director","I-director","O"],"target_index":null,"target_label":null},"label_list":["song","actor","review","average_ratings","director","trailer","title","rating","year","genre","character","plot"]}
{"id":"87","dataset":"mit-movie","split":"dev","instance":{"id":"87","prompt_labels":"is(O) there(O) an(O) animated(B-genre) adult(I-genre) fantasy(I-genre) movie(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, director, song, average ratings, character, year, rating, review, genre, actor, trailer, plot and O.\nSentence: is there an animated adult fantasy movie","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","an","animated","adult","fantasy","movie"],"labels":["O","O","O","B-genre","I-genre","I-genre","O"],"target_index":null,"target_label":null},"label_list":["title","director","song","average_ratings","character","year","rating","review","genre","actor","trailer","plot"]}
{"id":"93","dataset":"mit-movie","split":"dev","instance":{"id":"93","prompt_labels":"who(O) directed(O) princess(B-title) bride(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, plot, character, year, trailer, title, review, genre, rating, actor, average ratings, director and O.\nSentence: who directed princess bride","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","directed","princess","bride"],"labels":["O","O","B-title","I-title"],"target_index":null,"target_label":null},"label_list":["song","plot","character","year","trailer","title","review","genre","rating","actor","average_ratings","director"]}
{"id":"95","dataset":"mit-movie","split":"dev","instance":{"id":"95","prompt_labels":"name(O) a(O) western(B-genre) comedy(I-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, genre, trailer, average ratings, title, song, actor, character, review, plot, rating, director and O.\nSentence: name a western comedy","prediction_output":null,"prediction_outputs":null,"group":null,"words":["name","a","western","comedy"],"labels":["O","O","B-genre","I-genre"],"target_index":null,"target_label":null},"label_list":["year","genre","trailer","average_ratings","title","song","actor","character","review","plot","rating","director"]}
{"id":"97","dataset":"mit-movie","split":"dev","instance":{"id":"97","prompt_labels":"how(O) many(O) movies(O) have(O) starred(O) brad(B-actor) pitt(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, director, title, actor, rating, character, song, plot, average ratings, genre, trailer, review and O.\nSentence: how many movies have starred brad pitt","prediction_output":null,"prediction_outputs":null,"group":null,"words":["how","many","movies","have","starred","brad","pitt"],"labels":["O","O","O","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["year","director","title","actor","rating","character","song","plot","average_ratings","genre","trailer","review"]}
{"id":"100","dataset":"mit-movie","split":"dev","instance":{"id":"100","prompt_labels":"channing(B-actor) tatum(I-actor) has(O) played(O) what(O) starring(O) roles(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, average ratings, director, review, title, rating, year, genre, actor, trailer, plot, song and O.\nSentence: channing tatum has played what starring roles","prediction_output":null,"prediction_outputs":null,"group":null,"words":["channing","tatum","has","played","what","starring","roles"],"labels":["B-actor","I-actor","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["character","average_ratings","director","review","title","rating","year","genre","actor","trailer","plot","song"]}
{"id":"101","dataset":"mit-movie","split":"dev","instance":{"id":"101","prompt_labels":"list(O) of(O) actors(O) a(B-title) beautiful(I-title) mind(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, rating, song, director, year, actor, review, trailer, average ratings, character, genre, plot and O.\nSentence: list of actors a beautiful mind","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","of","actors","a","beautiful","mind"],"labels":["O","O","O","B-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["title","rating","song","director","year","actor","review","trailer","average_ratings","character","genre","plot"]}
{"id":"113","dataset":"mit-movie","split":"dev","instance":{"id":"113","prompt_labels":"are(O) there(O) any(O) pg(B-rating) movies(I-rating) with(O) car(B-plot) chases(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, genre, title, plot, rating, actor, director, song, year, trailer, average ratings, character and O.\nSentence: are there any pg movies with car chases","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","pg","movies","with","car","chases"],"labels":["O","O","O","B-rating","I-rating","O","B-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["review","genre","title","plot","rating","actor","director","song","year","trailer","average_ratings","character"]}
{"id":"117","dataset":"mit-movie","split":"dev","instance":{"id":"117","prompt_labels":"who(O) directed(B-director) runaway(B-title) jury(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, year, review, plot, character, director, rating, genre, actor, average ratings, song, trailer and O.\nSentence: who directed runaway jury","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","directed","runaway","jury"],"labels":["O","B-director","B-title","I-title"],"target_index":null,"target_label":null},"label_list":["title","year","review","plot","character","director","rating","genre","actor","average_ratings","song","trailer"]}
{"id":"119","dataset":"mit-movie","split":"dev","instance":{"id":"119","prompt_labels":"what(O) popular(B-review) films(O) released(O) last(B-year) month(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, year, title, genre, plot, director, average ratings, actor, song, rating, review, character and O.\nSentence: what popular films released last month","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","popular","films","released","last","month"],"labels":["O","B-review","O","O","B-year","I-year"],"target_index":null,"target_label":null},"label_list":["trailer","year","title","genre","plot","director","average_ratings","actor","song","rating","review","character"]}
{"id":"120","dataset":"mit-movie","split":"dev","instance":{"id":"120","prompt_labels":"did(O) dean(B-director) parisot(I-director) direct(O) sigourney(B-actor) weaver(I-actor) and(O) tim(B-actor) allen(I-actor) in(O) a(O) comedy(B-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, average ratings, trailer, genre, song, character, year, actor, plot, rating, director, title and O.\nSentence: did dean parisot direct sigourney weaver and tim allen in a comedy","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","dean","parisot","direct","sigourney","weaver","and","tim","allen","in","a","comedy"],"labels":["O","B-director","I-director","O","B-actor","I-actor","O","B-actor","I-actor","O","O","B-genre"],"target_index":null,"target_label":null},"label_list":["review","average_ratings","trailer","genre","song","character","year","actor","plot","rating","director","title"]}
{"id":"124","dataset":"mit-movie","split":"dev","instance":{"id":"124","prompt_labels":"how(O) many(O) films(O) did(O) clive(B-actor) owen(I-actor) play(O) in(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, genre, trailer, song, character, actor, director, title, review, rating, plot, average ratings and O.\nSentence: how many films did clive owen play in","prediction_output":null,"prediction_outputs":null,"group":null,"words":["how","many","films","did","clive","owen","play","in"],"labels":["O","O","O","O","B-actor","I-actor","O","O"],"target_index":null,"target_label":null},"label_list":["year","genre","trailer","song","character","actor","director","title","review","rating","plot","average_ratings"]}
{"id":"125","dataset":"mit-movie","split":"dev","instance":{"id":"125","prompt_labels":"are(O) there(O) any(O) films(O) that(O) harmony(B-director) korine(I-director) regrets(B-review) directing(O) andor(O) releasing(O) to(O) the(O) public(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, genre, director, actor, review, character, title, trailer, song, plot, year, average ratings and O.\nSentence: are there any films that harmony korine regrets directing andor releasing to the public","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","films","that","harmony","korine","regrets","directing","andor","releasing","to","the","public"],"labels":["O","O","O","O","O","B-director","I-director","B-review","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["rating","genre","director","actor","review","character","title","trailer","song","plot","year","average_ratings"]}
{"id":"133","dataset":"mit-movie","split":"dev","instance":{"id":"133","prompt_labels":"how(O) many(O) movies(O) did(O) christopher(B-actor) walkin(I-actor) star(O) in(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, year, average ratings, director, song, trailer, plot, rating, review, actor, genre, title and O.\nSentence: how many movies did christopher walkin star in","prediction_output":null,"prediction_outputs":null,"group":null,"words":["how","many","movies","did","christopher","walkin","star","in"],"labels":["O","O","O","O","B-actor","I-actor","O","O"],"target_index":null,"target_label":null},"label_list":["character","year","average_ratings","director","song","trailer","plot","rating","review","actor","genre","title"]}
{"id":"136","dataset":"mit-movie","split":"dev","instance":{"id":"136","prompt_labels":"waht(O) was(O) the(O) plot(O) of(O) the(B-title) deep(I-title) blue(I-title) sea(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, genre, review, trailer, character, year, average ratings, rating, title, actor, song, director and O.\nSentence: waht was the plot of the deep blue sea","prediction_output":null,"prediction_outputs":null,"group":null,"words":["waht","was","the","plot","of","the","deep","blue","sea"],"labels":["O","O","O","O","O","B-title","I-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["plot","genre","review","trailer","character","year","average_ratings","rating","title","actor","song","director"]}
{"id":"142","dataset":"mit-movie","split":"dev","instance":{"id":"142","prompt_labels":"what(O) techno(B-genre) thriller(O) gets(O) a(O) low(B-average ratings) rating(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, actor, director, review, song, genre, rating, title, plot, average ratings, trailer, year and O.\nSentence: what techno thriller gets a low rating","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","techno","thriller","gets","a","low","rating"],"labels":["O","B-genre","O","O","O","B-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["character","actor","director","review","song","genre","rating","title","plot","average_ratings","trailer","year"]}
{"id":"145","dataset":"mit-movie","split":"dev","instance":{"id":"145","prompt_labels":"what(O) is(O) the(O) most(B-year) recent(I-year) sean(B-actor) connery(I-actor) film(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, song, rating, title, genre, character, year, trailer, average ratings, review, plot, director and O.\nSentence: what is the most recent sean connery film","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","most","recent","sean","connery","film"],"labels":["O","O","O","B-year","I-year","B-actor","I-actor","O"],"target_index":null,"target_label":null},"label_list":["actor","song","rating","title","genre","character","year","trailer","average_ratings","review","plot","director"]}
{"id":"147","dataset":"mit-movie","split":"dev","instance":{"id":"147","prompt_labels":"what(O) is(O) a(O) recent(O) george(B-actor) clooney(I-actor) movie(O) with(O) high(B-average ratings) viewers(I-average ratings) rating(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, plot, average ratings, actor, director, year, review, song, rating, title, genre, trailer and O.\nSentence: what is a recent george clooney movie with high viewers rating","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","a","recent","george","clooney","movie","with","high","viewers","rating"],"labels":["O","O","O","O","B-actor","I-actor","O","O","B-average ratings","I-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["character","plot","average_ratings","actor","director","year","review","song","rating","title","genre","trailer"]}
{"id":"148","dataset":"mit-movie","split":"dev","instance":{"id":"148","prompt_labels":"which(O) animated(B-genre) childrens(I-genre) movies(I-genre) are(O) considered(B-review) timeless(I-review)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, director, average ratings, song, character, title, actor, review, plot, trailer, year, genre and O.\nSentence: which animated childrens movies are considered timeless","prediction_output":null,"prediction_outputs":null,"group":null,"words":["which","animated","childrens","movies","are","considered","timeless"],"labels":["O","B-genre","I-genre","I-genre","O","B-review","I-review"],"target_index":null,"target_label":null},"label_list":["rating","director","average_ratings","song","character","title","actor","review","plot","trailer","year","genre"]}
{"id":"152","dataset":"mit-movie","split":"dev","instance":{"id":"152","prompt_labels":"show(O) me(O) terry(B-director) gilliam(I-director) movies(O) starring(O) jeff(B-actor) bridges(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, director, genre, review, rating, actor, character, plot, song, average ratings, year, trailer and O.\nSentence: show me terry gilliam movies starring jeff bridges","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","terry","gilliam","movies","starring","jeff","bridges"],"labels":["O","O","B-director","I-director","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["title","director","genre","review","rating","actor","character","plot","song","average_ratings","year","trailer"]}
{"id":"153","dataset":"mit-movie","split":"dev","instance":{"id":"153","prompt_labels":"was(O) there(O) a(O) trailer(O) for(O) bowling(B-title) for(I-title) columbine(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, plot, character, year, song, title, trailer, genre, director, rating, review, actor and O.\nSentence: was there a trailer for bowling for columbine","prediction_output":null,"prediction_outputs":null,"group":null,"words":["was","there","a","trailer","for","bowling","for","columbine"],"labels":["O","O","O","O","O","B-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["average_ratings","plot","character","year","song","title","trailer","genre","director","rating","review","actor"]}
{"id":"157","dataset":"mit-movie","split":"dev","instance":{"id":"157","prompt_labels":"what(O) movies(O) have(O) batman(B-plot) and(I-plot) robin(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, actor, director, genre, average ratings, title, song, plot, character, year, review, rating and O.\nSentence: what movies have batman and robin","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","movies","have","batman","and","robin"],"labels":["O","O","O","B-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["trailer","actor","director","genre","average_ratings","title","song","plot","character","year","review","rating"]}
{"id":"160","dataset":"mit-movie","split":"dev","instance":{"id":"160","prompt_labels":"did(O) vin(B-actor) diesel(I-actor) star(O) in(O) any(O) comdedies(B-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, actor, rating, title, year, review, plot, character, song, trailer, average ratings, genre and O.\nSentence: did vin diesel star in any comdedies","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","vin","diesel","star","in","any","comdedies"],"labels":["O","B-actor","I-actor","O","O","O","B-genre"],"target_index":null,"target_label":null},"label_list":["director","actor","rating","title","year","review","plot","character","song","trailer","average_ratings","genre"]}
{"id":"161","dataset":"mit-movie","split":"dev","instance":{"id":"161","prompt_labels":"did(O) ray(B-director) liota(I-director) direct(O) any(O) films(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, trailer, rating, director, genre, actor, average ratings, title, character, plot, review, song and O.\nSentence: did ray liota direct any films","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","ray","liota","direct","any","films"],"labels":["O","B-director","I-director","O","O","O"],"target_index":null,"target_label":null},"label_list":["year","trailer","rating","director","genre","actor","average_ratings","title","character","plot","review","song"]}
{"id":"164","dataset":"mit-movie","split":"dev","instance":{"id":"164","prompt_labels":"find(O) me(O) a(O) movie(O) with(O) the(O) song(O) lets(B-song) hear(I-song) it(I-song) for(I-song) the(I-song) boys(I-song)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, average ratings, title, actor, character, trailer, rating, year, genre, review, plot, director and O.\nSentence: find me a movie with the song lets hear it for the boys","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","movie","with","the","song","lets","hear","it","for","the","boys"],"labels":["O","O","O","O","O","O","O","B-song","I-song","I-song","I-song","I-song","I-song"],"target_index":null,"target_label":null},"label_list":["song","average_ratings","title","actor","character","trailer","rating","year","genre","review","plot","director"]}
{"id":"165","dataset":"mit-movie","split":"dev","instance":{"id":"165","prompt_labels":"what(O) was(O) the(B-title) fog(I-title) rated(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, song, trailer, actor, average ratings, genre, review, character, year, rating, director, plot and O.\nSentence: what was the fog rated","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","fog","rated"],"labels":["O","O","B-title","I-title","O"],"target_index":null,"target_label":null},"label_list":["title","song","trailer","actor","average_ratings","genre","review","character","year","rating","director","plot"]}
{"id":"167","dataset":"mit-movie","split":"dev","instance":{"id":"167","prompt_labels":"what(O) was(O) goodfellas(B-title) rated(B-rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, year, character, director, plot, genre, average ratings, song, actor, review, trailer, title and O.\nSentence: what was goodfellas rated","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","goodfellas","rated"],"labels":["O","O","B-title","B-rating"],"target_index":null,"target_label":null},"label_list":["rating","year","character","director","plot","genre","average_ratings","song","actor","review","trailer","title"]}
{"id":"168","dataset":"mit-movie","split":"dev","instance":{"id":"168","prompt_labels":"what(O) was(O) the(O) name(O) of(O) the(O) donkey(B-character) in(O) shrek(B-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, actor, director, average ratings, rating, plot, song, trailer, character, genre, review, year and O.\nSentence: what was the name of the donkey in shrek","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","name","of","the","donkey","in","shrek"],"labels":["O","O","O","O","O","O","B-character","O","B-title"],"target_index":null,"target_label":null},"label_list":["title","actor","director","average_ratings","rating","plot","song","trailer","character","genre","review","year"]}
{"id":"170","dataset":"mit-movie","split":"dev","instance":{"id":"170","prompt_labels":"what(O) are(O) some(O) horror(B-genre) movies(O) from(O) the(O) 1970s(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, trailer, review, director, year, plot, character, title, genre, actor, song, average ratings and O.\nSentence: what are some horror movies from the 1970s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","some","horror","movies","from","the","1970s"],"labels":["O","O","O","B-genre","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["rating","trailer","review","director","year","plot","character","title","genre","actor","song","average_ratings"]}
{"id":"171","dataset":"mit-movie","split":"dev","instance":{"id":"171","prompt_labels":"show(O) me(O) a(O) deborah(B-actor) harry(I-actor) movie(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, character, plot, year, actor, director, review, average ratings, song, title, trailer, genre and O.\nSentence: show me a deborah harry movie","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","a","deborah","harry","movie"],"labels":["O","O","O","B-actor","I-actor","O"],"target_index":null,"target_label":null},"label_list":["rating","character","plot","year","actor","director","review","average_ratings","song","title","trailer","genre"]}
{"id":"172","dataset":"mit-movie","split":"dev","instance":{"id":"172","prompt_labels":"kung(B-title) fu(I-title) panda(I-title) 2(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, genre, trailer, review, character, year, director, plot, title, average ratings, rating, song and O.\nSentence: kung fu panda 2","prediction_output":null,"prediction_outputs":null,"group":null,"words":["kung","fu","panda","2"],"labels":["B-title","I-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["actor","genre","trailer","review","character","year","director","plot","title","average_ratings","rating","song"]}
{"id":"173","dataset":"mit-movie","split":"dev","instance":{"id":"173","prompt_labels":"what(O) year(B-year) was(O) the(O) movie(O) the(B-title) seven(I-title) year(I-title) itch(I-title) made(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, actor, genre, rating, song, review, director, character, average ratings, plot, year, trailer and O.\nSentence: what year was the movie the seven year itch made","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","year","was","the","movie","the","seven","year","itch","made"],"labels":["O","B-year","O","O","O","B-title","I-title","I-title","I-title","O"],"target_index":null,"target_label":null},"label_list":["title","actor","genre","rating","song","review","director","character","average_ratings","plot","year","trailer"]}
{"id":"175","dataset":"mit-movie","split":"dev","instance":{"id":"175","prompt_labels":"what(O) movie(O) stared(O) john(B-actor) travolta(I-actor) and(O) debra(B-actor) winger(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, genre, trailer, review, year, actor, character, song, plot, rating, director, average ratings and O.\nSentence: what movie stared john travolta and debra winger","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","movie","stared","john","travolta","and","debra","winger"],"labels":["O","O","O","B-actor","I-actor","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["title","genre","trailer","review","year","actor","character","song","plot","rating","director","average_ratings"]}
{"id":"176","dataset":"mit-movie","split":"dev","instance":{"id":"176","prompt_labels":"directors(O) of(O) all(O) the(O) batman(B-title) movies(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, title, genre, average ratings, character, director, trailer, review, rating, plot, song, actor and O.\nSentence: directors of all the batman movies","prediction_output":null,"prediction_outputs":null,"group":null,"words":["directors","of","all","the","batman","movies"],"labels":["O","O","O","O","B-title","O"],"target_index":null,"target_label":null},"label_list":["year","title","genre","average_ratings","character","director","trailer","review","rating","plot","song","actor"]}
{"id":"177","dataset":"mit-movie","split":"dev","instance":{"id":"177","prompt_labels":"what(O) are(O) the(O) best(B-review) werewolf(B-plot) movies(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, title, year, plot, rating, song, genre, trailer, review, director, actor, character and O.\nSentence: what are the best werewolf movies","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","the","best","werewolf","movies"],"labels":["O","O","O","B-review","B-plot","O"],"target_index":null,"target_label":null},"label_list":["average_ratings","title","year","plot","rating","song","genre","trailer","review","director","actor","character"]}
{"id":"182","dataset":"mit-movie","split":"dev","instance":{"id":"182","prompt_labels":"are(O) there(O) any(O) movies(O) set(O) in(O) the(O) middle(B-plot) east(I-plot) starring(O) george(B-actor) clooney(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, average ratings, title, genre, song, character, plot, review, year, rating, trailer, director and O.\nSentence: are there any movies set in the middle east starring george clooney","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","movies","set","in","the","middle","east","starring","george","clooney"],"labels":["O","O","O","O","O","O","O","B-plot","I-plot","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["actor","average_ratings","title","genre","song","character","plot","review","year","rating","trailer","director"]}
{"id":"183","dataset":"mit-movie","split":"dev","instance":{"id":"183","prompt_labels":"what(O) was(O) a(O) love(B-plot) story(I-plot) about(I-plot) a(I-plot) woman(I-plot) who(I-plot) had(I-plot) alzheimers(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, title, average ratings, character, year, trailer, song, review, director, rating, plot, actor and O.\nSentence: what was a love story about a woman who had alzheimers","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","a","love","story","about","a","woman","who","had","alzheimers"],"labels":["O","O","O","B-plot","I-plot","I-plot","I-plot","I-plot","I-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["genre","title","average_ratings","character","year","trailer","song","review","director","rating","plot","actor"]}
{"id":"185","dataset":"mit-movie","split":"dev","instance":{"id":"185","prompt_labels":"who(O) played(O) as(O) princess(B-character) fiona(I-character) in(O) shrek(B-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, title, review, year, song, director, average ratings, genre, character, actor, trailer, rating and O.\nSentence: who played as princess fiona in shrek","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","played","as","princess","fiona","in","shrek"],"labels":["O","O","O","B-character","I-character","O","B-title"],"target_index":null,"target_label":null},"label_list":["plot","title","review","year","song","director","average_ratings","genre","character","actor","trailer","rating"]}
{"id":"186","dataset":"mit-movie","split":"dev","instance":{"id":"186","prompt_labels":"is(O) the(B-title) last(I-title) airbender(I-title) rated(B-rating) g(I-rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, average ratings, character, actor, title, director, trailer, year, song, plot, genre, review and O.\nSentence: is the last airbender rated g","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","the","last","airbender","rated","g"],"labels":["O","B-title","I-title","I-title","B-rating","I-rating"],"target_index":null,"target_label":null},"label_list":["rating","average_ratings","character","actor","title","director","trailer","year","song","plot","genre","review"]}
{"id":"188","dataset":"mit-movie","split":"dev","instance":{"id":"188","prompt_labels":"show(O) me(O) dramas(B-genre) about(O) the(O) british(B-plot) royal(I-plot) family(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, title, rating, song, character, year, plot, actor, trailer, average ratings, review, genre and O.\nSentence: show me dramas about the british royal family","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","dramas","about","the","british","royal","family"],"labels":["O","O","B-genre","O","O","B-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["director","title","rating","song","character","year","plot","actor","trailer","average_ratings","review","genre"]}
{"id":"189","dataset":"mit-movie","split":"dev","instance":{"id":"189","prompt_labels":"what(O) was(O) the(O) first(B-year) movie(O) ever(O) released(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, song, character, director, plot, title, review, trailer, actor, average ratings, genre, year and O.\nSentence: what was the first movie ever released","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","first","movie","ever","released"],"labels":["O","O","O","B-year","O","O","O"],"target_index":null,"target_label":null},"label_list":["rating","song","character","director","plot","title","review","trailer","actor","average_ratings","genre","year"]}
{"id":"192","dataset":"mit-movie","split":"dev","instance":{"id":"192","prompt_labels":"what(O) was(O) the(O) title(O) of(O) the(O) bio(O) pic(O) about(O) robert(B-actor) e(I-actor) howard(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, actor, year, average ratings, title, director, plot, trailer, genre, review, character, song and O.\nSentence: what was the title of the bio pic about robert e howard","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","title","of","the","bio","pic","about","robert","e","howard"],"labels":["O","O","O","O","O","O","O","O","O","B-actor","I-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["rating","actor","year","average_ratings","title","director","plot","trailer","genre","review","character","song"]}
{"id":"193","dataset":"mit-movie","split":"dev","instance":{"id":"193","prompt_labels":"what(O) is(O) the(O) theme(O) song(O) to(O) stand(B-title) by(I-title) me(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, review, average ratings, director, trailer, actor, character, song, rating, year, plot, title and O.\nSentence: what is the theme song to stand by me","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","theme","song","to","stand","by","me"],"labels":["O","O","O","O","O","O","B-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["genre","review","average_ratings","director","trailer","actor","character","song","rating","year","plot","title"]}
{"id":"194","dataset":"mit-movie","split":"dev","instance":{"id":"194","prompt_labels":"when(O) did(O) runaway(B-title) jury(I-title) come(O) out(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, genre, title, trailer, year, director, character, song, plot, review, rating, actor and O.\nSentence: when did runaway jury come out","prediction_output":null,"prediction_outputs":null,"group":null,"words":["when","did","runaway","jury","come","out"],"labels":["O","O","B-title","I-title","O","O"],"target_index":null,"target_label":null},"label_list":["average_ratings","genre","title","trailer","year","director","character","song","plot","review","rating","actor"]}
{"id":"195","dataset":"mit-movie","split":"dev","instance":{"id":"195","prompt_labels":"who(O) directed(O) the(O) japanese(O) film(O) versus(B-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, director, title, actor, plot, average ratings, review, year, trailer, song, character, rating and O.\nSentence: who directed the japanese film versus","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","directed","the","japanese","film","versus"],"labels":["O","O","O","O","O","B-title"],"target_index":null,"target_label":null},"label_list":["genre","director","title","actor","plot","average_ratings","review","year","trailer","song","character","rating"]}
{"id":"197","dataset":"mit-movie","split":"dev","instance":{"id":"197","prompt_labels":"name(O) a(O) kirk(B-actor) douglas(I-actor) science(B-genre) fiction(I-genre) film(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, actor, director, title, review, year, plot, song, genre, average ratings, trailer, character and O.\nSentence: name a kirk douglas science fiction film","prediction_output":null,"prediction_outputs":null,"group":null,"words":["name","a","kirk","douglas","science","fiction","film"],"labels":["O","O","B-actor","I-actor","B-genre","I-genre","O"],"target_index":null,"target_label":null},"label_list":["rating","actor","director","title","review","year","plot","song","genre","average_ratings","trailer","character"]}
{"id":"198","dataset":"mit-movie","split":"dev","instance":{"id":"198","prompt_labels":"are(O) there(O) any(O) five(B-average ratings) star(I-average ratings) kevin(B-actor) bacon(I-actor) movies(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, actor, character, year, song, average ratings, trailer, rating, title, director, genre, plot and O.\nSentence: are there any five star kevin bacon movies","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","five","star","kevin","bacon","movies"],"labels":["O","O","O","B-average ratings","I-average ratings","B-actor","I-actor","O"],"target_index":null,"target_label":null},"label_list":["review","actor","character","year","song","average_ratings","trailer","rating","title","director","genre","plot"]}
{"id":"2","dataset":"mit-restaurant","split":"dev","instance":{"id":"2","prompt_labels":"any(O) bbq(B-Cuisine) places(O) open(B-Hours) before(I-Hours) 5(I-Hours) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Hours, Restaurant Name, Price, Amenity, Rating, Cuisine, Location and O.\nSentence: any bbq places open before 5 nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["any","bbq","places","open","before","5","nearby"],"labels":["O","B-Cuisine","O","B-Hours","I-Hours","I-Hours","B-Location"],"target_index":null,"target_label":null},"label_list":["dish","hours","restaurant_name","price","amenity","rating","cuisine","location"]}
{"id":"4","dataset":"mit-restaurant","split":"dev","instance":{"id":"4","prompt_labels":"any(O) good(O) cheap(B-Price) german(B-Cuisine) restaurants(O) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Amenity, Cuisine, Price, Location, Rating, Restaurant Name, Hours and O.\nSentence: any good cheap german restaurants nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["any","good","cheap","german","restaurants","nearby"],"labels":["O","O","B-Price","B-Cuisine","O","B-Location"],"target_index":null,"target_label":null},"label_list":["dish","amenity","cuisine","price","location","rating","restaurant_name","hours"]}
{"id":"6","dataset":"mit-restaurant","split":"dev","instance":{"id":"6","prompt_labels":"any(O) good(B-Rating) place(O) to(O) get(O) a(O) pie(B-Dish) at(O) an(O) affordable(B-Price) price(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Cuisine, Price, Rating, Amenity, Dish, Location and O.\nSentence: any good place to get a pie at an affordable price","prediction_output":null,"prediction_outputs":null,"group":null,"words":["any","good","place","to","get","a","pie","at","an","affordable","price"],"labels":["O","B-Rating","O","O","O","O","B-Dish","O","O","B-Price","O"],"target_index":null,"target_label":null},"label_list":["restaurant_name","hours","cuisine","price","rating","amenity","dish","location"]}
{"id":"11","dataset":"mit-restaurant","split":"dev","instance":{"id":"11","prompt_labels":"any(O) reasonably(B-Price) priced(O) indian(B-Cuisine) restaurants(O) in(B-Location) the(I-Location) theater(I-Location) district(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Rating, Location, Amenity, Price, Restaurant Name, Cuisine, Dish and O.\nSentence: any reasonably priced indian restaurants in the theater district","prediction_output":null,"prediction_outputs":null,"group":null,"words":["any","reasonably","priced","indian","restaurants","in","the","theater","district"],"labels":["O","B-Price","O","B-Cuisine","O","B-Location","I-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["hours","rating","location","amenity","price","restaurant_name","cuisine","dish"]}
{"id":"13","dataset":"mit-restaurant","split":"dev","instance":{"id":"13","prompt_labels":"any(O) restaurants(O) that(O) still(O) allow(O) smoking(B-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Amenity, Price, Dish, Rating, Location, Restaurant Name, Hours and O.\nSentence: any restaurants that still allow smoking","prediction_output":null,"prediction_outputs":null,"group":null,"words":["any","restaurants","that","still","allow","smoking"],"labels":["O","O","O","O","O","B-Amenity"],"target_index":null,"target_label":null},"label_list":["cuisine","amenity","price","dish","rating","location","restaurant_name","hours"]}
{"id":"16","dataset":"mit-restaurant","split":"dev","instance":{"id":"16","prompt_labels":"anything(O) open(B-Hours) after(I-Hours) midnight(I-Hours) with(O) reasonable(B-Price) prices(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Dish, Hours, Location, Price, Cuisine, Amenity, Restaurant Name and O.\nSentence: anything open after midnight with reasonable prices","prediction_output":null,"prediction_outputs":null,"group":null,"words":["anything","open","after","midnight","with","reasonable","prices"],"labels":["O","B-Hours","I-Hours","I-Hours","O","B-Price","O"],"target_index":null,"target_label":null},"label_list":["rating","dish","hours","location","price","cuisine","amenity","restaurant_name"]}
{"id":"17","dataset":"mit-restaurant","split":"dev","instance":{"id":"17","prompt_labels":"are(O) children(B-Amenity) allowed(O) in(O) this(O) particular(O) sitting(B-Amenity) area(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Amenity, Dish, Cuisine, Price, Restaurant Name, Rating, Hours and O.\nSentence: are children allowed in this particular sitting area","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","children","allowed","in","this","particular","sitting","area"],"labels":["O","B-Amenity","O","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["location","amenity","dish","cuisine","price","restaurant_name","rating","hours"]}
{"id":"18","dataset":"mit-restaurant","split":"dev","instance":{"id":"18","prompt_labels":"are(O) reservations(O) available(O) for(O) four(O) people(O) for(O) 8(O) pm(O) tonight(O) at(O) 112(B-Restaurant Name) eatery(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Amenity, Rating, Dish, Location, Cuisine, Price and O.\nSentence: are reservations available for four people for 8 pm tonight at 112 eatery","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","reservations","available","for","four","people","for","8","pm","tonight","at","112","eatery"],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["hours","restaurant_name","amenity","rating","dish","location","cuisine","price"]}
{"id":"19","dataset":"mit-restaurant","split":"dev","instance":{"id":"19","prompt_labels":"are(O) the(O) portion(O) at(O) le(B-Restaurant Name) bec(I-Restaurant Name) fin(I-Restaurant Name) large(O) or(O) very(O) small(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Location, Cuisine, Rating, Restaurant Name, Hours, Dish, Amenity and O.\nSentence: are the portion at le bec fin large or very small","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","the","portion","at","le","bec","fin","large","or","very","small"],"labels":["O","O","O","O","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["price","location","cuisine","rating","restaurant_name","hours","dish","amenity"]}
{"id":"20","dataset":"mit-restaurant","split":"dev","instance":{"id":"20","prompt_labels":"are(O) there(O) any(O) 24(B-Hours) hour(I-Hours) breakfast(B-Cuisine) places(O) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Price, Hours, Rating, Cuisine, Location, Dish, Restaurant Name and O.\nSentence: are there any 24 hour breakfast places nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","24","hour","breakfast","places","nearby"],"labels":["O","O","O","B-Hours","I-Hours","B-Cuisine","O","B-Location"],"target_index":null,"target_label":null},"label_list":["amenity","price","hours","rating","cuisine","location","dish","restaurant_name"]}
{"id":"21","dataset":"mit-restaurant","split":"dev","instance":{"id":"21","prompt_labels":"are(O) there(O) any(O) 50s(B-Amenity) style(I-Amenity) diners(B-Cuisine) in(O) glendale(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Cuisine, Dish, Location, Hours, Restaurant Name, Amenity, Rating and O.\nSentence: are there any 50s style diners in glendale","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","50s","style","diners","in","glendale"],"labels":["O","O","O","B-Amenity","I-Amenity","B-Cuisine","O","B-Location"],"target_index":null,"target_label":null},"label_list":["price","cuisine","dish","location","hours","restaurant_name","amenity","rating"]}
{"id":"25","dataset":"mit-restaurant","split":"dev","instance":{"id":"25","prompt_labels":"are(O) there(O) any(O) cafeterias(B-Cuisine) near(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Price, Location, Hours, Amenity, Dish, Cuisine, Rating and O.\nSentence: are there any cafeterias near","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","cafeterias","near"],"labels":["O","O","O","B-Cuisine","B-Location"],"target_index":null,"target_label":null},"label_list":["restaurant_name","price","location","hours","amenity","dish","cuisine","rating"]}
{"id":"27","dataset":"mit-restaurant","split":"dev","instance":{"id":"27","prompt_labels":"are(O) there(O) any(O) chick(B-Restaurant Name) fil(I-Restaurant Name) as(I-Restaurant Name) in(B-Location) the(I-Location) city(I-Location) open(O) on(O) sunday(B-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Location, Price, Amenity, Dish, Cuisine, Rating and O.\nSentence: are there any chick fil as in the city open on sunday","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","chick","fil","as","in","the","city","open","on","sunday"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name","B-Location","I-Location","I-Location","O","O","B-Hours"],"target_index":null,"target_label":null},"label_list":["restaurant_name","hours","location","price","amenity","dish","cuisine","rating"]}
{"id":"28","dataset":"mit-restaurant","split":"dev","instance":{"id":"28","prompt_labels":"are(O) there(O) any(O) chicken(B-Dish) wing(I-Dish) places(O) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Location, Price, Amenity, Cuisine, Dish, Hours and O.\nSentence: are there any chicken wing places nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","chicken","wing","places","nearby"],"labels":["O","O","O","B-Dish","I-Dish","O","B-Location"],"target_index":null,"target_label":null},"label_list":["rating","restaurant_name","location","price","amenity","cuisine","dish","hours"]}
{"id":"30","dataset":"mit-restaurant","split":"dev","instance":{"id":"30","prompt_labels":"are(O) there(O) any(O) chinese(B-Cuisine) restaurants(O) near(B-Location) cheyenne(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Rating, Hours, Location, Amenity, Dish, Restaurant Name, Cuisine and O.\nSentence: are there any chinese restaurants near cheyenne","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","chinese","restaurants","near","cheyenne"],"labels":["O","O","O","B-Cuisine","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["price","rating","hours","location","amenity","dish","restaurant_name","cuisine"]}
{"id":"32","dataset":"mit-restaurant","split":"dev","instance":{"id":"32","prompt_labels":"are(O) there(O) any(O) dining(O) specials(B-Amenity) at(O) le(B-Restaurant Name) bec(I-Restaurant Name) fin(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Hours, Cuisine, Dish, Price, Restaurant Name, Location, Amenity and O.\nSentence: are there any dining specials at le bec fin","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","dining","specials","at","le","bec","fin"],"labels":["O","O","O","O","B-Amenity","O","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["rating","hours","cuisine","dish","price","restaurant_name","location","amenity"]}
{"id":"36","dataset":"mit-restaurant","split":"dev","instance":{"id":"36","prompt_labels":"are(O) there(O) any(O) fancy(B-Amenity) cambodian(B-Cuisine) places(O) on(O) seaver(B-Location) street(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Location, Price, Rating, Restaurant Name, Dish, Amenity, Hours and O.\nSentence: are there any fancy cambodian places on seaver street","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","fancy","cambodian","places","on","seaver","street"],"labels":["O","O","O","B-Amenity","B-Cuisine","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["cuisine","location","price","rating","restaurant_name","dish","amenity","hours"]}
{"id":"38","dataset":"mit-restaurant","split":"dev","instance":{"id":"38","prompt_labels":"are(O) there(O) any(O) fast(B-Cuisine) food(I-Cuisine) restaurants(O) that(O) are(O) kid(B-Amenity) friendly(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Price, Dish, Restaurant Name, Amenity, Cuisine, Location, Rating and O.\nSentence: are there any fast food restaurants that are kid friendly","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","fast","food","restaurants","that","are","kid","friendly"],"labels":["O","O","O","B-Cuisine","I-Cuisine","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["hours","price","dish","restaurant_name","amenity","cuisine","location","rating"]}
{"id":"39","dataset":"mit-restaurant","split":"dev","instance":{"id":"39","prompt_labels":"are(O) there(O) any(O) fine(B-Amenity) dining(I-Amenity) options(O) within(B-Location) 5(I-Location) miles(I-Location) of(O) my(O) location(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Dish, Hours, Cuisine, Amenity, Rating, Restaurant Name, Price and O.\nSentence: are there any fine dining options within 5 miles of my location","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","fine","dining","options","within","5","miles","of","my","location"],"labels":["O","O","O","B-Amenity","I-Amenity","O","B-Location","I-Location","I-Location","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","dish","hours","cuisine","amenity","rating","restaurant_name","price"]}
{"id":"40","dataset":"mit-restaurant","split":"dev","instance":{"id":"40","prompt_labels":"are(O) there(O) any(O) five(B-Rating) star(I-Rating) restaurants(O) around(B-Location) here(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Hours, Location, Dish, Cuisine, Rating, Price and O.\nSentence: are there any five star restaurants around here","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","five","star","restaurants","around","here"],"labels":["O","O","O","B-Rating","I-Rating","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["restaurant_name","amenity","hours","location","dish","cuisine","rating","price"]}
{"id":"41","dataset":"mit-restaurant","split":"dev","instance":{"id":"41","prompt_labels":"are(O) there(O) any(O) four(B-Rating) star(I-Rating) restaurants(O) in(O) this(O) town(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Location, Amenity, Restaurant Name, Cuisine, Price, Dish, Rating and O.\nSentence: are there any four star restaurants in this town","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","four","star","restaurants","in","this","town"],"labels":["O","O","O","B-Rating","I-Rating","O","O","O","B-Location"],"target_index":null,"target_label":null},"label_list":["hours","location","amenity","restaurant_name","cuisine","price","dish","rating"]}
{"id":"43","dataset":"mit-restaurant","split":"dev","instance":{"id":"43","prompt_labels":"are(O) there(O) any(O) good(O) family(B-Amenity) style(I-Amenity) restaurants(O) around(B-Location) boston(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Price, Cuisine, Hours, Location, Dish, Amenity, Restaurant Name and O.\nSentence: are there any good family style restaurants around boston","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","good","family","style","restaurants","around","boston"],"labels":["O","O","O","O","B-Amenity","I-Amenity","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["rating","price","cuisine","hours","location","dish","amenity","restaurant_name"]}
{"id":"44","dataset":"mit-restaurant","split":"dev","instance":{"id":"44","prompt_labels":"are(O) there(O) any(O) good(B-Rating) soul(B-Cuisine) food(O) restaurants(O) near(B-Location) by(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Restaurant Name, Location, Cuisine, Rating, Hours, Price, Dish and O.\nSentence: are there any good soul food restaurants near by","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","good","soul","food","restaurants","near","by"],"labels":["O","O","O","B-Rating","B-Cuisine","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["amenity","restaurant_name","location","cuisine","rating","hours","price","dish"]}
{"id":"46","dataset":"mit-restaurant","split":"dev","instance":{"id":"46","prompt_labels":"are(O) there(O) any(O) greek(B-Cuisine) restaurants(O) in(O) the(O) theater(B-Location) district(I-Location) of(O) the(O) back(B-Location) bay(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Price, Cuisine, Dish, Rating, Restaurant Name, Amenity, Location and O.\nSentence: are there any greek restaurants in the theater district of the back bay","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","greek","restaurants","in","the","theater","district","of","the","back","bay"],"labels":["O","O","O","B-Cuisine","O","O","O","B-Location","I-Location","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["hours","price","cuisine","dish","rating","restaurant_name","amenity","location"]}
{"id":"47","dataset":"mit-restaurant","split":"dev","instance":{"id":"47","prompt_labels":"are(O) there(O) any(O) hamburger(B-Cuisine) restaurants(O) close(B-Location) by(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Dish, Price, Amenity, Hours, Location, Restaurant Name, Rating and O.\nSentence: are there any hamburger restaurants close by","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","hamburger","restaurants","close","by"],"labels":["O","O","O","B-Cuisine","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["cuisine","dish","price","amenity","hours","location","restaurant_name","rating"]}
{"id":"52","dataset":"mit-restaurant","split":"dev","instance":{"id":"52","prompt_labels":"are(O) there(O) any(O) japanese(B-Cuisine) restaurants(O) in(B-Location) town(I-Location) that(O) do(O) discounts(B-Amenity) for(I-Amenity) bulk(I-Amenity) orders(I-Amenity) of(O) sushi(B-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Restaurant Name, Amenity, Rating, Price, Location, Dish, Hours and O.\nSentence: are there any japanese restaurants in town that do discounts for bulk orders of sushi","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","japanese","restaurants","in","town","that","do","discounts","for","bulk","orders","of","sushi"],"labels":["O","O","O","B-Cuisine","O","B-Location","I-Location","O","O","B-Amenity","I-Amenity","I-Amenity","I-Amenity","O","B-Amenity"],"target_index":null,"target_label":null},"label_list":["cuisine","restaurant_name","amenity","rating","price","location","dish","hours"]}
{"id":"54","dataset":"mit-restaurant","split":"dev","instance":{"id":"54","prompt_labels":"are(O) there(O) any(O) kid(B-Amenity) friendly(I-Amenity) restaurants(O) close(B-Location) by(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Restaurant Name, Hours, Cuisine, Location, Rating, Dish and O.\nSentence: are there any kid friendly restaurants close by","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","kid","friendly","restaurants","close","by"],"labels":["O","O","O","B-Amenity","I-Amenity","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["price","amenity","restaurant_name","hours","cuisine","location","rating","dish"]}
{"id":"56","dataset":"mit-restaurant","split":"dev","instance":{"id":"56","prompt_labels":"are(O) there(O) any(O) locally(B-Amenity) owned(I-Amenity) franchises(O) that(O) give(B-Amenity) money(I-Amenity) to(I-Amenity) charity(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Location, Rating, Dish, Hours, Amenity, Cuisine, Price and O.\nSentence: are there any locally owned franchises that give money to charity","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","locally","owned","franchises","that","give","money","to","charity"],"labels":["O","O","O","B-Amenity","I-Amenity","O","O","B-Amenity","I-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["restaurant_name","location","rating","dish","hours","amenity","cuisine","price"]}
{"id":"57","dataset":"mit-restaurant","split":"dev","instance":{"id":"57","prompt_labels":"are(O) there(O) any(O) maid(B-Restaurant Name) cafe(I-Restaurant Name) in(B-Location) town(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Amenity, Dish, Price, Restaurant Name, Rating, Location, Hours and O.\nSentence: are there any maid cafe in town","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","maid","cafe","in","town"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["cuisine","amenity","dish","price","restaurant_name","rating","location","hours"]}
{"id":"58","dataset":"mit-restaurant","split":"dev","instance":{"id":"58","prompt_labels":"are(O) there(O) any(O) mcdonalds(B-Restaurant Name) that(O) i(O) can(O) drive(O) too(O) in(B-Location) 3(I-Location) minutes(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Dish, Amenity, Location, Cuisine, Hours, Price, Rating and O.\nSentence: are there any mcdonalds that i can drive too in 3 minutes","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","mcdonalds","that","i","can","drive","too","in","3","minutes"],"labels":["O","O","O","B-Restaurant Name","O","O","O","O","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["restaurant_name","dish","amenity","location","cuisine","hours","price","rating"]}
{"id":"59","dataset":"mit-restaurant","split":"dev","instance":{"id":"59","prompt_labels":"are(O) there(O) any(O) mid(B-Price) priced(O) restaurants(O) within(B-Location) 5(I-Location) miles(I-Location) that(O) offer(B-Amenity) curb(I-Amenity) side(I-Amenity) pickup(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Price, Location, Hours, Rating, Cuisine, Dish and O.\nSentence: are there any mid priced restaurants within 5 miles that offer curb side pickup","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","mid","priced","restaurants","within","5","miles","that","offer","curb","side","pickup"],"labels":["O","O","O","B-Price","O","O","B-Location","I-Location","I-Location","O","B-Amenity","I-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["restaurant_name","amenity","price","location","hours","rating","cuisine","dish"]}
{"id":"60","dataset":"mit-restaurant","split":"dev","instance":{"id":"60","prompt_labels":"are(O) there(O) any(O) nationally(B-Rating) known(I-Rating) chefs(O) with(O) restaurants(O) in(B-Location) this(I-Location) city(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Restaurant Name, Location, Dish, Hours, Amenity, Rating, Price and O.\nSentence: are there any nationally known chefs with restaurants in this city","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","nationally","known","chefs","with","restaurants","in","this","city"],"labels":["O","O","O","B-Rating","I-Rating","O","O","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["cuisine","restaurant_name","location","dish","hours","amenity","rating","price"]}
{"id":"64","dataset":"mit-restaurant","split":"dev","instance":{"id":"64","prompt_labels":"are(O) there(O) any(O) places(O) around(B-Location) here(I-Location) that(O) has(O) tomato(B-Dish) sauce(I-Dish) based(I-Dish) dishes(I-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Dish, Restaurant Name, Amenity, Hours, Location, Cuisine, Rating and O.\nSentence: are there any places around here that has tomato sauce based dishes","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","places","around","here","that","has","tomato","sauce","based","dishes"],"labels":["O","O","O","O","B-Location","I-Location","O","O","B-Dish","I-Dish","I-Dish","I-Dish"],"target_index":null,"target_label":null},"label_list":["price","dish","restaurant_name","amenity","hours","location","cuisine","rating"]}
{"id":"66","dataset":"mit-restaurant","split":"dev","instance":{"id":"66","prompt_labels":"are(O) there(O) any(O) places(O) near(B-Location) by(I-Location) that(O) sell(O) hamburgers(B-Dish) and(O) pizza(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Amenity, Location, Dish, Cuisine, Rating, Price and O.\nSentence: are there any places near by that sell hamburgers and pizza","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","places","near","by","that","sell","hamburgers","and","pizza"],"labels":["O","O","O","O","B-Location","I-Location","O","O","B-Dish","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["hours","restaurant_name","amenity","location","dish","cuisine","rating","price"]}
{"id":"71","dataset":"mit-restaurant","split":"dev","instance":{"id":"71","prompt_labels":"are(O) there(O) any(O) restaurant(O) nearby(B-Location) that(O) serve(O) thai(B-Cuisine) food(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Rating, Location, Restaurant Name, Amenity, Hours, Cuisine, Dish and O.\nSentence: are there any restaurant nearby that serve thai food","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","restaurant","nearby","that","serve","thai","food"],"labels":["O","O","O","O","B-Location","O","O","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["price","rating","location","restaurant_name","amenity","hours","cuisine","dish"]}
{"id":"73","dataset":"mit-restaurant","split":"dev","instance":{"id":"73","prompt_labels":"are(O) there(O) any(O) restaurants(O) for(O) diabetics(O) that(O) serve(O) sugar(B-Dish) free(I-Dish) desserts(I-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Price, Hours, Restaurant Name, Cuisine, Location, Dish and O.\nSentence: are there any restaurants for diabetics that serve sugar free desserts","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","restaurants","for","diabetics","that","serve","sugar","free","desserts"],"labels":["O","O","O","O","O","O","O","O","B-Dish","I-Dish","I-Dish"],"target_index":null,"target_label":null},"label_list":["amenity","rating","price","hours","restaurant_name","cuisine","location","dish"]}
{"id":"75","dataset":"mit-restaurant","split":"dev","instance":{"id":"75","prompt_labels":"are(O) there(O) any(O) restaurants(O) nearby(B-Location) that(O) have(O) outdoor(B-Amenity) dining(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Location, Cuisine, Rating, Amenity, Price, Dish, Hours and O.\nSentence: are there any restaurants nearby that have outdoor dining","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","restaurants","nearby","that","have","outdoor","dining"],"labels":["O","O","O","O","B-Location","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["restaurant_name","location","cuisine","rating","amenity","price","dish","hours"]}
{"id":"76","dataset":"mit-restaurant","split":"dev","instance":{"id":"76","prompt_labels":"are(O) there(O) any(O) restaurants(O) on(O) kilmarnock(B-Location) street(I-Location) that(O) feature(O) large(B-Amenity) portions(I-Amenity) and(O) a(O) brewpub(B-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Hours, Price, Amenity, Restaurant Name, Location, Dish and O.\nSentence: are there any restaurants on kilmarnock street that feature large portions and a brewpub","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","restaurants","on","kilmarnock","street","that","feature","large","portions","and","a","brewpub"],"labels":["O","O","O","O","O","B-Location","I-Location","O","O","B-Amenity","I-Amenity","O","O","B-Amenity"],"target_index":null,"target_label":null},"label_list":["cuisine","rating","hours","price","amenity","restaurant_name","location","dish"]}
{"id":"77","dataset":"mit-restaurant","split":"dev","instance":{"id":"77","prompt_labels":"are(O) there(O) any(O) restaurants(O) on(O) the(O) way(B-Location) that(O) serve(O) hamburgers(B-Dish) and(O) are(O) open(B-Hours) after(I-Hours) 1(I-Hours) am(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Rating, Restaurant Name, Hours, Dish, Cuisine, Location and O.\nSentence: are there any restaurants on the way that serve hamburgers and are open after 1 am","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","restaurants","on","the","way","that","serve","hamburgers","and","are","open","after","1","am"],"labels":["O","O","O","O","O","O","B-Location","O","O","B-Dish","O","O","B-Hours","I-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["price","amenity","rating","restaurant_name","hours","dish","cuisine","location"]}
{"id":"78","dataset":"mit-restaurant","split":"dev","instance":{"id":"78","prompt_labels":"are(O) there(O) any(O) restaurants(O) on(O) the(O) way(B-Location) to(I-Location) my(I-Location) destination(I-Location) that(O) have(O) a(O) fireplace(B-Amenity) inside(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Cuisine, Location, Dish, Price, Hours, Amenity and O.\nSentence: are there any restaurants on the way to my destination that have a fireplace inside","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","restaurants","on","the","way","to","my","destination","that","have","a","fireplace","inside"],"labels":["O","O","O","O","O","O","B-Location","I-Location","I-Location","I-Location","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["rating","restaurant_name","cuisine","location","dish","price","hours","amenity"]}
{"id":"79","dataset":"mit-restaurant","split":"dev","instance":{"id":"79","prompt_labels":"are(O) there(O) any(O) restaurants(O) on(O) this(B-Location) side(I-Location) of(I-Location) the(I-Location) river(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Rating, Cuisine, Restaurant Name, Amenity, Price, Hours, Dish and O.\nSentence: are there any restaurants on this side of the river","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","restaurants","on","this","side","of","the","river"],"labels":["O","O","O","O","O","B-Location","I-Location","I-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["location","rating","cuisine","restaurant_name","amenity","price","hours","dish"]}
{"id":"81","dataset":"mit-restaurant","split":"dev","instance":{"id":"81","prompt_labels":"are(O) there(O) any(O) restaurants(O) that(O) are(O) open(O) 24(B-Hours) hours(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Cuisine, Rating, Dish, Price, Amenity, Hours, Restaurant Name and O.\nSentence: are there any restaurants that are open 24 hours","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","restaurants","that","are","open","24","hours"],"labels":["O","O","O","O","O","O","O","B-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["location","cuisine","rating","dish","price","amenity","hours","restaurant_name"]}
{"id":"85","dataset":"mit-restaurant","split":"dev","instance":{"id":"85","prompt_labels":"are(O) there(O) any(O) restaurants(O) with(O) valet(B-Amenity) parking(I-Amenity) and(O) a(O) multilingual(B-Amenity) staff(I-Amenity) near(B-Location) here(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Location, Restaurant Name, Rating, Dish, Amenity, Price, Cuisine and O.\nSentence: are there any restaurants with valet parking and a multilingual staff near here","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","restaurants","with","valet","parking","and","a","multilingual","staff","near","here"],"labels":["O","O","O","O","O","B-Amenity","I-Amenity","O","O","B-Amenity","I-Amenity","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["hours","location","restaurant_name","rating","dish","amenity","price","cuisine"]}
{"id":"87","dataset":"mit-restaurant","split":"dev","instance":{"id":"87","prompt_labels":"are(O) there(O) any(O) rib(B-Cuisine) joints(I-Cuisine) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Amenity, Location, Price, Dish, Cuisine, Rating and O.\nSentence: are there any rib joints nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","rib","joints","nearby"],"labels":["O","O","O","B-Cuisine","I-Cuisine","B-Location"],"target_index":null,"target_label":null},"label_list":["hours","restaurant_name","amenity","location","price","dish","cuisine","rating"]}
{"id":"88","dataset":"mit-restaurant","split":"dev","instance":{"id":"88","prompt_labels":"are(O) there(O) any(O) seafood(B-Cuisine) restaurants(O) near(B-Location) government(I-Location) center(I-Location) where(O) i(O) can(O) make(O) online(B-Location) reservations(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Amenity, Cuisine, Restaurant Name, Price, Rating, Dish, Location and O.\nSentence: are there any seafood restaurants near government center where i can make online reservations","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","seafood","restaurants","near","government","center","where","i","can","make","online","reservations"],"labels":["O","O","O","B-Cuisine","O","B-Location","I-Location","I-Location","O","O","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["hours","amenity","cuisine","restaurant_name","price","rating","dish","location"]}
{"id":"91","dataset":"mit-restaurant","split":"dev","instance":{"id":"91","prompt_labels":"are(O) there(O) any(O) sub(B-Cuisine) sandwich(I-Cuisine) shops(O) that(O) also(O) serve(O) beer(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Price, Rating, Location, Cuisine, Hours, Restaurant Name, Amenity and O.\nSentence: are there any sub sandwich shops that also serve beer","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","sub","sandwich","shops","that","also","serve","beer"],"labels":["O","O","O","B-Cuisine","I-Cuisine","O","O","O","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["dish","price","rating","location","cuisine","hours","restaurant_name","amenity"]}
{"id":"92","dataset":"mit-restaurant","split":"dev","instance":{"id":"92","prompt_labels":"are(O) there(O) any(O) sushi(B-Cuisine) restaurants(O) near(B-Location) colonel(I-Location) bell(I-Location) drive(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Restaurant Name, Hours, Amenity, Rating, Location, Dish, Price and O.\nSentence: are there any sushi restaurants near colonel bell drive","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","sushi","restaurants","near","colonel","bell","drive"],"labels":["O","O","O","B-Cuisine","O","B-Location","I-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["cuisine","restaurant_name","hours","amenity","rating","location","dish","price"]}
{"id":"93","dataset":"mit-restaurant","split":"dev","instance":{"id":"93","prompt_labels":"are(O) there(O) any(O) tapas(B-Dish) restaurants(O) with(O) good(B-Rating) reviews(I-Rating) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Rating, Cuisine, Amenity, Restaurant Name, Dish, Hours, Price and O.\nSentence: are there any tapas restaurants with good reviews nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","tapas","restaurants","with","good","reviews","nearby"],"labels":["O","O","O","B-Dish","O","O","B-Rating","I-Rating","B-Location"],"target_index":null,"target_label":null},"label_list":["location","rating","cuisine","amenity","restaurant_name","dish","hours","price"]}
{"id":"94","dataset":"mit-restaurant","split":"dev","instance":{"id":"94","prompt_labels":"are(O) there(O) any(O) turkish(B-Cuisine) restaurants(O) in(O) florida(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Price, Restaurant Name, Dish, Rating, Cuisine, Location and O.\nSentence: are there any turkish restaurants in florida","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","turkish","restaurants","in","florida"],"labels":["O","O","O","B-Cuisine","O","O","B-Location"],"target_index":null,"target_label":null},"label_list":["amenity","hours","price","restaurant_name","dish","rating","cuisine","location"]}
{"id":"95","dataset":"mit-restaurant","split":"dev","instance":{"id":"95","prompt_labels":"are(O) there(O) any(O) vegan(B-Cuisine) spots(I-Cuisine) that(O) are(O) open(B-Hours) after(I-Hours) 11(I-Hours) at(I-Hours) night(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Rating, Restaurant Name, Cuisine, Hours, Location, Dish and O.\nSentence: are there any vegan spots that are open after 11 at night","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","vegan","spots","that","are","open","after","11","at","night"],"labels":["O","O","O","B-Cuisine","I-Cuisine","O","O","B-Hours","I-Hours","I-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["price","amenity","rating","restaurant_name","cuisine","hours","location","dish"]}
{"id":"96","dataset":"mit-restaurant","split":"dev","instance":{"id":"96","prompt_labels":"are(O) there(O) any(O) vegetarian(B-Cuisine) restaurants(O) in(B-Location) this(I-Location) town(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Price, Cuisine, Restaurant Name, Amenity, Location, Dish, Rating and O.\nSentence: are there any vegetarian restaurants in this town","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","vegetarian","restaurants","in","this","town"],"labels":["O","O","O","B-Cuisine","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["hours","price","cuisine","restaurant_name","amenity","location","dish","rating"]}
{"id":"97","dataset":"mit-restaurant","split":"dev","instance":{"id":"97","prompt_labels":"are(O) there(O) any(O) vegetarian(B-Cuisine) restaurants(O) that(O) allow(O) you(O) to(O) order(B-Amenity) online(I-Amenity) ahead(I-Amenity) of(I-Amenity) time(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Hours, Dish, Location, Price, Rating, Restaurant Name, Amenity and O.\nSentence: are there any vegetarian restaurants that allow you to order online ahead of time","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","vegetarian","restaurants","that","allow","you","to","order","online","ahead","of","time"],"labels":["O","O","O","B-Cuisine","O","O","O","O","O","B-Amenity","I-Amenity","I-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["cuisine","hours","dish","location","price","rating","restaurant_name","amenity"]}
{"id":"98","dataset":"mit-restaurant","split":"dev","instance":{"id":"98","prompt_labels":"are(O) there(O) any(O) vietnamese(B-Cuisine) restaurants(O) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Rating, Dish, Restaurant Name, Location, Amenity, Price and O.\nSentence: are there any vietnamese restaurants nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","vietnamese","restaurants","nearby"],"labels":["O","O","O","B-Cuisine","O","B-Location"],"target_index":null,"target_label":null},"label_list":["hours","cuisine","rating","dish","restaurant_name","location","amenity","price"]}
{"id":"99","dataset":"mit-restaurant","split":"dev","instance":{"id":"99","prompt_labels":"are(O) there(O) are(O) any(O) cracker(B-Restaurant Name) barrells(I-Restaurant Name) on(O) long(B-Location) island(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Rating, Hours, Location, Cuisine, Price, Restaurant Name, Amenity and O.\nSentence: are there are any cracker barrells on long island","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","are","any","cracker","barrells","on","long","island"],"labels":["O","O","O","O","B-Restaurant Name","I-Restaurant Name","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["dish","rating","hours","location","cuisine","price","restaurant_name","amenity"]}
{"id":"100","dataset":"mit-restaurant","split":"dev","instance":{"id":"100","prompt_labels":"are(O) there(O) reservations(O) still(O) available(O) for(O) bar(B-Restaurant Name) la(I-Restaurant Name) grassa(I-Restaurant Name) for(O) 2(O) tomorrow(B-Hours) at(I-Hours) 7(I-Hours) pm(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Amenity, Restaurant Name, Rating, Dish, Location, Price, Cuisine and O.\nSentence: are there reservations still available for bar la grassa for 2 tomorrow at 7 pm","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","reservations","still","available","for","bar","la","grassa","for","2","tomorrow","at","7","pm"],"labels":["O","O","O","O","O","O","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name","O","O","B-Hours","I-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["hours","amenity","restaurant_name","rating","dish","location","price","cuisine"]}
{"id":"101","dataset":"mit-restaurant","split":"dev","instance":{"id":"101","prompt_labels":"areas(O) that(O) allow(B-Amenity) smoking(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Cuisine, Rating, Dish, Amenity, Hours, Location, Price and O.\nSentence: areas that allow smoking","prediction_output":null,"prediction_outputs":null,"group":null,"words":["areas","that","allow","smoking"],"labels":["O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["restaurant_name","cuisine","rating","dish","amenity","hours","location","price"]}
{"id":"103","dataset":"mit-restaurant","split":"dev","instance":{"id":"103","prompt_labels":"at(O) which(O) french(B-Cuisine) restaurant(O) can(O) i(O) dine(B-Amenity) outdoors(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Cuisine, Amenity, Location, Dish, Price, Rating and O.\nSentence: at which french restaurant can i dine outdoors","prediction_output":null,"prediction_outputs":null,"group":null,"words":["at","which","french","restaurant","can","i","dine","outdoors"],"labels":["O","O","B-Cuisine","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["restaurant_name","hours","cuisine","amenity","location","dish","price","rating"]}
{"id":"106","dataset":"mit-restaurant","split":"dev","instance":{"id":"106","prompt_labels":"borscht(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Restaurant Name, Amenity, Dish, Location, Price, Rating, Hours and O.\nSentence: borscht","prediction_output":null,"prediction_outputs":null,"group":null,"words":["borscht"],"labels":["B-Dish"],"target_index":null,"target_label":null},"label_list":["cuisine","restaurant_name","amenity","dish","location","price","rating","hours"]}
{"id":"108","dataset":"mit-restaurant","split":"dev","instance":{"id":"108","prompt_labels":"brasil(B-Cuisine) cuisine(O) near(B-Location) me(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Rating, Amenity, Location, Restaurant Name, Dish, Cuisine, Price and O.\nSentence: brasil cuisine near me","prediction_output":null,"prediction_outputs":null,"group":null,"words":["brasil","cuisine","near","me"],"labels":["B-Cuisine","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["hours","rating","amenity","location","restaurant_name","dish","cuisine","price"]}
{"id":"109","dataset":"mit-restaurant","split":"dev","instance":{"id":"109","prompt_labels":"burgers(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Location, Amenity, Price, Rating, Cuisine, Dish, Restaurant Name and O.\nSentence: burgers","prediction_output":null,"prediction_outputs":null,"group":null,"words":["burgers"],"labels":["B-Dish"],"target_index":null,"target_label":null},"label_list":["hours","location","amenity","price","rating","cuisine","dish","restaurant_name"]}
{"id":"110","dataset":"mit-restaurant","split":"dev","instance":{"id":"110","prompt_labels":"cafes(B-Cuisine) on(O) ashlannd(B-Location) street(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Dish, Hours, Amenity, Restaurant Name, Cuisine, Price, Rating and O.\nSentence: cafes on ashlannd street","prediction_output":null,"prediction_outputs":null,"group":null,"words":["cafes","on","ashlannd","street"],"labels":["B-Cuisine","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["location","dish","hours","amenity","restaurant_name","cuisine","price","rating"]}
{"id":"112","dataset":"mit-restaurant","split":"dev","instance":{"id":"112","prompt_labels":"call(O) chinese(B-Cuisine)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Restaurant Name, Price, Rating, Hours, Amenity, Location, Dish and O.\nSentence: call chinese","prediction_output":null,"prediction_outputs":null,"group":null,"words":["call","chinese"],"labels":["O","B-Cuisine"],"target_index":null,"target_label":null},"label_list":["cuisine","restaurant_name","price","rating","hours","amenity","location","dish"]}
{"id":"113","dataset":"mit-restaurant","split":"dev","instance":{"id":"113","prompt_labels":"call(O) dominos(B-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Cuisine, Hours, Restaurant Name, Rating, Dish, Amenity, Location and O.\nSentence: call dominos","prediction_output":null,"prediction_outputs":null,"group":null,"words":["call","dominos"],"labels":["O","B-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["price","cuisine","hours","restaurant_name","rating","dish","amenity","location"]}
{"id":"115","dataset":"mit-restaurant","split":"dev","instance":{"id":"115","prompt_labels":"can(O) i(O) bring(O) my(B-Amenity) kid(I-Amenity) to(O) any(O) of(O) the(O) restaurants(O) down(B-Location) town(I-Location) that(O) have(O) bars(B-Amenity) attached(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Cuisine, Restaurant Name, Price, Hours, Amenity, Dish, Location and O.\nSentence: can i bring my kid to any of the restaurants down town that have bars attached","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","i","bring","my","kid","to","any","of","the","restaurants","down","town","that","have","bars","attached"],"labels":["O","O","O","B-Amenity","I-Amenity","O","O","O","O","O","B-Location","I-Location","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["rating","cuisine","restaurant_name","price","hours","amenity","dish","location"]}
{"id":"118","dataset":"mit-restaurant","split":"dev","instance":{"id":"118","prompt_labels":"can(O) i(O) find(O) a(O) bar(B-Cuisine) and(I-Cuisine) grill(I-Cuisine) within(O) short(B-Location) walking(I-Location) distance(I-Location) of(I-Location) the(I-Location) shopping(I-Location) district(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Restaurant Name, Cuisine, Rating, Amenity, Hours, Price, Dish and O.\nSentence: can i find a bar and grill within short walking distance of the shopping district","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","i","find","a","bar","and","grill","within","short","walking","distance","of","the","shopping","district"],"labels":["O","O","O","O","B-Cuisine","I-Cuisine","I-Cuisine","O","B-Location","I-Location","I-Location","I-Location","I-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["location","restaurant_name","cuisine","rating","amenity","hours","price","dish"]}
{"id":"119","dataset":"mit-restaurant","split":"dev","instance":{"id":"119","prompt_labels":"can(O) i(O) find(O) a(O) good(B-Rating) chinese(B-Cuisine) buffet(B-Amenity) within(B-Location) 3(I-Location) miles(I-Location) from(O) me(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Location, Amenity, Restaurant Name, Rating, Hours, Dish and O.\nSentence: can i find a good chinese buffet within 3 miles from me","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","i","find","a","good","chinese","buffet","within","3","miles","from","me"],"labels":["O","O","O","O","B-Rating","B-Cuisine","B-Amenity","B-Location","I-Location","I-Location","O","O"],"target_index":null,"target_label":null},"label_list":["cuisine","price","location","amenity","restaurant_name","rating","hours","dish"]}
{"id":"123","dataset":"mit-restaurant","split":"dev","instance":{"id":"123","prompt_labels":"can(O) i(O) get(O) a(O) list(O) of(O) close(B-Location) fast(B-Cuisine) food(I-Cuisine) places(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Rating, Location, Price, Amenity, Cuisine, Hours, Dish and O.\nSentence: can i get a list of close fast food places","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","i","get","a","list","of","close","fast","food","places"],"labels":["O","O","O","O","O","O","B-Location","B-Cuisine","I-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["restaurant_name","rating","location","price","amenity","cuisine","hours","dish"]}
{"id":"125","dataset":"mit-restaurant","split":"dev","instance":{"id":"125","prompt_labels":"can(O) i(O) get(O) hambers(B-Dish) at(O) lone(B-Restaurant Name) star(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Dish, Cuisine, Location, Rating, Price, Restaurant Name and O.\nSentence: can i get hambers at lone star","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","i","get","hambers","at","lone","star"],"labels":["O","O","O","B-Dish","O","B-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["amenity","hours","dish","cuisine","location","rating","price","restaurant_name"]}
{"id":"126","dataset":"mit-restaurant","split":"dev","instance":{"id":"126","prompt_labels":"can(O) i(O) get(O) raw(B-Cuisine) vegan(I-Cuisine) food(O) in(O) honolulu(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Restaurant Name, Rating, Location, Cuisine, Price, Amenity, Hours and O.\nSentence: can i get raw vegan food in honolulu","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","i","get","raw","vegan","food","in","honolulu"],"labels":["O","O","O","B-Cuisine","I-Cuisine","O","O","B-Location"],"target_index":null,"target_label":null},"label_list":["dish","restaurant_name","rating","location","cuisine","price","amenity","hours"]}
{"id":"127","dataset":"mit-restaurant","split":"dev","instance":{"id":"127","prompt_labels":"can(O) i(O) get(O) sushi(B-Dish) on(O) a(O) prix(B-Amenity) fixe(I-Amenity) menu(I-Amenity) with(O) reasonable(B-Price) prices(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Restaurant Name, Hours, Location, Price, Dish, Cuisine and O.\nSentence: can i get sushi on a prix fixe menu with reasonable prices","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","i","get","sushi","on","a","prix","fixe","menu","with","reasonable","prices"],"labels":["O","O","O","B-Dish","O","O","B-Amenity","I-Amenity","I-Amenity","O","B-Price","O"],"target_index":null,"target_label":null},"label_list":["rating","amenity","restaurant_name","hours","location","price","dish","cuisine"]}
{"id":"128","dataset":"mit-restaurant","split":"dev","instance":{"id":"128","prompt_labels":"can(O) i(O) have(O) the(O) phone(O) number(O) for(O) kfc(B-Restaurant Name) in(O) los(B-Location) angeles(I-Location) ca(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Amenity, Hours, Cuisine, Dish, Restaurant Name, Rating, Price and O.\nSentence: can i have the phone number for kfc in los angeles ca","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","i","have","the","phone","number","for","kfc","in","los","angeles","ca"],"labels":["O","O","O","O","O","O","O","B-Restaurant Name","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["location","amenity","hours","cuisine","dish","restaurant_name","rating","price"]}
{"id":"130","dataset":"mit-restaurant","split":"dev","instance":{"id":"130","prompt_labels":"can(O) i(O) valet(B-Amenity) park(I-Amenity) at(O) the(O) blue(B-Restaurant Name) coyote(I-Restaurant Name) grill(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Dish, Hours, Amenity, Cuisine, Price, Restaurant Name, Rating and O.\nSentence: can i valet park at the blue coyote grill","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","i","valet","park","at","the","blue","coyote","grill"],"labels":["O","O","B-Amenity","I-Amenity","O","O","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["location","dish","hours","amenity","cuisine","price","restaurant_name","rating"]}
{"id":"131","dataset":"mit-restaurant","split":"dev","instance":{"id":"131","prompt_labels":"can(O) i(O) wear(B-Amenity) shorts(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Rating, Location, Cuisine, Hours, Dish, Amenity, Price and O.\nSentence: can i wear shorts","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","i","wear","shorts"],"labels":["O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["restaurant_name","rating","location","cuisine","hours","dish","amenity","price"]}
{"id":"133","dataset":"mit-restaurant","split":"dev","instance":{"id":"133","prompt_labels":"can(O) you(O) find(O) a(O) burger(B-Cuisine) joint(I-Cuisine) with(O) a(O) smoking(B-Amenity) section(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Location, Hours, Cuisine, Rating, Price, Dish, Amenity and O.\nSentence: can you find a burger joint with a smoking section","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","a","burger","joint","with","a","smoking","section"],"labels":["O","O","O","O","B-Cuisine","I-Cuisine","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["restaurant_name","location","hours","cuisine","rating","price","dish","amenity"]}
{"id":"134","dataset":"mit-restaurant","split":"dev","instance":{"id":"134","prompt_labels":"can(O) you(O) find(O) a(O) cheap(B-Price) vietnamese(B-Cuisine) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Amenity, Hours, Location, Price, Cuisine, Dish and O.\nSentence: can you find a cheap vietnamese restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","a","cheap","vietnamese","restaurant"],"labels":["O","O","O","O","B-Price","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["rating","restaurant_name","amenity","hours","location","price","cuisine","dish"]}
{"id":"135","dataset":"mit-restaurant","split":"dev","instance":{"id":"135","prompt_labels":"can(O) you(O) find(O) a(O) fast(B-Cuisine) food(I-Cuisine) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Dish, Hours, Location, Cuisine, Restaurant Name, Price, Amenity and O.\nSentence: can you find a fast food restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","a","fast","food","restaurant"],"labels":["O","O","O","O","B-Cuisine","I-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["rating","dish","hours","location","cuisine","restaurant_name","price","amenity"]}
{"id":"136","dataset":"mit-restaurant","split":"dev","instance":{"id":"136","prompt_labels":"can(O) you(O) find(O) a(O) highly(B-Rating) rated(I-Rating) long(B-Restaurant Name) john(I-Restaurant Name) silvers(I-Restaurant Name) open(B-Hours) this(I-Hours) late(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Price, Amenity, Restaurant Name, Location, Rating, Dish and O.\nSentence: can you find a highly rated long john silvers open this late","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","a","highly","rated","long","john","silvers","open","this","late"],"labels":["O","O","O","O","B-Rating","I-Rating","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name","B-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["hours","cuisine","price","amenity","restaurant_name","location","rating","dish"]}
{"id":"141","dataset":"mit-restaurant","split":"dev","instance":{"id":"141","prompt_labels":"can(O) you(O) find(O) a(O) restaurant(O) that(O) serves(O) duck(B-Dish) that(O) not(B-Price) cheap(I-Price) near(B-Location) here(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Price, Location, Cuisine, Restaurant Name, Hours, Amenity, Dish and O.\nSentence: can you find a restaurant that serves duck that not cheap near here","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","a","restaurant","that","serves","duck","that","not","cheap","near","here"],"labels":["O","O","O","O","O","O","O","B-Dish","O","B-Price","I-Price","B-Location","O"],"target_index":null,"target_label":null},"label_list":["rating","price","location","cuisine","restaurant_name","hours","amenity","dish"]}
{"id":"144","dataset":"mit-restaurant","split":"dev","instance":{"id":"144","prompt_labels":"can(O) you(O) find(O) a(O) site(O) where(O) i(O) can(O) see(O) reviews(O) on(O) restaurant(O) downtown(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Restaurant Name, Price, Hours, Amenity, Rating, Dish, Cuisine and O.\nSentence: can you find a site where i can see reviews on restaurant downtown","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","a","site","where","i","can","see","reviews","on","restaurant","downtown"],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-Location"],"target_index":null,"target_label":null},"label_list":["location","restaurant_name","price","hours","amenity","rating","dish","cuisine"]}
{"id":"146","dataset":"mit-restaurant","split":"dev","instance":{"id":"146","prompt_labels":"can(O) you(O) find(O) a(O) thai(B-Cuisine) japanese(I-Cuisine) fusion(I-Cuisine) restaurant(O) in(B-Location) town(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Hours, Location, Amenity, Price, Dish, Cuisine and O.\nSentence: can you find a thai japanese fusion restaurant in town","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","a","thai","japanese","fusion","restaurant","in","town"],"labels":["O","O","O","O","B-Cuisine","I-Cuisine","I-Cuisine","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["rating","restaurant_name","hours","location","amenity","price","dish","cuisine"]}
{"id":"147","dataset":"mit-restaurant","split":"dev","instance":{"id":"147","prompt_labels":"can(O) you(O) find(O) an(O) italian(B-Cuisine) restaurant(O) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Dish, Rating, Location, Hours, Amenity, Cuisine, Restaurant Name and O.\nSentence: can you find an italian restaurant nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","an","italian","restaurant","nearby"],"labels":["O","O","O","O","B-Cuisine","O","B-Location"],"target_index":null,"target_label":null},"label_list":["price","dish","rating","location","hours","amenity","cuisine","restaurant_name"]}
{"id":"150","dataset":"mit-restaurant","split":"dev","instance":{"id":"150","prompt_labels":"can(O) you(O) find(O) east(B-Restaurant Name) dedham(B-Location) pizzeria(B-Cuisine) that(O) have(O) a(O) dine(O) at(O) bar(O) location(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Cuisine, Hours, Amenity, Rating, Price, Restaurant Name, Location and O.\nSentence: can you find east dedham pizzeria that have a dine at bar location","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","east","dedham","pizzeria","that","have","a","dine","at","bar","location"],"labels":["O","O","O","B-Restaurant Name","B-Location","B-Cuisine","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["dish","cuisine","hours","amenity","rating","price","restaurant_name","location"]}
{"id":"155","dataset":"mit-restaurant","split":"dev","instance":{"id":"155","prompt_labels":"can(O) you(O) find(O) me(O) a(O) kid(B-Amenity) friendly(I-Amenity) seafood(B-Cuisine) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Dish, Cuisine, Amenity, Hours, Rating, Price, Restaurant Name and O.\nSentence: can you find me a kid friendly seafood restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","me","a","kid","friendly","seafood","restaurant"],"labels":["O","O","O","O","O","B-Amenity","I-Amenity","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["location","dish","cuisine","amenity","hours","rating","price","restaurant_name"]}
{"id":"159","dataset":"mit-restaurant","split":"dev","instance":{"id":"159","prompt_labels":"can(O) you(O) find(O) me(O) a(O) not(B-Price) cheap(I-Price) lunch(B-Hours) spot(O) that(O) serves(O) pasteur(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Restaurant Name, Dish, Rating, Hours, Cuisine, Location and O.\nSentence: can you find me a not cheap lunch spot that serves pasteur","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","me","a","not","cheap","lunch","spot","that","serves","pasteur"],"labels":["O","O","O","O","O","B-Price","I-Price","B-Hours","O","O","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["price","amenity","restaurant_name","dish","rating","hours","cuisine","location"]}
{"id":"165","dataset":"mit-restaurant","split":"dev","instance":{"id":"165","prompt_labels":"can(O) you(O) find(O) me(O) a(O) restaurant(O) that(O) has(O) entrees(O) priced(O) between(B-Price) 15(I-Price) and(I-Price) 20(I-Price) dollars(I-Price)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Restaurant Name, Rating, Cuisine, Price, Location, Hours and O.\nSentence: can you find me a restaurant that has entrees priced between 15 and 20 dollars","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","me","a","restaurant","that","has","entrees","priced","between","15","and","20","dollars"],"labels":["O","O","O","O","O","O","O","O","O","O","B-Price","I-Price","I-Price","I-Price","I-Price"],"target_index":null,"target_label":null},"label_list":["amenity","dish","restaurant_name","rating","cuisine","price","location","hours"]}
{"id":"169","dataset":"mit-restaurant","split":"dev","instance":{"id":"169","prompt_labels":"can(O) you(O) find(O) me(O) some(O) malaysian(B-Cuisine) food(O) late(B-Hours) at(I-Hours) night(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Amenity, Cuisine, Rating, Dish, Restaurant Name, Location, Price and O.\nSentence: can you find me some malaysian food late at night","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","me","some","malaysian","food","late","at","night"],"labels":["O","O","O","O","O","B-Cuisine","O","B-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["hours","amenity","cuisine","rating","dish","restaurant_name","location","price"]}
{"id":"170","dataset":"mit-restaurant","split":"dev","instance":{"id":"170","prompt_labels":"can(O) you(O) find(O) me(O) some(O) take(B-Amenity) out(I-Amenity) ribs(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Cuisine, Restaurant Name, Hours, Rating, Price, Location, Amenity and O.\nSentence: can you find me some take out ribs","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","me","some","take","out","ribs"],"labels":["O","O","O","O","O","B-Amenity","I-Amenity","B-Dish"],"target_index":null,"target_label":null},"label_list":["dish","cuisine","restaurant_name","hours","rating","price","location","amenity"]}
{"id":"171","dataset":"mit-restaurant","split":"dev","instance":{"id":"171","prompt_labels":"can(O) you(O) find(O) out(O) if(O) the(O) best(B-Restaurant Name) little(I-Restaurant Name) restaurant(I-Restaurant Name) has(O) dancing(B-Amenity) and(O) a(O) good(B-Amenity) looking(I-Amenity) atmosphere(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Price, Restaurant Name, Dish, Cuisine, Location, Hours and O.\nSentence: can you find out if the best little restaurant has dancing and a good looking atmosphere","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","out","if","the","best","little","restaurant","has","dancing","and","a","good","looking","atmosphere"],"labels":["O","O","O","O","O","O","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name","O","B-Amenity","O","O","B-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["rating","amenity","price","restaurant_name","dish","cuisine","location","hours"]}
{"id":"174","dataset":"mit-restaurant","split":"dev","instance":{"id":"174","prompt_labels":"can(O) you(O) find(O) the(O) nearest(B-Location) health(B-Cuisine) food(I-Cuisine) store(I-Cuisine) and(O) bar(B-Cuisine)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Hours, Cuisine, Amenity, Dish, Price, Restaurant Name, Location and O.\nSentence: can you find the nearest health food store and bar","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","the","nearest","health","food","store","and","bar"],"labels":["O","O","O","O","B-Location","B-Cuisine","I-Cuisine","I-Cuisine","O","B-Cuisine"],"target_index":null,"target_label":null},"label_list":["rating","hours","cuisine","amenity","dish","price","restaurant_name","location"]}
{"id":"175","dataset":"mit-restaurant","split":"dev","instance":{"id":"175","prompt_labels":"can(O) you(O) find(O) the(O) waterfront(B-Location) restaurant(O) albertos(B-Restaurant Name) deli(I-Restaurant Name) of(O) course(O) thats(O) open(B-Hours) until(I-Hours) 11(I-Hours) pm(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Price, Rating, Cuisine, Dish, Location, Amenity, Restaurant Name and O.\nSentence: can you find the waterfront restaurant albertos deli of course thats open until 11 pm","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","the","waterfront","restaurant","albertos","deli","of","course","thats","open","until","11","pm"],"labels":["O","O","O","O","B-Location","O","B-Restaurant Name","I-Restaurant Name","O","O","O","B-Hours","I-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["hours","price","rating","cuisine","dish","location","amenity","restaurant_name"]}
{"id":"176","dataset":"mit-restaurant","split":"dev","instance":{"id":"176","prompt_labels":"can(O) you(O) find(O) us(O) an(O) ice(B-Cuisine) cream(I-Cuisine) shop(O) near(B-Location) haight(I-Location) street(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Price, Cuisine, Dish, Amenity, Location, Restaurant Name, Rating and O.\nSentence: can you find us an ice cream shop near haight street","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","us","an","ice","cream","shop","near","haight","street"],"labels":["O","O","O","O","O","B-Cuisine","I-Cuisine","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["hours","price","cuisine","dish","amenity","location","restaurant_name","rating"]}
{"id":"177","dataset":"mit-restaurant","split":"dev","instance":{"id":"177","prompt_labels":"can(O) you(O) get(O) me(O) a(O) list(O) of(O) chinese(B-Cuisine) food(O) places(O) in(O) great(B-Location) neck(I-Location) ny(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Cuisine, Dish, Restaurant Name, Hours, Amenity, Rating, Price and O.\nSentence: can you get me a list of chinese food places in great neck ny","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","get","me","a","list","of","chinese","food","places","in","great","neck","ny"],"labels":["O","O","O","O","O","O","O","B-Cuisine","O","O","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["location","cuisine","dish","restaurant_name","hours","amenity","rating","price"]}
{"id":"178","dataset":"mit-restaurant","split":"dev","instance":{"id":"178","prompt_labels":"can(O) you(O) get(O) pork(B-Dish) in(O) chinatown(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Rating, Cuisine, Location, Restaurant Name, Amenity, Dish, Price and O.\nSentence: can you get pork in chinatown","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","get","pork","in","chinatown"],"labels":["O","O","O","B-Dish","O","B-Location"],"target_index":null,"target_label":null},"label_list":["hours","rating","cuisine","location","restaurant_name","amenity","dish","price"]}
{"id":"179","dataset":"mit-restaurant","split":"dev","instance":{"id":"179","prompt_labels":"can(O) you(O) give(O) me(O) the(O) name(B-Restaurant Name) of(O) the(O) restaurant(O) on(O) green(B-Location) st(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Location, Hours, Restaurant Name, Dish, Price, Cuisine and O.\nSentence: can you give me the name of the restaurant on green st","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","give","me","the","name","of","the","restaurant","on","green","st"],"labels":["O","O","O","O","O","B-Restaurant Name","O","O","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["amenity","rating","location","hours","restaurant_name","dish","price","cuisine"]}
{"id":"181","dataset":"mit-restaurant","split":"dev","instance":{"id":"181","prompt_labels":"can(O) you(O) help(O) me(O) find(O) a(O) fancy(B-Amenity) restaurant(O) with(O) 5(B-Rating) star(I-Rating) ratings(I-Rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Amenity, Hours, Cuisine, Dish, Restaurant Name, Rating, Price and O.\nSentence: can you help me find a fancy restaurant with 5 star ratings","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","help","me","find","a","fancy","restaurant","with","5","star","ratings"],"labels":["O","O","O","O","O","O","B-Amenity","O","O","B-Rating","I-Rating","I-Rating"],"target_index":null,"target_label":null},"label_list":["location","amenity","hours","cuisine","dish","restaurant_name","rating","price"]}
{"id":"182","dataset":"mit-restaurant","split":"dev","instance":{"id":"182","prompt_labels":"can(O) you(O) help(O) me(O) find(O) a(O) high(B-Price) end(I-Price) restaurant(O) where(O) i(O) can(O) have(O) lunch(B-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Rating, Cuisine, Restaurant Name, Price, Dish, Amenity, Location and O.\nSentence: can you help me find a high end restaurant where i can have lunch","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","help","me","find","a","high","end","restaurant","where","i","can","have","lunch"],"labels":["O","O","O","O","O","O","B-Price","I-Price","O","O","O","O","O","B-Amenity"],"target_index":null,"target_label":null},"label_list":["hours","rating","cuisine","restaurant_name","price","dish","amenity","location"]}
{"id":"191","dataset":"mit-restaurant","split":"dev","instance":{"id":"191","prompt_labels":"can(O) you(O) locate(O) a(O) diner(B-Cuisine) that(O) has(O) a(O) smoking(B-Amenity) section(I-Amenity) in(O) this(B-Location) area(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Hours, Rating, Location, Cuisine, Dish, Restaurant Name and O.\nSentence: can you locate a diner that has a smoking section in this area","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","locate","a","diner","that","has","a","smoking","section","in","this","area"],"labels":["O","O","O","O","B-Cuisine","O","O","O","B-Amenity","I-Amenity","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["price","amenity","hours","rating","location","cuisine","dish","restaurant_name"]}
{"id":"192","dataset":"mit-restaurant","split":"dev","instance":{"id":"192","prompt_labels":"can(O) you(O) locate(O) a(O) restaurant(O) that(O) sell(O) burgers(B-Dish) after(O) 12(B-Hours) 00(I-Hours) am(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Price, Dish, Cuisine, Hours, Location, Restaurant Name and O.\nSentence: can you locate a restaurant that sell burgers after 12 00 am","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","locate","a","restaurant","that","sell","burgers","after","12","00","am"],"labels":["O","O","O","O","O","O","O","B-Dish","O","B-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["rating","amenity","price","dish","cuisine","hours","location","restaurant_name"]}
{"id":"198","dataset":"mit-restaurant","split":"dev","instance":{"id":"198","prompt_labels":"can(O) you(O) make(O) me(O) a(O) reservation(B-Cuisine) for(O) todai(B-Cuisine) for(O) thursday(B-Restaurant Name) for(O) two(O) people(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Cuisine, Rating, Amenity, Hours, Price, Restaurant Name and O.\nSentence: can you make me a reservation for todai for thursday for two people","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","make","me","a","reservation","for","todai","for","thursday","for","two","people"],"labels":["O","O","O","O","O","B-Cuisine","O","B-Cuisine","O","B-Restaurant Name","O","O","O"],"target_index":null,"target_label":null},"label_list":["dish","location","cuisine","rating","amenity","hours","price","restaurant_name"]}
{"id":"199","dataset":"mit-restaurant","split":"dev","instance":{"id":"199","prompt_labels":"can(O) you(O) make(O) reservations(O) for(O) two(O) at(O) heartland(B-Restaurant Name) restaurant(I-Restaurant Name) for(O) tonight(B-Hours) at(I-Hours) 7(I-Hours) 30(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Cuisine, Restaurant Name, Rating, Amenity, Location, Dish, Hours and O.\nSentence: can you make reservations for two at heartland restaurant for tonight at 7 30","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","make","reservations","for","two","at","heartland","restaurant","for","tonight","at","7","30"],"labels":["O","O","O","O","O","O","O","B-Restaurant Name","I-Restaurant Name","O","B-Hours","I-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["price","cuisine","restaurant_name","rating","amenity","location","dish","hours"]}
{"id":"1","dataset":"crossner_ai","split":"dev","instance":{"id":"1","prompt_labels":"From(O) this(O) perspective(O) ,(O) SVM(B-algorithm) is(O) closely(O) related(O) to(O) other(O) fundamental(O) classification(O) algorithms(O) such(O) as(O) regularized(B-algorithm) least-squares(I-algorithm) logistic(B-algorithm) regression(I-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, researcher, metric, programming language, person, conference, product, algorithm, organization, field, location, country, university and O.\nSentence: From this perspective , SVM is closely related to other fundamental classification algorithms such as regularized least-squares logistic regression .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["From","this","perspective",",","SVM","is","closely","related","to","other","fundamental","classification","algorithms","such","as","regularized","least-squares","logistic","regression","."],"labels":["O","O","O","O","B-algorithm","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","B-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["task","researcher","metric","programming_language","person","conference","product","algorithm","organization","field","location","country","university"]}
{"id":"2","dataset":"crossner_ai","split":"dev","instance":{"id":"2","prompt_labels":"Brion(B-person) James(I-person) portrays(O) Leon(B-person) Kowalski(I-person) ,(O) a(O) combat(O) and(O) laborer(O) replicant(O) ,(O) and(O) Joanna(B-person) Cassidy(I-person) portrays(O) Zhora(B-person) ,(O) an(O) assassin(O) replicant(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, programming language, metric, field, location, product, algorithm, task, conference, researcher, university, person and O.\nSentence: Brion James portrays Leon Kowalski , a combat and laborer replicant , and Joanna Cassidy portrays Zhora , an assassin replicant .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Brion","James","portrays","Leon","Kowalski",",","a","combat","and","laborer","replicant",",","and","Joanna","Cassidy","portrays","Zhora",",","an","assassin","replicant","."],"labels":["B-person","I-person","O","B-person","I-person","O","O","O","O","O","O","O","O","B-person","I-person","O","B-person","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","country","programming_language","metric","field","location","product","algorithm","task","conference","researcher","university","person"]}
{"id":"3","dataset":"crossner_ai","split":"dev","instance":{"id":"3","prompt_labels":"The(O) first(O) picture(O) to(O) be(O) scanned(O) ,(O) stored(O) ,(O) and(O) recreated(O) in(O) digital(O) pixels(O) was(O) displayed(O) on(O) the(O) Standards(B-product) Eastern(I-product) Automatic(I-product) Computer(I-product) ((O) SEAC(B-product) )(O) at(O) NIST(B-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, field, algorithm, task, country, university, product, researcher, location, person, organization, conference, metric and O.\nSentence: The first picture to be scanned , stored , and recreated in digital pixels was displayed on the Standards Eastern Automatic Computer ( SEAC ) at NIST .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","first","picture","to","be","scanned",",","stored",",","and","recreated","in","digital","pixels","was","displayed","on","the","Standards","Eastern","Automatic","Computer","(","SEAC",")","at","NIST","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","I-product","I-product","I-product","O","B-product","O","O","B-organization","O"],"target_index":null,"target_label":null},"label_list":["programming_language","field","algorithm","task","country","university","product","researcher","location","person","organization","conference","metric"]}
{"id":"7","dataset":"crossner_ai","split":"dev","instance":{"id":"7","prompt_labels":"Since(O) the(O) Google(B-organization) acquisition(O) ,(O) the(O) company(O) has(O) notched(O) up(O) a(O) number(O) of(O) significant(O) achievements(O) ,(O) perhaps(O) the(O) most(O) notable(O) being(O) the(O) creation(O) of(O) AlphaGo(B-product) ,(O) a(O) program(O) that(O) defeated(O) world(O) champion(O) Lee(B-person) Sedol(I-person) at(O) the(O) complex(O) game(O) of(O) Go(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, field, university, task, metric, algorithm, programming language, product, conference, person, researcher, location, country and O.\nSentence: Since the Google acquisition , the company has notched up a number of significant achievements , perhaps the most notable being the creation of AlphaGo , a program that defeated world champion Lee Sedol at the complex game of Go .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Since","the","Google","acquisition",",","the","company","has","notched","up","a","number","of","significant","achievements",",","perhaps","the","most","notable","being","the","creation","of","AlphaGo",",","a","program","that","defeated","world","champion","Lee","Sedol","at","the","complex","game","of","Go","."],"labels":["O","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","field","university","task","metric","algorithm","programming_language","product","conference","person","researcher","location","country"]}
{"id":"9","dataset":"crossner_ai","split":"dev","instance":{"id":"9","prompt_labels":"Machine(B-field) learning(I-field) techniques(O) ,(O) either(O) Supervised(B-field) learning(I-field) or(O) Unsupervised(B-field) learning(I-field) ,(O) have(O) been(O) used(O) to(O) induce(O) such(O) rules(O) automatically(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, person, field, researcher, task, location, product, university, conference, metric, algorithm, organization, country and O.\nSentence: Machine learning techniques , either Supervised learning or Unsupervised learning , have been used to induce such rules automatically .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Machine","learning","techniques",",","either","Supervised","learning","or","Unsupervised","learning",",","have","been","used","to","induce","such","rules","automatically","."],"labels":["B-field","I-field","O","O","O","B-field","I-field","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","person","field","researcher","task","location","product","university","conference","metric","algorithm","organization","country"]}
{"id":"11","dataset":"crossner_ai","split":"dev","instance":{"id":"11","prompt_labels":"Since(O) the(O) Log(B-metric) loss(I-metric) is(O) differentiable(O) ,(O) a(O) gradient-based(O) method(O) can(O) be(O) used(O) to(O) optimize(O) the(O) model(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, algorithm, organization, product, location, country, task, university, person, programming language, metric, conference, field and O.\nSentence: Since the Log loss is differentiable , a gradient-based method can be used to optimize the model .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Since","the","Log","loss","is","differentiable",",","a","gradient-based","method","can","be","used","to","optimize","the","model","."],"labels":["O","O","B-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","algorithm","organization","product","location","country","task","university","person","programming_language","metric","conference","field"]}
{"id":"13","dataset":"crossner_ai","split":"dev","instance":{"id":"13","prompt_labels":",(O) ((O) 2002(O) )(O) as(O) the(O) automatic(O) metric(O) for(O) Machine(B-task) translation(I-task) ((O) MT(B-task) )(O) evaluation(O) ,(O) many(O) other(O) methods(O) have(O) been(O) proposed(O) to(O) revise(O) or(O) improve(O) it(O) ,(O) such(O) as(O) TER(B-metric) ,(O) METEOR(B-metric) ,(O) Banerjee(B-researcher) and(O) Lavie(B-researcher) ,(O) ((O) 2005(O) )(O) etc(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, product, person, conference, organization, location, researcher, field, algorithm, task, metric, country, programming language and O.\nSentence: , ( 2002 ) as the automatic metric for Machine translation ( MT ) evaluation , many other methods have been proposed to revise or improve it , such as TER , METEOR , Banerjee and Lavie , ( 2005 ) etc .","prediction_output":null,"prediction_outputs":null,"group":null,"words":[",","(","2002",")","as","the","automatic","metric","for","Machine","translation","(","MT",")","evaluation",",","many","other","methods","have","been","proposed","to","revise","or","improve","it",",","such","as","TER",",","METEOR",",","Banerjee","and","Lavie",",","(","2005",")","etc","."],"labels":["O","O","O","O","O","O","O","O","O","B-task","I-task","O","B-task","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","O","B-metric","O","B-researcher","O","B-researcher","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","product","person","conference","organization","location","researcher","field","algorithm","task","metric","country","programming_language"]}
{"id":"14","dataset":"crossner_ai","split":"dev","instance":{"id":"14","prompt_labels":"It(O) includes(O) an(O) upper(O) ontology(O) ,(O) created(O) by(O) the(O) IEEE(B-organization) working(O) group(O) P1600.1(O) ((O) originally(O) by(O) Ian(B-researcher) Niles(I-researcher) and(O) Adam(B-researcher) Pease(I-researcher) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, task, person, product, conference, researcher, organization, location, algorithm, programming language, metric, country, field and O.\nSentence: It includes an upper ontology , created by the IEEE working group P1600.1 ( originally by Ian Niles and Adam Pease ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","includes","an","upper","ontology",",","created","by","the","IEEE","working","group","P1600.1","(","originally","by","Ian","Niles","and","Adam","Pease",")","."],"labels":["O","O","O","O","O","O","O","O","O","B-organization","O","O","O","O","O","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","O"],"target_index":null,"target_label":null},"label_list":["university","task","person","product","conference","researcher","organization","location","algorithm","programming_language","metric","country","field"]}
{"id":"15","dataset":"crossner_ai","split":"dev","instance":{"id":"15","prompt_labels":"In(O) Cryo(O) Electron(O) Tomography(O) ,(O) where(O) the(O) limited(O) number(O) of(O) projections(O) are(O) acquired(O) due(O) to(O) the(O) hardware(O) limitations(O) and(O) to(O) avoid(O) the(O) biological(O) specimen(O) damage(O) ,(O) it(O) can(O) be(O) used(O) along(O) with(O) compressive(B-algorithm) sensing(I-algorithm) techniques(I-algorithm) or(O) regularization(B-algorithm) functions(I-algorithm) ((O) e.g.(O) Huber(B-metric) loss(I-metric) )(O) to(O) improve(O) the(O) reconstruction(O) for(O) better(O) interpretation(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, product, programming language, university, person, field, location, conference, algorithm, organization, task, country, researcher and O.\nSentence: In Cryo Electron Tomography , where the limited number of projections are acquired due to the hardware limitations and to avoid the biological specimen damage , it can be used along with compressive sensing techniques or regularization functions ( e.g. Huber loss ) to improve the reconstruction for better interpretation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","Cryo","Electron","Tomography",",","where","the","limited","number","of","projections","are","acquired","due","to","the","hardware","limitations","and","to","avoid","the","biological","specimen","damage",",","it","can","be","used","along","with","compressive","sensing","techniques","or","regularization","functions","(","e.g.","Huber","loss",")","to","improve","the","reconstruction","for","better","interpretation","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","I-algorithm","O","O","B-metric","I-metric","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","product","programming_language","university","person","field","location","conference","algorithm","organization","task","country","researcher"]}
{"id":"19","dataset":"crossner_ai","split":"dev","instance":{"id":"19","prompt_labels":"Unsupervised(B-field) learning(I-field) ,(O) on(O) the(O) other(O) hand(O) ,(O) assumes(O) training(O) data(O) that(O) has(O) not(O) been(O) hand-labeled(O) ,(O) and(O) attempts(O) to(O) find(O) inherent(O) patterns(O) in(O) the(O) data(O) that(O) can(O) then(O) be(O) used(O) to(O) determine(O) the(O) correct(O) output(O) value(O) for(O) new(O) data(O) instances(O) ..(O) A(O) combination(O) of(O) the(O) two(O) that(O) has(O) recently(O) been(O) explored(O) is(O) semi-supervised(B-field) learning(I-field) ,(O) which(O) uses(O) a(O) combination(O) of(O) labeled(O) and(O) unlabeled(O) data(O) ((O) typically(O) a(O) small(O) set(O) of(O) labeled(O) data(O) combined(O) with(O) a(O) large(O) amount(O) of(O) unlabeled(O) data(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, metric, person, researcher, algorithm, task, location, programming language, university, country, conference, field, organization and O.\nSentence: Unsupervised learning , on the other hand , assumes training data that has not been hand-labeled , and attempts to find inherent patterns in the data that can then be used to determine the correct output value for new data instances .. A combination of the two that has recently been explored is semi-supervised learning , which uses a combination of labeled and unlabeled data ( typically a small set of labeled data combined with a large amount of unlabeled data ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Unsupervised","learning",",","on","the","other","hand",",","assumes","training","data","that","has","not","been","hand-labeled",",","and","attempts","to","find","inherent","patterns","in","the","data","that","can","then","be","used","to","determine","the","correct","output","value","for","new","data","instances","..","A","combination","of","the","two","that","has","recently","been","explored","is","semi-supervised","learning",",","which","uses","a","combination","of","labeled","and","unlabeled","data","(","typically","a","small","set","of","labeled","data","combined","with","a","large","amount","of","unlabeled","data",")","."],"labels":["B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["product","metric","person","researcher","algorithm","task","location","programming_language","university","country","conference","field","organization"]}
{"id":"20","dataset":"crossner_ai","split":"dev","instance":{"id":"20","prompt_labels":"Despite(O) those(O) humanoid(O) robots(O) for(O) utilitarian(O) uses(O) ,(O) there(O) are(O) some(O) humanoid(O) robots(O) which(O) aims(O) at(O) entertainment(O) uses(O) ,(O) such(O) as(O) Sony(B-organization) '(O) s(O) QRIO(B-product) and(O) Wow(B-organization) Wee(I-organization) '(O) s(O) RoboSapien(B-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, location, algorithm, country, organization, task, product, university, person, conference, programming language, metric, field and O.\nSentence: Despite those humanoid robots for utilitarian uses , there are some humanoid robots which aims at entertainment uses , such as Sony ' s QRIO and Wow Wee ' s RoboSapien .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Despite","those","humanoid","robots","for","utilitarian","uses",",","there","are","some","humanoid","robots","which","aims","at","entertainment","uses",",","such","as","Sony","'","s","QRIO","and","Wow","Wee","'","s","RoboSapien","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","O","B-product","O","B-organization","I-organization","O","O","B-product","O"],"target_index":null,"target_label":null},"label_list":["researcher","location","algorithm","country","organization","task","product","university","person","conference","programming_language","metric","field"]}
{"id":"21","dataset":"crossner_ai","split":"dev","instance":{"id":"21","prompt_labels":"Webber(B-researcher) became(O) a(O) Fellow(O) of(O) the(O) Association(B-conference) for(I-conference) the(I-conference) Advancement(I-conference) of(I-conference) Artificial(I-conference) Intelligence(I-conference) in(O) 1991(O) ,(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, field, algorithm, programming language, location, university, researcher, organization, product, metric, person, task, conference and O.\nSentence: Webber became a Fellow of the Association for the Advancement of Artificial Intelligence in 1991 ,","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Webber","became","a","Fellow","of","the","Association","for","the","Advancement","of","Artificial","Intelligence","in","1991",","],"labels":["B-researcher","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","field","algorithm","programming_language","location","university","researcher","organization","product","metric","person","task","conference"]}
{"id":"25","dataset":"crossner_ai","split":"dev","instance":{"id":"25","prompt_labels":"Expo(B-location) II(I-location) was(O) announced(O) as(O) being(O) the(O) locale(O) for(O) the(O) world(O) premiere(O) of(O) several(O) films(O) never(O) before(O) seen(O) in(O) 3D(O) ,(O) including(O) The(O) Diamond(O) Wizard(O) and(O) the(O) Universal(O) short(O) ,(O) Hawaiian(O) Nights(O) with(O) Mamie(B-person) Van(I-person) Doren(I-person) and(O) Pinky(B-person) Lee(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, algorithm, person, researcher, metric, university, product, location, conference, organization, country, task, field and O.\nSentence: Expo II was announced as being the locale for the world premiere of several films never before seen in 3D , including The Diamond Wizard and the Universal short , Hawaiian Nights with Mamie Van Doren and Pinky Lee .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Expo","II","was","announced","as","being","the","locale","for","the","world","premiere","of","several","films","never","before","seen","in","3D",",","including","The","Diamond","Wizard","and","the","Universal","short",",","Hawaiian","Nights","with","Mamie","Van","Doren","and","Pinky","Lee","."],"labels":["B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","I-person","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["programming_language","algorithm","person","researcher","metric","university","product","location","conference","organization","country","task","field"]}
{"id":"28","dataset":"crossner_ai","split":"dev","instance":{"id":"28","prompt_labels":"It(O) 's(O) easy(O) to(O) check(O) that(O) the(O) logistic(B-metric) loss(I-metric) and(O) binary(B-metric) cross(I-metric) entropy(I-metric) loss(I-metric) ((O) Log(B-metric) loss(I-metric) )(O) are(O) in(O) fact(O) the(O) same(O) ((O) up(O) to(O) a(O) multiplicative(O) constant(O) math(O) \\(O) frac(O) {(O) 1(O) }(O) {(O) \\(O) log(O) ((O) 2(O) )(O) }(O) /(O) math(O) )(O) .The(O) cross(B-metric) entropy(I-metric) loss(I-metric) is(O) closely(O) related(O) to(O) the(O) Kullback-Leibler(B-metric) divergence(I-metric) between(O) the(O) empirical(O) distribution(O) and(O) the(O) predicted(O) distribution(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, task, person, algorithm, country, conference, product, university, metric, field, researcher, organization, programming language and O.\nSentence: It 's easy to check that the logistic loss and binary cross entropy loss ( Log loss ) are in fact the same ( up to a multiplicative constant math \\ frac { 1 } { \\ log ( 2 ) } / math ) .The cross entropy loss is closely related to the Kullback-Leibler divergence between the empirical distribution and the predicted distribution .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","'s","easy","to","check","that","the","logistic","loss","and","binary","cross","entropy","loss","(","Log","loss",")","are","in","fact","the","same","(","up","to","a","multiplicative","constant","math","\\","frac","{","1","}","{","\\","log","(","2",")","}","/","math",")",".The","cross","entropy","loss","is","closely","related","to","the","Kullback-Leibler","divergence","between","the","empirical","distribution","and","the","predicted","distribution","."],"labels":["O","O","O","O","O","O","O","B-metric","I-metric","O","B-metric","I-metric","I-metric","I-metric","O","B-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O","O","O","O","B-metric","I-metric","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","task","person","algorithm","country","conference","product","university","metric","field","researcher","organization","programming_language"]}
{"id":"30","dataset":"crossner_ai","split":"dev","instance":{"id":"30","prompt_labels":"This(O) research(O) was(O) fundamental(O) to(O) the(O) development(O) of(O) modern(O) techniques(O) of(O) speech(B-task) synthesis(I-task) ,(O) reading(B-task) machines(I-task) for(I-task) the(I-task) blind(I-task) ,(O) the(O) study(O) of(O) speech(B-task) perception(I-task) and(O) speech(B-task) recognition(I-task) ,(O) and(O) the(O) development(O) of(O) the(O) motor(B-task) theory(I-task) of(I-task) speech(I-task) perception(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, product, location, programming language, person, country, field, researcher, organization, conference, university, metric, task and O.\nSentence: This research was fundamental to the development of modern techniques of speech synthesis , reading machines for the blind , the study of speech perception and speech recognition , and the development of the motor theory of speech perception .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","research","was","fundamental","to","the","development","of","modern","techniques","of","speech","synthesis",",","reading","machines","for","the","blind",",","the","study","of","speech","perception","and","speech","recognition",",","and","the","development","of","the","motor","theory","of","speech","perception","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-task","I-task","O","B-task","I-task","I-task","I-task","I-task","O","O","O","O","B-task","I-task","O","B-task","I-task","O","O","O","O","O","O","B-task","I-task","I-task","I-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["algorithm","product","location","programming_language","person","country","field","researcher","organization","conference","university","metric","task"]}
{"id":"31","dataset":"crossner_ai","split":"dev","instance":{"id":"31","prompt_labels":"The(O) Arduino(B-product) integrated(O) development(O) environment(O) ((O) IDE(O) )(O) is(O) a(O) cross-platform(O) application(O) ((O) for(O) Windows(B-product) ,(O) macOS(B-product) ,(O) and(O) Linux(B-product) )(O) that(O) is(O) written(O) in(O) the(O) programming(O) language(O) Java(B-programming language) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, product, conference, metric, field, researcher, person, location, algorithm, organization, programming language, task, country and O.\nSentence: The Arduino integrated development environment ( IDE ) is a cross-platform application ( for Windows , macOS , and Linux ) that is written in the programming language Java .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Arduino","integrated","development","environment","(","IDE",")","is","a","cross-platform","application","(","for","Windows",",","macOS",",","and","Linux",")","that","is","written","in","the","programming","language","Java","."],"labels":["O","B-product","O","O","O","O","O","O","O","O","O","O","O","O","B-product","O","B-product","O","O","B-product","O","O","O","O","O","O","O","O","B-programming language","O"],"target_index":null,"target_label":null},"label_list":["university","product","conference","metric","field","researcher","person","location","algorithm","organization","programming_language","task","country"]}
{"id":"33","dataset":"crossner_ai","split":"dev","instance":{"id":"33","prompt_labels":"Only(O) a(O) few(O) non-Japanese(O) companies(O) ultimately(O) managed(O) to(O) survive(O) in(O) this(O) market(O) ,(O) the(O) major(O) ones(O) being(O) :(O) Adept(B-organization) Technology(I-organization) ,(O) Stubli(B-organization) ,(O) the(O) Sweden(B-country) -(O) Switzerland(B-country) company(O) ABB(B-organization) Asea(I-organization) Brown(I-organization) Boveri(I-organization) ,(O) the(O) Germany(B-country) company(O) KUKA(B-organization) Robotics(I-organization) and(O) the(O) Italy(B-country) company(O) Comau(B-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, country, programming language, product, field, researcher, location, conference, task, university, algorithm, person, organization and O.\nSentence: Only a few non-Japanese companies ultimately managed to survive in this market , the major ones being : Adept Technology , Stubli , the Sweden - Switzerland company ABB Asea Brown Boveri , the Germany company KUKA Robotics and the Italy company Comau .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Only","a","few","non-Japanese","companies","ultimately","managed","to","survive","in","this","market",",","the","major","ones","being",":","Adept","Technology",",","Stubli",",","the","Sweden","-","Switzerland","company","ABB","Asea","Brown","Boveri",",","the","Germany","company","KUKA","Robotics","and","the","Italy","company","Comau","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","B-organization","O","O","B-country","O","B-country","O","B-organization","I-organization","I-organization","I-organization","O","O","B-country","O","B-organization","I-organization","O","O","B-country","O","B-organization","O"],"target_index":null,"target_label":null},"label_list":["metric","country","programming_language","product","field","researcher","location","conference","task","university","algorithm","person","organization"]}
{"id":"34","dataset":"crossner_ai","split":"dev","instance":{"id":"34","prompt_labels":"The(O) research(O) activities(O) include(O) an(O) annual(O) research(O) conference(O) ,(O) the(O) RuleML(B-conference) Symposium(I-conference) ,(O) also(O) known(O) as(O) RuleML(B-conference) for(O) short(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, algorithm, metric, field, task, conference, product, organization, researcher, programming language, university, person and O.\nSentence: The research activities include an annual research conference , the RuleML Symposium , also known as RuleML for short .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","research","activities","include","an","annual","research","conference",",","the","RuleML","Symposium",",","also","known","as","RuleML","for","short","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-conference","I-conference","O","O","O","O","B-conference","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","country","algorithm","metric","field","task","conference","product","organization","researcher","programming_language","university","person"]}
{"id":"36","dataset":"crossner_ai","split":"dev","instance":{"id":"36","prompt_labels":"He(O) has(O) won(O) awards(O) from(O) the(O) American(B-organization) Psychological(I-organization) Association(I-organization) ,(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) the(O) Royal(B-organization) ,(O) the(O) Cognitive(B-organization) Neuroscience(I-organization) Society(I-organization) and(O) the(O) American(B-organization) Humanist(I-organization) Association(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, metric, researcher, programming language, university, task, field, country, algorithm, person, organization, conference, location and O.\nSentence: He has won awards from the American Psychological Association , the National Academy of Sciences , the Royal , the Cognitive Neuroscience Society and the American Humanist Association .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","has","won","awards","from","the","American","Psychological","Association",",","the","National","Academy","of","Sciences",",","the","Royal",",","the","Cognitive","Neuroscience","Society","and","the","American","Humanist","Association","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["product","metric","researcher","programming_language","university","task","field","country","algorithm","person","organization","conference","location"]}
{"id":"39","dataset":"crossner_ai","split":"dev","instance":{"id":"39","prompt_labels":"General(O) sampling(O) from(O) the(O) truncated(O) normal(O) can(O) be(O) achieved(O) using(O) approximations(O) to(O) the(O) normal(O) CDF(B-algorithm) and(O) the(O) probit(B-algorithm) function(I-algorithm) ,(O) and(O) R(B-programming language) has(O) a(O) function(O) codertnorm(O) ((O) )(O) /(O) code(O) for(O) generating(O) truncated-normal(O) samples(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, person, product, researcher, university, organization, task, location, metric, field, conference, country, algorithm and O.\nSentence: General sampling from the truncated normal can be achieved using approximations to the normal CDF and the probit function , and R has a function codertnorm ( ) / code for generating truncated-normal samples .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["General","sampling","from","the","truncated","normal","can","be","achieved","using","approximations","to","the","normal","CDF","and","the","probit","function",",","and","R","has","a","function","codertnorm","(",")","/","code","for","generating","truncated-normal","samples","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","O","O","B-algorithm","I-algorithm","O","O","B-programming language","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","person","product","researcher","university","organization","task","location","metric","field","conference","country","algorithm"]}
{"id":"41","dataset":"crossner_ai","split":"dev","instance":{"id":"41","prompt_labels":"A(O) Java(B-programming language) implementation(O) using(O) zero(O) based(O) array(O) indexes(O) along(O) with(O) a(O) convenience(O) method(O) for(O) printing(O) the(O) solved(O) order(O) of(O) operations(O) :(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, field, organization, programming language, metric, university, location, algorithm, conference, product, task, country, researcher and O.\nSentence: A Java implementation using zero based array indexes along with a convenience method for printing the solved order of operations :","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","Java","implementation","using","zero","based","array","indexes","along","with","a","convenience","method","for","printing","the","solved","order","of","operations",":"],"labels":["O","B-programming language","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","field","organization","programming_language","metric","university","location","algorithm","conference","product","task","country","researcher"]}
{"id":"43","dataset":"crossner_ai","split":"dev","instance":{"id":"43","prompt_labels":"The(O) ACL(B-conference) has(O) a(O) European(O) ((O) European(B-conference) Chapter(I-conference) of(I-conference) the(I-conference) Association(I-conference) for(I-conference) Computational(I-conference) Linguistics(I-conference) )(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, researcher, university, product, country, algorithm, field, conference, task, location, programming language, metric, organization and O.\nSentence: The ACL has a European ( European Chapter of the Association for Computational Linguistics )","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","ACL","has","a","European","(","European","Chapter","of","the","Association","for","Computational","Linguistics",")"],"labels":["O","B-conference","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O"],"target_index":null,"target_label":null},"label_list":["person","researcher","university","product","country","algorithm","field","conference","task","location","programming_language","metric","organization"]}
{"id":"50","dataset":"crossner_ai","split":"dev","instance":{"id":"50","prompt_labels":"An(O) example(O) of(O) a(O) typical(O) computer(B-field) vision(I-field) computation(O) pipeline(O) for(O) Facial(B-product) recognition(I-product) system(I-product) using(O) k(B-algorithm) -NN(I-algorithm) including(O) feature(B-task) extraction(I-task) and(O) dimension(B-task) reduction(I-task) pre-processing(O) steps(O) ((O) usually(O) implemented(O) with(O) OpenCV(B-product) )(O) :(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, algorithm, metric, person, product, university, location, field, programming language, researcher, conference, task and O.\nSentence: An example of a typical computer vision computation pipeline for Facial recognition system using k -NN including feature extraction and dimension reduction pre-processing steps ( usually implemented with OpenCV ) :","prediction_output":null,"prediction_outputs":null,"group":null,"words":["An","example","of","a","typical","computer","vision","computation","pipeline","for","Facial","recognition","system","using","k","-NN","including","feature","extraction","and","dimension","reduction","pre-processing","steps","(","usually","implemented","with","OpenCV",")",":"],"labels":["O","O","O","O","O","B-field","I-field","O","O","O","B-product","I-product","I-product","O","B-algorithm","I-algorithm","O","B-task","I-task","O","B-task","I-task","O","O","O","O","O","O","B-product","O","O"],"target_index":null,"target_label":null},"label_list":["country","organization","algorithm","metric","person","product","university","location","field","programming_language","researcher","conference","task"]}
{"id":"51","dataset":"crossner_ai","split":"dev","instance":{"id":"51","prompt_labels":"It(O) has(O) a(O) rich(O) set(O) of(O) features(O) ,(O) libraries(O) for(O) constraint(B-algorithm) logic(I-algorithm) programming(I-algorithm) ,(O) multithreading(O) ,(O) unit(O) testing(O) ,(O) GUI(O) ,(O) interfacing(O) to(O) Java(B-programming language) ,(O) ODBC(B-product) and(O) others(O) ,(O) literate(B-algorithm) programming(I-algorithm) ,(O) a(O) web(O) server(O) ,(O) SGML(O) ,(O) RDF(O) ,(O) RDFS(O) ,(O) developer(O) tools(O) ((O) including(O) an(O) IDE(O) with(O) a(O) GUI(O) debugger(O) and(O) GUI(O) profiler(O) )(O) ,(O) and(O) extensive(O) documentation(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, algorithm, country, task, researcher, programming language, organization, conference, field, university, person, metric, product and O.\nSentence: It has a rich set of features , libraries for constraint logic programming , multithreading , unit testing , GUI , interfacing to Java , ODBC and others , literate programming , a web server , SGML , RDF , RDFS , developer tools ( including an IDE with a GUI debugger and GUI profiler ) , and extensive documentation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","has","a","rich","set","of","features",",","libraries","for","constraint","logic","programming",",","multithreading",",","unit","testing",",","GUI",",","interfacing","to","Java",",","ODBC","and","others",",","literate","programming",",","a","web","server",",","SGML",",","RDF",",","RDFS",",","developer","tools","(","including","an","IDE","with","a","GUI","debugger","and","GUI","profiler",")",",","and","extensive","documentation","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","B-programming language","O","B-product","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","algorithm","country","task","researcher","programming_language","organization","conference","field","university","person","metric","product"]}
{"id":"52","dataset":"crossner_ai","split":"dev","instance":{"id":"52","prompt_labels":"In(O) computer(B-field) vision(I-field) and(O) image(B-field) processing(I-field) ,(O) the(O) notion(O) of(O) scale(O) space(O) representation(O) and(O) Gaussian(O) derivative(O) operators(O) is(O) as(O) a(O) canonical(O) multi-scale(O) representation(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, researcher, organization, country, programming language, product, metric, field, task, university, conference, location, algorithm and O.\nSentence: In computer vision and image processing , the notion of scale space representation and Gaussian derivative operators is as a canonical multi-scale representation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","computer","vision","and","image","processing",",","the","notion","of","scale","space","representation","and","Gaussian","derivative","operators","is","as","a","canonical","multi-scale","representation","."],"labels":["O","B-field","I-field","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","researcher","organization","country","programming_language","product","metric","field","task","university","conference","location","algorithm"]}
{"id":"53","dataset":"crossner_ai","split":"dev","instance":{"id":"53","prompt_labels":"He(O) is(O) also(O) the(O) President(O) of(O) the(O) Neural(B-organization) Information(I-organization) Processing(I-organization) Systems(I-organization) Foundation(I-organization) ,(O) a(O) non-profit(O) organization(O) that(O) oversees(O) the(O) annual(O) Conference(B-conference) on(I-conference) Neural(I-conference) Information(I-conference) Processing(I-conference) Systems(I-conference) Conference(I-conference) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, algorithm, field, conference, product, person, location, programming language, researcher, task, university, metric and O.\nSentence: He is also the President of the Neural Information Processing Systems Foundation , a non-profit organization that oversees the annual Conference on Neural Information Processing Systems Conference .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","is","also","the","President","of","the","Neural","Information","Processing","Systems","Foundation",",","a","non-profit","organization","that","oversees","the","annual","Conference","on","Neural","Information","Processing","Systems","Conference","."],"labels":["O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O"],"target_index":null,"target_label":null},"label_list":["country","organization","algorithm","field","conference","product","person","location","programming_language","researcher","task","university","metric"]}
{"id":"54","dataset":"crossner_ai","split":"dev","instance":{"id":"54","prompt_labels":"For(O) regression(B-task) analysis(I-task) problems(O) the(O) squared(B-metric) error(I-metric) can(O) be(O) used(O) as(O) a(O) loss(O) function(O) ,(O) for(O) classification(B-task) the(O) cross(B-metric) entropy(I-metric) can(O) be(O) used(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, algorithm, metric, conference, university, person, task, product, organization, field, country, programming language, researcher and O.\nSentence: For regression analysis problems the squared error can be used as a loss function , for classification the cross entropy can be used .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","regression","analysis","problems","the","squared","error","can","be","used","as","a","loss","function",",","for","classification","the","cross","entropy","can","be","used","."],"labels":["O","B-task","I-task","O","O","B-metric","I-metric","O","O","O","O","O","O","O","O","O","B-task","O","B-metric","I-metric","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","algorithm","metric","conference","university","person","task","product","organization","field","country","programming_language","researcher"]}
{"id":"55","dataset":"crossner_ai","split":"dev","instance":{"id":"55","prompt_labels":"Lafferty(B-researcher) served(O) many(O) prestigious(O) positions(O) ,(O) including(O) :(O) 1(O) )(O) program(O) co-chair(O) and(O) general(O) co-chair(O) of(O) the(O) Neural(B-conference) Information(I-conference) Processing(I-conference) Systems(I-conference) ((O) Conference(B-conference) on(I-conference) Neural(I-conference) Information(I-conference) Processing(I-conference) Systems(I-conference) )(O) Foundation(O) conferences(O) ;(O) 2(O) )(O) co-director(O) of(O) CMU(B-university) 's(O) new(O) Ph.D.(O) Machine(B-field) Learning(I-field) Ph.D.(O) Program(O) ;(O) 3(O) )(O) associate(O) editor(O) of(O) the(O) Journal(B-conference) of(I-conference) Machine(I-conference) Learning(I-conference) Research(I-conference)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, country, programming language, university, organization, task, conference, person, location, metric, field, product, algorithm and O.\nSentence: Lafferty served many prestigious positions , including : 1 ) program co-chair and general co-chair of the Neural Information Processing Systems ( Conference on Neural Information Processing Systems ) Foundation conferences ; 2 ) co-director of CMU 's new Ph.D. Machine Learning Ph.D. Program ; 3 ) associate editor of the Journal of Machine Learning Research","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Lafferty","served","many","prestigious","positions",",","including",":","1",")","program","co-chair","and","general","co-chair","of","the","Neural","Information","Processing","Systems","(","Conference","on","Neural","Information","Processing","Systems",")","Foundation","conferences",";","2",")","co-director","of","CMU","'s","new","Ph.D.","Machine","Learning","Ph.D.","Program",";","3",")","associate","editor","of","the","Journal","of","Machine","Learning","Research"],"labels":["B-researcher","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","O","O","O","O","O","O","O","B-university","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference"],"target_index":null,"target_label":null},"label_list":["researcher","country","programming_language","university","organization","task","conference","person","location","metric","field","product","algorithm"]}
{"id":"57","dataset":"crossner_ai","split":"dev","instance":{"id":"57","prompt_labels":"Apertium(B-product) is(O) a(O) shallow-transfer(B-product) machine(I-product) translation(I-product) system(I-product) ,(O) which(O) uses(O) finite(B-algorithm) state(I-algorithm) transducer(I-algorithm) s(O) for(O) all(O) of(O) its(O) lexical(O) transformations(O) ,(O) and(O) hidden(B-algorithm) Markov(I-algorithm) model(I-algorithm) s(O) for(O) part-of-speech(B-task) tagging(I-task) or(O) word(B-task) category(I-task) disambiguation(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, country, organization, conference, metric, researcher, person, programming language, field, task, algorithm, university, location and O.\nSentence: Apertium is a shallow-transfer machine translation system , which uses finite state transducer s for all of its lexical transformations , and hidden Markov model s for part-of-speech tagging or word category disambiguation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Apertium","is","a","shallow-transfer","machine","translation","system",",","which","uses","finite","state","transducer","s","for","all","of","its","lexical","transformations",",","and","hidden","Markov","model","s","for","part-of-speech","tagging","or","word","category","disambiguation","."],"labels":["B-product","O","O","B-product","I-product","I-product","I-product","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","B-task","I-task","O","B-task","I-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["product","country","organization","conference","metric","researcher","person","programming_language","field","task","algorithm","university","location"]}
{"id":"58","dataset":"crossner_ai","split":"dev","instance":{"id":"58","prompt_labels":"The(O) natural(O) gradient(O) of(O) mathE(O) f(O) ((O) x(O) )(O) /(O) math(O) ,(O) complying(O) with(O) the(O) Fisher(B-metric) information(I-metric) metric(I-metric) ((O) an(O) informational(O) distance(O) measure(O) between(O) probability(O) distributions(O) and(O) the(O) curvature(O) of(O) the(O) relative(B-metric) entropy(I-metric) )(O) ,(O) now(O) reads(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, field, metric, algorithm, conference, university, task, programming language, organization, location, country, person, researcher and O.\nSentence: The natural gradient of mathE f ( x ) / math , complying with the Fisher information metric ( an informational distance measure between probability distributions and the curvature of the relative entropy ) , now reads","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","natural","gradient","of","mathE","f","(","x",")","/","math",",","complying","with","the","Fisher","information","metric","(","an","informational","distance","measure","between","probability","distributions","and","the","curvature","of","the","relative","entropy",")",",","now","reads"],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["product","field","metric","algorithm","conference","university","task","programming_language","organization","location","country","person","researcher"]}
{"id":"60","dataset":"crossner_ai","split":"dev","instance":{"id":"60","prompt_labels":"The(O) most(O) influential(O) implementation(O) of(O) Planner(B-product) was(O) the(O) subset(O) of(O) Planner(B-product) ,(O) called(O) Micro-Planner(B-product) ,(O) implemented(O) by(O) Gerald(B-researcher) Jay(I-researcher) Sussman(I-researcher) ,(O) Eugene(B-researcher) Charniak(I-researcher) and(O) Terry(B-researcher) Winograd(I-researcher) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, country, university, field, metric, product, person, task, location, programming language, organization, conference, researcher and O.\nSentence: The most influential implementation of Planner was the subset of Planner , called Micro-Planner , implemented by Gerald Jay Sussman , Eugene Charniak and Terry Winograd .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","most","influential","implementation","of","Planner","was","the","subset","of","Planner",",","called","Micro-Planner",",","implemented","by","Gerald","Jay","Sussman",",","Eugene","Charniak","and","Terry","Winograd","."],"labels":["O","O","O","O","O","B-product","O","O","O","O","B-product","O","O","B-product","O","O","O","B-researcher","I-researcher","I-researcher","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O"],"target_index":null,"target_label":null},"label_list":["algorithm","country","university","field","metric","product","person","task","location","programming_language","organization","conference","researcher"]}
{"id":"62","dataset":"crossner_ai","split":"dev","instance":{"id":"62","prompt_labels":"New(O) features(O) in(O) Office(B-product) XP(I-product) include(O) smart(O) tags(O) ,(O) a(O) selection-based(O) search(O) feature(O) that(O) recognizes(O) different(O) types(O) of(O) text(O) in(O) a(O) document(O) so(O) that(O) users(O) can(O) perform(O) additional(O) actions(O) ;(O) a(O) task(O) pane(O) interface(O) that(O) consolidates(O) popular(O) menu(O) bar(O) commands(O) on(O) the(O) right(O) side(O) of(O) the(O) screen(O) to(O) facilitate(O) quick(O) access(O) to(O) them(O) ;(O) new(O) document(B-task) collaboration(I-task) capabilities(O) ,(O) support(O) for(O) MSN(B-product) Groups(I-product) and(O) SharePoint(B-product) ;(O) and(O) integrated(O) handwriting(B-task) recognition(I-task) and(O) speech(B-task) recognition(I-task) capabilities(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, conference, person, programming language, researcher, algorithm, field, organization, task, metric, product, location, university and O.\nSentence: New features in Office XP include smart tags , a selection-based search feature that recognizes different types of text in a document so that users can perform additional actions ; a task pane interface that consolidates popular menu bar commands on the right side of the screen to facilitate quick access to them ; new document collaboration capabilities , support for MSN Groups and SharePoint ; and integrated handwriting recognition and speech recognition capabilities .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["New","features","in","Office","XP","include","smart","tags",",","a","selection-based","search","feature","that","recognizes","different","types","of","text","in","a","document","so","that","users","can","perform","additional","actions",";","a","task","pane","interface","that","consolidates","popular","menu","bar","commands","on","the","right","side","of","the","screen","to","facilitate","quick","access","to","them",";","new","document","collaboration","capabilities",",","support","for","MSN","Groups","and","SharePoint",";","and","integrated","handwriting","recognition","and","speech","recognition","capabilities","."],"labels":["O","O","O","B-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-task","I-task","O","O","O","O","B-product","I-product","O","B-product","O","O","O","B-task","I-task","O","B-task","I-task","O","O"],"target_index":null,"target_label":null},"label_list":["country","conference","person","programming_language","researcher","algorithm","field","organization","task","metric","product","location","university"]}
{"id":"64","dataset":"crossner_ai","split":"dev","instance":{"id":"64","prompt_labels":"In(O) 2001(O) ,(O) Mehler(B-researcher) was(O) elected(O) a(O) foreign(O) honorary(O) member(O) of(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) ,(O) and(O) in(O) 2003(O) ,(O) he(O) was(O) elected(O) a(O) Fellow(O) of(O) the(O) American(B-organization) Association(I-organization) for(I-organization) the(I-organization) Advancement(I-organization) of(I-organization) Science(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, product, task, conference, university, algorithm, location, country, organization, field, metric, researcher, person and O.\nSentence: In 2001 , Mehler was elected a foreign honorary member of the American Academy of Arts and Sciences , and in 2003 , he was elected a Fellow of the American Association for the Advancement of Science .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2001",",","Mehler","was","elected","a","foreign","honorary","member","of","the","American","Academy","of","Arts","and","Sciences",",","and","in","2003",",","he","was","elected","a","Fellow","of","the","American","Association","for","the","Advancement","of","Science","."],"labels":["O","O","O","B-researcher","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["programming_language","product","task","conference","university","algorithm","location","country","organization","field","metric","researcher","person"]}
{"id":"65","dataset":"crossner_ai","split":"dev","instance":{"id":"65","prompt_labels":"The(O) extension(O) of(O) this(O) concept(O) to(O) non-binary(B-task) classifications(I-task) yields(O) the(O) confusion(B-metric) matrix(I-metric) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, field, product, location, country, person, organization, conference, metric, researcher, task, algorithm, university and O.\nSentence: The extension of this concept to non-binary classifications yields the confusion matrix .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","extension","of","this","concept","to","non-binary","classifications","yields","the","confusion","matrix","."],"labels":["O","O","O","O","O","O","B-task","I-task","O","O","B-metric","I-metric","O"],"target_index":null,"target_label":null},"label_list":["programming_language","field","product","location","country","person","organization","conference","metric","researcher","task","algorithm","university"]}
{"id":"66","dataset":"crossner_ai","split":"dev","instance":{"id":"66","prompt_labels":"An(O) updated(O) measurement(O) noise(O) variance(O) estimate(O) can(O) be(O) obtained(O) from(O) the(O) maximum(B-algorithm) likelihood(I-algorithm) calculation(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, algorithm, task, person, university, field, conference, programming language, organization, product, location, researcher, metric and O.\nSentence: An updated measurement noise variance estimate can be obtained from the maximum likelihood calculation","prediction_output":null,"prediction_outputs":null,"group":null,"words":["An","updated","measurement","noise","variance","estimate","can","be","obtained","from","the","maximum","likelihood","calculation"],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["country","algorithm","task","person","university","field","conference","programming_language","organization","product","location","researcher","metric"]}
{"id":"67","dataset":"crossner_ai","split":"dev","instance":{"id":"67","prompt_labels":"In(O) machine(B-field) learning(I-field) ,(O) the(O) perceptron(B-algorithm) is(O) an(O) algorithm(O) for(O) supervised(B-field) learning(I-field) of(O) binary(B-task) classification(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, programming language, metric, country, person, task, product, conference, organization, algorithm, location, field, researcher and O.\nSentence: In machine learning , the perceptron is an algorithm for supervised learning of binary classification .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","machine","learning",",","the","perceptron","is","an","algorithm","for","supervised","learning","of","binary","classification","."],"labels":["O","B-field","I-field","O","O","B-algorithm","O","O","O","O","B-field","I-field","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["university","programming_language","metric","country","person","task","product","conference","organization","algorithm","location","field","researcher"]}
{"id":"70","dataset":"crossner_ai","split":"dev","instance":{"id":"70","prompt_labels":"Information(B-task) Dissemination(I-task) is(O) also(O) part(O) of(O) ELRA(B-conference) 's(O) missions(O) which(O) is(O) carried(O) through(O) both(O) the(O) organisation(O) of(O) the(O) conference(O) LREC(B-conference) and(O) the(O) Language(B-conference) Resources(I-conference) and(I-conference) Evaluation(I-conference) Journal(I-conference) edited(O) by(O) Springer(B-conference) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, task, researcher, location, conference, country, product, university, person, algorithm, organization, programming language, field and O.\nSentence: Information Dissemination is also part of ELRA 's missions which is carried through both the organisation of the conference LREC and the Language Resources and Evaluation Journal edited by Springer .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Information","Dissemination","is","also","part","of","ELRA","'s","missions","which","is","carried","through","both","the","organisation","of","the","conference","LREC","and","the","Language","Resources","and","Evaluation","Journal","edited","by","Springer","."],"labels":["B-task","I-task","O","O","O","O","B-conference","O","O","O","O","O","O","O","O","O","O","O","O","B-conference","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","O","O","B-conference","O"],"target_index":null,"target_label":null},"label_list":["metric","task","researcher","location","conference","country","product","university","person","algorithm","organization","programming_language","field"]}
{"id":"73","dataset":"crossner_ai","split":"dev","instance":{"id":"73","prompt_labels":"Stochastic(B-algorithm) gradient(I-algorithm) descent(I-algorithm) is(O) a(O) popular(O) algorithm(O) for(O) training(O) a(O) wide(O) range(O) of(O) models(O) in(O) machine(B-field) learning(I-field) ,(O) including(O) ((O) linear(O) )(O) support(B-algorithm) vector(I-algorithm) machine(I-algorithm) s(O) ,(O) logistic(B-algorithm) regression(I-algorithm) ((O) see(O) ,(O) e.g.(O) ,(O) Vowpal(B-algorithm) Wabbit(I-algorithm) )(O) and(O) graphical(B-algorithm) model(I-algorithm) s.Jenny(B-researcher) Rose(I-researcher) Finkel(I-researcher) ,(O) Alex(B-researcher) Kleeman(I-researcher) ,(O) Christopher(B-researcher) D.(I-researcher) Manning(I-researcher) ((O) 2008(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, task, programming language, location, conference, product, person, organization, algorithm, metric, researcher, field, country and O.\nSentence: Stochastic gradient descent is a popular algorithm for training a wide range of models in machine learning , including ( linear ) support vector machine s , logistic regression ( see , e.g. , Vowpal Wabbit ) and graphical model s.Jenny Rose Finkel , Alex Kleeman , Christopher D. Manning ( 2008 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Stochastic","gradient","descent","is","a","popular","algorithm","for","training","a","wide","range","of","models","in","machine","learning",",","including","(","linear",")","support","vector","machine","s",",","logistic","regression","(","see",",","e.g.",",","Vowpal","Wabbit",")","and","graphical","model","s.Jenny","Rose","Finkel",",","Alex","Kleeman",",","Christopher","D.","Manning","(","2008",")","."],"labels":["B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","B-algorithm","I-algorithm","O","O","O","O","O","B-algorithm","I-algorithm","O","O","B-algorithm","I-algorithm","B-researcher","I-researcher","I-researcher","O","B-researcher","I-researcher","O","B-researcher","I-researcher","I-researcher","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","task","programming_language","location","conference","product","person","organization","algorithm","metric","researcher","field","country"]}
{"id":"76","dataset":"crossner_ai","split":"dev","instance":{"id":"76","prompt_labels":"Sensitivity(B-metric) is(O) not(O) the(O) same(O) as(O) the(O) precision(B-metric) or(O) positive(B-metric) predictive(I-metric) value(I-metric) ((O) ratio(O) of(O) TRUE(B-metric) positives(I-metric) to(O) combined(O) TRUE(B-metric) and(I-metric) FALSE(I-metric) positives(I-metric) )(O) ,(O) which(O) is(O) as(O) much(O) a(O) statement(O) about(O) the(O) proportion(O) of(O) actual(O) positives(O) in(O) the(O) population(O) being(O) tested(O) as(O) it(O) is(O) about(O) the(O) test(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, organization, product, university, researcher, person, conference, task, country, algorithm, field, metric, location and O.\nSentence: Sensitivity is not the same as the precision or positive predictive value ( ratio of TRUE positives to combined TRUE and FALSE positives ) , which is as much a statement about the proportion of actual positives in the population being tested as it is about the test .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Sensitivity","is","not","the","same","as","the","precision","or","positive","predictive","value","(","ratio","of","TRUE","positives","to","combined","TRUE","and","FALSE","positives",")",",","which","is","as","much","a","statement","about","the","proportion","of","actual","positives","in","the","population","being","tested","as","it","is","about","the","test","."],"labels":["B-metric","O","O","O","O","O","O","B-metric","O","B-metric","I-metric","I-metric","O","O","O","B-metric","I-metric","O","O","B-metric","I-metric","I-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","organization","product","university","researcher","person","conference","task","country","algorithm","field","metric","location"]}
{"id":"77","dataset":"crossner_ai","split":"dev","instance":{"id":"77","prompt_labels":"The(O) screenplay(O) by(O) Hampton(B-person) Fancher(I-person) !(O) --(O) Not(O) titled(O) Android(B-product) initially(O) -(O) See(O) Sammon(B-person) ,(O) pp.(O) 32(O) and(O) 38(O) for(O) explanation(O) --(O) was(O) optioned(O) in(O) 1977(O) .(O) Sammon(B-person) ,(O) pp.(O) 23-30(O) Producer(O) Michael(B-person) Deeley(I-person) became(O) interested(O) in(O) Fancher(B-person) 's(O) draft(O) and(O) convinced(O) director(O) Ridley(B-person) Scott(I-person) to(O) film(O) it(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, task, metric, country, programming language, product, field, algorithm, person, conference, researcher, organization, location and O.\nSentence: The screenplay by Hampton Fancher ! -- Not titled Android initially - See Sammon , pp. 32 and 38 for explanation -- was optioned in 1977 . Sammon , pp. 23-30 Producer Michael Deeley became interested in Fancher 's draft and convinced director Ridley Scott to film it .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","screenplay","by","Hampton","Fancher","!","--","Not","titled","Android","initially","-","See","Sammon",",","pp.","32","and","38","for","explanation","--","was","optioned","in","1977",".","Sammon",",","pp.","23-30","Producer","Michael","Deeley","became","interested","in","Fancher","'s","draft","and","convinced","director","Ridley","Scott","to","film","it","."],"labels":["O","O","O","B-person","I-person","O","O","O","O","B-product","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","O","O","O","O","B-person","I-person","O","O","O","B-person","O","O","O","O","O","B-person","I-person","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","task","metric","country","programming_language","product","field","algorithm","person","conference","researcher","organization","location"]}
{"id":"80","dataset":"crossner_ai","split":"dev","instance":{"id":"80","prompt_labels":"The(O) system(O) uses(O) a(O) combination(O) of(O) techniques(O) from(O) computational(B-field) linguistics(I-field) ,(O) information(B-task) retrieval(I-task) and(O) knowledge(B-task) representation(I-task) for(I-task) finding(I-task) answers(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, person, algorithm, researcher, country, organization, field, conference, location, metric, product, university, task and O.\nSentence: The system uses a combination of techniques from computational linguistics , information retrieval and knowledge representation for finding answers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","system","uses","a","combination","of","techniques","from","computational","linguistics",",","information","retrieval","and","knowledge","representation","for","finding","answers","."],"labels":["O","O","O","O","O","O","O","O","B-field","I-field","O","B-task","I-task","O","B-task","I-task","I-task","I-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["programming_language","person","algorithm","researcher","country","organization","field","conference","location","metric","product","university","task"]}
{"id":"82","dataset":"crossner_ai","split":"dev","instance":{"id":"82","prompt_labels":"Researchers(O) have(O) attempted(O) a(O) number(O) of(O) methods(O) such(O) as(O) optical(B-algorithm) flow(I-algorithm) ,(O) Kalman(B-algorithm) filtering(I-algorithm) ,(O) Hidden(B-algorithm) Markov(I-algorithm) model(I-algorithm) s(O) ,(O) etc(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, researcher, organization, product, algorithm, programming language, university, field, task, location, person, conference, country and O.\nSentence: Researchers have attempted a number of methods such as optical flow , Kalman filtering , Hidden Markov model s , etc .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Researchers","have","attempted","a","number","of","methods","such","as","optical","flow",",","Kalman","filtering",",","Hidden","Markov","model","s",",","etc","."],"labels":["O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","B-algorithm","I-algorithm","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","researcher","organization","product","algorithm","programming_language","university","field","task","location","person","conference","country"]}
{"id":"83","dataset":"crossner_ai","split":"dev","instance":{"id":"83","prompt_labels":"She(O) has(O) held(O) the(O) positions(O) of(O) President(O) ,(O) Vice(O) President(O) ,(O) and(O) Secretary-Treasurer(O) of(O) the(O) Association(B-conference) for(I-conference) Computational(I-conference) Linguistics(I-conference) and(O) has(O) been(O) a(O) board(O) member(O) and(O) secretary(O) of(O) the(O) board(O) of(O) the(O) Computing(B-organization) Research(I-organization) Association(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, field, product, programming language, metric, task, location, country, organization, algorithm, university, person, conference and O.\nSentence: She has held the positions of President , Vice President , and Secretary-Treasurer of the Association for Computational Linguistics and has been a board member and secretary of the board of the Computing Research Association .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","has","held","the","positions","of","President",",","Vice","President",",","and","Secretary-Treasurer","of","the","Association","for","Computational","Linguistics","and","has","been","a","board","member","and","secretary","of","the","board","of","the","Computing","Research","Association","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["researcher","field","product","programming_language","metric","task","location","country","organization","algorithm","university","person","conference"]}
{"id":"86","dataset":"crossner_ai","split":"dev","instance":{"id":"86","prompt_labels":"A(O) collaborative(B-product) robot(I-product) or(O) cobot(B-product) is(O) a(O) robot(O) that(O) can(O) safely(O) and(O) effectively(O) interact(O) with(O) human(O) workers(O) while(O) performing(O) simple(O) industrial(O) tasks(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, task, researcher, product, location, university, conference, field, metric, programming language, country, person, organization and O.\nSentence: A collaborative robot or cobot is a robot that can safely and effectively interact with human workers while performing simple industrial tasks .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","collaborative","robot","or","cobot","is","a","robot","that","can","safely","and","effectively","interact","with","human","workers","while","performing","simple","industrial","tasks","."],"labels":["O","B-product","I-product","O","B-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["algorithm","task","researcher","product","location","university","conference","field","metric","programming_language","country","person","organization"]}
{"id":"87","dataset":"crossner_ai","split":"dev","instance":{"id":"87","prompt_labels":"This(O) overall(O) framework(O) has(O) been(O) applied(O) to(O) a(O) large(O) variety(O) of(O) problems(O) in(O) computer(B-field) vision(I-field) ,(O) including(O) feature(B-task) detection(I-task) ,(O) feature(B-task) classification(I-task) ,(O) image(B-task) segmentation(I-task) ,(O) image(B-task) matching(I-task) ,(O) motion(B-task) estimation(I-task) ,(O) computation(B-task) of(I-task) shape(I-task) cues(I-task) and(O) object(B-task) recognition(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, conference, task, country, field, organization, person, researcher, product, algorithm, metric, programming language, location and O.\nSentence: This overall framework has been applied to a large variety of problems in computer vision , including feature detection , feature classification , image segmentation , image matching , motion estimation , computation of shape cues and object recognition .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","overall","framework","has","been","applied","to","a","large","variety","of","problems","in","computer","vision",",","including","feature","detection",",","feature","classification",",","image","segmentation",",","image","matching",",","motion","estimation",",","computation","of","shape","cues","and","object","recognition","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","I-task","I-task","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["university","conference","task","country","field","organization","person","researcher","product","algorithm","metric","programming_language","location"]}
{"id":"88","dataset":"crossner_ai","split":"dev","instance":{"id":"88","prompt_labels":"In(O) many(O) practical(O) applications(O) ,(O) parameter(B-task) estimation(I-task) for(O) naive(B-algorithm) Bayes(I-algorithm) models(I-algorithm) uses(O) the(O) method(O) of(O) maximum(B-algorithm) likelihood(I-algorithm) ;(O) other(O) words(O) ,(O) one(O) can(O) work(O) with(O) the(O) naive(B-algorithm) Bayes(I-algorithm) model(O) without(O) accepting(O) Bayesian(B-algorithm) probability(I-algorithm) or(O) using(O) any(O) Bayesian(B-algorithm) methods(I-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, task, university, researcher, organization, programming language, country, person, location, algorithm, conference, field, metric and O.\nSentence: In many practical applications , parameter estimation for naive Bayes models uses the method of maximum likelihood ; other words , one can work with the naive Bayes model without accepting Bayesian probability or using any Bayesian methods .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","many","practical","applications",",","parameter","estimation","for","naive","Bayes","models","uses","the","method","of","maximum","likelihood",";","other","words",",","one","can","work","with","the","naive","Bayes","model","without","accepting","Bayesian","probability","or","using","any","Bayesian","methods","."],"labels":["O","O","O","O","O","B-task","I-task","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","B-algorithm","I-algorithm","O","O","O","B-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["product","task","university","researcher","organization","programming_language","country","person","location","algorithm","conference","field","metric"]}
{"id":"89","dataset":"crossner_ai","split":"dev","instance":{"id":"89","prompt_labels":"Brothers(O) -(O) Victor(B-researcher) Gershevich(I-researcher) Katz(I-researcher) ,(O) American(O) mathematician(O) ,(O) professor(O) at(O) the(O) Massachusetts(B-university) Institute(I-university) of(I-university) Technology(I-university) ;(O) Mikhail(B-researcher) Gershevich(I-researcher) Katz(I-researcher) ,(O) Israeli(O) mathematician(O) ,(O) graduate(O) of(O) Harvard(B-university) and(O) Columbia(B-university) ((O) Ph.D.(O) ,(O) 1984(O) )(O) universities(O) ,(O) professor(O) at(O) Bar-Ilan(B-university) University(I-university) ,(O) author(O) of(O) the(O) monograph(O) Systolic(O) Geometry(O) and(O) Topology(O) ((O) Mathematical(O) Surveys(O) and(O) Monographs(O) ,(O) vol(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, university, country, person, conference, researcher, algorithm, task, organization, field, location, programming language, metric and O.\nSentence: Brothers - Victor Gershevich Katz , American mathematician , professor at the Massachusetts Institute of Technology ; Mikhail Gershevich Katz , Israeli mathematician , graduate of Harvard and Columbia ( Ph.D. , 1984 ) universities , professor at Bar-Ilan University , author of the monograph Systolic Geometry and Topology ( Mathematical Surveys and Monographs , vol .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Brothers","-","Victor","Gershevich","Katz",",","American","mathematician",",","professor","at","the","Massachusetts","Institute","of","Technology",";","Mikhail","Gershevich","Katz",",","Israeli","mathematician",",","graduate","of","Harvard","and","Columbia","(","Ph.D.",",","1984",")","universities",",","professor","at","Bar-Ilan","University",",","author","of","the","monograph","Systolic","Geometry","and","Topology","(","Mathematical","Surveys","and","Monographs",",","vol","."],"labels":["O","O","B-researcher","I-researcher","I-researcher","O","O","O","O","O","O","O","B-university","I-university","I-university","I-university","O","B-researcher","I-researcher","I-researcher","O","O","O","O","O","O","B-university","O","B-university","O","O","O","O","O","O","O","O","O","B-university","I-university","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["product","university","country","person","conference","researcher","algorithm","task","organization","field","location","programming_language","metric"]}
{"id":"90","dataset":"crossner_ai","split":"dev","instance":{"id":"90","prompt_labels":"In(O) 2000(O) Manuel(B-person) Toharia(I-person) ,(O) a(O) speaker(O) at(O) previous(O) Campus(B-conference) Parties(I-conference) ,(O) and(O) director(O) of(O) Prncipe(B-organization) Felipe(I-organization) 's(I-organization) Museum(I-organization) of(I-organization) Sciences(I-organization) in(O) Valencia(B-location) 's(I-location) City(I-location) of(I-location) arts(I-location) and(I-location) Sciences(I-location) suggested(O) that(O) Ragageles(B-person) expand(O) and(O) make(O) the(O) event(O) more(O) international(O) by(O) moving(O) it(O) to(O) the(O) famous(O) museum(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, metric, algorithm, organization, product, university, conference, researcher, person, task, field, programming language, location and O.\nSentence: In 2000 Manuel Toharia , a speaker at previous Campus Parties , and director of Prncipe Felipe 's Museum of Sciences in Valencia 's City of arts and Sciences suggested that Ragageles expand and make the event more international by moving it to the famous museum .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2000","Manuel","Toharia",",","a","speaker","at","previous","Campus","Parties",",","and","director","of","Prncipe","Felipe","'s","Museum","of","Sciences","in","Valencia","'s","City","of","arts","and","Sciences","suggested","that","Ragageles","expand","and","make","the","event","more","international","by","moving","it","to","the","famous","museum","."],"labels":["O","O","B-person","I-person","O","O","O","O","O","B-conference","I-conference","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-location","I-location","I-location","I-location","I-location","I-location","I-location","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","metric","algorithm","organization","product","university","conference","researcher","person","task","field","programming_language","location"]}
{"id":"98","dataset":"crossner_ai","split":"dev","instance":{"id":"98","prompt_labels":"Common(O) criteria(O) are(O) the(O) Mean(B-metric) Squared(I-metric) Error(I-metric) criterion(O) implemented(O) in(O) MSECriterion(B-metric) and(O) the(O) cross-entropy(B-metric) criterion(I-metric) implemented(O) in(O) NLLCriterion(B-metric) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, field, location, university, researcher, person, organization, task, programming language, conference, metric, algorithm, country and O.\nSentence: Common criteria are the Mean Squared Error criterion implemented in MSECriterion and the cross-entropy criterion implemented in NLLCriterion .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Common","criteria","are","the","Mean","Squared","Error","criterion","implemented","in","MSECriterion","and","the","cross-entropy","criterion","implemented","in","NLLCriterion","."],"labels":["O","O","O","O","B-metric","I-metric","I-metric","O","O","O","B-metric","O","O","B-metric","I-metric","O","O","B-metric","O"],"target_index":null,"target_label":null},"label_list":["product","field","location","university","researcher","person","organization","task","programming_language","conference","metric","algorithm","country"]}
{"id":"100","dataset":"crossner_ai","split":"dev","instance":{"id":"100","prompt_labels":"In(O) general(O) ,(O) computational(B-field) linguistics(I-field) draws(O) upon(O) the(O) involvement(O) of(O) linguists(O) ,(O) computer(B-field) science(I-field) ,(O) experts(O) in(O) artificial(B-field) intelligence(I-field) ,(O) mathematicians(O) ,(O) logicians(O) ,(O) philosophers(O) ,(O) cognitive(O) scientists(O) ,(O) cognitive(O) psychologists(O) ,(O) psycholinguists(O) ,(O) anthropologists(O) and(O) neuroscientists(O) ,(O) among(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, algorithm, organization, metric, conference, programming language, researcher, location, person, country, task, product, university and O.\nSentence: In general , computational linguistics draws upon the involvement of linguists , computer science , experts in artificial intelligence , mathematicians , logicians , philosophers , cognitive scientists , cognitive psychologists , psycholinguists , anthropologists and neuroscientists , among others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","general",",","computational","linguistics","draws","upon","the","involvement","of","linguists",",","computer","science",",","experts","in","artificial","intelligence",",","mathematicians",",","logicians",",","philosophers",",","cognitive","scientists",",","cognitive","psychologists",",","psycholinguists",",","anthropologists","and","neuroscientists",",","among","others","."],"labels":["O","O","O","B-field","I-field","O","O","O","O","O","O","O","B-field","I-field","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","algorithm","organization","metric","conference","programming_language","researcher","location","person","country","task","product","university"]}
{"id":"105","dataset":"crossner_ai","split":"dev","instance":{"id":"105","prompt_labels":"For(O) many(O) years(O) starting(O) from(O) 1986(O) ,(O) Miller(B-researcher) directed(O) the(O) development(O) of(O) WordNet(B-product) ,(O) a(O) large(O) computer-readable(O) electronic(O) reference(O) usable(O) in(O) applications(O) such(O) as(O) search(B-product) engines(I-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, university, task, algorithm, conference, organization, field, location, metric, product, programming language, researcher and O.\nSentence: For many years starting from 1986 , Miller directed the development of WordNet , a large computer-readable electronic reference usable in applications such as search engines .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","many","years","starting","from","1986",",","Miller","directed","the","development","of","WordNet",",","a","large","computer-readable","electronic","reference","usable","in","applications","such","as","search","engines","."],"labels":["O","O","O","O","O","O","O","B-researcher","O","O","O","O","B-product","O","O","O","O","O","O","O","O","O","O","O","B-product","I-product","O"],"target_index":null,"target_label":null},"label_list":["country","person","university","task","algorithm","conference","organization","field","location","metric","product","programming_language","researcher"]}
{"id":"106","dataset":"crossner_ai","split":"dev","instance":{"id":"106","prompt_labels":"Since(O) 2009(O) ,(O) the(O) recurrent(B-algorithm) neural(I-algorithm) network(I-algorithm) s(O) and(O) deep(B-algorithm) feedforward(I-algorithm) neural(I-algorithm) networks(I-algorithm) developed(O) in(O) the(O) research(O) group(O) of(O) Jrgen(B-researcher) Schmidhuber(I-researcher) at(O) the(O) Swiss(B-organization) AI(I-organization) Lab(I-organization) IDSIA(I-organization) have(O) won(O) several(O) international(O) handwriting(O) competitions(O) ..(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, conference, researcher, university, programming language, algorithm, country, organization, product, person, task, field, location and O.\nSentence: Since 2009 , the recurrent neural network s and deep feedforward neural networks developed in the research group of Jrgen Schmidhuber at the Swiss AI Lab IDSIA have won several international handwriting competitions ..","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Since","2009",",","the","recurrent","neural","network","s","and","deep","feedforward","neural","networks","developed","in","the","research","group","of","Jrgen","Schmidhuber","at","the","Swiss","AI","Lab","IDSIA","have","won","several","international","handwriting","competitions",".."],"labels":["O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","B-algorithm","I-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","B-researcher","I-researcher","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","conference","researcher","university","programming_language","algorithm","country","organization","product","person","task","field","location"]}
{"id":"107","dataset":"crossner_ai","split":"dev","instance":{"id":"107","prompt_labels":"The(O) software(O) is(O) implemented(O) in(O) C(B-programming language) +(I-programming language) +(I-programming language) and(O) it(O) is(O) wrapped(O) for(O) Python(B-programming language) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, product, metric, programming language, organization, country, researcher, algorithm, person, location, university, field, conference and O.\nSentence: The software is implemented in C + + and it is wrapped for Python .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","software","is","implemented","in","C","+","+","and","it","is","wrapped","for","Python","."],"labels":["O","O","O","O","O","B-programming language","I-programming language","I-programming language","O","O","O","O","O","B-programming language","O"],"target_index":null,"target_label":null},"label_list":["task","product","metric","programming_language","organization","country","researcher","algorithm","person","location","university","field","conference"]}
{"id":"108","dataset":"crossner_ai","split":"dev","instance":{"id":"108","prompt_labels":"In(O) 1857(O) ,(O) at(O) the(O) request(O) of(O) the(O) Tokugawa(B-country) Shogunate(I-country) ,(O) a(O) group(O) of(O) Dutch(O) engineers(O) began(O) work(O) on(O) the(O) Nagasaki(O) Yotetsusho(O) ,(O) a(O) modern(O) ,(O) Western-style(O) foundry(O) and(O) shipyard(O) near(O) the(O) Dutch(O) settlement(O) of(O) Dejima(O) ,(O) at(O) Nagasaki(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, field, metric, algorithm, university, researcher, person, product, task, programming language, country, conference, location and O.\nSentence: In 1857 , at the request of the Tokugawa Shogunate , a group of Dutch engineers began work on the Nagasaki Yotetsusho , a modern , Western-style foundry and shipyard near the Dutch settlement of Dejima , at Nagasaki .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1857",",","at","the","request","of","the","Tokugawa","Shogunate",",","a","group","of","Dutch","engineers","began","work","on","the","Nagasaki","Yotetsusho",",","a","modern",",","Western-style","foundry","and","shipyard","near","the","Dutch","settlement","of","Dejima",",","at","Nagasaki","."],"labels":["O","O","O","O","O","O","O","O","B-country","I-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","field","metric","algorithm","university","researcher","person","product","task","programming_language","country","conference","location"]}
{"id":"109","dataset":"crossner_ai","split":"dev","instance":{"id":"109","prompt_labels":"We(O) make(O) as(O) well(O) as(O) possible(O) precise(O) by(O) measuring(O) the(O) mean(B-metric) squared(I-metric) error(I-metric) between(O) mathy(O) /(O) math(O) and(O) math(O) \\(O) hat(O) {(O) f(O) }(O) ((O) x(O) ;(O) D(O) )(O) /(O) math(O) :(O) we(O) want(O) math(O) ((O) y(O) -(O) \\(O) hat(O) {(O) f(O) }(O) ((O) x(O) ;(O) D(O) )(O) )(O) ^(O) 2(O) /(O) math(O) to(O) be(O) minimal(O) ,(O) both(O) for(O) mathx(O) _(O) 1(O) ,(O) \\(O) dots(O) ,(O) x(O) _(O) n(O) /(O) math(O) and(O) for(O) points(O) outside(O) of(O) our(O) sample(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, university, task, location, researcher, conference, organization, field, algorithm, country, programming language, person, product and O.\nSentence: We make as well as possible precise by measuring the mean squared error between mathy / math and math \\ hat { f } ( x ; D ) / math : we want math ( y - \\ hat { f } ( x ; D ) ) ^ 2 / math to be minimal , both for mathx _ 1 , \\ dots , x _ n / math and for points outside of our sample .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["We","make","as","well","as","possible","precise","by","measuring","the","mean","squared","error","between","mathy","/","math","and","math","\\","hat","{","f","}","(","x",";","D",")","/","math",":","we","want","math","(","y","-","\\","hat","{","f","}","(","x",";","D",")",")","^","2","/","math","to","be","minimal",",","both","for","mathx","_","1",",","\\","dots",",","x","_","n","/","math","and","for","points","outside","of","our","sample","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","university","task","location","researcher","conference","organization","field","algorithm","country","programming_language","person","product"]}
{"id":"113","dataset":"crossner_ai","split":"dev","instance":{"id":"113","prompt_labels":")(O) In(O) addition(O) to(O) the(O) taxonomic(O) information(O) contained(O) in(O) OpenCyc(B-product) ,(O) ResearchCyc(B-product) includes(O) significantly(O) more(O) semantic(O) knowledge(O) ((O) i.e.(O) ,(O) additional(O) facts(O) and(O) rules(O) of(O) thumb(O) )(O) involving(O) the(O) concepts(O) in(O) its(O) knowledge(O) base(O) ;(O) it(O) also(O) includes(O) a(O) large(B-product) lexicon(I-product) ,(I-product) English(I-product) parsing(I-product) and(I-product) generation(I-product) tools(I-product) ,(O) and(O) Java(B-programming language) based(O) interfaces(B-product) for(I-product) knowledge(I-product) editing(I-product) and(I-product) querying(I-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, location, programming language, organization, algorithm, person, metric, country, researcher, field, conference, product, university and O.\nSentence: ) In addition to the taxonomic information contained in OpenCyc , ResearchCyc includes significantly more semantic knowledge ( i.e. , additional facts and rules of thumb ) involving the concepts in its knowledge base ; it also includes a large lexicon , English parsing and generation tools , and Java based interfaces for knowledge editing and querying .","prediction_output":null,"prediction_outputs":null,"group":null,"words":[")","In","addition","to","the","taxonomic","information","contained","in","OpenCyc",",","ResearchCyc","includes","significantly","more","semantic","knowledge","(","i.e.",",","additional","facts","and","rules","of","thumb",")","involving","the","concepts","in","its","knowledge","base",";","it","also","includes","a","large","lexicon",",","English","parsing","and","generation","tools",",","and","Java","based","interfaces","for","knowledge","editing","and","querying","."],"labels":["O","O","O","O","O","O","O","O","O","B-product","O","B-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","I-product","I-product","I-product","I-product","I-product","I-product","I-product","O","O","B-programming language","O","B-product","I-product","I-product","I-product","I-product","I-product","O"],"target_index":null,"target_label":null},"label_list":["task","location","programming_language","organization","algorithm","person","metric","country","researcher","field","conference","product","university"]}
{"id":"114","dataset":"crossner_ai","split":"dev","instance":{"id":"114","prompt_labels":"The(O) Hough(B-algorithm) transform(I-algorithm) is(O) a(O) feature(B-task) extraction(I-task) technique(O) used(O) in(O) image(B-field) analysis(I-field) ,(O) computer(B-field) vision(I-field) ,(O) and(O) digital(B-field) image(I-field) processing(I-field) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, field, country, location, product, person, algorithm, metric, organization, university, conference, task, researcher and O.\nSentence: The Hough transform is a feature extraction technique used in image analysis , computer vision , and digital image processing .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Hough","transform","is","a","feature","extraction","technique","used","in","image","analysis",",","computer","vision",",","and","digital","image","processing","."],"labels":["O","B-algorithm","I-algorithm","O","O","B-task","I-task","O","O","O","B-field","I-field","O","B-field","I-field","O","O","B-field","I-field","I-field","O"],"target_index":null,"target_label":null},"label_list":["programming_language","field","country","location","product","person","algorithm","metric","organization","university","conference","task","researcher"]}
{"id":"116","dataset":"crossner_ai","split":"dev","instance":{"id":"116","prompt_labels":"LSTM(B-algorithm) was(O) proposed(O) in(O) 1997(O) by(O) Sepp(B-researcher) Hochreiter(I-researcher) and(O) Jrgen(B-researcher) Schmidhuber(I-researcher) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, programming language, product, metric, person, algorithm, country, task, university, location, organization, conference, researcher and O.\nSentence: LSTM was proposed in 1997 by Sepp Hochreiter and Jrgen Schmidhuber .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["LSTM","was","proposed","in","1997","by","Sepp","Hochreiter","and","Jrgen","Schmidhuber","."],"labels":["B-algorithm","O","O","O","O","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O"],"target_index":null,"target_label":null},"label_list":["field","programming_language","product","metric","person","algorithm","country","task","university","location","organization","conference","researcher"]}
{"id":"118","dataset":"crossner_ai","split":"dev","instance":{"id":"118","prompt_labels":"He(O) also(O) contributed(O) much(O) through(O) the(O) establishment(O) of(O) ELRA(B-conference) and(O) the(O) LREC(B-conference) conference(I-conference) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, programming language, location, product, conference, task, field, algorithm, person, organization, country, metric, researcher and O.\nSentence: He also contributed much through the establishment of ELRA and the LREC conference .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","also","contributed","much","through","the","establishment","of","ELRA","and","the","LREC","conference","."],"labels":["O","O","O","O","O","O","O","O","B-conference","O","O","B-conference","I-conference","O"],"target_index":null,"target_label":null},"label_list":["university","programming_language","location","product","conference","task","field","algorithm","person","organization","country","metric","researcher"]}
{"id":"122","dataset":"crossner_ai","split":"dev","instance":{"id":"122","prompt_labels":"The(O) robot(O) kit(O) is(O) Android-based(O) ,(O) and(O) it(O) is(O) programmed(O) using(O) Java(B-programming language) ,(O) the(O) Blocks(O) programming(O) interface(O) ,(O) or(O) other(O) Android(B-product) programming(I-product) systems(I-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, location, task, country, product, university, organization, programming language, field, algorithm, person, metric, conference and O.\nSentence: The robot kit is Android-based , and it is programmed using Java , the Blocks programming interface , or other Android programming systems .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","robot","kit","is","Android-based",",","and","it","is","programmed","using","Java",",","the","Blocks","programming","interface",",","or","other","Android","programming","systems","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-programming language","O","O","O","O","O","O","O","O","B-product","I-product","I-product","O"],"target_index":null,"target_label":null},"label_list":["researcher","location","task","country","product","university","organization","programming_language","field","algorithm","person","metric","conference"]}
{"id":"123","dataset":"crossner_ai","split":"dev","instance":{"id":"123","prompt_labels":"The(O) method(O) of(O) defining(O) the(O) linked(O) list(O) specifies(O) the(O) use(O) of(O) a(O) depth-first(B-algorithm) search(I-algorithm) or(O) a(O) breadth-first(B-algorithm) search(I-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, country, organization, location, researcher, field, person, conference, algorithm, metric, product, university, task and O.\nSentence: The method of defining the linked list specifies the use of a depth-first search or a breadth-first search .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","method","of","defining","the","linked","list","specifies","the","use","of","a","depth-first","search","or","a","breadth-first","search","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","B-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["programming_language","country","organization","location","researcher","field","person","conference","algorithm","metric","product","university","task"]}
{"id":"125","dataset":"crossner_ai","split":"dev","instance":{"id":"125","prompt_labels":"An(O) example(O) of(O) a(O) semantic(B-algorithm) network(I-algorithm) is(O) WordNet(B-product) ,(O) a(O) lexical(O) database(O) of(O) English(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, algorithm, metric, country, programming language, researcher, location, conference, university, field, person, product, organization and O.\nSentence: An example of a semantic network is WordNet , a lexical database of English .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["An","example","of","a","semantic","network","is","WordNet",",","a","lexical","database","of","English","."],"labels":["O","O","O","O","B-algorithm","I-algorithm","O","B-product","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["task","algorithm","metric","country","programming_language","researcher","location","conference","university","field","person","product","organization"]}
{"id":"126","dataset":"crossner_ai","split":"dev","instance":{"id":"126","prompt_labels":"Speech(B-task) recognition(I-task) is(O) an(O) interdisciplinary(O) subfield(O) of(O) computer(B-field) science(I-field) and(O) computational(B-field) linguistics(I-field) that(O) develops(O) methodologies(O) and(O) technologies(O) that(O) enable(O) the(O) recognition(B-task) and(I-task) translation(I-task) of(I-task) spoken(I-task) language(I-task) into(O) text(O) by(O) computers(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, metric, researcher, programming language, location, task, country, product, person, university, conference, organization, algorithm and O.\nSentence: Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Speech","recognition","is","an","interdisciplinary","subfield","of","computer","science","and","computational","linguistics","that","develops","methodologies","and","technologies","that","enable","the","recognition","and","translation","of","spoken","language","into","text","by","computers","."],"labels":["B-task","I-task","O","O","O","O","O","B-field","I-field","O","B-field","I-field","O","O","O","O","O","O","O","O","B-task","I-task","I-task","I-task","I-task","I-task","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","metric","researcher","programming_language","location","task","country","product","person","university","conference","organization","algorithm"]}
{"id":"129","dataset":"crossner_ai","split":"dev","instance":{"id":"129","prompt_labels":"He(O) was(O) elected(O) to(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) and(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) and(O) has(O) received(O) a(O) series(O) of(O) awards(O) :(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, programming language, country, organization, conference, university, person, product, task, metric, researcher, location, algorithm and O.\nSentence: He was elected to the American Academy of Arts and Sciences and the National Academy of Sciences and has received a series of awards :","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","elected","to","the","American","Academy","of","Arts","and","Sciences","and","the","National","Academy","of","Sciences","and","has","received","a","series","of","awards",":"],"labels":["O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","programming_language","country","organization","conference","university","person","product","task","metric","researcher","location","algorithm"]}
{"id":"131","dataset":"crossner_ai","split":"dev","instance":{"id":"131","prompt_labels":"Where(O) BLEU(B-metric) simply(O) calculates(O) n-gram(B-metric) precision(I-metric) adding(O) equal(O) weight(O) to(O) each(O) one(O) ,(O) NIST(B-metric) also(O) calculates(O) how(O) informative(O) a(O) particular(O) n-gram(O) is(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, product, country, metric, university, algorithm, programming language, organization, field, location, researcher, conference, person and O.\nSentence: Where BLEU simply calculates n-gram precision adding equal weight to each one , NIST also calculates how informative a particular n-gram is .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Where","BLEU","simply","calculates","n-gram","precision","adding","equal","weight","to","each","one",",","NIST","also","calculates","how","informative","a","particular","n-gram","is","."],"labels":["O","B-metric","O","O","B-metric","I-metric","O","O","O","O","O","O","O","B-metric","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["task","product","country","metric","university","algorithm","programming_language","organization","field","location","researcher","conference","person"]}
{"id":"134","dataset":"crossner_ai","split":"dev","instance":{"id":"134","prompt_labels":"The(O) following(O) MATLAB(B-product) code(O) demonstrates(O) a(O) concrete(O) solution(O) for(O) solving(O) the(O) non-linear(O) system(O) of(O) equations(O) presented(O) in(O) the(O) previous(O) section(O) :(O) See(O) also(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, person, researcher, field, location, conference, product, university, algorithm, programming language, country, organization, metric and O.\nSentence: The following MATLAB code demonstrates a concrete solution for solving the non-linear system of equations presented in the previous section : See also","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","following","MATLAB","code","demonstrates","a","concrete","solution","for","solving","the","non-linear","system","of","equations","presented","in","the","previous","section",":","See","also"],"labels":["O","O","B-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["task","person","researcher","field","location","conference","product","university","algorithm","programming_language","country","organization","metric"]}
{"id":"136","dataset":"crossner_ai","split":"dev","instance":{"id":"136","prompt_labels":"It(O) was(O) first(O) used(O) by(O) Lawrence(B-researcher) J.(I-researcher) Fogel(I-researcher) in(O) the(O) US(B-country) in(O) 1960(O) in(O) order(O) to(O) use(O) simulated(O) evolution(O) as(O) a(O) learning(O) process(O) aiming(O) to(O) generate(O) artificial(B-field) intelligence(I-field) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, person, algorithm, programming language, conference, location, product, university, task, country, metric, organization, researcher and O.\nSentence: It was first used by Lawrence J. Fogel in the US in 1960 in order to use simulated evolution as a learning process aiming to generate artificial intelligence .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","first","used","by","Lawrence","J.","Fogel","in","the","US","in","1960","in","order","to","use","simulated","evolution","as","a","learning","process","aiming","to","generate","artificial","intelligence","."],"labels":["O","O","O","O","O","B-researcher","I-researcher","I-researcher","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O"],"target_index":null,"target_label":null},"label_list":["field","person","algorithm","programming_language","conference","location","product","university","task","country","metric","organization","researcher"]}
{"id":"138","dataset":"crossner_ai","split":"dev","instance":{"id":"138","prompt_labels":"In(O) such(O) cases(O) ,(O) cloud(B-field) computing(I-field) and(O) open(O) source(O) programming(O) language(O) R(B-programming language) can(O) help(O) smaller(O) banks(O) to(O) adopt(O) risk(O) analytics(O) and(O) support(O) branch(O) level(O) monitoring(O) by(O) applying(O) predictive(B-field) analytics(I-field) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, metric, product, programming language, organization, university, country, conference, algorithm, researcher, field, person, location and O.\nSentence: In such cases , cloud computing and open source programming language R can help smaller banks to adopt risk analytics and support branch level monitoring by applying predictive analytics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","such","cases",",","cloud","computing","and","open","source","programming","language","R","can","help","smaller","banks","to","adopt","risk","analytics","and","support","branch","level","monitoring","by","applying","predictive","analytics","."],"labels":["O","O","O","O","B-field","I-field","O","O","O","O","O","B-programming language","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O"],"target_index":null,"target_label":null},"label_list":["task","metric","product","programming_language","organization","university","country","conference","algorithm","researcher","field","person","location"]}
{"id":"140","dataset":"crossner_ai","split":"dev","instance":{"id":"140","prompt_labels":"In(O) this(O) process(O) ,(O) which(O) is(O) known(O) as(O) cross-validation(B-algorithm) ,(O) the(O) MSE(B-metric) is(O) often(O) called(O) the(O) mean(B-metric) squared(I-metric) prediction(I-metric) error(I-metric) ,(O) and(O) is(O) computed(O) as(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, algorithm, researcher, university, task, country, conference, field, person, programming language, metric, product and O.\nSentence: In this process , which is known as cross-validation , the MSE is often called the mean squared prediction error , and is computed as","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","this","process",",","which","is","known","as","cross-validation",",","the","MSE","is","often","called","the","mean","squared","prediction","error",",","and","is","computed","as"],"labels":["O","O","O","O","O","O","O","O","B-algorithm","O","O","B-metric","O","O","O","O","B-metric","I-metric","I-metric","I-metric","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","organization","algorithm","researcher","university","task","country","conference","field","person","programming_language","metric","product"]}
{"id":"141","dataset":"crossner_ai","split":"dev","instance":{"id":"141","prompt_labels":"OMR(B-task) is(O) generally(O) distinguished(O) from(O) optical(B-task) character(I-task) recognition(I-task) ((O) OCR(B-task) )(O) by(O) the(O) fact(O) that(O) a(O) complicated(O) pattern(B-field) recognition(I-field) engine(O) is(O) not(O) required(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, task, conference, metric, researcher, person, country, algorithm, programming language, product, university, location, organization and O.\nSentence: OMR is generally distinguished from optical character recognition ( OCR ) by the fact that a complicated pattern recognition engine is not required .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["OMR","is","generally","distinguished","from","optical","character","recognition","(","OCR",")","by","the","fact","that","a","complicated","pattern","recognition","engine","is","not","required","."],"labels":["B-task","O","O","O","O","B-task","I-task","I-task","O","B-task","O","O","O","O","O","O","O","B-field","I-field","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","task","conference","metric","researcher","person","country","algorithm","programming_language","product","university","location","organization"]}
{"id":"142","dataset":"crossner_ai","split":"dev","instance":{"id":"142","prompt_labels":"In(O) 2018(O) and(O) 2019(O) ,(O) the(O) Championship(O) was(O) be(O) held(O) in(O) Houston(B-location) and(O) Detroit(B-location) ,(O) Michigan(B-location) at(O) the(O) TCF(B-location) Center(I-location) and(O) Ford(B-location) Field(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, field, person, conference, organization, metric, product, algorithm, task, programming language, location, university, researcher and O.\nSentence: In 2018 and 2019 , the Championship was be held in Houston and Detroit , Michigan at the TCF Center and Ford Field .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2018","and","2019",",","the","Championship","was","be","held","in","Houston","and","Detroit",",","Michigan","at","the","TCF","Center","and","Ford","Field","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","O","B-location","I-location","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["country","field","person","conference","organization","metric","product","algorithm","task","programming_language","location","university","researcher"]}
{"id":"144","dataset":"crossner_ai","split":"dev","instance":{"id":"144","prompt_labels":"Two(O) examples(O) of(O) popular(O) parallel(B-product) robots(I-product) are(O) the(O) Stewart(B-product) platform(I-product) and(O) the(O) Delta(B-product) robot(I-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, field, task, product, conference, metric, organization, person, country, location, programming language, algorithm, researcher and O.\nSentence: Two examples of popular parallel robots are the Stewart platform and the Delta robot .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Two","examples","of","popular","parallel","robots","are","the","Stewart","platform","and","the","Delta","robot","."],"labels":["O","O","O","O","B-product","I-product","O","O","B-product","I-product","O","O","B-product","I-product","O"],"target_index":null,"target_label":null},"label_list":["university","field","task","product","conference","metric","organization","person","country","location","programming_language","algorithm","researcher"]}
{"id":"145","dataset":"crossner_ai","split":"dev","instance":{"id":"145","prompt_labels":"((O) Nevertheless(O) ,(O) the(O) ReLU(B-algorithm) activation(I-algorithm) function(I-algorithm) ,(O) which(O) is(O) non-differentiable(O) at(O) 0(O) ,(O) has(O) become(O) quite(O) popular(O) ,(O) e.g.(O) in(O) AlexNet(B-algorithm) )(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, conference, university, product, field, organization, researcher, person, task, algorithm, programming language, metric and O.\nSentence: ( Nevertheless , the ReLU activation function , which is non-differentiable at 0 , has become quite popular , e.g. in AlexNet )","prediction_output":null,"prediction_outputs":null,"group":null,"words":["(","Nevertheless",",","the","ReLU","activation","function",",","which","is","non-differentiable","at","0",",","has","become","quite","popular",",","e.g.","in","AlexNet",")"],"labels":["O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","O"],"target_index":null,"target_label":null},"label_list":["location","country","conference","university","product","field","organization","researcher","person","task","algorithm","programming_language","metric"]}
{"id":"149","dataset":"crossner_ai","split":"dev","instance":{"id":"149","prompt_labels":"Since(O) paraphrase(B-task) recognition(I-task) can(O) be(O) posed(O) as(O) a(O) classification(B-task) problem(O) ,(O) most(O) standard(O) evaluations(O) metrics(O) such(O) as(O) accuracy(B-metric) ,(O) f1(B-metric) score(I-metric) ,(O) or(O) an(O) ROC(B-metric) curve(I-metric) do(O) relatively(O) well(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, person, organization, country, programming language, location, task, metric, product, algorithm, university, conference, field and O.\nSentence: Since paraphrase recognition can be posed as a classification problem , most standard evaluations metrics such as accuracy , f1 score , or an ROC curve do relatively well .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Since","paraphrase","recognition","can","be","posed","as","a","classification","problem",",","most","standard","evaluations","metrics","such","as","accuracy",",","f1","score",",","or","an","ROC","curve","do","relatively","well","."],"labels":["O","B-task","I-task","O","O","O","O","O","B-task","O","O","O","O","O","O","O","O","B-metric","O","B-metric","I-metric","O","O","O","B-metric","I-metric","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","person","organization","country","programming_language","location","task","metric","product","algorithm","university","conference","field"]}
{"id":"152","dataset":"crossner_ai","split":"dev","instance":{"id":"152","prompt_labels":"An(O) example(O) of(O) non-linear(O) normalization(O) is(O) when(O) the(O) normalization(O) follows(O) a(O) sigmoid(B-algorithm) function(I-algorithm) ,(O) in(O) that(O) case(O) ,(O) the(O) normalized(O) image(O) is(O) computed(O) according(O) to(O) the(O) formula(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, organization, product, task, researcher, country, programming language, conference, field, university, algorithm, location, person and O.\nSentence: An example of non-linear normalization is when the normalization follows a sigmoid function , in that case , the normalized image is computed according to the formula","prediction_output":null,"prediction_outputs":null,"group":null,"words":["An","example","of","non-linear","normalization","is","when","the","normalization","follows","a","sigmoid","function",",","in","that","case",",","the","normalized","image","is","computed","according","to","the","formula"],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","organization","product","task","researcher","country","programming_language","conference","field","university","algorithm","location","person"]}
{"id":"153","dataset":"crossner_ai","split":"dev","instance":{"id":"153","prompt_labels":"It(O) has(O) been(O) pointed(O) out(O) that(O) precision(B-metric) is(O) usually(O) twinned(O) with(O) recall(B-metric) to(O) overcome(O) this(O) problem(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, university, task, organization, product, algorithm, metric, person, location, conference, programming language, researcher, country and O.\nSentence: It has been pointed out that precision is usually twinned with recall to overcome this problem","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","has","been","pointed","out","that","precision","is","usually","twinned","with","recall","to","overcome","this","problem"],"labels":["O","O","O","O","O","O","B-metric","O","O","O","O","B-metric","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","university","task","organization","product","algorithm","metric","person","location","conference","programming_language","researcher","country"]}
{"id":"157","dataset":"crossner_ai","split":"dev","instance":{"id":"157","prompt_labels":"These(O) Intelligent(O) Chatbots(B-product) make(O) use(O) of(O) all(O) kinds(O) of(O) artificial(B-field) intelligence(I-field) like(O) image(B-task) moderation(I-task) and(O) natural(B-task) language(I-task) understanding(I-task) ((O) NLU(B-task) )(O) ,(O) natural(B-task) language(I-task) generation(I-task) ((O) NLG(B-task) )(O) ,(O) machine(B-field) learning(I-field) and(O) deep(B-field) learning(I-field) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, task, product, university, organization, conference, algorithm, country, location, programming language, field, person, metric and O.\nSentence: These Intelligent Chatbots make use of all kinds of artificial intelligence like image moderation and natural language understanding ( NLU ) , natural language generation ( NLG ) , machine learning and deep learning .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["These","Intelligent","Chatbots","make","use","of","all","kinds","of","artificial","intelligence","like","image","moderation","and","natural","language","understanding","(","NLU",")",",","natural","language","generation","(","NLG",")",",","machine","learning","and","deep","learning","."],"labels":["O","O","B-product","O","O","O","O","O","O","B-field","I-field","O","B-task","I-task","O","B-task","I-task","I-task","O","B-task","O","O","B-task","I-task","I-task","O","B-task","O","O","B-field","I-field","O","B-field","I-field","O"],"target_index":null,"target_label":null},"label_list":["researcher","task","product","university","organization","conference","algorithm","country","location","programming_language","field","person","metric"]}
{"id":"159","dataset":"crossner_ai","split":"dev","instance":{"id":"159","prompt_labels":"The(O) information(O) is(O) a(O) blend(O) of(O) sitemaps(O) and(O) RSS(O) and(O) is(O) created(O) using(O) the(O) Information(B-algorithm) Model(I-algorithm) ((O) IM(B-algorithm) )(O) and(O) Biomedical(B-algorithm) Resource(I-algorithm) Ontology(I-algorithm) ((O) BRO(B-algorithm) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, product, university, programming language, researcher, location, algorithm, field, person, metric, task, country, conference and O.\nSentence: The information is a blend of sitemaps and RSS and is created using the Information Model ( IM ) and Biomedical Resource Ontology ( BRO ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","information","is","a","blend","of","sitemaps","and","RSS","and","is","created","using","the","Information","Model","(","IM",")","and","Biomedical","Resource","Ontology","(","BRO",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","B-algorithm","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","O","O"],"target_index":null,"target_label":null},"label_list":["organization","product","university","programming_language","researcher","location","algorithm","field","person","metric","task","country","conference"]}
{"id":"160","dataset":"crossner_ai","split":"dev","instance":{"id":"160","prompt_labels":"Recent(O) text(B-task) recognition(I-task) is(O) based(O) on(O) Recurrent(B-algorithm) neural(I-algorithm) network(I-algorithm) ((O) Long(B-algorithm) short-term(I-algorithm) memory(I-algorithm) )(O) and(O) does(O) not(O) require(O) a(O) language(B-algorithm) model(I-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, metric, programming language, field, conference, university, task, country, organization, researcher, person, location, algorithm and O.\nSentence: Recent text recognition is based on Recurrent neural network ( Long short-term memory ) and does not require a language model .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Recent","text","recognition","is","based","on","Recurrent","neural","network","(","Long","short-term","memory",")","and","does","not","require","a","language","model","."],"labels":["O","B-task","I-task","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","B-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["product","metric","programming_language","field","conference","university","task","country","organization","researcher","person","location","algorithm"]}
{"id":"161","dataset":"crossner_ai","split":"dev","instance":{"id":"161","prompt_labels":"Popular(O) loss(O) functions(O) include(O) the(O) hinge(B-metric) loss(I-metric) ((O) for(O) linear(B-algorithm) SVMs(I-algorithm) )(O) and(O) the(O) log(B-metric) loss(I-metric) ((O) for(O) logistic(B-algorithm) regression(I-algorithm) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, task, researcher, university, organization, location, metric, product, country, field, person, algorithm, programming language and O.\nSentence: Popular loss functions include the hinge loss ( for linear SVMs ) and the log loss ( for logistic regression ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Popular","loss","functions","include","the","hinge","loss","(","for","linear","SVMs",")","and","the","log","loss","(","for","logistic","regression",")","."],"labels":["O","O","O","O","O","B-metric","I-metric","O","O","B-algorithm","I-algorithm","O","O","O","B-metric","I-metric","O","O","B-algorithm","I-algorithm","O","O"],"target_index":null,"target_label":null},"label_list":["conference","task","researcher","university","organization","location","metric","product","country","field","person","algorithm","programming_language"]}
{"id":"162","dataset":"crossner_ai","split":"dev","instance":{"id":"162","prompt_labels":"SSIM(B-metric) is(O) designed(O) to(O) improve(O) on(O) traditional(O) methods(O) such(O) as(O) peak(B-metric) signal-to-noise(I-metric) ratio(I-metric) ((O) PSNR(B-metric) )(O) and(O) mean(B-metric) squared(I-metric) error(I-metric) ((O) MSE(B-metric) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, algorithm, location, researcher, conference, product, university, metric, field, person, task, organization, country and O.\nSentence: SSIM is designed to improve on traditional methods such as peak signal-to-noise ratio ( PSNR ) and mean squared error ( MSE ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["SSIM","is","designed","to","improve","on","traditional","methods","such","as","peak","signal-to-noise","ratio","(","PSNR",")","and","mean","squared","error","(","MSE",")","."],"labels":["B-metric","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","B-metric","O","O","B-metric","I-metric","I-metric","O","B-metric","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","algorithm","location","researcher","conference","product","university","metric","field","person","task","organization","country"]}
{"id":"163","dataset":"crossner_ai","split":"dev","instance":{"id":"163","prompt_labels":"His(O) work(O) inspired(O) subsequent(O) generations(O) of(O) robotics(O) researchers(O) such(O) as(O) Rodney(B-researcher) Brooks(I-researcher) ,(O) Hans(B-researcher) Moravec(I-researcher) and(O) Mark(B-researcher) Tilden(I-researcher) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, algorithm, researcher, programming language, location, field, person, task, organization, university, country, product, metric and O.\nSentence: His work inspired subsequent generations of robotics researchers such as Rodney Brooks , Hans Moravec and Mark Tilden .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","work","inspired","subsequent","generations","of","robotics","researchers","such","as","Rodney","Brooks",",","Hans","Moravec","and","Mark","Tilden","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O"],"target_index":null,"target_label":null},"label_list":["conference","algorithm","researcher","programming_language","location","field","person","task","organization","university","country","product","metric"]}
{"id":"164","dataset":"crossner_ai","split":"dev","instance":{"id":"164","prompt_labels":"Further(O) pulse(O) training(O) is(O) not(O) differentiable(O) ,(O) eliminating(O) backpropagation(B-algorithm) -based(O) training(O) methods(O) like(O) gradient(B-algorithm) descent(I-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, field, metric, person, product, programming language, organization, university, conference, researcher, location, country, algorithm and O.\nSentence: Further pulse training is not differentiable , eliminating backpropagation -based training methods like gradient descent .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Further","pulse","training","is","not","differentiable",",","eliminating","backpropagation","-based","training","methods","like","gradient","descent","."],"labels":["O","O","O","O","O","O","O","O","B-algorithm","O","O","O","O","B-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["task","field","metric","person","product","programming_language","organization","university","conference","researcher","location","country","algorithm"]}
{"id":"165","dataset":"crossner_ai","split":"dev","instance":{"id":"165","prompt_labels":"This(O) relations(O) can(O) be(O) easily(O) represented(O) with(O) a(O) confusion(B-metric) matrix(I-metric) ,(O) a(O) table(O) which(O) describes(O) the(O) accuracy(B-metric) of(O) a(O) classification(B-task) model(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, researcher, country, product, task, person, university, field, conference, organization, programming language, location, algorithm and O.\nSentence: This relations can be easily represented with a confusion matrix , a table which describes the accuracy of a classification model .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","relations","can","be","easily","represented","with","a","confusion","matrix",",","a","table","which","describes","the","accuracy","of","a","classification","model","."],"labels":["O","O","O","O","O","O","O","O","B-metric","I-metric","O","O","O","O","O","O","B-metric","O","O","B-task","O","O"],"target_index":null,"target_label":null},"label_list":["metric","researcher","country","product","task","person","university","field","conference","organization","programming_language","location","algorithm"]}
{"id":"167","dataset":"crossner_ai","split":"dev","instance":{"id":"167","prompt_labels":"During(O) his(O) time(O) at(O) Duke(B-university) ,(O) he(O) worked(O) on(O) an(O) automated(O) crossword(O) solver(O) PROVERB(B-product) ,(O) which(O) won(O) an(O) Outstanding(O) Paper(O) Award(O) in(O) 1999(O) from(O) AAAI(B-conference) and(O) competed(O) in(O) the(O) American(O) Crossword(O) Puzzle(O) Tournament(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, country, researcher, person, task, product, organization, university, conference, field, programming language, location, metric and O.\nSentence: During his time at Duke , he worked on an automated crossword solver PROVERB , which won an Outstanding Paper Award in 1999 from AAAI and competed in the American Crossword Puzzle Tournament .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","his","time","at","Duke",",","he","worked","on","an","automated","crossword","solver","PROVERB",",","which","won","an","Outstanding","Paper","Award","in","1999","from","AAAI","and","competed","in","the","American","Crossword","Puzzle","Tournament","."],"labels":["O","O","O","O","B-university","O","O","O","O","O","O","O","O","B-product","O","O","O","O","O","O","O","O","O","O","B-conference","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["algorithm","country","researcher","person","task","product","organization","university","conference","field","programming_language","location","metric"]}
{"id":"168","dataset":"crossner_ai","split":"dev","instance":{"id":"168","prompt_labels":"Headquartered(O) in(O) Rochester(B-location) Hills(I-location) ,(O) Michigan(B-location) ,(O) the(O) company(O) had(O) 10(O) regional(O) locations(O) in(O) the(B-country) U.S.(I-country) ,(O) Canada(B-country) ,(O) Mexico(B-country) and(O) Brazil(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, algorithm, product, programming language, conference, field, task, researcher, person, university, organization, metric, location and O.\nSentence: Headquartered in Rochester Hills , Michigan , the company had 10 regional locations in the U.S. , Canada , Mexico and Brazil .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Headquartered","in","Rochester","Hills",",","Michigan",",","the","company","had","10","regional","locations","in","the","U.S.",",","Canada",",","Mexico","and","Brazil","."],"labels":["O","O","B-location","I-location","O","B-location","O","O","O","O","O","O","O","O","B-country","I-country","O","B-country","O","B-country","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["country","algorithm","product","programming_language","conference","field","task","researcher","person","university","organization","metric","location"]}
{"id":"172","dataset":"crossner_ai","split":"dev","instance":{"id":"172","prompt_labels":"The(O) Apple(B-product) iOS(I-product) operating(I-product) system(I-product) used(O) on(O) the(O) iPhone(B-product) ,(O) iPad(B-product) and(O) iPod(B-product) Touch(I-product) uses(O) VoiceOver(B-product) speech(I-product) synthesis(I-product) accessibility(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, programming language, location, algorithm, conference, researcher, task, person, university, product, field, country, metric and O.\nSentence: The Apple iOS operating system used on the iPhone , iPad and iPod Touch uses VoiceOver speech synthesis accessibility .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Apple","iOS","operating","system","used","on","the","iPhone",",","iPad","and","iPod","Touch","uses","VoiceOver","speech","synthesis","accessibility","."],"labels":["O","B-product","I-product","I-product","I-product","O","O","O","B-product","O","B-product","O","B-product","I-product","O","B-product","I-product","I-product","O","O"],"target_index":null,"target_label":null},"label_list":["organization","programming_language","location","algorithm","conference","researcher","task","person","university","product","field","country","metric"]}
{"id":"174","dataset":"crossner_ai","split":"dev","instance":{"id":"174","prompt_labels":"This(O) is(O) done(O) using(O) standard(O) neural(O) net(O) training(O) algorithms(O) such(O) as(O) stochastic(B-algorithm) gradient(I-algorithm) descent(I-algorithm) with(O) backpropagation(B-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, organization, metric, university, task, person, country, programming language, location, conference, product, algorithm, researcher and O.\nSentence: This is done using standard neural net training algorithms such as stochastic gradient descent with backpropagation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","is","done","using","standard","neural","net","training","algorithms","such","as","stochastic","gradient","descent","with","backpropagation","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","O"],"target_index":null,"target_label":null},"label_list":["field","organization","metric","university","task","person","country","programming_language","location","conference","product","algorithm","researcher"]}
{"id":"178","dataset":"crossner_ai","split":"dev","instance":{"id":"178","prompt_labels":"Decision(B-algorithm) tree(I-algorithm) learning(I-algorithm) ,(O) neural(B-algorithm) networks(I-algorithm) ,(O) or(O) a(O) naive(B-algorithm) Bayes(I-algorithm) classifier(I-algorithm) could(O) be(O) used(O) in(O) combination(O) with(O) measures(O) of(O) model(O) quality(O) such(O) as(O) balanced(B-metric) accuracy(I-metric)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, person, researcher, task, country, programming language, field, university, product, organization, metric, algorithm, location and O.\nSentence: Decision tree learning , neural networks , or a naive Bayes classifier could be used in combination with measures of model quality such as balanced accuracy","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Decision","tree","learning",",","neural","networks",",","or","a","naive","Bayes","classifier","could","be","used","in","combination","with","measures","of","model","quality","such","as","balanced","accuracy"],"labels":["B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","I-algorithm","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric"],"target_index":null,"target_label":null},"label_list":["conference","person","researcher","task","country","programming_language","field","university","product","organization","metric","algorithm","location"]}
{"id":"181","dataset":"crossner_ai","split":"dev","instance":{"id":"181","prompt_labels":"In(O) information(B-field) theory(I-field) and(O) computer(B-field) science(I-field) ,(O) a(O) code(O) is(O) usually(O) considered(O) as(O) an(O) algorithm(O) that(O) uniquely(O) represents(O) symbols(O) from(O) some(O) source(O) alphabet(O) ,(O) by(O) encoded(O) strings(O) ,(O) which(O) may(O) be(O) in(O) some(O) other(O) target(O) alphabet(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, researcher, task, conference, algorithm, university, programming language, person, country, metric, organization, location, field and O.\nSentence: In information theory and computer science , a code is usually considered as an algorithm that uniquely represents symbols from some source alphabet , by encoded strings , which may be in some other target alphabet .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","information","theory","and","computer","science",",","a","code","is","usually","considered","as","an","algorithm","that","uniquely","represents","symbols","from","some","source","alphabet",",","by","encoded","strings",",","which","may","be","in","some","other","target","alphabet","."],"labels":["O","B-field","I-field","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["product","researcher","task","conference","algorithm","university","programming_language","person","country","metric","organization","location","field"]}
{"id":"182","dataset":"crossner_ai","split":"dev","instance":{"id":"182","prompt_labels":"A(O) fairly(O) simple(O) non-linear(O) function(O) ,(O) the(O) sigmoid(B-algorithm) function(I-algorithm) such(O) as(O) the(O) logistic(B-algorithm) function(I-algorithm) also(O) has(O) an(O) easily(O) calculated(O) derivative(O) ,(O) which(O) can(O) be(O) important(O) when(O) calculating(O) the(O) weight(O) updates(O) in(O) the(O) network(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, field, person, product, country, metric, location, task, organization, algorithm, university, conference, programming language and O.\nSentence: A fairly simple non-linear function , the sigmoid function such as the logistic function also has an easily calculated derivative , which can be important when calculating the weight updates in the network .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","fairly","simple","non-linear","function",",","the","sigmoid","function","such","as","the","logistic","function","also","has","an","easily","calculated","derivative",",","which","can","be","important","when","calculating","the","weight","updates","in","the","network","."],"labels":["O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","field","person","product","country","metric","location","task","organization","algorithm","university","conference","programming_language"]}
{"id":"183","dataset":"crossner_ai","split":"dev","instance":{"id":"183","prompt_labels":"apek(B-person) was(O) born(O) in(O) Hronov(B-location) ,(O) Bohemia(B-location) ((O) Austria-Hungary(B-country) ,(O) later(O) Czechoslovakia(B-country) ,(O) now(O) the(O) Czech(B-country) Republic(I-country) )(O) in(O) 1887(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, product, conference, university, task, country, metric, researcher, programming language, person, organization, location, algorithm and O.\nSentence: apek was born in Hronov , Bohemia ( Austria-Hungary , later Czechoslovakia , now the Czech Republic ) in 1887 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["apek","was","born","in","Hronov",",","Bohemia","(","Austria-Hungary",",","later","Czechoslovakia",",","now","the","Czech","Republic",")","in","1887","."],"labels":["B-person","O","O","O","B-location","O","B-location","O","B-country","O","O","B-country","O","O","O","B-country","I-country","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","product","conference","university","task","country","metric","researcher","programming_language","person","organization","location","algorithm"]}
{"id":"185","dataset":"crossner_ai","split":"dev","instance":{"id":"185","prompt_labels":"Aspects(O) of(O) ontology(O) editors(O) include(O) :(O) visual(B-task) navigation(I-task) possibilities(O) within(O) the(O) knowledge(B-task) model(I-task) ,(O) inference(B-task) engine(I-task) s(O) and(O) extraction(B-task) ;(O) support(B-task) for(I-task) modules(I-task) ;(O) the(O) import(O) and(O) export(O) of(O) foreign(O) knowledge(B-task) representation(I-task) languages(O) for(O) ontology(B-task) matching(I-task) ;(O) and(O) the(O) support(O) of(O) meta-ontologies(B-task) such(O) as(O) OWL-S(B-product) ,(O) Dublin(B-product) Core(I-product) ,(O) etc(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, location, programming language, task, field, country, person, algorithm, metric, organization, product, conference, university and O.\nSentence: Aspects of ontology editors include : visual navigation possibilities within the knowledge model , inference engine s and extraction ; support for modules ; the import and export of foreign knowledge representation languages for ontology matching ; and the support of meta-ontologies such as OWL-S , Dublin Core , etc .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Aspects","of","ontology","editors","include",":","visual","navigation","possibilities","within","the","knowledge","model",",","inference","engine","s","and","extraction",";","support","for","modules",";","the","import","and","export","of","foreign","knowledge","representation","languages","for","ontology","matching",";","and","the","support","of","meta-ontologies","such","as","OWL-S",",","Dublin","Core",",","etc","."],"labels":["O","O","O","O","O","O","B-task","I-task","O","O","O","B-task","I-task","O","B-task","I-task","O","O","B-task","O","B-task","I-task","I-task","O","O","O","O","O","O","O","B-task","I-task","O","O","B-task","I-task","O","O","O","O","O","B-task","O","O","B-product","O","B-product","I-product","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","location","programming_language","task","field","country","person","algorithm","metric","organization","product","conference","university"]}
{"id":"186","dataset":"crossner_ai","split":"dev","instance":{"id":"186","prompt_labels":"The(O) FBI(B-organization) has(O) also(O) instituted(O) its(O) Next(O) Generation(O) Identification(O) program(O) to(O) include(O) face(B-task) recognition(I-task) ,(O) as(O) well(O) as(O) more(O) traditional(O) biometrics(B-field) like(O) fingerprints(O) and(O) iris(O) scans(O) ,(O) which(O) can(O) pull(O) from(O) both(O) criminal(O) and(O) civil(O) databases(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, conference, organization, product, person, algorithm, location, field, task, programming language, country, researcher, university and O.\nSentence: The FBI has also instituted its Next Generation Identification program to include face recognition , as well as more traditional biometrics like fingerprints and iris scans , which can pull from both criminal and civil databases .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","FBI","has","also","instituted","its","Next","Generation","Identification","program","to","include","face","recognition",",","as","well","as","more","traditional","biometrics","like","fingerprints","and","iris","scans",",","which","can","pull","from","both","criminal","and","civil","databases","."],"labels":["O","B-organization","O","O","O","O","O","O","O","O","O","O","B-task","I-task","O","O","O","O","O","O","B-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","conference","organization","product","person","algorithm","location","field","task","programming_language","country","researcher","university"]}
{"id":"187","dataset":"crossner_ai","split":"dev","instance":{"id":"187","prompt_labels":"For(O) the(O) 2016(O) season(O) ,(O) Samantha(B-person) Ponder(I-person) was(O) added(O) as(O) host(O) ,(O) replacing(O) Molly(B-person) McGrath(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, algorithm, country, metric, researcher, task, location, person, university, field, conference, programming language, product and O.\nSentence: For the 2016 season , Samantha Ponder was added as host , replacing Molly McGrath .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","the","2016","season",",","Samantha","Ponder","was","added","as","host",",","replacing","Molly","McGrath","."],"labels":["O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["organization","algorithm","country","metric","researcher","task","location","person","university","field","conference","programming_language","product"]}
{"id":"191","dataset":"crossner_ai","split":"dev","instance":{"id":"191","prompt_labels":"Apple(B-organization) Inc(I-organization) introduced(O) Face(B-product) ID(I-product) on(O) the(O) flagship(O) iPhone(B-product) X(I-product) as(O) a(O) biometric(O) authentication(O) successor(O) to(O) the(O) Touch(B-product) ID(I-product) ,(O) a(O) fingerprint(O) based(O) system(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, algorithm, country, person, programming language, organization, task, conference, field, metric, product, university, location and O.\nSentence: Apple Inc introduced Face ID on the flagship iPhone X as a biometric authentication successor to the Touch ID , a fingerprint based system .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Apple","Inc","introduced","Face","ID","on","the","flagship","iPhone","X","as","a","biometric","authentication","successor","to","the","Touch","ID",",","a","fingerprint","based","system","."],"labels":["B-organization","I-organization","O","B-product","I-product","O","O","O","B-product","I-product","O","O","O","O","O","O","O","B-product","I-product","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","algorithm","country","person","programming_language","organization","task","conference","field","metric","product","university","location"]}
{"id":"192","dataset":"crossner_ai","split":"dev","instance":{"id":"192","prompt_labels":"Or(O) combine(O) the(O) F-measure(B-metric) with(O) the(O) R-square(B-metric) evaluated(O) for(O) the(O) raw(O) model(O) output(O) and(O) the(O) target(O) ;(O) or(O) the(O) cost(B-metric) /(I-metric) gain(I-metric) matrix(I-metric) with(O) the(O) correlation(B-metric) coefficient(I-metric) ,(O) and(O) so(O) on(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, metric, location, university, conference, product, researcher, task, organization, algorithm, person, country, field and O.\nSentence: Or combine the F-measure with the R-square evaluated for the raw model output and the target ; or the cost / gain matrix with the correlation coefficient , and so on .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Or","combine","the","F-measure","with","the","R-square","evaluated","for","the","raw","model","output","and","the","target",";","or","the","cost","/","gain","matrix","with","the","correlation","coefficient",",","and","so","on","."],"labels":["O","O","O","B-metric","O","O","B-metric","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","I-metric","O","O","B-metric","I-metric","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","metric","location","university","conference","product","researcher","task","organization","algorithm","person","country","field"]}
{"id":"197","dataset":"crossner_ai","split":"dev","instance":{"id":"197","prompt_labels":"These(O) systems(O) ,(O) such(O) as(O) Siri(B-product) of(O) the(O) iOS(B-product) operating(I-product) system(I-product) ,(O) operate(O) on(O) a(O) similar(O) pattern-recognizing(O) technique(O) as(O) that(O) of(O) text-based(O) systems(O) ,(O) but(O) with(O) the(O) former(O) ,(O) the(O) user(O) input(O) is(O) conducted(O) through(O) speech(B-task) recognition(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, programming language, location, task, researcher, person, organization, university, product, country, conference, algorithm, field and O.\nSentence: These systems , such as Siri of the iOS operating system , operate on a similar pattern-recognizing technique as that of text-based systems , but with the former , the user input is conducted through speech recognition .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["These","systems",",","such","as","Siri","of","the","iOS","operating","system",",","operate","on","a","similar","pattern-recognizing","technique","as","that","of","text-based","systems",",","but","with","the","former",",","the","user","input","is","conducted","through","speech","recognition","."],"labels":["O","O","O","O","O","B-product","O","O","B-product","I-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["metric","programming_language","location","task","researcher","person","organization","university","product","country","conference","algorithm","field"]}
{"id":"198","dataset":"crossner_ai","split":"dev","instance":{"id":"198","prompt_labels":"More(O) exotic(B-algorithm) fitness(I-algorithm) functions(I-algorithm) that(O) explore(O) model(O) granularity(O) include(O) the(O) area(O) under(O) the(O) ROC(B-metric) curve(I-metric) and(O) rank(O) measure(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, product, algorithm, conference, programming language, country, metric, organization, location, university, field, person, researcher and O.\nSentence: More exotic fitness functions that explore model granularity include the area under the ROC curve and rank measure .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["More","exotic","fitness","functions","that","explore","model","granularity","include","the","area","under","the","ROC","curve","and","rank","measure","."],"labels":["O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","B-metric","I-metric","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["task","product","algorithm","conference","programming_language","country","metric","organization","location","university","field","person","researcher"]}
{"id":"199","dataset":"crossner_ai","split":"dev","instance":{"id":"199","prompt_labels":"The(O) term(O) Semantic(B-product) Web(I-product) was(O) coined(O) by(O) Tim(B-researcher) Berners-Lee(I-researcher) ,(O) the(O) inventor(O) of(O) the(O) World(B-product) Wide(I-product) Web(I-product) and(O) director(O) of(O) the(O) World(B-organization) Wide(I-organization) Web(I-organization) Consortium(I-organization) ((O) W3C(B-organization) )(O) ,(O) which(O) oversees(O) the(O) development(O) of(O) proposed(O) Semantic(B-product) Web(I-product) standards(I-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, person, organization, product, country, location, researcher, metric, task, field, conference, programming language, algorithm and O.\nSentence: The term Semantic Web was coined by Tim Berners-Lee , the inventor of the World Wide Web and director of the World Wide Web Consortium ( W3C ) , which oversees the development of proposed Semantic Web standards .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","term","Semantic","Web","was","coined","by","Tim","Berners-Lee",",","the","inventor","of","the","World","Wide","Web","and","director","of","the","World","Wide","Web","Consortium","(","W3C",")",",","which","oversees","the","development","of","proposed","Semantic","Web","standards","."],"labels":["O","O","B-product","I-product","O","O","O","B-researcher","I-researcher","O","O","O","O","O","B-product","I-product","I-product","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","O","O","B-product","I-product","I-product","O"],"target_index":null,"target_label":null},"label_list":["university","person","organization","product","country","location","researcher","metric","task","field","conference","programming_language","algorithm"]}
{"id":"0","dataset":"crossner_literature","split":"dev","instance":{"id":"0","prompt_labels":"In(O) 1982(O) ,(O) she(O) wrote(O) the(O) novel(B-literary genre) The(B-book) Color(I-book) Purple(I-book) ,(O) for(O) which(O) she(O) won(O) the(O) National(B-award) Book(I-award) Award(I-award) for(I-award) hardcover(I-award) fiction(I-award) ,(O) and(O) the(O) Pulitzer(B-award) Prize(I-award) for(I-award) Fiction(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, location, writer, organization, country, person, literary genre, poem, event, magazine, award and O.\nSentence: In 1982 , she wrote the novel The Color Purple , for which she won the National Book Award for hardcover fiction , and the Pulitzer Prize for Fiction .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1982",",","she","wrote","the","novel","The","Color","Purple",",","for","which","she","won","the","National","Book","Award","for","hardcover","fiction",",","and","the","Pulitzer","Prize","for","Fiction","."],"labels":["O","O","O","O","O","O","B-literary genre","B-book","I-book","I-book","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["book","location","writer","organization","country","person","literary_genre","poem","event","magazine","award"]}
{"id":"1","dataset":"crossner_literature","split":"dev","instance":{"id":"1","prompt_labels":"His(O) most(O) famous(O) work(O) is(O) the(O) versified(O) topographical(O) description(O) of(O) northern(B-location) Norway(I-location) ,(O) Nordlands(B-poem) Trompet(I-poem) ((O) The(B-poem) Trumpet(I-poem) of(I-poem) Nordland(I-poem) )(O) ,(O) and(O) some(O) psalms(O) still(O) in(O) use(O) ,(O) most(O) prominently(O) Herre(B-poem) Gud(I-poem) ,(I-poem) ditt(I-poem) dyre(I-poem) navn(I-poem) og(I-poem) re(I-poem) ((O) Good(B-poem) Lord(I-poem) ,(I-poem) thy(I-poem) precious(I-poem) name(I-poem) and(I-poem) glory(I-poem) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, organization, person, location, literary genre, poem, book, country, writer, magazine and O.\nSentence: His most famous work is the versified topographical description of northern Norway , Nordlands Trompet ( The Trumpet of Nordland ) , and some psalms still in use , most prominently Herre Gud , ditt dyre navn og re ( Good Lord , thy precious name and glory ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","most","famous","work","is","the","versified","topographical","description","of","northern","Norway",",","Nordlands","Trompet","(","The","Trumpet","of","Nordland",")",",","and","some","psalms","still","in","use",",","most","prominently","Herre","Gud",",","ditt","dyre","navn","og","re","(","Good","Lord",",","thy","precious","name","and","glory",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-poem","I-poem","O","B-poem","I-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","B-poem","I-poem","I-poem","I-poem","I-poem","I-poem","I-poem","I-poem","O","B-poem","I-poem","I-poem","I-poem","I-poem","I-poem","I-poem","I-poem","O","O"],"target_index":null,"target_label":null},"label_list":["event","award","organization","person","location","literary_genre","poem","book","country","writer","magazine"]}
{"id":"9","dataset":"crossner_literature","split":"dev","instance":{"id":"9","prompt_labels":"In(O) 2012(O) ,(O) the(O) Nobel(O) Records(O) were(O) opened(O) after(O) 50(O) years(O) and(O) it(O) was(O) revealed(O) that(O) Anouilh(B-writer) was(O) among(O) a(O) shortlist(O) of(O) authors(O) considered(O) for(O) the(O) 1962(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) ,(O) along(O) with(O) John(B-writer) Steinbeck(I-writer) ((O) winner(O) )(O) ,(O) Robert(B-writer) Graves(I-writer) ,(O) Lawrence(B-writer) Durrell(I-writer) and(O) Karen(B-writer) Blixen(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, person, magazine, poem, writer, book, literary genre, award, event, country and O.\nSentence: In 2012 , the Nobel Records were opened after 50 years and it was revealed that Anouilh was among a shortlist of authors considered for the 1962 Nobel Prize in Literature , along with John Steinbeck ( winner ) , Robert Graves , Lawrence Durrell and Karen Blixen .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2012",",","the","Nobel","Records","were","opened","after","50","years","and","it","was","revealed","that","Anouilh","was","among","a","shortlist","of","authors","considered","for","the","1962","Nobel","Prize","in","Literature",",","along","with","John","Steinbeck","(","winner",")",",","Robert","Graves",",","Lawrence","Durrell","and","Karen","Blixen","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O","B-writer","I-writer","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["location","organization","person","magazine","poem","writer","book","literary_genre","award","event","country"]}
{"id":"12","dataset":"crossner_literature","split":"dev","instance":{"id":"12","prompt_labels":"Later(O) in(O) his(O) life(O) ,(O) Ginsberg(B-writer) formed(O) a(O) bridge(O) between(O) the(O) beat(O) movement(O) of(O) the(O) 1950s(O) and(O) the(O) hippie(O) s(O) of(O) the(O) 1960s(O) ,(O) befriending(O) ,(O) among(O) others(O) ,(O) Timothy(B-writer) Leary(I-writer) ,(O) Ken(B-writer) Kesey(I-writer) ,(O) Hunter(B-writer) S.(I-writer) Thompson(I-writer) ,(O) and(O) Bob(B-writer) Dylan(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, magazine, person, location, award, book, country, organization, writer, literary genre, poem and O.\nSentence: Later in his life , Ginsberg formed a bridge between the beat movement of the 1950s and the hippie s of the 1960s , befriending , among others , Timothy Leary , Ken Kesey , Hunter S. Thompson , and Bob Dylan .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Later","in","his","life",",","Ginsberg","formed","a","bridge","between","the","beat","movement","of","the","1950s","and","the","hippie","s","of","the","1960s",",","befriending",",","among","others",",","Timothy","Leary",",","Ken","Kesey",",","Hunter","S.","Thompson",",","and","Bob","Dylan","."],"labels":["O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["event","magazine","person","location","award","book","country","organization","writer","literary_genre","poem"]}
{"id":"14","dataset":"crossner_literature","split":"dev","instance":{"id":"14","prompt_labels":"He(O) is(O) the(O) protagonist(O) of(O) Robert(B-book) Coover(I-book) '(O) s(O) short(B-literary genre) story(I-literary genre) Charlie(B-book) in(I-book) the(I-book) House(I-book) of(I-book) Rue(I-book) ((O) 1980(O) ;(O) reprinted(O) in(O) Coover(B-writer) 's(O) 1987(O) collection(O) A(B-book) Night(I-book) at(I-book) the(I-book) Movies(I-book) )(O) ,(O) and(O) of(O) Glen(B-writer) David(I-writer) Gold(I-writer) '(O) s(O) Sunnyside(B-book) ((O) 2009(O) )(O) ,(O) a(O) historical(B-literary genre) novel(I-literary genre) set(O) in(O) the(O) First(B-event) World(I-event) War(I-event) period(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, country, book, magazine, organization, poem, event, location, writer, literary genre, person and O.\nSentence: He is the protagonist of Robert Coover ' s short story Charlie in the House of Rue ( 1980 ; reprinted in Coover 's 1987 collection A Night at the Movies ) , and of Glen David Gold ' s Sunnyside ( 2009 ) , a historical novel set in the First World War period .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","is","the","protagonist","of","Robert","Coover","'","s","short","story","Charlie","in","the","House","of","Rue","(","1980",";","reprinted","in","Coover","'s","1987","collection","A","Night","at","the","Movies",")",",","and","of","Glen","David","Gold","'","s","Sunnyside","(","2009",")",",","a","historical","novel","set","in","the","First","World","War","period","."],"labels":["O","O","O","O","O","B-book","I-book","O","O","B-literary genre","I-literary genre","B-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","O","B-writer","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-writer","I-writer","I-writer","O","O","B-book","O","O","O","O","O","B-literary genre","I-literary genre","O","O","O","B-event","I-event","I-event","O","O"],"target_index":null,"target_label":null},"label_list":["award","country","book","magazine","organization","poem","event","location","writer","literary_genre","person"]}
{"id":"19","dataset":"crossner_literature","split":"dev","instance":{"id":"19","prompt_labels":"When(O) Davies(B-writer) retired(O) from(O) his(O) position(O) at(O) the(O) university(O) ,(O) his(O) seventh(O) novel(B-literary genre) ,(O) a(O) satire(B-literary genre) of(O) academic(O) life(O) ,(O) The(B-book) Rebel(I-book) Angels(I-book) ((O) 1981(O) )(O) ,(O) was(O) published(O) ,(O) followed(O) by(O) What(B-book) 's(I-book) Bred(I-book) in(I-book) the(I-book) Bone(I-book) ((O) 1985(O) )(O) which(O) was(O) short-listed(O) for(O) the(O) Booker(B-award) Prize(I-award) for(I-award) fiction(I-award) in(O) 1986(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, magazine, poem, book, literary genre, event, person, writer, organization, country, location and O.\nSentence: When Davies retired from his position at the university , his seventh novel , a satire of academic life , The Rebel Angels ( 1981 ) , was published , followed by What 's Bred in the Bone ( 1985 ) which was short-listed for the Booker Prize for fiction in 1986 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["When","Davies","retired","from","his","position","at","the","university",",","his","seventh","novel",",","a","satire","of","academic","life",",","The","Rebel","Angels","(","1981",")",",","was","published",",","followed","by","What","'s","Bred","in","the","Bone","(","1985",")","which","was","short-listed","for","the","Booker","Prize","for","fiction","in","1986","."],"labels":["O","B-writer","O","O","O","O","O","O","O","O","O","O","B-literary genre","O","O","B-literary genre","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","magazine","poem","book","literary_genre","event","person","writer","organization","country","location"]}
{"id":"26","dataset":"crossner_literature","split":"dev","instance":{"id":"26","prompt_labels":"His(O) father(O) Noah(B-person) Webster(I-person) Sr.(I-person) ((O) 1722-1813(O) )(O) was(O) a(O) descendant(O) of(O) Connecticut(B-location) Governor(O) John(B-writer) Webster(I-writer) ;(O) his(O) mother(O) Mercy(B-person) ((I-person) Steele(I-person) )(I-person) Webster(I-person) ((O) 1727-1794(O) )(O) was(O) a(O) descendant(O) of(O) Governor(O) William(B-person) Bradford(I-person) of(O) Plymouth(B-country) Colony(I-country) .(O) Noah(B-person) had(O) two(O) brothers(O) ,(O) Abraham(B-person) ((O) 1751-1831(O) )(O) and(O) Charles(B-person) ((O) b(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, person, event, writer, literary genre, organization, book, magazine, country, award, location and O.\nSentence: His father Noah Webster Sr. ( 1722-1813 ) was a descendant of Connecticut Governor John Webster ; his mother Mercy ( Steele ) Webster ( 1727-1794 ) was a descendant of Governor William Bradford of Plymouth Colony . Noah had two brothers , Abraham ( 1751-1831 ) and Charles ( b .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","father","Noah","Webster","Sr.","(","1722-1813",")","was","a","descendant","of","Connecticut","Governor","John","Webster",";","his","mother","Mercy","(","Steele",")","Webster","(","1727-1794",")","was","a","descendant","of","Governor","William","Bradford","of","Plymouth","Colony",".","Noah","had","two","brothers",",","Abraham","(","1751-1831",")","and","Charles","(","b","."],"labels":["O","O","B-person","I-person","I-person","O","O","O","O","O","O","O","B-location","O","B-writer","I-writer","O","O","O","B-person","I-person","I-person","I-person","I-person","O","O","O","O","O","O","O","O","B-person","I-person","O","B-country","I-country","O","B-person","O","O","O","O","B-person","O","O","O","O","B-person","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","person","event","writer","literary_genre","organization","book","magazine","country","award","location"]}
{"id":"29","dataset":"crossner_literature","split":"dev","instance":{"id":"29","prompt_labels":"Other(O) figures(O) in(O) literature(O) who(O) were(O) strongly(O) influenced(O) by(O) Schopenhauer(B-writer) were(O) Thomas(B-writer) Mann(I-writer) ,(O) Afanasy(B-writer) Fet(I-writer) ,(O) Joris-Karl(B-writer) Huysmans(I-writer) and(O) George(B-writer) Santayana(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, organization, writer, event, person, magazine, literary genre, book, award, poem and O.\nSentence: Other figures in literature who were strongly influenced by Schopenhauer were Thomas Mann , Afanasy Fet , Joris-Karl Huysmans and George Santayana .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","figures","in","literature","who","were","strongly","influenced","by","Schopenhauer","were","Thomas","Mann",",","Afanasy","Fet",",","Joris-Karl","Huysmans","and","George","Santayana","."],"labels":["O","O","O","O","O","O","O","O","O","B-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["location","country","organization","writer","event","person","magazine","literary_genre","book","award","poem"]}
{"id":"30","dataset":"crossner_literature","split":"dev","instance":{"id":"30","prompt_labels":"In(O) addition(O) ,(O) Dogpatch(O) characters(O) were(O) used(O) in(O) national(O) campaigns(O) for(O) the(O) U.S.(B-organization) Treasury(I-organization) ,(O) the(O) Cancer(B-organization) Foundation(I-organization) ,(O) the(O) March(B-organization) of(I-organization) Dimes(I-organization) ,(O) the(O) National(B-organization) Heart(I-organization) Fund(I-organization) ,(O) the(O) Sister(B-organization) Kenny(I-organization) Foundation(I-organization) ,(O) the(O) Boy(B-organization) Scouts(I-organization) of(I-organization) America(I-organization) ,(O) Community(B-organization) Chest(I-organization) ,(O) the(O) National(B-organization) Reading(I-organization) Council(I-organization) ,(O) Minnesota(B-organization) Tuberculosis(I-organization) and(I-organization) Health(I-organization) Association(I-organization) ,(O) Christmas(B-organization) Seals(I-organization) ,(O) the(O) National(B-organization) Amputation(I-organization) Foundation(I-organization) ,(O) and(O) Disabled(B-organization) American(I-organization) Veterans(I-organization) ,(O) among(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, book, event, person, poem, country, magazine, organization, writer, award, location and O.\nSentence: In addition , Dogpatch characters were used in national campaigns for the U.S. Treasury , the Cancer Foundation , the March of Dimes , the National Heart Fund , the Sister Kenny Foundation , the Boy Scouts of America , Community Chest , the National Reading Council , Minnesota Tuberculosis and Health Association , Christmas Seals , the National Amputation Foundation , and Disabled American Veterans , among others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","addition",",","Dogpatch","characters","were","used","in","national","campaigns","for","the","U.S.","Treasury",",","the","Cancer","Foundation",",","the","March","of","Dimes",",","the","National","Heart","Fund",",","the","Sister","Kenny","Foundation",",","the","Boy","Scouts","of","America",",","Community","Chest",",","the","National","Reading","Council",",","Minnesota","Tuberculosis","and","Health","Association",",","Christmas","Seals",",","the","National","Amputation","Foundation",",","and","Disabled","American","Veterans",",","among","others","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","book","event","person","poem","country","magazine","organization","writer","award","location"]}
{"id":"31","dataset":"crossner_literature","split":"dev","instance":{"id":"31","prompt_labels":"According(O) to(O) Entertainment(B-magazine) Weekly(I-magazine) ,(O) Raimi(B-person) had(O) expressed(O) an(O) interest(O) in(O) directing(O) a(O) film(O) version(O) of(O) The(B-book) Hobbit(I-book) ,(O) the(O) prequel(O) to(O) the(O) Lord(B-book) of(I-book) the(I-book) Rings(I-book) trilogy(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, book, event, poem, literary genre, location, person, magazine, country, writer, award and O.\nSentence: According to Entertainment Weekly , Raimi had expressed an interest in directing a film version of The Hobbit , the prequel to the Lord of the Rings trilogy .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["According","to","Entertainment","Weekly",",","Raimi","had","expressed","an","interest","in","directing","a","film","version","of","The","Hobbit",",","the","prequel","to","the","Lord","of","the","Rings","trilogy","."],"labels":["O","O","B-magazine","I-magazine","O","B-person","O","O","O","O","O","O","O","O","O","O","B-book","I-book","O","O","O","O","O","B-book","I-book","I-book","I-book","O","O"],"target_index":null,"target_label":null},"label_list":["organization","book","event","poem","literary_genre","location","person","magazine","country","writer","award"]}
{"id":"32","dataset":"crossner_literature","split":"dev","instance":{"id":"32","prompt_labels":"Ansgar(B-writer) received(O) the(O) mission(O) of(O) evangelizing(O) pagan(O) Denmark(B-country) ,(O) Norway(B-country) and(O) Sweden(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, person, writer, organization, magazine, poem, event, country, literary genre, book and O.\nSentence: Ansgar received the mission of evangelizing pagan Denmark , Norway and Sweden .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Ansgar","received","the","mission","of","evangelizing","pagan","Denmark",",","Norway","and","Sweden","."],"labels":["B-writer","O","O","O","O","O","O","B-country","O","B-country","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["location","award","person","writer","organization","magazine","poem","event","country","literary_genre","book"]}
{"id":"33","dataset":"crossner_literature","split":"dev","instance":{"id":"33","prompt_labels":"The(O) poem(B-literary genre) is(O) quoted(O) by(O) Sue(B-writer) Bridehead(I-writer) in(O) Thomas(B-writer) Hardy(I-writer) '(O) s(O) 1895(O) novel(B-literary genre) ,(O) Jude(B-book) the(I-book) Obscure(I-book) and(O) also(O) by(O) Edward(B-writer) Ashburnham(I-writer) in(O) Ford(B-writer) Madox(I-writer) .(O) Ford(B-writer) '(O) s(O) The(B-book) Good(I-book) Soldier(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, writer, award, book, magazine, event, person, poem, country, literary genre and O.\nSentence: The poem is quoted by Sue Bridehead in Thomas Hardy ' s 1895 novel , Jude the Obscure and also by Edward Ashburnham in Ford Madox . Ford ' s The Good Soldier .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","poem","is","quoted","by","Sue","Bridehead","in","Thomas","Hardy","'","s","1895","novel",",","Jude","the","Obscure","and","also","by","Edward","Ashburnham","in","Ford","Madox",".","Ford","'","s","The","Good","Soldier","."],"labels":["O","B-literary genre","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","O","O","B-literary genre","O","B-book","I-book","I-book","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","O","O","B-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["location","organization","writer","award","book","magazine","event","person","poem","country","literary_genre"]}
{"id":"38","dataset":"crossner_literature","split":"dev","instance":{"id":"38","prompt_labels":"Marsters(B-person) moved(O) to(O) Chicago(B-location) ,(O) where(O) his(O) first(O) professional(O) acting(O) role(O) was(O) Ferdinand(O) in(O) The(B-book) Tempest(I-book) at(O) the(O) Goodman(B-location) Theatre(I-location) in(O) 1987(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, location, book, award, writer, event, person, magazine, literary genre, poem and O.\nSentence: Marsters moved to Chicago , where his first professional acting role was Ferdinand in The Tempest at the Goodman Theatre in 1987 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Marsters","moved","to","Chicago",",","where","his","first","professional","acting","role","was","Ferdinand","in","The","Tempest","at","the","Goodman","Theatre","in","1987","."],"labels":["B-person","O","O","B-location","O","O","O","O","O","O","O","O","O","O","B-book","I-book","O","O","B-location","I-location","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","organization","location","book","award","writer","event","person","magazine","literary_genre","poem"]}
{"id":"42","dataset":"crossner_literature","split":"dev","instance":{"id":"42","prompt_labels":"She(O) was(O) appointed(O) Burmese(O) ambassador(O) to(O) India(B-country) and(O) Nepal(B-country) in(O) 1960(O) ,(O) and(O) Aung(B-writer) San(I-writer) Suu(I-writer) Kyi(I-writer) followed(O) her(O) there(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, literary genre, award, writer, organization, magazine, event, book, location, person, poem and O.\nSentence: She was appointed Burmese ambassador to India and Nepal in 1960 , and Aung San Suu Kyi followed her there .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","was","appointed","Burmese","ambassador","to","India","and","Nepal","in","1960",",","and","Aung","San","Suu","Kyi","followed","her","there","."],"labels":["O","O","O","O","O","O","B-country","O","B-country","O","O","O","O","B-writer","I-writer","I-writer","I-writer","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","literary_genre","award","writer","organization","magazine","event","book","location","person","poem"]}
{"id":"43","dataset":"crossner_literature","split":"dev","instance":{"id":"43","prompt_labels":"Although(O) the(O) reforms(O) brought(O) by(O) Nikita(B-person) Khrushchev(I-person) freed(O) him(O) from(O) exile(O) in(O) 1956(O) ,(O) the(O) publication(O) of(O) Cancer(B-book) Ward(I-book) ((O) 1968(O) )(O) ,(O) August(B-book) 1914(I-book) ((O) 1971(O) )(O) ,(O) and(O) The(B-book) Gulag(I-book) Archipelago(I-book) ((O) 1973(O) )(O) beyond(O) the(O) Soviet(B-country) Union(I-country) angered(O) authorities(O) ,(O) and(O) Solzhenitsyn(B-writer) lost(O) his(O) Soviet(O) citizenship(O) in(O) 1974(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, organization, location, person, poem, magazine, country, book, writer, literary genre and O.\nSentence: Although the reforms brought by Nikita Khrushchev freed him from exile in 1956 , the publication of Cancer Ward ( 1968 ) , August 1914 ( 1971 ) , and The Gulag Archipelago ( 1973 ) beyond the Soviet Union angered authorities , and Solzhenitsyn lost his Soviet citizenship in 1974 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Although","the","reforms","brought","by","Nikita","Khrushchev","freed","him","from","exile","in","1956",",","the","publication","of","Cancer","Ward","(","1968",")",",","August","1914","(","1971",")",",","and","The","Gulag","Archipelago","(","1973",")","beyond","the","Soviet","Union","angered","authorities",",","and","Solzhenitsyn","lost","his","Soviet","citizenship","in","1974","."],"labels":["O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","B-book","I-book","O","O","O","O","B-book","I-book","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","B-country","I-country","O","O","O","O","B-writer","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","event","organization","location","person","poem","magazine","country","book","writer","literary_genre"]}
{"id":"44","dataset":"crossner_literature","split":"dev","instance":{"id":"44","prompt_labels":"Hesser(B-writer) lives(O) in(O) Brooklyn(B-location) Heights(I-location) with(O) her(O) husband(O) ,(O) Tad(B-writer) Friend(I-writer) ,(O) a(O) staff(O) writer(O) for(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) ,(O) and(O) their(O) two(O) children(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, award, writer, magazine, literary genre, organization, poem, person, book, event and O.\nSentence: Hesser lives in Brooklyn Heights with her husband , Tad Friend , a staff writer for The New Yorker , and their two children .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Hesser","lives","in","Brooklyn","Heights","with","her","husband",",","Tad","Friend",",","a","staff","writer","for","The","New","Yorker",",","and","their","two","children","."],"labels":["B-writer","O","O","B-location","I-location","O","O","O","O","B-writer","I-writer","O","O","O","O","O","B-magazine","I-magazine","I-magazine","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","location","award","writer","magazine","literary_genre","organization","poem","person","book","event"]}
{"id":"45","dataset":"crossner_literature","split":"dev","instance":{"id":"45","prompt_labels":"He(O) dedicated(O) his(O) poem(B-literary genre) Fragments(B-poem) of(I-poem) Olympian(I-poem) Gossip(I-poem) to(O) Viereck(B-location) ,(O) a(O) work(O) in(O) which(O) Tesla(B-organization) ridiculed(O) the(O) scientific(O) establishment(O) of(O) the(O) day(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, writer, award, magazine, literary genre, country, poem, location, book, event, organization and O.\nSentence: He dedicated his poem Fragments of Olympian Gossip to Viereck , a work in which Tesla ridiculed the scientific establishment of the day .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","dedicated","his","poem","Fragments","of","Olympian","Gossip","to","Viereck",",","a","work","in","which","Tesla","ridiculed","the","scientific","establishment","of","the","day","."],"labels":["O","O","O","B-literary genre","B-poem","I-poem","I-poem","I-poem","O","B-location","O","O","O","O","O","B-organization","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","writer","award","magazine","literary_genre","country","poem","location","book","event","organization"]}
{"id":"47","dataset":"crossner_literature","split":"dev","instance":{"id":"47","prompt_labels":"In(O) March(O) 2020(O) ,(O) a(O) third(O) season(O) of(O) Cosmos(O) named(O) Cosmos(O) :(O) Possible(O) Worlds(O) ,(O) for(O) which(O) Druyan(B-writer) was(O) executive(O) producer(O) ,(O) writer(O) ,(O) and(O) director(O) premiered(O) on(O) National(B-magazine) Geographic(I-magazine) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, poem, event, literary genre, magazine, organization, writer, book, location, country, person and O.\nSentence: In March 2020 , a third season of Cosmos named Cosmos : Possible Worlds , for which Druyan was executive producer , writer , and director premiered on National Geographic .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","March","2020",",","a","third","season","of","Cosmos","named","Cosmos",":","Possible","Worlds",",","for","which","Druyan","was","executive","producer",",","writer",",","and","director","premiered","on","National","Geographic","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","O"],"target_index":null,"target_label":null},"label_list":["award","poem","event","literary_genre","magazine","organization","writer","book","location","country","person"]}
{"id":"48","dataset":"crossner_literature","split":"dev","instance":{"id":"48","prompt_labels":"He(O) spent(O) most(O) of(O) the(O) war(O) flying(O) between(O) the(O) U.S.(B-country) and(O) India(B-country) ,(O) via(O) the(O) Azores(B-location) and(O) North(B-location) Africa(I-location) or(O) South(B-location) America(I-location) ,(O) Nigeria(B-country) ,(O) and(O) Central(B-location) Africa(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, magazine, literary genre, writer, book, person, location, poem, award, country, event and O.\nSentence: He spent most of the war flying between the U.S. and India , via the Azores and North Africa or South America , Nigeria , and Central Africa .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","spent","most","of","the","war","flying","between","the","U.S.","and","India",",","via","the","Azores","and","North","Africa","or","South","America",",","Nigeria",",","and","Central","Africa","."],"labels":["O","O","O","O","O","O","O","O","O","B-country","O","B-country","O","O","O","B-location","O","B-location","I-location","O","B-location","I-location","O","B-country","O","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["organization","magazine","literary_genre","writer","book","person","location","poem","award","country","event"]}
{"id":"50","dataset":"crossner_literature","split":"dev","instance":{"id":"50","prompt_labels":"Kirkus(B-magazine) Reviews(I-magazine) described(O) it(O) as(O) Predictable(O) ,(O) certainly(O) ,(O) and(O) less(O) imaginative(O) than(O) Consider(B-book) Phlebas(I-book) Consider(B-book) Phlebas(I-book) ,(O) but(O) technically(O) much(O) more(O) solid(O) :(O) honorably(O) crafted(O) work(O) ,(O) often(O) engrossing(O) despite(O) some(O) sluggish(O) patches(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, person, writer, literary genre, country, event, location, award, organization, book, poem and O.\nSentence: Kirkus Reviews described it as Predictable , certainly , and less imaginative than Consider Phlebas Consider Phlebas , but technically much more solid : honorably crafted work , often engrossing despite some sluggish patches .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Kirkus","Reviews","described","it","as","Predictable",",","certainly",",","and","less","imaginative","than","Consider","Phlebas","Consider","Phlebas",",","but","technically","much","more","solid",":","honorably","crafted","work",",","often","engrossing","despite","some","sluggish","patches","."],"labels":["B-magazine","I-magazine","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","B-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","person","writer","literary_genre","country","event","location","award","organization","book","poem"]}
{"id":"53","dataset":"crossner_literature","split":"dev","instance":{"id":"53","prompt_labels":"Rolling(B-magazine) Stone(I-magazine) magazine(O) ranked(O) him(O) number(O) 13(O) in(O) its(O) list(O) of(O) 100(O) Greatest(O) Artists(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, person, award, location, magazine, poem, organization, writer, book, literary genre and O.\nSentence: Rolling Stone magazine ranked him number 13 in its list of 100 Greatest Artists .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Rolling","Stone","magazine","ranked","him","number","13","in","its","list","of","100","Greatest","Artists","."],"labels":["B-magazine","I-magazine","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","event","person","award","location","magazine","poem","organization","writer","book","literary_genre"]}
{"id":"55","dataset":"crossner_literature","split":"dev","instance":{"id":"55","prompt_labels":"Mere(B-book) Christianity(I-book) was(O) voted(O) best(O) book(O) of(O) the(O) 20th(O) century(O) by(O) Christianity(B-magazine) Today(I-magazine) in(O) 2000(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, award, organization, person, writer, event, literary genre, poem, country, magazine, location and O.\nSentence: Mere Christianity was voted best book of the 20th century by Christianity Today in 2000 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Mere","Christianity","was","voted","best","book","of","the","20th","century","by","Christianity","Today","in","2000","."],"labels":["B-book","I-book","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","O","O","O"],"target_index":null,"target_label":null},"label_list":["book","award","organization","person","writer","event","literary_genre","poem","country","magazine","location"]}
{"id":"57","dataset":"crossner_literature","split":"dev","instance":{"id":"57","prompt_labels":"Although(O) Nietzsche(B-writer) had(O) previously(O) announced(O) at(O) the(O) end(O) of(O) On(B-book) the(I-book) Genealogy(I-book) of(I-book) Morality(I-book) a(O) new(O) work(O) with(O) the(O) title(O) The(B-book) Will(I-book) to(I-book) Power(I-book) :(I-book) Attempt(I-book) at(I-book) a(I-book) Revaluation(I-book) of(I-book) All(I-book) Values(I-book) ,(O) he(O) eventually(O) seems(O) to(O) have(O) abandoned(O) this(O) idea(O) and(O) instead(O) used(O) some(O) of(O) the(O) draft(O) passages(O) to(O) compose(O) Twilight(B-book) of(I-book) the(I-book) Idols(I-book) and(O) The(B-book) Antichrist(I-book) in(O) 1888(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, poem, magazine, award, book, person, writer, literary genre, event, organization, location and O.\nSentence: Although Nietzsche had previously announced at the end of On the Genealogy of Morality a new work with the title The Will to Power : Attempt at a Revaluation of All Values , he eventually seems to have abandoned this idea and instead used some of the draft passages to compose Twilight of the Idols and The Antichrist in 1888 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Although","Nietzsche","had","previously","announced","at","the","end","of","On","the","Genealogy","of","Morality","a","new","work","with","the","title","The","Will","to","Power",":","Attempt","at","a","Revaluation","of","All","Values",",","he","eventually","seems","to","have","abandoned","this","idea","and","instead","used","some","of","the","draft","passages","to","compose","Twilight","of","the","Idols","and","The","Antichrist","in","1888","."],"labels":["O","B-writer","O","O","O","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","B-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","I-book","O","B-book","I-book","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","poem","magazine","award","book","person","writer","literary_genre","event","organization","location"]}
{"id":"59","dataset":"crossner_literature","split":"dev","instance":{"id":"59","prompt_labels":"He(O) received(O) an(O) Academy(B-award) Awards(I-award) nomination(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Supporting(I-award) Actor(I-award) for(O) 1987(O) 's(O) Broadcast(O) News(O) and(O) was(O) widely(O) praised(O) for(O) his(O) performance(O) in(O) the(O) 2011(O) film(O) Drive(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, award, event, book, poem, organization, location, magazine, country, person, literary genre and O.\nSentence: He received an Academy Awards nomination for Academy Award for Best Supporting Actor for 1987 's Broadcast News and was widely praised for his performance in the 2011 film Drive .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","received","an","Academy","Awards","nomination","for","Academy","Award","for","Best","Supporting","Actor","for","1987","'s","Broadcast","News","and","was","widely","praised","for","his","performance","in","the","2011","film","Drive","."],"labels":["O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["writer","award","event","book","poem","organization","location","magazine","country","person","literary_genre"]}
{"id":"61","dataset":"crossner_literature","split":"dev","instance":{"id":"61","prompt_labels":"Bernard(B-person) Miles(I-person) gave(O) Milligan(B-person) his(O) first(O) straight(O) acting(O) role(O) ,(O) as(O) Ben(B-person) Gunn(I-person) ,(O) in(O) the(O) Mermaid(B-location) Theatre(I-location) production(O) of(O) Treasure(B-book) Island(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, location, award, event, person, poem, writer, book, organization, country, literary genre and O.\nSentence: Bernard Miles gave Milligan his first straight acting role , as Ben Gunn , in the Mermaid Theatre production of Treasure Island .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Bernard","Miles","gave","Milligan","his","first","straight","acting","role",",","as","Ben","Gunn",",","in","the","Mermaid","Theatre","production","of","Treasure","Island","."],"labels":["B-person","I-person","O","B-person","O","O","O","O","O","O","O","B-person","I-person","O","O","O","B-location","I-location","O","O","B-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["magazine","location","award","event","person","poem","writer","book","organization","country","literary_genre"]}
{"id":"63","dataset":"crossner_literature","split":"dev","instance":{"id":"63","prompt_labels":"In(O) 1930(O) he(O) was(O) nominated(O) for(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) by(O) Swedish(O) author(O) Anders(B-writer) sterling(I-writer) ,(O) but(O) was(O) passed(O) over(O) in(O) favor(O) of(O) Sinclair(B-writer) Lewis(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, person, writer, literary genre, magazine, poem, event, book, award, country and O.\nSentence: In 1930 he was nominated for the Nobel Prize in Literature by Swedish author Anders sterling , but was passed over in favor of Sinclair Lewis .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1930","he","was","nominated","for","the","Nobel","Prize","in","Literature","by","Swedish","author","Anders","sterling",",","but","was","passed","over","in","favor","of","Sinclair","Lewis","."],"labels":["O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["location","organization","person","writer","literary_genre","magazine","poem","event","book","award","country"]}
{"id":"64","dataset":"crossner_literature","split":"dev","instance":{"id":"64","prompt_labels":"Among(O) his(O) childhood(O) favorites(O) were(O) Charles(B-writer) Dickens(I-writer) ,(O) Tobias(B-writer) Smollett(I-writer) ,(O) Mark(B-writer) Twain(I-writer) ,(O) Booth(B-writer) Tarkington(I-writer) ,(O) and(O) later(O) ,(O) Robert(B-writer) Benchley(I-writer) and(O) S.(B-writer) J.(I-writer) Perelman(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, country, organization, magazine, writer, location, literary genre, poem, book, event and O.\nSentence: Among his childhood favorites were Charles Dickens , Tobias Smollett , Mark Twain , Booth Tarkington , and later , Robert Benchley and S. J. Perelman .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Among","his","childhood","favorites","were","Charles","Dickens",",","Tobias","Smollett",",","Mark","Twain",",","Booth","Tarkington",",","and","later",",","Robert","Benchley","and","S.","J.","Perelman","."],"labels":["O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["person","award","country","organization","magazine","writer","location","literary_genre","poem","book","event"]}
{"id":"65","dataset":"crossner_literature","split":"dev","instance":{"id":"65","prompt_labels":"Among(O) the(O) books(O) found(O) in(O) his(O) library(O) ((O) as(O) evidenced(O) in(O) Lovecraft(B-book) 's(I-book) Library(I-book) by(O) S.(B-writer) T.(I-writer) Joshi(I-writer) )(O) was(O) The(B-book) Seven(I-book) Who(I-book) Were(I-book) Hanged(I-book) by(O) Leonid(B-writer) Andreyev(I-writer) and(O) A(B-book) Strange(I-book) Manuscript(I-book) Found(I-book) in(I-book) a(I-book) Copper(I-book) Cylinder(I-book) by(O) James(B-writer) De(I-writer) Mille(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, magazine, location, person, literary genre, book, poem, event, award, writer and O.\nSentence: Among the books found in his library ( as evidenced in Lovecraft 's Library by S. T. Joshi ) was The Seven Who Were Hanged by Leonid Andreyev and A Strange Manuscript Found in a Copper Cylinder by James De Mille .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Among","the","books","found","in","his","library","(","as","evidenced","in","Lovecraft","'s","Library","by","S.","T.","Joshi",")","was","The","Seven","Who","Were","Hanged","by","Leonid","Andreyev","and","A","Strange","Manuscript","Found","in","a","Copper","Cylinder","by","James","De","Mille","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","O","B-writer","I-writer","I-writer","O","O","B-book","I-book","I-book","I-book","I-book","O","B-writer","I-writer","O","B-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","O","B-writer","I-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["organization","country","magazine","location","person","literary_genre","book","poem","event","award","writer"]}
{"id":"66","dataset":"crossner_literature","split":"dev","instance":{"id":"66","prompt_labels":"He(O) soon(O) produced(O) acclaimed(O) translations(O) of(O) Sndor(B-writer) Petfi(I-writer) ,(O) Johann(B-writer) Wolfgang(I-writer) von(I-writer) Goethe(I-writer) ,(O) Rainer(B-writer) Maria(I-writer) Rilke(I-writer) ,(O) Paul(B-writer) Verlaine(I-writer) ,(O) Taras(B-writer) Shevchenko(I-writer) ,(O) and(O) Nikoloz(B-writer) Baratashvili(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, magazine, event, poem, literary genre, country, organization, person, location, writer, award and O.\nSentence: He soon produced acclaimed translations of Sndor Petfi , Johann Wolfgang von Goethe , Rainer Maria Rilke , Paul Verlaine , Taras Shevchenko , and Nikoloz Baratashvili .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","soon","produced","acclaimed","translations","of","Sndor","Petfi",",","Johann","Wolfgang","von","Goethe",",","Rainer","Maria","Rilke",",","Paul","Verlaine",",","Taras","Shevchenko",",","and","Nikoloz","Baratashvili","."],"labels":["O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","I-writer","I-writer","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["book","magazine","event","poem","literary_genre","country","organization","person","location","writer","award"]}
{"id":"67","dataset":"crossner_literature","split":"dev","instance":{"id":"67","prompt_labels":"This(O) aversion(O) to(O) war(O) also(O) led(O) Einstein(B-person) to(O) befriend(O) author(O) Upton(B-writer) Sinclair(I-writer) and(O) film(O) star(O) Charlie(B-person) Chaplin(I-person) ,(O) both(O) noted(O) for(O) their(O) pacifism(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, organization, person, writer, poem, literary genre, award, country, book, magazine and O.\nSentence: This aversion to war also led Einstein to befriend author Upton Sinclair and film star Charlie Chaplin , both noted for their pacifism .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","aversion","to","war","also","led","Einstein","to","befriend","author","Upton","Sinclair","and","film","star","Charlie","Chaplin",",","both","noted","for","their","pacifism","."],"labels":["O","O","O","O","O","O","B-person","O","O","O","B-writer","I-writer","O","O","O","B-person","I-person","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","location","organization","person","writer","poem","literary_genre","award","country","book","magazine"]}
{"id":"69","dataset":"crossner_literature","split":"dev","instance":{"id":"69","prompt_labels":"Since(O) release(O) ,(O) the(O) film(O) has(O) divided(O) critics(O) but(O) generally(O) received(O) praise(O) ;(O) initial(O) reviews(O) ranged(O) from(O) Melody(B-magazine) Maker(I-magazine) calling(O) it(O) the(O) greatest(O) horror(O) film(O) made(O) in(O) Britain(B-country) ,(O) to(O) Roger(B-writer) Ebert(I-writer) decrying(O) its(O) bankruptcy(O) of(O) imagination(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, poem, book, country, organization, magazine, literary genre, location, writer, award, person and O.\nSentence: Since release , the film has divided critics but generally received praise ; initial reviews ranged from Melody Maker calling it the greatest horror film made in Britain , to Roger Ebert decrying its bankruptcy of imagination .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Since","release",",","the","film","has","divided","critics","but","generally","received","praise",";","initial","reviews","ranged","from","Melody","Maker","calling","it","the","greatest","horror","film","made","in","Britain",",","to","Roger","Ebert","decrying","its","bankruptcy","of","imagination","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","O","O","O","O","O","O","O","O","B-country","O","O","B-writer","I-writer","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","poem","book","country","organization","magazine","literary_genre","location","writer","award","person"]}
{"id":"70","dataset":"crossner_literature","split":"dev","instance":{"id":"70","prompt_labels":"Tarkovsky(B-writer) was(O) the(O) recipient(O) of(O) several(O) awards(O) at(O) the(O) Cannes(B-event) Film(I-event) Festival(I-event) throughout(O) his(O) career(O) ((O) including(O) the(O) FIPRESCI(B-award) prize(I-award) ,(O) the(O) Prize(B-award) of(I-award) the(I-award) Ecumenical(I-award) Jury(I-award) ,(O) and(O) the(O) Grand(B-award) Prix(I-award) Spcial(I-award) du(I-award) Jury(I-award) )(O) and(O) winner(O) of(O) the(O) Golden(B-award) Lion(I-award) award(I-award) at(O) the(O) Venice(B-event) Film(I-event) Festival(I-event) for(O) his(O) debut(O) film(O) Ivan(O) 's(O) Childhood(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, book, poem, literary genre, country, magazine, writer, event, person, location and O.\nSentence: Tarkovsky was the recipient of several awards at the Cannes Film Festival throughout his career ( including the FIPRESCI prize , the Prize of the Ecumenical Jury , and the Grand Prix Spcial du Jury ) and winner of the Golden Lion award at the Venice Film Festival for his debut film Ivan 's Childhood .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Tarkovsky","was","the","recipient","of","several","awards","at","the","Cannes","Film","Festival","throughout","his","career","(","including","the","FIPRESCI","prize",",","the","Prize","of","the","Ecumenical","Jury",",","and","the","Grand","Prix","Spcial","du","Jury",")","and","winner","of","the","Golden","Lion","award","at","the","Venice","Film","Festival","for","his","debut","film","Ivan","'s","Childhood","."],"labels":["B-writer","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","O","O","B-award","I-award","I-award","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","award","book","poem","literary_genre","country","magazine","writer","event","person","location"]}
{"id":"71","dataset":"crossner_literature","split":"dev","instance":{"id":"71","prompt_labels":"The(O) first(O) global(O) recognition(O) came(O) in(O) 1950(O) when(O) Gwendolyn(B-writer) Brooks(I-writer) was(O) the(O) first(O) black(O) American(O) to(O) win(O) a(O) Pulitzer(B-award) Prize(I-award) for(O) Literature(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, location, country, poem, book, organization, award, magazine, event, writer, person and O.\nSentence: The first global recognition came in 1950 when Gwendolyn Brooks was the first black American to win a Pulitzer Prize for Literature .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","first","global","recognition","came","in","1950","when","Gwendolyn","Brooks","was","the","first","black","American","to","win","a","Pulitzer","Prize","for","Literature","."],"labels":["O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","B-award","I-award","O","O","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","location","country","poem","book","organization","award","magazine","event","writer","person"]}
{"id":"74","dataset":"crossner_literature","split":"dev","instance":{"id":"74","prompt_labels":"He(O) is(O) best(O) remembered(O) for(O) his(O) science(B-literary genre) fiction(I-literary genre) ,(O) including(O) The(B-book) Demolished(I-book) Man(I-book) ,(O) winner(O) of(O) the(O) inaugural(O) Hugo(B-award) Award(I-award) in(O) 1953(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, magazine, award, event, organization, country, book, person, literary genre, location, writer and O.\nSentence: He is best remembered for his science fiction , including The Demolished Man , winner of the inaugural Hugo Award in 1953 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","is","best","remembered","for","his","science","fiction",",","including","The","Demolished","Man",",","winner","of","the","inaugural","Hugo","Award","in","1953","."],"labels":["O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","B-book","I-book","I-book","O","O","O","O","O","B-award","I-award","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","magazine","award","event","organization","country","book","person","literary_genre","location","writer"]}
{"id":"75","dataset":"crossner_literature","split":"dev","instance":{"id":"75","prompt_labels":"Amos(B-writer) prophesied(O) during(O) the(O) reign(O) of(O) Jeroboam(B-person) II(I-person) ,(O) King(O) of(O) Israel(B-country) ,(O) and(O) of(O) Uzziah(B-person) of(O) Kingdom(B-country) of(I-country) Judah(I-country) ,(O) which(O) places(O) him(O) in(O) the(O) first(O) half(O) of(O) the(O) 8th(O) century(O) BC(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, magazine, location, book, person, poem, award, organization, literary genre, writer, country and O.\nSentence: Amos prophesied during the reign of Jeroboam II , King of Israel , and of Uzziah of Kingdom of Judah , which places him in the first half of the 8th century BC .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Amos","prophesied","during","the","reign","of","Jeroboam","II",",","King","of","Israel",",","and","of","Uzziah","of","Kingdom","of","Judah",",","which","places","him","in","the","first","half","of","the","8th","century","BC","."],"labels":["B-writer","O","O","O","O","O","B-person","I-person","O","O","O","B-country","O","O","O","B-person","O","B-country","I-country","I-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","magazine","location","book","person","poem","award","organization","literary_genre","writer","country"]}
{"id":"76","dataset":"crossner_literature","split":"dev","instance":{"id":"76","prompt_labels":"Jorge(B-writer) Luis(I-writer) Borges(I-writer) wrote(O) a(O) contemporary(O) bestiary(O) of(O) sorts(O) ,(O) the(O) Book(B-book) of(I-book) Imaginary(I-book) Beings(I-book) ,(O) which(O) collects(O) imaginary(O) beasts(O) from(O) bestiaries(O) and(O) fiction(B-literary genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, magazine, person, award, event, writer, organization, location, poem, literary genre, country and O.\nSentence: Jorge Luis Borges wrote a contemporary bestiary of sorts , the Book of Imaginary Beings , which collects imaginary beasts from bestiaries and fiction .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Jorge","Luis","Borges","wrote","a","contemporary","bestiary","of","sorts",",","the","Book","of","Imaginary","Beings",",","which","collects","imaginary","beasts","from","bestiaries","and","fiction","."],"labels":["B-writer","I-writer","I-writer","O","O","O","O","O","O","O","O","B-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","B-literary genre","O"],"target_index":null,"target_label":null},"label_list":["book","magazine","person","award","event","writer","organization","location","poem","literary_genre","country"]}
{"id":"77","dataset":"crossner_literature","split":"dev","instance":{"id":"77","prompt_labels":"The(O) Australia(B-country) n(O) composer(O) Peter(B-writer) Sculthorpe(I-writer) quoted(O) parts(O) of(O) it(O) in(O) his(O) opera(O) or(O) music(O) theatre(O) work(O) Rites(O) of(O) Passage(O) ((O) 1972-73(O) )(O) ,(O) which(O) was(O) commissioned(O) for(O) the(O) opening(O) of(O) the(O) Sydney(B-location) Opera(I-location) House(I-location) but(O) was(O) not(O) ready(O) in(O) time(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, event, poem, literary genre, magazine, country, organization, award, writer, book and O.\nSentence: The Australia n composer Peter Sculthorpe quoted parts of it in his opera or music theatre work Rites of Passage ( 1972-73 ) , which was commissioned for the opening of the Sydney Opera House but was not ready in time .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Australia","n","composer","Peter","Sculthorpe","quoted","parts","of","it","in","his","opera","or","music","theatre","work","Rites","of","Passage","(","1972-73",")",",","which","was","commissioned","for","the","opening","of","the","Sydney","Opera","House","but","was","not","ready","in","time","."],"labels":["O","B-country","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","person","event","poem","literary_genre","magazine","country","organization","award","writer","book"]}
{"id":"79","dataset":"crossner_literature","split":"dev","instance":{"id":"79","prompt_labels":"He(O) came(O) to(O) wide(O) public(O) attention(O) with(O) his(O) first(O) book(O) Poems(B-poem) at(O) the(O) age(O) of(O) twenty-three(O) in(O) 1930(O) ;(O) it(O) was(O) followed(O) in(O) 1932(O) by(O) The(B-poem) Orators(I-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, country, event, poem, writer, book, location, organization, magazine, person, award and O.\nSentence: He came to wide public attention with his first book Poems at the age of twenty-three in 1930 ; it was followed in 1932 by The Orators .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","came","to","wide","public","attention","with","his","first","book","Poems","at","the","age","of","twenty-three","in","1930",";","it","was","followed","in","1932","by","The","Orators","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-poem","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-poem","I-poem","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","country","event","poem","writer","book","location","organization","magazine","person","award"]}
{"id":"80","dataset":"crossner_literature","split":"dev","instance":{"id":"80","prompt_labels":"Julia(B-writer) was(O) the(O) niece(O) of(O) poet(O) and(O) critic(O) Matthew(B-writer) Arnold(I-writer) and(O) the(O) sister(O) of(O) Mary(B-writer) Augusta(I-writer) Ward(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, literary genre, location, organization, country, person, book, award, magazine, writer, event and O.\nSentence: Julia was the niece of poet and critic Matthew Arnold and the sister of Mary Augusta Ward .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Julia","was","the","niece","of","poet","and","critic","Matthew","Arnold","and","the","sister","of","Mary","Augusta","Ward","."],"labels":["B-writer","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","B-writer","I-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["poem","literary_genre","location","organization","country","person","book","award","magazine","writer","event"]}
{"id":"81","dataset":"crossner_literature","split":"dev","instance":{"id":"81","prompt_labels":"It(O) was(O) adapted(O) by(O) Talbot(B-writer) Jennings(I-writer) ,(O) Tess(B-writer) Slesinger(I-writer) ,(O) and(O) Claudine(B-writer) West(I-writer) from(O) the(O) play(O) by(O) Owen(B-writer) Davis(I-writer) and(O) Donald(B-writer) Davis(I-writer) ,(O) which(O) was(O) in(O) itself(O) based(O) on(O) the(O) 1931(O) The(B-book) Good(I-book) Earth(I-book) by(O) Nobel(B-award) Prize(I-award) -winning(O) author(O) Pearl(B-writer) S.(I-writer) Buck(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, poem, country, magazine, person, book, event, location, literary genre, writer, award and O.\nSentence: It was adapted by Talbot Jennings , Tess Slesinger , and Claudine West from the play by Owen Davis and Donald Davis , which was in itself based on the 1931 The Good Earth by Nobel Prize -winning author Pearl S. Buck .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","adapted","by","Talbot","Jennings",",","Tess","Slesinger",",","and","Claudine","West","from","the","play","by","Owen","Davis","and","Donald","Davis",",","which","was","in","itself","based","on","the","1931","The","Good","Earth","by","Nobel","Prize","-winning","author","Pearl","S.","Buck","."],"labels":["O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","O","B-writer","I-writer","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","O","B-award","I-award","O","O","B-writer","I-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["organization","poem","country","magazine","person","book","event","location","literary_genre","writer","award"]}
{"id":"82","dataset":"crossner_literature","split":"dev","instance":{"id":"82","prompt_labels":"His(O) best-known(O) works(O) include(O) Demian(B-book) ,(O) Steppenwolf(B-book) ,(O) Siddhartha(B-book) ,(O) and(O) The(B-book) Glass(I-book) Bead(I-book) Game(I-book) ,(O) each(O) of(O) which(O) explores(O) an(O) individual(O) 's(O) search(O) for(O) authenticity(O) ,(O) self-knowledge(O) and(O) spirituality(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, organization, literary genre, person, magazine, writer, event, country, book, location, award and O.\nSentence: His best-known works include Demian , Steppenwolf , Siddhartha , and The Glass Bead Game , each of which explores an individual 's search for authenticity , self-knowledge and spirituality .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","best-known","works","include","Demian",",","Steppenwolf",",","Siddhartha",",","and","The","Glass","Bead","Game",",","each","of","which","explores","an","individual","'s","search","for","authenticity",",","self-knowledge","and","spirituality","."],"labels":["O","O","O","O","B-book","O","B-book","O","B-book","O","O","B-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","organization","literary_genre","person","magazine","writer","event","country","book","location","award"]}
{"id":"84","dataset":"crossner_literature","split":"dev","instance":{"id":"84","prompt_labels":"In(O) New(B-location) York(I-location) ,(O) he(O) socialized(O) at(O) the(O) Hydra(B-organization) Club(I-organization) ,(O) an(O) organization(O) of(O) New(B-location) York(I-location) 's(O) science(B-literary genre) fiction(I-literary genre) writers(O) ,(O) including(O) such(O) luminaries(O) as(O) Isaac(B-writer) Asimov(I-writer) ,(O) James(B-writer) Blish(I-writer) ,(O) Anthony(B-writer) Boucher(I-writer) ,(O) Avram(B-writer) Davidson(I-writer) ,(O) Judith(B-writer) Merril(I-writer) ,(O) and(O) Theodore(B-writer) Sturgeon(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, writer, poem, award, book, country, event, organization, literary genre, location, person and O.\nSentence: In New York , he socialized at the Hydra Club , an organization of New York 's science fiction writers , including such luminaries as Isaac Asimov , James Blish , Anthony Boucher , Avram Davidson , Judith Merril , and Theodore Sturgeon .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","New","York",",","he","socialized","at","the","Hydra","Club",",","an","organization","of","New","York","'s","science","fiction","writers",",","including","such","luminaries","as","Isaac","Asimov",",","James","Blish",",","Anthony","Boucher",",","Avram","Davidson",",","Judith","Merril",",","and","Theodore","Sturgeon","."],"labels":["O","B-location","I-location","O","O","O","O","O","B-organization","I-organization","O","O","O","O","B-location","I-location","O","B-literary genre","I-literary genre","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["magazine","writer","poem","award","book","country","event","organization","literary_genre","location","person"]}
{"id":"85","dataset":"crossner_literature","split":"dev","instance":{"id":"85","prompt_labels":"Wajda(B-person) made(O) two(O) more(O) increasingly(O) accomplished(O) films(O) ,(O) which(O) developed(O) further(O) the(O) anti-war(O) theme(O) of(O) A(O) Generation(O) :(O) Kana(O) ((O) 1956(O) )(O) ((O) Special(B-award) Jury(I-award) Prize(I-award) at(O) Cannes(B-event) Film(I-event) Festival(I-event) in(O) 1957(O) ,(O) shared(O) with(O) Bergman(B-person) 's(O) The(O) Seventh(O) Seal(O) )(O) and(O) Ashes(O) and(O) Diamonds(O) ((O) 1958(O) )(O) with(O) Zbigniew(B-person) Cybulski(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, writer, organization, magazine, event, country, person, location, poem, award, literary genre and O.\nSentence: Wajda made two more increasingly accomplished films , which developed further the anti-war theme of A Generation : Kana ( 1956 ) ( Special Jury Prize at Cannes Film Festival in 1957 , shared with Bergman 's The Seventh Seal ) and Ashes and Diamonds ( 1958 ) with Zbigniew Cybulski .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Wajda","made","two","more","increasingly","accomplished","films",",","which","developed","further","the","anti-war","theme","of","A","Generation",":","Kana","(","1956",")","(","Special","Jury","Prize","at","Cannes","Film","Festival","in","1957",",","shared","with","Bergman","'s","The","Seventh","Seal",")","and","Ashes","and","Diamonds","(","1958",")","with","Zbigniew","Cybulski","."],"labels":["B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","O","B-event","I-event","I-event","O","O","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["book","writer","organization","magazine","event","country","person","location","poem","award","literary_genre"]}
{"id":"87","dataset":"crossner_literature","split":"dev","instance":{"id":"87","prompt_labels":"The(B-book) Great(I-book) Hunt(I-book) is(O) a(O) fantasy(B-literary genre) novel(I-literary genre) by(O) United(B-country) States(I-country) author(O) Robert(B-writer) Jordan(I-writer) ,(O) the(O) second(O) book(O) of(O) The(B-book) Wheel(I-book) of(I-book) Time(I-book) series(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, organization, award, magazine, person, book, event, poem, writer, location, country and O.\nSentence: The Great Hunt is a fantasy novel by United States author Robert Jordan , the second book of The Wheel of Time series .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Great","Hunt","is","a","fantasy","novel","by","United","States","author","Robert","Jordan",",","the","second","book","of","The","Wheel","of","Time","series","."],"labels":["B-book","I-book","I-book","O","O","B-literary genre","I-literary genre","O","B-country","I-country","O","B-writer","I-writer","O","O","O","O","O","B-book","I-book","I-book","I-book","O","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","organization","award","magazine","person","book","event","poem","writer","location","country"]}
{"id":"88","dataset":"crossner_literature","split":"dev","instance":{"id":"88","prompt_labels":"The(B-book) Man(I-book) in(I-book) the(I-book) High(I-book) Castle(I-book) ((O) 1962(O) )(O) is(O) set(O) in(O) an(O) alternate(B-literary genre) history(I-literary genre) in(O) which(O) the(O) United(B-country) States(I-country) is(O) ruled(O) by(O) the(O) victorious(O) Axis(O) powers(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, award, writer, poem, book, country, location, magazine, person, literary genre and O.\nSentence: The Man in the High Castle ( 1962 ) is set in an alternate history in which the United States is ruled by the victorious Axis powers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Man","in","the","High","Castle","(","1962",")","is","set","in","an","alternate","history","in","which","the","United","States","is","ruled","by","the","victorious","Axis","powers","."],"labels":["B-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","O","B-country","I-country","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","event","award","writer","poem","book","country","location","magazine","person","literary_genre"]}
{"id":"90","dataset":"crossner_literature","split":"dev","instance":{"id":"90","prompt_labels":"Confucianism(O) reached(O) its(O) peak(O) of(O) influence(O) during(O) the(O) Tang(B-country) dynasty(I-country) and(O) Song(B-country) dynasty(I-country) Dynasties(O) under(O) a(O) rebranded(O) Confucianism(O) called(O) Neo-Confucianism(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, literary genre, person, writer, award, organization, magazine, event, book, poem, location and O.\nSentence: Confucianism reached its peak of influence during the Tang dynasty and Song dynasty Dynasties under a rebranded Confucianism called Neo-Confucianism .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Confucianism","reached","its","peak","of","influence","during","the","Tang","dynasty","and","Song","dynasty","Dynasties","under","a","rebranded","Confucianism","called","Neo-Confucianism","."],"labels":["O","O","O","O","O","O","O","O","B-country","I-country","O","B-country","I-country","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","literary_genre","person","writer","award","organization","magazine","event","book","poem","location"]}
{"id":"92","dataset":"crossner_literature","split":"dev","instance":{"id":"92","prompt_labels":"Before(O) writing(O) Dracula(B-book) ,(O) Stoker(B-writer) met(O) rmin(B-writer) Vmbry(I-writer) ,(O) a(O) Hungarian-Jewish(O) writer(O) and(O) traveller(O) ((O) born(O) in(O) Szent-Gyrgy(B-location) ,(O) Kingdom(B-country) of(I-country) Hungary(I-country) now(O) Svt(B-location) Jur(I-location) ,(O) Slovakia(B-country) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, poem, person, book, country, magazine, award, location, writer, literary genre and O.\nSentence: Before writing Dracula , Stoker met rmin Vmbry , a Hungarian-Jewish writer and traveller ( born in Szent-Gyrgy , Kingdom of Hungary now Svt Jur , Slovakia ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Before","writing","Dracula",",","Stoker","met","rmin","Vmbry",",","a","Hungarian-Jewish","writer","and","traveller","(","born","in","Szent-Gyrgy",",","Kingdom","of","Hungary","now","Svt","Jur",",","Slovakia",")","."],"labels":["O","O","B-book","O","B-writer","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","B-location","O","B-country","I-country","I-country","O","B-location","I-location","O","B-country","O","O"],"target_index":null,"target_label":null},"label_list":["organization","event","poem","person","book","country","magazine","award","location","writer","literary_genre"]}
{"id":"100","dataset":"crossner_literature","split":"dev","instance":{"id":"100","prompt_labels":"During(O) his(O) first(O) years(O) as(O) bishop(O) ,(O) Athanasius(B-writer) visited(O) the(O) churches(O) of(O) his(O) territory(O) ,(O) which(O) at(O) that(O) time(O) included(O) all(O) of(O) Egypt(B-country) and(O) Libya(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, event, country, award, magazine, person, book, poem, location, organization, literary genre and O.\nSentence: During his first years as bishop , Athanasius visited the churches of his territory , which at that time included all of Egypt and Libya .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","his","first","years","as","bishop",",","Athanasius","visited","the","churches","of","his","territory",",","which","at","that","time","included","all","of","Egypt","and","Libya","."],"labels":["O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["writer","event","country","award","magazine","person","book","poem","location","organization","literary_genre"]}
{"id":"101","dataset":"crossner_literature","split":"dev","instance":{"id":"101","prompt_labels":"Nimzowitsch(B-person) 's(O) vanity(O) and(O) faith(O) in(O) his(O) ideas(O) of(O) overprotection(O) provoked(O) Hans(B-person) Kmoch(I-person) to(O) write(O) a(O) parody(O) about(O) him(O) in(O) February(O) 1928(O) in(O) the(O) Wiener(B-magazine) Schachzeitung(I-magazine) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, book, person, writer, location, poem, organization, magazine, country, literary genre and O.\nSentence: Nimzowitsch 's vanity and faith in his ideas of overprotection provoked Hans Kmoch to write a parody about him in February 1928 in the Wiener Schachzeitung .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Nimzowitsch","'s","vanity","and","faith","in","his","ideas","of","overprotection","provoked","Hans","Kmoch","to","write","a","parody","about","him","in","February","1928","in","the","Wiener","Schachzeitung","."],"labels":["B-person","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","O"],"target_index":null,"target_label":null},"label_list":["event","award","book","person","writer","location","poem","organization","magazine","country","literary_genre"]}
{"id":"102","dataset":"crossner_literature","split":"dev","instance":{"id":"102","prompt_labels":"He(O) liked(O) Pedro(B-writer) Caldern(I-writer) de(I-writer) la(I-writer) Barca(I-writer) ,(O) Lope(B-writer) de(I-writer) Vega(I-writer) ,(O) Miguel(B-writer) de(I-writer) Cervantes(I-writer) ,(O) and(O) especially(O) Baltasar(B-writer) Gracin(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, writer, organization, location, event, poem, book, award, country, literary genre, person and O.\nSentence: He liked Pedro Caldern de la Barca , Lope de Vega , Miguel de Cervantes , and especially Baltasar Gracin .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","liked","Pedro","Caldern","de","la","Barca",",","Lope","de","Vega",",","Miguel","de","Cervantes",",","and","especially","Baltasar","Gracin","."],"labels":["O","O","B-writer","I-writer","I-writer","I-writer","I-writer","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["magazine","writer","organization","location","event","poem","book","award","country","literary_genre","person"]}
{"id":"107","dataset":"crossner_literature","split":"dev","instance":{"id":"107","prompt_labels":"The(O) work(O) was(O) such(O) a(O) popular(O) success(O) that(O) the(O) poet(O) wrote(O) a(O) sequel(O) ,(O) Remedia(B-poem) Amoris(I-poem) ((O) Remedies(B-poem) for(I-poem) Love(I-poem) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, location, award, magazine, country, book, writer, poem, person, event, organization and O.\nSentence: The work was such a popular success that the poet wrote a sequel , Remedia Amoris ( Remedies for Love ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","work","was","such","a","popular","success","that","the","poet","wrote","a","sequel",",","Remedia","Amoris","(","Remedies","for","Love",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-poem","I-poem","O","B-poem","I-poem","I-poem","O","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","location","award","magazine","country","book","writer","poem","person","event","organization"]}
{"id":"108","dataset":"crossner_literature","split":"dev","instance":{"id":"108","prompt_labels":"As(O) early(O) as(O) the(O) 13th(O) century(O) when(O) Dante(B-writer) Alighieri(I-writer) depicted(O) him(O) in(O) Limbo(O) alongside(O) the(O) virtuous(O) non-Christian(O) thinkers(O) in(O) his(O) Divine(B-poem) Comedy(I-poem) such(O) as(O) Virgil(B-person) ,(O) Averroes(B-person) ,(O) Homer(B-person) ,(O) Horace(B-person) ,(O) Ovid(B-person) ,(O) Lucan(B-person) ,(O) Socrates(B-person) ,(O) Plato(B-person) ,(O) and(O) Saladin(B-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, person, organization, magazine, award, poem, literary genre, writer, book, location and O.\nSentence: As early as the 13th century when Dante Alighieri depicted him in Limbo alongside the virtuous non-Christian thinkers in his Divine Comedy such as Virgil , Averroes , Homer , Horace , Ovid , Lucan , Socrates , Plato , and Saladin .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["As","early","as","the","13th","century","when","Dante","Alighieri","depicted","him","in","Limbo","alongside","the","virtuous","non-Christian","thinkers","in","his","Divine","Comedy","such","as","Virgil",",","Averroes",",","Homer",",","Horace",",","Ovid",",","Lucan",",","Socrates",",","Plato",",","and","Saladin","."],"labels":["O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","B-poem","I-poem","O","O","B-person","O","B-person","O","B-person","O","B-person","O","B-person","O","B-person","O","B-person","O","B-person","O","O","B-person","O"],"target_index":null,"target_label":null},"label_list":["event","country","person","organization","magazine","award","poem","literary_genre","writer","book","location"]}
{"id":"111","dataset":"crossner_literature","split":"dev","instance":{"id":"111","prompt_labels":"The(O) three(O) films(O) garnered(O) prestigious(O) international(O) awards(O) ,(O) including(O) the(O) Golden(B-award) Lion(I-award) for(O) Best(O) Film(O) at(O) the(O) Venice(B-event) Film(I-event) Festival(I-event) and(O) the(O) Silver(B-award) Bear(I-award) for(I-award) Best(I-award) Director(I-award) at(O) the(O) 44th(B-event) Berlin(I-event) International(I-event) Film(I-event) Festival(I-event) ,(O) in(O) addition(O) to(O) three(O) Academy(B-award) Awards(I-award) nominations(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, award, country, magazine, person, literary genre, book, writer, event, poem and O.\nSentence: The three films garnered prestigious international awards , including the Golden Lion for Best Film at the Venice Film Festival and the Silver Bear for Best Director at the 44th Berlin International Film Festival , in addition to three Academy Awards nominations .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","three","films","garnered","prestigious","international","awards",",","including","the","Golden","Lion","for","Best","Film","at","the","Venice","Film","Festival","and","the","Silver","Bear","for","Best","Director","at","the","44th","Berlin","International","Film","Festival",",","in","addition","to","three","Academy","Awards","nominations","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","O","O","O","B-event","I-event","I-event","O","O","B-award","I-award","I-award","I-award","I-award","O","O","B-event","I-event","I-event","I-event","I-event","O","O","O","O","O","B-award","I-award","O","O"],"target_index":null,"target_label":null},"label_list":["location","organization","award","country","magazine","person","literary_genre","book","writer","event","poem"]}
{"id":"115","dataset":"crossner_literature","split":"dev","instance":{"id":"115","prompt_labels":"He(O) particularly(O) revered(O) Johann(B-writer) Wolfgang(I-writer) von(I-writer) Goethe(I-writer) ,(O) Petrarch(B-writer) ,(O) Pedro(B-writer) Caldern(I-writer) de(I-writer) la(I-writer) Barca(I-writer) and(O) William(B-writer) Shakespeare(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, award, event, magazine, writer, organization, book, location, person, poem, country and O.\nSentence: He particularly revered Johann Wolfgang von Goethe , Petrarch , Pedro Caldern de la Barca and William Shakespeare .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","particularly","revered","Johann","Wolfgang","von","Goethe",",","Petrarch",",","Pedro","Caldern","de","la","Barca","and","William","Shakespeare","."],"labels":["O","O","O","B-writer","I-writer","I-writer","I-writer","O","B-writer","O","B-writer","I-writer","I-writer","I-writer","I-writer","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","award","event","magazine","writer","organization","book","location","person","poem","country"]}
{"id":"118","dataset":"crossner_literature","split":"dev","instance":{"id":"118","prompt_labels":"Liszt(B-writer) included(O) Weiheimer(B-writer) 's(O) symphony(O) on(O) Friedrich(B-writer) Schiller(I-writer) '(O) s(O) Ritter(B-poem) Toggenburg(I-poem) on(O) the(O) program(O) for(O) the(O) court(O) concerts(O) that(O) he(O) conducted(O) on(O) 13(O) March(O) 1860(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, magazine, person, award, book, event, poem, writer, organization, literary genre and O.\nSentence: Liszt included Weiheimer 's symphony on Friedrich Schiller ' s Ritter Toggenburg on the program for the court concerts that he conducted on 13 March 1860 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Liszt","included","Weiheimer","'s","symphony","on","Friedrich","Schiller","'","s","Ritter","Toggenburg","on","the","program","for","the","court","concerts","that","he","conducted","on","13","March","1860","."],"labels":["B-writer","O","B-writer","O","O","O","B-writer","I-writer","O","O","B-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","country","magazine","person","award","book","event","poem","writer","organization","literary_genre"]}
{"id":"120","dataset":"crossner_literature","split":"dev","instance":{"id":"120","prompt_labels":"The(O) only(O) completed(O) screenplay(O) ,(O) Heaven(B-book) ,(O) was(O) filmed(O) by(O) Tom(B-person) Tykwer(I-person) and(O) premiered(O) in(O) 2002(O) at(O) the(O) Berlin(B-event) International(I-event) Film(I-event) Festival(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, person, location, literary genre, country, book, organization, writer, magazine, award, event and O.\nSentence: The only completed screenplay , Heaven , was filmed by Tom Tykwer and premiered in 2002 at the Berlin International Film Festival .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","only","completed","screenplay",",","Heaven",",","was","filmed","by","Tom","Tykwer","and","premiered","in","2002","at","the","Berlin","International","Film","Festival","."],"labels":["O","O","O","O","O","B-book","O","O","O","O","B-person","I-person","O","O","O","O","O","O","B-event","I-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["poem","person","location","literary_genre","country","book","organization","writer","magazine","award","event"]}
{"id":"127","dataset":"crossner_literature","split":"dev","instance":{"id":"127","prompt_labels":"Federico(B-writer) Fellini(I-writer) ,(O) Burke(B-writer) and(O) Waller(B-writer) ,(O) 12(O) His(O) films(O) have(O) ranked(O) in(O) polls(O) such(O) as(O) Cahiers(B-magazine) du(I-magazine) cinma(I-magazine) and(O) Sight(B-magazine) &(I-magazine) Sound(I-magazine) ,(O) which(O) lists(O) his(O) 1963(O) film(O) 8(O) (O) as(O) the(O) 10th-greatest(O) film(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, event, location, literary genre, person, poem, magazine, country, award, writer, organization and O.\nSentence: Federico Fellini , Burke and Waller , 12 His films have ranked in polls such as Cahiers du cinma and Sight & Sound , which lists his 1963 film 8  as the 10th-greatest film .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Federico","Fellini",",","Burke","and","Waller",",","12","His","films","have","ranked","in","polls","such","as","Cahiers","du","cinma","and","Sight","&","Sound",",","which","lists","his","1963","film","8","","as","the","10th-greatest","film","."],"labels":["B-writer","I-writer","O","B-writer","O","B-writer","O","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","I-magazine","O","B-magazine","I-magazine","I-magazine","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["book","event","location","literary_genre","person","poem","magazine","country","award","writer","organization"]}
{"id":"128","dataset":"crossner_literature","split":"dev","instance":{"id":"128","prompt_labels":"David(B-writer) Lardner(I-writer) worked(O) for(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) as(O) a(O) general(O) reporter(O) and(O) war(O) correspondent(O) before(O) he(O) was(O) killed(O) by(O) a(O) landmine(O) near(O) Aachen(B-location) ,(O) Germany(B-country) on(O) October(O) 19(O) ,(O) 1944(O) ,(O) less(O) than(O) one(O) month(O) after(O) his(O) arrival(O) in(O) Europe(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, literary genre, writer, book, poem, award, event, location, country, magazine and O.\nSentence: David Lardner worked for The New Yorker as a general reporter and war correspondent before he was killed by a landmine near Aachen , Germany on October 19 , 1944 , less than one month after his arrival in Europe .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["David","Lardner","worked","for","The","New","Yorker","as","a","general","reporter","and","war","correspondent","before","he","was","killed","by","a","landmine","near","Aachen",",","Germany","on","October","19",",","1944",",","less","than","one","month","after","his","arrival","in","Europe","."],"labels":["B-writer","I-writer","O","O","B-magazine","I-magazine","I-magazine","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["person","organization","literary_genre","writer","book","poem","award","event","location","country","magazine"]}
{"id":"129","dataset":"crossner_literature","split":"dev","instance":{"id":"129","prompt_labels":"His(O) most(O) famous(O) works(O) are(O) The(B-book) Book(I-book) of(I-book) Healing(I-book) ,(O) a(O) philosophical(O) and(O) scientific(O) encyclopedia(O) ,(O) and(O) The(B-book) Canon(I-book) of(I-book) Medicine(I-book) ,(O) a(O) medical(O) encyclopedia(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, book, event, literary genre, organization, country, poem, location, award, writer, person and O.\nSentence: His most famous works are The Book of Healing , a philosophical and scientific encyclopedia , and The Canon of Medicine , a medical encyclopedia","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","most","famous","works","are","The","Book","of","Healing",",","a","philosophical","and","scientific","encyclopedia",",","and","The","Canon","of","Medicine",",","a","medical","encyclopedia"],"labels":["O","O","O","O","O","B-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","B-book","I-book","I-book","I-book","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","book","event","literary_genre","organization","country","poem","location","award","writer","person"]}
{"id":"130","dataset":"crossner_literature","split":"dev","instance":{"id":"130","prompt_labels":"At(O) the(O) National(B-award) Book(I-award) Award(I-award) s(O) ceremony(O) in(O) November(O) 2014(O) ,(O) Handler(B-writer) made(O) a(O) controversial(O) remark(O) after(O) author(O) Jacqueline(B-writer) Woodson(I-writer) was(O) presented(O) with(O) an(O) award(O) for(O) young(O) people(O) 's(O) literature(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, organization, country, event, location, literary genre, book, award, person, writer, poem and O.\nSentence: At the National Book Award s ceremony in November 2014 , Handler made a controversial remark after author Jacqueline Woodson was presented with an award for young people 's literature .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["At","the","National","Book","Award","s","ceremony","in","November","2014",",","Handler","made","a","controversial","remark","after","author","Jacqueline","Woodson","was","presented","with","an","award","for","young","people","'s","literature","."],"labels":["O","O","B-award","I-award","I-award","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","organization","country","event","location","literary_genre","book","award","person","writer","poem"]}
{"id":"132","dataset":"crossner_literature","split":"dev","instance":{"id":"132","prompt_labels":"Some(O) of(O) these(O) friends(O) include(O) :(O) David(B-writer) Amram(I-writer) ,(O) Bob(B-writer) Kaufman(I-writer) ;(O) Diane(B-writer) di(I-writer) Prima(I-writer) ;(O) Jim(B-writer) Cohn(I-writer) ;(O) poets(O) associated(O) with(O) the(O) Black(B-organization) Mountain(I-organization) College(I-organization) such(O) as(O) Charles(B-writer) Olson(I-writer) ,(O) Robert(B-writer) Creeley(I-writer) ,(O) and(O) Denise(B-writer) Levertov(I-writer) ;(O) poets(O) associated(O) with(O) the(O) New(B-organization) York(I-organization) School(I-organization) such(O) as(O) Frank(B-writer) O(I-writer) 'Hara(I-writer) and(O) Kenneth(B-writer) Koch(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, book, poem, magazine, event, person, literary genre, organization, award, location, writer and O.\nSentence: Some of these friends include : David Amram , Bob Kaufman ; Diane di Prima ; Jim Cohn ; poets associated with the Black Mountain College such as Charles Olson , Robert Creeley , and Denise Levertov ; poets associated with the New York School such as Frank O 'Hara and Kenneth Koch .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","of","these","friends","include",":","David","Amram",",","Bob","Kaufman",";","Diane","di","Prima",";","Jim","Cohn",";","poets","associated","with","the","Black","Mountain","College","such","as","Charles","Olson",",","Robert","Creeley",",","and","Denise","Levertov",";","poets","associated","with","the","New","York","School","such","as","Frank","O","'Hara","and","Kenneth","Koch","."],"labels":["O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-writer","I-writer","O","B-writer","I-writer","O","O","B-writer","I-writer","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["country","book","poem","magazine","event","person","literary_genre","organization","award","location","writer"]}
{"id":"133","dataset":"crossner_literature","split":"dev","instance":{"id":"133","prompt_labels":"Again(O) ,(O) in(O) the(O) English(B-event) Renaissance(I-event) fantasy(I-event) Armor(B-book) of(I-book) Light(I-book) by(O) Melissa(B-writer) Scott(I-writer) and(O) Lisa(B-writer) A.(I-writer) Barnett(I-writer) ,(O) the(O) magic(O) used(O) in(O) the(O) book(O) ,(O) by(O) Dr.(B-person) John(I-person) Dee(I-person) and(O) others(O) ,(O) actually(O) was(O) practiced(O) in(O) the(O) Renaissance(B-event) ;(O) positing(O) a(O) secret(O) history(O) of(O) effective(O) magic(O) makes(O) this(O) an(O) alternate(B-literary genre) history(I-literary genre) with(O) a(O) POD(O) ,(O) Sir(B-person) Philip(I-person) Sidney(I-person) '(O) s(O) surviving(O) the(O) Battle(B-event) of(I-event) Zutphen(I-event) in(O) 1586(O) ,(O) and(O) shortly(O) thereafter(O) saving(O) the(O) life(O) of(O) Christopher(B-person) Marlowe(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, person, event, book, country, writer, organization, magazine, award, location, poem and O.\nSentence: Again , in the English Renaissance fantasy Armor of Light by Melissa Scott and Lisa A. Barnett , the magic used in the book , by Dr. John Dee and others , actually was practiced in the Renaissance ; positing a secret history of effective magic makes this an alternate history with a POD , Sir Philip Sidney ' s surviving the Battle of Zutphen in 1586 , and shortly thereafter saving the life of Christopher Marlowe .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Again",",","in","the","English","Renaissance","fantasy","Armor","of","Light","by","Melissa","Scott","and","Lisa","A.","Barnett",",","the","magic","used","in","the","book",",","by","Dr.","John","Dee","and","others",",","actually","was","practiced","in","the","Renaissance",";","positing","a","secret","history","of","effective","magic","makes","this","an","alternate","history","with","a","POD",",","Sir","Philip","Sidney","'","s","surviving","the","Battle","of","Zutphen","in","1586",",","and","shortly","thereafter","saving","the","life","of","Christopher","Marlowe","."],"labels":["O","O","O","O","B-event","I-event","I-event","B-book","I-book","I-book","O","B-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","O","O","O","B-person","I-person","I-person","O","O","O","O","O","O","O","O","B-event","O","O","O","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","O","O","B-person","I-person","I-person","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","person","event","book","country","writer","organization","magazine","award","location","poem"]}
{"id":"134","dataset":"crossner_literature","split":"dev","instance":{"id":"134","prompt_labels":"Disraeli(B-writer) 's(O) early(O) silver(B-literary genre) fork(I-literary genre) novels(I-literary genre) Vivian(B-book) Grey(I-book) ((O) 1826(O) )(O) and(O) The(B-book) Young(I-book) Duke(I-book) ((O) 1831(O) )(O) featured(O) romanticised(O) depictions(O) of(O) aristocratic(O) life(O) ((O) despite(O) his(O) ignorance(O) of(O) it(O) )(O) with(O) character(O) sketches(O) of(O) well-known(O) public(O) figures(O) lightly(O) disguised(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, poem, literary genre, book, magazine, event, person, location, country, writer and O.\nSentence: Disraeli 's early silver fork novels Vivian Grey ( 1826 ) and The Young Duke ( 1831 ) featured romanticised depictions of aristocratic life ( despite his ignorance of it ) with character sketches of well-known public figures lightly disguised .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Disraeli","'s","early","silver","fork","novels","Vivian","Grey","(","1826",")","and","The","Young","Duke","(","1831",")","featured","romanticised","depictions","of","aristocratic","life","(","despite","his","ignorance","of","it",")","with","character","sketches","of","well-known","public","figures","lightly","disguised","."],"labels":["B-writer","O","O","B-literary genre","I-literary genre","I-literary genre","B-book","I-book","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","award","poem","literary_genre","book","magazine","event","person","location","country","writer"]}
{"id":"136","dataset":"crossner_literature","split":"dev","instance":{"id":"136","prompt_labels":"Investigative(O) journalist(O) Michael(B-writer) Specter(I-writer) ,(O) in(O) an(O) article(O) in(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) on(O) 25(O) August(O) 2014(O) entitled(O) Seeds(O) of(O) Doubt(O) ,(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, magazine, book, event, organization, country, person, award, location, literary genre, writer and O.\nSentence: Investigative journalist Michael Specter , in an article in The New Yorker on 25 August 2014 entitled Seeds of Doubt ,","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Investigative","journalist","Michael","Specter",",","in","an","article","in","The","New","Yorker","on","25","August","2014","entitled","Seeds","of","Doubt",","],"labels":["O","O","B-writer","I-writer","O","O","O","O","O","B-magazine","I-magazine","I-magazine","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","magazine","book","event","organization","country","person","award","location","literary_genre","writer"]}
{"id":"142","dataset":"crossner_literature","split":"dev","instance":{"id":"142","prompt_labels":"Lerner(B-writer) and(O) Loewe(B-writer) 's(O) run(O) of(O) success(O) continued(O) with(O) their(O) next(O) project(O) ,(O) a(O) film(O) adaptation(O) of(O) stories(O) from(O) Colette(B-writer) ,(O) the(O) Academy(B-award) Awards(I-award) -winning(O) film(O) musical(O) Gigi(O) ,(O) starring(O) Leslie(B-person) Caron(I-person) ,(O) Louis(B-person) Jourdan(I-person) and(O) Maurice(B-person) Chevalier(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, event, writer, award, poem, magazine, location, organization, person, literary genre, country and O.\nSentence: Lerner and Loewe 's run of success continued with their next project , a film adaptation of stories from Colette , the Academy Awards -winning film musical Gigi , starring Leslie Caron , Louis Jourdan and Maurice Chevalier .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Lerner","and","Loewe","'s","run","of","success","continued","with","their","next","project",",","a","film","adaptation","of","stories","from","Colette",",","the","Academy","Awards","-winning","film","musical","Gigi",",","starring","Leslie","Caron",",","Louis","Jourdan","and","Maurice","Chevalier","."],"labels":["B-writer","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","O","O","B-award","I-award","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["book","event","writer","award","poem","magazine","location","organization","person","literary_genre","country"]}
{"id":"143","dataset":"crossner_literature","split":"dev","instance":{"id":"143","prompt_labels":"For(O) example(O) ,(O) Susanna(B-writer) Moodie(I-writer) and(O) Catharine(B-writer) Parr(I-writer) Traill(I-writer) ,(O) English(O) sisters(O) who(O) adopted(O) the(O) country(O) as(O) their(O) own(O) ,(O) moved(O) to(O) Upper(B-country) Canada(I-country) in(O) 1832(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, literary genre, magazine, award, event, person, location, writer, book, country, organization and O.\nSentence: For example , Susanna Moodie and Catharine Parr Traill , English sisters who adopted the country as their own , moved to Upper Canada in 1832 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","example",",","Susanna","Moodie","and","Catharine","Parr","Traill",",","English","sisters","who","adopted","the","country","as","their","own",",","moved","to","Upper","Canada","in","1832","."],"labels":["O","O","O","B-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","literary_genre","magazine","award","event","person","location","writer","book","country","organization"]}
{"id":"144","dataset":"crossner_literature","split":"dev","instance":{"id":"144","prompt_labels":"In(O) a(O) review(O) in(O) The(B-magazine) Dial(I-magazine) ,(O) T.(B-writer) S.(I-writer) Eliot(I-writer) said(O) of(O) Ulysses(B-book) :(O) I(O) hold(O) this(O) book(O) to(O) be(O) the(O) most(O) important(O) expression(O) which(O) the(O) present(O) age(O) has(O) found(O) ;(O) it(O) is(O) a(O) book(O) to(O) which(O) we(O) are(O) all(O) indebted(O) ,(O) and(O) from(O) which(O) none(O) of(O) us(O) can(O) escape(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, location, literary genre, book, event, award, person, organization, country, poem, writer and O.\nSentence: In a review in The Dial , T. S. Eliot said of Ulysses : I hold this book to be the most important expression which the present age has found ; it is a book to which we are all indebted , and from which none of us can escape .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","a","review","in","The","Dial",",","T.","S.","Eliot","said","of","Ulysses",":","I","hold","this","book","to","be","the","most","important","expression","which","the","present","age","has","found",";","it","is","a","book","to","which","we","are","all","indebted",",","and","from","which","none","of","us","can","escape","."],"labels":["O","O","O","O","B-magazine","I-magazine","O","B-writer","I-writer","I-writer","O","O","B-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","location","literary_genre","book","event","award","person","organization","country","poem","writer"]}
{"id":"145","dataset":"crossner_literature","split":"dev","instance":{"id":"145","prompt_labels":"Spengler(B-person) spent(O) his(O) final(O) years(O) in(O) Munich(B-location) ,(O) listening(O) to(O) Ludwig(B-person) van(I-person) Beethoven(I-person) ,(O) reading(O) Molire(B-writer) and(O) Shakespeare(B-writer) ,(O) buying(O) several(O) thousand(O) books(O) ,(O) and(O) collecting(O) ancient(O) Turkey(B-country) ,(O) Persia(B-country) n(O) and(O) India(B-country) n(O) weapons(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, poem, location, event, organization, country, writer, book, person, magazine, literary genre and O.\nSentence: Spengler spent his final years in Munich , listening to Ludwig van Beethoven , reading Molire and Shakespeare , buying several thousand books , and collecting ancient Turkey , Persia n and India n weapons .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Spengler","spent","his","final","years","in","Munich",",","listening","to","Ludwig","van","Beethoven",",","reading","Molire","and","Shakespeare",",","buying","several","thousand","books",",","and","collecting","ancient","Turkey",",","Persia","n","and","India","n","weapons","."],"labels":["B-person","O","O","O","O","O","B-location","O","O","O","B-person","I-person","I-person","O","O","B-writer","O","B-writer","O","O","O","O","O","O","O","O","O","B-country","O","B-country","O","O","B-country","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","poem","location","event","organization","country","writer","book","person","magazine","literary_genre"]}
{"id":"146","dataset":"crossner_literature","split":"dev","instance":{"id":"146","prompt_labels":"Novelist(O) James(B-writer) Joyce(I-writer) noted(O) that(O) the(O) TRUE(O) symbol(O) of(O) the(O) British(B-country) Empire(I-country) is(O) Robinson(B-book) Crusoe(I-book) ,(O) to(O) whom(O) he(O) ascribed(O) stereotypical(O) and(O) somewhat(O) hostile(O) English(O) racial(O) characteristics(O) :(O) He(O) is(O) the(O) TRUE(O) prototype(O) of(O) the(O) British(O) colonist(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, book, writer, poem, event, organization, magazine, country, award, person, literary genre and O.\nSentence: Novelist James Joyce noted that the TRUE symbol of the British Empire is Robinson Crusoe , to whom he ascribed stereotypical and somewhat hostile English racial characteristics : He is the TRUE prototype of the British colonist .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Novelist","James","Joyce","noted","that","the","TRUE","symbol","of","the","British","Empire","is","Robinson","Crusoe",",","to","whom","he","ascribed","stereotypical","and","somewhat","hostile","English","racial","characteristics",":","He","is","the","TRUE","prototype","of","the","British","colonist","."],"labels":["O","B-writer","I-writer","O","O","O","O","O","O","O","B-country","I-country","O","B-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","book","writer","poem","event","organization","magazine","country","award","person","literary_genre"]}
{"id":"147","dataset":"crossner_literature","split":"dev","instance":{"id":"147","prompt_labels":"Thompson(B-writer) remains(O) best(O) known(O) for(O) Fear(B-book) and(I-book) Loathing(I-book) in(I-book) Las(I-book) Vegas(I-book) ((O) 1971(O) )(O) ,(O) a(O) book(O) first(O) serialized(O) in(O) Rolling(B-magazine) Stone(I-magazine) in(O) which(O) he(O) grapples(O) with(O) the(O) implications(O) of(O) what(O) he(O) considered(O) the(O) failure(O) of(O) the(O) 1960s(B-event) counterculture(I-event) movement(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, country, award, event, organization, poem, magazine, person, book, literary genre, location and O.\nSentence: Thompson remains best known for Fear and Loathing in Las Vegas ( 1971 ) , a book first serialized in Rolling Stone in which he grapples with the implications of what he considered the failure of the 1960s counterculture movement .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Thompson","remains","best","known","for","Fear","and","Loathing","in","Las","Vegas","(","1971",")",",","a","book","first","serialized","in","Rolling","Stone","in","which","he","grapples","with","the","implications","of","what","he","considered","the","failure","of","the","1960s","counterculture","movement","."],"labels":["B-writer","O","O","O","O","B-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["writer","country","award","event","organization","poem","magazine","person","book","literary_genre","location"]}
{"id":"149","dataset":"crossner_literature","split":"dev","instance":{"id":"149","prompt_labels":"Out(O) of(O) public(O) office(O) for(O) the(O) first(O) time(O) since(O) the(O) 1960s(O) ,(O) Bush(B-person) became(O) chairman(O) on(O) the(O) Executive(O) Committee(O) of(O) the(O) First(B-organization) International(I-organization) Bank(I-organization) in(O) Houston.(B-location) continued(O) his(O) membership(O) in(O) the(O) Council(B-organization) on(I-organization) Foreign(I-organization) Relations(I-organization) ,(O) and(O) joined(O) the(O) Trilateral(B-organization) Commission(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, organization, book, literary genre, location, country, event, magazine, poem, writer and O.\nSentence: Out of public office for the first time since the 1960s , Bush became chairman on the Executive Committee of the First International Bank in Houston. continued his membership in the Council on Foreign Relations , and joined the Trilateral Commission .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Out","of","public","office","for","the","first","time","since","the","1960s",",","Bush","became","chairman","on","the","Executive","Committee","of","the","First","International","Bank","in","Houston.","continued","his","membership","in","the","Council","on","Foreign","Relations",",","and","joined","the","Trilateral","Commission","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-person","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-location","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","B-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["person","award","organization","book","literary_genre","location","country","event","magazine","poem","writer"]}
{"id":"152","dataset":"crossner_literature","split":"dev","instance":{"id":"152","prompt_labels":"Elkin(B-writer) won(O) the(O) National(B-award) Book(I-award) Critics(I-award) Circle(I-award) Award(I-award) on(O) two(O) occasions(O) :(O) for(O) George(B-book) Mills(I-book) in(O) 1982(O) and(O) for(O) Mrs.(B-book) Ted(I-book) Bliss(I-book) ,(O) his(O) last(O) novel(B-literary genre) ,(O) in(O) 1995(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, book, writer, literary genre, country, location, person, poem, magazine, event and O.\nSentence: Elkin won the National Book Critics Circle Award on two occasions : for George Mills in 1982 and for Mrs. Ted Bliss , his last novel , in 1995 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Elkin","won","the","National","Book","Critics","Circle","Award","on","two","occasions",":","for","George","Mills","in","1982","and","for","Mrs.","Ted","Bliss",",","his","last","novel",",","in","1995","."],"labels":["B-writer","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","O","O","B-book","I-book","O","O","O","O","B-book","I-book","I-book","O","O","O","B-literary genre","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","award","book","writer","literary_genre","country","location","person","poem","magazine","event"]}
{"id":"153","dataset":"crossner_literature","split":"dev","instance":{"id":"153","prompt_labels":"In(O) November(O) 1924(O) he(O) was(O) awarded(O) the(O) Nobel(B-award) Prize(I-award) for(I-award) Literature(I-award) over(O) rivals(O) Thomas(B-writer) Mann(I-writer) ,(O) George(B-writer) Bernard(I-writer) Shaw(I-writer) and(O) Thomas(B-writer) Hardy(I-writer) ,(O) after(O) he(O) had(O) been(O) nominated(O) by(O) Anders(B-writer) sterling(I-writer) ,(O) member(O) of(O) the(O) Swedish(B-organization) Academy(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, writer, book, award, poem, country, literary genre, person, location, organization, magazine and O.\nSentence: In November 1924 he was awarded the Nobel Prize for Literature over rivals Thomas Mann , George Bernard Shaw and Thomas Hardy , after he had been nominated by Anders sterling , member of the Swedish Academy .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","November","1924","he","was","awarded","the","Nobel","Prize","for","Literature","over","rivals","Thomas","Mann",",","George","Bernard","Shaw","and","Thomas","Hardy",",","after","he","had","been","nominated","by","Anders","sterling",",","member","of","the","Swedish","Academy","."],"labels":["O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","B-writer","I-writer","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","B-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["event","writer","book","award","poem","country","literary_genre","person","location","organization","magazine"]}
{"id":"154","dataset":"crossner_literature","split":"dev","instance":{"id":"154","prompt_labels":"Many(O) old(O) and(O) new(O) friends(O) and(O) family(O) showed(O) up(O) to(O) support(O) the(O) Pranksters(O) on(O) this(O) tour(O) ,(O) which(O) took(O) them(O) from(O) Seattle(B-location) 's(O) Bumbershoot(B-event) all(O) along(O) the(O) West(B-location) Coast(I-location) ,(O) including(O) a(O) sold-out(O) two-night(O) run(O) at(O) The(B-location) Fillmore(I-location) in(O) San(B-location) Francisco(I-location) to(O) Boulder(B-location) ,(O) Colorado(B-location) ,(O) where(O) they(O) coaxed(O) the(O) Beat(O) Generation(O) poet(O) Allen(B-writer) Ginsberg(I-writer) into(O) performing(O) with(O) them(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, literary genre, award, book, writer, organization, country, magazine, event, location, poem and O.\nSentence: Many old and new friends and family showed up to support the Pranksters on this tour , which took them from Seattle 's Bumbershoot all along the West Coast , including a sold-out two-night run at The Fillmore in San Francisco to Boulder , Colorado , where they coaxed the Beat Generation poet Allen Ginsberg into performing with them .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Many","old","and","new","friends","and","family","showed","up","to","support","the","Pranksters","on","this","tour",",","which","took","them","from","Seattle","'s","Bumbershoot","all","along","the","West","Coast",",","including","a","sold-out","two-night","run","at","The","Fillmore","in","San","Francisco","to","Boulder",",","Colorado",",","where","they","coaxed","the","Beat","Generation","poet","Allen","Ginsberg","into","performing","with","them","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-event","O","O","O","B-location","I-location","O","O","O","O","O","O","O","B-location","I-location","O","B-location","I-location","O","B-location","O","B-location","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","literary_genre","award","book","writer","organization","country","magazine","event","location","poem"]}
{"id":"157","dataset":"crossner_literature","split":"dev","instance":{"id":"157","prompt_labels":"In(O) 1987(O) Allan(B-person) Gotthelf(I-person) ,(O) George(B-person) Walsh(I-person) and(O) David(B-person) Kelley(I-person) co-founded(O) the(O) Ayn(B-organization) Rand(I-organization) Society(I-organization) ,(O) a(O) group(O) affiliated(O) with(O) the(O) American(B-organization) Philosophical(I-organization) Association(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, location, writer, person, award, book, poem, event, literary genre, magazine and O.\nSentence: In 1987 Allan Gotthelf , George Walsh and David Kelley co-founded the Ayn Rand Society , a group affiliated with the American Philosophical Association .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1987","Allan","Gotthelf",",","George","Walsh","and","David","Kelley","co-founded","the","Ayn","Rand","Society",",","a","group","affiliated","with","the","American","Philosophical","Association","."],"labels":["O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["organization","country","location","writer","person","award","book","poem","event","literary_genre","magazine"]}
{"id":"159","dataset":"crossner_literature","split":"dev","instance":{"id":"159","prompt_labels":"In(O) 2000(O) ,(O) Anderson(B-writer) starred(O) in(O) the(O) film(O) The(O) House(O) of(O) Mirth(O) with(O) Eric(B-person) Stoltz(I-person) -(O) Terence(B-person) Davies(I-person) '(O) adaptation(O) of(O) the(O) Edith(B-writer) Wharton(I-writer) novel(B-literary genre) of(O) the(O) The(B-book) House(I-book) of(I-book) Mirth(I-book) -(O) for(O) which(O) she(O) won(O) critical(O) acclaim(O) and(O) awards(O) such(O) as(O) the(O) British(B-award) Independent(I-award) Film(I-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) ,(O) Village(B-award) Voice(I-award) Film(I-award) Poll(I-award) Best(I-award) Lead(I-award) Performance(I-award) ,(O) and(O) a(O) nomination(O) for(O) the(O) National(B-award) Society(I-award) of(I-award) Film(I-award) Critics(I-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, literary genre, award, location, writer, person, book, event, country, poem, magazine and O.\nSentence: In 2000 , Anderson starred in the film The House of Mirth with Eric Stoltz - Terence Davies ' adaptation of the Edith Wharton novel of the The House of Mirth - for which she won critical acclaim and awards such as the British Independent Film Award for Best Actress , Village Voice Film Poll Best Lead Performance , and a nomination for the National Society of Film Critics Award for Best Actress .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2000",",","Anderson","starred","in","the","film","The","House","of","Mirth","with","Eric","Stoltz","-","Terence","Davies","'","adaptation","of","the","Edith","Wharton","novel","of","the","The","House","of","Mirth","-","for","which","she","won","critical","acclaim","and","awards","such","as","the","British","Independent","Film","Award","for","Best","Actress",",","Village","Voice","Film","Poll","Best","Lead","Performance",",","and","a","nomination","for","the","National","Society","of","Film","Critics","Award","for","Best","Actress","."],"labels":["O","O","O","B-writer","O","O","O","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","O","O","O","B-writer","I-writer","B-literary genre","O","O","B-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["organization","literary_genre","award","location","writer","person","book","event","country","poem","magazine"]}
{"id":"161","dataset":"crossner_literature","split":"dev","instance":{"id":"161","prompt_labels":"Gordon(B-person) continued(O) her(O) stage(O) acting(O) career(O) in(O) the(O) 1950s(O) ,(O) and(O) was(O) nominated(O) for(O) a(O) 1956(O) Tony(B-award) Award(I-award) ,(O) for(O) Tony(B-award) Award(I-award) for(I-award) Best(I-award) Performance(I-award) by(O) a(O) Leading(O) Actress(O) in(O) a(O) Play(O) ,(O) for(O) her(O) portrayal(O) of(O) Dolly(B-person) Levi(I-person) in(O) Thornton(B-writer) Wilder(I-writer) '(O) s(O) The(B-book) Matchmaker(I-book) ,(O) a(O) role(O) she(O) also(O) played(O) in(O) London(B-location) ,(O) Edinburgh(B-location) and(O) Berlin(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, poem, organization, literary genre, location, event, book, magazine, award, country, writer and O.\nSentence: Gordon continued her stage acting career in the 1950s , and was nominated for a 1956 Tony Award , for Tony Award for Best Performance by a Leading Actress in a Play , for her portrayal of Dolly Levi in Thornton Wilder ' s The Matchmaker , a role she also played in London , Edinburgh and Berlin .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Gordon","continued","her","stage","acting","career","in","the","1950s",",","and","was","nominated","for","a","1956","Tony","Award",",","for","Tony","Award","for","Best","Performance","by","a","Leading","Actress","in","a","Play",",","for","her","portrayal","of","Dolly","Levi","in","Thornton","Wilder","'","s","The","Matchmaker",",","a","role","she","also","played","in","London",",","Edinburgh","and","Berlin","."],"labels":["B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","B-writer","I-writer","O","O","B-book","I-book","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["person","poem","organization","literary_genre","location","event","book","magazine","award","country","writer"]}
{"id":"162","dataset":"crossner_literature","split":"dev","instance":{"id":"162","prompt_labels":"On(O) February(O) 2(O) ,(O) 1966(O) ,(O) he(O) made(O) his(O) Broadway(B-organization) debut(O) as(O) Harry(B-person) Roat(I-person) ,(I-person) Jr(I-person) in(O) Frederick(B-writer) Knott(I-writer) '(O) s(O) Wait(B-book) Until(I-book) Dark(I-book) at(O) the(O) Ethel(B-location) Barrymore(I-location) Theatre(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, literary genre, person, poem, award, country, book, location, magazine, organization, writer and O.\nSentence: On February 2 , 1966 , he made his Broadway debut as Harry Roat , Jr in Frederick Knott ' s Wait Until Dark at the Ethel Barrymore Theatre .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","February","2",",","1966",",","he","made","his","Broadway","debut","as","Harry","Roat",",","Jr","in","Frederick","Knott","'","s","Wait","Until","Dark","at","the","Ethel","Barrymore","Theatre","."],"labels":["O","O","O","O","O","O","O","O","O","B-organization","O","O","B-person","I-person","I-person","I-person","O","B-writer","I-writer","O","O","B-book","I-book","I-book","O","O","B-location","I-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["event","literary_genre","person","poem","award","country","book","location","magazine","organization","writer"]}
{"id":"164","dataset":"crossner_literature","split":"dev","instance":{"id":"164","prompt_labels":"In(O) 2005(O) ,(O) Arkham(B-organization) House(I-organization) was(O) awarded(O) the(O) World(B-award) Fantasy(I-award) Award(I-award) for(I-award) Small(I-award) Press(I-award) Achievements(I-award) -(O) the(O) trophy(O) at(O) that(O) time(O) was(O) a(O) bust(O) of(O) H.(B-writer) P.(I-writer) Lovecraft(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, award, poem, writer, organization, country, book, event, literary genre, magazine and O.\nSentence: In 2005 , Arkham House was awarded the World Fantasy Award for Small Press Achievements - the trophy at that time was a bust of H. P. Lovecraft .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2005",",","Arkham","House","was","awarded","the","World","Fantasy","Award","for","Small","Press","Achievements","-","the","trophy","at","that","time","was","a","bust","of","H.","P.","Lovecraft","."],"labels":["O","O","O","B-organization","I-organization","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["location","person","award","poem","writer","organization","country","book","event","literary_genre","magazine"]}
{"id":"165","dataset":"crossner_literature","split":"dev","instance":{"id":"165","prompt_labels":"In(O) addition(O) to(O) receiving(O) a(O) star(O) on(O) the(O) Hollywood(B-location) Walk(I-location) of(I-location) Fame(I-location) ,(O) media(O) appearances(O) included(O) write-ups(O) in(O) CCM(B-magazine) Magazine(I-magazine) ,(O) and(O) a(O) performance(O) on(O) The(O) View(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, magazine, writer, country, literary genre, organization, book, location, person, poem, award and O.\nSentence: In addition to receiving a star on the Hollywood Walk of Fame , media appearances included write-ups in CCM Magazine , and a performance on The View .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","addition","to","receiving","a","star","on","the","Hollywood","Walk","of","Fame",",","media","appearances","included","write-ups","in","CCM","Magazine",",","and","a","performance","on","The","View","."],"labels":["O","O","O","O","O","O","O","O","B-location","I-location","I-location","I-location","O","O","O","O","O","O","B-magazine","I-magazine","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","magazine","writer","country","literary_genre","organization","book","location","person","poem","award"]}
{"id":"167","dataset":"crossner_literature","split":"dev","instance":{"id":"167","prompt_labels":"In(O) the(O) same(O) year(O) ,(O) he(O) produced(O) the(O) first(O) French(O) language(O) editions(O) of(O) Joseph(B-writer) Conrad(I-writer) '(O) s(O) Heart(B-book) of(I-book) Darkness(I-book) and(O) Lord(B-book) Jim(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, location, magazine, writer, literary genre, organization, country, award, person, poem, event and O.\nSentence: In the same year , he produced the first French language editions of Joseph Conrad ' s Heart of Darkness and Lord Jim .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","same","year",",","he","produced","the","first","French","language","editions","of","Joseph","Conrad","'","s","Heart","of","Darkness","and","Lord","Jim","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","B-book","I-book","I-book","O","B-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["book","location","magazine","writer","literary_genre","organization","country","award","person","poem","event"]}
{"id":"169","dataset":"crossner_literature","split":"dev","instance":{"id":"169","prompt_labels":"It(O) is(O) based(O) on(O) H.(B-writer) P.(I-writer) Lovecraft(I-writer) '(O) s(O) Cthulhu(O) Mythos(O) ,(O) particularly(O) At(B-book) the(I-book) Mountains(I-book) of(I-book) Madness(I-book) ,(O) and(O) is(O) a(O) follow-up(O) to(O) Infogrames(B-organization) '(O) earlier(O) Shadow(O) of(O) the(O) Comet(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, book, literary genre, organization, writer, person, poem, award, magazine, location, country and O.\nSentence: It is based on H. P. Lovecraft ' s Cthulhu Mythos , particularly At the Mountains of Madness , and is a follow-up to Infogrames ' earlier Shadow of the Comet .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","is","based","on","H.","P.","Lovecraft","'","s","Cthulhu","Mythos",",","particularly","At","the","Mountains","of","Madness",",","and","is","a","follow-up","to","Infogrames","'","earlier","Shadow","of","the","Comet","."],"labels":["O","O","O","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","B-organization","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","book","literary_genre","organization","writer","person","poem","award","magazine","location","country"]}
{"id":"171","dataset":"crossner_literature","split":"dev","instance":{"id":"171","prompt_labels":"But(O) opponents(O) of(O) this(O) proposition(O) claim(O) that(O) Rabindranath(B-writer) Tagore(I-writer) mentioned(O) only(O) the(O) border(O) states(O) of(O) India(B-country) to(O) include(O) complete(O) India(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, book, event, writer, poem, literary genre, award, organization, location, person, magazine and O.\nSentence: But opponents of this proposition claim that Rabindranath Tagore mentioned only the border states of India to include complete India .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["But","opponents","of","this","proposition","claim","that","Rabindranath","Tagore","mentioned","only","the","border","states","of","India","to","include","complete","India","."],"labels":["O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","B-country","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["country","book","event","writer","poem","literary_genre","award","organization","location","person","magazine"]}
{"id":"172","dataset":"crossner_literature","split":"dev","instance":{"id":"172","prompt_labels":"In(O) 1857(O) ,(O) Dickens(B-writer) hired(O) professional(O) actresses(O) for(O) the(O) play(O) The(B-book) Frozen(I-book) Deep(I-book) ,(O) written(O) by(O) him(O) and(O) his(O) protg(O) ,(O) Wilkie(B-writer) Collins(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, writer, literary genre, organization, person, book, location, magazine, poem, event, award and O.\nSentence: In 1857 , Dickens hired professional actresses for the play The Frozen Deep , written by him and his protg , Wilkie Collins .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1857",",","Dickens","hired","professional","actresses","for","the","play","The","Frozen","Deep",",","written","by","him","and","his","protg",",","Wilkie","Collins","."],"labels":["O","O","O","B-writer","O","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["country","writer","literary_genre","organization","person","book","location","magazine","poem","event","award"]}
{"id":"173","dataset":"crossner_literature","split":"dev","instance":{"id":"173","prompt_labels":"A(O) documentary(O) film(O) about(O) Rivers(O) ,(O) Joan(O) Rivers(O) :(O) A(O) Piece(O) of(O) Work(O) ,(O) premiered(O) at(O) the(O) San(B-event) Francisco(I-event) International(I-event) Film(I-event) Festival(I-event) on(O) May(O) 6(O) ,(O) 2010(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, organization, location, country, award, poem, writer, literary genre, magazine, event, person and O.\nSentence: A documentary film about Rivers , Joan Rivers : A Piece of Work , premiered at the San Francisco International Film Festival on May 6 , 2010 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","documentary","film","about","Rivers",",","Joan","Rivers",":","A","Piece","of","Work",",","premiered","at","the","San","Francisco","International","Film","Festival","on","May","6",",","2010","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["book","organization","location","country","award","poem","writer","literary_genre","magazine","event","person"]}
{"id":"176","dataset":"crossner_literature","split":"dev","instance":{"id":"176","prompt_labels":"Satirical(B-literary genre) poets(O) outside(O) England(B-country) include(O) Poland(B-country) '(O) s(O) Ignacy(B-writer) Krasicki(I-writer) ,(O) Azerbaijan(B-country) '(O) s(O) Mirza(B-writer) Alakbar(I-writer) Sabir(I-writer) and(O) Portugal(B-country) '(O) s(O) Manuel(B-writer) Maria(I-writer) Barbosa(I-writer) du(I-writer) Bocage(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, literary genre, organization, country, award, writer, person, location, poem, event, book and O.\nSentence: Satirical poets outside England include Poland ' s Ignacy Krasicki , Azerbaijan ' s Mirza Alakbar Sabir and Portugal ' s Manuel Maria Barbosa du Bocage .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Satirical","poets","outside","England","include","Poland","'","s","Ignacy","Krasicki",",","Azerbaijan","'","s","Mirza","Alakbar","Sabir","and","Portugal","'","s","Manuel","Maria","Barbosa","du","Bocage","."],"labels":["B-literary genre","O","O","B-country","O","B-country","O","O","B-writer","I-writer","O","B-country","O","O","B-writer","I-writer","I-writer","O","B-country","O","O","B-writer","I-writer","I-writer","I-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["magazine","literary_genre","organization","country","award","writer","person","location","poem","event","book"]}
{"id":"178","dataset":"crossner_literature","split":"dev","instance":{"id":"178","prompt_labels":"Guillaume(B-writer) Apollinaire(I-writer) ,(O) Andr(B-writer) Salmon(I-writer) and(O) Max(B-writer) Jacob(I-writer) sought(O) him(O) out(O) in(O) his(O) truncated(O) apartment(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, book, magazine, person, writer, poem, event, award, literary genre, country and O.\nSentence: Guillaume Apollinaire , Andr Salmon and Max Jacob sought him out in his truncated apartment .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Guillaume","Apollinaire",",","Andr","Salmon","and","Max","Jacob","sought","him","out","in","his","truncated","apartment","."],"labels":["B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","location","book","magazine","person","writer","poem","event","award","literary_genre","country"]}
{"id":"179","dataset":"crossner_literature","split":"dev","instance":{"id":"179","prompt_labels":"It(O) has(O) been(O) credited(O) by(O) American(O) poets(O) like(O) W.(B-writer) S.(I-writer) Merwin(I-writer) ,(O) and(O) American(O) scholars(O) like(O) Clare(B-writer) Cavanagh(I-writer) ,(O) with(O) having(O) a(O) profound(O) impact(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, magazine, person, location, event, poem, award, literary genre, book, organization, country and O.\nSentence: It has been credited by American poets like W. S. Merwin , and American scholars like Clare Cavanagh , with having a profound impact .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","has","been","credited","by","American","poets","like","W.","S.","Merwin",",","and","American","scholars","like","Clare","Cavanagh",",","with","having","a","profound","impact","."],"labels":["O","O","O","O","O","O","O","O","B-writer","I-writer","I-writer","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["writer","magazine","person","location","event","poem","award","literary_genre","book","organization","country"]}
{"id":"181","dataset":"crossner_literature","split":"dev","instance":{"id":"181","prompt_labels":"Like(O) his(O) contemporaries(O) Algernon(B-writer) Blackwood(I-writer) and(O) Arthur(B-writer) Machen(I-writer) ,(O) Rohmer(B-writer) claimed(O) membership(O) to(O) one(O) of(O) the(O) factions(O) of(O) the(O) qabbalistic(O) Hermetic(B-organization) Order(I-organization) of(I-organization) the(I-organization) Golden(I-organization) Dawn(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, award, organization, literary genre, event, magazine, book, poem, location, country, person and O.\nSentence: Like his contemporaries Algernon Blackwood and Arthur Machen , Rohmer claimed membership to one of the factions of the qabbalistic Hermetic Order of the Golden Dawn .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Like","his","contemporaries","Algernon","Blackwood","and","Arthur","Machen",",","Rohmer","claimed","membership","to","one","of","the","factions","of","the","qabbalistic","Hermetic","Order","of","the","Golden","Dawn","."],"labels":["O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["writer","award","organization","literary_genre","event","magazine","book","poem","location","country","person"]}
{"id":"183","dataset":"crossner_literature","split":"dev","instance":{"id":"183","prompt_labels":"The(O) Pulitzer(B-award) Prize(I-award) -winning(O) The(B-book) Grapes(I-book) of(I-book) Wrath(I-book) ((O) 1939(O) )(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, magazine, literary genre, award, person, writer, poem, location, book, country, organization and O.\nSentence: The Pulitzer Prize -winning The Grapes of Wrath ( 1939 )","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Pulitzer","Prize","-winning","The","Grapes","of","Wrath","(","1939",")"],"labels":["O","B-award","I-award","O","B-book","I-book","I-book","I-book","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","magazine","literary_genre","award","person","writer","poem","location","book","country","organization"]}
{"id":"186","dataset":"crossner_literature","split":"dev","instance":{"id":"186","prompt_labels":"Thereafter(O) ,(O) the(O) first(O) piece(O) to(O) provide(O) substantial(O) information(O) about(O) Pynchon(B-writer) 's(O) personal(O) life(O) was(O) a(O) biographical(O) account(O) written(O) by(O) a(O) former(O) Cornell(B-organization) University(I-organization) friend(O) ,(O) Jules(B-writer) Siegel(I-writer) ,(O) and(O) published(O) in(O) Playboy(B-magazine) magazine(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, magazine, event, poem, literary genre, location, award, writer, country, person, organization and O.\nSentence: Thereafter , the first piece to provide substantial information about Pynchon 's personal life was a biographical account written by a former Cornell University friend , Jules Siegel , and published in Playboy magazine .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Thereafter",",","the","first","piece","to","provide","substantial","information","about","Pynchon","'s","personal","life","was","a","biographical","account","written","by","a","former","Cornell","University","friend",",","Jules","Siegel",",","and","published","in","Playboy","magazine","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","B-writer","I-writer","O","O","O","O","B-magazine","O","O"],"target_index":null,"target_label":null},"label_list":["book","magazine","event","poem","literary_genre","location","award","writer","country","person","organization"]}
{"id":"187","dataset":"crossner_literature","split":"dev","instance":{"id":"187","prompt_labels":"H.(B-writer) P.(I-writer) Lovecraft(I-writer) stated(O) that(O) in(O) sheer(O) daemonic(O) strangeness(O) and(O) fertility(O) of(O) conception(O) ,(O) Clark(B-writer) Ashton(I-writer) Smith(I-writer) is(O) perhaps(O) unexcelled(O) ,(O) and(O) Ray(B-writer) Bradbury(I-writer) said(O) that(O) Smith(B-writer) filled(O) my(O) mind(O) with(O) incredible(O) worlds(O) ,(O) impossibly(O) beautiful(O) cities(O) ,(O) and(O) still(O) more(O) fantastic(O) creatures(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, person, award, book, poem, event, literary genre, writer, country, location, organization and O.\nSentence: H. P. Lovecraft stated that in sheer daemonic strangeness and fertility of conception , Clark Ashton Smith is perhaps unexcelled , and Ray Bradbury said that Smith filled my mind with incredible worlds , impossibly beautiful cities , and still more fantastic creatures .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["H.","P.","Lovecraft","stated","that","in","sheer","daemonic","strangeness","and","fertility","of","conception",",","Clark","Ashton","Smith","is","perhaps","unexcelled",",","and","Ray","Bradbury","said","that","Smith","filled","my","mind","with","incredible","worlds",",","impossibly","beautiful","cities",",","and","still","more","fantastic","creatures","."],"labels":["B-writer","I-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","I-writer","O","O","O","O","O","B-writer","I-writer","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","person","award","book","poem","event","literary_genre","writer","country","location","organization"]}
{"id":"188","dataset":"crossner_literature","split":"dev","instance":{"id":"188","prompt_labels":"In(O) September(O) 2006(O) Kenneth(B-person) Branagh(I-person) announced(O) at(O) the(O) Venice(B-event) Film(I-event) Festival(I-event) his(O) new(O) film(O) of(O) the(O) play(O) ,(O) with(O) the(O) screenplay(O) by(O) Nobel(B-award) laureate(O) Harold(B-writer) Pinter(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, magazine, poem, writer, organization, person, book, event, literary genre, country, location and O.\nSentence: In September 2006 Kenneth Branagh announced at the Venice Film Festival his new film of the play , with the screenplay by Nobel laureate Harold Pinter .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","September","2006","Kenneth","Branagh","announced","at","the","Venice","Film","Festival","his","new","film","of","the","play",",","with","the","screenplay","by","Nobel","laureate","Harold","Pinter","."],"labels":["O","O","O","B-person","I-person","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","B-award","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["award","magazine","poem","writer","organization","person","book","event","literary_genre","country","location"]}
{"id":"190","dataset":"crossner_literature","split":"dev","instance":{"id":"190","prompt_labels":"Another(O) poem(O) that(O) is(O) ambiguous(O) in(O) this(O) respect(O) is(O) The(B-poem) Virgin(I-poem) Carrying(I-poem) a(I-poem) Lantern(I-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, magazine, country, literary genre, book, person, writer, poem, organization, location, event and O.\nSentence: Another poem that is ambiguous in this respect is The Virgin Carrying a Lantern .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Another","poem","that","is","ambiguous","in","this","respect","is","The","Virgin","Carrying","a","Lantern","."],"labels":["O","O","O","O","O","O","O","O","O","B-poem","I-poem","I-poem","I-poem","I-poem","O"],"target_index":null,"target_label":null},"label_list":["award","magazine","country","literary_genre","book","person","writer","poem","organization","location","event"]}
{"id":"191","dataset":"crossner_literature","split":"dev","instance":{"id":"191","prompt_labels":"Of(O) Things(O) to(O) Come(O) ,(O) The(B-magazine) New(I-magazine) York(I-magazine) Times(I-magazine) Book(I-magazine) Review(I-magazine) ,(O) October(O) 26(O) ,(O) 1975(O) Theodore(B-writer) Sturgeon(I-writer) praised(O) The(B-book) Dispossessed(I-book) as(O) a(O) beautifully(O) written(O) ,(O) beautifully(O) composed(O) book(O) ,(O) saying(O) it(O) performs(O) one(O) of(O) science(B-literary genre) fiction(I-literary genre) 's(O) prime(O) functions(O) ,(O) which(O) is(O) to(O) create(O) another(O) kind(O) of(O) social(O) system(O) to(O) see(O) how(O) it(O) would(O) work(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, location, event, magazine, organization, country, poem, award, person, writer, book and O.\nSentence: Of Things to Come , The New York Times Book Review , October 26 , 1975 Theodore Sturgeon praised The Dispossessed as a beautifully written , beautifully composed book , saying it performs one of science fiction 's prime functions , which is to create another kind of social system to see how it would work .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Of","Things","to","Come",",","The","New","York","Times","Book","Review",",","October","26",",","1975","Theodore","Sturgeon","praised","The","Dispossessed","as","a","beautifully","written",",","beautifully","composed","book",",","saying","it","performs","one","of","science","fiction","'s","prime","functions",",","which","is","to","create","another","kind","of","social","system","to","see","how","it","would","work","."],"labels":["O","O","O","O","O","B-magazine","I-magazine","I-magazine","I-magazine","I-magazine","I-magazine","O","O","O","O","O","B-writer","I-writer","O","B-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","location","event","magazine","organization","country","poem","award","person","writer","book"]}
{"id":"193","dataset":"crossner_literature","split":"dev","instance":{"id":"193","prompt_labels":"In(O) view(O) of(O) the(O) success(O) of(O) her(O) novels(B-literary genre) ,(O) particularly(O) Jane(B-book) Eyre(I-book) ,(O) Bront(B-writer) was(O) persuaded(O) by(O) her(O) publisher(O) to(O) make(O) occasional(O) visits(O) to(O) London(B-location) ,(O) where(O) she(O) revealed(O) her(O) TRUE(O) identity(O) and(O) began(O) to(O) move(O) in(O) more(O) exalted(O) social(O) circles(O) ,(O) becoming(O) friends(O) with(O) Harriet(B-writer) Martineau(I-writer) and(O) Elizabeth(B-writer) Gaskell(I-writer) ,(O) and(O) acquainted(O) with(O) William(B-writer) Makepeace(I-writer) Thackeray(I-writer) and(O) G.H.(B-writer) Lewes(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, magazine, writer, poem, event, person, country, location, award, book, organization and O.\nSentence: In view of the success of her novels , particularly Jane Eyre , Bront was persuaded by her publisher to make occasional visits to London , where she revealed her TRUE identity and began to move in more exalted social circles , becoming friends with Harriet Martineau and Elizabeth Gaskell , and acquainted with William Makepeace Thackeray and G.H. Lewes .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","view","of","the","success","of","her","novels",",","particularly","Jane","Eyre",",","Bront","was","persuaded","by","her","publisher","to","make","occasional","visits","to","London",",","where","she","revealed","her","TRUE","identity","and","began","to","move","in","more","exalted","social","circles",",","becoming","friends","with","Harriet","Martineau","and","Elizabeth","Gaskell",",","and","acquainted","with","William","Makepeace","Thackeray","and","G.H.","Lewes","."],"labels":["O","O","O","O","O","O","O","B-literary genre","O","O","B-book","I-book","O","B-writer","O","O","O","O","O","O","O","O","O","O","B-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","O","O","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","magazine","writer","poem","event","person","country","location","award","book","organization"]}
{"id":"195","dataset":"crossner_literature","split":"dev","instance":{"id":"195","prompt_labels":"Big(B-person) Brother(I-person) is(O) a(O) fictional(O) character(O) and(O) symbol(O) in(O) George(B-writer) Orwell(I-writer) '(O) s(O) dystopian(B-literary genre) novel(I-literary genre) Nineteen(B-book) Eighty-Four(I-book) ,(O) published(O) in(O) 1949(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, book, writer, country, literary genre, location, organization, event, person, award, poem and O.\nSentence: Big Brother is a fictional character and symbol in George Orwell ' s dystopian novel Nineteen Eighty-Four , published in 1949 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Big","Brother","is","a","fictional","character","and","symbol","in","George","Orwell","'","s","dystopian","novel","Nineteen","Eighty-Four",",","published","in","1949","."],"labels":["B-person","I-person","O","O","O","O","O","O","O","B-writer","I-writer","O","O","B-literary genre","I-literary genre","B-book","I-book","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","book","writer","country","literary_genre","location","organization","event","person","award","poem"]}
{"id":"196","dataset":"crossner_literature","split":"dev","instance":{"id":"196","prompt_labels":"In(O) May(O) 1999(O) ,(O) after(O) the(O) Council(B-organization) of(I-organization) Fashion(I-organization) Designers(I-organization) of(I-organization) America(I-organization) recognized(O) Cher(B-writer) with(O) an(O) award(O) for(O) her(O) influence(O) on(O) fashion(O) ,(O) Robin(B-writer) Givhan(I-writer) of(O) the(O) Los(B-organization) Angeles(I-organization) Times(I-organization) called(O) her(O) a(O) fashion(O) visionary(O) for(O) striking(O) just(O) the(O) right(O) note(O) of(O) contemporary(O) wretched(O) excess(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, magazine, writer, country, event, literary genre, book, award, location, poem and O.\nSentence: In May 1999 , after the Council of Fashion Designers of America recognized Cher with an award for her influence on fashion , Robin Givhan of the Los Angeles Times called her a fashion visionary for striking just the right note of contemporary wretched excess .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","May","1999",",","after","the","Council","of","Fashion","Designers","of","America","recognized","Cher","with","an","award","for","her","influence","on","fashion",",","Robin","Givhan","of","the","Los","Angeles","Times","called","her","a","fashion","visionary","for","striking","just","the","right","note","of","contemporary","wretched","excess","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-writer","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","person","magazine","writer","country","event","literary_genre","book","award","location","poem"]}
{"id":"197","dataset":"crossner_literature","split":"dev","instance":{"id":"197","prompt_labels":"Adams(B-writer) 's(O) posthumously(O) published(O) work(O) ,(O) The(B-book) Salmon(I-book) of(I-book) Doubt(I-book) ,(O) features(O) several(O) articles(O) by(O) him(O) on(O) the(O) subject(O) of(O) technology(O) ,(O) including(O) reprints(O) of(O) articles(O) that(O) originally(O) ran(O) in(O) MacUser(B-magazine) magazine(O) ,(O) and(O) in(O) The(B-organization) Independent(I-organization) on(I-organization) Sunday(I-organization) newspaper(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, poem, award, literary genre, location, writer, magazine, organization, event, country, book and O.\nSentence: Adams 's posthumously published work , The Salmon of Doubt , features several articles by him on the subject of technology , including reprints of articles that originally ran in MacUser magazine , and in The Independent on Sunday newspaper .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Adams","'s","posthumously","published","work",",","The","Salmon","of","Doubt",",","features","several","articles","by","him","on","the","subject","of","technology",",","including","reprints","of","articles","that","originally","ran","in","MacUser","magazine",",","and","in","The","Independent","on","Sunday","newspaper","."],"labels":["B-writer","O","O","O","O","O","B-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-magazine","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O"],"target_index":null,"target_label":null},"label_list":["person","poem","award","literary_genre","location","writer","magazine","organization","event","country","book"]}
{"id":"198","dataset":"crossner_literature","split":"dev","instance":{"id":"198","prompt_labels":"When(O) Martin(B-writer) Gardner(I-writer) retired(O) from(O) writing(O) his(O) Mathematical(B-book) Games(I-book) column(O) for(O) Scientific(B-magazine) American(I-magazine) magazine(O) ,(O) Hofstadter(B-writer) succeeded(O) him(O) in(O) 1981-1983(O) with(O) a(O) column(O) titled(O) Metamagical(B-book) Themas(I-book) ((O) an(O) anagram(O) of(O) Mathematical(B-book) Games(I-book) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, event, literary genre, person, location, country, poem, award, writer, book, organization and O.\nSentence: When Martin Gardner retired from writing his Mathematical Games column for Scientific American magazine , Hofstadter succeeded him in 1981-1983 with a column titled Metamagical Themas ( an anagram of Mathematical Games ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["When","Martin","Gardner","retired","from","writing","his","Mathematical","Games","column","for","Scientific","American","magazine",",","Hofstadter","succeeded","him","in","1981-1983","with","a","column","titled","Metamagical","Themas","(","an","anagram","of","Mathematical","Games",")","."],"labels":["O","B-writer","I-writer","O","O","O","O","B-book","I-book","O","O","B-magazine","I-magazine","O","O","B-writer","O","O","O","O","O","O","O","O","B-book","I-book","O","O","O","O","B-book","I-book","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","event","literary_genre","person","location","country","poem","award","writer","book","organization"]}
{"id":"2","dataset":"crossner_music","split":"dev","instance":{"id":"2","prompt_labels":"During(O) the(O) 1990s(O) ,(O) many(O) releases(O) included(O) recordings(O) of(O) classical(O) compositions(O) :(O) Pictures(B-song) at(I-song) an(I-song) Exhibition(I-song) ((O) on(O) Turn(B-album) of(I-album) the(I-album) Tides(I-album) )(O) ,(O) Largo(B-song) ((O) from(O) Xerxes(O) )(O) ((O) on(O) Tyranny(B-album) of(I-album) Beauty(I-album) )(O) ,(O) Symphony(B-song) in(I-song) A(I-song) Minor(I-song) ((O) by(O) J.(B-musical artist) S.(I-musical artist) Bach(I-musical artist) )(O) ,(O) and(O) Concerto(B-song) in(I-song) A(I-song) Major(I-song) /(I-song) Adagio(I-song) ((O) by(O) Wolfgang(B-musical artist) Amadeus(I-musical artist) Mozart(I-musical artist) )(O) ((O) both(O) on(O) Ambient(B-album) Monkeys(I-album) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, person, album, song, musical instrument, event, country, band, award, musical artist, location, organization and O.\nSentence: During the 1990s , many releases included recordings of classical compositions : Pictures at an Exhibition ( on Turn of the Tides ) , Largo ( from Xerxes ) ( on Tyranny of Beauty ) , Symphony in A Minor ( by J. S. Bach ) , and Concerto in A Major / Adagio ( by Wolfgang Amadeus Mozart ) ( both on Ambient Monkeys ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","the","1990s",",","many","releases","included","recordings","of","classical","compositions",":","Pictures","at","an","Exhibition","(","on","Turn","of","the","Tides",")",",","Largo","(","from","Xerxes",")","(","on","Tyranny","of","Beauty",")",",","Symphony","in","A","Minor","(","by","J.","S.","Bach",")",",","and","Concerto","in","A","Major","/","Adagio","(","by","Wolfgang","Amadeus","Mozart",")","(","both","on","Ambient","Monkeys",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-song","I-song","I-song","I-song","O","O","B-album","I-album","I-album","I-album","O","O","B-song","O","O","O","O","O","O","B-album","I-album","I-album","O","O","B-song","I-song","I-song","I-song","O","O","B-musical artist","I-musical artist","I-musical artist","O","O","O","B-song","I-song","I-song","I-song","I-song","I-song","O","O","B-musical artist","I-musical artist","I-musical artist","O","O","O","O","B-album","I-album","O","O"],"target_index":null,"target_label":null},"label_list":["music_genre","person","album","song","musical_instrument","event","country","band","award","musical_artist","location","organization"]}
{"id":"5","dataset":"crossner_music","split":"dev","instance":{"id":"5","prompt_labels":"His(O) style(O) incorporates(O) elements(O) of(O) Rock(B-music genre) music(I-music genre) ,(O) blues(B-music genre) ,(O) Soul(B-music genre) music(I-music genre) ,(O) R(B-music genre) &(I-music genre) B(I-music genre) ,(O) funk(B-music genre) ,(O) jazz(B-music genre) ,(O) reggae(B-music genre) ,(O) hard(B-music genre) rock(I-music genre) ,(O) Psychedelic(B-music genre) rock(I-music genre) ,(O) Pop(B-music genre) music(I-music genre) ,(O) Folk(B-music genre) music(I-music genre) ,(O) and(O) ballads(B-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, location, award, band, musical artist, musical instrument, music genre, country, album, event, song and O.\nSentence: His style incorporates elements of Rock music , blues , Soul music , R & B , funk , jazz , reggae , hard rock , Psychedelic rock , Pop music , Folk music , and ballads .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","style","incorporates","elements","of","Rock","music",",","blues",",","Soul","music",",","R","&","B",",","funk",",","jazz",",","reggae",",","hard","rock",",","Psychedelic","rock",",","Pop","music",",","Folk","music",",","and","ballads","."],"labels":["O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","O","B-music genre","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O","B-music genre","O"],"target_index":null,"target_label":null},"label_list":["person","organization","location","award","band","musical_artist","musical_instrument","music_genre","country","album","event","song"]}
{"id":"8","dataset":"crossner_music","split":"dev","instance":{"id":"8","prompt_labels":"The(O) album(O) was(O) certified(O) seven-times(O) platinum(O) in(O) Australia(B-country) by(O) the(O) Australian(B-organization) Recording(I-organization) Industry(I-organization) Association(I-organization) ((O) ARIA(B-organization) )(O) ,(O) five-times(O) platinum(O) in(O) the(O) UK(B-country) by(O) the(O) British(B-organization) Phonographic(I-organization) Industry(I-organization) ((O) BPI(B-organization) )(O) ,(O) and(O) platinum(O) in(O) the(O) US(B-country) by(O) the(O) Recording(B-organization) Industry(I-organization) Association(I-organization) of(I-organization) America(I-organization) ((O) RIAA(B-organization) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, musical artist, country, person, band, event, album, song, location, musical instrument, organization, music genre and O.\nSentence: The album was certified seven-times platinum in Australia by the Australian Recording Industry Association ( ARIA ) , five-times platinum in the UK by the British Phonographic Industry ( BPI ) , and platinum in the US by the Recording Industry Association of America ( RIAA ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","album","was","certified","seven-times","platinum","in","Australia","by","the","Australian","Recording","Industry","Association","(","ARIA",")",",","five-times","platinum","in","the","UK","by","the","British","Phonographic","Industry","(","BPI",")",",","and","platinum","in","the","US","by","the","Recording","Industry","Association","of","America","(","RIAA",")","."],"labels":["O","O","O","O","O","O","O","B-country","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","B-country","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","B-country","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O"],"target_index":null,"target_label":null},"label_list":["award","musical_artist","country","person","band","event","album","song","location","musical_instrument","organization","music_genre"]}
{"id":"9","dataset":"crossner_music","split":"dev","instance":{"id":"9","prompt_labels":"Ernest(B-musical artist) Jennings(I-musical artist) Ford(I-musical artist) ((O) February(O) 13(O) ,(O) 1919(O) -(O) October(O) 17(O) ,(O) 1991(O) )(O) ,(O) known(O) professionally(O) as(O) Tennessee(B-musical artist) Ernie(I-musical artist) Ford(I-musical artist) ,(O) was(O) an(O) American(O) singer(O) and(O) television(O) host(O) who(O) enjoyed(O) success(O) in(O) the(O) Country(B-music genre) music(I-music genre) ,(O) Pop(B-music genre) music(I-music genre) ,(O) and(O) Gospel(B-music genre) music(I-music genre) musical(O) genres(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, musical artist, musical instrument, song, album, music genre, organization, band, award, country, person and O.\nSentence: Ernest Jennings Ford ( February 13 , 1919 - October 17 , 1991 ) , known professionally as Tennessee Ernie Ford , was an American singer and television host who enjoyed success in the Country music , Pop music , and Gospel music musical genres .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Ernest","Jennings","Ford","(","February","13",",","1919","-","October","17",",","1991",")",",","known","professionally","as","Tennessee","Ernie","Ford",",","was","an","American","singer","and","television","host","who","enjoyed","success","in","the","Country","music",",","Pop","music",",","and","Gospel","music","musical","genres","."],"labels":["B-musical artist","I-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O","B-music genre","I-music genre","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","location","musical_artist","musical_instrument","song","album","music_genre","organization","band","award","country","person"]}
{"id":"11","dataset":"crossner_music","split":"dev","instance":{"id":"11","prompt_labels":"Some(O) of(O) his(O) most(O) celebrated(O) designs(O) adorned(O) the(O) sleeves(O) of(O) albums(O) such(O) as(O) Midnight(B-album) Blue(I-album) ,(O) Out(B-album) to(I-album) Lunch(I-album) !(I-album) ,(O) Unity(B-album) ,(O) Somethin(B-album) '(I-album) Else(I-album) ,(O) Let(B-album) Freedom(I-album) Ring(I-album) ,(O) Hub-Tones(B-album) ,(O) No(B-album) Room(I-album) for(I-album) Squares(I-album) ,(O) Cool(B-album) Struttin(I-album) '(I-album) ,(O) and(O) The(B-album) Sidewinder(I-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, album, musical instrument, country, music genre, event, song, band, musical artist, person, organization and O.\nSentence: Some of his most celebrated designs adorned the sleeves of albums such as Midnight Blue , Out to Lunch ! , Unity , Somethin ' Else , Let Freedom Ring , Hub-Tones , No Room for Squares , Cool Struttin ' , and The Sidewinder .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","of","his","most","celebrated","designs","adorned","the","sleeves","of","albums","such","as","Midnight","Blue",",","Out","to","Lunch","!",",","Unity",",","Somethin","'","Else",",","Let","Freedom","Ring",",","Hub-Tones",",","No","Room","for","Squares",",","Cool","Struttin","'",",","and","The","Sidewinder","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","O","B-album","I-album","I-album","I-album","O","B-album","O","B-album","I-album","I-album","O","B-album","I-album","I-album","O","B-album","O","B-album","I-album","I-album","I-album","O","B-album","I-album","I-album","O","O","B-album","I-album","O"],"target_index":null,"target_label":null},"label_list":["award","location","album","musical_instrument","country","music_genre","event","song","band","musical_artist","person","organization"]}
{"id":"13","dataset":"crossner_music","split":"dev","instance":{"id":"13","prompt_labels":"Although(O) his(O) bandmate(O) Martin(B-musical artist) Gore(I-musical artist) continues(O) to(O) be(O) the(O) main(O) songwriter(O) for(O) Depeche(B-band) Mode(I-band) ,(O) Gahan(B-musical artist) has(O) contributed(O) a(O) number(O) of(O) songs(O) to(O) the(O) albums(O) Playing(B-album) the(I-album) Angel(I-album) ((O) 2005(O) )(O) ,(O) Sounds(B-album) of(I-album) the(I-album) Universe(I-album) ((O) 2009(O) )(O) ,(O) Delta(B-album) Machine(I-album) ((O) 2013(O) )(O) and(O) Spirit(B-album) ((O) 2017(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, music genre, song, event, musical instrument, organization, location, musical artist, award, band, person, album and O.\nSentence: Although his bandmate Martin Gore continues to be the main songwriter for Depeche Mode , Gahan has contributed a number of songs to the albums Playing the Angel ( 2005 ) , Sounds of the Universe ( 2009 ) , Delta Machine ( 2013 ) and Spirit ( 2017 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Although","his","bandmate","Martin","Gore","continues","to","be","the","main","songwriter","for","Depeche","Mode",",","Gahan","has","contributed","a","number","of","songs","to","the","albums","Playing","the","Angel","(","2005",")",",","Sounds","of","the","Universe","(","2009",")",",","Delta","Machine","(","2013",")","and","Spirit","(","2017",")","."],"labels":["O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","B-band","I-band","O","B-musical artist","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","B-album","I-album","O","O","O","O","B-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","music_genre","song","event","musical_instrument","organization","location","musical_artist","award","band","person","album"]}
{"id":"15","dataset":"crossner_music","split":"dev","instance":{"id":"15","prompt_labels":"The(O) best-selling(O) album(O) in(O) the(O) band(O) 's(O) catalog(O) ,(O) I(O) Against(B-album) I(O) is(O) an(O) album(O) that(O) mixes(O) American(O) hardcore(B-music genre) punk(I-music genre) with(O) funk(B-music genre) ,(O) Soul(B-music genre) music(I-music genre) ,(O) reggae(B-music genre) and(O) Heavy(B-music genre) metal(I-music genre) music(I-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, song, country, organization, musical artist, music genre, location, person, award, musical instrument, album, event and O.\nSentence: The best-selling album in the band 's catalog , I Against I is an album that mixes American hardcore punk with funk , Soul music , reggae and Heavy metal music .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","best-selling","album","in","the","band","'s","catalog",",","I","Against","I","is","an","album","that","mixes","American","hardcore","punk","with","funk",",","Soul","music",",","reggae","and","Heavy","metal","music","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-album","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["band","song","country","organization","musical_artist","music_genre","location","person","award","musical_instrument","album","event"]}
{"id":"19","dataset":"crossner_music","split":"dev","instance":{"id":"19","prompt_labels":"The(O) film(O) was(O) nominated(O) for(O) the(O) Academy(B-award) Awards(I-award) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) ,(O) as(O) well(O) as(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Production(I-award) Design(I-award) ((O) Carroll(B-person) Clark(I-person) and(O) Van(B-person) Nest(I-person) Polglase(I-person) )(O) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Song(I-award) ((O) Irving(B-musical artist) Berlin(I-musical artist) for(O) Cheek(B-song) to(I-song) Cheek(I-song) )(O) ,(O) and(O) Dance(B-award) Direction(I-award) ((O) Hermes(B-person) Pan(I-person) for(O) Piccolino(O) and(O) Top(O) Hat(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, musical artist, country, award, event, location, band, music genre, musical instrument, album, song and O.\nSentence: The film was nominated for the Academy Awards for Academy Award for Best Picture , as well as Academy Award for Best Production Design ( Carroll Clark and Van Nest Polglase ) , Academy Award for Best Original Song ( Irving Berlin for Cheek to Cheek ) , and Dance Direction ( Hermes Pan for Piccolino and Top Hat ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","film","was","nominated","for","the","Academy","Awards","for","Academy","Award","for","Best","Picture",",","as","well","as","Academy","Award","for","Best","Production","Design","(","Carroll","Clark","and","Van","Nest","Polglase",")",",","Academy","Award","for","Best","Original","Song","(","Irving","Berlin","for","Cheek","to","Cheek",")",",","and","Dance","Direction","(","Hermes","Pan","for","Piccolino","and","Top","Hat",")","."],"labels":["O","O","O","O","O","O","B-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-person","I-person","O","B-person","I-person","I-person","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-musical artist","I-musical artist","O","B-song","I-song","I-song","O","O","O","B-award","I-award","O","B-person","I-person","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","person","musical_artist","country","award","event","location","band","music_genre","musical_instrument","album","song"]}
{"id":"20","dataset":"crossner_music","split":"dev","instance":{"id":"20","prompt_labels":"It(O) received(O) 14(O) nominations(O) at(O) the(O) 89th(B-award) Academy(I-award) Awards(I-award) ,(O) tying(O) the(O) record(O) for(O) most(O) nominations(O) with(O) All(O) About(O) Eve(O) ((O) 1950(O) )(O) and(O) Titanic(O) ((O) 1997(O) )(O) ,(O) and(O) won(O) the(O) awards(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Cinematography(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Score(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Song(I-award) ,(O) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Production(I-award) Design(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, band, location, song, music genre, event, album, musical artist, organization, musical instrument, country and O.\nSentence: It received 14 nominations at the 89th Academy Awards , tying the record for most nominations with All About Eve ( 1950 ) and Titanic ( 1997 ) , and won the awards for Academy Award for Best Director , Academy Award for Best Actress , Academy Award for Best Cinematography , Academy Award for Best Original Score , Academy Award for Best Original Song , and Academy Award for Best Production Design .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","received","14","nominations","at","the","89th","Academy","Awards",",","tying","the","record","for","most","nominations","with","All","About","Eve","(","1950",")","and","Titanic","(","1997",")",",","and","won","the","awards","for","Academy","Award","for","Best","Director",",","Academy","Award","for","Best","Actress",",","Academy","Award","for","Best","Cinematography",",","Academy","Award","for","Best","Original","Score",",","Academy","Award","for","Best","Original","Song",",","and","Academy","Award","for","Best","Production","Design","."],"labels":["O","O","O","O","O","O","B-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["person","award","band","location","song","music_genre","event","album","musical_artist","organization","musical_instrument","country"]}
{"id":"21","dataset":"crossner_music","split":"dev","instance":{"id":"21","prompt_labels":"Western(B-music genre) music(I-music genre) artists(O) such(O) as(O) Michael(B-musical artist) Martin(I-musical artist) Murphey(I-musical artist) ,(O) and(O) artists(O) within(O) the(O) aforementioned(O) styles(O) and(O) genres(O) ,(O) have(O) seen(O) continued(O) success(O) throughout(O) their(O) respective(O) fields(O) ,(O) including(O) the(O) likes(O) of(O) The(B-band) Great(I-band) Divide(I-band) ,(O) Lorenzo(B-musical artist) Antonio(I-musical artist) ,(O) Sparx(B-band) ,(O) Pat(B-musical artist) Green(I-musical artist) ,(O) and(O) Jack(B-musical artist) Ingram(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, music genre, musical artist, award, person, event, country, organization, song, album, location, musical instrument and O.\nSentence: Western music artists such as Michael Martin Murphey , and artists within the aforementioned styles and genres , have seen continued success throughout their respective fields , including the likes of The Great Divide , Lorenzo Antonio , Sparx , Pat Green , and Jack Ingram .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Western","music","artists","such","as","Michael","Martin","Murphey",",","and","artists","within","the","aforementioned","styles","and","genres",",","have","seen","continued","success","throughout","their","respective","fields",",","including","the","likes","of","The","Great","Divide",",","Lorenzo","Antonio",",","Sparx",",","Pat","Green",",","and","Jack","Ingram","."],"labels":["B-music genre","I-music genre","O","O","O","B-musical artist","I-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","I-band","O","B-musical artist","I-musical artist","O","B-band","O","B-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["band","music_genre","musical_artist","award","person","event","country","organization","song","album","location","musical_instrument"]}
{"id":"22","dataset":"crossner_music","split":"dev","instance":{"id":"22","prompt_labels":"The(O) band(O) have(O) received(O) seven(O) Grammy(B-award) Award(I-award) s(O) ,(O) four(O) Brit(B-award) Awards(I-award) ,(O) an(O) Academy(B-award) Award(I-award) ((O) for(O) Best(B-award) Original(I-award) Song(I-award) Score(I-award) for(O) the(O) 1970(O) film(O) Let(O) It(O) Be(O) )(O) and(O) fifteen(O) Ivor(B-award) Novello(I-award) Awards(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, song, country, musical instrument, award, band, musical artist, organization, album, event, music genre and O.\nSentence: The band have received seven Grammy Award s , four Brit Awards , an Academy Award ( for Best Original Song Score for the 1970 film Let It Be ) and fifteen Ivor Novello Awards .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","band","have","received","seven","Grammy","Award","s",",","four","Brit","Awards",",","an","Academy","Award","(","for","Best","Original","Song","Score","for","the","1970","film","Let","It","Be",")","and","fifteen","Ivor","Novello","Awards","."],"labels":["O","O","O","O","O","B-award","I-award","O","O","O","B-award","I-award","O","O","B-award","I-award","O","O","B-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["location","person","song","country","musical_instrument","award","band","musical_artist","organization","album","event","music_genre"]}
{"id":"23","dataset":"crossner_music","split":"dev","instance":{"id":"23","prompt_labels":"She(O) rose(O) to(O) stardom(O) in(O) the(O) romantic(O) comedy(O) Roman(O) Holiday(O) ((O) 1953(O) )(O) ,(O) alongside(O) Gregory(B-person) Peck(I-person) ,(O) for(O) which(O) she(O) was(O) the(O) first(O) actress(O) to(O) win(O) an(O) Academy(B-award) Awards(I-award) ,(O) a(O) Golden(B-award) Globe(I-award) Awards(I-award) ,(O) and(O) a(O) British(B-award) Academy(I-award) Film(I-award) Awards(I-award) for(O) a(O) single(O) performance(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, band, music genre, country, location, musical instrument, award, person, song, musical artist, album, event and O.\nSentence: She rose to stardom in the romantic comedy Roman Holiday ( 1953 ) , alongside Gregory Peck , for which she was the first actress to win an Academy Awards , a Golden Globe Awards , and a British Academy Film Awards for a single performance .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","rose","to","stardom","in","the","romantic","comedy","Roman","Holiday","(","1953",")",",","alongside","Gregory","Peck",",","for","which","she","was","the","first","actress","to","win","an","Academy","Awards",",","a","Golden","Globe","Awards",",","and","a","British","Academy","Film","Awards","for","a","single","performance","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","band","music_genre","country","location","musical_instrument","award","person","song","musical_artist","album","event"]}
{"id":"26","dataset":"crossner_music","split":"dev","instance":{"id":"26","prompt_labels":"Special(O) guests(O) were(O) Pete(B-musical artist) Seeger(I-musical artist) ,(O) Bonnie(B-musical artist) Raitt(I-musical artist) ,(O) David(B-musical artist) Bromberg(I-musical artist) and(O) Jerry(B-musical artist) Jeff(I-musical artist) Walker(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, band, musical artist, music genre, organization, musical instrument, song, award, location, album, person, country and O.\nSentence: Special guests were Pete Seeger , Bonnie Raitt , David Bromberg and Jerry Jeff Walker .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Special","guests","were","Pete","Seeger",",","Bonnie","Raitt",",","David","Bromberg","and","Jerry","Jeff","Walker","."],"labels":["O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["event","band","musical_artist","music_genre","organization","musical_instrument","song","award","location","album","person","country"]}
{"id":"27","dataset":"crossner_music","split":"dev","instance":{"id":"27","prompt_labels":"Under(O) the(O) current(O) voting(O) system(O) ,(O) in(O) place(O) since(O) 2016(O) ,(O) the(O) highest-scoring(O) winner(O) is(O) Salvador(B-musical artist) Sobral(I-musical artist) of(O) Portugal(B-country) who(O) won(O) the(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2017(I-event) in(O) Kiev(B-location) ,(O) Ukraine(B-country) ,(O) with(O) 758(O) points(O) ;(O) under(O) the(O) previous(O) system(O) ,(O) the(O) highest-scoring(O) winner(O) was(O) Alexander(B-musical artist) Rybak(I-musical artist) of(O) Norway(B-country) with(O) 387(O) points(O) in(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2009(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, event, musical instrument, music genre, song, award, person, organization, country, musical artist, album, location and O.\nSentence: Under the current voting system , in place since 2016 , the highest-scoring winner is Salvador Sobral of Portugal who won the Eurovision Song Contest 2017 in Kiev , Ukraine , with 758 points ; under the previous system , the highest-scoring winner was Alexander Rybak of Norway with 387 points in Eurovision Song Contest 2009 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Under","the","current","voting","system",",","in","place","since","2016",",","the","highest-scoring","winner","is","Salvador","Sobral","of","Portugal","who","won","the","Eurovision","Song","Contest","2017","in","Kiev",",","Ukraine",",","with","758","points",";","under","the","previous","system",",","the","highest-scoring","winner","was","Alexander","Rybak","of","Norway","with","387","points","in","Eurovision","Song","Contest","2009","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-country","O","O","O","B-event","I-event","I-event","I-event","O","B-location","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-country","O","O","O","O","B-event","I-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["band","event","musical_instrument","music_genre","song","award","person","organization","country","musical_artist","album","location"]}
{"id":"30","dataset":"crossner_music","split":"dev","instance":{"id":"30","prompt_labels":"Some(O) modern(O) artists(O) that(O) primarily(O) or(O) entirely(O) produce(O) country(B-music genre) pop(I-music genre) music(I-music genre) include(O) Kacey(B-musical artist) Musgraves(I-musical artist) ,(O) Maren(B-musical artist) Morris(I-musical artist) ,(O) Kelsea(B-musical artist) Ballerini(I-musical artist) ,(O) Sam(B-musical artist) Hunt(I-musical artist) ,(O) Kane(B-musical artist) Brown(I-musical artist) ,(O) Chris(B-musical artist) Lane(I-musical artist) ,(O) and(O) Dan(B-band) +(I-band) Shay(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, song, album, musical instrument, country, organization, band, award, musical artist, person, location, event and O.\nSentence: Some modern artists that primarily or entirely produce country pop music include Kacey Musgraves , Maren Morris , Kelsea Ballerini , Sam Hunt , Kane Brown , Chris Lane , and Dan + Shay .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","modern","artists","that","primarily","or","entirely","produce","country","pop","music","include","Kacey","Musgraves",",","Maren","Morris",",","Kelsea","Ballerini",",","Sam","Hunt",",","Kane","Brown",",","Chris","Lane",",","and","Dan","+","Shay","."],"labels":["O","O","O","O","O","O","O","O","B-music genre","I-music genre","I-music genre","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","B-band","I-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["music_genre","song","album","musical_instrument","country","organization","band","award","musical_artist","person","location","event"]}
{"id":"31","dataset":"crossner_music","split":"dev","instance":{"id":"31","prompt_labels":"In(O) the(O) summer(O) of(O) 2004(O) ,(O) Diab(B-musical artist) ,(O) having(O) left(O) Alam(B-organization) El(I-organization) Phan(I-organization) ,(O) released(O) his(O) first(O) album(O) with(O) Rotana(B-organization) Records(I-organization) ,(O) Leily(B-album) Nahary(I-album) ,(O) which(O) he(O) followed(O) up(O) with(O) the(O) hugely(O) successful(O) Kammel(B-album) Kalamak(I-album) ((O) 2005(O) )(O) ,(O) and(O) El(B-album) Lilady(I-album) ((O) 2007(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, band, music genre, country, award, organization, song, album, musical instrument, person, event, musical artist and O.\nSentence: In the summer of 2004 , Diab , having left Alam El Phan , released his first album with Rotana Records , Leily Nahary , which he followed up with the hugely successful Kammel Kalamak ( 2005 ) , and El Lilady ( 2007 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","summer","of","2004",",","Diab",",","having","left","Alam","El","Phan",",","released","his","first","album","with","Rotana","Records",",","Leily","Nahary",",","which","he","followed","up","with","the","hugely","successful","Kammel","Kalamak","(","2005",")",",","and","El","Lilady","(","2007",")","."],"labels":["O","O","O","O","O","O","B-musical artist","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","B-organization","I-organization","O","B-album","I-album","O","O","O","O","O","O","O","O","O","B-album","I-album","O","O","O","O","O","B-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","band","music_genre","country","award","organization","song","album","musical_instrument","person","event","musical_artist"]}
{"id":"32","dataset":"crossner_music","split":"dev","instance":{"id":"32","prompt_labels":"Four(O) singles(O) -(O) Until(B-song) It(I-song) Sleeps(I-song) ,(O) Hero(B-song) of(I-song) the(I-song) Day(I-song) ,(O) Mama(B-song) Said(I-song) ,(O) and(O) King(B-song) Nothing(I-song) -(O) were(O) released(O) as(O) part(O) of(O) the(O) marketing(O) campaign(O) for(O) the(O) album(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, song, musical instrument, album, person, musical artist, location, country, event, award, band, music genre and O.\nSentence: Four singles - Until It Sleeps , Hero of the Day , Mama Said , and King Nothing - were released as part of the marketing campaign for the album .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Four","singles","-","Until","It","Sleeps",",","Hero","of","the","Day",",","Mama","Said",",","and","King","Nothing","-","were","released","as","part","of","the","marketing","campaign","for","the","album","."],"labels":["O","O","O","B-song","I-song","I-song","O","B-song","I-song","I-song","I-song","O","B-song","I-song","O","O","B-song","I-song","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","song","musical_instrument","album","person","musical_artist","location","country","event","award","band","music_genre"]}
{"id":"37","dataset":"crossner_music","split":"dev","instance":{"id":"37","prompt_labels":"Several(O) albums(O) that(O) continued(O) this(O) style(O) ,(O) which(O) had(O) come(O) to(O) be(O) known(O) as(O) technical(B-music genre) thrash(I-music genre) metal(I-music genre) ,(O) were(O) released(O) in(O) 1991(O) ,(O) such(O) as(O) Overkill(B-band) 's(O) Horrorscope(B-album) ,(O) Heathen(B-band) '(O) s(O) Victims(B-album) of(I-album) Deception(I-album) ,(O) Dark(B-band) Angel(I-band) '(O) s(O) Time(B-album) Does(I-album) Not(I-album) Heal(I-album) ,(O) Sepultura(B-band) 's(O) Arise(B-album) ,(O) and(O) Coroner(B-band) 's(O) Mental(B-album) Vortex(I-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical instrument, location, musical artist, band, music genre, person, organization, award, song, country, album and O.\nSentence: Several albums that continued this style , which had come to be known as technical thrash metal , were released in 1991 , such as Overkill 's Horrorscope , Heathen ' s Victims of Deception , Dark Angel ' s Time Does Not Heal , Sepultura 's Arise , and Coroner 's Mental Vortex .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Several","albums","that","continued","this","style",",","which","had","come","to","be","known","as","technical","thrash","metal",",","were","released","in","1991",",","such","as","Overkill","'s","Horrorscope",",","Heathen","'","s","Victims","of","Deception",",","Dark","Angel","'","s","Time","Does","Not","Heal",",","Sepultura","'s","Arise",",","and","Coroner","'s","Mental","Vortex","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","I-music genre","O","O","O","O","O","O","O","O","B-band","O","B-album","O","B-band","O","O","B-album","I-album","I-album","O","B-band","I-band","O","O","B-album","I-album","I-album","I-album","O","B-band","O","B-album","O","O","B-band","O","B-album","I-album","O"],"target_index":null,"target_label":null},"label_list":["event","musical_instrument","location","musical_artist","band","music_genre","person","organization","award","song","country","album"]}
{"id":"38","dataset":"crossner_music","split":"dev","instance":{"id":"38","prompt_labels":"Burton(B-person) 's(O) work(O) on(O) Sweeney(O) Todd(O) won(O) the(O) National(B-award) Board(I-award) of(I-award) Review(I-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) ,(O) and(O) won(O) an(O) Academy(B-award) Awards(I-award) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Production(I-award) Design(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, music genre, person, musical artist, organization, country, event, album, award, musical instrument, song, location and O.\nSentence: Burton 's work on Sweeney Todd won the National Board of Review Award for Best Director , and won an Academy Awards for Academy Award for Best Production Design .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Burton","'s","work","on","Sweeney","Todd","won","the","National","Board","of","Review","Award","for","Best","Director",",","and","won","an","Academy","Awards","for","Academy","Award","for","Best","Production","Design","."],"labels":["B-person","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","B-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["band","music_genre","person","musical_artist","organization","country","event","album","award","musical_instrument","song","location"]}
{"id":"39","dataset":"crossner_music","split":"dev","instance":{"id":"39","prompt_labels":"It(O) received(O) a(O) total(O) of(O) 13(O) Academy(B-award) Awards(I-award) nominations(O) ,(O) including(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) -(O) a(O) record(O) for(O) any(O) film(O) released(O) by(O) Walt(B-organization) Disney(I-organization) Studios(I-organization) -(O) and(O) won(O) five(O) :(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) for(O) Andrews(B-person) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Film(I-award) Editing(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Score(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Visual(I-award) Effects(I-award) ,(O) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Song(I-award) for(O) Chim(B-song) Chim(I-song) Cher-ee(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, musical artist, organization, location, song, award, event, country, band, person, album, musical instrument and O.\nSentence: It received a total of 13 Academy Awards nominations , including Academy Award for Best Picture - a record for any film released by Walt Disney Studios - and won five : Academy Award for Best Actress for Andrews , Academy Award for Best Film Editing , Academy Award for Best Original Score , Academy Award for Best Visual Effects , and Academy Award for Best Original Song for Chim Chim Cher-ee .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","received","a","total","of","13","Academy","Awards","nominations",",","including","Academy","Award","for","Best","Picture","-","a","record","for","any","film","released","by","Walt","Disney","Studios","-","and","won","five",":","Academy","Award","for","Best","Actress","for","Andrews",",","Academy","Award","for","Best","Film","Editing",",","Academy","Award","for","Best","Original","Score",",","Academy","Award","for","Best","Visual","Effects",",","and","Academy","Award","for","Best","Original","Song","for","Chim","Chim","Cher-ee","."],"labels":["O","O","O","O","O","O","B-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","B-person","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["music_genre","musical_artist","organization","location","song","award","event","country","band","person","album","musical_instrument"]}
{"id":"40","dataset":"crossner_music","split":"dev","instance":{"id":"40","prompt_labels":"Notable(O) rockabilly(B-music genre) revivalists(O) and(O) psychobilly(B-music genre) performers(O) from(O) the(O) 1990s(O) and(O) first(O) decade(O) of(O) the(O) 21st(O) century(O) include(O) Scott(B-musical artist) Owen(I-musical artist) ((O) from(O) the(O) Australian(O) band(O) The(B-band) Living(I-band) End(I-band) )(O) ,(O) Jimbo(B-musical artist) Wallace(I-musical artist) ((O) from(O) the(O) US(B-country) band(O) Reverend(B-band) Horton(I-band) Heat(I-band) )(O) ,(O) Kim(B-musical artist) Nekroman(I-musical artist) ((O) Nekromantix(B-band) )(O) ,(O) Patricia(B-musical artist) Day(I-musical artist) ((O) HorrorPops(B-band) )(O) ,(O) Geoff(B-musical artist) Kresge(I-musical artist) ((O) Tiger(B-band) Army(I-band) ,(O) ex-(O) AFI(B-band) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, location, musical artist, album, award, band, song, musical instrument, organization, music genre, person and O.\nSentence: Notable rockabilly revivalists and psychobilly performers from the 1990s and first decade of the 21st century include Scott Owen ( from the Australian band The Living End ) , Jimbo Wallace ( from the US band Reverend Horton Heat ) , Kim Nekroman ( Nekromantix ) , Patricia Day ( HorrorPops ) , Geoff Kresge ( Tiger Army , ex- AFI ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Notable","rockabilly","revivalists","and","psychobilly","performers","from","the","1990s","and","first","decade","of","the","21st","century","include","Scott","Owen","(","from","the","Australian","band","The","Living","End",")",",","Jimbo","Wallace","(","from","the","US","band","Reverend","Horton","Heat",")",",","Kim","Nekroman","(","Nekromantix",")",",","Patricia","Day","(","HorrorPops",")",",","Geoff","Kresge","(","Tiger","Army",",","ex-","AFI",")","."],"labels":["O","B-music genre","O","O","B-music genre","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","B-band","I-band","I-band","O","O","B-musical artist","I-musical artist","O","O","O","B-country","O","B-band","I-band","I-band","O","O","B-musical artist","I-musical artist","O","B-band","O","O","B-musical artist","I-musical artist","O","B-band","O","O","B-musical artist","I-musical artist","O","B-band","I-band","O","O","B-band","O","O"],"target_index":null,"target_label":null},"label_list":["event","country","location","musical_artist","album","award","band","song","musical_instrument","organization","music_genre","person"]}
{"id":"41","dataset":"crossner_music","split":"dev","instance":{"id":"41","prompt_labels":"In(O) 2009(O) ,(O) Paltrow(B-musical artist) received(O) a(O) Grammy(B-award) Award(I-award) nomination(O) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Spoken(I-award) Word(I-award) Album(I-award) for(I-award) Children(I-award) for(O) the(O) children(O) 's(O) audiobook(O) Brown(O) Bear(O) and(O) Friends(O) and(O) won(O) the(O) Primetime(B-award) Emmy(I-award) Award(I-award) for(I-award) Outstanding(I-award) Guest(I-award) Actress(I-award) in(I-award) a(I-award) Comedy(I-award) Series(I-award) for(O) her(O) guest(O) role(O) as(O) Holly(B-person) Holliday(I-person) on(O) the(O) Fox(O) musical(O) comedy-drama(O) television(O) series(O) Glee(O) in(O) 2011(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, event, organization, person, band, musical artist, country, location, award, album, music genre, musical instrument and O.\nSentence: In 2009 , Paltrow received a Grammy Award nomination for Grammy Award for Best Spoken Word Album for Children for the children 's audiobook Brown Bear and Friends and won the Primetime Emmy Award for Outstanding Guest Actress in a Comedy Series for her guest role as Holly Holliday on the Fox musical comedy-drama television series Glee in 2011 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2009",",","Paltrow","received","a","Grammy","Award","nomination","for","Grammy","Award","for","Best","Spoken","Word","Album","for","Children","for","the","children","'s","audiobook","Brown","Bear","and","Friends","and","won","the","Primetime","Emmy","Award","for","Outstanding","Guest","Actress","in","a","Comedy","Series","for","her","guest","role","as","Holly","Holliday","on","the","Fox","musical","comedy-drama","television","series","Glee","in","2011","."],"labels":["O","O","O","B-musical artist","O","O","B-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["song","event","organization","person","band","musical_artist","country","location","award","album","music_genre","musical_instrument"]}
{"id":"42","dataset":"crossner_music","split":"dev","instance":{"id":"42","prompt_labels":"In(O) 1956(O) ,(O) the(O) arrival(O) of(O) rockabilly(O) was(O) underlined(O) by(O) the(O) success(O) of(O) songs(O) like(O) Folsom(B-song) Prison(I-song) Blues(I-song) by(O) Johnny(B-musical artist) Cash(I-musical artist) ,(O) Blue(B-song) Suede(I-song) Shoes(I-song) by(O) Perkins(B-musical artist) and(O) the(O) No.(O) 1(O) hit(O) Heartbreak(B-song) Hotel(I-song) by(O) Presley(B-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, location, song, award, person, event, music genre, musical artist, musical instrument, country, band, organization and O.\nSentence: In 1956 , the arrival of rockabilly was underlined by the success of songs like Folsom Prison Blues by Johnny Cash , Blue Suede Shoes by Perkins and the No. 1 hit Heartbreak Hotel by Presley .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1956",",","the","arrival","of","rockabilly","was","underlined","by","the","success","of","songs","like","Folsom","Prison","Blues","by","Johnny","Cash",",","Blue","Suede","Shoes","by","Perkins","and","the","No.","1","hit","Heartbreak","Hotel","by","Presley","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-song","I-song","I-song","O","B-musical artist","I-musical artist","O","B-song","I-song","I-song","O","B-musical artist","O","O","O","O","O","B-song","I-song","O","B-musical artist","O"],"target_index":null,"target_label":null},"label_list":["album","location","song","award","person","event","music_genre","musical_artist","musical_instrument","country","band","organization"]}
{"id":"43","dataset":"crossner_music","split":"dev","instance":{"id":"43","prompt_labels":"Olympia(B-album) 71(I-album) ,(O) Olympia(B-album) 74(I-album) ,(O) and(O) Olympia(B-album) 77(I-album) are(O) live(O) albums(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, band, award, album, music genre, song, person, organization, musical instrument, location, musical artist, country and O.\nSentence: Olympia 71 , Olympia 74 , and Olympia 77 are live albums .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Olympia","71",",","Olympia","74",",","and","Olympia","77","are","live","albums","."],"labels":["B-album","I-album","O","B-album","I-album","O","O","B-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","band","award","album","music_genre","song","person","organization","musical_instrument","location","musical_artist","country"]}
{"id":"44","dataset":"crossner_music","split":"dev","instance":{"id":"44","prompt_labels":"There(O) are(O) many(O) kinds(O) of(O) bubens(B-musical instrument) ,(O) including(O) def(B-musical instrument) ,(O) daf(B-musical instrument) ,(O) or(O) qaval(B-musical instrument) ((O) Azerbaijan(B-country) )(O) ,(O) daf(B-musical instrument) or(O) khaval(B-musical instrument) ((O) Armenia(B-country) )(O) ,(O) daira(B-musical instrument) ((O) Georgia(B-country) )(O) ,(O) doira(B-musical instrument) ((O) Uzbekistan(B-country) and(O) Tajikistan(B-country) )(O) ,(O) daire(B-musical instrument) or(O) def(B-musical instrument) ((O) Iran(B-country) )(O) ,(O) bendeir(B-musical instrument) ((O) Arab(O) countries(O) )(O) ,(O) pandero(B-musical instrument) ((O) Spain(B-country) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, musical artist, country, award, album, organization, location, band, musical instrument, event, person, song and O.\nSentence: There are many kinds of bubens , including def , daf , or qaval ( Azerbaijan ) , daf or khaval ( Armenia ) , daira ( Georgia ) , doira ( Uzbekistan and Tajikistan ) , daire or def ( Iran ) , bendeir ( Arab countries ) , pandero ( Spain ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["There","are","many","kinds","of","bubens",",","including","def",",","daf",",","or","qaval","(","Azerbaijan",")",",","daf","or","khaval","(","Armenia",")",",","daira","(","Georgia",")",",","doira","(","Uzbekistan","and","Tajikistan",")",",","daire","or","def","(","Iran",")",",","bendeir","(","Arab","countries",")",",","pandero","(","Spain",")","."],"labels":["O","O","O","O","O","B-musical instrument","O","O","B-musical instrument","O","B-musical instrument","O","O","B-musical instrument","O","B-country","O","O","B-musical instrument","O","B-musical instrument","O","B-country","O","O","B-musical instrument","O","B-country","O","O","B-musical instrument","O","B-country","O","B-country","O","O","B-musical instrument","O","B-musical instrument","O","B-country","O","O","B-musical instrument","O","O","O","O","O","B-musical instrument","O","B-country","O","O"],"target_index":null,"target_label":null},"label_list":["music_genre","musical_artist","country","award","album","organization","location","band","musical_instrument","event","person","song"]}
{"id":"45","dataset":"crossner_music","split":"dev","instance":{"id":"45","prompt_labels":"The(O) venue(O) hosted(O) Badminton(B-event) at(I-event) the(I-event) 2012(I-event) Summer(I-event) Olympics(I-event) and(O) Gymnastics(B-event) at(I-event) the(I-event) 2012(I-event) Summer(I-event) Olympics(I-event) at(O) the(O) 2012(B-event) Summer(I-event) Olympics(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, music genre, person, award, event, band, song, musical instrument, organization, musical artist, album, country and O.\nSentence: The venue hosted Badminton at the 2012 Summer Olympics and Gymnastics at the 2012 Summer Olympics at the 2012 Summer Olympics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","venue","hosted","Badminton","at","the","2012","Summer","Olympics","and","Gymnastics","at","the","2012","Summer","Olympics","at","the","2012","Summer","Olympics","."],"labels":["O","O","O","B-event","I-event","I-event","I-event","I-event","I-event","O","B-event","I-event","I-event","I-event","I-event","I-event","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["location","music_genre","person","award","event","band","song","musical_instrument","organization","musical_artist","album","country"]}
{"id":"47","dataset":"crossner_music","split":"dev","instance":{"id":"47","prompt_labels":"One(O) of(O) the(O) main(O) differences(O) between(O) American(O) and(O) European(O) pop(B-music genre) is(O) that(O) Europop(B-music genre) is(O) generally(O) more(O) Dance(B-music genre) music(I-music genre) and(O) Trance(B-music genre) music(I-music genre) oriented(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, musical artist, location, album, musical instrument, award, song, band, country, organization, person, event and O.\nSentence: One of the main differences between American and European pop is that Europop is generally more Dance music and Trance music oriented .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["One","of","the","main","differences","between","American","and","European","pop","is","that","Europop","is","generally","more","Dance","music","and","Trance","music","oriented","."],"labels":["O","O","O","O","O","O","O","O","O","B-music genre","O","O","B-music genre","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["music_genre","musical_artist","location","album","musical_instrument","award","song","band","country","organization","person","event"]}
{"id":"50","dataset":"crossner_music","split":"dev","instance":{"id":"50","prompt_labels":"Castellano(B-musical artist) switched(O) to(O) rhythm(B-musical instrument) guitar(I-musical instrument) and(O) keyboards(B-musical instrument) ((O) Castellano(B-musical artist) also(O) filled(O) in(O) on(O) lead(O) guitar(B-musical instrument) and(O) vocals(O) for(O) an(O) ailing(O) Buck(B-musical artist) Dharma(I-musical artist) in(O) two(O) shows(O) in(O) 2005(O) )(O) ,(O) and(O) the(O) position(O) of(O) bassist(O) was(O) taken(O) up(O) by(O) Rudy(B-musical artist) Sarzo(I-musical artist) ((O) previously(O) a(O) member(O) of(O) Quiet(B-band) Riot(I-band) ,(O) Whitesnake(B-band) ,(O) Ozzy(B-musical artist) Osbourne(I-musical artist) and(O) Dio(B-musical artist) )(O) ,(O) with(O) the(O) band(O) employing(O) Danny(B-musical artist) Miranda(I-musical artist) and(O) Jon(B-musical artist) Rogers(I-musical artist) as(O) guest(O) bassists(O) to(O) fill(O) in(O) when(O) Sarzo(B-musical artist) was(O) unavailable(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, event, musical artist, location, award, music genre, band, musical instrument, organization, song, album and O.\nSentence: Castellano switched to rhythm guitar and keyboards ( Castellano also filled in on lead guitar and vocals for an ailing Buck Dharma in two shows in 2005 ) , and the position of bassist was taken up by Rudy Sarzo ( previously a member of Quiet Riot , Whitesnake , Ozzy Osbourne and Dio ) , with the band employing Danny Miranda and Jon Rogers as guest bassists to fill in when Sarzo was unavailable .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Castellano","switched","to","rhythm","guitar","and","keyboards","(","Castellano","also","filled","in","on","lead","guitar","and","vocals","for","an","ailing","Buck","Dharma","in","two","shows","in","2005",")",",","and","the","position","of","bassist","was","taken","up","by","Rudy","Sarzo","(","previously","a","member","of","Quiet","Riot",",","Whitesnake",",","Ozzy","Osbourne","and","Dio",")",",","with","the","band","employing","Danny","Miranda","and","Jon","Rogers","as","guest","bassists","to","fill","in","when","Sarzo","was","unavailable","."],"labels":["B-musical artist","O","O","B-musical instrument","I-musical instrument","O","B-musical instrument","O","B-musical artist","O","O","O","O","O","B-musical instrument","O","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","B-band","I-band","O","B-band","O","B-musical artist","I-musical artist","O","B-musical artist","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","B-musical artist","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","country","event","musical_artist","location","award","music_genre","band","musical_instrument","organization","song","album"]}
{"id":"51","dataset":"crossner_music","split":"dev","instance":{"id":"51","prompt_labels":"This(O) album(O) featured(O) vocal(O) contributions(O) by(O) Vicotnik(B-musical artist) of(O) Ved(B-band) Buens(I-band) Ende(I-band) and(O) Ddheimsgard(B-band) and(O) Aldrahn(B-musical artist) of(O) Ddheimsgard(B-band) and(O) Zyklon-B(B-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, country, organization, song, event, album, music genre, band, location, award, musical artist, person and O.\nSentence: This album featured vocal contributions by Vicotnik of Ved Buens Ende and Ddheimsgard and Aldrahn of Ddheimsgard and Zyklon-B .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","album","featured","vocal","contributions","by","Vicotnik","of","Ved","Buens","Ende","and","Ddheimsgard","and","Aldrahn","of","Ddheimsgard","and","Zyklon-B","."],"labels":["O","O","O","O","O","O","B-musical artist","O","B-band","I-band","I-band","O","B-band","O","B-musical artist","O","B-band","O","B-band","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","country","organization","song","event","album","music_genre","band","location","award","musical_artist","person"]}
{"id":"55","dataset":"crossner_music","split":"dev","instance":{"id":"55","prompt_labels":"The(O) film(O) won(O) Academy(B-award) Awards(I-award) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) ((O) James(B-person) Cagney(I-person) )(O) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Score(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Sound(I-award) Mixing(I-award) ((O) Nathan(B-musical artist) Levinson(I-musical artist) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, album, music genre, location, person, musical artist, band, event, musical instrument, award, country, organization and O.\nSentence: The film won Academy Awards for Academy Award for Best Actor ( James Cagney ) , Academy Award for Best Original Score and Academy Award for Best Sound Mixing ( Nathan Levinson ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","film","won","Academy","Awards","for","Academy","Award","for","Best","Actor","(","James","Cagney",")",",","Academy","Award","for","Best","Original","Score","and","Academy","Award","for","Best","Sound","Mixing","(","Nathan","Levinson",")","."],"labels":["O","O","O","B-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","B-person","I-person","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-musical artist","I-musical artist","O","O"],"target_index":null,"target_label":null},"label_list":["song","album","music_genre","location","person","musical_artist","band","event","musical_instrument","award","country","organization"]}
{"id":"59","dataset":"crossner_music","split":"dev","instance":{"id":"59","prompt_labels":"Since(O) TG(B-band) has(O) permanently(O) disbanded(O) following(O) the(O) death(O) of(O) Christopherson(B-musical artist) ,(O) the(O) label(O) 's(O) plan(O) is(O) to(O) re-release(O) the(O) original(O) TG(B-band) albums(O) ((O) The(B-album) Second(I-album) Annual(I-album) Report(I-album) ,(O) D.o.A(B-album) :(I-album) The(I-album) Third(I-album) and(I-album) Final(I-album) Report(I-album) ,(O) 20(B-album) Jazz(I-album) Funk(I-album) Greats(I-album) ,(O) Heathen(B-album) Earth(I-album) and(O) Greatest(B-album) Hits(I-album) )(O) on(O) the(O) label(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, award, location, musical artist, band, person, song, album, musical instrument, organization, music genre and O.\nSentence: Since TG has permanently disbanded following the death of Christopherson , the label 's plan is to re-release the original TG albums ( The Second Annual Report , D.o.A : The Third and Final Report , 20 Jazz Funk Greats , Heathen Earth and Greatest Hits ) on the label .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Since","TG","has","permanently","disbanded","following","the","death","of","Christopherson",",","the","label","'s","plan","is","to","re-release","the","original","TG","albums","(","The","Second","Annual","Report",",","D.o.A",":","The","Third","and","Final","Report",",","20","Jazz","Funk","Greats",",","Heathen","Earth","and","Greatest","Hits",")","on","the","label","."],"labels":["O","B-band","O","O","O","O","O","O","O","B-musical artist","O","O","O","O","O","O","O","O","O","O","B-band","O","O","B-album","I-album","I-album","I-album","O","B-album","I-album","I-album","I-album","I-album","I-album","I-album","O","B-album","I-album","I-album","I-album","O","B-album","I-album","O","B-album","I-album","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","event","award","location","musical_artist","band","person","song","album","musical_instrument","organization","music_genre"]}
{"id":"60","dataset":"crossner_music","split":"dev","instance":{"id":"60","prompt_labels":"Boogie(B-band) Down(I-band) and(O) KRS(B-musical artist) retorted(O) angrily(O) with(O) songs(O) such(O) as(O) The(B-song) Bridge(I-song) is(I-song) Over(I-song) and(O) South(B-song) Bronx(I-song) ,(O) which(O) started(O) one(O) of(O) the(O) first(O) notable(O) hip(B-music genre) hop(I-music genre) wars(O) as(O) MC(B-musical artist) Shan(I-musical artist) ,(O) Marley(B-musical artist) Marl(I-musical artist) ,(O) Roxanne(B-musical artist) Shant(I-musical artist) and(O) Blaq(B-musical artist) Poet(I-musical artist) all(O) released(O) songs(O) featuring(O) verses(O) personally(O) attacking(O) KRS(B-musical artist) and(O) Scott(B-musical artist) La(I-musical artist) Rock(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, album, song, award, musical artist, country, music genre, event, organization, musical instrument, band and O.\nSentence: Boogie Down and KRS retorted angrily with songs such as The Bridge is Over and South Bronx , which started one of the first notable hip hop wars as MC Shan , Marley Marl , Roxanne Shant and Blaq Poet all released songs featuring verses personally attacking KRS and Scott La Rock .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Boogie","Down","and","KRS","retorted","angrily","with","songs","such","as","The","Bridge","is","Over","and","South","Bronx",",","which","started","one","of","the","first","notable","hip","hop","wars","as","MC","Shan",",","Marley","Marl",",","Roxanne","Shant","and","Blaq","Poet","all","released","songs","featuring","verses","personally","attacking","KRS","and","Scott","La","Rock","."],"labels":["B-band","I-band","O","B-musical artist","O","O","O","O","O","O","B-song","I-song","I-song","I-song","O","B-song","I-song","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","B-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["location","person","album","song","award","musical_artist","country","music_genre","event","organization","musical_instrument","band"]}
{"id":"61","dataset":"crossner_music","split":"dev","instance":{"id":"61","prompt_labels":"Other(O) top-10(O) entries(O) from(O) 2015(O) like(O) Mark(B-musical artist) Ronson(I-musical artist) '(O) s(O) disco(B-song) groove-infused(I-song) Uptown(B-song) Funk(I-song) ,(O) Maroon(B-band) 5(I-band) '(O) s(O) Sugar(B-song) ,(O) the(B-band) Weeknd(I-band) '(O) s(O) Can(B-song) 't(I-song) Feel(I-song) My(I-song) Face(I-song) and(O) Jason(B-musical artist) Derulo(I-musical artist) '(O) s(O) Want(B-song) to(I-song) Want(I-song) Me(I-song) also(O) ascended(O) the(O) charts(O) and(O) have(O) a(O) strong(O) disco(B-music genre) influence(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, music genre, country, organization, event, location, person, album, award, musical instrument, band, musical artist and O.\nSentence: Other top-10 entries from 2015 like Mark Ronson ' s disco groove-infused Uptown Funk , Maroon 5 ' s Sugar , the Weeknd ' s Can 't Feel My Face and Jason Derulo ' s Want to Want Me also ascended the charts and have a strong disco influence .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","top-10","entries","from","2015","like","Mark","Ronson","'","s","disco","groove-infused","Uptown","Funk",",","Maroon","5","'","s","Sugar",",","the","Weeknd","'","s","Can","'t","Feel","My","Face","and","Jason","Derulo","'","s","Want","to","Want","Me","also","ascended","the","charts","and","have","a","strong","disco","influence","."],"labels":["O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","B-song","I-song","B-song","I-song","O","B-band","I-band","O","O","B-song","O","B-band","I-band","O","O","B-song","I-song","I-song","I-song","I-song","O","B-musical artist","I-musical artist","O","O","B-song","I-song","I-song","I-song","O","O","O","O","O","O","O","O","B-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["song","music_genre","country","organization","event","location","person","album","award","musical_instrument","band","musical_artist"]}
{"id":"63","dataset":"crossner_music","split":"dev","instance":{"id":"63","prompt_labels":"In(O) December(O) 2011(O) the(O) band(O) announced(O) they(O) would(O) be(O) performing(O) Tellin(B-album) '(I-album) Stories(I-album) in(O) its(O) entirety(O) at(O) London(B-location) 's(O) HMV(B-location) Hammersmith(I-location) Apollo(I-location) ,(O) O2(B-location) Apollo(I-location) Manchester(I-location) and(O) Glasgow(B-location) 's(O) Barrowland(B-location) Ballroom(I-location) in(O) June(O) 2012(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, music genre, musical instrument, award, person, location, musical artist, event, organization, album, band, song and O.\nSentence: In December 2011 the band announced they would be performing Tellin ' Stories in its entirety at London 's HMV Hammersmith Apollo , O2 Apollo Manchester and Glasgow 's Barrowland Ballroom in June 2012 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","December","2011","the","band","announced","they","would","be","performing","Tellin","'","Stories","in","its","entirety","at","London","'s","HMV","Hammersmith","Apollo",",","O2","Apollo","Manchester","and","Glasgow","'s","Barrowland","Ballroom","in","June","2012","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O","B-location","O","B-location","I-location","I-location","O","B-location","I-location","I-location","O","B-location","O","B-location","I-location","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","music_genre","musical_instrument","award","person","location","musical_artist","event","organization","album","band","song"]}
{"id":"65","dataset":"crossner_music","split":"dev","instance":{"id":"65","prompt_labels":"The(O) Eurasian(B-organization) Economic(I-organization) Union(I-organization) ,(O) the(O) Gulf(B-organization) Cooperation(I-organization) Council(I-organization) ,(O) CARICOM(B-organization) and(O) the(O) European(B-organization) Union(I-organization) are(O) current(O) examples(O) of(O) single(O) markets(O) ,(O) although(O) the(O) Gulf(B-organization) Cooperation(I-organization) Council(I-organization) '(O) s(O) single(O) market(O) has(O) been(O) described(O) as(O) malfunctioning(O) in(O) 2014(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, award, musical instrument, song, music genre, organization, person, musical artist, location, event, band, country and O.\nSentence: The Eurasian Economic Union , the Gulf Cooperation Council , CARICOM and the European Union are current examples of single markets , although the Gulf Cooperation Council ' s single market has been described as malfunctioning in 2014 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Eurasian","Economic","Union",",","the","Gulf","Cooperation","Council",",","CARICOM","and","the","European","Union","are","current","examples","of","single","markets",",","although","the","Gulf","Cooperation","Council","'","s","single","market","has","been","described","as","malfunctioning","in","2014","."],"labels":["O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["album","award","musical_instrument","song","music_genre","organization","person","musical_artist","location","event","band","country"]}
{"id":"67","dataset":"crossner_music","split":"dev","instance":{"id":"67","prompt_labels":"In(O) 1990(O) ,(O) he(O) appeared(O) on(O) Kool(B-song) Thing(I-song) ,(O) a(O) song(O) by(O) the(O) alternative(B-music genre) rock(I-music genre) band(O) Sonic(B-band) Youth(I-band) ,(O) and(O) along(O) with(O) Flavor(B-musical artist) Flav(I-musical artist) ,(O) he(O) sang(O) on(O) George(B-musical artist) Clinton(I-musical artist) '(O) s(O) song(O) Tweakin(B-song) '(O) ,(O) which(O) appears(O) on(O) his(O) 1989(O) album(O) The(B-album) Cinderella(I-album) Theory(I-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, music genre, award, location, musical instrument, band, album, event, song, country, musical artist, organization and O.\nSentence: In 1990 , he appeared on Kool Thing , a song by the alternative rock band Sonic Youth , and along with Flavor Flav , he sang on George Clinton ' s song Tweakin ' , which appears on his 1989 album The Cinderella Theory .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1990",",","he","appeared","on","Kool","Thing",",","a","song","by","the","alternative","rock","band","Sonic","Youth",",","and","along","with","Flavor","Flav",",","he","sang","on","George","Clinton","'","s","song","Tweakin","'",",","which","appears","on","his","1989","album","The","Cinderella","Theory","."],"labels":["O","O","O","O","O","O","B-song","I-song","O","O","O","O","O","B-music genre","I-music genre","O","B-band","I-band","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","B-musical artist","I-musical artist","O","O","O","B-song","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O"],"target_index":null,"target_label":null},"label_list":["person","music_genre","award","location","musical_instrument","band","album","event","song","country","musical_artist","organization"]}
{"id":"68","dataset":"crossner_music","split":"dev","instance":{"id":"68","prompt_labels":"They(O) also(O) visited(O) South(B-location) America(I-location) for(O) the(O) second(O) time(O) ((O) the(O) first(O) time(O) being(O) in(O) 1999(O) )(O) ,(O) arriving(O) at(O) Chile(B-country) ,(O) Argentina(B-country) ,(O) and(O) Brazil(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, song, musical artist, event, person, music genre, location, musical instrument, country, album, award, band and O.\nSentence: They also visited South America for the second time ( the first time being in 1999 ) , arriving at Chile , Argentina , and Brazil .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["They","also","visited","South","America","for","the","second","time","(","the","first","time","being","in","1999",")",",","arriving","at","Chile",",","Argentina",",","and","Brazil","."],"labels":["O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","B-country","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["organization","song","musical_artist","event","person","music_genre","location","musical_instrument","country","album","award","band"]}
{"id":"72","dataset":"crossner_music","split":"dev","instance":{"id":"72","prompt_labels":"They(O) were(O) accompanied(O) by(O) a(O) varying(O) number(O) of(O) session(O) musicians(O) and(O) some(O) relatively(O) consistent(O) band(O) members(O) such(O) as(O) guitarist(O) Ian(B-musical artist) Bairnson(I-musical artist) ,(O) arranger(O) Andrew(B-musical artist) Powell(I-musical artist) ,(O) bassist(O) and(O) vocalist(O) David(B-musical artist) Paton(I-musical artist) ,(O) drummer(O) Stuart(B-musical artist) Elliott(I-musical artist) ,(O) and(O) vocalists(O) Lenny(B-musical artist) Zakatek(I-musical artist) and(O) Chris(B-musical artist) Rainbow(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, location, event, musical artist, musical instrument, music genre, award, person, song, band, organization, country and O.\nSentence: They were accompanied by a varying number of session musicians and some relatively consistent band members such as guitarist Ian Bairnson , arranger Andrew Powell , bassist and vocalist David Paton , drummer Stuart Elliott , and vocalists Lenny Zakatek and Chris Rainbow .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["They","were","accompanied","by","a","varying","number","of","session","musicians","and","some","relatively","consistent","band","members","such","as","guitarist","Ian","Bairnson",",","arranger","Andrew","Powell",",","bassist","and","vocalist","David","Paton",",","drummer","Stuart","Elliott",",","and","vocalists","Lenny","Zakatek","and","Chris","Rainbow","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","O","O","O","O","B-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["album","location","event","musical_artist","musical_instrument","music_genre","award","person","song","band","organization","country"]}
{"id":"73","dataset":"crossner_music","split":"dev","instance":{"id":"73","prompt_labels":"His(O) 13(O) Grammy(B-award) Award(I-award) nominations(O) have(O) resulted(O) in(O) 2(O) awards(O) won(O) ,(O) along(O) with(O) Billboard(B-award) Music(I-award) Award(I-award) s(O) ,(O) Country(B-award) Music(I-award) Association(I-award) Awards(I-award) ,(O) and(O) many(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, musical artist, album, award, event, band, music genre, organization, location, country, song, person and O.\nSentence: His 13 Grammy Award nominations have resulted in 2 awards won , along with Billboard Music Award s , Country Music Association Awards , and many others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","13","Grammy","Award","nominations","have","resulted","in","2","awards","won",",","along","with","Billboard","Music","Award","s",",","Country","Music","Association","Awards",",","and","many","others","."],"labels":["O","O","B-award","I-award","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","musical_artist","album","award","event","band","music_genre","organization","location","country","song","person"]}
{"id":"77","dataset":"crossner_music","split":"dev","instance":{"id":"77","prompt_labels":"Schmidt(B-person) was(O) born(O) in(O) Pozsony(B-location) ((O) known(O) in(O) German(O) as(O) Pressburg(B-location) )(O) ,(O) in(O) the(O) Hungary(B-country) part(O) of(O) the(O) Austria-Hungary(B-location) ((O) the(O) city(O) is(O) now(O) Bratislava(B-location) ,(O) capital(O) of(O) Slovakia(B-country) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, musical artist, person, song, award, organization, event, musical instrument, band, location, country, album and O.\nSentence: Schmidt was born in Pozsony ( known in German as Pressburg ) , in the Hungary part of the Austria-Hungary ( the city is now Bratislava , capital of Slovakia ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Schmidt","was","born","in","Pozsony","(","known","in","German","as","Pressburg",")",",","in","the","Hungary","part","of","the","Austria-Hungary","(","the","city","is","now","Bratislava",",","capital","of","Slovakia",")","."],"labels":["B-person","O","O","O","B-location","O","O","O","O","O","B-location","O","O","O","O","B-country","O","O","O","B-location","O","O","O","O","O","B-location","O","O","O","B-country","O","O"],"target_index":null,"target_label":null},"label_list":["music_genre","musical_artist","person","song","award","organization","event","musical_instrument","band","location","country","album"]}
{"id":"78","dataset":"crossner_music","split":"dev","instance":{"id":"78","prompt_labels":"In(O) 2005(O) Jamieson(B-musical artist) won(O) Best(O) Male(O) Performer(O) in(O) the(O) second(B-award) annual(I-award) Jack(I-award) Awards(I-award) ,(O) Jamieson(B-musical artist) showcased(O) the(O) sounds(O) of(O) Grinspoon(B-band) to(O) millions(O) of(O) viewers(O) in(O) March(O) 2006(O) ,(O) playing(O) live(O) at(O) Melbourne(B-location) Cricket(I-location) Ground(I-location) as(O) part(O) of(O) the(O) closing(O) ceremony(O) of(O) the(O) 2006(B-event) Commonwealth(I-event) Games(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, person, musical instrument, organization, musical artist, award, country, song, event, band, music genre, location and O.\nSentence: In 2005 Jamieson won Best Male Performer in the second annual Jack Awards , Jamieson showcased the sounds of Grinspoon to millions of viewers in March 2006 , playing live at Melbourne Cricket Ground as part of the closing ceremony of the 2006 Commonwealth Games .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2005","Jamieson","won","Best","Male","Performer","in","the","second","annual","Jack","Awards",",","Jamieson","showcased","the","sounds","of","Grinspoon","to","millions","of","viewers","in","March","2006",",","playing","live","at","Melbourne","Cricket","Ground","as","part","of","the","closing","ceremony","of","the","2006","Commonwealth","Games","."],"labels":["O","O","B-musical artist","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","B-musical artist","O","O","O","O","B-band","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["album","person","musical_instrument","organization","musical_artist","award","country","song","event","band","music_genre","location"]}
{"id":"79","dataset":"crossner_music","split":"dev","instance":{"id":"79","prompt_labels":"Funds(O) raised(O) by(O) the(O) project(O) will(O) go(O) to(O) Amazon(B-organization) Watch(I-organization) and(O) Extinction(B-organization) Rebellion(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, band, event, musical instrument, country, album, person, musical artist, music genre, location, organization, award and O.\nSentence: Funds raised by the project will go to Amazon Watch and Extinction Rebellion .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Funds","raised","by","the","project","will","go","to","Amazon","Watch","and","Extinction","Rebellion","."],"labels":["O","O","O","O","O","O","O","O","B-organization","I-organization","O","B-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["song","band","event","musical_instrument","country","album","person","musical_artist","music_genre","location","organization","award"]}
{"id":"80","dataset":"crossner_music","split":"dev","instance":{"id":"80","prompt_labels":"This(O) is(O) the(O) method(O) used(O) by(O) Bands(B-organization) of(I-organization) America(I-organization) ,(O) the(O) Indiana(B-organization) State(I-organization) School(I-organization) Music(I-organization) Association(I-organization) ,(O) Kentucky(B-organization) Music(I-organization) Educators(I-organization) Association(I-organization) and(O) the(O) University(B-organization) Interscholastic(I-organization) League(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, musical artist, event, album, location, musical instrument, music genre, band, award, country, organization, song and O.\nSentence: This is the method used by Bands of America , the Indiana State School Music Association , Kentucky Music Educators Association and the University Interscholastic League .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","is","the","method","used","by","Bands","of","America",",","the","Indiana","State","School","Music","Association",",","Kentucky","Music","Educators","Association","and","the","University","Interscholastic","League","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["person","musical_artist","event","album","location","musical_instrument","music_genre","band","award","country","organization","song"]}
{"id":"81","dataset":"crossner_music","split":"dev","instance":{"id":"81","prompt_labels":"Featuring(O) LaLonde(B-musical artist) and(O) Alexander(B-musical artist) ,(O) Primus(B-band) recorded(O) the(O) live(O) album(O) Suck(B-album) on(I-album) This(I-album) in(O) 1989(O) ,(O) followed(O) by(O) four(O) studio(O) albums(O) :(O) Frizzle(B-album) Fry(I-album) ,(O) Sailing(B-album) the(I-album) Seas(I-album) of(I-album) Cheese(I-album) ,(O) Pork(B-album) Soda(I-album) ,(O) and(O) Tales(B-album) from(I-album) the(I-album) Punchbowl(I-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, award, music genre, album, musical artist, organization, location, event, band, country, person, musical instrument and O.\nSentence: Featuring LaLonde and Alexander , Primus recorded the live album Suck on This in 1989 , followed by four studio albums : Frizzle Fry , Sailing the Seas of Cheese , Pork Soda , and Tales from the Punchbowl .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Featuring","LaLonde","and","Alexander",",","Primus","recorded","the","live","album","Suck","on","This","in","1989",",","followed","by","four","studio","albums",":","Frizzle","Fry",",","Sailing","the","Seas","of","Cheese",",","Pork","Soda",",","and","Tales","from","the","Punchbowl","."],"labels":["O","B-musical artist","O","B-musical artist","O","B-band","O","O","O","O","B-album","I-album","I-album","O","O","O","O","O","O","O","O","O","B-album","I-album","O","B-album","I-album","I-album","I-album","I-album","O","B-album","I-album","O","O","B-album","I-album","I-album","I-album","O"],"target_index":null,"target_label":null},"label_list":["song","award","music_genre","album","musical_artist","organization","location","event","band","country","person","musical_instrument"]}
{"id":"82","dataset":"crossner_music","split":"dev","instance":{"id":"82","prompt_labels":"In(O) 1977(O) Summer(O) ,(O) Moroder(B-musical artist) and(O) Bellotte(B-musical artist) further(O) released(O) I(B-song) Feel(I-song) Love(I-song) ,(O) as(O) the(O) B-side(O) of(O) Can(B-song) 't(I-song) We(I-song) Just(I-song) Sit(I-song) Down(I-song) ((O) And(B-song) Talk(I-song) It(I-song) Over(I-song) )(O) ,(O) which(O) revolutionized(O) dance(O) music(O) with(O) its(O) mostly(O) Electronic(B-music genre) music(I-music genre) production(O) and(O) was(O) a(O) massive(O) worldwide(O) success(O) ,(O) spawning(O) the(O) Hi-NRG(B-music genre) subgenre(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, album, award, musical instrument, person, country, music genre, organization, musical artist, song, event, band and O.\nSentence: In 1977 Summer , Moroder and Bellotte further released I Feel Love , as the B-side of Can 't We Just Sit Down ( And Talk It Over ) , which revolutionized dance music with its mostly Electronic music production and was a massive worldwide success , spawning the Hi-NRG subgenre .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1977","Summer",",","Moroder","and","Bellotte","further","released","I","Feel","Love",",","as","the","B-side","of","Can","'t","We","Just","Sit","Down","(","And","Talk","It","Over",")",",","which","revolutionized","dance","music","with","its","mostly","Electronic","music","production","and","was","a","massive","worldwide","success",",","spawning","the","Hi-NRG","subgenre","."],"labels":["O","O","O","O","B-musical artist","O","B-musical artist","O","O","B-song","I-song","I-song","O","O","O","O","O","B-song","I-song","I-song","I-song","I-song","I-song","O","B-song","I-song","I-song","I-song","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","B-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["location","album","award","musical_instrument","person","country","music_genre","organization","musical_artist","song","event","band"]}
{"id":"84","dataset":"crossner_music","split":"dev","instance":{"id":"84","prompt_labels":"Other(O) acts(O) who(O) became(O) prominent(O) in(O) the(O) alt-country(B-music genre) genre(I-music genre) during(O) the(O) 1990s(O) and(O) 2000s(O) included(O) The(B-band) Bottle(I-band) Rockets(I-band) ,(O) The(B-band) Handsome(I-band) Family(I-band) ,(O) Blue(B-band) Mountain(I-band) ,(O) Robbie(B-band) Fulks(I-band) ,(O) Blood(B-band) Oranges(I-band) ,(O) Bright(B-band) Eyes(I-band) ,(O) Drive-By(B-band) Truckers(I-band) ,(O) Old(B-band) 97(I-band) 's(I-band) ,(O) Old(B-band) Crow(I-band) Medicine(I-band) Show(I-band) ,(O) Nickel(B-band) Creek(I-band) ,(O) Neko(B-musical artist) Case(I-musical artist) ,(O) and(O) Whiskeytown(B-band) ,(O) whose(O) lead(O) singer(O) Ryan(B-musical artist) Adams(I-musical artist) later(O) had(O) a(O) successful(O) solo-career(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, album, band, person, musical instrument, song, event, country, musical artist, music genre, organization and O.\nSentence: Other acts who became prominent in the alt-country genre during the 1990s and 2000s included The Bottle Rockets , The Handsome Family , Blue Mountain , Robbie Fulks , Blood Oranges , Bright Eyes , Drive-By Truckers , Old 97 's , Old Crow Medicine Show , Nickel Creek , Neko Case , and Whiskeytown , whose lead singer Ryan Adams later had a successful solo-career .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","acts","who","became","prominent","in","the","alt-country","genre","during","the","1990s","and","2000s","included","The","Bottle","Rockets",",","The","Handsome","Family",",","Blue","Mountain",",","Robbie","Fulks",",","Blood","Oranges",",","Bright","Eyes",",","Drive-By","Truckers",",","Old","97","'s",",","Old","Crow","Medicine","Show",",","Nickel","Creek",",","Neko","Case",",","and","Whiskeytown",",","whose","lead","singer","Ryan","Adams","later","had","a","successful","solo-career","."],"labels":["O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","B-band","I-band","I-band","O","B-band","I-band","I-band","O","B-band","I-band","O","B-band","I-band","O","B-band","I-band","O","B-band","I-band","O","B-band","I-band","O","B-band","I-band","I-band","O","B-band","I-band","I-band","I-band","O","B-band","I-band","O","B-musical artist","I-musical artist","O","O","B-band","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","award","album","band","person","musical_instrument","song","event","country","musical_artist","music_genre","organization"]}
{"id":"85","dataset":"crossner_music","split":"dev","instance":{"id":"85","prompt_labels":"The(O) West(O) Side(O) sound(O) had(O) strong(O) rhythmic(O) support(O) from(O) a(O) rhythm(B-musical instrument) guitar(I-musical instrument) ,(O) bass(B-musical instrument) guitar(I-musical instrument) and(O) drums(B-musical instrument) and(O) as(O) perfected(O) by(O) Guy(B-band) ,(O) Freddie(B-musical artist) King(I-musical artist) ,(O) Magic(B-musical artist) Slim(I-musical artist) and(O) Luther(B-musical artist) Allison(I-musical artist) was(O) dominated(O) by(O) amplified(O) electric(B-musical instrument) lead(I-musical instrument) guitar(I-musical instrument) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, band, musical artist, person, musical instrument, award, event, location, album, song, country, organization and O.\nSentence: The West Side sound had strong rhythmic support from a rhythm guitar , bass guitar and drums and as perfected by Guy , Freddie King , Magic Slim and Luther Allison was dominated by amplified electric lead guitar .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","West","Side","sound","had","strong","rhythmic","support","from","a","rhythm","guitar",",","bass","guitar","and","drums","and","as","perfected","by","Guy",",","Freddie","King",",","Magic","Slim","and","Luther","Allison","was","dominated","by","amplified","electric","lead","guitar","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-musical instrument","I-musical instrument","O","B-musical instrument","I-musical instrument","O","B-musical instrument","O","O","O","O","B-band","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O","B-musical instrument","I-musical instrument","I-musical instrument","O"],"target_index":null,"target_label":null},"label_list":["music_genre","band","musical_artist","person","musical_instrument","award","event","location","album","song","country","organization"]}
{"id":"90","dataset":"crossner_music","split":"dev","instance":{"id":"90","prompt_labels":"Electropop(B-music genre) pioneers(O) Haruomi(B-musical artist) Hosono(I-musical artist) and(O) Ryuichi(B-musical artist) Sakamoto(I-musical artist) of(O) the(O) Yellow(B-band) Magic(I-band) Orchestra(I-band) produced(O) a(O) 1978(O) Electronic(B-music genre) music(I-music genre) album(O) ,(O) Cochin(B-album) Moon(I-album) ,(O) based(O) on(O) an(O) experimental(O) fusion(O) of(O) electronic(B-music genre) music(I-music genre) and(O) Bollywood-inspired(B-music genre) Indian(I-music genre) music(I-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, person, band, event, music genre, location, award, musical artist, musical instrument, album, song and O.\nSentence: Electropop pioneers Haruomi Hosono and Ryuichi Sakamoto of the Yellow Magic Orchestra produced a 1978 Electronic music album , Cochin Moon , based on an experimental fusion of electronic music and Bollywood-inspired Indian music .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Electropop","pioneers","Haruomi","Hosono","and","Ryuichi","Sakamoto","of","the","Yellow","Magic","Orchestra","produced","a","1978","Electronic","music","album",",","Cochin","Moon",",","based","on","an","experimental","fusion","of","electronic","music","and","Bollywood-inspired","Indian","music","."],"labels":["B-music genre","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","B-band","I-band","I-band","O","O","O","B-music genre","I-music genre","O","O","B-album","I-album","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["country","organization","person","band","event","music_genre","location","award","musical_artist","musical_instrument","album","song"]}
{"id":"91","dataset":"crossner_music","split":"dev","instance":{"id":"91","prompt_labels":"Black(B-album) Holes(I-album) and(I-album) Revelations(I-album) ((O) 2006(O) )(O) incorporated(O) Electronic(B-music genre) music(I-music genre) and(O) Pop(B-music genre) music(I-music genre) elements(O) ,(O) displayed(O) in(O) singles(O) such(O) as(O) Supermassive(B-song) Black(I-song) Hole(I-song) ,(O) Their(O) seventh(O) album(O) ,(O) Drones(B-album) ((O) 2015(O) )(O) ,(O) was(O) a(O) concept(O) album(O) about(O) drone(B-album) warfare(I-album) and(O) returned(O) to(O) a(O) harder(O) rock(B-music genre) sound(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, organization, country, location, event, musical artist, band, album, award, song, person, music genre and O.\nSentence: Black Holes and Revelations ( 2006 ) incorporated Electronic music and Pop music elements , displayed in singles such as Supermassive Black Hole , Their seventh album , Drones ( 2015 ) , was a concept album about drone warfare and returned to a harder rock sound .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Black","Holes","and","Revelations","(","2006",")","incorporated","Electronic","music","and","Pop","music","elements",",","displayed","in","singles","such","as","Supermassive","Black","Hole",",","Their","seventh","album",",","Drones","(","2015",")",",","was","a","concept","album","about","drone","warfare","and","returned","to","a","harder","rock","sound","."],"labels":["B-album","I-album","I-album","I-album","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O","O","O","O","O","O","B-song","I-song","I-song","O","O","O","O","O","B-album","O","O","O","O","O","O","O","O","O","B-album","I-album","O","O","O","O","O","B-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","organization","country","location","event","musical_artist","band","album","award","song","person","music_genre"]}
{"id":"92","dataset":"crossner_music","split":"dev","instance":{"id":"92","prompt_labels":"The(B-band) 5th(I-band) Dimension(I-band) is(O) an(O) United(B-country) States(I-country) popular(B-music genre) music(I-music genre) vocal(O) group(O) ,(O) whose(O) repertoire(O) includes(O) Pop(B-music genre) music(I-music genre) ,(O) Rhythm(B-music genre) and(I-music genre) blues(I-music genre) ,(O) Soul(B-music genre) music(I-music genre) ,(O) jazz(B-music genre) ,(O) light(B-music genre) opera(I-music genre) ,(O) and(O) Broadway(B-music genre) -(O) the(O) melange(O) was(O) coined(O) as(O) Champagne(B-music genre) Soul(I-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, musical instrument, song, country, band, musical artist, person, event, album, music genre, award, location and O.\nSentence: The 5th Dimension is an United States popular music vocal group , whose repertoire includes Pop music , Rhythm and blues , Soul music , jazz , light opera , and Broadway - the melange was coined as Champagne Soul .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","5th","Dimension","is","an","United","States","popular","music","vocal","group",",","whose","repertoire","includes","Pop","music",",","Rhythm","and","blues",",","Soul","music",",","jazz",",","light","opera",",","and","Broadway","-","the","melange","was","coined","as","Champagne","Soul","."],"labels":["B-band","I-band","I-band","O","O","B-country","I-country","B-music genre","I-music genre","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","O","B-music genre","O","O","O","O","O","O","B-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["organization","musical_instrument","song","country","band","musical_artist","person","event","album","music_genre","award","location"]}
{"id":"98","dataset":"crossner_music","split":"dev","instance":{"id":"98","prompt_labels":"Lynn(B-musical artist) has(O) received(O) numerous(O) awards(O) and(O) other(O) accolades(O) for(O) her(O) groundbreaking(O) role(O) in(O) country(B-music genre) music(I-music genre) ,(O) including(O) awards(O) from(O) both(O) the(O) Country(B-organization) Music(I-organization) Association(I-organization) and(O) Academy(B-organization) of(I-organization) Country(I-organization) Music(I-organization) as(O) a(O) duet(O) partner(O) and(O) an(O) individual(O) artist(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, song, band, music genre, album, musical artist, location, country, musical instrument, event, person, organization and O.\nSentence: Lynn has received numerous awards and other accolades for her groundbreaking role in country music , including awards from both the Country Music Association and Academy of Country Music as a duet partner and an individual artist .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Lynn","has","received","numerous","awards","and","other","accolades","for","her","groundbreaking","role","in","country","music",",","including","awards","from","both","the","Country","Music","Association","and","Academy","of","Country","Music","as","a","duet","partner","and","an","individual","artist","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","song","band","music_genre","album","musical_artist","location","country","musical_instrument","event","person","organization"]}
{"id":"100","dataset":"crossner_music","split":"dev","instance":{"id":"100","prompt_labels":"Taylor(B-musical artist) Momsen(I-musical artist) ,(O) Marcus(B-band) Durant(I-band) ,(O) Brandi(B-musical artist) Carlile(I-musical artist) and(O) Taylor(B-musical artist) Hawkins(I-musical artist) contributed(O) vocals(O) to(O) Soundgarden(B-band) ,(O) who(O) performed(O) Rusty(B-song) Cage(I-song) ,(O) Flower(B-song) ,(O) Outshined(B-song) ,(O) Drawing(B-song) Flies(I-song) ,(O) Loud(B-song) Love(I-song) ,(O) I(B-song) Awake(I-song) ,(O) The(B-song) Day(I-song) I(I-song) Tried(I-song) to(I-song) Live(I-song) and(O) Black(B-song) Hole(I-song) Sun(I-song) ,(O) making(O) this(O) their(O) only(O) performance(O) since(O) Cornell(B-musical artist) 's(O) death(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, album, musical artist, country, organization, event, music genre, song, location, band, musical instrument, award and O.\nSentence: Taylor Momsen , Marcus Durant , Brandi Carlile and Taylor Hawkins contributed vocals to Soundgarden , who performed Rusty Cage , Flower , Outshined , Drawing Flies , Loud Love , I Awake , The Day I Tried to Live and Black Hole Sun , making this their only performance since Cornell 's death .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Taylor","Momsen",",","Marcus","Durant",",","Brandi","Carlile","and","Taylor","Hawkins","contributed","vocals","to","Soundgarden",",","who","performed","Rusty","Cage",",","Flower",",","Outshined",",","Drawing","Flies",",","Loud","Love",",","I","Awake",",","The","Day","I","Tried","to","Live","and","Black","Hole","Sun",",","making","this","their","only","performance","since","Cornell","'s","death","."],"labels":["B-musical artist","I-musical artist","O","B-band","I-band","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","B-band","O","O","O","B-song","I-song","O","B-song","O","B-song","O","B-song","I-song","O","B-song","I-song","O","B-song","I-song","O","B-song","I-song","I-song","I-song","I-song","I-song","O","B-song","I-song","I-song","O","O","O","O","O","O","O","B-musical artist","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","album","musical_artist","country","organization","event","music_genre","song","location","band","musical_instrument","award"]}
{"id":"101","dataset":"crossner_music","split":"dev","instance":{"id":"101","prompt_labels":"The(O) film(O) received(O) several(O) Golden(B-award) Globe(I-award) Awards(I-award) and(O) Academy(B-award) Awards(I-award) nominations(O) ,(O) and(O) earned(O) Kidman(O) a(O) fourth(O) Screen(B-award) Actors(I-award) Guild(I-award) Award(I-award) nomination(O) ,(O) as(O) part(O) of(O) the(O) Screen(B-award) Actors(I-award) Guild(I-award) Award(I-award) for(I-award) Outstanding(I-award) Performance(I-award) by(I-award) a(I-award) Cast(I-award) in(I-award) a(I-award) Motion(I-award) Picture(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, band, person, event, album, organization, song, country, location, award, musical instrument, musical artist and O.\nSentence: The film received several Golden Globe Awards and Academy Awards nominations , and earned Kidman a fourth Screen Actors Guild Award nomination , as part of the Screen Actors Guild Award for Outstanding Performance by a Cast in a Motion Picture .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","film","received","several","Golden","Globe","Awards","and","Academy","Awards","nominations",",","and","earned","Kidman","a","fourth","Screen","Actors","Guild","Award","nomination",",","as","part","of","the","Screen","Actors","Guild","Award","for","Outstanding","Performance","by","a","Cast","in","a","Motion","Picture","."],"labels":["O","O","O","O","B-award","I-award","I-award","O","B-award","I-award","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["music_genre","band","person","event","album","organization","song","country","location","award","musical_instrument","musical_artist"]}
{"id":"103","dataset":"crossner_music","split":"dev","instance":{"id":"103","prompt_labels":"New(B-location) York(I-location) 's(O) rock(B-music genre) scene(O) includes(O) clubs(O) such(O) as(O) Irving(B-location) Plaza(I-location) ,(O) while(O) the(O) city(O) 's(O) avant-garde(O) downtown(O) scene(O) includes(O) The(B-location) Kitchen(I-location) ,(O) Roulette(B-location) ,(O) and(O) Knitting(B-location) Factory(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, music genre, musical artist, award, organization, album, location, country, band, musical instrument, song, event and O.\nSentence: New York 's rock scene includes clubs such as Irving Plaza , while the city 's avant-garde downtown scene includes The Kitchen , Roulette , and Knitting Factory .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["New","York","'s","rock","scene","includes","clubs","such","as","Irving","Plaza",",","while","the","city","'s","avant-garde","downtown","scene","includes","The","Kitchen",",","Roulette",",","and","Knitting","Factory","."],"labels":["B-location","I-location","O","B-music genre","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","O","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["person","music_genre","musical_artist","award","organization","album","location","country","band","musical_instrument","song","event"]}
{"id":"109","dataset":"crossner_music","split":"dev","instance":{"id":"109","prompt_labels":"First(O) ,(O) they(O) played(O) a(O) series(O) of(O) concerts(O) at(O) the(O) Glr(B-location) Theatre(I-location) in(O) Ennis(B-location) ,(O) County(B-location) Clare(B-location) ((O) on(O) 23(O) &(O) amp(O) ;(O) 24(O) January(O) 2004(O) )(O) and(O) at(O) Vicar(B-location) Street(I-location) in(O) Dublin(B-location) ((O) on(O) 30(O) &(O) amp(O) ;(O) 31(O) January(O) and(O) on(O) 4(O) &(O) amp(O) ;(O) 5(O) ,(O) 11(O) &(O) amp(O) ;(O) 12(O) February(O) 2004(O) )(O) ,(O) which(O) were(O) recorded(O) and(O) from(O) which(O) selected(O) material(O) was(O) released(O) on(O) the(O) CD(O) Live(B-album) 2004(I-album) and(O) its(O) associated(O) DVD(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, country, event, award, musical artist, band, song, location, organization, person, music genre, album and O.\nSentence: First , they played a series of concerts at the Glr Theatre in Ennis , County Clare ( on 23 & amp ; 24 January 2004 ) and at Vicar Street in Dublin ( on 30 & amp ; 31 January and on 4 & amp ; 5 , 11 & amp ; 12 February 2004 ) , which were recorded and from which selected material was released on the CD Live 2004 and its associated DVD .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["First",",","they","played","a","series","of","concerts","at","the","Glr","Theatre","in","Ennis",",","County","Clare","(","on","23","&","amp",";","24","January","2004",")","and","at","Vicar","Street","in","Dublin","(","on","30","&","amp",";","31","January","and","on","4","&","amp",";","5",",","11","&","amp",";","12","February","2004",")",",","which","were","recorded","and","from","which","selected","material","was","released","on","the","CD","Live","2004","and","its","associated","DVD","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","O","B-location","B-location","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","country","event","award","musical_artist","band","song","location","organization","person","music_genre","album"]}
{"id":"111","dataset":"crossner_music","split":"dev","instance":{"id":"111","prompt_labels":"British(O) genres(O) such(O) as(O) Lovers(B-music genre) rock(I-music genre) ,(O) Ragga(B-music genre) jungle(I-music genre) and(O) grime(B-music genre) are(O) also(O) influenced(O) by(O) Jamaican(O) music(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, person, organization, location, band, musical instrument, album, music genre, event, award, country, musical artist and O.\nSentence: British genres such as Lovers rock , Ragga jungle and grime are also influenced by Jamaican music .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["British","genres","such","as","Lovers","rock",",","Ragga","jungle","and","grime","are","also","influenced","by","Jamaican","music","."],"labels":["O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["song","person","organization","location","band","musical_instrument","album","music_genre","event","award","country","musical_artist"]}
{"id":"112","dataset":"crossner_music","split":"dev","instance":{"id":"112","prompt_labels":"some(O) of(O) his(O) performances(O) will(O) finish(O) without(O) some(O) blues(B-music genre) and(O) Tejano(B-music genre) music(I-music genre) tunes(O) playing(O) as(O) well(O) as(O) Surf(B-music genre) music(I-music genre) instrumentals(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, music genre, band, location, album, award, musical artist, person, event, organization, song, musical instrument and O.\nSentence: some of his performances will finish without some blues and Tejano music tunes playing as well as Surf music instrumentals .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["some","of","his","performances","will","finish","without","some","blues","and","Tejano","music","tunes","playing","as","well","as","Surf","music","instrumentals","."],"labels":["O","O","O","O","O","O","O","O","B-music genre","O","B-music genre","I-music genre","O","O","O","O","O","B-music genre","I-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["country","music_genre","band","location","album","award","musical_artist","person","event","organization","song","musical_instrument"]}
{"id":"113","dataset":"crossner_music","split":"dev","instance":{"id":"113","prompt_labels":"They(O) have(O) won(O) two(O) Grammy(B-award) Award(I-award) s(O) ,(O) six(O) American(B-award) Music(I-award) Awards(I-award) ,(O) two(O) Billboard(B-award) Music(I-award) Award(I-award) ,(O) four(O) MTV(B-award) Video(I-award) Music(I-award) Award(I-award) s(O) ,(O) 10(O) MTV(B-award) Europe(I-award) Music(I-award) Award(I-award) and(O) three(O) World(B-award) Music(I-award) Awards(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, song, person, organization, music genre, musical instrument, band, location, award, event, musical artist, album and O.\nSentence: They have won two Grammy Award s , six American Music Awards , two Billboard Music Award , four MTV Video Music Award s , 10 MTV Europe Music Award and three World Music Awards .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["They","have","won","two","Grammy","Award","s",",","six","American","Music","Awards",",","two","Billboard","Music","Award",",","four","MTV","Video","Music","Award","s",",","10","MTV","Europe","Music","Award","and","three","World","Music","Awards","."],"labels":["O","O","O","O","B-award","I-award","O","O","O","B-award","I-award","I-award","O","O","B-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["country","song","person","organization","music_genre","musical_instrument","band","location","award","event","musical_artist","album"]}
{"id":"120","dataset":"crossner_music","split":"dev","instance":{"id":"120","prompt_labels":"Spears(O) has(O) earned(O) numerous(O) awards(O) and(O) accolades(O) ,(O) including(O) a(O) Grammy(B-award) Award(I-award) ;(O) seven(O) Guinness(B-award) World(I-award) Records(I-award) ;(O) six(O) MTV(B-award) Video(I-award) Music(I-award) Awards(I-award) ,(O) including(O) the(O) Michael(B-award) Jackson(I-award) Video(I-award) Vanguard(I-award) Award(I-award) ;(O) seven(O) Billboard(B-award) Music(I-award) Awards(I-award) ,(O) including(O) the(O) Millennium(B-award) Award(I-award) ;(O) the(O) inaugural(B-award) Radio(I-award) Disney(I-award) Icon(I-award) Award(I-award) ;(O) the(O) GLAAD(B-award) Media(I-award) Award(I-award) '(O) s(O) Vanguard(B-award) Award(I-award) and(O) a(O) star(O) on(O) the(O) Hollywood(B-location) Walk(I-location) of(I-location) Fame(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, person, award, music genre, band, location, musical instrument, musical artist, organization, song, album and O.\nSentence: Spears has earned numerous awards and accolades , including a Grammy Award ; seven Guinness World Records ; six MTV Video Music Awards , including the Michael Jackson Video Vanguard Award ; seven Billboard Music Awards , including the Millennium Award ; the inaugural Radio Disney Icon Award ; the GLAAD Media Award ' s Vanguard Award and a star on the Hollywood Walk of Fame .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Spears","has","earned","numerous","awards","and","accolades",",","including","a","Grammy","Award",";","seven","Guinness","World","Records",";","six","MTV","Video","Music","Awards",",","including","the","Michael","Jackson","Video","Vanguard","Award",";","seven","Billboard","Music","Awards",",","including","the","Millennium","Award",";","the","inaugural","Radio","Disney","Icon","Award",";","the","GLAAD","Media","Award","'","s","Vanguard","Award","and","a","star","on","the","Hollywood","Walk","of","Fame","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","O","O","B-award","I-award","O","O","O","O","O","B-location","I-location","I-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["event","country","person","award","music_genre","band","location","musical_instrument","musical_artist","organization","song","album"]}
{"id":"121","dataset":"crossner_music","split":"dev","instance":{"id":"121","prompt_labels":"The(O) venue(O) was(O) the(O) site(O) of(O) several(O) Commonwealth(O) Games(O) sports(O) in(O) 1978(B-event) Commonwealth(I-event) Games(I-event) ,(O) and(O) part(O) of(O) Universiade(B-event) ((O) the(O) World(B-event) University(I-event) Games(I-event) )(O) in(O) 1983(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, location, event, organization, award, music genre, song, album, band, country, person, musical instrument and O.\nSentence: The venue was the site of several Commonwealth Games sports in 1978 Commonwealth Games , and part of Universiade ( the World University Games ) in 1983 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","venue","was","the","site","of","several","Commonwealth","Games","sports","in","1978","Commonwealth","Games",",","and","part","of","Universiade","(","the","World","University","Games",")","in","1983","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","B-event","O","O","B-event","I-event","I-event","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","location","event","organization","award","music_genre","song","album","band","country","person","musical_instrument"]}
{"id":"122","dataset":"crossner_music","split":"dev","instance":{"id":"122","prompt_labels":"Kraftwerk(B-musical artist) 's(O) musical(O) style(O) and(O) image(O) can(O) be(O) heard(O) and(O) seen(O) in(O) 1980s(O) synthpop(B-music genre) groups(O) such(O) as(O) Gary(B-musical artist) Numan(I-musical artist) ,(O) Ultravox(B-band) ,(O) John(B-musical artist) Foxx(I-musical artist) ,(O) Orchestral(B-band) Manoeuvres(I-band) in(I-band) the(I-band) Dark(I-band) ,(O) Human(B-band) League(I-band) ,(O) Depeche(B-band) Mode(I-band) ,(O) Visage(B-band) ,(O) and(O) Soft(B-band) Cell(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, event, music genre, band, organization, person, country, location, album, musical artist, song, award and O.\nSentence: Kraftwerk 's musical style and image can be heard and seen in 1980s synthpop groups such as Gary Numan , Ultravox , John Foxx , Orchestral Manoeuvres in the Dark , Human League , Depeche Mode , Visage , and Soft Cell .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Kraftwerk","'s","musical","style","and","image","can","be","heard","and","seen","in","1980s","synthpop","groups","such","as","Gary","Numan",",","Ultravox",",","John","Foxx",",","Orchestral","Manoeuvres","in","the","Dark",",","Human","League",",","Depeche","Mode",",","Visage",",","and","Soft","Cell","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","O","O","O","B-musical artist","I-musical artist","O","B-band","O","B-musical artist","I-musical artist","O","B-band","I-band","I-band","I-band","I-band","O","B-band","I-band","O","B-band","I-band","O","B-band","O","O","B-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","event","music_genre","band","organization","person","country","location","album","musical_artist","song","award"]}
{"id":"123","dataset":"crossner_music","split":"dev","instance":{"id":"123","prompt_labels":"Styles(O) like(O) Rock(B-music genre) music(I-music genre) ,(O) Heavy(B-music genre) metal(I-music genre) music(I-music genre) ,(O) Pop(B-music genre) rock(I-music genre) ,(O) Folk(B-music genre) rock(I-music genre) ,(O) Neo-Romantic(B-music genre) ,(O) Pop(B-music genre) and(O) some(O) experimental(O) styles(O) ((O) e.g.(O) Cantorock(B-music genre) )(O) were(O) introduced(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, country, music genre, song, musical artist, album, event, person, musical instrument, award, band and O.\nSentence: Styles like Rock music , Heavy metal music , Pop rock , Folk rock , Neo-Romantic , Pop and some experimental styles ( e.g. Cantorock ) were introduced .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Styles","like","Rock","music",",","Heavy","metal","music",",","Pop","rock",",","Folk","rock",",","Neo-Romantic",",","Pop","and","some","experimental","styles","(","e.g.","Cantorock",")","were","introduced","."],"labels":["O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","O","O","O","O","O","O","B-music genre","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","organization","country","music_genre","song","musical_artist","album","event","person","musical_instrument","award","band"]}
{"id":"124","dataset":"crossner_music","split":"dev","instance":{"id":"124","prompt_labels":"High-pitched(O) screaming(O) is(O) occasionally(O) utilized(O) in(O) death(B-music genre) metal(I-music genre) ,(O) being(O) heard(O) in(O) songs(O) by(O) Death(B-band) ,(O) Aborted(B-band) ,(O) Exhumed(B-band) ,(O) Dying(B-band) Fetus(I-band) ,(O) Cannibal(B-band) Corpse(I-band) ,(O) and(O) Deicide(B-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, event, location, award, album, song, music genre, organization, musical instrument, band, country, person and O.\nSentence: High-pitched screaming is occasionally utilized in death metal , being heard in songs by Death , Aborted , Exhumed , Dying Fetus , Cannibal Corpse , and Deicide .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["High-pitched","screaming","is","occasionally","utilized","in","death","metal",",","being","heard","in","songs","by","Death",",","Aborted",",","Exhumed",",","Dying","Fetus",",","Cannibal","Corpse",",","and","Deicide","."],"labels":["O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","B-band","O","B-band","O","B-band","O","B-band","I-band","O","B-band","I-band","O","O","B-band","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","event","location","award","album","song","music_genre","organization","musical_instrument","band","country","person"]}
{"id":"125","dataset":"crossner_music","split":"dev","instance":{"id":"125","prompt_labels":"Rodgers(B-musical artist) was(O) the(O) first(O) person(O) to(O) win(O) what(O) is(O) considered(O) the(O) top(O) American(O) entertainment(O) awards(O) in(O) television(O) ,(O) recording(O) ,(O) movies(O) ,(O) and(O) Broadway(B-organization) -(O) an(O) Emmy(B-award) Award(I-award) ,(O) a(O) Grammy(B-award) Award(I-award) ,(O) an(O) Academy(B-award) Awards(I-award) ,(O) and(O) a(O) Tony(B-award) Award(I-award) -(O) now(O) known(O) collectively(O) as(O) an(O) EGOT(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, song, award, musical instrument, organization, musical artist, location, band, music genre, country, event, person and O.\nSentence: Rodgers was the first person to win what is considered the top American entertainment awards in television , recording , movies , and Broadway - an Emmy Award , a Grammy Award , an Academy Awards , and a Tony Award - now known collectively as an EGOT .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Rodgers","was","the","first","person","to","win","what","is","considered","the","top","American","entertainment","awards","in","television",",","recording",",","movies",",","and","Broadway","-","an","Emmy","Award",",","a","Grammy","Award",",","an","Academy","Awards",",","and","a","Tony","Award","-","now","known","collectively","as","an","EGOT","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","O","B-award","I-award","O","O","B-award","I-award","O","O","B-award","I-award","O","O","O","B-award","I-award","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["album","song","award","musical_instrument","organization","musical_artist","location","band","music_genre","country","event","person"]}
{"id":"126","dataset":"crossner_music","split":"dev","instance":{"id":"126","prompt_labels":"During(O) his(O) musical(O) career(O) ,(O) Tankian(B-musical artist) has(O) released(O) five(O) albums(O) with(O) System(B-band) of(I-band) a(I-band) Down(I-band) ,(O) one(O) with(O) Arto(B-musical artist) Tunboyacyan(I-musical artist) ((O) Serart(B-album) )(O) ,(O) as(O) well(O) as(O) the(O) five(O) solo(O) albums(O) Elect(B-album) the(I-album) Dead(I-album) ,(O) Imperfect(B-album) Harmonies(I-album) ,(O) Harakiri(B-album) ,(O) Orca(B-album) Symphony(I-album) No.(I-album) 1(I-album) ,(O) and(O) Jazz-Iz-Christ(B-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, location, country, album, music genre, organization, musical instrument, band, song, person, musical artist and O.\nSentence: During his musical career , Tankian has released five albums with System of a Down , one with Arto Tunboyacyan ( Serart ) , as well as the five solo albums Elect the Dead , Imperfect Harmonies , Harakiri , Orca Symphony No. 1 , and Jazz-Iz-Christ .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","his","musical","career",",","Tankian","has","released","five","albums","with","System","of","a","Down",",","one","with","Arto","Tunboyacyan","(","Serart",")",",","as","well","as","the","five","solo","albums","Elect","the","Dead",",","Imperfect","Harmonies",",","Harakiri",",","Orca","Symphony","No.","1",",","and","Jazz-Iz-Christ","."],"labels":["O","O","O","O","O","B-musical artist","O","O","O","O","O","B-band","I-band","I-band","I-band","O","O","O","B-musical artist","I-musical artist","O","B-album","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O","B-album","I-album","O","B-album","O","B-album","I-album","I-album","I-album","O","O","B-album","O"],"target_index":null,"target_label":null},"label_list":["event","award","location","country","album","music_genre","organization","musical_instrument","band","song","person","musical_artist"]}
{"id":"128","dataset":"crossner_music","split":"dev","instance":{"id":"128","prompt_labels":"They(O) restarted(O) to(O) perform(O) live(O) regularly(O) ,(O) touring(O) the(O) world(O) with(O) rapturous(O) feedbacks(O) :(O) they(O) brought(O) their(O) distinguishable(O) sound(O) in(O) great(O) venues(O) such(O) as(O) the(O) Kings(B-location) Place(I-location) in(O) London(B-location) ,(O) the(O) Soma(B-event) Festival(I-event) in(O) Belfast(B-location) ,(O) the(O) Bolshoi(B-location) Theatre(I-location) in(O) Moscow(B-location) and(O) the(O) Star(B-location) Pine(I-location) 's(I-location) cafe(I-location) in(O) Tokyo(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, band, organization, location, musical instrument, person, event, music genre, musical artist, album, award, country and O.\nSentence: They restarted to perform live regularly , touring the world with rapturous feedbacks : they brought their distinguishable sound in great venues such as the Kings Place in London , the Soma Festival in Belfast , the Bolshoi Theatre in Moscow and the Star Pine 's cafe in Tokyo .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["They","restarted","to","perform","live","regularly",",","touring","the","world","with","rapturous","feedbacks",":","they","brought","their","distinguishable","sound","in","great","venues","such","as","the","Kings","Place","in","London",",","the","Soma","Festival","in","Belfast",",","the","Bolshoi","Theatre","in","Moscow","and","the","Star","Pine","'s","cafe","in","Tokyo","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","O","O","B-event","I-event","O","B-location","O","O","B-location","I-location","O","B-location","O","O","B-location","I-location","I-location","I-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["song","band","organization","location","musical_instrument","person","event","music_genre","musical_artist","album","award","country"]}
{"id":"129","dataset":"crossner_music","split":"dev","instance":{"id":"129","prompt_labels":"The(O) Hall(O) is(O) flanked(O) by(O) The(O) Royal(B-location) Lyceum(I-location) Theatre(I-location) on(O) the(O) right(O) and(O) The(O) Traverse(B-location) Theatre(I-location) on(O) the(O) left(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, album, award, musical artist, event, person, music genre, song, organization, location, country, band and O.\nSentence: The Hall is flanked by The Royal Lyceum Theatre on the right and The Traverse Theatre on the left .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Hall","is","flanked","by","The","Royal","Lyceum","Theatre","on","the","right","and","The","Traverse","Theatre","on","the","left","."],"labels":["O","O","O","O","O","O","B-location","I-location","I-location","O","O","O","O","O","B-location","I-location","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","album","award","musical_artist","event","person","music_genre","song","organization","location","country","band"]}
{"id":"133","dataset":"crossner_music","split":"dev","instance":{"id":"133","prompt_labels":"In(O) 2006(O) ,(O) the(O) Doha(B-organization) Asian(I-organization) Games(I-organization) Organising(I-organization) Committee(I-organization) has(O) named(O) Gary(B-musical artist) Valenciano(I-musical artist) the(O) official(O) performer(O) of(O) the(O) 2006(B-event) Asian(I-event) Games(I-event) '(O) theme(O) song(O) ,(O) Side(B-song) By(I-song) Side(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, song, band, event, country, musical artist, music genre, location, person, album, musical instrument, award and O.\nSentence: In 2006 , the Doha Asian Games Organising Committee has named Gary Valenciano the official performer of the 2006 Asian Games ' theme song , Side By Side .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2006",",","the","Doha","Asian","Games","Organising","Committee","has","named","Gary","Valenciano","the","official","performer","of","the","2006","Asian","Games","'","theme","song",",","Side","By","Side","."],"labels":["O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-musical artist","I-musical artist","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","B-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["organization","song","band","event","country","musical_artist","music_genre","location","person","album","musical_instrument","award"]}
{"id":"134","dataset":"crossner_music","split":"dev","instance":{"id":"134","prompt_labels":"DC(B-band) Talk(I-band) ((O) stylized(O) as(O) dc(B-band) Talk(I-band) )(O) is(O) a(O) Christian(B-music genre) hip(I-music genre) hop(I-music genre) and(O) Christian(B-music genre) rock(I-music genre) trio(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, music genre, event, person, organization, album, song, award, musical instrument, band, country, musical artist and O.\nSentence: DC Talk ( stylized as dc Talk ) is a Christian hip hop and Christian rock trio .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["DC","Talk","(","stylized","as","dc","Talk",")","is","a","Christian","hip","hop","and","Christian","rock","trio","."],"labels":["B-band","I-band","O","O","O","B-band","I-band","O","O","O","B-music genre","I-music genre","I-music genre","O","B-music genre","I-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["location","music_genre","event","person","organization","album","song","award","musical_instrument","band","country","musical_artist"]}
{"id":"140","dataset":"crossner_music","split":"dev","instance":{"id":"140","prompt_labels":"Black(B-music genre) metal(I-music genre) album(O) covers(O) are(O) typically(O) dark(O) and(O) tend(O) to(O) be(O) atmospheric(O) or(O) provocative(O) ;(O) some(O) feature(O) natural(O) or(O) fantasy(O) landscapes(O) ((O) for(O) example(O) Burzum(B-band) '(O) s(O) Filosofem(B-album) and(O) Emperor(B-band) 's(O) In(B-album) the(I-album) Nightside(I-album) Eclipse(I-album) )(O) while(O) others(O) are(O) violent(O) ,(O) sexually(O) transgressive(O) ,(O) sacrilegious(O) ,(O) or(O) iconoclastic(O) ((O) for(O) example(O) Marduk(B-band) '(O) s(O) Fuck(B-album) Me(I-album) Jesus(I-album) and(O) Dimmu(B-band) Borgir(I-band) '(O) s(O) In(B-album) Sorte(I-album) Diaboli(I-album) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, song, album, organization, band, person, music genre, country, musical instrument, location, musical artist, event and O.\nSentence: Black metal album covers are typically dark and tend to be atmospheric or provocative ; some feature natural or fantasy landscapes ( for example Burzum ' s Filosofem and Emperor 's In the Nightside Eclipse ) while others are violent , sexually transgressive , sacrilegious , or iconoclastic ( for example Marduk ' s Fuck Me Jesus and Dimmu Borgir ' s In Sorte Diaboli ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Black","metal","album","covers","are","typically","dark","and","tend","to","be","atmospheric","or","provocative",";","some","feature","natural","or","fantasy","landscapes","(","for","example","Burzum","'","s","Filosofem","and","Emperor","'s","In","the","Nightside","Eclipse",")","while","others","are","violent",",","sexually","transgressive",",","sacrilegious",",","or","iconoclastic","(","for","example","Marduk","'","s","Fuck","Me","Jesus","and","Dimmu","Borgir","'","s","In","Sorte","Diaboli",")","."],"labels":["B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","O","O","B-album","O","B-band","O","B-album","I-album","I-album","I-album","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","O","O","B-album","I-album","I-album","O","B-band","I-band","O","O","B-album","I-album","I-album","O","O"],"target_index":null,"target_label":null},"label_list":["award","song","album","organization","band","person","music_genre","country","musical_instrument","location","musical_artist","event"]}
{"id":"141","dataset":"crossner_music","split":"dev","instance":{"id":"141","prompt_labels":"Parliament-Funkadelic(B-band) 's(O) musical(O) influence(O) can(O) also(O) be(O) heard(O) in(O) rhythm(B-music genre) and(I-music genre) blues(I-music genre) ,(O) Soul(B-music genre) music(I-music genre) ,(O) electronica(B-music genre) ,(O) Gospel(B-music genre) music(I-music genre) ,(O) jazz(B-music genre) ,(O) and(O) New(B-music genre) wave(I-music genre) music(I-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, musical instrument, organization, award, musical artist, event, song, album, person, music genre, country, band and O.\nSentence: Parliament-Funkadelic 's musical influence can also be heard in rhythm and blues , Soul music , electronica , Gospel music , jazz , and New wave music .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Parliament-Funkadelic","'s","musical","influence","can","also","be","heard","in","rhythm","and","blues",",","Soul","music",",","electronica",",","Gospel","music",",","jazz",",","and","New","wave","music","."],"labels":["B-band","O","O","O","O","O","O","O","O","B-music genre","I-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","O","O","B-music genre","I-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["location","musical_instrument","organization","award","musical_artist","event","song","album","person","music_genre","country","band"]}
{"id":"142","dataset":"crossner_music","split":"dev","instance":{"id":"142","prompt_labels":"The(O) set(O) includes(O) the(O) first(O) round(O) of(O) the(O) remastered(O) series(O) plus(O) the(O) long-awaited(O) remastered(O) versions(O) of(O) On(B-album) Your(I-album) Feet(I-album) or(I-album) on(I-album) Your(I-album) Knees(I-album) ((O) 1975(O) )(O) ,(O) Mirrors(B-album) ,(O) Cultsaurus(B-album) Erectus(I-album) ,(O) Fire(B-album) Of(I-album) Unknown(I-album) Origin(I-album) ,(O) Extraterrestrial(B-album) Live(I-album) ,(O) The(B-album) Revlution(I-album) by(I-album) Night(I-album) ,(O) Club(B-album) Ninja(I-album) and(O) Imaginos(B-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, location, country, award, song, organization, band, event, musical instrument, album, music genre, person and O.\nSentence: The set includes the first round of the remastered series plus the long-awaited remastered versions of On Your Feet or on Your Knees ( 1975 ) , Mirrors , Cultsaurus Erectus , Fire Of Unknown Origin , Extraterrestrial Live , The Revlution by Night , Club Ninja and Imaginos .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","set","includes","the","first","round","of","the","remastered","series","plus","the","long-awaited","remastered","versions","of","On","Your","Feet","or","on","Your","Knees","(","1975",")",",","Mirrors",",","Cultsaurus","Erectus",",","Fire","Of","Unknown","Origin",",","Extraterrestrial","Live",",","The","Revlution","by","Night",",","Club","Ninja","and","Imaginos","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","I-album","I-album","I-album","I-album","O","O","O","O","B-album","O","B-album","I-album","O","B-album","I-album","I-album","I-album","O","B-album","I-album","O","B-album","I-album","I-album","I-album","O","B-album","I-album","O","B-album","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","location","country","award","song","organization","band","event","musical_instrument","album","music_genre","person"]}
{"id":"143","dataset":"crossner_music","split":"dev","instance":{"id":"143","prompt_labels":"In(O) 1994(O) ,(O) Yauch(B-musical artist) and(O) activist(O) Erin(B-musical artist) Potts(I-musical artist) The(O) events(O) became(O) annual(O) ,(O) and(O) shortly(O) after(O) went(O) international(O) with(O) acts(O) such(O) as(O) Live(B-band) ,(O) Mike(B-musical artist) Mills(I-musical artist) and(O) Michael(B-musical artist) Stipe(I-musical artist) of(O) R.E.M.(B-band) ,(O) Rage(B-band) Against(I-band) the(I-band) Machine(I-band) ,(O) The(B-band) Smashing(I-band) Pumpkins(I-band) ,(O) and(O) U2(B-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical instrument, event, album, organization, song, musical artist, location, country, award, person, music genre and O.\nSentence: In 1994 , Yauch and activist Erin Potts The events became annual , and shortly after went international with acts such as Live , Mike Mills and Michael Stipe of R.E.M. , Rage Against the Machine , The Smashing Pumpkins , and U2 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1994",",","Yauch","and","activist","Erin","Potts","The","events","became","annual",",","and","shortly","after","went","international","with","acts","such","as","Live",",","Mike","Mills","and","Michael","Stipe","of","R.E.M.",",","Rage","Against","the","Machine",",","The","Smashing","Pumpkins",",","and","U2","."],"labels":["O","O","O","B-musical artist","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-band","O","B-band","I-band","I-band","I-band","O","B-band","I-band","I-band","O","O","B-band","O"],"target_index":null,"target_label":null},"label_list":["band","musical_instrument","event","album","organization","song","musical_artist","location","country","award","person","music_genre"]}
{"id":"145","dataset":"crossner_music","split":"dev","instance":{"id":"145","prompt_labels":"UK(B-country) Christmas(B-event) hit(O) singles(O) ;(O) Merry(B-song) Xmas(I-song) Everybody(I-song) by(O) Slade(B-band) ,(O) I(B-song) Wish(I-song) It(I-song) Could(I-song) Be(I-song) Christmas(I-song) Everyday(I-song) by(O) Wizzard(B-band) and(O) Lonely(B-song) This(I-song) Christmas(I-song) by(O) Mud(B-band) ,(O) all(O) of(O) which(O) have(O) remained(O) hugely(O) popular(O) ..(O) 14(O) December(O) 2012(O) PRS(B-organization) press(O) release(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, event, band, music genre, country, organization, musical artist, album, location, award, person, musical instrument and O.\nSentence: UK Christmas hit singles ; Merry Xmas Everybody by Slade , I Wish It Could Be Christmas Everyday by Wizzard and Lonely This Christmas by Mud , all of which have remained hugely popular .. 14 December 2012 PRS press release .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["UK","Christmas","hit","singles",";","Merry","Xmas","Everybody","by","Slade",",","I","Wish","It","Could","Be","Christmas","Everyday","by","Wizzard","and","Lonely","This","Christmas","by","Mud",",","all","of","which","have","remained","hugely","popular","..","14","December","2012","PRS","press","release","."],"labels":["B-country","B-event","O","O","O","B-song","I-song","I-song","O","B-band","O","B-song","I-song","I-song","I-song","I-song","I-song","I-song","O","B-band","O","B-song","I-song","I-song","O","B-band","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","O","O"],"target_index":null,"target_label":null},"label_list":["song","event","band","music_genre","country","organization","musical_artist","album","location","award","person","musical_instrument"]}
{"id":"148","dataset":"crossner_music","split":"dev","instance":{"id":"148","prompt_labels":"In(O) the(O) 1990s(O) ,(O) country(B-music genre) music(I-music genre) became(O) a(O) worldwide(O) phenomenon(O) thanks(O) to(O) Garth(B-musical artist) Brooks(I-musical artist) ,(O) Other(O) artists(O) that(O) experienced(O) success(O) during(O) this(O) time(O) included(O) Clint(B-musical artist) Black(I-musical artist) ,(O) Sammy(B-musical artist) Kershaw(I-musical artist) ,(O) Aaron(B-musical artist) Tippin(I-musical artist) ,(O) Travis(B-musical artist) Tritt(I-musical artist) ,(O) Alan(B-musical artist) Jackson(I-musical artist) and(O) the(O) newly(O) formed(O) duo(O) of(O) Brooks(B-band) &(I-band) amp(I-band) ;(I-band) Dunn(I-band) ;(O) George(B-musical artist) Strait(I-musical artist) ,(O) whose(O) career(O) began(O) in(O) the(O) 1980s(O) ,(O) also(O) continued(O) to(O) have(O) widespread(O) success(O) in(O) this(O) decade(O) and(O) beyond(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, location, band, musical instrument, country, musical artist, award, person, music genre, organization, album, event and O.\nSentence: In the 1990s , country music became a worldwide phenomenon thanks to Garth Brooks , Other artists that experienced success during this time included Clint Black , Sammy Kershaw , Aaron Tippin , Travis Tritt , Alan Jackson and the newly formed duo of Brooks & amp ; Dunn ; George Strait , whose career began in the 1980s , also continued to have widespread success in this decade and beyond .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","1990s",",","country","music","became","a","worldwide","phenomenon","thanks","to","Garth","Brooks",",","Other","artists","that","experienced","success","during","this","time","included","Clint","Black",",","Sammy","Kershaw",",","Aaron","Tippin",",","Travis","Tritt",",","Alan","Jackson","and","the","newly","formed","duo","of","Brooks","&","amp",";","Dunn",";","George","Strait",",","whose","career","began","in","the","1980s",",","also","continued","to","have","widespread","success","in","this","decade","and","beyond","."],"labels":["O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O","O","O","B-band","I-band","I-band","I-band","I-band","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["song","location","band","musical_instrument","country","musical_artist","award","person","music_genre","organization","album","event"]}
{"id":"149","dataset":"crossner_music","split":"dev","instance":{"id":"149","prompt_labels":"Sidney(B-musical artist) Simien(I-musical artist) ((O) April(O) 9(O) ,(O) 1938(O) -(O) February(O) 25(O) ,(O) 1998(O) )(O) ,(O) known(O) as(O) Rockin(B-musical artist) '(I-musical artist) Sidney(I-musical artist) and(O) Count(B-musical artist) Rockin(I-musical artist) '(I-musical artist) Sidney(I-musical artist) ,(O) was(O) an(O) American(O) Rhythm(B-music genre) and(I-music genre) blues(I-music genre) ,(O) zydeco(B-music genre) ,(O) and(O) Soul(B-music genre) music(I-music genre) musician(O) who(O) began(O) recording(O) in(O) the(O) late(O) 1950s(O) and(O) continued(O) performing(O) until(O) his(O) death(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, location, event, musical instrument, album, musical artist, song, music genre, award, band, person and O.\nSentence: Sidney Simien ( April 9 , 1938 - February 25 , 1998 ) , known as Rockin ' Sidney and Count Rockin ' Sidney , was an American Rhythm and blues , zydeco , and Soul music musician who began recording in the late 1950s and continued performing until his death .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Sidney","Simien","(","April","9",",","1938","-","February","25",",","1998",")",",","known","as","Rockin","'","Sidney","and","Count","Rockin","'","Sidney",",","was","an","American","Rhythm","and","blues",",","zydeco",",","and","Soul","music","musician","who","began","recording","in","the","late","1950s","and","continued","performing","until","his","death","."],"labels":["B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","I-musical artist","O","O","O","O","B-music genre","I-music genre","I-music genre","O","B-music genre","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","country","location","event","musical_instrument","album","musical_artist","song","music_genre","award","band","person"]}
{"id":"152","dataset":"crossner_music","split":"dev","instance":{"id":"152","prompt_labels":"The(O) Academy(O) sought(O) to(O) promote(O) country(B-music genre) /(I-music genre) western(I-music genre) music(I-music genre) in(O) the(O) western(O) states(O) ;(O) this(O) was(O) in(O) contrast(O) to(O) the(O) Country(B-organization) Music(I-organization) Association(I-organization) ,(O) based(O) in(O) Nashville(B-location) ,(O) Tennessee(B-location) ((O) then(O) the(O) center(O) of(O) the(O) pop-oriented(O) Nashville(B-music genre) sound(I-music genre) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, music genre, musical artist, album, song, award, organization, country, band, location, musical instrument and O.\nSentence: The Academy sought to promote country / western music in the western states ; this was in contrast to the Country Music Association , based in Nashville , Tennessee ( then the center of the pop-oriented Nashville sound ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Academy","sought","to","promote","country","/","western","music","in","the","western","states",";","this","was","in","contrast","to","the","Country","Music","Association",",","based","in","Nashville",",","Tennessee","(","then","the","center","of","the","pop-oriented","Nashville","sound",")","."],"labels":["O","O","O","O","O","B-music genre","I-music genre","I-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","B-location","O","B-location","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["person","event","music_genre","musical_artist","album","song","award","organization","country","band","location","musical_instrument"]}
{"id":"153","dataset":"crossner_music","split":"dev","instance":{"id":"153","prompt_labels":"Muggs(B-musical artist) released(O) Soul(O) Assassins(O) :(O) Chapter(O) 1(O) featuring(O) contributions(O) from(O) Dr.(B-musical artist) Dre(I-musical artist) ,(O) KRS-One(B-musical artist) ,(O) Wyclef(B-musical artist) Jean(I-musical artist) and(O) Mobb(B-band) Deep(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, event, musical instrument, music genre, location, song, award, musical artist, album, organization, person, country and O.\nSentence: Muggs released Soul Assassins : Chapter 1 featuring contributions from Dr. Dre , KRS-One , Wyclef Jean and Mobb Deep .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Muggs","released","Soul","Assassins",":","Chapter","1","featuring","contributions","from","Dr.","Dre",",","KRS-One",",","Wyclef","Jean","and","Mobb","Deep","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","O","B-musical artist","I-musical artist","O","B-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["band","event","musical_instrument","music_genre","location","song","award","musical_artist","album","organization","person","country"]}
{"id":"154","dataset":"crossner_music","split":"dev","instance":{"id":"154","prompt_labels":"In(O) recognition(O) of(O) her(O) film(O) career(O) ,(O) she(O) received(O) BAFTA(B-award) 's(I-award) Lifetime(I-award) Achievement(I-award) Award(I-award) ,(O) the(O) Golden(B-award) Globe(I-award) Cecil(I-award) B.(I-award) DeMille(I-award) Award(I-award) ,(O) the(O) Screen(B-award) Actors(I-award) Guild(I-award) Life(I-award) Achievement(I-award) Award(I-award) ,(O) and(O) the(O) Special(B-award) Tony(I-award) Award(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, person, country, award, song, musical instrument, band, event, music genre, album, organization, location and O.\nSentence: In recognition of her film career , she received BAFTA 's Lifetime Achievement Award , the Golden Globe Cecil B. DeMille Award , the Screen Actors Guild Life Achievement Award , and the Special Tony Award .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","recognition","of","her","film","career",",","she","received","BAFTA","'s","Lifetime","Achievement","Award",",","the","Golden","Globe","Cecil","B.","DeMille","Award",",","the","Screen","Actors","Guild","Life","Achievement","Award",",","and","the","Special","Tony","Award","."],"labels":["O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","B-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","person","country","award","song","musical_instrument","band","event","music_genre","album","organization","location"]}
{"id":"155","dataset":"crossner_music","split":"dev","instance":{"id":"155","prompt_labels":"It(O) also(O) produced(O) the(O) Top(O) 5(O) single(O) Rage(B-song) Hard(I-song) ((O) #(O) 1(O) in(O) Germany(B-country) )(O) ,(O) Top(O) 20(O) single(O) Warriors(B-song) of(I-song) the(I-song) Wasteland(I-song) and(O) Top(O) 30(O) single(O) Watching(B-song) the(I-song) Wildlife(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, organization, country, location, person, music genre, album, musical artist, song, musical instrument, award, event and O.\nSentence: It also produced the Top 5 single Rage Hard ( # 1 in Germany ) , Top 20 single Warriors of the Wasteland and Top 30 single Watching the Wildlife .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","also","produced","the","Top","5","single","Rage","Hard","(","#","1","in","Germany",")",",","Top","20","single","Warriors","of","the","Wasteland","and","Top","30","single","Watching","the","Wildlife","."],"labels":["O","O","O","O","O","O","O","B-song","I-song","O","O","O","O","B-country","O","O","O","O","O","B-song","I-song","I-song","I-song","O","O","O","O","B-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["band","organization","country","location","person","music_genre","album","musical_artist","song","musical_instrument","award","event"]}
{"id":"157","dataset":"crossner_music","split":"dev","instance":{"id":"157","prompt_labels":"In(O) the(O) 2010s(O) ,(O) the(O) alt-country(B-music genre) genre(I-music genre) saw(O) an(O) increase(O) in(O) its(O) critical(O) and(O) commercial(O) popularity(O) ,(O) owing(O) to(O) the(O) success(O) of(O) artists(O) such(O) as(O) The(B-band) Civil(I-band) Wars(I-band) ,(O) Chris(B-musical artist) Stapleton(I-musical artist) ,(O) Sturgill(B-musical artist) Simpson(I-musical artist) ,(O) Jason(B-musical artist) Isbell(I-musical artist) ,(O) Lydia(B-band) Loveless(I-band) and(O) Margo(B-musical artist) Price(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, person, musical instrument, music genre, organization, event, award, album, country, band, location, song and O.\nSentence: In the 2010s , the alt-country genre saw an increase in its critical and commercial popularity , owing to the success of artists such as The Civil Wars , Chris Stapleton , Sturgill Simpson , Jason Isbell , Lydia Loveless and Margo Price .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","2010s",",","the","alt-country","genre","saw","an","increase","in","its","critical","and","commercial","popularity",",","owing","to","the","success","of","artists","such","as","The","Civil","Wars",",","Chris","Stapleton",",","Sturgill","Simpson",",","Jason","Isbell",",","Lydia","Loveless","and","Margo","Price","."],"labels":["O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","I-band","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-band","I-band","O","B-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","person","musical_instrument","music_genre","organization","event","award","album","country","band","location","song"]}
{"id":"158","dataset":"crossner_music","split":"dev","instance":{"id":"158","prompt_labels":"At(O) the(O) 51st(B-award) Academy(I-award) Awards(I-award) ,(O) it(O) was(O) nominated(O) for(O) nine(O) Academy(B-award) Awards(I-award) ,(O) and(O) won(O) five(O) :(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) for(O) Cimino(B-person) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Supporting(I-award) Actor(I-award) for(O) Walken(B-person) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Sound(I-award) Mixing(I-award) ,(O) and(O) Best(B-award) Film(I-award) Editing(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, musical artist, music genre, event, person, album, award, band, musical instrument, location, song, country and O.\nSentence: At the 51st Academy Awards , it was nominated for nine Academy Awards , and won five : Academy Award for Best Picture , Academy Award for Best Director for Cimino , Academy Award for Best Supporting Actor for Walken , Academy Award for Best Sound Mixing , and Best Film Editing .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["At","the","51st","Academy","Awards",",","it","was","nominated","for","nine","Academy","Awards",",","and","won","five",":","Academy","Award","for","Best","Picture",",","Academy","Award","for","Best","Director","for","Cimino",",","Academy","Award","for","Best","Supporting","Actor","for","Walken",",","Academy","Award","for","Best","Sound","Mixing",",","and","Best","Film","Editing","."],"labels":["O","O","B-award","I-award","I-award","O","O","O","O","O","O","B-award","I-award","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","B-person","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-person","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["organization","musical_artist","music_genre","event","person","album","award","band","musical_instrument","location","song","country"]}
{"id":"159","dataset":"crossner_music","split":"dev","instance":{"id":"159","prompt_labels":"Biafra(B-musical artist) was(O) also(O) working(O) with(O) a(O) band(O) known(O) as(O) Jello(B-band) Biafra(I-band) and(I-band) the(I-band) Guantanamo(I-band) School(I-band) of(I-band) Medicine(I-band) ,(O) which(O) included(O) Ralph(B-musical artist) Spight(I-musical artist) of(O) Victims(B-band) Family(I-band) on(O) guitar(B-musical instrument) and(O) Billy(B-musical artist) Gould(I-musical artist) of(O) Faith(B-band) No(I-band) More(I-band) on(O) bass(B-musical instrument) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, song, person, musical artist, event, location, organization, album, music genre, country, band, musical instrument and O.\nSentence: Biafra was also working with a band known as Jello Biafra and the Guantanamo School of Medicine , which included Ralph Spight of Victims Family on guitar and Billy Gould of Faith No More on bass .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Biafra","was","also","working","with","a","band","known","as","Jello","Biafra","and","the","Guantanamo","School","of","Medicine",",","which","included","Ralph","Spight","of","Victims","Family","on","guitar","and","Billy","Gould","of","Faith","No","More","on","bass","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","B-band","I-band","I-band","I-band","I-band","I-band","I-band","I-band","O","O","O","B-musical artist","I-musical artist","O","B-band","I-band","O","B-musical instrument","O","B-musical artist","I-musical artist","O","B-band","I-band","I-band","O","B-musical instrument","O"],"target_index":null,"target_label":null},"label_list":["award","song","person","musical_artist","event","location","organization","album","music_genre","country","band","musical_instrument"]}
{"id":"161","dataset":"crossner_music","split":"dev","instance":{"id":"161","prompt_labels":"The(O) three(O) venues(O) regarded(O) as(O) the(O) most(O) important(O) in(O) this(O) decade(O) were(O) the(O) Golden(B-location) Torch(I-location) in(O) Tunstall(B-location) ,(O) Stoke-on-Trent(B-location) ((O) 1971(O) to(O) 1972(O) )(O) ,(O) Blackpool(B-location) Mecca(I-location) ((O) 1971(O) to(O) 1979(O) )(O) and(O) Wigan(B-location) Casino(I-location) ((O) 1973(O) to(O) 1981(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, music genre, musical artist, person, band, musical instrument, album, song, country, organization, location and O.\nSentence: The three venues regarded as the most important in this decade were the Golden Torch in Tunstall , Stoke-on-Trent ( 1971 to 1972 ) , Blackpool Mecca ( 1971 to 1979 ) and Wigan Casino ( 1973 to 1981 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","three","venues","regarded","as","the","most","important","in","this","decade","were","the","Golden","Torch","in","Tunstall",",","Stoke-on-Trent","(","1971","to","1972",")",",","Blackpool","Mecca","(","1971","to","1979",")","and","Wigan","Casino","(","1973","to","1981",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","O","B-location","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","award","music_genre","musical_artist","person","band","musical_instrument","album","song","country","organization","location"]}
{"id":"163","dataset":"crossner_music","split":"dev","instance":{"id":"163","prompt_labels":"Minogue(B-musical artist) has(O) sold(O) 70(O) million(O) records(O) worldwide(O) and(O) has(O) earned(O) numerous(O) awards(O) and(O) accolades(O) ,(O) including(O) a(O) Grammy(B-award) Award(I-award) ,(O) three(O) Brit(B-award) Awards(I-award) ,(O) 17(O) ARIA(B-award) Music(I-award) Awards(I-award) ,(O) two(O) MTV(B-award) Europe(I-award) Music(I-award) Award(I-award) and(O) two(O) MTV(B-award) Video(I-award) Music(I-award) Award(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, location, album, organization, musical artist, event, award, band, music genre, song, country, person and O.\nSentence: Minogue has sold 70 million records worldwide and has earned numerous awards and accolades , including a Grammy Award , three Brit Awards , 17 ARIA Music Awards , two MTV Europe Music Award and two MTV Video Music Award .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Minogue","has","sold","70","million","records","worldwide","and","has","earned","numerous","awards","and","accolades",",","including","a","Grammy","Award",",","three","Brit","Awards",",","17","ARIA","Music","Awards",",","two","MTV","Europe","Music","Award","and","two","MTV","Video","Music","Award","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","B-award","I-award","O","O","B-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","location","album","organization","musical_artist","event","award","band","music_genre","song","country","person"]}
{"id":"164","dataset":"crossner_music","split":"dev","instance":{"id":"164","prompt_labels":"The(O) festival(O) features(O) bands(O) from(O) different(O) places(O) in(O) Argentina(B-country) ,(O) as(O) well(O) as(O) international(O) artists(O) from(O) Brazil(B-country) ,(O) Uruguay(B-country) ,(O) Chile(B-country) ,(O) Peru(B-country) and(O) the(O) United(B-country) States(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, band, album, person, location, song, musical instrument, musical artist, music genre, award, event and O.\nSentence: The festival features bands from different places in Argentina , as well as international artists from Brazil , Uruguay , Chile , Peru and the United States .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","festival","features","bands","from","different","places","in","Argentina",",","as","well","as","international","artists","from","Brazil",",","Uruguay",",","Chile",",","Peru","and","the","United","States","."],"labels":["O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","B-country","O","B-country","O","B-country","O","B-country","O","O","B-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["organization","country","band","album","person","location","song","musical_instrument","musical_artist","music_genre","award","event"]}
{"id":"170","dataset":"crossner_music","split":"dev","instance":{"id":"170","prompt_labels":"Some(O) of(O) the(O) early(O) stars(O) on(O) the(O) Opry(B-location) were(O) Uncle(B-musical artist) Dave(I-musical artist) Macon(I-musical artist) ,(O) Roy(B-musical artist) Acuff(I-musical artist) and(O) African(O) American(O) harmonica(B-musical instrument) player(O) DeFord(B-musical artist) Bailey(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, country, location, song, musical artist, award, band, music genre, organization, person, musical instrument, event and O.\nSentence: Some of the early stars on the Opry were Uncle Dave Macon , Roy Acuff and African American harmonica player DeFord Bailey .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","of","the","early","stars","on","the","Opry","were","Uncle","Dave","Macon",",","Roy","Acuff","and","African","American","harmonica","player","DeFord","Bailey","."],"labels":["O","O","O","O","O","O","O","B-location","O","B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","B-musical instrument","O","B-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["album","country","location","song","musical_artist","award","band","music_genre","organization","person","musical_instrument","event"]}
{"id":"172","dataset":"crossner_music","split":"dev","instance":{"id":"172","prompt_labels":"Bands(O) like(O) Flogging(B-band) Molly(I-band) ,(O) Black(B-band) 47(I-band) ,(O) Dropkick(B-band) Murphys(I-band) ,(O) The(B-band) Young(I-band) Dubliners(I-band) ,(O) The(B-band) Tossers(I-band) introduced(O) a(O) hybrid(O) of(O) Celtic(B-music genre) rock(I-music genre) ,(O) Punk(B-music genre) rock(I-music genre) ,(O) reggae(B-music genre) ,(O) Hardcore(B-music genre) punk(I-music genre) and(O) other(O) elements(O) in(O) the(O) 1990s(O) that(O) has(O) become(O) popular(O) with(O) Irish-American(O) youth(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, country, event, musical instrument, person, band, album, song, location, organization, music genre, award and O.\nSentence: Bands like Flogging Molly , Black 47 , Dropkick Murphys , The Young Dubliners , The Tossers introduced a hybrid of Celtic rock , Punk rock , reggae , Hardcore punk and other elements in the 1990s that has become popular with Irish-American youth .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Bands","like","Flogging","Molly",",","Black","47",",","Dropkick","Murphys",",","The","Young","Dubliners",",","The","Tossers","introduced","a","hybrid","of","Celtic","rock",",","Punk","rock",",","reggae",",","Hardcore","punk","and","other","elements","in","the","1990s","that","has","become","popular","with","Irish-American","youth","."],"labels":["O","O","B-band","I-band","O","B-band","I-band","O","B-band","I-band","O","B-band","I-band","I-band","O","B-band","I-band","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","country","event","musical_instrument","person","band","album","song","location","organization","music_genre","award"]}
{"id":"173","dataset":"crossner_music","split":"dev","instance":{"id":"173","prompt_labels":"Their(O) eighth(O) album(O) ,(O) Hesitation(B-album) Marks(I-album) ((O) 2013(O) )(O) ,(O) was(O) followed(O) by(O) a(O) trilogy(O) consisting(O) of(O) the(O) EPs(O) Not(B-album) the(I-album) Actual(I-album) Events(I-album) ((O) 2016(O) )(O) and(O) Add(B-album) Violence(I-album) ((O) 2017(O) )(O) and(O) their(O) ninth(O) album(O) Bad(B-album) Witch(I-album) ((O) 2018(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, music genre, musical artist, band, event, song, location, album, award, country, musical instrument, person and O.\nSentence: Their eighth album , Hesitation Marks ( 2013 ) , was followed by a trilogy consisting of the EPs Not the Actual Events ( 2016 ) and Add Violence ( 2017 ) and their ninth album Bad Witch ( 2018 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Their","eighth","album",",","Hesitation","Marks","(","2013",")",",","was","followed","by","a","trilogy","consisting","of","the","EPs","Not","the","Actual","Events","(","2016",")","and","Add","Violence","(","2017",")","and","their","ninth","album","Bad","Witch","(","2018",")","."],"labels":["O","O","O","O","B-album","I-album","O","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","B-album","I-album","O","O","O","O","O","O","O","B-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","music_genre","musical_artist","band","event","song","location","album","award","country","musical_instrument","person"]}
{"id":"174","dataset":"crossner_music","split":"dev","instance":{"id":"174","prompt_labels":"He(O) has(O) appeared(O) as(O) a(O) featured(O) artist(O) on(O) many(O) other(O) songs(O) and(O) albums(O) ,(O) having(O) collaborated(O) with(O) artists(O) such(O) as(O) Janet(B-musical artist) Jackson(I-musical artist) ,(O) Kool(B-musical artist) Moe(I-musical artist) Dee(I-musical artist) ,(O) The(B-band) Dope(I-band) Poet(I-band) Society(I-band) ,(O) Run-D.M.C.(B-band) ,(O) Ice(B-musical artist) Cube(I-musical artist) ,(O) Boom(B-band) Boom(I-band) Satellites(I-band) ,(O) Rage(B-band) Against(I-band) the(I-band) Machine(I-band) ,(O) Anthrax(B-band) ,(O) John(B-musical artist) Mellencamp(I-musical artist) and(O) many(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, album, country, person, music genre, location, organization, song, award, musical artist, musical instrument, band and O.\nSentence: He has appeared as a featured artist on many other songs and albums , having collaborated with artists such as Janet Jackson , Kool Moe Dee , The Dope Poet Society , Run-D.M.C. , Ice Cube , Boom Boom Satellites , Rage Against the Machine , Anthrax , John Mellencamp and many others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","has","appeared","as","a","featured","artist","on","many","other","songs","and","albums",",","having","collaborated","with","artists","such","as","Janet","Jackson",",","Kool","Moe","Dee",",","The","Dope","Poet","Society",",","Run-D.M.C.",",","Ice","Cube",",","Boom","Boom","Satellites",",","Rage","Against","the","Machine",",","Anthrax",",","John","Mellencamp","and","many","others","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","B-band","I-band","I-band","I-band","O","B-band","O","B-musical artist","I-musical artist","O","B-band","I-band","I-band","O","B-band","I-band","I-band","I-band","O","B-band","O","B-musical artist","I-musical artist","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","album","country","person","music_genre","location","organization","song","award","musical_artist","musical_instrument","band"]}
{"id":"176","dataset":"crossner_music","split":"dev","instance":{"id":"176","prompt_labels":"Besides(O) fronting(O) his(O) own(O) band(O) and(O) rap(B-music genre) projects(O) ,(O) Ice-T(B-musical artist) has(O) also(O) collaborated(O) with(O) other(O) hard(B-music genre) rock(I-music genre) and(O) metal(B-music genre) bands(O) ,(O) such(O) as(O) Icepick(B-band) ,(O) Motrhead(B-band) ,(O) Slayer(B-band) ,(O) Pro-Pain(B-band) ,(O) and(O) Six(B-band) Feet(I-band) Under(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, band, award, musical artist, album, organization, musical instrument, person, country, location, music genre, song and O.\nSentence: Besides fronting his own band and rap projects , Ice-T has also collaborated with other hard rock and metal bands , such as Icepick , Motrhead , Slayer , Pro-Pain , and Six Feet Under .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Besides","fronting","his","own","band","and","rap","projects",",","Ice-T","has","also","collaborated","with","other","hard","rock","and","metal","bands",",","such","as","Icepick",",","Motrhead",",","Slayer",",","Pro-Pain",",","and","Six","Feet","Under","."],"labels":["O","O","O","O","O","O","B-music genre","O","O","B-musical artist","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","O","O","O","O","B-band","O","B-band","O","B-band","O","B-band","O","O","B-band","I-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["event","band","award","musical_artist","album","organization","musical_instrument","person","country","location","music_genre","song"]}
{"id":"180","dataset":"crossner_music","split":"dev","instance":{"id":"180","prompt_labels":"Alexander(B-musical artist) von(I-musical artist) Meilenwald(I-musical artist) from(O) German(B-country) band(O) Nagelfar(B-band) considers(O) Ungod(B-band) '(O) s(O) 1993(O) debut(O) Circle(B-album) of(I-album) the(I-album) Seven(I-album) Infernal(I-album) Pacts(I-album) ,(O) Desaster(B-band) '(O) s(O) 1994(O) demo(O) Lost(B-album) in(I-album) the(I-album) Ages(I-album) ,(O) Tha-Norr(B-band) '(O) s(O) 1995(O) album(O) Wolfenzeitalter(B-album) ,(O) Lunar(B-band) Aurora(I-band) '(O) s(O) 1996(O) debut(O) Weltengnger(B-album) and(O) Katharsis(B-band) '(O) s(O) 2000(O) debut(O) 666(B-album) Alexander(B-musical artist) von(I-musical artist) Meilenwald(I-musical artist) :(O) 5(O) Klassiker(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, band, country, musical instrument, song, location, event, person, album, award, musical artist, organization and O.\nSentence: Alexander von Meilenwald from German band Nagelfar considers Ungod ' s 1993 debut Circle of the Seven Infernal Pacts , Desaster ' s 1994 demo Lost in the Ages , Tha-Norr ' s 1995 album Wolfenzeitalter , Lunar Aurora ' s 1996 debut Weltengnger and Katharsis ' s 2000 debut 666 Alexander von Meilenwald : 5 Klassiker .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Alexander","von","Meilenwald","from","German","band","Nagelfar","considers","Ungod","'","s","1993","debut","Circle","of","the","Seven","Infernal","Pacts",",","Desaster","'","s","1994","demo","Lost","in","the","Ages",",","Tha-Norr","'","s","1995","album","Wolfenzeitalter",",","Lunar","Aurora","'","s","1996","debut","Weltengnger","and","Katharsis","'","s","2000","debut","666","Alexander","von","Meilenwald",":","5","Klassiker","."],"labels":["B-musical artist","I-musical artist","I-musical artist","O","B-country","O","B-band","O","B-band","O","O","O","O","B-album","I-album","I-album","I-album","I-album","I-album","O","B-band","O","O","O","O","B-album","I-album","I-album","I-album","O","B-band","O","O","O","O","B-album","O","B-band","I-band","O","O","O","O","B-album","O","B-band","O","O","O","O","B-album","B-musical artist","I-musical artist","I-musical artist","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["music_genre","band","country","musical_instrument","song","location","event","person","album","award","musical_artist","organization"]}
{"id":"183","dataset":"crossner_music","split":"dev","instance":{"id":"183","prompt_labels":"In(O) February(O) 1984(O) ,(O) Queen(B-band) released(O) their(O) eleventh(O) studio(O) album(O) ,(O) The(B-album) Works(I-album) ,(O) which(O) included(O) the(O) successful(O) singles(O) Radio(B-song) Ga(I-song) Ga(I-song) ,(O) Hammer(B-song) to(I-song) Fall(I-song) and(O) I(B-song) Want(I-song) to(I-song) Break(I-song) Free(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, song, album, organization, band, award, person, music genre, musical artist, musical instrument, country and O.\nSentence: In February 1984 , Queen released their eleventh studio album , The Works , which included the successful singles Radio Ga Ga , Hammer to Fall and I Want to Break Free .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","February","1984",",","Queen","released","their","eleventh","studio","album",",","The","Works",",","which","included","the","successful","singles","Radio","Ga","Ga",",","Hammer","to","Fall","and","I","Want","to","Break","Free","."],"labels":["O","O","O","O","B-band","O","O","O","O","O","O","B-album","I-album","O","O","O","O","O","O","B-song","I-song","I-song","O","B-song","I-song","I-song","O","B-song","I-song","I-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["location","event","song","album","organization","band","award","person","music_genre","musical_artist","musical_instrument","country"]}
{"id":"187","dataset":"crossner_music","split":"dev","instance":{"id":"187","prompt_labels":"Fredriksson(B-musical artist) returned(O) in(O) 2006(O) with(O) an(O) album(O) of(O) Swedish(O) cover(O) songs(O) ,(O) titled(O) Min(B-album) bste(I-album) vn(I-album) ((O) My(B-album) Best(I-album) Friend(I-album) )(O) ,(O) while(O) Gessle(B-musical artist) recorded(O) two(O) more(O) solo(O) albums(O) ,(O) En(B-album) hndig(I-album) man(I-album) ((O) A(B-album) Handy(I-album) Man(I-album) )(O) ((O) 2007(O) )(O) and(O) Party(B-album) Crasher(I-album) ((O) 2008(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, song, music genre, award, album, musical instrument, event, country, band, musical artist, location, organization and O.\nSentence: Fredriksson returned in 2006 with an album of Swedish cover songs , titled Min bste vn ( My Best Friend ) , while Gessle recorded two more solo albums , En hndig man ( A Handy Man ) ( 2007 ) and Party Crasher ( 2008 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Fredriksson","returned","in","2006","with","an","album","of","Swedish","cover","songs",",","titled","Min","bste","vn","(","My","Best","Friend",")",",","while","Gessle","recorded","two","more","solo","albums",",","En","hndig","man","(","A","Handy","Man",")","(","2007",")","and","Party","Crasher","(","2008",")","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O","B-album","I-album","I-album","O","O","O","B-musical artist","O","O","O","O","O","O","B-album","I-album","I-album","O","B-album","I-album","I-album","O","O","O","O","O","B-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","song","music_genre","award","album","musical_instrument","event","country","band","musical_artist","location","organization"]}
{"id":"188","dataset":"crossner_music","split":"dev","instance":{"id":"188","prompt_labels":"They(O) were(O) inducted(O) by(O) Chuck(B-musical artist) D(I-musical artist) and(O) LL(B-musical artist) Cool(I-musical artist) J(I-musical artist) on(O) April(O) 14(O) ,(O) 2012(O) therefore(O) the(O) group(O) didn(O) 't(O) perform(O) ;(O) instead(O) Black(B-musical artist) Thought(I-musical artist) ,(O) Travie(B-musical artist) from(O) Gym(B-band) Class(I-band) Heroes(I-band) and(O) Kid(B-musical artist) Rock(I-musical artist) performed(O) a(O) medley(O) of(O) their(O) songs(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, person, organization, album, song, musical artist, event, music genre, location, award, country, musical instrument and O.\nSentence: They were inducted by Chuck D and LL Cool J on April 14 , 2012 therefore the group didn 't perform ; instead Black Thought , Travie from Gym Class Heroes and Kid Rock performed a medley of their songs .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["They","were","inducted","by","Chuck","D","and","LL","Cool","J","on","April","14",",","2012","therefore","the","group","didn","'t","perform",";","instead","Black","Thought",",","Travie","from","Gym","Class","Heroes","and","Kid","Rock","performed","a","medley","of","their","songs","."],"labels":["O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","O","B-band","I-band","I-band","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["band","person","organization","album","song","musical_artist","event","music_genre","location","award","country","musical_instrument"]}
{"id":"189","dataset":"crossner_music","split":"dev","instance":{"id":"189","prompt_labels":"Following(O) in(O) the(O) footsteps(O) of(O) Gene(B-musical artist) Autry(I-musical artist) ,(O) Lydia(B-musical artist) Mendoza(I-musical artist) ,(O) Roy(B-person) Rogers(I-person) ,(O) and(O) Patsy(B-musical artist) Montana(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, award, person, musical artist, album, band, country, musical instrument, song, event, music genre and O.\nSentence: Following in the footsteps of Gene Autry , Lydia Mendoza , Roy Rogers , and Patsy Montana .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Following","in","the","footsteps","of","Gene","Autry",",","Lydia","Mendoza",",","Roy","Rogers",",","and","Patsy","Montana","."],"labels":["O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-person","I-person","O","O","B-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["location","organization","award","person","musical_artist","album","band","country","musical_instrument","song","event","music_genre"]}
{"id":"190","dataset":"crossner_music","split":"dev","instance":{"id":"190","prompt_labels":"It(O) was(O) nominated(O) for(O) four(O) Academy(B-award) Awards(I-award) ,(O) including(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) ((O) De(B-person) Niro(I-person) )(O) ,(O) and(O) received(O) the(O) Palme(B-award) d(I-award) 'Or(I-award) at(O) the(O) 1976(B-event) Cannes(I-event) Film(I-event) Festival(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, person, musical instrument, musical artist, song, award, organization, location, event, album, country, music genre and O.\nSentence: It was nominated for four Academy Awards , including Academy Award for Best Picture and Academy Award for Best Actor ( De Niro ) , and received the Palme d 'Or at the 1976 Cannes Film Festival .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","nominated","for","four","Academy","Awards",",","including","Academy","Award","for","Best","Picture","and","Academy","Award","for","Best","Actor","(","De","Niro",")",",","and","received","the","Palme","d","'Or","at","the","1976","Cannes","Film","Festival","."],"labels":["O","O","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","B-person","I-person","O","O","O","O","O","B-award","I-award","I-award","O","O","B-event","I-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["band","person","musical_instrument","musical_artist","song","award","organization","location","event","album","country","music_genre"]}
{"id":"191","dataset":"crossner_music","split":"dev","instance":{"id":"191","prompt_labels":"It(O) was(O) designed(O) by(O) Kenzo(B-person) Tange(I-person) and(O) built(O) between(O) 1961(O) and(O) 1964(O) to(O) house(O) Swimming(O) at(O) the(O) 1964(B-event) Summer(I-event) Olympics(I-event) and(O) Diving(O) at(O) the(O) 1964(B-event) Summer(I-event) Olympics(I-event) events(O) in(O) the(O) 1964(B-event) Summer(I-event) Olympics(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, album, event, country, song, band, organization, musical instrument, award, musical artist, music genre, person and O.\nSentence: It was designed by Kenzo Tange and built between 1961 and 1964 to house Swimming at the 1964 Summer Olympics and Diving at the 1964 Summer Olympics events in the 1964 Summer Olympics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","designed","by","Kenzo","Tange","and","built","between","1961","and","1964","to","house","Swimming","at","the","1964","Summer","Olympics","and","Diving","at","the","1964","Summer","Olympics","events","in","the","1964","Summer","Olympics","."],"labels":["O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","B-event","I-event","I-event","O","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["location","album","event","country","song","band","organization","musical_instrument","award","musical_artist","music_genre","person"]}
{"id":"194","dataset":"crossner_music","split":"dev","instance":{"id":"194","prompt_labels":"He(O) was(O) the(O) guitarist(O) for(O) the(O) 1980s(O) Hi-NRG(B-music genre) ,(O) Synth-pop(B-music genre) band(O) ,(O) Frankie(B-band) Goes(I-band) to(I-band) Hollywood(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, band, country, song, location, award, person, musical artist, event, music genre, musical instrument, organization and O.\nSentence: He was the guitarist for the 1980s Hi-NRG , Synth-pop band , Frankie Goes to Hollywood .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","the","guitarist","for","the","1980s","Hi-NRG",",","Synth-pop","band",",","Frankie","Goes","to","Hollywood","."],"labels":["O","O","O","O","O","O","O","B-music genre","O","B-music genre","O","O","B-band","I-band","I-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["album","band","country","song","location","award","person","musical_artist","event","music_genre","musical_instrument","organization"]}
{"id":"196","dataset":"crossner_music","split":"dev","instance":{"id":"196","prompt_labels":"In(O) July(O) 2010(O) ,(O) Dayne(B-musical artist) released(O) Facing(B-song) a(I-song) Miracle(I-song) ,(O) the(O) official(O) theme(O) song(O) to(O) the(O) 2010(B-event) Gay(I-event) Games(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, event, music genre, musical instrument, person, organization, song, location, band, album, country, award and O.\nSentence: In July 2010 , Dayne released Facing a Miracle , the official theme song to the 2010 Gay Games .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","July","2010",",","Dayne","released","Facing","a","Miracle",",","the","official","theme","song","to","the","2010","Gay","Games","."],"labels":["O","O","O","O","B-musical artist","O","B-song","I-song","I-song","O","O","O","O","O","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","event","music_genre","musical_instrument","person","organization","song","location","band","album","country","award"]}
{"id":"0","dataset":"crossner_politics","split":"dev","instance":{"id":"0","prompt_labels":"At(O) the(O) 2001(B-election) Italian(I-election) general(I-election) election(I-election) the(O) Greens(O) formed(O) a(O) joint(O) list(O) with(O) the(O) Italian(B-political party) Democratic(I-political party) Socialists(I-political party) ((O) SDI(B-political party) )(O) :(O) The(B-political party) Sunflower(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, election, person, politician, political party, organization, country and O.\nSentence: At the 2001 Italian general election the Greens formed a joint list with the Italian Democratic Socialists ( SDI ) : The Sunflower .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["At","the","2001","Italian","general","election","the","Greens","formed","a","joint","list","with","the","Italian","Democratic","Socialists","(","SDI",")",":","The","Sunflower","."],"labels":["O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","B-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["location","event","election","person","politician","political_party","organization","country"]}
{"id":"1","dataset":"crossner_politics","split":"dev","instance":{"id":"1","prompt_labels":"For(O) the(O) 2009(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) Italy(I-election) the(O) Greens(O) formed(O) a(O) joint(O) list(O) with(O) the(O) Movement(B-political party) for(I-political party) the(I-political party) Left(I-political party) ((O) MpS(B-political party) )(O) -(O) a(O) moderate(O) split(O) from(O) the(O) PRC(B-political party) -(O) ,(O) the(O) Socialist(B-political party) Party(I-political party) ((O) PS(B-political party) )(O) -(O) successor(O) of(O) the(O) SDI(B-political party) -(O) ,(O) SD(B-political party) and(O) Unite(B-political party) the(I-political party) Left(I-political party) ((O) UlS(B-political party) )(O) :(O) Left(B-political party) Ecology(I-political party) Freedom(I-political party) ((O) SL(B-political party) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, person, political party, politician, organization, election, event and O.\nSentence: For the 2009 European Parliament election in Italy the Greens formed a joint list with the Movement for the Left ( MpS ) - a moderate split from the PRC - , the Socialist Party ( PS ) - successor of the SDI - , SD and Unite the Left ( UlS ) : Left Ecology Freedom ( SL ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","the","2009","European","Parliament","election","in","Italy","the","Greens","formed","a","joint","list","with","the","Movement","for","the","Left","(","MpS",")","-","a","moderate","split","from","the","PRC","-",",","the","Socialist","Party","(","PS",")","-","successor","of","the","SDI","-",",","SD","and","Unite","the","Left","(","UlS",")",":","Left","Ecology","Freedom","(","SL",")","."],"labels":["O","O","B-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","B-political party","O","O","O","B-political party","I-political party","O","B-political party","O","O","O","O","O","B-political party","O","O","B-political party","O","B-political party","I-political party","I-political party","O","B-political party","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O"],"target_index":null,"target_label":null},"label_list":["country","location","person","political_party","politician","organization","election","event"]}
{"id":"2","dataset":"crossner_politics","split":"dev","instance":{"id":"2","prompt_labels":"Sitting(O) as(O) a(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) Member(O) of(O) Parliament(B-organization) ((O) MP(O) )(O) for(O) Niagara(O) Falls(O) ,(O) she(O) joined(O) the(O) Canadian(O) Cabinet(O) after(O) the(O) Liberals(O) defeated(O) the(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) government(O) of(O) John(B-politician) Diefenbaker(I-politician) in(O) the(O) 1963(B-election) Canadian(I-election) federal(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, political party, country, person, event, organization, politician, election and O.\nSentence: Sitting as a Liberal Party of Canada Member of Parliament ( MP ) for Niagara Falls , she joined the Canadian Cabinet after the Liberals defeated the Progressive Conservative Party of Canada government of John Diefenbaker in the 1963 Canadian federal election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Sitting","as","a","Liberal","Party","of","Canada","Member","of","Parliament","(","MP",")","for","Niagara","Falls",",","she","joined","the","Canadian","Cabinet","after","the","Liberals","defeated","the","Progressive","Conservative","Party","of","Canada","government","of","John","Diefenbaker","in","the","1963","Canadian","federal","election","."],"labels":["O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-politician","I-politician","O","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["location","political_party","country","person","event","organization","politician","election"]}
{"id":"6","dataset":"crossner_politics","split":"dev","instance":{"id":"6","prompt_labels":"Their(O) destinies(O) are(O) all(O) altered(O) and(O) shaped(O) by(O) the(O) historical(O) event(O) ((O) for(O) example(O) :(O) Simard(B-politician) becomes(O) a(O) sovereigntist(O) and(O) will(O) leave(O) the(O) Quebec(B-political party) Liberal(I-political party) Party(I-political party) for(O) the(O) Parti(B-political party) Qubcois(I-political party) ;(O) Dumont(B-politician) will(O) also(O) slam(O) the(O) Liberal(O) door(O) to(O) later(O) help(O) create(O) and(O) finally(O) become(O) leader(O) of(O) the(O) Action(B-political party) dmocratique(I-political party) du(I-political party) Qubec(I-political party) ,(O) or(O) ADQ(B-political party) ,(O) and(O) support(O) the(O) Yes(O) side(O) of(O) the(O) 1995(B-event) referendum(I-event) on(O) independence(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, political party, politician, organization, election, person, event and O.\nSentence: Their destinies are all altered and shaped by the historical event ( for example : Simard becomes a sovereigntist and will leave the Quebec Liberal Party for the Parti Qubcois ; Dumont will also slam the Liberal door to later help create and finally become leader of the Action dmocratique du Qubec , or ADQ , and support the Yes side of the 1995 referendum on independence ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Their","destinies","are","all","altered","and","shaped","by","the","historical","event","(","for","example",":","Simard","becomes","a","sovereigntist","and","will","leave","the","Quebec","Liberal","Party","for","the","Parti","Qubcois",";","Dumont","will","also","slam","the","Liberal","door","to","later","help","create","and","finally","become","leader","of","the","Action","dmocratique","du","Qubec",",","or","ADQ",",","and","support","the","Yes","side","of","the","1995","referendum","on","independence",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","O","B-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","O","O","O","O","O","O","O","O","B-event","I-event","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","country","political_party","politician","organization","election","person","event"]}
{"id":"7","dataset":"crossner_politics","split":"dev","instance":{"id":"7","prompt_labels":"Of(O) the(O) current(O) first(O) ministers(O) ,(O) four(O) are(O) from(O) a(O) Liberal(B-political party) Party(I-political party) ,(O) four(O) are(O) from(O) a(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) ,(O) and(O) one(O) is(O) from(O) a(O) New(B-political party) Democratic(I-political party) Party(I-political party) ;(O) three(O) others(O) are(O) from(O) local(O) parties(O) ((O) the(O) Coalition(B-political party) Avenir(I-political party) Qubec(I-political party) ,(O) the(O) Saskatchewan(B-political party) Party(I-political party) ,(O) and(O) the(O) United(B-political party) Conservative(I-political party) Party(I-political party) )(O) and(O) two(O) are(O) non-partisan(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, country, politician, election, location, event, organization and O.\nSentence: Of the current first ministers , four are from a Liberal Party , four are from a Progressive Conservative Party , and one is from a New Democratic Party ; three others are from local parties ( the Coalition Avenir Qubec , the Saskatchewan Party , and the United Conservative Party ) and two are non-partisan .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Of","the","current","first","ministers",",","four","are","from","a","Liberal","Party",",","four","are","from","a","Progressive","Conservative","Party",",","and","one","is","from","a","New","Democratic","Party",";","three","others","are","from","local","parties","(","the","Coalition","Avenir","Qubec",",","the","Saskatchewan","Party",",","and","the","United","Conservative","Party",")","and","two","are","non-partisan","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","political_party","country","politician","election","location","event","organization"]}
{"id":"9","dataset":"crossner_politics","split":"dev","instance":{"id":"9","prompt_labels":"He(O) stood(O) for(O) the(O) Green(B-political party) party(I-political party) in(O) Oxford(B-organization) West(I-organization) and(I-organization) Abingdon(I-organization) in(O) the(O) 1992(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) 1997(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) and(O) 2001(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) general(O) elections(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, organization, politician, event, location, person, country and O.\nSentence: He stood for the Green party in Oxford West and Abingdon in the 1992 United Kingdom general election , 1997 United Kingdom general election , and 2001 United Kingdom general election general elections .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","stood","for","the","Green","party","in","Oxford","West","and","Abingdon","in","the","1992","United","Kingdom","general","election",",","1997","United","Kingdom","general","election",",","and","2001","United","Kingdom","general","election","general","elections","."],"labels":["O","O","O","O","B-political party","I-political party","O","B-organization","I-organization","I-organization","I-organization","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","political_party","organization","politician","event","location","person","country"]}
{"id":"13","dataset":"crossner_politics","split":"dev","instance":{"id":"13","prompt_labels":"In(O) Australia(B-country) ,(O) a(O) number(O) of(O) single(O) issue(O) parties(O) have(O) been(O) elected(O) to(O) federal(O) and(O) state(O) parliaments(O) such(O) as(O) the(O) Animal(B-political party) Justice(I-political party) Party(I-political party) ,(O) Dignity(B-political party) for(I-political party) Disability(I-political party) ,(O) Australian(B-political party) Motoring(I-political party) Enthusiast(I-political party) Party(I-political party) and(O) the(O) Australian(B-political party) Sex(I-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, organization, location, political party, election, country, event and O.\nSentence: In Australia , a number of single issue parties have been elected to federal and state parliaments such as the Animal Justice Party , Dignity for Disability , Australian Motoring Enthusiast Party and the Australian Sex Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","Australia",",","a","number","of","single","issue","parties","have","been","elected","to","federal","and","state","parliaments","such","as","the","Animal","Justice","Party",",","Dignity","for","Disability",",","Australian","Motoring","Enthusiast","Party","and","the","Australian","Sex","Party","."],"labels":["O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["person","politician","organization","location","political_party","election","country","event"]}
{"id":"14","dataset":"crossner_politics","split":"dev","instance":{"id":"14","prompt_labels":"In(O) the(O) United(B-country) States(I-country) ,(O) such(O) voter(O) turnout(O) organizations(O) include(O) the(O) League(B-organization) of(I-organization) Women(I-organization) Voters(I-organization) ,(O) Rock(B-organization) the(I-organization) Vote(I-organization) ,(O) The(B-organization) Voter(I-organization) Participation(I-organization) Center(I-organization) and(O) Vote.org(O) ,(O) which(O) attempt(O) to(O) motivate(O) potential(O) voters(O) to(O) register(O) and(O) to(O) vote(O) in(O) the(O) belief(O) that(O) failure(O) of(O) any(O) eligible(O) voter(O) to(O) vote(O) in(O) any(O) election(O) is(O) a(O) loss(O) to(O) society(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, country, politician, political party, person, event, organization, location and O.\nSentence: In the United States , such voter turnout organizations include the League of Women Voters , Rock the Vote , The Voter Participation Center and Vote.org , which attempt to motivate potential voters to register and to vote in the belief that failure of any eligible voter to vote in any election is a loss to society .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","United","States",",","such","voter","turnout","organizations","include","the","League","of","Women","Voters",",","Rock","the","Vote",",","The","Voter","Participation","Center","and","Vote.org",",","which","attempt","to","motivate","potential","voters","to","register","and","to","vote","in","the","belief","that","failure","of","any","eligible","voter","to","vote","in","any","election","is","a","loss","to","society","."],"labels":["O","O","B-country","I-country","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","country","politician","political_party","person","event","organization","location"]}
{"id":"17","dataset":"crossner_politics","split":"dev","instance":{"id":"17","prompt_labels":"The(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) took(O) back(O) this(O) historically(O) New(B-political party) Democratic(I-political party) Party(I-political party) ((O) NDP(B-political party) )(O) seat(O) in(O) 2004(B-election) Canadian(I-election) federal(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, organization, politician, location, person, political party, election and O.\nSentence: The Conservative Party of Canada took back this historically New Democratic Party ( NDP ) seat in 2004 Canadian federal election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Conservative","Party","of","Canada","took","back","this","historically","New","Democratic","Party","(","NDP",")","seat","in","2004","Canadian","federal","election","."],"labels":["O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["country","event","organization","politician","location","person","political_party","election"]}
{"id":"22","dataset":"crossner_politics","split":"dev","instance":{"id":"22","prompt_labels":"For(O) the(O) 2016(B-election) Belarusian(I-election) parliamentary(I-election) election(I-election) ,(O) the(O) party(O) formed(O) an(O) alliance(O) with(O) the(O) BPF(B-political party) Party(I-political party) ,(O) the(O) Belarusian(B-political party) Christian(I-political party) Democracy(I-political party) ,(O) the(O) Social(B-political party) Democratic(I-political party) Party(I-political party) ((O) Assembly(B-political party) )(O) ,(O) the(O) '(B-political party) Za(I-political party) svabodu(I-political party) '(I-political party) movement(I-political party) ,(O) the(O) Belarusian(B-political party) Green(I-political party) Party(I-political party) ,(O) the(O) Belarusian(B-political party) Liberal(I-political party) Party(I-political party) of(I-political party) Freedom(I-political party) and(I-political party) Progress(I-political party) ,(O) the(O) Trade(B-political party) Union(I-political party) of(I-political party) Electric(I-political party) Industry(I-political party) and(O) independent(O) candidates(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, politician, political party, person, country, election, location and O.\nSentence: For the 2016 Belarusian parliamentary election , the party formed an alliance with the BPF Party , the Belarusian Christian Democracy , the Social Democratic Party ( Assembly ) , the ' Za svabodu ' movement , the Belarusian Green Party , the Belarusian Liberal Party of Freedom and Progress , the Trade Union of Electric Industry and independent candidates .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","the","2016","Belarusian","parliamentary","election",",","the","party","formed","an","alliance","with","the","BPF","Party",",","the","Belarusian","Christian","Democracy",",","the","Social","Democratic","Party","(","Assembly",")",",","the","'","Za","svabodu","'","movement",",","the","Belarusian","Green","Party",",","the","Belarusian","Liberal","Party","of","Freedom","and","Progress",",","the","Trade","Union","of","Electric","Industry","and","independent","candidates","."],"labels":["O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","organization","politician","political_party","person","country","election","location"]}
{"id":"23","dataset":"crossner_politics","split":"dev","instance":{"id":"23","prompt_labels":"It(O) was(O) succeeded(O) in(O) the(O) Flemish(B-organization) Community(I-organization) of(O) Belgium(B-location) by(O) the(O) Open(B-political party) Vlaamse(I-political party) Liberalen(I-political party) en(I-political party) Democraten(I-political party) ((O) VLD(B-political party) )(O) and(O) in(O) the(O) French(B-country) Community(I-country) by(O) the(O) Liberal(B-political party) Reformist(I-political party) Party(I-political party) ,(O) Parti(B-political party) des(I-political party) Rformes(I-political party) et(I-political party) des(I-political party) Liberts(I-political party) de(I-political party) Wallonie(I-political party) and(O) the(O) current-day(O) Mouvement(B-political party) Rformateur(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, organization, politician, political party, election, person, event and O.\nSentence: It was succeeded in the Flemish Community of Belgium by the Open Vlaamse Liberalen en Democraten ( VLD ) and in the French Community by the Liberal Reformist Party , Parti des Rformes et des Liberts de Wallonie and the current-day Mouvement Rformateur .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","succeeded","in","the","Flemish","Community","of","Belgium","by","the","Open","Vlaamse","Liberalen","en","Democraten","(","VLD",")","and","in","the","French","Community","by","the","Liberal","Reformist","Party",",","Parti","des","Rformes","et","des","Liberts","de","Wallonie","and","the","current-day","Mouvement","Rformateur","."],"labels":["O","O","O","O","O","B-organization","I-organization","O","B-location","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","B-country","I-country","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["location","country","organization","politician","political_party","election","person","event"]}
{"id":"24","dataset":"crossner_politics","split":"dev","instance":{"id":"24","prompt_labels":"In(O) March(O) 2002(O) the(O) PRL(B-political party) merged(O) with(O) the(O) German-speaking(O) Partei(B-political party) fr(I-political party) Freiheit(I-political party) und(I-political party) Fortschritt(I-political party) ((O) PFF(B-political party) )(O) of(O) the(O) East(B-political party) Cantons(I-political party) ,(O) the(O) Democratic(B-political party) Front(I-political party) of(I-political party) Francophones(I-political party) ((O) FDF(B-political party) )(O) and(O) the(O) Mouvement(B-political party) des(I-political party) Citoyens(I-political party) pour(I-political party) le(I-political party) Changement(I-political party) ((O) MCC(B-political party) )(O) into(O) the(O) Mouvement(B-political party) Rformateur(I-political party) ((O) MR(B-political party) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, political party, election, country, location, organization, event, person and O.\nSentence: In March 2002 the PRL merged with the German-speaking Partei fr Freiheit und Fortschritt ( PFF ) of the East Cantons , the Democratic Front of Francophones ( FDF ) and the Mouvement des Citoyens pour le Changement ( MCC ) into the Mouvement Rformateur ( MR ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","March","2002","the","PRL","merged","with","the","German-speaking","Partei","fr","Freiheit","und","Fortschritt","(","PFF",")","of","the","East","Cantons",",","the","Democratic","Front","of","Francophones","(","FDF",")","and","the","Mouvement","des","Citoyens","pour","le","Changement","(","MCC",")","into","the","Mouvement","Rformateur","(","MR",")","."],"labels":["O","O","O","O","B-political party","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","O","B-political party","O","O"],"target_index":null,"target_label":null},"label_list":["politician","political_party","election","country","location","organization","event","person"]}
{"id":"25","dataset":"crossner_politics","split":"dev","instance":{"id":"25","prompt_labels":"When(O) the(O) seat(O) was(O) created(O) it(O) was(O) nominally(O) held(O) by(O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) ((O) DUP(B-political party) )(O) ,(O) based(O) on(O) mapping(O) the(O) 1992(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) results(O) onto(O) the(O) new(O) boundaries(O) ,(O) but(O) this(O) was(O) because(O) the(O) Ulster(B-political party) Unionist(I-political party) Party(I-political party) ((O) UUP(B-political party) )(O) had(O) not(O) contested(O) the(O) equivalent(O) area(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, country, politician, organization, election, person, location and O.\nSentence: When the seat was created it was nominally held by the Democratic Unionist Party ( DUP ) , based on mapping the 1992 United Kingdom general election results onto the new boundaries , but this was because the Ulster Unionist Party ( UUP ) had not contested the equivalent area .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["When","the","seat","was","created","it","was","nominally","held","by","the","Democratic","Unionist","Party","(","DUP",")",",","based","on","mapping","the","1992","United","Kingdom","general","election","results","onto","the","new","boundaries",",","but","this","was","because","the","Ulster","Unionist","Party","(","UUP",")","had","not","contested","the","equivalent","area","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","political_party","country","politician","organization","election","person","location"]}
{"id":"26","dataset":"crossner_politics","split":"dev","instance":{"id":"26","prompt_labels":"The(O) American(B-organization) Conservative(I-organization) Union(I-organization) consistently(O) rated(O) Jones(B-politician) low(O) among(O) his(O) Republican(O) colleagues(O) for(O) support(O) of(O) the(O) conservative(O) political(O) platform(O) ,(O) though(O) he(O) received(O) higher(O) ratings(O) from(O) the(O) Conservative(B-organization) Review(I-organization) and(O) Club(B-organization) for(I-organization) Growth(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, political party, country, politician, person, location, organization and O.\nSentence: The American Conservative Union consistently rated Jones low among his Republican colleagues for support of the conservative political platform , though he received higher ratings from the Conservative Review and Club for Growth .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","American","Conservative","Union","consistently","rated","Jones","low","among","his","Republican","colleagues","for","support","of","the","conservative","political","platform",",","though","he","received","higher","ratings","from","the","Conservative","Review","and","Club","for","Growth","."],"labels":["O","B-organization","I-organization","I-organization","O","O","B-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["election","event","political_party","country","politician","person","location","organization"]}
{"id":"31","dataset":"crossner_politics","split":"dev","instance":{"id":"31","prompt_labels":"Originally(O) a(O) member(O) of(O) the(O) Centre(B-political party) of(I-political party) Social(I-political party) Democrats(I-political party) ((O) CDS(B-political party) )(O) ,(O) the(O) Christian(O) Democrat(O) component(O) of(O) the(O) Union(B-political party) for(I-political party) French(I-political party) Democracy(I-political party) ((O) UDF(B-political party) )(O) party(O) ,(O) he(O) later(O) joined(O) the(O) Union(B-political party) for(I-political party) a(I-political party) Popular(I-political party) Movement(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, person, political party, politician, location, country, event, organization and O.\nSentence: Originally a member of the Centre of Social Democrats ( CDS ) , the Christian Democrat component of the Union for French Democracy ( UDF ) party , he later joined the Union for a Popular Movement .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Originally","a","member","of","the","Centre","of","Social","Democrats","(","CDS",")",",","the","Christian","Democrat","component","of","the","Union","for","French","Democracy","(","UDF",")","party",",","he","later","joined","the","Union","for","a","Popular","Movement","."],"labels":["O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["election","person","political_party","politician","location","country","event","organization"]}
{"id":"34","dataset":"crossner_politics","split":"dev","instance":{"id":"34","prompt_labels":"In(O) December(O) 2017(O) IdV(B-political party) was(O) a(O) founding(O) member(O) of(O) the(O) Popular(B-political party) Civic(I-political party) List(I-political party) ((O) CP(B-political party) )(O) ,(O) a(O) centrist(O) electoral(O) list(O) within(O) the(O) centre-left(O) coalition(O) ,(O) along(O) with(O) Popular(B-political party) Alternative(I-political party) ((O) AP(B-political party) )(O) ,(O) the(O) Centrists(B-political party) for(I-political party) Europe(I-political party) ((O) CpE(B-political party) )(O) ,(O) Solidary(B-political party) Democracy(I-political party) ((O) DemoS(B-political party) )(O) ,(O) the(O) Union(B-political party) for(I-political party) Trentino(I-political party) ((O) UpT(B-political party) )(O) ,(O) Italy(B-political party) is(I-political party) Popular(I-political party) ((O) IP(B-political party) )(O) and(O) minor(O) parties(O) /(O) groups(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, person, event, location, politician, country, organization, political party and O.\nSentence: In December 2017 IdV was a founding member of the Popular Civic List ( CP ) , a centrist electoral list within the centre-left coalition , along with Popular Alternative ( AP ) , the Centrists for Europe ( CpE ) , Solidary Democracy ( DemoS ) , the Union for Trentino ( UpT ) , Italy is Popular ( IP ) and minor parties / groups .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","December","2017","IdV","was","a","founding","member","of","the","Popular","Civic","List","(","CP",")",",","a","centrist","electoral","list","within","the","centre-left","coalition",",","along","with","Popular","Alternative","(","AP",")",",","the","Centrists","for","Europe","(","CpE",")",",","Solidary","Democracy","(","DemoS",")",",","the","Union","for","Trentino","(","UpT",")",",","Italy","is","Popular","(","IP",")","and","minor","parties","/","groups","."],"labels":["O","O","O","B-political party","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","B-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","person","event","location","politician","country","organization","political_party"]}
{"id":"35","dataset":"crossner_politics","split":"dev","instance":{"id":"35","prompt_labels":"The(O) party(O) includes(O) former(O) Italian(B-political party) Communist(I-political party) Party(I-political party) and(O) former(O) Lega(B-political party) Nord(I-political party) ,(O) as(O) well(O) as(O) former(O) Italian(B-political party) Social(I-political party) Movement(I-political party) and(O) several(O) former(O) Christian(O) Democrats(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, event, country, person, organization, political party, location and O.\nSentence: The party includes former Italian Communist Party and former Lega Nord , as well as former Italian Social Movement and several former Christian Democrats .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","party","includes","former","Italian","Communist","Party","and","former","Lega","Nord",",","as","well","as","former","Italian","Social","Movement","and","several","former","Christian","Democrats","."],"labels":["O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["politician","election","event","country","person","organization","political_party","location"]}
{"id":"38","dataset":"crossner_politics","split":"dev","instance":{"id":"38","prompt_labels":"Before(O) the(O) merger(O) of(O) the(O) Queensland(B-location) branches(O) of(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) and(O) Nationals(O) as(O) the(O) Liberal(B-political party) National(I-political party) Party(I-political party) of(I-political party) Queensland(I-political party) ,(O) the(O) National(O) Party(O) had(O) been(O) the(O) senior(O) partner(O) in(O) the(O) non-(O) Australian(B-political party) Labor(I-political party) Party(I-political party) Coalition(O) since(O) 1924(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, election, organization, location, country, politician, political party and O.\nSentence: Before the merger of the Queensland branches of the Liberal Party of Australia and Nationals as the Liberal National Party of Queensland , the National Party had been the senior partner in the non- Australian Labor Party Coalition since 1924 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Before","the","merger","of","the","Queensland","branches","of","the","Liberal","Party","of","Australia","and","Nationals","as","the","Liberal","National","Party","of","Queensland",",","the","National","Party","had","been","the","senior","partner","in","the","non-","Australian","Labor","Party","Coalition","since","1924","."],"labels":["O","O","O","O","O","B-location","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","event","election","organization","location","country","politician","political_party"]}
{"id":"40","dataset":"crossner_politics","split":"dev","instance":{"id":"40","prompt_labels":"The(O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) ((O) SDLP(B-political party) )(O) stood(O) a(O) candidate(O) against(O) her(O) in(O) the(O) February(B-election) 1974(I-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) the(O) nationalist(O) vote(O) was(O) strongly(O) divided(O) ,(O) allowing(O) John(B-politician) Dunlop(I-politician) of(O) the(O) Vanguard(B-political party) Progressive(I-political party) Unionist(I-political party) Party(I-political party) to(O) win(O) with(O) the(O) support(O) of(O) the(O) UUP(B-political party) and(O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) ((O) DUP(B-political party) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, politician, election, political party, person, country, event and O.\nSentence: The Social Democratic and Labour Party ( SDLP ) stood a candidate against her in the February 1974 United Kingdom general election and the nationalist vote was strongly divided , allowing John Dunlop of the Vanguard Progressive Unionist Party to win with the support of the UUP and the Democratic Unionist Party ( DUP ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Social","Democratic","and","Labour","Party","(","SDLP",")","stood","a","candidate","against","her","in","the","February","1974","United","Kingdom","general","election","and","the","nationalist","vote","was","strongly","divided",",","allowing","John","Dunlop","of","the","Vanguard","Progressive","Unionist","Party","to","win","with","the","support","of","the","UUP","and","the","Democratic","Unionist","Party","(","DUP",")","."],"labels":["O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","B-political party","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O"],"target_index":null,"target_label":null},"label_list":["organization","location","politician","election","political_party","person","country","event"]}
{"id":"41","dataset":"crossner_politics","split":"dev","instance":{"id":"41","prompt_labels":"In(O) 1990(B-election) Australian(I-election) federal(I-election) election(I-election) Caldicott(O) unsuccessfully(O) contested(O) the(O) House(B-organization) of(I-organization) Representatives(I-organization) New(I-organization) South(I-organization) Wales(I-organization) seat(O) of(O) Richmond(B-location) ,(O) a(O) seat(O) held(O) since(O) the(O) inaugural(O) 1901(B-election) Australian(I-election) federal(I-election) election(I-election) by(O) conservatives(O) ,(O) and(O) by(O) the(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) since(O) it(O) first(O) contested(O) elections(O) at(O) the(O) 1922(B-election) Australian(I-election) federal(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, person, country, election, location, politician, political party and O.\nSentence: In 1990 Australian federal election Caldicott unsuccessfully contested the House of Representatives New South Wales seat of Richmond , a seat held since the inaugural 1901 Australian federal election by conservatives , and by the National Party of Australia since it first contested elections at the 1922 Australian federal election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1990","Australian","federal","election","Caldicott","unsuccessfully","contested","the","House","of","Representatives","New","South","Wales","seat","of","Richmond",",","a","seat","held","since","the","inaugural","1901","Australian","federal","election","by","conservatives",",","and","by","the","National","Party","of","Australia","since","it","first","contested","elections","at","the","1922","Australian","federal","election","."],"labels":["O","B-election","I-election","I-election","I-election","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-location","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["event","organization","person","country","election","location","politician","political_party"]}
{"id":"45","dataset":"crossner_politics","split":"dev","instance":{"id":"45","prompt_labels":"For(O) the(O) 2011(B-election) Peruvian(I-election) general(I-election) election(I-election) ,(O) the(O) party(O) joined(O) forces(O) with(O) We(B-political party) Are(I-political party) Peru(I-political party) and(O) Possible(B-political party) Peru(I-political party) to(O) form(O) the(O) Peru(B-organization) Possible(I-organization) Alliance(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, event, organization, political party, country, person, election and O.\nSentence: For the 2011 Peruvian general election , the party joined forces with We Are Peru and Possible Peru to form the Peru Possible Alliance .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","the","2011","Peruvian","general","election",",","the","party","joined","forces","with","We","Are","Peru","and","Possible","Peru","to","form","the","Peru","Possible","Alliance","."],"labels":["O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","O","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["politician","location","event","organization","political_party","country","person","election"]}
{"id":"47","dataset":"crossner_politics","split":"dev","instance":{"id":"47","prompt_labels":"Other(O) parties(O) such(O) as(O) the(O) Alliance(B-political party) Party(I-political party) of(I-political party) Northern(I-political party) Ireland(I-political party) ,(O) Progressive(B-political party) Unionist(I-political party) Party(I-political party) ,(O) Unionist(B-political party) Party(I-political party) of(I-political party) Northern(I-political party) Ireland(I-political party) ,(O) Conservatives(B-political party) and(I-political party) the(I-political party) Workers(I-political party) '(I-political party) Party(I-political party) have(O) at(O) times(O) polled(O) significantly(O) ,(O) as(O) have(O) independent(O) candidates(O) ,(O) with(O) the(O) result(O) that(O) many(O) elections(O) have(O) been(O) won(O) on(O) comparatively(O) low(O) shares(O) of(O) the(O) vote(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, location, election, event, politician, person, political party and O.\nSentence: Other parties such as the Alliance Party of Northern Ireland , Progressive Unionist Party , Unionist Party of Northern Ireland , Conservatives and the Workers ' Party have at times polled significantly , as have independent candidates , with the result that many elections have been won on comparatively low shares of the vote .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","parties","such","as","the","Alliance","Party","of","Northern","Ireland",",","Progressive","Unionist","Party",",","Unionist","Party","of","Northern","Ireland",",","Conservatives","and","the","Workers","'","Party","have","at","times","polled","significantly",",","as","have","independent","candidates",",","with","the","result","that","many","elections","have","been","won","on","comparatively","low","shares","of","the","vote","."],"labels":["O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","organization","location","election","event","politician","person","political_party"]}
{"id":"49","dataset":"crossner_politics","split":"dev","instance":{"id":"49","prompt_labels":"AP(B-political party) was(O) born(O) as(O) an(O) anti-establishment(O) party(O) in(O) 1993(O) ,(O) in(O) parallel(O) with(O) the(O) rise(O) of(O) Lega(B-political party) Nord(I-political party) in(O) Italy(B-country) ,(O) of(O) which(O) it(O) has(O) been(O) long(O) considered(O) the(O) Sanmarinese(O) counterpart(O) ,(O) but(O) has(O) since(O) then(O) become(O) a(O) stable(O) political(O) force(O) in(O) San(B-location) Marino(I-location) ,(O) participating(O) in(O) government(O) coalitions(O) with(O) the(O) centrist(O) Sammarinese(B-political party) Christian(I-political party) Democratic(I-political party) Party(I-political party) ((O) PDCS(B-political party) )(O) as(O) well(O) as(O) with(O) the(O) centre-left(O) Party(B-political party) of(I-political party) Socialists(I-political party) and(I-political party) Democrats(I-political party) ((O) PSD(B-political party) )(O) since(O) 2002(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, location, organization, person, politician, political party, country and O.\nSentence: AP was born as an anti-establishment party in 1993 , in parallel with the rise of Lega Nord in Italy , of which it has been long considered the Sanmarinese counterpart , but has since then become a stable political force in San Marino , participating in government coalitions with the centrist Sammarinese Christian Democratic Party ( PDCS ) as well as with the centre-left Party of Socialists and Democrats ( PSD ) since 2002 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["AP","was","born","as","an","anti-establishment","party","in","1993",",","in","parallel","with","the","rise","of","Lega","Nord","in","Italy",",","of","which","it","has","been","long","considered","the","Sanmarinese","counterpart",",","but","has","since","then","become","a","stable","political","force","in","San","Marino",",","participating","in","government","coalitions","with","the","centrist","Sammarinese","Christian","Democratic","Party","(","PDCS",")","as","well","as","with","the","centre-left","Party","of","Socialists","and","Democrats","(","PSD",")","since","2002","."],"labels":["B-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","election","location","organization","person","politician","political_party","country"]}
{"id":"50","dataset":"crossner_politics","split":"dev","instance":{"id":"50","prompt_labels":"In(O) 1987(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) Adams(B-politician) narrowly(O) held(O) his(O) seat(O) ,(O) but(O) lost(O) it(O) in(O) the(O) 1992(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) amidst(O) a(O) strong(O) tactical(O) voting(O) campaign(O) in(O) favour(O) of(O) Joe(B-politician) Hendron(I-politician) of(O) the(O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) by(O) unionists(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, politician, event, election, organization, political party, person and O.\nSentence: In 1987 United Kingdom general election Adams narrowly held his seat , but lost it in the 1992 United Kingdom general election amidst a strong tactical voting campaign in favour of Joe Hendron of the Social Democratic and Labour Party by unionists","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1987","United","Kingdom","general","election","Adams","narrowly","held","his","seat",",","but","lost","it","in","the","1992","United","Kingdom","general","election","amidst","a","strong","tactical","voting","campaign","in","favour","of","Joe","Hendron","of","the","Social","Democratic","and","Labour","Party","by","unionists"],"labels":["O","B-election","I-election","I-election","I-election","I-election","B-politician","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O"],"target_index":null,"target_label":null},"label_list":["country","location","politician","event","election","organization","political_party","person"]}
{"id":"51","dataset":"crossner_politics","split":"dev","instance":{"id":"51","prompt_labels":"In(O) 1997(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) Adams(B-politician) regained(O) the(O) seat(O) and(O) held(O) it(O) in(O) 2001(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) 2005(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 2010(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, person, election, country, political party, location, politician and O.\nSentence: In 1997 United Kingdom general election Adams regained the seat and held it in 2001 United Kingdom general election , 2005 United Kingdom general election and 2010 United Kingdom general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1997","United","Kingdom","general","election","Adams","regained","the","seat","and","held","it","in","2001","United","Kingdom","general","election",",","2005","United","Kingdom","general","election","and","2010","United","Kingdom","general","election","."],"labels":["O","B-election","I-election","I-election","I-election","I-election","B-politician","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["organization","event","person","election","country","political_party","location","politician"]}
{"id":"55","dataset":"crossner_politics","split":"dev","instance":{"id":"55","prompt_labels":"In(O) the(O) 1979(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) the(O) constituency(O) witnessed(O) a(O) very(O) close(O) three-way(O) fight(O) between(O) Peter(B-politician) Robinson(I-politician) of(O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) ,(O) William(B-politician) Craig(I-politician) for(O) the(O) UUP(B-political party) and(O) Oliver(B-politician) Napier(I-politician) for(O) the(O) Alliance(B-political party) Party(I-political party) of(I-political party) Northern(I-political party) Ireland(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, organization, country, location, event, political party, person and O.\nSentence: In the 1979 United Kingdom general election the constituency witnessed a very close three-way fight between Peter Robinson of the Democratic Unionist Party , William Craig for the UUP and Oliver Napier for the Alliance Party of Northern Ireland .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","1979","United","Kingdom","general","election","the","constituency","witnessed","a","very","close","three-way","fight","between","Peter","Robinson","of","the","Democratic","Unionist","Party",",","William","Craig","for","the","UUP","and","Oliver","Napier","for","the","Alliance","Party","of","Northern","Ireland","."],"labels":["O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","B-political party","I-political party","I-political party","O","B-politician","I-politician","O","O","B-political party","O","B-politician","I-politician","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["election","politician","organization","country","location","event","political_party","person"]}
{"id":"59","dataset":"crossner_politics","split":"dev","instance":{"id":"59","prompt_labels":"It(O) was(O) widely(O) expected(O) that(O) a(O) coalition(O) between(O) supporters(O) of(O) the(O) Orange(B-political party) Movement(I-political party) would(O) form(O) Ukraine(B-country) 's(O) next(O) government(O) ,(O) but(O) after(O) three(O) months(O) of(O) negotiations(O) and(O) a(O) failure(O) to(O) reach(O) an(O) agreement(O) the(O) proposed(O) coalition(O) collapsed(O) following(O) the(O) decision(O) of(O) the(O) Socialist(B-political party) Party(I-political party) of(I-political party) Ukraine(I-political party) to(O) support(O) the(O) formation(O) of(O) the(O) anti-crisis(O) coalition(O) with(O) Party(B-political party) of(I-political party) Regions(I-political party) and(O) the(O) Communist(B-political party) Party(I-political party) of(I-political party) Ukraine(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, political party, person, event, organization, politician, location and O.\nSentence: It was widely expected that a coalition between supporters of the Orange Movement would form Ukraine 's next government , but after three months of negotiations and a failure to reach an agreement the proposed coalition collapsed following the decision of the Socialist Party of Ukraine to support the formation of the anti-crisis coalition with Party of Regions and the Communist Party of Ukraine .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","widely","expected","that","a","coalition","between","supporters","of","the","Orange","Movement","would","form","Ukraine","'s","next","government",",","but","after","three","months","of","negotiations","and","a","failure","to","reach","an","agreement","the","proposed","coalition","collapsed","following","the","decision","of","the","Socialist","Party","of","Ukraine","to","support","the","formation","of","the","anti-crisis","coalition","with","Party","of","Regions","and","the","Communist","Party","of","Ukraine","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["country","election","political_party","person","event","organization","politician","location"]}
{"id":"61","dataset":"crossner_politics","split":"dev","instance":{"id":"61","prompt_labels":"The(O) UUP(B-political party) had(O) their(O) best(O) result(O) in(O) the(O) election(O) ,(O) in(O) part(O) due(O) to(O) no(O) candidate(O) from(O) either(O) the(O) UK(B-political party) Unionist(I-political party) Party(I-political party) or(O) Northern(B-political party) Ireland(I-political party) Unionist(I-political party) Party(I-political party) defending(O) one(O) of(O) the(O) seats(O) won(O) in(O) 1998(B-election) Northern(I-election) Ireland(I-election) Assembly(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, person, election, event, politician, political party, country and O.\nSentence: The UUP had their best result in the election , in part due to no candidate from either the UK Unionist Party or Northern Ireland Unionist Party defending one of the seats won in 1998 Northern Ireland Assembly election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","UUP","had","their","best","result","in","the","election",",","in","part","due","to","no","candidate","from","either","the","UK","Unionist","Party","or","Northern","Ireland","Unionist","Party","defending","one","of","the","seats","won","in","1998","Northern","Ireland","Assembly","election","."],"labels":["O","B-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["organization","location","person","election","event","politician","political_party","country"]}
{"id":"62","dataset":"crossner_politics","split":"dev","instance":{"id":"62","prompt_labels":"The(O) Member(O) of(O) Parliament(B-organization) since(O) 1997(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) is(O) Sir(B-politician) Jeffrey(I-politician) Donaldson(I-politician) who(O) was(O) elected(O) as(O) a(O) member(O) of(O) the(O) Ulster(B-political party) Unionist(I-political party) Party(I-political party) but(O) switched(O) to(O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) in(O) 2004(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, politician, election, organization, country, event, location and O.\nSentence: The Member of Parliament since 1997 United Kingdom general election is Sir Jeffrey Donaldson who was elected as a member of the Ulster Unionist Party but switched to the Democratic Unionist Party in 2004 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Member","of","Parliament","since","1997","United","Kingdom","general","election","is","Sir","Jeffrey","Donaldson","who","was","elected","as","a","member","of","the","Ulster","Unionist","Party","but","switched","to","the","Democratic","Unionist","Party","in","2004","."],"labels":["O","O","O","B-organization","O","B-election","I-election","I-election","I-election","I-election","O","B-politician","I-politician","I-politician","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","political_party","politician","election","organization","country","event","location"]}
{"id":"63","dataset":"crossner_politics","split":"dev","instance":{"id":"63","prompt_labels":"Farry(B-politician) was(O) elected(O) to(O) the(O) position(O) in(O) the(O) 2019(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) replacing(O) the(O) incumbent(O) Sylvia(B-politician) Hermon(I-politician) ,(O) who(O) had(O) held(O) the(O) position(O) since(O) being(O) elected(O) to(O) it(O) in(O) the(O) 2001(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) but(O) chose(O) not(O) to(O) contest(O) in(O) the(O) 2019(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, election, event, politician, person, country, location and O.\nSentence: Farry was elected to the position in the 2019 United Kingdom general election , replacing the incumbent Sylvia Hermon , who had held the position since being elected to it in the 2001 United Kingdom general election , but chose not to contest in the 2019 United Kingdom general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Farry","was","elected","to","the","position","in","the","2019","United","Kingdom","general","election",",","replacing","the","incumbent","Sylvia","Hermon",",","who","had","held","the","position","since","being","elected","to","it","in","the","2001","United","Kingdom","general","election",",","but","chose","not","to","contest","in","the","2019","United","Kingdom","general","election","."],"labels":["B-politician","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["political_party","organization","election","event","politician","person","country","location"]}
{"id":"65","dataset":"crossner_politics","split":"dev","instance":{"id":"65","prompt_labels":"Examples(O) of(O) regional(O) parties(O) that(O) do(O) not(O) generally(O) campaign(O) for(O) greater(O) autonomy(O) or(O) federalism(O) include(O) most(O) provincial(O) parties(O) in(O) Canada(B-country) ,(O) most(O) regional(O) and(O) minority(O) parties(O) in(O) Europe(B-location) ,(O) notably(O) including(O) the(O) Christian(B-political party) Social(I-political party) Union(I-political party) in(I-political party) Bavaria(I-political party) in(O) Bavaria(B-location) ((O) Germany(B-country) )(O) ,(O) most(O) parties(O) in(O) Belgium(B-location) ,(O) most(O) parties(O) in(O) Northern(B-country) Ireland(I-country) ,(O) the(O) Istrian(B-political party) Democratic(I-political party) Assembly(I-political party) in(O) Istria(B-location) and(O) the(O) Alliance(B-political party) of(I-political party) Primorje-Gorski(I-political party) Kotar(I-political party) in(O) Primorje-Gorski(B-location) Kotar(I-location) ((O) both(O) counties(O) of(O) Croatia(B-country) )(O) ,(O) and(O) most(O) political(O) parties(O) in(O) India(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, person, election, organization, political party, event, politician and O.\nSentence: Examples of regional parties that do not generally campaign for greater autonomy or federalism include most provincial parties in Canada , most regional and minority parties in Europe , notably including the Christian Social Union in Bavaria in Bavaria ( Germany ) , most parties in Belgium , most parties in Northern Ireland , the Istrian Democratic Assembly in Istria and the Alliance of Primorje-Gorski Kotar in Primorje-Gorski Kotar ( both counties of Croatia ) , and most political parties in India .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Examples","of","regional","parties","that","do","not","generally","campaign","for","greater","autonomy","or","federalism","include","most","provincial","parties","in","Canada",",","most","regional","and","minority","parties","in","Europe",",","notably","including","the","Christian","Social","Union","in","Bavaria","in","Bavaria","(","Germany",")",",","most","parties","in","Belgium",",","most","parties","in","Northern","Ireland",",","the","Istrian","Democratic","Assembly","in","Istria","and","the","Alliance","of","Primorje-Gorski","Kotar","in","Primorje-Gorski","Kotar","(","both","counties","of","Croatia",")",",","and","most","political","parties","in","India","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","B-location","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-location","O","B-country","O","O","O","O","O","B-location","O","O","O","O","B-country","I-country","O","O","B-political party","I-political party","I-political party","O","B-location","O","O","B-political party","I-political party","I-political party","I-political party","O","B-location","I-location","O","O","O","O","B-country","O","O","O","O","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["location","country","person","election","organization","political_party","event","politician"]}
{"id":"67","dataset":"crossner_politics","split":"dev","instance":{"id":"67","prompt_labels":"Ayaan(B-politician) Hirsi(I-politician) Ali(I-politician) is(O) a(O) Fellow(O) with(O) the(O) Hoover(B-organization) Institution(I-organization) at(O) Stanford(B-organization) University(I-organization) ,(O) a(O) Fellow(O) with(O) the(O) Future(B-organization) of(I-organization) Diplomacy(I-organization) Project(I-organization) at(O) the(O) Belfer(B-organization) Center(I-organization) for(I-organization) Science(I-organization) and(I-organization) International(I-organization) Affairs(I-organization) at(O) The(O) Harvard(B-organization) Kennedy(I-organization) School(I-organization) ,(O) a(O) visiting(O) scholar(O) at(O) the(O) American(B-organization) Enterprise(I-organization) Institute(I-organization) in(O) Washington(B-location) ,(I-location) D.C.(I-location) ,(O) and(O) a(O) member(O) of(O) the(O) Council(B-organization) on(I-organization) Foreign(I-organization) Relations(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, location, politician, election, organization, political party, country and O.\nSentence: Ayaan Hirsi Ali is a Fellow with the Hoover Institution at Stanford University , a Fellow with the Future of Diplomacy Project at the Belfer Center for Science and International Affairs at The Harvard Kennedy School , a visiting scholar at the American Enterprise Institute in Washington , D.C. , and a member of the Council on Foreign Relations .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Ayaan","Hirsi","Ali","is","a","Fellow","with","the","Hoover","Institution","at","Stanford","University",",","a","Fellow","with","the","Future","of","Diplomacy","Project","at","the","Belfer","Center","for","Science","and","International","Affairs","at","The","Harvard","Kennedy","School",",","a","visiting","scholar","at","the","American","Enterprise","Institute","in","Washington",",","D.C.",",","and","a","member","of","the","Council","on","Foreign","Relations","."],"labels":["B-politician","I-politician","I-politician","O","O","O","O","O","B-organization","I-organization","O","B-organization","I-organization","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-location","I-location","I-location","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["event","person","location","politician","election","organization","political_party","country"]}
{"id":"68","dataset":"crossner_politics","split":"dev","instance":{"id":"68","prompt_labels":"Most(O) major(O) federal(O) political(O) parties(O) ,(O) including(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) ,(O) the(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) ,(O) the(O) New(B-political party) Democratic(I-political party) Party(I-political party) and(O) the(O) Green(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) support(O) maintaining(O) the(O) status(O) quo(O) with(O) Quebec(B-location) remaining(O) part(O) of(O) Canada(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, politician, event, political party, country, person, election, organization and O.\nSentence: Most major federal political parties , including the Liberal Party of Canada , the Conservative Party of Canada , the New Democratic Party and the Green Party of Canada support maintaining the status quo with Quebec remaining part of Canada .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Most","major","federal","political","parties",",","including","the","Liberal","Party","of","Canada",",","the","Conservative","Party","of","Canada",",","the","New","Democratic","Party","and","the","Green","Party","of","Canada","support","maintaining","the","status","quo","with","Quebec","remaining","part","of","Canada","."],"labels":["O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","B-location","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["location","politician","event","political_party","country","person","election","organization"]}
{"id":"69","dataset":"crossner_politics","split":"dev","instance":{"id":"69","prompt_labels":"However(O) ,(O) with(O) the(O) onset(O) of(O) the(B-event) Troubles(I-event) ,(O) new(O) parties(O) emerged(O) that(O) appealed(O) to(O) the(O) party(O) 's(O) support(O) base(O) ,(O) including(O) the(O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) ((O) SDLP(B-political party) )(O) ,(O) the(O) Alliance(B-political party) Party(I-political party) of(I-political party) Northern(I-political party) Ireland(I-political party) and(O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, election, political party, politician, country, organization, person, event and O.\nSentence: However , with the onset of the Troubles , new parties emerged that appealed to the party 's support base , including the Social Democratic and Labour Party ( SDLP ) , the Alliance Party of Northern Ireland and the Democratic Unionist Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["However",",","with","the","onset","of","the","Troubles",",","new","parties","emerged","that","appealed","to","the","party","'s","support","base",",","including","the","Social","Democratic","and","Labour","Party","(","SDLP",")",",","the","Alliance","Party","of","Northern","Ireland","and","the","Democratic","Unionist","Party","."],"labels":["O","O","O","O","O","O","B-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["location","election","political_party","politician","country","organization","person","event"]}
{"id":"70","dataset":"crossner_politics","split":"dev","instance":{"id":"70","prompt_labels":"President(O) Kim(B-politician) Dae-jung(I-politician) '(O) s(O) National(B-political party) Congress(I-political party) for(I-political party) New(I-political party) Politics(I-political party) ((O) NCNP(B-political party) )(O) re-branded(O) itself(O) to(O) Millennium(B-political party) Democratic(I-political party) Party(I-political party) ((O) MDP(B-political party) )(O) in(O) 2000(O) ,(O) but(O) was(O) struggling(O) as(O) it(O) had(O) defeated(O) by(O) the(O) Liberty(B-political party) Korea(I-political party) Party(I-political party) ((O) GNP(B-political party) )(O) both(O) the(O) 2000(B-election) South(I-election) Korean(I-election) legislative(I-election) election(I-election) and(O) 2002(B-election) South(I-election) Korean(I-election) local(I-election) elections(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, organization, country, person, politician, event, location and O.\nSentence: President Kim Dae-jung ' s National Congress for New Politics ( NCNP ) re-branded itself to Millennium Democratic Party ( MDP ) in 2000 , but was struggling as it had defeated by the Liberty Korea Party ( GNP ) both the 2000 South Korean legislative election and 2002 South Korean local elections .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["President","Kim","Dae-jung","'","s","National","Congress","for","New","Politics","(","NCNP",")","re-branded","itself","to","Millennium","Democratic","Party","(","MDP",")","in","2000",",","but","was","struggling","as","it","had","defeated","by","the","Liberty","Korea","Party","(","GNP",")","both","the","2000","South","Korean","legislative","election","and","2002","South","Korean","local","elections","."],"labels":["O","B-politician","I-politician","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["election","political_party","organization","country","person","politician","event","location"]}
{"id":"71","dataset":"crossner_politics","split":"dev","instance":{"id":"71","prompt_labels":"The(O) Freedom(B-political party) Party(I-political party) was(O) subsequently(O) expelled(O) from(O) the(O) Liberal(B-organization) International(I-organization) ,(O) and(O) the(O) remaining(O) liberals(O) seceded(O) to(O) found(O) the(O) Liberal(B-political party) Forum(I-political party) ((O) Liberales(B-political party) Forum(I-political party) ,(O) member(O) Liberal(B-organization) International(I-organization) ,(O) Alliance(B-political party) of(I-political party) Liberals(I-political party) and(I-political party) Democrats(I-political party) for(I-political party) Europe(I-political party) Party(I-political party) )(O) in(O) 1993(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, country, politician, person, location, election, organization and O.\nSentence: The Freedom Party was subsequently expelled from the Liberal International , and the remaining liberals seceded to found the Liberal Forum ( Liberales Forum , member Liberal International , Alliance of Liberals and Democrats for Europe Party ) in 1993 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Freedom","Party","was","subsequently","expelled","from","the","Liberal","International",",","and","the","remaining","liberals","seceded","to","found","the","Liberal","Forum","(","Liberales","Forum",",","member","Liberal","International",",","Alliance","of","Liberals","and","Democrats","for","Europe","Party",")","in","1993","."],"labels":["O","B-political party","I-political party","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","B-political party","I-political party","O","O","B-organization","I-organization","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","event","country","politician","person","location","election","organization"]}
{"id":"74","dataset":"crossner_politics","split":"dev","instance":{"id":"74","prompt_labels":"The(O) main(O) line(O) of(O) conflict(O) in(O) France(B-country) during(O) the(O) 19th(O) century(O) was(O) between(O) monarchists(O) ((O) mainly(O) Legitimists(O) and(O) Orlanist(O) s(O) ,(O) but(O) also(O) Bonapartism(O) )(O) and(O) republicans(O) ((O) Radical-Socialists(O) ,(O) Opportunist(B-organization) Republicans(I-organization) ,(O) and(O) later(O) socialists(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, location, event, election, politician, political party, organization and O.\nSentence: The main line of conflict in France during the 19th century was between monarchists ( mainly Legitimists and Orlanist s , but also Bonapartism ) and republicans ( Radical-Socialists , Opportunist Republicans , and later socialists ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","main","line","of","conflict","in","France","during","the","19th","century","was","between","monarchists","(","mainly","Legitimists","and","Orlanist","s",",","but","also","Bonapartism",")","and","republicans","(","Radical-Socialists",",","Opportunist","Republicans",",","and","later","socialists",")","."],"labels":["O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","country","location","event","election","politician","political_party","organization"]}
{"id":"75","dataset":"crossner_politics","split":"dev","instance":{"id":"75","prompt_labels":"She(O) was(O) elected(O) to(O) the(O) Ontario(B-organization) legislature(I-organization) in(O) the(O) 1995(B-election) Ontario(I-election) general(I-election) election(I-election) ,(O) defeating(O) Ontario(B-political party) Liberal(I-political party) Party(I-political party) Joe(B-politician) Dickson(I-politician) and(O) incumbent(O) Ontario(B-political party) New(I-political party) Democratic(I-political party) Party(I-political party) Jim(B-politician) Wiseman(I-politician) by(O) a(O) significant(O) margin(O) in(O) the(O) riding(O) of(O) Durham(B-location) West(I-location) ,(O) east(O) of(O) Toronto(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, organization, politician, location, political party, election, event and O.\nSentence: She was elected to the Ontario legislature in the 1995 Ontario general election , defeating Ontario Liberal Party Joe Dickson and incumbent Ontario New Democratic Party Jim Wiseman by a significant margin in the riding of Durham West , east of Toronto .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","was","elected","to","the","Ontario","legislature","in","the","1995","Ontario","general","election",",","defeating","Ontario","Liberal","Party","Joe","Dickson","and","incumbent","Ontario","New","Democratic","Party","Jim","Wiseman","by","a","significant","margin","in","the","riding","of","Durham","West",",","east","of","Toronto","."],"labels":["O","O","O","O","O","B-organization","I-organization","O","O","B-election","I-election","I-election","I-election","O","O","B-political party","I-political party","I-political party","B-politician","I-politician","O","O","B-political party","I-political party","I-political party","I-political party","B-politician","I-politician","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["country","person","organization","politician","location","political_party","election","event"]}
{"id":"77","dataset":"crossner_politics","split":"dev","instance":{"id":"77","prompt_labels":"In(O) recent(O) years(O) ,(O) the(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) has(O) had(O) the(O) most(O) success(O) in(O) the(O) city(O) :(O) its(O) members(O) were(O) elected(O) in(O) all(O) but(O) four(O) elections(O) since(O) 1953(O) :(O) 1974(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 1980(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2004(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) and(O) 2006(B-election) Canadian(I-election) federal(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, organization, political party, event, election, politician, country and O.\nSentence: In recent years , the Progressive Conservative Party of Canada has had the most success in the city : its members were elected in all but four elections since 1953 : 1974 Canadian federal election , 1980 Canadian federal election , 2004 Canadian federal election , and 2006 Canadian federal election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","recent","years",",","the","Progressive","Conservative","Party","of","Canada","has","had","the","most","success","in","the","city",":","its","members","were","elected","in","all","but","four","elections","since","1953",":","1974","Canadian","federal","election",",","1980","Canadian","federal","election",",","2004","Canadian","federal","election",",","and","2006","Canadian","federal","election","."],"labels":["O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["person","location","organization","political_party","event","election","politician","country"]}
{"id":"79","dataset":"crossner_politics","split":"dev","instance":{"id":"79","prompt_labels":"He(O) was(O) the(O) American(B-political party) Independent(I-political party) Party(I-political party) vice(O) presidential(O) nominee(O) under(O) John(B-politician) G.(I-politician) Schmitz(I-politician) in(O) 1972(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) and(O) the(O) American(O) Party(O) presidential(O) nominee(O) in(O) 1976(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, location, election, person, event, political party, country and O.\nSentence: He was the American Independent Party vice presidential nominee under John G. Schmitz in 1972 United States presidential election and the American Party presidential nominee in 1976 United States presidential election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","the","American","Independent","Party","vice","presidential","nominee","under","John","G.","Schmitz","in","1972","United","States","presidential","election","and","the","American","Party","presidential","nominee","in","1976","United","States","presidential","election","."],"labels":["O","O","O","B-political party","I-political party","I-political party","O","O","O","O","B-politician","I-politician","I-politician","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["organization","politician","location","election","person","event","political_party","country"]}
{"id":"81","dataset":"crossner_politics","split":"dev","instance":{"id":"81","prompt_labels":"He(O) won(O) reelection(O) in(O) 1997(O) as(O) a(O) Reform(B-political party) Party(I-political party) Member(O) ,(O) in(O) 2000(O) as(O) a(O) member(O) of(O) the(O) Canadian(B-political party) Alliance(I-political party) and(O) in(O) 2004(O) as(O) a(O) member(O) of(O) the(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) ,(O) until(O) Black(B-politician) defeated(O) him(O) in(O) the(O) 2006(B-election) Canadian(I-election) federal(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, event, politician, political party, person, organization, election and O.\nSentence: He won reelection in 1997 as a Reform Party Member , in 2000 as a member of the Canadian Alliance and in 2004 as a member of the Conservative Party of Canada , until Black defeated him in the 2006 Canadian federal election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","won","reelection","in","1997","as","a","Reform","Party","Member",",","in","2000","as","a","member","of","the","Canadian","Alliance","and","in","2004","as","a","member","of","the","Conservative","Party","of","Canada",",","until","Black","defeated","him","in","the","2006","Canadian","federal","election","."],"labels":["O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-politician","O","O","O","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["location","country","event","politician","political_party","person","organization","election"]}
{"id":"82","dataset":"crossner_politics","split":"dev","instance":{"id":"82","prompt_labels":"Kmeit(B-politician) did(O) quite(O) well(O) ,(O) and(O) in(O) 1993(O) ,(O) when(O) the(O) LDP(B-political party) was(O) for(O) the(O) first(O) time(O) declared(O) an(O) opposition(O) party(O) ,(O) the(O) Kmeit(B-political party) became(O) one(O) of(O) the(O) ruling(O) parties(O) ,(O) headed(O) by(O) the(O) liberal(O) Japan(B-political party) New(I-political party) Party(I-political party) ,(O) but(O) which(O) also(O) included(O) the(O) Democratic(B-political party) Socialist(I-political party) Party(I-political party) ,(O) Japan(B-political party) Renewal(I-political party) Party(I-political party) ,(O) the(O) New(B-political party) Party(I-political party) Sakigake(I-political party) ,(O) and(O) the(O) Japan(B-political party) Socialist(I-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, organization, political party, event, country, election, location and O.\nSentence: Kmeit did quite well , and in 1993 , when the LDP was for the first time declared an opposition party , the Kmeit became one of the ruling parties , headed by the liberal Japan New Party , but which also included the Democratic Socialist Party , Japan Renewal Party , the New Party Sakigake , and the Japan Socialist Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Kmeit","did","quite","well",",","and","in","1993",",","when","the","LDP","was","for","the","first","time","declared","an","opposition","party",",","the","Kmeit","became","one","of","the","ruling","parties",",","headed","by","the","liberal","Japan","New","Party",",","but","which","also","included","the","Democratic","Socialist","Party",",","Japan","Renewal","Party",",","the","New","Party","Sakigake",",","and","the","Japan","Socialist","Party","."],"labels":["B-politician","O","O","O","O","O","O","O","O","O","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["person","politician","organization","political_party","event","country","election","location"]}
{"id":"84","dataset":"crossner_politics","split":"dev","instance":{"id":"84","prompt_labels":"Under(O) Terre(B-politician) 'Blanche(I-politician) ,(O) the(O) AWB(B-organization) swore(O) to(O) use(O) violence(O) to(O) preserve(O) minority(O) rule(O) ,(O) opposing(O) any(O) concessions(O) offered(O) to(O) the(O) African(B-political party) National(I-political party) Congress(I-political party) -(O) an(O) organisation(O) AWB(B-organization) supporters(O) repeatedly(O) branded(O) as(O) Marxist(O) terrorist(O) s(O) Immediately(O) prior(O) to(O) South(B-country) Africa(I-country) 's(O) 1994(B-election) South(I-election) African(I-election) general(I-election) election(I-election) ,(O) Terre(B-politician) 'Blanche(I-politician) 's(O) followers(O) were(O) linked(O) to(O) a(O) number(O) of(O) bombings(O) and(O) assassinations(O) targeting(O) the(O) South(B-political party) African(I-political party) Communist(I-political party) Party(I-political party) ;(O) armed(O) AWB(B-organization) commandos(O) participated(O) in(O) the(O) crisis(O) in(O) Bophuthatswana(B-country) in(O) 1994(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, event, politician, person, location, election, country and O.\nSentence: Under Terre 'Blanche , the AWB swore to use violence to preserve minority rule , opposing any concessions offered to the African National Congress - an organisation AWB supporters repeatedly branded as Marxist terrorist s Immediately prior to South Africa 's 1994 South African general election , Terre 'Blanche 's followers were linked to a number of bombings and assassinations targeting the South African Communist Party ; armed AWB commandos participated in the crisis in Bophuthatswana in 1994 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Under","Terre","'Blanche",",","the","AWB","swore","to","use","violence","to","preserve","minority","rule",",","opposing","any","concessions","offered","to","the","African","National","Congress","-","an","organisation","AWB","supporters","repeatedly","branded","as","Marxist","terrorist","s","Immediately","prior","to","South","Africa","'s","1994","South","African","general","election",",","Terre","'Blanche","'s","followers","were","linked","to","a","number","of","bombings","and","assassinations","targeting","the","South","African","Communist","Party",";","armed","AWB","commandos","participated","in","the","crisis","in","Bophuthatswana","in","1994","."],"labels":["O","B-politician","I-politician","O","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","B-organization","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O","B-election","I-election","I-election","I-election","I-election","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-organization","O","O","O","O","O","O","B-country","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","political_party","event","politician","person","location","election","country"]}
{"id":"85","dataset":"crossner_politics","split":"dev","instance":{"id":"85","prompt_labels":"She(O) was(O) a(O) candidate(O) in(O) the(O) 1984(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) 1992(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ((O) 339(O) votes(O) )(O) ,(O) 1996(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) and(O) 2004(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, location, event, person, organization, country, political party and O.\nSentence: She was a candidate in the 1984 United States presidential election , 1992 United States presidential election ( 339 votes ) , 1996 United States presidential election , and 2004 United States presidential election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","was","a","candidate","in","the","1984","United","States","presidential","election",",","1992","United","States","presidential","election","(","339","votes",")",",","1996","United","States","presidential","election",",","and","2004","United","States","presidential","election","."],"labels":["O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["politician","election","location","event","person","organization","country","political_party"]}
{"id":"90","dataset":"crossner_politics","split":"dev","instance":{"id":"90","prompt_labels":"Other(O) political(O) parties(O) that(O) have(O) practiced(O) fusion(O) include(O) the(O) Conservative(B-political party) Party(I-political party) of(I-political party) New(I-political party) York(I-political party) State(I-political party) ,(O) the(O) Working(B-political party) Families(I-political party) Party(I-political party) and(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) New(I-political party) York(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, organization, political party, politician, person, location, country and O.\nSentence: Other political parties that have practiced fusion include the Conservative Party of New York State , the Working Families Party and the Liberal Party of New York .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","political","parties","that","have","practiced","fusion","include","the","Conservative","Party","of","New","York","State",",","the","Working","Families","Party","and","the","Liberal","Party","of","New","York","."],"labels":["O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["election","event","organization","political_party","politician","person","location","country"]}
{"id":"92","dataset":"crossner_politics","split":"dev","instance":{"id":"92","prompt_labels":"Supporters(O) of(O) the(O) campaign(O) included(O) Margo(B-person) Kingston(I-person) ((O) journalist(O) )(O) ,(O) John(B-politician) Valder(I-politician) ((O) previous(O) president(O) of(O) Howard(O) 's(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) )(O) ,(O) Brian(B-person) Deegan(I-person) ((O) former(O) magistrate(O) ,(O) who(O) stood(O) against(O) Alexander(B-politician) Downer(I-politician) )(O) ,(O) Andrew(B-politician) Wilkie(I-politician) ((O) Australian(B-political party) Greens(I-political party) candidate(O) )(O) ,(O) Alex(B-person) Broun(I-person) playwright(O) and(O) Nicole(B-politician) Campbell(I-politician) ((O) Australian(B-political party) Labor(I-political party) Party(I-political party) candidate(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, location, person, election, organization, politician, event and O.\nSentence: Supporters of the campaign included Margo Kingston ( journalist ) , John Valder ( previous president of Howard 's Liberal Party of Australia ) , Brian Deegan ( former magistrate , who stood against Alexander Downer ) , Andrew Wilkie ( Australian Greens candidate ) , Alex Broun playwright and Nicole Campbell ( Australian Labor Party candidate ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Supporters","of","the","campaign","included","Margo","Kingston","(","journalist",")",",","John","Valder","(","previous","president","of","Howard","'s","Liberal","Party","of","Australia",")",",","Brian","Deegan","(","former","magistrate",",","who","stood","against","Alexander","Downer",")",",","Andrew","Wilkie","(","Australian","Greens","candidate",")",",","Alex","Broun","playwright","and","Nicole","Campbell","(","Australian","Labor","Party","candidate",")","."],"labels":["O","O","O","O","O","B-person","I-person","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-person","I-person","O","O","O","O","O","O","O","B-politician","I-politician","O","O","B-politician","I-politician","O","B-political party","I-political party","O","O","O","B-person","I-person","O","O","B-politician","I-politician","O","B-political party","I-political party","I-political party","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","political_party","location","person","election","organization","politician","event"]}
{"id":"96","dataset":"crossner_politics","split":"dev","instance":{"id":"96","prompt_labels":"Although(O) several(O) parties(O) are(O) typically(O) represented(O) in(O) parliament(O) ,(O) Canada(B-country) has(O) historically(O) had(O) two(O) dominant(O) political(O) parties(O) :(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) and(O) the(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) ((O) preceded(O) by(O) the(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) and(O) the(O) Conservative(B-political party) Party(I-political party) ((O) 1867-1942(O) )(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, political party, politician, election, location, country, event and O.\nSentence: Although several parties are typically represented in parliament , Canada has historically had two dominant political parties : the Liberal Party of Canada and the Conservative Party of Canada ( preceded by the Progressive Conservative Party of Canada and the Conservative Party ( 1867-1942 ) ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Although","several","parties","are","typically","represented","in","parliament",",","Canada","has","historically","had","two","dominant","political","parties",":","the","Liberal","Party","of","Canada","and","the","Conservative","Party","of","Canada","(","preceded","by","the","Progressive","Conservative","Party","of","Canada","and","the","Conservative","Party","(","1867-1942",")",")","."],"labels":["O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","person","political_party","politician","election","location","country","event"]}
{"id":"99","dataset":"crossner_politics","split":"dev","instance":{"id":"99","prompt_labels":"Australia(B-country) has(O) a(O) de(O) facto(O) two-party(O) system(O) ,(O) with(O) the(O) Australian(B-political party) Labor(I-political party) Party(I-political party) and(O) the(O) Coalition(O) of(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) ,(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) ,(O) the(O) Liberal(B-political party) National(I-political party) Party(I-political party) of(I-political party) Queensland(I-political party) and(O) Country(B-political party) Liberal(I-political party) Party(I-political party) dominating(O) Parliamentary(O) elections(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, location, person, political party, politician, organization, election and O.\nSentence: Australia has a de facto two-party system , with the Australian Labor Party and the Coalition of the Liberal Party of Australia , National Party of Australia , the Liberal National Party of Queensland and Country Liberal Party dominating Parliamentary elections .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Australia","has","a","de","facto","two-party","system",",","with","the","Australian","Labor","Party","and","the","Coalition","of","the","Liberal","Party","of","Australia",",","National","Party","of","Australia",",","the","Liberal","National","Party","of","Queensland","and","Country","Liberal","Party","dominating","Parliamentary","elections","."],"labels":["B-country","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","event","location","person","political_party","politician","organization","election"]}
{"id":"100","dataset":"crossner_politics","split":"dev","instance":{"id":"100","prompt_labels":"ALP(B-political party) =(O) Australian(B-political party) Labor(I-political party) Party(I-political party) ,(O) L(O) +(O) NP(O) =(O) grouping(O) of(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) /(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) /(O) Liberal(B-political party) National(I-political party) Party(I-political party) of(I-political party) Queensland(I-political party) /(O) Country(B-political party) Liberal(I-political party) Party(I-political party) Coalition(O) parties(O) ((O) and(O) predecessors(O) )(O) ,(O) Oth(O) =(O) other(O) parties(O) and(O) independents(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, country, election, politician, location, organization, political party and O.\nSentence: ALP = Australian Labor Party , L + NP = grouping of Liberal Party of Australia / National Party of Australia / Liberal National Party of Queensland / Country Liberal Party Coalition parties ( and predecessors ) , Oth = other parties and independents .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["ALP","=","Australian","Labor","Party",",","L","+","NP","=","grouping","of","Liberal","Party","of","Australia","/","National","Party","of","Australia","/","Liberal","National","Party","of","Queensland","/","Country","Liberal","Party","Coalition","parties","(","and","predecessors",")",",","Oth","=","other","parties","and","independents","."],"labels":["B-political party","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","person","country","election","politician","location","organization","political_party"]}
{"id":"101","dataset":"crossner_politics","split":"dev","instance":{"id":"101","prompt_labels":"The(O) five(O) largest(O) parties(O) in(O) the(O) election(O) were(O) the(O) National(B-political party) Party(I-political party) of(I-political party) Indonesia(I-political party) ((O) Partai(B-political party) Nasional(I-political party) Indonesia(I-political party) )(O) ,(O) Masyumi(B-political party) ,(O) Nahdlatul(B-political party) Ulama(I-political party) ,(O) the(O) Communist(B-political party) Party(I-political party) of(I-political party) Indonesia(I-political party) ((O) Partai(B-political party) Komunis(I-political party) Indonesia(I-political party) ,(O) PKI(B-political party) )(O) ,(O) and(O) the(O) Indonesian(B-political party) Islamic(I-political party) Union(I-political party) Party(I-political party) ((O) Partai(B-political party) Sarekat(I-political party) Islam(I-political party) Indonesia(I-political party) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, election, event, organization, location, country, person and O.\nSentence: The five largest parties in the election were the National Party of Indonesia ( Partai Nasional Indonesia ) , Masyumi , Nahdlatul Ulama , the Communist Party of Indonesia ( Partai Komunis Indonesia , PKI ) , and the Indonesian Islamic Union Party ( Partai Sarekat Islam Indonesia ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","five","largest","parties","in","the","election","were","the","National","Party","of","Indonesia","(","Partai","Nasional","Indonesia",")",",","Masyumi",",","Nahdlatul","Ulama",",","the","Communist","Party","of","Indonesia","(","Partai","Komunis","Indonesia",",","PKI",")",",","and","the","Indonesian","Islamic","Union","Party","(","Partai","Sarekat","Islam","Indonesia",")","."],"labels":["O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","B-political party","O","B-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","politician","election","event","organization","location","country","person"]}
{"id":"103","dataset":"crossner_politics","split":"dev","instance":{"id":"103","prompt_labels":"There(O) are(O) numerous(O) other(O) groups(O) ,(O) including(O) communists(O) ,(O) European(B-political party) Green(I-political party) Party(I-political party) ,(O) European(B-political party) Free(I-political party) Alliance(I-political party) ,(O) conservatives(O) ,(O) Alliance(B-political party) of(I-political party) Liberals(I-political party) and(I-political party) Democrats(I-political party) for(I-political party) Europe(I-political party) and(O) eurosceptics(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, organization, country, location, politician, election, political party and O.\nSentence: There are numerous other groups , including communists , European Green Party , European Free Alliance , conservatives , Alliance of Liberals and Democrats for Europe and eurosceptics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["There","are","numerous","other","groups",",","including","communists",",","European","Green","Party",",","European","Free","Alliance",",","conservatives",",","Alliance","of","Liberals","and","Democrats","for","Europe","and","eurosceptics","."],"labels":["O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","event","organization","country","location","politician","election","political_party"]}
{"id":"104","dataset":"crossner_politics","split":"dev","instance":{"id":"104","prompt_labels":"Those(O) were(O) the(O) Croatian(B-political party) Democratic(I-political party) Union(I-political party) ,(O) the(O) Croatian(B-political party) Peasant(I-political party) Party(I-political party) ,(O) the(O) Croatian(B-political party) People(I-political party) 's(I-political party) Party(I-political party) -(I-political party) Liberal(I-political party) Democrats(I-political party) ,(O) the(O) Croatian(B-political party) Social(I-political party) Liberal(I-political party) Party(I-political party) ,(O) Social(B-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Croatia(I-political party) and(O) the(O) Bridge(B-political party) of(I-political party) Independent(I-political party) Lists(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, location, election, country, politician, person, organization and O.\nSentence: Those were the Croatian Democratic Union , the Croatian Peasant Party , the Croatian People 's Party - Liberal Democrats , the Croatian Social Liberal Party , Social Democratic Party of Croatia and the Bridge of Independent Lists .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Those","were","the","Croatian","Democratic","Union",",","the","Croatian","Peasant","Party",",","the","Croatian","People","'s","Party","-","Liberal","Democrats",",","the","Croatian","Social","Liberal","Party",",","Social","Democratic","Party","of","Croatia","and","the","Bridge","of","Independent","Lists","."],"labels":["O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["political_party","event","location","election","country","politician","person","organization"]}
{"id":"105","dataset":"crossner_politics","split":"dev","instance":{"id":"105","prompt_labels":"Those(O) are(O) ((O) in(O) alphabetical(O) order(O) )(O) :(O) the(O) Alliance(B-political party) of(I-political party) Primorje-Gorski(I-political party) Kotar(I-political party) ((O) previously(O) known(O) as(O) the(O) Rijeka(B-political party) Democratic(I-political party) Alliance(I-political party) )(O) ,(O) the(O) Croatian(B-political party) Christian(I-political party) Democratic(I-political party) Union(I-political party) ,(O) the(O) Croatian(B-political party) Citizen(I-political party) Party(I-political party) ,(O) the(O) Croatian(B-political party) Democratic(I-political party) Alliance(I-political party) of(I-political party) Slavonia(I-political party) and(I-political party) Baranja(I-political party) ,(O) the(O) Croatian(B-political party) Democratic(I-political party) Peasant(I-political party) Party(I-political party) ,(O) the(O) Croatian(B-political party) Independent(I-political party) Democrats(I-political party) ,(O) the(O) Croatian(B-political party) Party(I-political party) of(I-political party) Pensioners(I-political party) ,(O) the(O) Croatian(B-political party) Party(I-political party) of(I-political party) Rights(I-political party) ,(O) the(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, country, organization, person, event, location, politician, election and O.\nSentence: Those are ( in alphabetical order ) : the Alliance of Primorje-Gorski Kotar ( previously known as the Rijeka Democratic Alliance ) , the Croatian Christian Democratic Union , the Croatian Citizen Party , the Croatian Democratic Alliance of Slavonia and Baranja , the Croatian Democratic Peasant Party , the Croatian Independent Democrats , the Croatian Party of Pensioners , the Croatian Party of Rights , the .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Those","are","(","in","alphabetical","order",")",":","the","Alliance","of","Primorje-Gorski","Kotar","(","previously","known","as","the","Rijeka","Democratic","Alliance",")",",","the","Croatian","Christian","Democratic","Union",",","the","Croatian","Citizen","Party",",","the","Croatian","Democratic","Alliance","of","Slavonia","and","Baranja",",","the","Croatian","Democratic","Peasant","Party",",","the","Croatian","Independent","Democrats",",","the","Croatian","Party","of","Pensioners",",","the","Croatian","Party","of","Rights",",","the","."],"labels":["O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","country","organization","person","event","location","politician","election"]}
{"id":"106","dataset":"crossner_politics","split":"dev","instance":{"id":"106","prompt_labels":"Ante(B-politician) Starevi(I-politician) Croatian(B-political party) Party(I-political party) of(I-political party) Rights(I-political party) dr(I-political party) ,(O) Dalmatian(B-political party) Action(I-political party) ,(O) the(O) Democratic(B-political party) Centre(I-political party) ,(O) the(O) Istrian(B-political party) Democratic(I-political party) Assembly(I-political party) ,(O) the(O) Liberal(B-political party) Party(I-political party) ,(O) the(O) Party(B-political party) of(I-political party) Liberal(I-political party) Democrats(I-political party) ,(O) the(O) Serb(B-political party) Democratic(I-political party) Party(I-political party) ,(O) the(O) Slavonia-Baranja(B-political party) Croatian(I-political party) Party(I-political party) and(O) the(O) Social(B-political party) Democratic(I-political party) Action(I-political party) of(I-political party) Croatia(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, election, location, organization, political party, event, person and O.\nSentence: Ante Starevi Croatian Party of Rights dr , Dalmatian Action , the Democratic Centre , the Istrian Democratic Assembly , the Liberal Party , the Party of Liberal Democrats , the Serb Democratic Party , the Slavonia-Baranja Croatian Party and the Social Democratic Action of Croatia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Ante","Starevi","Croatian","Party","of","Rights","dr",",","Dalmatian","Action",",","the","Democratic","Centre",",","the","Istrian","Democratic","Assembly",",","the","Liberal","Party",",","the","Party","of","Liberal","Democrats",",","the","Serb","Democratic","Party",",","the","Slavonia-Baranja","Croatian","Party","and","the","Social","Democratic","Action","of","Croatia","."],"labels":["B-politician","I-politician","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","O","O","B-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["politician","country","election","location","organization","political_party","event","person"]}
{"id":"112","dataset":"crossner_politics","split":"dev","instance":{"id":"112","prompt_labels":"In(O) 2017(B-election) Czech(I-election) legislative(I-election) election(I-election) Koruna(B-politician) esk(I-politician) together(O) with(O) Conservative(B-political party) Party(I-political party) and(O) Club(B-political party) of(I-political party) Committed(I-political party) Non-Party(I-political party) Members(I-political party) agreed(O) on(O) joint(O) endorsement(O) of(O) TOP(B-political party) 9(I-political party) ,(O) while(O) TOP(O) 9(O) added(O) candidates(O) of(O) the(O) smaller(O) parties(O) on(O) their(O) list(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, election, politician, location, event, person, organization, country and O.\nSentence: In 2017 Czech legislative election Koruna esk together with Conservative Party and Club of Committed Non-Party Members agreed on joint endorsement of TOP 9 , while TOP 9 added candidates of the smaller parties on their list .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2017","Czech","legislative","election","Koruna","esk","together","with","Conservative","Party","and","Club","of","Committed","Non-Party","Members","agreed","on","joint","endorsement","of","TOP","9",",","while","TOP","9","added","candidates","of","the","smaller","parties","on","their","list","."],"labels":["O","B-election","I-election","I-election","I-election","B-politician","I-politician","O","O","B-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","election","politician","location","event","person","organization","country"]}
{"id":"113","dataset":"crossner_politics","split":"dev","instance":{"id":"113","prompt_labels":"During(O) this(O) period(O) ,(O) despite(O) no(O) longer(O) being(O) in(O) charge(O) of(O) external(O) affairs(O) ,(O) Ismail(B-person) expressed(O) strong(O) support(O) for(O) an(O) Association(B-organization) of(I-organization) Southeast(I-organization) Asia(I-organization) ,(O) telling(O) the(O) media(O) that(O) We(O) look(O) forward(O) to(O) a(O) regional(O) association(O) embracing(O) Thailand(B-country) ,(O) Burma(B-country) ,(O) Indonesia(B-country) ,(O) Singapore(B-country) ,(O) Malaysia(B-country) ,(O) Philippines(B-country) ,(O) Cambodia(B-country) ,(O) Laos(B-country) and(O) Vietnam(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, country, political party, person, politician, election, organization and O.\nSentence: During this period , despite no longer being in charge of external affairs , Ismail expressed strong support for an Association of Southeast Asia , telling the media that We look forward to a regional association embracing Thailand , Burma , Indonesia , Singapore , Malaysia , Philippines , Cambodia , Laos and Vietnam .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","this","period",",","despite","no","longer","being","in","charge","of","external","affairs",",","Ismail","expressed","strong","support","for","an","Association","of","Southeast","Asia",",","telling","the","media","that","We","look","forward","to","a","regional","association","embracing","Thailand",",","Burma",",","Indonesia",",","Singapore",",","Malaysia",",","Philippines",",","Cambodia",",","Laos","and","Vietnam","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["location","event","country","political_party","person","politician","election","organization"]}
{"id":"118","dataset":"crossner_politics","split":"dev","instance":{"id":"118","prompt_labels":"Vice(O) presidents(O) Chen(B-politician) Cheng(I-politician) ,(O) Yen(B-politician) Chia-kan(I-politician) ,(O) and(O) Lien(B-politician) Chan(I-politician) all(O) served(O) as(O) premier(O) concurrently(O) as(O) vice(O) president(O) during(O) part(O) of(O) their(O) terms(O) ,(O) and(O) vice(O) president(O) Annette(B-politician) Lu(I-politician) has(O) at(O) times(O) been(O) mentioned(O) as(O) a(O) possible(O) candidate(O) for(O) premiership(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, organization, location, politician, election, political party, event and O.\nSentence: Vice presidents Chen Cheng , Yen Chia-kan , and Lien Chan all served as premier concurrently as vice president during part of their terms , and vice president Annette Lu has at times been mentioned as a possible candidate for premiership .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Vice","presidents","Chen","Cheng",",","Yen","Chia-kan",",","and","Lien","Chan","all","served","as","premier","concurrently","as","vice","president","during","part","of","their","terms",",","and","vice","president","Annette","Lu","has","at","times","been","mentioned","as","a","possible","candidate","for","premiership","."],"labels":["O","O","B-politician","I-politician","O","B-politician","I-politician","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","country","organization","location","politician","election","political_party","event"]}
{"id":"120","dataset":"crossner_politics","split":"dev","instance":{"id":"120","prompt_labels":"Malaya(B-country) united(O) with(O) Crown(B-country) Colony(I-country) of(I-country) North(I-country) Borneo(I-country) ,(O) Crown(B-country) Colony(I-country) of(I-country) Sarawak(I-country) ,(O) and(O) Colony(B-country) of(I-country) Singapore(I-country) on(O) 16(O) September(O) 1963(O) to(O) become(O) Malaysia(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, country, political party, person, event, location, organization and O.\nSentence: Malaya united with Crown Colony of North Borneo , Crown Colony of Sarawak , and Colony of Singapore on 16 September 1963 to become Malaysia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Malaya","united","with","Crown","Colony","of","North","Borneo",",","Crown","Colony","of","Sarawak",",","and","Colony","of","Singapore","on","16","September","1963","to","become","Malaysia","."],"labels":["B-country","O","O","B-country","I-country","I-country","I-country","I-country","O","B-country","I-country","I-country","I-country","O","O","B-country","I-country","I-country","O","O","O","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["election","politician","country","political_party","person","event","location","organization"]}
{"id":"121","dataset":"crossner_politics","split":"dev","instance":{"id":"121","prompt_labels":"It(O) is(O) a(O) founding(O) member(O) of(O) ASEAN(B-organization) ,(O) EAS(B-organization) ,(O) Organisation(B-organization) of(I-organization) Islamic(I-organization) Cooperation(I-organization) and(O) a(O) member(O) of(O) Asia-Pacific(B-organization) Economic(I-organization) Cooperation(I-organization) ,(O) the(O) Commonwealth(B-organization) of(I-organization) Nations(I-organization) and(O) the(O) Non-Aligned(B-organization) Movement(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, person, politician, election, political party, event, country and O.\nSentence: It is a founding member of ASEAN , EAS , Organisation of Islamic Cooperation and a member of Asia-Pacific Economic Cooperation , the Commonwealth of Nations and the Non-Aligned Movement .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","is","a","founding","member","of","ASEAN",",","EAS",",","Organisation","of","Islamic","Cooperation","and","a","member","of","Asia-Pacific","Economic","Cooperation",",","the","Commonwealth","of","Nations","and","the","Non-Aligned","Movement","."],"labels":["O","O","O","O","O","O","B-organization","O","B-organization","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["organization","location","person","politician","election","political_party","event","country"]}
{"id":"122","dataset":"crossner_politics","split":"dev","instance":{"id":"122","prompt_labels":"Stanley(B-politician) ((O) then(O) known(O) as(O) the(O) Honourable(O) Edward(B-politician) Lyulph(I-politician) Stanley(I-politician) )(O) contested(O) Oldham(B-location) ,(O) in(O) the(O) Liberal(B-political party) interest(O) ,(O) at(O) elections(O) in(O) 1872(O) ,(O) 1874(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) 1880(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 1885(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, person, organization, event, political party, politician, election and O.\nSentence: Stanley ( then known as the Honourable Edward Lyulph Stanley ) contested Oldham , in the Liberal interest , at elections in 1872 , 1874 United Kingdom general election , 1880 United Kingdom general election and 1885 United Kingdom general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Stanley","(","then","known","as","the","Honourable","Edward","Lyulph","Stanley",")","contested","Oldham",",","in","the","Liberal","interest",",","at","elections","in","1872",",","1874","United","Kingdom","general","election",",","1880","United","Kingdom","general","election","and","1885","United","Kingdom","general","election","."],"labels":["B-politician","O","O","O","O","O","O","B-politician","I-politician","I-politician","O","O","B-location","O","O","O","B-political party","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["location","country","person","organization","event","political_party","politician","election"]}
{"id":"123","dataset":"crossner_politics","split":"dev","instance":{"id":"123","prompt_labels":"Malagodi(B-politician) managed(O) to(O) draw(O) some(O) votes(O) from(O) the(O) Italian(B-political party) Social(I-political party) Movement(I-political party) ,(O) the(O) Monarchist(B-political party) National(I-political party) Party(I-political party) and(O) especially(O) Christian(O) Democracy(O) ,(O) whose(O) electoral(O) base(O) was(O) composed(O) also(O) by(O) conservatives(O) suspicious(O) of(O) the(O) Socialists(O) ,(O) increasing(O) the(O) party(O) 's(O) share(O) to(O) a(O) historical(O) record(O) of(O) 7.0(O) %(O) in(O) the(O) 1963(B-election) Italian(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, politician, election, political party, event, person, location and O.\nSentence: Malagodi managed to draw some votes from the Italian Social Movement , the Monarchist National Party and especially Christian Democracy , whose electoral base was composed also by conservatives suspicious of the Socialists , increasing the party 's share to a historical record of 7.0 % in the 1963 Italian general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Malagodi","managed","to","draw","some","votes","from","the","Italian","Social","Movement",",","the","Monarchist","National","Party","and","especially","Christian","Democracy",",","whose","electoral","base","was","composed","also","by","conservatives","suspicious","of","the","Socialists",",","increasing","the","party","'s","share","to","a","historical","record","of","7.0","%","in","the","1963","Italian","general","election","."],"labels":["B-politician","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["organization","country","politician","election","political_party","event","person","location"]}
{"id":"126","dataset":"crossner_politics","split":"dev","instance":{"id":"126","prompt_labels":"For(O) the(O) 2018(B-election) Pakistani(I-election) general(I-election) election(I-election) ,(O) PML-F(B-political party) lead(O) a(O) new(O) coalition(O) named(O) Grand(B-political party) Democratic(I-political party) Alliance(I-political party) with(O) Awami(B-political party) Tahreek(I-political party) ,(O) National(B-political party) Peoples(I-political party) Party(I-political party) ,(O) Pakistan(B-political party) Peoples(I-political party) Party(I-political party) Workers(I-political party) and(O) Pakistan(B-political party) Peoples(I-political party) Muslim(I-political party) League(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, location, event, person, politician, organization, election, country and O.\nSentence: For the 2018 Pakistani general election , PML-F lead a new coalition named Grand Democratic Alliance with Awami Tahreek , National Peoples Party , Pakistan Peoples Party Workers and Pakistan Peoples Muslim League .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","the","2018","Pakistani","general","election",",","PML-F","lead","a","new","coalition","named","Grand","Democratic","Alliance","with","Awami","Tahreek",",","National","Peoples","Party",",","Pakistan","Peoples","Party","Workers","and","Pakistan","Peoples","Muslim","League","."],"labels":["O","O","B-election","I-election","I-election","I-election","O","B-political party","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["political_party","location","event","person","politician","organization","election","country"]}
{"id":"128","dataset":"crossner_politics","split":"dev","instance":{"id":"128","prompt_labels":"The(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) government(O) of(O) the(O) time(O) ,(O) headed(O) by(O) Prime(O) Minister(O) John(B-politician) Diefenbaker(I-politician) ,(O) did(O) not(O) accept(O) the(O) invitation(O) to(O) establish(O) a(O) new(O) Canadian(O) flag(O) ,(O) so(O) Pearson(B-politician) made(O) it(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) policy(O) in(O) 1961(O) ,(O) and(O) part(O) of(O) the(O) party(O) 's(O) election(O) platform(O) in(O) the(O) 1962(B-election) Canadian(I-election) federal(I-election) election(I-election) and(O) 1963(B-election) Canadian(I-election) federal(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, political party, politician, event, organization, location, election and O.\nSentence: The Progressive Conservative Party of Canada government of the time , headed by Prime Minister John Diefenbaker , did not accept the invitation to establish a new Canadian flag , so Pearson made it Liberal Party of Canada policy in 1961 , and part of the party 's election platform in the 1962 Canadian federal election and 1963 Canadian federal election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Progressive","Conservative","Party","of","Canada","government","of","the","time",",","headed","by","Prime","Minister","John","Diefenbaker",",","did","not","accept","the","invitation","to","establish","a","new","Canadian","flag",",","so","Pearson","made","it","Liberal","Party","of","Canada","policy","in","1961",",","and","part","of","the","party","'s","election","platform","in","the","1962","Canadian","federal","election","and","1963","Canadian","federal","election","."],"labels":["O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["person","country","political_party","politician","event","organization","location","election"]}
{"id":"130","dataset":"crossner_politics","split":"dev","instance":{"id":"130","prompt_labels":"He(O) contested(O) Cardiff(B-politician) North(I-politician) in(O) October(B-election) 1974(I-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 1979(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) for(O) the(O) Liberals(B-political party) ,(O) before(O) fighting(O) Cardiff(B-politician) Central(I-politician) in(O) 1983(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 1987(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) for(O) the(O) SDP-Liberal(B-political party) Alliance(I-political party) ,(O) but(O) was(O) unsuccessful(O) on(O) each(O) occasion(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, political party, person, event, election, politician, location and O.\nSentence: He contested Cardiff North in October 1974 United Kingdom general election and 1979 United Kingdom general election for the Liberals , before fighting Cardiff Central in 1983 United Kingdom general election and 1987 United Kingdom general election for the SDP-Liberal Alliance , but was unsuccessful on each occasion .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","contested","Cardiff","North","in","October","1974","United","Kingdom","general","election","and","1979","United","Kingdom","general","election","for","the","Liberals",",","before","fighting","Cardiff","Central","in","1983","United","Kingdom","general","election","and","1987","United","Kingdom","general","election","for","the","SDP-Liberal","Alliance",",","but","was","unsuccessful","on","each","occasion","."],"labels":["O","O","B-politician","I-politician","O","B-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","B-political party","O","O","O","B-politician","I-politician","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","country","political_party","person","event","election","politician","location"]}
{"id":"132","dataset":"crossner_politics","split":"dev","instance":{"id":"132","prompt_labels":"She(O) was(O) re-elected(O) in(O) the(O) federal(O) general(O) elections(O) of(O) 1997(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2000(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2004(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2006(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) and(O) 2008(B-election) Canadian(I-election) federal(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, politician, organization, country, location, person, event and O.\nSentence: She was re-elected in the federal general elections of 1997 Canadian federal election , 2000 Canadian federal election , 2004 Canadian federal election , 2006 Canadian federal election , and 2008 Canadian federal election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","was","re-elected","in","the","federal","general","elections","of","1997","Canadian","federal","election",",","2000","Canadian","federal","election",",","2004","Canadian","federal","election",",","2006","Canadian","federal","election",",","and","2008","Canadian","federal","election","."],"labels":["O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["election","political_party","politician","organization","country","location","person","event"]}
{"id":"138","dataset":"crossner_politics","split":"dev","instance":{"id":"138","prompt_labels":"However(O) ,(O) in(O) the(O) 2004(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2006(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2008(B-election) Canadian(I-election) federal(I-election) election(I-election) and(O) 2011(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) the(O) entire(O) region(O) ,(O) incrementally(O) swung(O) away(O) from(O) the(O) Liberals(B-political party) to(O) support(O) the(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) ,(O) including(O) in(O) London(B-location) ,(O) where(O) three-way(O) vote-splitting(O) resulted(O) in(O) two(O) ridings(O) switching(O) from(O) Liberal(B-political party) to(O) Conservative(B-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, organization, location, political party, person, country, politician and O.\nSentence: However , in the 2004 Canadian federal election , 2006 Canadian federal election , 2008 Canadian federal election and 2011 Canadian federal election , the entire region , incrementally swung away from the Liberals to support the Conservative Party of Canada , including in London , where three-way vote-splitting resulted in two ridings switching from Liberal to Conservative .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["However",",","in","the","2004","Canadian","federal","election",",","2006","Canadian","federal","election",",","2008","Canadian","federal","election","and","2011","Canadian","federal","election",",","the","entire","region",",","incrementally","swung","away","from","the","Liberals","to","support","the","Conservative","Party","of","Canada",",","including","in","London",",","where","three-way","vote-splitting","resulted","in","two","ridings","switching","from","Liberal","to","Conservative","."],"labels":["O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","B-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","B-location","O","O","O","O","O","O","O","O","O","O","B-political party","O","B-political party","O"],"target_index":null,"target_label":null},"label_list":["election","event","organization","location","political_party","person","country","politician"]}
{"id":"145","dataset":"crossner_politics","split":"dev","instance":{"id":"145","prompt_labels":"Christian(B-organization) Voice(I-organization) was(O) the(O) first(O) of(O) the(O) Christian(O) Right(O) groups(O) ,(O) pre-dating(O) the(O) Christian(B-organization) Coalition(I-organization) of(I-organization) America(I-organization) ,(O) American(B-organization) Coalition(I-organization) for(I-organization) Traditional(I-organization) Values(I-organization) ,(O) Concerned(B-organization) Women(I-organization) for(I-organization) America(I-organization) ,(O) Moral(B-organization) Majority(I-organization) ,(O) Family(B-organization) Research(I-organization) Council(I-organization) ,(O) and(O) other(O) Christian(O) political(O) groups(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, political party, politician, event, election, person, location and O.\nSentence: Christian Voice was the first of the Christian Right groups , pre-dating the Christian Coalition of America , American Coalition for Traditional Values , Concerned Women for America , Moral Majority , Family Research Council , and other Christian political groups .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Christian","Voice","was","the","first","of","the","Christian","Right","groups",",","pre-dating","the","Christian","Coalition","of","America",",","American","Coalition","for","Traditional","Values",",","Concerned","Women","for","America",",","Moral","Majority",",","Family","Research","Council",",","and","other","Christian","political","groups","."],"labels":["B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","country","political_party","politician","event","election","person","location"]}
{"id":"146","dataset":"crossner_politics","split":"dev","instance":{"id":"146","prompt_labels":"In(O) Kota(B-location) Kinabalu(I-location) ,(O) United(B-political party) Pasokmomogun(I-political party) Kadazandusun(I-political party) Murut(I-political party) Organisation(I-political party) ((O) UPKO(B-political party) )(O) led(O) by(O) its(O) Secretary-General(O) Datuk(O) Wilfred(B-politician) Madius(I-politician) Tangau(I-politician) ,(O) on(O) 23(O) September(O) 2008(O) ,(O) joined(O) its(O) 3(O) other(O) Barisan(B-political party) Nasional(I-political party) ((O) BN(B-political party) )(O) counterparts(O) Malaysian(B-political party) Chinese(I-political party) Association(I-political party) ,(O) Parti(B-political party) Gerakan(I-political party) Rakyat(I-political party) Malaysia(I-political party) and(O) Malaysian(B-political party) Indian(I-political party) Congress(I-political party) ,(O) petitioning(O) Government(O) review(O) of(O) ISA(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, event, country, location, political party, election, person and O.\nSentence: In Kota Kinabalu , United Pasokmomogun Kadazandusun Murut Organisation ( UPKO ) led by its Secretary-General Datuk Wilfred Madius Tangau , on 23 September 2008 , joined its 3 other Barisan Nasional ( BN ) counterparts Malaysian Chinese Association , Parti Gerakan Rakyat Malaysia and Malaysian Indian Congress , petitioning Government review of ISA .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","Kota","Kinabalu",",","United","Pasokmomogun","Kadazandusun","Murut","Organisation","(","UPKO",")","led","by","its","Secretary-General","Datuk","Wilfred","Madius","Tangau",",","on","23","September","2008",",","joined","its","3","other","Barisan","Nasional","(","BN",")","counterparts","Malaysian","Chinese","Association",",","Parti","Gerakan","Rakyat","Malaysia","and","Malaysian","Indian","Congress",",","petitioning","Government","review","of","ISA","."],"labels":["O","B-location","I-location","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","B-politician","I-politician","I-politician","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","B-political party","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","politician","event","country","location","political_party","election","person"]}
{"id":"147","dataset":"crossner_politics","split":"dev","instance":{"id":"147","prompt_labels":"In(O) the(O) 1969(B-election) Malaysian(I-election) general(I-election) election(I-election) ,(O) MCA(B-political party) lost(O) more(O) than(O) half(O) its(O) seats(O) to(O) the(O) new(O) ,(O) mainly(O) Chinese(O) Malaysian(O) ,(O) opposition(O) parties(O) Democratic(B-political party) Action(I-political party) Party(I-political party) ((O) DAP(B-political party) )(O) and(O) Parti(B-political party) Gerakan(I-political party) Rakyat(I-political party) Malaysia(I-political party) ((O) Gerakan(B-political party) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, politician, political party, location, person, organization, event and O.\nSentence: In the 1969 Malaysian general election , MCA lost more than half its seats to the new , mainly Chinese Malaysian , opposition parties Democratic Action Party ( DAP ) and Parti Gerakan Rakyat Malaysia ( Gerakan ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","1969","Malaysian","general","election",",","MCA","lost","more","than","half","its","seats","to","the","new",",","mainly","Chinese","Malaysian",",","opposition","parties","Democratic","Action","Party","(","DAP",")","and","Parti","Gerakan","Rakyat","Malaysia","(","Gerakan",")","."],"labels":["O","O","B-election","I-election","I-election","I-election","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O"],"target_index":null,"target_label":null},"label_list":["country","election","politician","political_party","location","person","organization","event"]}
{"id":"149","dataset":"crossner_politics","split":"dev","instance":{"id":"149","prompt_labels":"Patteson(B-person) was(O) a(O) member(O) of(O) the(O) Board(O) of(O) Trustees(O) of(O) West(B-organization) Virginia(I-organization) Wesleyan(I-organization) ,(O) and(O) of(O) a(O) number(O) of(O) societies(O) :(O) Free(B-organization) mason(I-organization) s(O) ,(O) Knights(B-organization) Templar(I-organization) ,(O) Moose(B-organization) International(I-organization) ,(O) Lions(B-organization) Clubs(I-organization) International(I-organization) ,(O) Chamber(B-organization) of(I-organization) Commerce(I-organization) ,(O) American(B-organization) Legion(I-organization) ,(O) Sons(B-organization) of(I-organization) the(I-organization) American(I-organization) Revolution(I-organization) and(O) Benevolent(B-organization) and(I-organization) Protective(I-organization) Order(I-organization) of(I-organization) Elks(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, politician, person, organization, country, event, location and O.\nSentence: Patteson was a member of the Board of Trustees of West Virginia Wesleyan , and of a number of societies : Free mason s , Knights Templar , Moose International , Lions Clubs International , Chamber of Commerce , American Legion , Sons of the American Revolution and Benevolent and Protective Order of Elks .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Patteson","was","a","member","of","the","Board","of","Trustees","of","West","Virginia","Wesleyan",",","and","of","a","number","of","societies",":","Free","mason","s",",","Knights","Templar",",","Moose","International",",","Lions","Clubs","International",",","Chamber","of","Commerce",",","American","Legion",",","Sons","of","the","American","Revolution","and","Benevolent","and","Protective","Order","of","Elks","."],"labels":["B-person","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","B-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["election","political_party","politician","person","organization","country","event","location"]}
{"id":"151","dataset":"crossner_politics","split":"dev","instance":{"id":"151","prompt_labels":"Before(O) the(O) 2000(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) West(B-location) Virginia(I-location) had(O) been(O) won(O) by(O) the(O) Democratic(O) nominee(O) every(O) time(O) since(O) 1932(O) ((O) except(O) for(O) the(O) Republican(O) landslides(O) of(O) 1956(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) 1972(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) and(O) 1984(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, event, country, organization, person, political party, location and O.\nSentence: Before the 2000 United States presidential election , West Virginia had been won by the Democratic nominee every time since 1932 ( except for the Republican landslides of 1956 United States presidential election , 1972 United States presidential election , and 1984 United States presidential election ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Before","the","2000","United","States","presidential","election",",","West","Virginia","had","been","won","by","the","Democratic","nominee","every","time","since","1932","(","except","for","the","Republican","landslides","of","1956","United","States","presidential","election",",","1972","United","States","presidential","election",",","and","1984","United","States","presidential","election",")","."],"labels":["O","O","B-election","I-election","I-election","I-election","I-election","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O","O"],"target_index":null,"target_label":null},"label_list":["politician","election","event","country","organization","person","political_party","location"]}
{"id":"152","dataset":"crossner_politics","split":"dev","instance":{"id":"152","prompt_labels":"Molloy(B-politician) ran(O) for(O) the(O) Canadian(B-organization) House(I-organization) of(I-organization) Commons(I-organization) in(O) the(O) 1921(B-election) Canadian(I-election) federal(I-election) election(I-election) as(O) a(O) candidate(O) of(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) ,(O) and(O) lost(O) to(O) Progressive(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) candidate(O) Robert(B-politician) Alexander(I-politician) Hoey(I-politician) by(O) 1,397(O) votes(O) in(O) the(O) riding(O) of(O) Springfield(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, event, politician, country, location, person, political party and O.\nSentence: Molloy ran for the Canadian House of Commons in the 1921 Canadian federal election as a candidate of the Liberal Party of Canada , and lost to Progressive Party of Canada candidate Robert Alexander Hoey by 1,397 votes in the riding of Springfield .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Molloy","ran","for","the","Canadian","House","of","Commons","in","the","1921","Canadian","federal","election","as","a","candidate","of","the","Liberal","Party","of","Canada",",","and","lost","to","Progressive","Party","of","Canada","candidate","Robert","Alexander","Hoey","by","1,397","votes","in","the","riding","of","Springfield","."],"labels":["B-politician","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-politician","I-politician","I-politician","O","O","O","O","O","O","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["election","organization","event","politician","country","location","person","political_party"]}
{"id":"157","dataset":"crossner_politics","split":"dev","instance":{"id":"157","prompt_labels":"Despite(O) the(O) unsuccessful(O) attempt(O) at(O) the(O) 1987(B-election) Sarawak(I-election) state(I-election) election(I-election) ,(O) Abdul(B-politician) Rahman(I-politician) continued(O) his(O) struggle(O) with(O) his(O) allies(O) ,(O) Sarawak(B-location) Dayak(B-political party) People(I-political party) 's(I-political party) Party(I-political party) against(O) Taib(B-politician) 's(O) led(O) Sarawak(B-location) Barisan(B-political party) Nasional(I-political party) until(O) 1991(B-election) Sarawak(I-election) state(I-election) election(I-election) when(O) Taib(B-politician) 's(O) coalition(O) won(O) an(O) overwhelming(O) majority(O) of(O) 49(O) out(O) of(O) 56(O) seats(O) in(O) the(O) state(O) assembly(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, election, politician, political party, event, location, person, country and O.\nSentence: Despite the unsuccessful attempt at the 1987 Sarawak state election , Abdul Rahman continued his struggle with his allies , Sarawak Dayak People 's Party against Taib 's led Sarawak Barisan Nasional until 1991 Sarawak state election when Taib 's coalition won an overwhelming majority of 49 out of 56 seats in the state assembly .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Despite","the","unsuccessful","attempt","at","the","1987","Sarawak","state","election",",","Abdul","Rahman","continued","his","struggle","with","his","allies",",","Sarawak","Dayak","People","'s","Party","against","Taib","'s","led","Sarawak","Barisan","Nasional","until","1991","Sarawak","state","election","when","Taib","'s","coalition","won","an","overwhelming","majority","of","49","out","of","56","seats","in","the","state","assembly","."],"labels":["O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-politician","I-politician","O","O","O","O","O","O","O","B-location","B-political party","I-political party","I-political party","I-political party","O","B-politician","O","O","B-location","B-political party","I-political party","O","B-election","I-election","I-election","I-election","O","B-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","election","politician","political_party","event","location","person","country"]}
{"id":"158","dataset":"crossner_politics","split":"dev","instance":{"id":"158","prompt_labels":"In(O) British(B-location) Columbia(I-location) ,(O) the(O) British(B-political party) Columbia(I-political party) Social(I-political party) Credit(I-political party) Party(I-political party) was(O) replaced(O) as(O) the(O) party(O) of(O) the(O) centre-right(O) by(O) the(O) British(B-political party) Columbia(I-political party) Liberal(I-political party) Party(I-political party) ,(O) and(O) in(O) Alberta(B-location) the(O) Alberta(B-political party) Social(I-political party) Credit(I-political party) Party(I-political party) were(O) completely(O) annihilated(O) by(O) the(O) more(O) moderate(O) Alberta(B-political party) Progressive(I-political party) Conservative(I-political party) Party(I-political party) ,(O) leaving(O) both(O) parties(O) as(O) marginal(O) political(O) forces(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, election, politician, location, country, event, person and O.\nSentence: In British Columbia , the British Columbia Social Credit Party was replaced as the party of the centre-right by the British Columbia Liberal Party , and in Alberta the Alberta Social Credit Party were completely annihilated by the more moderate Alberta Progressive Conservative Party , leaving both parties as marginal political forces .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","British","Columbia",",","the","British","Columbia","Social","Credit","Party","was","replaced","as","the","party","of","the","centre-right","by","the","British","Columbia","Liberal","Party",",","and","in","Alberta","the","Alberta","Social","Credit","Party","were","completely","annihilated","by","the","more","moderate","Alberta","Progressive","Conservative","Party",",","leaving","both","parties","as","marginal","political","forces","."],"labels":["O","B-location","I-location","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","B-location","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","organization","election","politician","location","country","event","person"]}
{"id":"159","dataset":"crossner_politics","split":"dev","instance":{"id":"159","prompt_labels":"AD(B-political party) members(O) were(O) mainly(O) former(O) Italian(B-political party) Republican(I-political party) Party(I-political party) and(O) former(O) Italian(B-political party) Socialist(I-political party) Party(I-political party) ,(O) was(O) a(O) former(O) member(O) of(O) the(O) Italian(B-political party) Communist(I-political party) Party(I-political party) and(O) the(O) Democratic(B-political party) Party(I-political party) of(I-political party) the(I-political party) Left(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, person, organization, political party, location, politician, event, country and O.\nSentence: AD members were mainly former Italian Republican Party and former Italian Socialist Party , was a former member of the Italian Communist Party and the Democratic Party of the Left .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["AD","members","were","mainly","former","Italian","Republican","Party","and","former","Italian","Socialist","Party",",","was","a","former","member","of","the","Italian","Communist","Party","and","the","Democratic","Party","of","the","Left","."],"labels":["B-political party","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["election","person","organization","political_party","location","politician","event","country"]}
{"id":"160","dataset":"crossner_politics","split":"dev","instance":{"id":"160","prompt_labels":"The(O) party(O) ran(O) in(O) the(O) 1994(B-election) Italian(I-election) general(I-election) election(I-election) within(O) the(O) Alliance(B-political party) of(I-political party) Progressives(I-political party) and(O) obtained(O) a(O) mere(O) 1.2(O) %(O) of(O) the(O) vote(O) ,(O) due(O) to(O) the(O) uneasy(O) alliance(O) with(O) the(O) traditional(O) left(O) and(O) the(O) competition(O) by(O) Silvio(B-politician) Berlusconi(I-politician) '(O) s(O) Forza(B-political party) Italia(I-political party) ,(O) which(O) embraced(O) most(O) of(O) AD(B-political party) 's(O) policies(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, event, politician, location, country, organization, election and O.\nSentence: The party ran in the 1994 Italian general election within the Alliance of Progressives and obtained a mere 1.2 % of the vote , due to the uneasy alliance with the traditional left and the competition by Silvio Berlusconi ' s Forza Italia , which embraced most of AD 's policies .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","party","ran","in","the","1994","Italian","general","election","within","the","Alliance","of","Progressives","and","obtained","a","mere","1.2","%","of","the","vote",",","due","to","the","uneasy","alliance","with","the","traditional","left","and","the","competition","by","Silvio","Berlusconi","'","s","Forza","Italia",",","which","embraced","most","of","AD","'s","policies","."],"labels":["O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","B-political party","I-political party","O","O","O","O","O","B-political party","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","political_party","event","politician","location","country","organization","election"]}
{"id":"163","dataset":"crossner_politics","split":"dev","instance":{"id":"163","prompt_labels":"The(O) Unionist(B-political party) Party(I-political party) ((O) ,(O) UP(B-political party) )(O) was(O) a(O) pre-apartheid(O) South(O) African(O) political(O) party(O) ,(O) which(O) contested(O) elections(O) to(O) the(O) Union(B-country) of(I-country) South(I-country) Africa(I-country) parliament(O) from(O) the(O) 1910(B-election) South(I-election) African(I-election) general(I-election) election(I-election) until(O) its(O) merger(O) into(O) the(O) South(B-political party) African(I-political party) Party(I-political party) just(O) before(O) the(O) 1921(B-election) South(I-election) African(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, event, election, organization, location, political party, politician and O.\nSentence: The Unionist Party ( , UP ) was a pre-apartheid South African political party , which contested elections to the Union of South Africa parliament from the 1910 South African general election until its merger into the South African Party just before the 1921 South African general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Unionist","Party","(",",","UP",")","was","a","pre-apartheid","South","African","political","party",",","which","contested","elections","to","the","Union","of","South","Africa","parliament","from","the","1910","South","African","general","election","until","its","merger","into","the","South","African","Party","just","before","the","1921","South","African","general","election","."],"labels":["O","B-political party","I-political party","O","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","I-country","I-country","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["person","country","event","election","organization","location","political_party","politician"]}
{"id":"164","dataset":"crossner_politics","split":"dev","instance":{"id":"164","prompt_labels":"The(O) Socialist(B-political party) Party(I-political party) of(I-political party) the(I-political party) United(I-political party) States(I-political party) ((O) SPUS(B-political party) )(O) -(O) its(O) name(O) inspired(O) by(O) co-thinkers(O) in(O) the(O) Socialist(B-political party) Party(I-political party) of(I-political party) Great(I-political party) Britain(I-political party) ((O) SPGB(B-political party) )(O) and(O) the(O) original(O) ((O) non-WSM(O) )(O) Socialist(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) ((O) SPC(B-political party) )(O) -(O) was(O) established(O) on(O) July(O) 7(O) ,(O) 1916(O) by(O) 42(O) defecting(O) members(O) of(O) Local(B-location) Detroit(I-location) of(O) the(O) Socialist(B-political party) Party(I-political party) of(I-political party) America(I-political party) ((O) SPA(B-political party) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, organization, country, location, election, political party, event and O.\nSentence: The Socialist Party of the United States ( SPUS ) - its name inspired by co-thinkers in the Socialist Party of Great Britain ( SPGB ) and the original ( non-WSM ) Socialist Party of Canada ( SPC ) - was established on July 7 , 1916 by 42 defecting members of Local Detroit of the Socialist Party of America ( SPA ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Socialist","Party","of","the","United","States","(","SPUS",")","-","its","name","inspired","by","co-thinkers","in","the","Socialist","Party","of","Great","Britain","(","SPGB",")","and","the","original","(","non-WSM",")","Socialist","Party","of","Canada","(","SPC",")","-","was","established","on","July","7",",","1916","by","42","defecting","members","of","Local","Detroit","of","the","Socialist","Party","of","America","(","SPA",")","."],"labels":["O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O"],"target_index":null,"target_label":null},"label_list":["politician","person","organization","country","location","election","political_party","event"]}
{"id":"165","dataset":"crossner_politics","split":"dev","instance":{"id":"165","prompt_labels":"He(O) was(O) elected(O) as(O) a(O) Social(B-organization) Credit(I-organization) MLA(I-organization) in(O) Vancouver(B-location) South(I-location) in(O) 1975(B-election) British(I-election) Columbia(I-election) general(I-election) election(I-election) ,(O) 1979(B-election) British(I-election) Columbia(I-election) general(I-election) election(I-election) ,(O) 1983(B-election) British(I-election) Columbia(I-election) general(I-election) election(I-election) and(O) 1986(B-election) British(I-election) Columbia(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, location, politician, person, political party, election, event and O.\nSentence: He was elected as a Social Credit MLA in Vancouver South in 1975 British Columbia general election , 1979 British Columbia general election , 1983 British Columbia general election and 1986 British Columbia general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","elected","as","a","Social","Credit","MLA","in","Vancouver","South","in","1975","British","Columbia","general","election",",","1979","British","Columbia","general","election",",","1983","British","Columbia","general","election","and","1986","British","Columbia","general","election","."],"labels":["O","O","O","O","O","B-organization","I-organization","I-organization","O","B-location","I-location","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["country","organization","location","politician","person","political_party","election","event"]}
{"id":"166","dataset":"crossner_politics","split":"dev","instance":{"id":"166","prompt_labels":"He(O) ran(O) as(O) the(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) candidate(O) for(O) the(O) riding(O) of(O) Vancouver(B-location) Quadra(I-location) in(O) the(O) 2004(B-election) Canadian(I-election) federal(I-election) election(I-election) and(O) again(O) in(O) 2006(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) losing(O) both(O) times(O) to(O) Liberal(O) Stephen(B-politician) Owen(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, politician, event, election, person, country, location and O.\nSentence: He ran as the Conservative Party of Canada candidate for the riding of Vancouver Quadra in the 2004 Canadian federal election and again in 2006 Canadian federal election , losing both times to Liberal Stephen Owen .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","ran","as","the","Conservative","Party","of","Canada","candidate","for","the","riding","of","Vancouver","Quadra","in","the","2004","Canadian","federal","election","and","again","in","2006","Canadian","federal","election",",","losing","both","times","to","Liberal","Stephen","Owen","."],"labels":["O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","B-location","I-location","O","O","B-election","I-election","I-election","I-election","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["political_party","organization","politician","event","election","person","country","location"]}
{"id":"167","dataset":"crossner_politics","split":"dev","instance":{"id":"167","prompt_labels":"During(O) the(O) 2005(B-election) German(I-election) federal(I-election) election(I-election) campaign(O) ,(O) Angela(B-politician) Merkel(I-politician) ,(O) leader(O) of(O) the(O) Christian(B-political party) Democratic(I-political party) Union(I-political party) of(I-political party) Germany(I-political party) /(O) Christian(B-political party) Social(I-political party) Union(I-political party) in(I-political party) Bavaria(I-political party) ,(O) announced(O) that(O) Kirchhof(B-politician) would(O) serve(O) as(O) minister(O) of(O) finance(O) if(O) she(O) formed(O) a(O) government(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, political party, country, event, organization, location, person and O.\nSentence: During the 2005 German federal election campaign , Angela Merkel , leader of the Christian Democratic Union of Germany / Christian Social Union in Bavaria , announced that Kirchhof would serve as minister of finance if she formed a government .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","the","2005","German","federal","election","campaign",",","Angela","Merkel",",","leader","of","the","Christian","Democratic","Union","of","Germany","/","Christian","Social","Union","in","Bavaria",",","announced","that","Kirchhof","would","serve","as","minister","of","finance","if","she","formed","a","government","."],"labels":["O","O","B-election","I-election","I-election","I-election","O","O","B-politician","I-politician","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","B-politician","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","politician","political_party","country","event","organization","location","person"]}
{"id":"168","dataset":"crossner_politics","split":"dev","instance":{"id":"168","prompt_labels":"Being(O) a(O) political(O) club(O) with(O) the(O) intent(O) of(O) assuring(O) a(O) strong(O) left-wing(O) presence(O) in(O) a(O) party(O) and(O) to(O) influence(O) it(O) ,(O) its(O) nature(O) is(O) notably(O) reminiscent(O) of(O) the(O) New(B-political party) Democratic(I-political party) Party(I-political party) '(O) s(O) New(B-political party) Politics(I-political party) Initiative(I-political party) or(O) ,(O) to(O) a(O) lesser(O) extent(O) ,(O) of(O) the(O) New(B-organization) Democratic(I-organization) Party(I-organization) Socialist(I-organization) Caucus(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, political party, election, country, event, organization, location and O.\nSentence: Being a political club with the intent of assuring a strong left-wing presence in a party and to influence it , its nature is notably reminiscent of the New Democratic Party ' s New Politics Initiative or , to a lesser extent , of the New Democratic Party Socialist Caucus .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Being","a","political","club","with","the","intent","of","assuring","a","strong","left-wing","presence","in","a","party","and","to","influence","it",",","its","nature","is","notably","reminiscent","of","the","New","Democratic","Party","'","s","New","Politics","Initiative","or",",","to","a","lesser","extent",",","of","the","New","Democratic","Party","Socialist","Caucus","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["person","politician","political_party","election","country","event","organization","location"]}
{"id":"170","dataset":"crossner_politics","split":"dev","instance":{"id":"170","prompt_labels":"The(O) seat(O) was(O) retained(O) in(O) elections(O) in(O) 1990(B-election) Costa(I-election) Rican(I-election) general(I-election) election(I-election) and(O) 1994(B-election) Costa(I-election) Rican(I-election) general(I-election) election(I-election) ,(O) but(O) a(O) loss(O) of(O) support(O) in(O) the(O) 1998(B-election) Costa(I-election) Rican(I-election) general(I-election) election(I-election) saw(O) its(O) share(O) of(O) the(O) vote(O) drop(O) to(O) 0.5(O) %(O) ,(O) resulting(O) in(O) it(O) losing(O) its(O) solitary(O) seat(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, organization, politician, event, location, election, political party and O.\nSentence: The seat was retained in elections in 1990 Costa Rican general election and 1994 Costa Rican general election , but a loss of support in the 1998 Costa Rican general election saw its share of the vote drop to 0.5 % , resulting in it losing its solitary seat .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","seat","was","retained","in","elections","in","1990","Costa","Rican","general","election","and","1994","Costa","Rican","general","election",",","but","a","loss","of","support","in","the","1998","Costa","Rican","general","election","saw","its","share","of","the","vote","drop","to","0.5","%",",","resulting","in","it","losing","its","solitary","seat","."],"labels":["O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","country","organization","politician","event","location","election","political_party"]}
{"id":"171","dataset":"crossner_politics","split":"dev","instance":{"id":"171","prompt_labels":"Conversely(O) ,(O) many(O) Indo-Fijian(O) supporters(O) of(O) the(O) National(B-political party) Federation(I-political party) Party(I-political party) ((O) NFP(B-political party) )(O) in(O) the(O) 2001(B-election) Fijian(I-election) general(I-election) election(I-election) may(O) not(O) have(O) been(O) aware(O) that(O) votes(O) for(O) NFP(B-political party) candidates(O) ,(O) all(O) of(O) whom(O) lost(O) ,(O) were(O) to(O) be(O) transferred(O) to(O) the(O) indigenous-dominated(O) Soqosoqo(B-political party) Duavata(I-political party) ni(I-political party) Lewenivanua(I-political party) ((O) SDL(B-political party) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, country, organization, location, election, person, event and O.\nSentence: Conversely , many Indo-Fijian supporters of the National Federation Party ( NFP ) in the 2001 Fijian general election may not have been aware that votes for NFP candidates , all of whom lost , were to be transferred to the indigenous-dominated Soqosoqo Duavata ni Lewenivanua ( SDL ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Conversely",",","many","Indo-Fijian","supporters","of","the","National","Federation","Party","(","NFP",")","in","the","2001","Fijian","general","election","may","not","have","been","aware","that","votes","for","NFP","candidates",",","all","of","whom","lost",",","were","to","be","transferred","to","the","indigenous-dominated","Soqosoqo","Duavata","ni","Lewenivanua","(","SDL",")","."],"labels":["O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","politician","country","organization","location","election","person","event"]}
{"id":"173","dataset":"crossner_politics","split":"dev","instance":{"id":"173","prompt_labels":"Palestinian(O) groups(O) that(O) have(O) been(O) involved(O) in(O) politically(O) motivated(O) violence(O) include(O) the(O) Palestinian(B-organization) Liberation(I-organization) Organization(I-organization) ((O) PLO(B-organization) )(O) ,(O) Fatah(B-political party) ,(O) the(O) Popular(B-political party) Front(I-political party) for(I-political party) the(I-political party) Liberation(I-political party) of(I-political party) Palestine(I-political party) ((O) PFLP(B-political party) )(O) ,(O) the(O) Popular(B-organization) Front(I-organization) for(I-organization) the(I-organization) Liberation(I-organization) of(I-organization) Palestine(I-organization) -(I-organization) General(I-organization) Command(I-organization) ((O) PFLP-GC(B-organization) )(O) ,(O) the(O) Democratic(B-political party) Front(I-political party) for(I-political party) the(I-political party) Liberation(I-political party) of(I-political party) Palestine(I-political party) ,(O) the(O) Abu(B-organization) Nidal(I-organization) Organization(I-organization) ,(O) the(O) Palestinian(B-organization) Islamic(I-organization) Jihad(I-organization) ,(O) and(O) Hamas(B-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, election, politician, political party, country, organization, event and O.\nSentence: Palestinian groups that have been involved in politically motivated violence include the Palestinian Liberation Organization ( PLO ) , Fatah , the Popular Front for the Liberation of Palestine ( PFLP ) , the Popular Front for the Liberation of Palestine - General Command ( PFLP-GC ) , the Democratic Front for the Liberation of Palestine , the Abu Nidal Organization , the Palestinian Islamic Jihad , and Hamas .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Palestinian","groups","that","have","been","involved","in","politically","motivated","violence","include","the","Palestinian","Liberation","Organization","(","PLO",")",",","Fatah",",","the","Popular","Front","for","the","Liberation","of","Palestine","(","PFLP",")",",","the","Popular","Front","for","the","Liberation","of","Palestine","-","General","Command","(","PFLP-GC",")",",","the","Democratic","Front","for","the","Liberation","of","Palestine",",","the","Abu","Nidal","Organization",",","the","Palestinian","Islamic","Jihad",",","and","Hamas","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O","B-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","O"],"target_index":null,"target_label":null},"label_list":["location","person","election","politician","political_party","country","organization","event"]}
{"id":"174","dataset":"crossner_politics","split":"dev","instance":{"id":"174","prompt_labels":"The(O) body(O) thus(O) oversaw(O) the(O) activities(O) of(O) the(O) Bulgarian(B-political party) Communist(I-political party) Party(I-political party) ((O) BKP(B-political party) )(O) ,(O) the(O) League(B-political party) of(I-political party) Communists(I-political party) of(I-political party) Yugoslavia(I-political party) ((O) KPJ(B-political party) )(O) ,(O) the(O) Communist(B-political party) Party(I-political party) of(I-political party) Greece(I-political party) ((O) KKE(B-political party) )(O) ,(O) the(O) Communist(B-political party) Party(I-political party) of(I-political party) Turkey(I-political party) ((O) TKP(B-political party) )(O) ,(O) and(O) ,(O) to(O) a(O) certain(O) measure(O) ,(O) those(O) of(O) the(O) Romanian(B-political party) Communist(I-political party) Party(I-political party) ((O) PCdR(B-political party) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, country, event, person, location, politician, political party, organization and O.\nSentence: The body thus oversaw the activities of the Bulgarian Communist Party ( BKP ) , the League of Communists of Yugoslavia ( KPJ ) , the Communist Party of Greece ( KKE ) , the Communist Party of Turkey ( TKP ) , and , to a certain measure , those of the Romanian Communist Party ( PCdR ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","body","thus","oversaw","the","activities","of","the","Bulgarian","Communist","Party","(","BKP",")",",","the","League","of","Communists","of","Yugoslavia","(","KPJ",")",",","the","Communist","Party","of","Greece","(","KKE",")",",","the","Communist","Party","of","Turkey","(","TKP",")",",","and",",","to","a","certain","measure",",","those","of","the","Romanian","Communist","Party","(","PCdR",")","."],"labels":["O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O"],"target_index":null,"target_label":null},"label_list":["election","country","event","person","location","politician","political_party","organization"]}
{"id":"175","dataset":"crossner_politics","split":"dev","instance":{"id":"175","prompt_labels":"Page(B-political party) 's(I-political party) party(I-political party) affiliation(O) remained(O) with(O) different(O) facets(O) of(O) the(O) Democratic(B-political party) Party(I-political party) ,(O) and(O) moved(O) over(O) time(O) from(O) the(O) Democratic-Republican(B-political party) Party(I-political party) to(O) the(O) Jacksonian(O) democracy(O) to(O) the(O) Democrats(O) to(O) the(O) Free(B-political party) Soil(I-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, person, political party, country, event, election, politician and O.\nSentence: Page 's party affiliation remained with different facets of the Democratic Party , and moved over time from the Democratic-Republican Party to the Jacksonian democracy to the Democrats to the Free Soil Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Page","'s","party","affiliation","remained","with","different","facets","of","the","Democratic","Party",",","and","moved","over","time","from","the","Democratic-Republican","Party","to","the","Jacksonian","democracy","to","the","Democrats","to","the","Free","Soil","Party","."],"labels":["B-political party","I-political party","I-political party","O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["organization","location","person","political_party","country","event","election","politician"]}
{"id":"176","dataset":"crossner_politics","split":"dev","instance":{"id":"176","prompt_labels":"Williams(B-politician) stood(O) unsuccessfully(O) for(O) Labour(B-political party) in(O) Coventry(B-location) at(O) the(O) 1923(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 1924(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) s(O) ,(O) but(O) was(O) elected(O) to(O) the(O) National(B-organization) Executive(I-organization) Committee(I-organization) of(O) the(O) party(O) ,(O) serving(O) as(O) its(O) chair(O) in(O) 1925(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, location, country, person, politician, organization, event, election and O.\nSentence: Williams stood unsuccessfully for Labour in Coventry at the 1923 United Kingdom general election and 1924 United Kingdom general election s , but was elected to the National Executive Committee of the party , serving as its chair in 1925 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Williams","stood","unsuccessfully","for","Labour","in","Coventry","at","the","1923","United","Kingdom","general","election","and","1924","United","Kingdom","general","election","s",",","but","was","elected","to","the","National","Executive","Committee","of","the","party",",","serving","as","its","chair","in","1925","."],"labels":["B-politician","O","O","O","B-political party","O","B-location","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","location","country","person","politician","organization","event","election"]}
{"id":"178","dataset":"crossner_politics","split":"dev","instance":{"id":"178","prompt_labels":"A(O) cash(O) book(O) kept(O) by(O) Flick(B-organization) company(I-organization) accountant(O) Rudolph(B-person) Diehl(I-person) listed(O) that(O) next(O) to(O) other(O) transfers(O) ,(O) 250,000(O) Deutschemark(O) was(O) transferred(O) to(O) Christian(B-political party) Social(I-political party) Union(I-political party) in(I-political party) Bavaria(I-political party) chairman(O) Franz(B-politician) Josef(I-politician) Strauss(I-politician) and(O) 565,000(O) Deutschemark(O) were(O) transferred(O) to(O) Christian(B-political party) Democratic(I-political party) Union(I-political party) of(I-political party) Germany(I-political party) chairman(O) Helmut(B-politician) Kohl(I-politician) ,(O) as(O) well(O) as(O) payments(O) to(O) FDP(B-political party) and(O) Social(B-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Germany(I-political party) politicians(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, political party, person, election, politician, location, event and O.\nSentence: A cash book kept by Flick company accountant Rudolph Diehl listed that next to other transfers , 250,000 Deutschemark was transferred to Christian Social Union in Bavaria chairman Franz Josef Strauss and 565,000 Deutschemark were transferred to Christian Democratic Union of Germany chairman Helmut Kohl , as well as payments to FDP and Social Democratic Party of Germany politicians .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","cash","book","kept","by","Flick","company","accountant","Rudolph","Diehl","listed","that","next","to","other","transfers",",","250,000","Deutschemark","was","transferred","to","Christian","Social","Union","in","Bavaria","chairman","Franz","Josef","Strauss","and","565,000","Deutschemark","were","transferred","to","Christian","Democratic","Union","of","Germany","chairman","Helmut","Kohl",",","as","well","as","payments","to","FDP","and","Social","Democratic","Party","of","Germany","politicians","."],"labels":["O","O","O","O","O","B-organization","I-organization","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-politician","I-politician","I-politician","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-politician","I-politician","O","O","O","O","O","O","B-political party","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O"],"target_index":null,"target_label":null},"label_list":["country","organization","political_party","person","election","politician","location","event"]}
{"id":"179","dataset":"crossner_politics","split":"dev","instance":{"id":"179","prompt_labels":"Two(O) years(O) of(O) proceedings(O) clarified(O) that(O) between(O) 1969(O) and(O) 1989(O) ,(O) politicians(O) of(O) all(O) major(O) parties(O) ((O) Christian(B-political party) Democratic(I-political party) Union(I-political party) of(I-political party) Germany(I-political party) ,(O) Christian(B-political party) Social(I-political party) Union(I-political party) in(I-political party) Bavaria(I-political party) ,(O) FDP(B-political party) ,(O) and(O) Social(B-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Germany(I-political party) )(O) had(O) received(O) money(O) from(O) the(O) Flick(B-organization) company(I-organization) :(O) a(O) total(O) of(O) 25(O) million(O) Deutschemark(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, election, country, person, organization, event, location, politician and O.\nSentence: Two years of proceedings clarified that between 1969 and 1989 , politicians of all major parties ( Christian Democratic Union of Germany , Christian Social Union in Bavaria , FDP , and Social Democratic Party of Germany ) had received money from the Flick company : a total of 25 million Deutschemark .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Two","years","of","proceedings","clarified","that","between","1969","and","1989",",","politicians","of","all","major","parties","(","Christian","Democratic","Union","of","Germany",",","Christian","Social","Union","in","Bavaria",",","FDP",",","and","Social","Democratic","Party","of","Germany",")","had","received","money","from","the","Flick","company",":","a","total","of","25","million","Deutschemark","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","election","country","person","organization","event","location","politician"]}
{"id":"182","dataset":"crossner_politics","split":"dev","instance":{"id":"182","prompt_labels":"President(O) John(B-politician) Adams(I-politician) ,(O) a(O) Federalist(B-political party) Party(I-political party) elected(O) two(O) years(O) prior(O) in(O) the(O) 1796(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) remained(O) popular(O) during(O) a(O) time(O) of(O) national(O) economic(O) growth(O) ,(O) and(O) the(O) Federalists(O) made(O) a(O) modest(O) gain(O) of(O) three(O) seats(O) at(O) the(O) expense(O) of(O) the(O) opposition(O) Democratic-Republican(B-political party) Party(I-political party) ,(O) the(O) party(O) of(O) Vice(O) President(O) and(O) future(O) President(O) Thomas(B-politician) Jefferson(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, political party, organization, election, person, event, location and O.\nSentence: President John Adams , a Federalist Party elected two years prior in the 1796 United States presidential election , remained popular during a time of national economic growth , and the Federalists made a modest gain of three seats at the expense of the opposition Democratic-Republican Party , the party of Vice President and future President Thomas Jefferson .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["President","John","Adams",",","a","Federalist","Party","elected","two","years","prior","in","the","1796","United","States","presidential","election",",","remained","popular","during","a","time","of","national","economic","growth",",","and","the","Federalists","made","a","modest","gain","of","three","seats","at","the","expense","of","the","opposition","Democratic-Republican","Party",",","the","party","of","Vice","President","and","future","President","Thomas","Jefferson","."],"labels":["O","B-politician","I-politician","O","O","B-political party","I-political party","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["politician","country","political_party","organization","election","person","event","location"]}
{"id":"183","dataset":"crossner_politics","split":"dev","instance":{"id":"183","prompt_labels":"She(O) was(O) the(O) lead(O) Senate(O) candidate(O) at(O) the(O) 2007(B-election) Australian(I-election) federal(I-election) election(I-election) ,(O) again(O) at(O) the(O) 2010(B-election) Australian(I-election) federal(I-election) election(I-election) ,(O) in(O) which(O) she(O) became(O) the(O) first(O) Greens(B-political party) candidate(O) elected(O) in(O) Queensland(B-location) ,(O) and(O) the(O) 2019(B-election) Australian(I-election) federal(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, political party, event, politician, organization, person, location and O.\nSentence: She was the lead Senate candidate at the 2007 Australian federal election , again at the 2010 Australian federal election , in which she became the first Greens candidate elected in Queensland , and the 2019 Australian federal election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","was","the","lead","Senate","candidate","at","the","2007","Australian","federal","election",",","again","at","the","2010","Australian","federal","election",",","in","which","she","became","the","first","Greens","candidate","elected","in","Queensland",",","and","the","2019","Australian","federal","election","."],"labels":["O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","B-political party","O","O","O","B-location","O","O","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["country","election","political_party","event","politician","organization","person","location"]}
{"id":"186","dataset":"crossner_politics","split":"dev","instance":{"id":"186","prompt_labels":"A(O) significant(O) number(O) of(O) Straight(B-organization) Left(I-organization) faction(O) members(O) had(O) developed(O) close(O) personal(O) friendships(O) with(O) members(O) of(O) fraternal(O) communist(O) parties(O) ,(O) particularly(O) the(O) Tudeh(B-political party) Party(I-political party) of(I-political party) Iran(I-political party) ,(O) Iraqi(B-political party) Communist(I-political party) Party(I-political party) ,(O) South(O) African(O) and(O) Communist(B-political party) Party(I-political party) of(I-political party) Greece(I-political party) parties(O) ,(O) who(O) were(O) well(O) organised(O) on(O) most(O) British(O) University(O) campuses(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, event, country, person, election, politician, location and O.\nSentence: A significant number of Straight Left faction members had developed close personal friendships with members of fraternal communist parties , particularly the Tudeh Party of Iran , Iraqi Communist Party , South African and Communist Party of Greece parties , who were well organised on most British University campuses .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","significant","number","of","Straight","Left","faction","members","had","developed","close","personal","friendships","with","members","of","fraternal","communist","parties",",","particularly","the","Tudeh","Party","of","Iran",",","Iraqi","Communist","Party",",","South","African","and","Communist","Party","of","Greece","parties",",","who","were","well","organised","on","most","British","University","campuses","."],"labels":["O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","organization","event","country","person","election","politician","location"]}
{"id":"188","dataset":"crossner_politics","split":"dev","instance":{"id":"188","prompt_labels":"He(O) was(O) voted(O) into(O) Parliament(O) with(O) the(O) Liberal(B-political party) Democratic(I-political party) Union(I-political party) ,(O) in(O) the(O) 1956(B-election) Greek(I-election) legislative(I-election) election(I-election) s(O) ,(O) but(O) in(O) the(O) 1958(B-election) Greek(I-election) legislative(I-election) election(I-election) s(O) ,(O) as(O) head(O) of(O) the(O) Union(B-political party) of(I-political party) Populars(I-political party) ,(O) he(O) failed(O) to(O) be(O) elected(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, event, location, person, politician, election, political party and O.\nSentence: He was voted into Parliament with the Liberal Democratic Union , in the 1956 Greek legislative election s , but in the 1958 Greek legislative election s , as head of the Union of Populars , he failed to be elected .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","voted","into","Parliament","with","the","Liberal","Democratic","Union",",","in","the","1956","Greek","legislative","election","s",",","but","in","the","1958","Greek","legislative","election","s",",","as","head","of","the","Union","of","Populars",",","he","failed","to","be","elected","."],"labels":["O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","country","event","location","person","politician","election","political_party"]}
{"id":"189","dataset":"crossner_politics","split":"dev","instance":{"id":"189","prompt_labels":"Other(O) groups(O) supporting(O) their(O) repeal(O) include(O) the(O) National(B-organization) Conference(I-organization) of(I-organization) Insurance(I-organization) Legislators(I-organization) ,(O) the(O) American(B-organization) Bar(I-organization) Association(I-organization) ,(O) the(O) American(B-organization) College(I-organization) of(I-organization) Emergency(I-organization) Physicians(I-organization) ,(O) Mothers(B-organization) Against(I-organization) Drunk(I-organization) Driving(I-organization) ,(O) the(O) National(B-organization) Commission(I-organization) Against(I-organization) Drunk(I-organization) Driving(I-organization) ,(O) and(O) the(O) American(B-organization) Medical(I-organization) Association(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, election, politician, organization, political party, location, person and O.\nSentence: Other groups supporting their repeal include the National Conference of Insurance Legislators , the American Bar Association , the American College of Emergency Physicians , Mothers Against Drunk Driving , the National Commission Against Drunk Driving , and the American Medical Association .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","groups","supporting","their","repeal","include","the","National","Conference","of","Insurance","Legislators",",","the","American","Bar","Association",",","the","American","College","of","Emergency","Physicians",",","Mothers","Against","Drunk","Driving",",","the","National","Commission","Against","Drunk","Driving",",","and","the","American","Medical","Association","."],"labels":["O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["country","event","election","politician","organization","political_party","location","person"]}
{"id":"190","dataset":"crossner_politics","split":"dev","instance":{"id":"190","prompt_labels":"Grande(B-person) was(O) elected(O) to(O) the(O) Ontario(B-organization) legislature(I-organization) in(O) the(O) 1975(B-election) Ontario(I-election) general(I-election) election(I-election) ,(O) and(O) re-elected(O) in(O) 1977(B-election) Ontario(I-election) general(I-election) election(I-election) ,(O) 1981(B-election) Ontario(I-election) general(I-election) election(I-election) and(O) 1985(B-election) Ontario(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, election, organization, country, event, location, person and O.\nSentence: Grande was elected to the Ontario legislature in the 1975 Ontario general election , and re-elected in 1977 Ontario general election , 1981 Ontario general election and 1985 Ontario general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Grande","was","elected","to","the","Ontario","legislature","in","the","1975","Ontario","general","election",",","and","re-elected","in","1977","Ontario","general","election",",","1981","Ontario","general","election","and","1985","Ontario","general","election","."],"labels":["B-person","O","O","O","O","B-organization","I-organization","O","O","B-election","I-election","I-election","I-election","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["political_party","politician","election","organization","country","event","location","person"]}
{"id":"195","dataset":"crossner_politics","split":"dev","instance":{"id":"195","prompt_labels":"In(O) Belgian(O) politics(O) ,(O) the(O) term(O) Jamaica(B-country) coalition(O) refers(O) to(O) a(O) coalition(O) of(O) Christian(O) democrats(O) ((O) Christen-Democratisch(B-political party) en(I-political party) Vlaams(I-political party) and(O) Centre(B-political party) dmocrate(I-political party) humaniste(I-political party) )(O) ,(O) liberals(O) ((O) Open(B-political party) Vlaamse(I-political party) Liberalen(I-political party) en(I-political party) Democraten(I-political party) and(O) Mouvement(B-political party) Rformateur(I-political party) )(O) and(O) greens(O) ((O) Groen(B-political party) and(O) Ecolo(B-political party) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, political party, event, location, country, organization, election, person and O.\nSentence: In Belgian politics , the term Jamaica coalition refers to a coalition of Christian democrats ( Christen-Democratisch en Vlaams and Centre dmocrate humaniste ) , liberals ( Open Vlaamse Liberalen en Democraten and Mouvement Rformateur ) and greens ( Groen and Ecolo ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","Belgian","politics",",","the","term","Jamaica","coalition","refers","to","a","coalition","of","Christian","democrats","(","Christen-Democratisch","en","Vlaams","and","Centre","dmocrate","humaniste",")",",","liberals","(","Open","Vlaamse","Liberalen","en","Democraten","and","Mouvement","Rformateur",")","and","greens","(","Groen","and","Ecolo",")","."],"labels":["O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","O","O","O","O","B-political party","O","B-political party","O","O"],"target_index":null,"target_label":null},"label_list":["politician","political_party","event","location","country","organization","election","person"]}
{"id":"196","dataset":"crossner_politics","split":"dev","instance":{"id":"196","prompt_labels":"The(O) same(O) boundaries(O) were(O) used(O) in(O) the(O) 1922(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1923(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1924(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1929(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1931(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1935(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) the(O) 1945(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, election, location, politician, event, country, organization, political party and O.\nSentence: The same boundaries were used in the 1922 United Kingdom general election , the 1923 United Kingdom general election , the 1924 United Kingdom general election , the 1929 United Kingdom general election , the 1931 United Kingdom general election , the 1935 United Kingdom general election and the 1945 United Kingdom general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","same","boundaries","were","used","in","the","1922","United","Kingdom","general","election",",","the","1923","United","Kingdom","general","election",",","the","1924","United","Kingdom","general","election",",","the","1929","United","Kingdom","general","election",",","the","1931","United","Kingdom","general","election",",","the","1935","United","Kingdom","general","election","and","the","1945","United","Kingdom","general","election","."],"labels":["O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["person","election","location","politician","event","country","organization","political_party"]}
{"id":"197","dataset":"crossner_politics","split":"dev","instance":{"id":"197","prompt_labels":"1885(O) boundaries(O) were(O) also(O) used(O) in(O) the(O) 1886(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1892(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1895(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1900(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1906(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) January(B-election) 1910(I-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) the(O) December(B-election) 1910(I-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, organization, person, country, event, election, political party and O.\nSentence: 1885 boundaries were also used in the 1886 United Kingdom general election , the 1892 United Kingdom general election , the 1895 United Kingdom general election , the 1900 United Kingdom general election , the 1906 United Kingdom general election , the January 1910 United Kingdom general election and the December 1910 United Kingdom general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["1885","boundaries","were","also","used","in","the","1886","United","Kingdom","general","election",",","the","1892","United","Kingdom","general","election",",","the","1895","United","Kingdom","general","election",",","the","1900","United","Kingdom","general","election",",","the","1906","United","Kingdom","general","election",",","the","January","1910","United","Kingdom","general","election","and","the","December","1910","United","Kingdom","general","election","."],"labels":["O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["politician","location","organization","person","country","event","election","political_party"]}
{"id":"198","dataset":"crossner_politics","split":"dev","instance":{"id":"198","prompt_labels":"East(B-location) Aberdeenshire(I-location) retained(O) the(O) same(O) boundaries(O) for(O) the(O) 1959(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1964(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1966(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1970(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) February(B-election) 1974(I-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) the(O) October(B-election) 1974(I-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, election, political party, event, location, person, country, politician and O.\nSentence: East Aberdeenshire retained the same boundaries for the 1959 United Kingdom general election , the 1964 United Kingdom general election , the 1966 United Kingdom general election , the 1970 United Kingdom general election , the February 1974 United Kingdom general election and the October 1974 United Kingdom general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["East","Aberdeenshire","retained","the","same","boundaries","for","the","1959","United","Kingdom","general","election",",","the","1964","United","Kingdom","general","election",",","the","1966","United","Kingdom","general","election",",","the","1970","United","Kingdom","general","election",",","the","February","1974","United","Kingdom","general","election","and","the","October","1974","United","Kingdom","general","election","."],"labels":["B-location","I-location","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["organization","election","political_party","event","location","person","country","politician"]}
{"id":"2","dataset":"crossner_science","split":"dev","instance":{"id":"2","prompt_labels":"Labeled(O) genomic(O) DNA(O) is(O) extracted(O) from(O) nuclei(O) and(O) fragmented(O) by(O) HaeIII(O) digestion(O) and(O) sonication(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, discipline, scientist, event, protein, chemical element, university, chemical compound, country, location, person, award, organization, enzyme, theory, academic journal and O.\nSentence: Labeled genomic DNA is extracted from nuclei and fragmented by HaeIII digestion and sonication .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Labeled","genomic","DNA","is","extracted","from","nuclei","and","fragmented","by","HaeIII","digestion","and","sonication","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","discipline","scientist","event","protein","chemical_element","university","chemical_compound","country","location","person","award","organization","enzyme","theory","academic_journal"]}
{"id":"3","dataset":"crossner_science","split":"dev","instance":{"id":"3","prompt_labels":"He(O) attended(O) the(O) U.S.(B-university) Air(I-university) Force(I-university) Institute(I-university) of(I-university) Technology(I-university) for(O) a(O) year(O) ,(O) earning(O) a(O) bachelor(O) 's(O) degree(O) in(O) aeromechanics(B-discipline) ,(O) and(O) received(O) his(O) test(O) pilot(O) training(O) at(O) Edwards(B-organization) Air(I-organization) Force(I-organization) Base(I-organization) in(O) California(B-location) before(O) his(O) assignment(O) as(O) a(O) test(O) pilot(O) at(O) Wright-Patterson(B-organization) Air(I-organization) Force(I-organization) Base(I-organization) in(O) Ohio(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, award, protein, scientist, theory, organization, astronomical object, academic journal, event, person, university, chemical element, discipline, enzyme, country, location and O.\nSentence: He attended the U.S. Air Force Institute of Technology for a year , earning a bachelor 's degree in aeromechanics , and received his test pilot training at Edwards Air Force Base in California before his assignment as a test pilot at Wright-Patterson Air Force Base in Ohio .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","attended","the","U.S.","Air","Force","Institute","of","Technology","for","a","year",",","earning","a","bachelor","'s","degree","in","aeromechanics",",","and","received","his","test","pilot","training","at","Edwards","Air","Force","Base","in","California","before","his","assignment","as","a","test","pilot","at","Wright-Patterson","Air","Force","Base","in","Ohio","."],"labels":["O","O","O","B-university","I-university","I-university","I-university","I-university","I-university","O","O","O","O","O","O","O","O","O","O","B-discipline","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-location","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","award","protein","scientist","theory","organization","astronomical_object","academic_journal","event","person","university","chemical_element","discipline","enzyme","country","location"]}
{"id":"7","dataset":"crossner_science","split":"dev","instance":{"id":"7","prompt_labels":"The(O) Olympic(B-location) golf(I-location) course(I-location) is(O) a(O) new(O) venue(O) built(O) for(O) the(O) Golf(O) at(O) the(O) 2016(B-event) Summer(I-event) Olympics(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, award, country, protein, enzyme, person, scientist, chemical element, university, organization, academic journal, theory, event, location, astronomical object, discipline and O.\nSentence: The Olympic golf course is a new venue built for the Golf at the 2016 Summer Olympics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Olympic","golf","course","is","a","new","venue","built","for","the","Golf","at","the","2016","Summer","Olympics","."],"labels":["O","B-location","I-location","I-location","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","award","country","protein","enzyme","person","scientist","chemical_element","university","organization","academic_journal","theory","event","location","astronomical_object","discipline"]}
{"id":"8","dataset":"crossner_science","split":"dev","instance":{"id":"8","prompt_labels":"Removing(O) a(O) TAD(O) boundary(O) ((O) for(O) example(O) ,(O) using(O) CRISPR(O) to(O) delete(O) the(O) relevant(O) region(O) of(O) the(O) genome(O) )(O) can(O) allow(O) new(O) promoter-enhancer(O) contacts(O) to(O) form(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, theory, chemical compound, country, organization, chemical element, protein, person, event, award, astronomical object, scientist, university, location, academic journal, discipline and O.\nSentence: Removing a TAD boundary ( for example , using CRISPR to delete the relevant region of the genome ) can allow new promoter-enhancer contacts to form .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Removing","a","TAD","boundary","(","for","example",",","using","CRISPR","to","delete","the","relevant","region","of","the","genome",")","can","allow","new","promoter-enhancer","contacts","to","form","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["enzyme","theory","chemical_compound","country","organization","chemical_element","protein","person","event","award","astronomical_object","scientist","university","location","academic_journal","discipline"]}
{"id":"10","dataset":"crossner_science","split":"dev","instance":{"id":"10","prompt_labels":"He(O) is(O) currently(O) Director(O) of(O) the(O) Yale(B-organization) Center(I-organization) for(I-organization) the(I-organization) Study(I-organization) of(I-organization) Globalization(I-organization) at(O) Yale(B-university) University(I-university) ,(O) is(O) the(O) Latin(O) American(O) co-chair(O) of(O) the(O) Inter-American(B-organization) Dialogue(I-organization) ,(O) and(O) is(O) on(O) the(O) board(O) of(O) directors(O) of(O) Citigroup(B-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, protein, award, location, organization, scientist, astronomical object, country, discipline, chemical element, academic journal, chemical compound, enzyme, theory, person, university and O.\nSentence: He is currently Director of the Yale Center for the Study of Globalization at Yale University , is the Latin American co-chair of the Inter-American Dialogue , and is on the board of directors of Citigroup .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","is","currently","Director","of","the","Yale","Center","for","the","Study","of","Globalization","at","Yale","University",",","is","the","Latin","American","co-chair","of","the","Inter-American","Dialogue",",","and","is","on","the","board","of","directors","of","Citigroup","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-university","I-university","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","B-organization","O"],"target_index":null,"target_label":null},"label_list":["event","protein","award","location","organization","scientist","astronomical_object","country","discipline","chemical_element","academic_journal","chemical_compound","enzyme","theory","person","university"]}
{"id":"11","dataset":"crossner_science","split":"dev","instance":{"id":"11","prompt_labels":"Saturn(B-astronomical object) is(O) usually(O) depicted(O) with(O) a(O) scythe(O) or(O) sickle(O) ,(O) and(O) the(O) planetary(O) symbol(O) has(O) apparently(O) evolved(O) from(O) a(O) picture(O) of(O) this(O) attribute(O) ,(O) in(O) Kamateros(O) ((O) 12th(O) century(O) )(O) shown(O) in(O) a(O) shape(O) similar(O) to(O) the(O) letter(O) eta(O) (O) ,(O) with(O) the(O) horizontal(O) stroke(O) added(O) along(O) with(O) the(O) Christianization(O) of(O) the(O) other(O) symbols(O) in(O) the(O) early(O) 16th(O) century(O) ,(O) Saturn(B-astronomical object) ((O) U(O) +(O) 2644(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, theory, academic journal, chemical compound, scientist, enzyme, location, organization, country, chemical element, protein, event, astronomical object, person, discipline, university and O.\nSentence: Saturn is usually depicted with a scythe or sickle , and the planetary symbol has apparently evolved from a picture of this attribute , in Kamateros ( 12th century ) shown in a shape similar to the letter eta  , with the horizontal stroke added along with the Christianization of the other symbols in the early 16th century , Saturn ( U + 2644 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Saturn","is","usually","depicted","with","a","scythe","or","sickle",",","and","the","planetary","symbol","has","apparently","evolved","from","a","picture","of","this","attribute",",","in","Kamateros","(","12th","century",")","shown","in","a","shape","similar","to","the","letter","eta","",",","with","the","horizontal","stroke","added","along","with","the","Christianization","of","the","other","symbols","in","the","early","16th","century",",","Saturn","(","U","+","2644",")","."],"labels":["B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","theory","academic_journal","chemical_compound","scientist","enzyme","location","organization","country","chemical_element","protein","event","astronomical_object","person","discipline","university"]}
{"id":"13","dataset":"crossner_science","split":"dev","instance":{"id":"13","prompt_labels":"In(O) addition(O) to(O) his(O) steady(O) research(O) output(O) ,(O) Naqvi(B-scientist) has(O) manifested(O) his(O) commitment(O) to(O) teaching(O) by(O) contributing(O) to(O) journals(O) devoted(O) to(O) didactical(O) aspects(O) of(O) science(O) ((O) American(B-academic journal) Journal(I-academic journal) of(I-academic journal) Physics(I-academic journal) ,(O) European(B-academic journal) Journal(I-academic journal) of(I-academic journal) Physics(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Chemical(I-academic journal) Education(I-academic journal) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, enzyme, scientist, university, chemical compound, award, chemical element, astronomical object, theory, location, protein, discipline, event, academic journal, person, organization and O.\nSentence: In addition to his steady research output , Naqvi has manifested his commitment to teaching by contributing to journals devoted to didactical aspects of science ( American Journal of Physics , European Journal of Physics , Journal of Chemical Education ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","addition","to","his","steady","research","output",",","Naqvi","has","manifested","his","commitment","to","teaching","by","contributing","to","journals","devoted","to","didactical","aspects","of","science","(","American","Journal","of","Physics",",","European","Journal","of","Physics",",","Journal","of","Chemical","Education",")","."],"labels":["O","O","O","O","O","O","O","O","B-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O"],"target_index":null,"target_label":null},"label_list":["country","enzyme","scientist","university","chemical_compound","award","chemical_element","astronomical_object","theory","location","protein","discipline","event","academic_journal","person","organization"]}
{"id":"14","dataset":"crossner_science","split":"dev","instance":{"id":"14","prompt_labels":"Most(O) of(O) the(O) outer(O) irregular(O) moon(O) s(O) of(O) Jupiter(B-astronomical object) and(O) Saturn(B-astronomical object) also(O) have(O) retrograde(O) orbits(O) ,(O) as(O) do(O) some(O) of(O) Uranus(B-astronomical object) '(O) s(O) outer(O) moons(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, event, chemical compound, theory, astronomical object, university, scientist, person, organization, academic journal, discipline, chemical element, award, enzyme, protein and O.\nSentence: Most of the outer irregular moon s of Jupiter and Saturn also have retrograde orbits , as do some of Uranus ' s outer moons .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Most","of","the","outer","irregular","moon","s","of","Jupiter","and","Saturn","also","have","retrograde","orbits",",","as","do","some","of","Uranus","'","s","outer","moons","."],"labels":["O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","country","event","chemical_compound","theory","astronomical_object","university","scientist","person","organization","academic_journal","discipline","chemical_element","award","enzyme","protein"]}
{"id":"15","dataset":"crossner_science","split":"dev","instance":{"id":"15","prompt_labels":"For(O) example(O) ,(O) that(O) ancestor(O) had(O) at(O) least(O) 7(O) Pax(O) genes(O) for(O) transcription(O) factor(O) s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, location, chemical element, academic journal, theory, chemical compound, astronomical object, university, enzyme, person, country, scientist, organization, protein, discipline and O.\nSentence: For example , that ancestor had at least 7 Pax genes for transcription factor s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","example",",","that","ancestor","had","at","least","7","Pax","genes","for","transcription","factor","s","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","event","location","chemical_element","academic_journal","theory","chemical_compound","astronomical_object","university","enzyme","person","country","scientist","organization","protein","discipline"]}
{"id":"16","dataset":"crossner_science","split":"dev","instance":{"id":"16","prompt_labels":"In(O) most(O) cases(O) ,(O) planets(O) named(O) with(O) Bayer(O) ,(O) Flamsteed(O) ,(O) and(O) or(O) Variable(O) star(O) designation(O) have(O) a(O) space(O) ,(O) but(O) usage(O) with(O) other(O) designations(O) varies(O) e.g.(O) WASP-12b(B-astronomical object) but(O) HD(B-astronomical object) 209458(I-astronomical object) b(I-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, award, university, person, chemical compound, protein, scientist, enzyme, discipline, theory, academic journal, location, organization, country, chemical element, event and O.\nSentence: In most cases , planets named with Bayer , Flamsteed , and or Variable star designation have a space , but usage with other designations varies e.g. WASP-12b but HD 209458 b .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","most","cases",",","planets","named","with","Bayer",",","Flamsteed",",","and","or","Variable","star","designation","have","a","space",",","but","usage","with","other","designations","varies","e.g.","WASP-12b","but","HD","209458","b","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","I-astronomical object","I-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","award","university","person","chemical_compound","protein","scientist","enzyme","discipline","theory","academic_journal","location","organization","country","chemical_element","event"]}
{"id":"17","dataset":"crossner_science","split":"dev","instance":{"id":"17","prompt_labels":"Schirra(B-person) was(O) a(O) 33rd(O) Degree(O) Mason(O) and(O) part(O) of(O) the(O) American(B-organization) Institute(I-organization) of(I-organization) Aeronautics(I-organization) and(I-organization) Astronautics(I-organization) ,(O) as(O) well(O) as(O) a(O) fellow(B-award) of(I-award) the(I-award) American(I-award) Astronautical(I-award) Society(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, country, award, chemical element, event, organization, enzyme, astronomical object, chemical compound, academic journal, location, university, discipline, scientist, person, protein and O.\nSentence: Schirra was a 33rd Degree Mason and part of the American Institute of Aeronautics and Astronautics , as well as a fellow of the American Astronautical Society .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Schirra","was","a","33rd","Degree","Mason","and","part","of","the","American","Institute","of","Aeronautics","and","Astronautics",",","as","well","as","a","fellow","of","the","American","Astronautical","Society","."],"labels":["B-person","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["theory","country","award","chemical_element","event","organization","enzyme","astronomical_object","chemical_compound","academic_journal","location","university","discipline","scientist","person","protein"]}
{"id":"20","dataset":"crossner_science","split":"dev","instance":{"id":"20","prompt_labels":"Generally(O) ,(O) Potassium(B-chemical compound) cyanide(I-chemical compound) or(O) its(O) less(O) toxic(O) surrogate(O) Zinc(B-chemical compound) cyanide(I-chemical compound) are(O) used(O) as(O) nucleophilic(B-chemical compound) cyanide(I-chemical compound) sources(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, academic journal, chemical element, location, enzyme, protein, organization, theory, astronomical object, award, event, discipline, person, country, university, chemical compound and O.\nSentence: Generally , Potassium cyanide or its less toxic surrogate Zinc cyanide are used as nucleophilic cyanide sources .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Generally",",","Potassium","cyanide","or","its","less","toxic","surrogate","Zinc","cyanide","are","used","as","nucleophilic","cyanide","sources","."],"labels":["O","O","B-chemical compound","I-chemical compound","O","O","O","O","O","B-chemical compound","I-chemical compound","O","O","O","B-chemical compound","I-chemical compound","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","academic_journal","chemical_element","location","enzyme","protein","organization","theory","astronomical_object","award","event","discipline","person","country","university","chemical_compound"]}
{"id":"27","dataset":"crossner_science","split":"dev","instance":{"id":"27","prompt_labels":"He(O) is(O) known(O) for(O) his(O) studies(O) on(O) DNA(O) and(O) RNA(B-enzyme) polymerase(I-enzyme) s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, academic journal, country, scientist, event, theory, astronomical object, enzyme, organization, person, chemical element, award, chemical compound, protein, university, discipline and O.\nSentence: He is known for his studies on DNA and RNA polymerase s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","is","known","for","his","studies","on","DNA","and","RNA","polymerase","s","."],"labels":["O","O","O","O","O","O","O","O","O","B-enzyme","I-enzyme","O","O"],"target_index":null,"target_label":null},"label_list":["location","academic_journal","country","scientist","event","theory","astronomical_object","enzyme","organization","person","chemical_element","award","chemical_compound","protein","university","discipline"]}
{"id":"28","dataset":"crossner_science","split":"dev","instance":{"id":"28","prompt_labels":"He(O) has(O) served(O) on(O) scientific(O) journal(O) editorial(O) boards(O) including(O) American(B-academic journal) Scientist(I-academic journal) ,(O) Physics(B-academic journal) of(I-academic journal) Fluids(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Fluid(I-academic journal) Mechanics(I-academic journal) ,(O) Physical(B-academic journal) Review(I-academic journal) E(I-academic journal) ,(O) Physical(B-academic journal) Review(I-academic journal) Letters(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Theoretical(I-academic journal) and(I-academic journal) Computational(I-academic journal) Fluid(I-academic journal) Dynamics(I-academic journal) ,(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, organization, academic journal, person, theory, event, country, chemical compound, astronomical object, award, scientist, location, protein, discipline, enzyme, university and O.\nSentence: He has served on scientific journal editorial boards including American Scientist , Physics of Fluids , Journal of Fluid Mechanics , Physical Review E , Physical Review Letters , Journal of Theoretical and Computational Fluid Dynamics ,","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","has","served","on","scientific","journal","editorial","boards","including","American","Scientist",",","Physics","of","Fluids",",","Journal","of","Fluid","Mechanics",",","Physical","Review","E",",","Physical","Review","Letters",",","Journal","of","Theoretical","and","Computational","Fluid","Dynamics",","],"labels":["O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","organization","academic_journal","person","theory","event","country","chemical_compound","astronomical_object","award","scientist","location","protein","discipline","enzyme","university"]}
{"id":"29","dataset":"crossner_science","split":"dev","instance":{"id":"29","prompt_labels":"This(O) led(O) to(O) a(O) vigorous(O) debate(O) between(O) the(O) biometricians(O) ,(O) who(O) supported(O) Galton(B-scientist) 's(O) ideas(O) ,(O) as(O) Walter(B-scientist) Weldon(I-scientist) ,(O) Arthur(B-scientist) Dukinfield(I-scientist) Darbishire(I-scientist) and(O) Karl(B-scientist) Pearson(I-scientist) ,(O) and(O) Mendelians(O) ,(O) who(O) supported(O) Bateson(B-scientist) 's(O) ((O) and(O) Mendel(B-scientist) 's(O) )(O) ideas(O) ,(O) such(O) as(O) Charles(B-scientist) Davenport(I-scientist) and(O) Wilhelm(B-scientist) Johannsen(I-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, scientist, discipline, chemical compound, location, person, event, country, theory, award, organization, protein, academic journal, university, chemical element, enzyme and O.\nSentence: This led to a vigorous debate between the biometricians , who supported Galton 's ideas , as Walter Weldon , Arthur Dukinfield Darbishire and Karl Pearson , and Mendelians , who supported Bateson 's ( and Mendel 's ) ideas , such as Charles Davenport and Wilhelm Johannsen .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","led","to","a","vigorous","debate","between","the","biometricians",",","who","supported","Galton","'s","ideas",",","as","Walter","Weldon",",","Arthur","Dukinfield","Darbishire","and","Karl","Pearson",",","and","Mendelians",",","who","supported","Bateson","'s","(","and","Mendel","'s",")","ideas",",","such","as","Charles","Davenport","and","Wilhelm","Johannsen","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O","O","O","B-scientist","O","O","O","B-scientist","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","scientist","discipline","chemical_compound","location","person","event","country","theory","award","organization","protein","academic_journal","university","chemical_element","enzyme"]}
{"id":"31","dataset":"crossner_science","split":"dev","instance":{"id":"31","prompt_labels":"Usually(O) ,(O) in(O) the(O) presence(O) of(O) NADPH(B-enzyme) dehydrogenase(I-enzyme) or(O) NADH(B-enzyme) dehydrogenase(I-enzyme) as(O) the(O) enzyme(O) ,(O) NADPH(B-chemical compound) or(O) NADH(B-chemical compound) is(O) the(O) reductant(O) that(O) converts(O) resazurin(O) to(O) resorufin(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, astronomical object, theory, event, university, location, academic journal, enzyme, chemical compound, scientist, country, person, award, protein, discipline, organization and O.\nSentence: Usually , in the presence of NADPH dehydrogenase or NADH dehydrogenase as the enzyme , NADPH or NADH is the reductant that converts resazurin to resorufin .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Usually",",","in","the","presence","of","NADPH","dehydrogenase","or","NADH","dehydrogenase","as","the","enzyme",",","NADPH","or","NADH","is","the","reductant","that","converts","resazurin","to","resorufin","."],"labels":["O","O","O","O","O","O","B-enzyme","I-enzyme","O","B-enzyme","I-enzyme","O","O","O","O","B-chemical compound","O","B-chemical compound","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","astronomical_object","theory","event","university","location","academic_journal","enzyme","chemical_compound","scientist","country","person","award","protein","discipline","organization"]}
{"id":"35","dataset":"crossner_science","split":"dev","instance":{"id":"35","prompt_labels":"The(O) album(O) was(O) nominated(O) at(O) the(O) 41st(B-award) Annual(I-award) Grammy(I-award) Awards(I-award) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Rap(I-award) Album(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, scientist, theory, chemical element, chemical compound, event, academic journal, astronomical object, location, award, country, enzyme, discipline, person, organization, protein and O.\nSentence: The album was nominated at the 41st Annual Grammy Awards for Grammy Award for Best Rap Album .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","album","was","nominated","at","the","41st","Annual","Grammy","Awards","for","Grammy","Award","for","Best","Rap","Album","."],"labels":["O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["university","scientist","theory","chemical_element","chemical_compound","event","academic_journal","astronomical_object","location","award","country","enzyme","discipline","person","organization","protein"]}
{"id":"39","dataset":"crossner_science","split":"dev","instance":{"id":"39","prompt_labels":"The(O) team(O) that(O) she(O) manages(O) has(O) specially(O) studied(O) the(O) role(O) of(O) Proinsulin(B-protein) /(O) insulin(B-protein) in(O) the(O) development(O) of(O) the(O) central(O) nervous(O) system(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, person, location, enzyme, country, chemical element, theory, astronomical object, scientist, organization, event, chemical compound, academic journal, university, award, discipline and O.\nSentence: The team that she manages has specially studied the role of Proinsulin / insulin in the development of the central nervous system .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","team","that","she","manages","has","specially","studied","the","role","of","Proinsulin","/","insulin","in","the","development","of","the","central","nervous","system","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-protein","O","B-protein","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["protein","person","location","enzyme","country","chemical_element","theory","astronomical_object","scientist","organization","event","chemical_compound","academic_journal","university","award","discipline"]}
{"id":"41","dataset":"crossner_science","split":"dev","instance":{"id":"41","prompt_labels":"Hamilton(B-person) was(O) a(O) visiting(O) professor(O) at(O) Harvard(B-university) University(I-university) and(O) later(O) spent(O) nine(O) months(O) with(O) the(O) Royal(B-organization) Society(I-organization) '(O) s(O) and(O) the(O) Royal(B-organization) Geographical(I-organization) Society(I-organization) '(O) s(O) Xavantina-Cachimbo(O) Expedition(O) as(O) a(O) visiting(O) professor(O) at(O) the(O) University(B-university) of(I-university) So(I-university) Paulo(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, theory, person, astronomical object, award, event, organization, protein, location, country, academic journal, enzyme, discipline, scientist, university, chemical element and O.\nSentence: Hamilton was a visiting professor at Harvard University and later spent nine months with the Royal Society ' s and the Royal Geographical Society ' s Xavantina-Cachimbo Expedition as a visiting professor at the University of So Paulo .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Hamilton","was","a","visiting","professor","at","Harvard","University","and","later","spent","nine","months","with","the","Royal","Society","'","s","and","the","Royal","Geographical","Society","'","s","Xavantina-Cachimbo","Expedition","as","a","visiting","professor","at","the","University","of","So","Paulo","."],"labels":["B-person","O","O","O","O","O","B-university","I-university","O","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","B-university","I-university","I-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","theory","person","astronomical_object","award","event","organization","protein","location","country","academic_journal","enzyme","discipline","scientist","university","chemical_element"]}
{"id":"42","dataset":"crossner_science","split":"dev","instance":{"id":"42","prompt_labels":"Michelson(B-scientist) was(O) a(O) member(O) of(O) the(O) Royal(B-organization) Society(I-organization) ,(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) the(O) American(B-organization) Physical(I-organization) Society(I-organization) and(O) the(O) American(B-organization) Association(I-organization) for(I-organization) the(I-organization) Advancement(I-organization) of(I-organization) Science(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, country, award, event, chemical element, theory, academic journal, discipline, enzyme, location, university, protein, scientist, organization, person, chemical compound and O.\nSentence: Michelson was a member of the Royal Society , the National Academy of Sciences , the American Physical Society and the American Association for the Advancement of Science .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Michelson","was","a","member","of","the","Royal","Society",",","the","National","Academy","of","Sciences",",","the","American","Physical","Society","and","the","American","Association","for","the","Advancement","of","Science","."],"labels":["B-scientist","O","O","O","O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","country","award","event","chemical_element","theory","academic_journal","discipline","enzyme","location","university","protein","scientist","organization","person","chemical_compound"]}
{"id":"45","dataset":"crossner_science","split":"dev","instance":{"id":"45","prompt_labels":"The(O) Asian(B-organization) Football(I-organization) Confederation(I-organization) ,(O) Oceania(B-organization) Football(I-organization) Confederation(I-organization) and(O) CONCACAF(B-organization) ((O) the(O) governing(O) body(O) of(O) football(O) in(O) North(O) and(O) Central(O) America(B-country) and(O) the(O) Caribbean(B-location) )(O) use(O) blue(O) text(O) on(O) their(O) logos(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, astronomical object, country, chemical element, organization, protein, university, person, theory, discipline, chemical compound, enzyme, event, award, scientist, academic journal and O.\nSentence: The Asian Football Confederation , Oceania Football Confederation and CONCACAF ( the governing body of football in North and Central America and the Caribbean ) use blue text on their logos .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Asian","Football","Confederation",",","Oceania","Football","Confederation","and","CONCACAF","(","the","governing","body","of","football","in","North","and","Central","America","and","the","Caribbean",")","use","blue","text","on","their","logos","."],"labels":["O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","O","O","O","O","B-country","O","O","B-location","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","astronomical_object","country","chemical_element","organization","protein","university","person","theory","discipline","chemical_compound","enzyme","event","award","scientist","academic_journal"]}
{"id":"46","dataset":"crossner_science","split":"dev","instance":{"id":"46","prompt_labels":"She(O) competed(O) in(O) the(O) 4(B-event) (I-event) 100(I-event) metres(I-event) relay(I-event) event(I-event) at(O) the(O) 2015(B-event) World(I-event) Championships(I-event) in(O) Athletics(O) in(O) Beijing(B-location) ,(O) China(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, astronomical object, person, enzyme, protein, discipline, academic journal, organization, chemical element, theory, chemical compound, university, location, scientist, award and O.\nSentence: She competed in the 4  100 metres relay event at the 2015 World Championships in Athletics in Beijing , China .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","competed","in","the","4","","100","metres","relay","event","at","the","2015","World","Championships","in","Athletics","in","Beijing",",","China","."],"labels":["O","O","O","O","B-event","I-event","I-event","I-event","I-event","I-event","O","O","B-event","I-event","I-event","O","O","O","B-location","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["event","country","astronomical_object","person","enzyme","protein","discipline","academic_journal","organization","chemical_element","theory","chemical_compound","university","location","scientist","award"]}
{"id":"47","dataset":"crossner_science","split":"dev","instance":{"id":"47","prompt_labels":"Tremaine(O) ,(O) along(O) with(O) Peter(B-scientist) Goldreich(I-scientist) ,(O) correctly(O) predicted(O) that(O) shepherd(B-astronomical object) moon(I-astronomical object) s(O) created(O) Saturn(B-astronomical object) '(O) s(O) thin(O) F(O) ring(O) ,(O) as(O) well(O) as(O) the(O) thin(O) rings(O) of(O) Uranus(B-astronomical object) in(O) 1979(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, academic journal, theory, chemical element, country, person, protein, location, discipline, organization, enzyme, chemical compound, award, astronomical object, event, university and O.\nSentence: Tremaine , along with Peter Goldreich , correctly predicted that shepherd moon s created Saturn ' s thin F ring , as well as the thin rings of Uranus in 1979 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Tremaine",",","along","with","Peter","Goldreich",",","correctly","predicted","that","shepherd","moon","s","created","Saturn","'","s","thin","F","ring",",","as","well","as","the","thin","rings","of","Uranus","in","1979","."],"labels":["O","O","O","O","B-scientist","I-scientist","O","O","O","O","B-astronomical object","I-astronomical object","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","academic_journal","theory","chemical_element","country","person","protein","location","discipline","organization","enzyme","chemical_compound","award","astronomical_object","event","university"]}
{"id":"51","dataset":"crossner_science","split":"dev","instance":{"id":"51","prompt_labels":"Genes(O) related(O) to(O) the(O) proximal(O) area(O) are(O) HFE2(O) ,(O) TXNIP(O) ,(O) POLR3GL(O) ,(O) LIX1L(O) ,(O) RBM8A(O) ,(O) PEX11B(O) ,(O) ITGA10(O) ,(O) ANKRD35(O) ,(O) PIAS3(O) ,(O) NUDT17(O) ,(O) POLR3C(O) ,(O) RNF115(O) ,(O) CD160(O) ,(O) PDZK1(O) ,(O) and(O) GPR89A(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, discipline, chemical compound, event, country, chemical element, award, protein, person, scientist, university, location, enzyme, academic journal, astronomical object, organization and O.\nSentence: Genes related to the proximal area are HFE2 , TXNIP , POLR3GL , LIX1L , RBM8A , PEX11B , ITGA10 , ANKRD35 , PIAS3 , NUDT17 , POLR3C , RNF115 , CD160 , PDZK1 , and GPR89A .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Genes","related","to","the","proximal","area","are","HFE2",",","TXNIP",",","POLR3GL",",","LIX1L",",","RBM8A",",","PEX11B",",","ITGA10",",","ANKRD35",",","PIAS3",",","NUDT17",",","POLR3C",",","RNF115",",","CD160",",","PDZK1",",","and","GPR89A","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["theory","discipline","chemical_compound","event","country","chemical_element","award","protein","person","scientist","university","location","enzyme","academic_journal","astronomical_object","organization"]}
{"id":"52","dataset":"crossner_science","split":"dev","instance":{"id":"52","prompt_labels":"From(O) 1884(O) to(O) 1888(O) he(O) studied(O) at(O) the(O) universities(O) of(O) Humboldt(B-university) University(I-university) of(I-university) Berlin(I-university) and(O) University(B-university) of(I-university) Strasbourg(I-university) ,(O) after(O) which(O) he(O) became(O) an(O) assistant(O) at(O) the(O) Academy(B-organization) of(I-organization) Mnster(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, theory, enzyme, protein, astronomical object, chemical compound, discipline, event, university, scientist, award, person, location, organization, chemical element, academic journal and O.\nSentence: From 1884 to 1888 he studied at the universities of Humboldt University of Berlin and University of Strasbourg , after which he became an assistant at the Academy of Mnster .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["From","1884","to","1888","he","studied","at","the","universities","of","Humboldt","University","of","Berlin","and","University","of","Strasbourg",",","after","which","he","became","an","assistant","at","the","Academy","of","Mnster","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-university","I-university","I-university","I-university","O","B-university","I-university","I-university","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["country","theory","enzyme","protein","astronomical_object","chemical_compound","discipline","event","university","scientist","award","person","location","organization","chemical_element","academic_journal"]}
{"id":"53","dataset":"crossner_science","split":"dev","instance":{"id":"53","prompt_labels":"The(O) effect(O) was(O) first(O) predicted(O) as(O) the(O) diffraction(O) of(O) electrons(O) from(O) a(O) standing(O) wave(O) of(O) light(O) by(O) Paul(B-scientist) Dirac(I-scientist) and(O) Pyotr(B-scientist) Kapitsa(I-scientist) ((O) or(O) Peter(B-scientist) Kapitza(I-scientist) )(O) in(O) 1933(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, academic journal, person, scientist, discipline, chemical compound, astronomical object, enzyme, theory, country, protein, organization, university, event, location, chemical element and O.\nSentence: The effect was first predicted as the diffraction of electrons from a standing wave of light by Paul Dirac and Pyotr Kapitsa ( or Peter Kapitza ) in 1933 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","effect","was","first","predicted","as","the","diffraction","of","electrons","from","a","standing","wave","of","light","by","Paul","Dirac","and","Pyotr","Kapitsa","(","or","Peter","Kapitza",")","in","1933","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","academic_journal","person","scientist","discipline","chemical_compound","astronomical_object","enzyme","theory","country","protein","organization","university","event","location","chemical_element"]}
{"id":"55","dataset":"crossner_science","split":"dev","instance":{"id":"55","prompt_labels":"Not(O) only(O) were(O) these(O) initiators(O) the(O) first(O) to(O) achieve(O) relatively(O) high(O) molecular(O) weight(O) poly(O) ((O) 1-alkenes(B-chemical compound) )(O) ((O) currently(O) the(O) most(O) widely(O) produced(O) thermoplastic(O) in(O) the(O) world(O) PE(B-chemical compound) ((O) Polyethylene(B-chemical compound) )(O) and(O) PP(B-chemical compound) ((O) Polypropylene(B-chemical compound) )(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, enzyme, organization, chemical compound, chemical element, university, academic journal, country, scientist, person, award, location, protein, theory, event, astronomical object and O.\nSentence: Not only were these initiators the first to achieve relatively high molecular weight poly ( 1-alkenes ) ( currently the most widely produced thermoplastic in the world PE ( Polyethylene ) and PP ( Polypropylene )","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Not","only","were","these","initiators","the","first","to","achieve","relatively","high","molecular","weight","poly","(","1-alkenes",")","(","currently","the","most","widely","produced","thermoplastic","in","the","world","PE","(","Polyethylene",")","and","PP","(","Polypropylene",")"],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","O","B-chemical compound","O","O","B-chemical compound","O","B-chemical compound","O"],"target_index":null,"target_label":null},"label_list":["discipline","enzyme","organization","chemical_compound","chemical_element","university","academic_journal","country","scientist","person","award","location","protein","theory","event","astronomical_object"]}
{"id":"56","dataset":"crossner_science","split":"dev","instance":{"id":"56","prompt_labels":"UCSI(B-university) University(I-university) ,(I-university) Sarawak(I-university) Campus(I-university) ,(O) University(B-university) College(I-university) of(I-university) Technology(I-university) Sarawak(I-university) ((O) UCTS(B-university) )(O) Tunku(B-university) Abdul(I-university) Rahman(I-university) University(I-university) College(I-university) ((I-university) Sabah(I-university) campus(I-university) )(I-university) ,(O) International(B-university) University(I-university) College(I-university) Of(I-university) Technology(I-university) Twintech(I-university) ((I-university) Sabah(I-university) campus(I-university) )(I-university) ,(O) and(O) Open(B-university) University(I-university) Malaysia(I-university) ((I-university) Sabah(I-university) campus(I-university) )(I-university) have(O) local(O) private(O) university(O) branch(O) campuses(O) in(O) East(B-location) Malaysia(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, university, enzyme, chemical element, academic journal, organization, event, theory, chemical compound, location, discipline, protein, astronomical object, person, scientist and O.\nSentence: UCSI University , Sarawak Campus , University College of Technology Sarawak ( UCTS ) Tunku Abdul Rahman University College ( Sabah campus ) , International University College Of Technology Twintech ( Sabah campus ) , and Open University Malaysia ( Sabah campus ) have local private university branch campuses in East Malaysia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["UCSI","University",",","Sarawak","Campus",",","University","College","of","Technology","Sarawak","(","UCTS",")","Tunku","Abdul","Rahman","University","College","(","Sabah","campus",")",",","International","University","College","Of","Technology","Twintech","(","Sabah","campus",")",",","and","Open","University","Malaysia","(","Sabah","campus",")","have","local","private","university","branch","campuses","in","East","Malaysia","."],"labels":["B-university","I-university","I-university","I-university","I-university","O","B-university","I-university","I-university","I-university","I-university","O","B-university","O","B-university","I-university","I-university","I-university","I-university","I-university","I-university","I-university","I-university","O","B-university","I-university","I-university","I-university","I-university","I-university","I-university","I-university","I-university","I-university","O","O","B-university","I-university","I-university","I-university","I-university","I-university","I-university","O","O","O","O","O","O","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["country","award","university","enzyme","chemical_element","academic_journal","organization","event","theory","chemical_compound","location","discipline","protein","astronomical_object","person","scientist"]}
{"id":"57","dataset":"crossner_science","split":"dev","instance":{"id":"57","prompt_labels":"He(O) also(O) included(O) perturbations(O) due(O) to(O) the(O) other(O) planets(O) ((O) principally(O) Jupiter(B-astronomical object) and(O) Venus(B-astronomical object) )(O) and(O) also(O) accounted(O) for(O) the(O) more(O) difficult(O) problem(O) of(O) the(O) non-spherical(O) nature(O) of(O) the(O) Earth(B-astronomical object) and(O) Moon(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, protein, country, theory, event, scientist, discipline, chemical element, person, organization, academic journal, location, enzyme, university, award, astronomical object and O.\nSentence: He also included perturbations due to the other planets ( principally Jupiter and Venus ) and also accounted for the more difficult problem of the non-spherical nature of the Earth and Moon .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","also","included","perturbations","due","to","the","other","planets","(","principally","Jupiter","and","Venus",")","and","also","accounted","for","the","more","difficult","problem","of","the","non-spherical","nature","of","the","Earth","and","Moon","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","protein","country","theory","event","scientist","discipline","chemical_element","person","organization","academic_journal","location","enzyme","university","award","astronomical_object"]}
{"id":"58","dataset":"crossner_science","split":"dev","instance":{"id":"58","prompt_labels":"Thompson(B-scientist) endowed(O) the(O) Rumford(B-award) medal(I-award) s(O) of(O) the(O) Royal(B-organization) Society(I-organization) and(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) ,(O) and(O) endowed(O) a(O) professorship(O) at(O) Harvard(B-university) University(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, astronomical object, person, protein, location, scientist, university, organization, chemical compound, country, enzyme, award, theory, event, academic journal, chemical element and O.\nSentence: Thompson endowed the Rumford medal s of the Royal Society and the American Academy of Arts and Sciences , and endowed a professorship at Harvard University .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Thompson","endowed","the","Rumford","medal","s","of","the","Royal","Society","and","the","American","Academy","of","Arts","and","Sciences",",","and","endowed","a","professorship","at","Harvard","University","."],"labels":["B-scientist","O","O","B-award","I-award","O","O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","B-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["discipline","astronomical_object","person","protein","location","scientist","university","organization","chemical_compound","country","enzyme","award","theory","event","academic_journal","chemical_element"]}
{"id":"61","dataset":"crossner_science","split":"dev","instance":{"id":"61","prompt_labels":"Robot(O) designer(O) Hans(B-scientist) Moravec(I-scientist) ,(O) cyberneticist(O) Kevin(B-scientist) Warwick(I-scientist) and(O) inventor(O) Ray(B-person) Kurzweil(I-person) have(O) predicted(O) that(O) humans(O) and(O) machines(O) will(O) merge(O) in(O) the(O) future(O) into(O) cyborg(O) s(O) that(O) are(O) more(O) capable(O) and(O) powerful(O) than(O) either(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, university, award, astronomical object, event, location, enzyme, academic journal, chemical element, chemical compound, discipline, theory, scientist, protein, country and O.\nSentence: Robot designer Hans Moravec , cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborg s that are more capable and powerful than either .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Robot","designer","Hans","Moravec",",","cyberneticist","Kevin","Warwick","and","inventor","Ray","Kurzweil","have","predicted","that","humans","and","machines","will","merge","in","the","future","into","cyborg","s","that","are","more","capable","and","powerful","than","either","."],"labels":["O","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","person","university","award","astronomical_object","event","location","enzyme","academic_journal","chemical_element","chemical_compound","discipline","theory","scientist","protein","country"]}
{"id":"62","dataset":"crossner_science","split":"dev","instance":{"id":"62","prompt_labels":"Matja(B-scientist) Perc(I-scientist) is(O) editorial(O) board(O) member(O) at(O) Physical(B-academic journal) Review(I-academic journal) E(I-academic journal) ,(O) New(B-academic journal) Journal(I-academic journal) of(I-academic journal) Physics(I-academic journal) ,(O) EPL(B-academic journal) ,(O) European(B-academic journal) Physical(I-academic journal) Journal(I-academic journal) B(I-academic journal) ,(O) Advances(B-academic journal) in(I-academic journal) Complex(I-academic journal) Systems(I-academic journal) ,(O) Frontiers(B-academic journal) in(I-academic journal) Interdisciplinary(I-academic journal) Physics(I-academic journal) ,(O) International(B-academic journal) Journal(I-academic journal) of(I-academic journal) Bifurcation(I-academic journal) and(I-academic journal) Chaos(I-academic journal) ,(O) PLOS(B-academic journal) ONE(I-academic journal) ,(O) Scientific(B-academic journal) Reports(I-academic journal) ,(O) Royal(B-academic journal) Society(I-academic journal) Open(I-academic journal) Science(I-academic journal) ,(O) '(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, chemical compound, university, enzyme, event, protein, organization, academic journal, award, scientist, country, discipline, theory, location, person, astronomical object and O.\nSentence: Matja Perc is editorial board member at Physical Review E , New Journal of Physics , EPL , European Physical Journal B , Advances in Complex Systems , Frontiers in Interdisciplinary Physics , International Journal of Bifurcation and Chaos , PLOS ONE , Scientific Reports , Royal Society Open Science , ' .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Matja","Perc","is","editorial","board","member","at","Physical","Review","E",",","New","Journal","of","Physics",",","EPL",",","European","Physical","Journal","B",",","Advances","in","Complex","Systems",",","Frontiers","in","Interdisciplinary","Physics",",","International","Journal","of","Bifurcation","and","Chaos",",","PLOS","ONE",",","Scientific","Reports",",","Royal","Society","Open","Science",",","'","."],"labels":["B-scientist","I-scientist","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","chemical_compound","university","enzyme","event","protein","organization","academic_journal","award","scientist","country","discipline","theory","location","person","astronomical_object"]}
{"id":"68","dataset":"crossner_science","split":"dev","instance":{"id":"68","prompt_labels":"The(O) concept(O) that(O) the(O) composition(O) of(O) plant(O) communities(O) such(O) as(O) temperate(O) broadleaf(O) forest(O) changes(O) by(O) a(O) process(O) of(O) ecological(O) succession(O) was(O) developed(O) by(O) Henry(B-scientist) Chandler(I-scientist) Cowles(I-scientist) ,(O) Arthur(B-scientist) Tansley(I-scientist) and(O) Frederic(B-scientist) Clements(I-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, chemical element, scientist, university, astronomical object, chemical compound, organization, protein, country, enzyme, location, person, award, event, academic journal, discipline and O.\nSentence: The concept that the composition of plant communities such as temperate broadleaf forest changes by a process of ecological succession was developed by Henry Chandler Cowles , Arthur Tansley and Frederic Clements .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","concept","that","the","composition","of","plant","communities","such","as","temperate","broadleaf","forest","changes","by","a","process","of","ecological","succession","was","developed","by","Henry","Chandler","Cowles",",","Arthur","Tansley","and","Frederic","Clements","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O"],"target_index":null,"target_label":null},"label_list":["theory","chemical_element","scientist","university","astronomical_object","chemical_compound","organization","protein","country","enzyme","location","person","award","event","academic_journal","discipline"]}
{"id":"70","dataset":"crossner_science","split":"dev","instance":{"id":"70","prompt_labels":"FastPP(O) can(O) be(O) used(O) on(O) unpurified(O) ,(O) complex(O) mixtures(O) of(O) proteins(O) and(O) proteins(O) fused(O) with(O) other(O) proteins(O) ,(O) such(O) as(O) Glutathione(B-protein) S-transferase(I-protein) or(O) Green(B-protein) fluorescent(I-protein) protein(I-protein) ,(O) as(O) long(O) as(O) the(O) sequence(O) that(O) is(O) the(O) target(O) of(O) the(O) western(O) blot(O) ,(O) e.g.(O) ,(O) His-tag(O) ,(O) is(O) directly(O) linked(O) to(O) the(O) protein(O) of(O) interest(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, location, academic journal, country, award, theory, discipline, person, protein, enzyme, university, organization, event, scientist, astronomical object, chemical element and O.\nSentence: FastPP can be used on unpurified , complex mixtures of proteins and proteins fused with other proteins , such as Glutathione S-transferase or Green fluorescent protein , as long as the sequence that is the target of the western blot , e.g. , His-tag , is directly linked to the protein of interest .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["FastPP","can","be","used","on","unpurified",",","complex","mixtures","of","proteins","and","proteins","fused","with","other","proteins",",","such","as","Glutathione","S-transferase","or","Green","fluorescent","protein",",","as","long","as","the","sequence","that","is","the","target","of","the","western","blot",",","e.g.",",","His-tag",",","is","directly","linked","to","the","protein","of","interest","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-protein","I-protein","O","B-protein","I-protein","I-protein","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","location","academic_journal","country","award","theory","discipline","person","protein","enzyme","university","organization","event","scientist","astronomical_object","chemical_element"]}
{"id":"71","dataset":"crossner_science","split":"dev","instance":{"id":"71","prompt_labels":"The(O) theory(O) has(O) been(O) published(O) in(O) three(O) peer-reviewed(O) journals(O) :(O) The(B-academic journal) Quarterly(I-academic journal) Review(I-academic journal) of(I-academic journal) Biology(I-academic journal) ,(O) Evolutionary(B-academic journal) Anthropology(I-academic journal) and(O) the(O) Journal(B-academic journal) of(I-academic journal) Theoretical(I-academic journal) Biology(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, protein, scientist, event, country, academic journal, person, award, organization, university, location, discipline, chemical element, theory, astronomical object, enzyme and O.\nSentence: The theory has been published in three peer-reviewed journals : The Quarterly Review of Biology , Evolutionary Anthropology and the Journal of Theoretical Biology .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","theory","has","been","published","in","three","peer-reviewed","journals",":","The","Quarterly","Review","of","Biology",",","Evolutionary","Anthropology","and","the","Journal","of","Theoretical","Biology","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","protein","scientist","event","country","academic_journal","person","award","organization","university","location","discipline","chemical_element","theory","astronomical_object","enzyme"]}
{"id":"78","dataset":"crossner_science","split":"dev","instance":{"id":"78","prompt_labels":"He(O) was(O) member(O) of(O) the(O) Deutsche(B-university) Akademie(I-university) of(I-university) Munich(I-university) ,(O) Swiss(B-organization) Physical(I-organization) Society(I-organization) of(I-organization) Zrich(I-organization) ,(O) Royal(B-organization) Philosophical(I-organization) Society(I-organization) of(I-organization) Glasgow(I-organization) ,(O) Royal(B-organization) Irish(I-organization) Academy(I-organization) ,(O) Hungarian(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) Academy(B-organization) of(I-organization) Sciences(I-organization) of(I-organization) the(I-organization) U.S.S.R.(I-organization) ,(O) Optical(B-organization) Society(I-organization) of(I-organization) America(I-organization) and(O) Mineralogical(B-organization) Society(I-organization) of(I-organization) America(I-organization) ,(O) Romanian(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) Catgut(B-organization) Acoustical(I-organization) Society(I-organization) of(I-organization) America(I-organization) ,(O) and(O) Czechoslovak(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, event, award, discipline, chemical compound, astronomical object, country, university, enzyme, theory, scientist, protein, academic journal, location, person, organization and O.\nSentence: He was member of the Deutsche Akademie of Munich , Swiss Physical Society of Zrich , Royal Philosophical Society of Glasgow , Royal Irish Academy , Hungarian Academy of Sciences , Academy of Sciences of the U.S.S.R. , Optical Society of America and Mineralogical Society of America , Romanian Academy of Sciences , Catgut Acoustical Society of America , and Czechoslovak Academy of Sciences .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","member","of","the","Deutsche","Akademie","of","Munich",",","Swiss","Physical","Society","of","Zrich",",","Royal","Philosophical","Society","of","Glasgow",",","Royal","Irish","Academy",",","Hungarian","Academy","of","Sciences",",","Academy","of","Sciences","of","the","U.S.S.R.",",","Optical","Society","of","America","and","Mineralogical","Society","of","America",",","Romanian","Academy","of","Sciences",",","Catgut","Acoustical","Society","of","America",",","and","Czechoslovak","Academy","of","Sciences","."],"labels":["O","O","O","O","O","B-university","I-university","I-university","I-university","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","event","award","discipline","chemical_compound","astronomical_object","country","university","enzyme","theory","scientist","protein","academic_journal","location","person","organization"]}
{"id":"81","dataset":"crossner_science","split":"dev","instance":{"id":"81","prompt_labels":"ESA(B-organization) 's(O) Advanced(B-organization) Concepts(I-organization) Team(I-organization) has(O) also(O) demonstrated(O) theoretically(O) that(O) a(O) deflection(O) of(O) 99942(B-astronomical object) Apophis(I-astronomical object) could(O) be(O) achieved(O) by(O) sending(O) a(O) simple(O) spacecraft(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, chemical compound, award, location, enzyme, organization, university, theory, protein, person, academic journal, scientist, chemical element, astronomical object, country, event and O.\nSentence: ESA 's Advanced Concepts Team has also demonstrated theoretically that a deflection of 99942 Apophis could be achieved by sending a simple spacecraft","prediction_output":null,"prediction_outputs":null,"group":null,"words":["ESA","'s","Advanced","Concepts","Team","has","also","demonstrated","theoretically","that","a","deflection","of","99942","Apophis","could","be","achieved","by","sending","a","simple","spacecraft"],"labels":["B-organization","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","B-astronomical object","I-astronomical object","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["discipline","chemical_compound","award","location","enzyme","organization","university","theory","protein","person","academic_journal","scientist","chemical_element","astronomical_object","country","event"]}
{"id":"83","dataset":"crossner_science","split":"dev","instance":{"id":"83","prompt_labels":"Other(O) higher(O) education(O) organizations(O) present(O) in(O) the(O) community(O) ,(O) but(O) not(O) offering(O) classes(O) locally(O) ,(O) include(O) the(O) Oak(B-organization) Ridge(I-organization) Institute(I-organization) for(I-organization) Science(I-organization) and(I-organization) Education(I-organization) ,(O) Oak(B-university) Ridge(I-university) Associated(I-university) Universities(I-university) ,(O) and(O) the(O) University(B-university) of(I-university) Tennessee(I-university) Forestry(B-organization) Stations(I-organization) and(O) Arboretum(B-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, academic journal, event, theory, person, university, scientist, discipline, country, protein, chemical compound, astronomical object, chemical element, enzyme, location and O.\nSentence: Other higher education organizations present in the community , but not offering classes locally , include the Oak Ridge Institute for Science and Education , Oak Ridge Associated Universities , and the University of Tennessee Forestry Stations and Arboretum .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","higher","education","organizations","present","in","the","community",",","but","not","offering","classes","locally",",","include","the","Oak","Ridge","Institute","for","Science","and","Education",",","Oak","Ridge","Associated","Universities",",","and","the","University","of","Tennessee","Forestry","Stations","and","Arboretum","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-university","I-university","I-university","I-university","O","O","O","B-university","I-university","I-university","B-organization","I-organization","O","B-organization","O"],"target_index":null,"target_label":null},"label_list":["organization","award","academic_journal","event","theory","person","university","scientist","discipline","country","protein","chemical_compound","astronomical_object","chemical_element","enzyme","location"]}
{"id":"84","dataset":"crossner_science","split":"dev","instance":{"id":"84","prompt_labels":"In(O) the(O) first(O) half(O) of(O) the(O) 20th(O) century(O) ,(O) advances(O) in(O) electronics(O) enabled(O) investigation(O) of(O) the(O) electrical(O) properties(O) of(O) nerve(O) cells(O) ,(O) culminating(O) in(O) work(O) by(O) Alan(B-scientist) Hodgkin(I-scientist) ,(O) Andrew(B-scientist) Huxley(I-scientist) ,(O) and(O) others(O) on(O) the(O) biophysics(O) of(O) the(O) action(O) potential(O) ,(O) and(O) the(O) work(O) of(O) Bernard(B-scientist) Katz(I-scientist) and(O) others(O) on(O) the(O) electrochemistry(B-discipline) of(O) the(O) synapse(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, scientist, academic journal, enzyme, protein, university, chemical element, person, theory, country, event, discipline, award, chemical compound, location, astronomical object and O.\nSentence: In the first half of the 20th century , advances in electronics enabled investigation of the electrical properties of nerve cells , culminating in work by Alan Hodgkin , Andrew Huxley , and others on the biophysics of the action potential , and the work of Bernard Katz and others on the electrochemistry of the synapse .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","first","half","of","the","20th","century",",","advances","in","electronics","enabled","investigation","of","the","electrical","properties","of","nerve","cells",",","culminating","in","work","by","Alan","Hodgkin",",","Andrew","Huxley",",","and","others","on","the","biophysics","of","the","action","potential",",","and","the","work","of","Bernard","Katz","and","others","on","the","electrochemistry","of","the","synapse","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","O","O","O","B-discipline","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","scientist","academic_journal","enzyme","protein","university","chemical_element","person","theory","country","event","discipline","award","chemical_compound","location","astronomical_object"]}
{"id":"85","dataset":"crossner_science","split":"dev","instance":{"id":"85","prompt_labels":"Jupiter(B-astronomical object) rarely(O) occults(O) Saturn(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, person, scientist, academic journal, theory, chemical compound, organization, country, enzyme, chemical element, location, discipline, university, astronomical object, award, event and O.\nSentence: Jupiter rarely occults Saturn .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Jupiter","rarely","occults","Saturn","."],"labels":["B-astronomical object","O","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["protein","person","scientist","academic_journal","theory","chemical_compound","organization","country","enzyme","chemical_element","location","discipline","university","astronomical_object","award","event"]}
{"id":"87","dataset":"crossner_science","split":"dev","instance":{"id":"87","prompt_labels":"He(O) is(O) known(O) for(O) his(O) studies(O) on(O) the(O) Pore-forming(B-protein) toxin(I-protein) and(O) T-cell(O) costimulatory(O) molecules(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, academic journal, country, university, theory, enzyme, location, discipline, scientist, protein, person, chemical compound, astronomical object, chemical element, organization and O.\nSentence: He is known for his studies on the Pore-forming toxin and T-cell costimulatory molecules .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","is","known","for","his","studies","on","the","Pore-forming","toxin","and","T-cell","costimulatory","molecules","."],"labels":["O","O","O","O","O","O","O","O","B-protein","I-protein","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","award","academic_journal","country","university","theory","enzyme","location","discipline","scientist","protein","person","chemical_compound","astronomical_object","chemical_element","organization"]}
{"id":"88","dataset":"crossner_science","split":"dev","instance":{"id":"88","prompt_labels":"The(O) name(O) was(O) suggested(O) by(O) John(B-scientist) Herschel(I-scientist) ((O) son(O) of(O) William(B-scientist) Herschel(I-scientist) ,(O) discoverer(O) of(O) Mimas(B-astronomical object) and(O) Enceladus(B-astronomical object) )(O) in(O) his(O) 1847(O) publication(O) Results(O) of(O) Astronomical(O) Observations(O) made(O) at(O) the(O) Cape(B-location) of(I-location) Good(I-location) Hope(I-location) ,(O) in(O) which(O) he(O) advocated(O) naming(O) the(O) moons(O) of(O) Saturn(B-astronomical object) after(O) the(O) Titans(B-astronomical object) ,(O) brothers(O) and(O) sisters(O) of(O) the(O) Titan(B-person) Cronus(I-person) ((O) whom(O) the(O) Romans(O) equated(O) with(O) their(O) god(O) Saturn(B-astronomical object) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, astronomical object, theory, academic journal, location, university, person, country, protein, organization, award, enzyme, discipline, chemical element, event, scientist and O.\nSentence: The name was suggested by John Herschel ( son of William Herschel , discoverer of Mimas and Enceladus ) in his 1847 publication Results of Astronomical Observations made at the Cape of Good Hope , in which he advocated naming the moons of Saturn after the Titans , brothers and sisters of the Titan Cronus ( whom the Romans equated with their god Saturn ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","name","was","suggested","by","John","Herschel","(","son","of","William","Herschel",",","discoverer","of","Mimas","and","Enceladus",")","in","his","1847","publication","Results","of","Astronomical","Observations","made","at","the","Cape","of","Good","Hope",",","in","which","he","advocated","naming","the","moons","of","Saturn","after","the","Titans",",","brothers","and","sisters","of","the","Titan","Cronus","(","whom","the","Romans","equated","with","their","god","Saturn",")","."],"labels":["O","O","O","O","O","B-scientist","I-scientist","O","O","O","B-scientist","I-scientist","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","I-location","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","B-astronomical object","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","B-astronomical object","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","astronomical_object","theory","academic_journal","location","university","person","country","protein","organization","award","enzyme","discipline","chemical_element","event","scientist"]}
{"id":"89","dataset":"crossner_science","split":"dev","instance":{"id":"89","prompt_labels":"The(O) testing(O) station(O) is(O) where(O) Rexer(B-scientist) ,(O) F.(B-scientist) Berkei(I-scientist) ,(O) W.(B-scientist) Borrmann(I-scientist) ,(O) W.(B-scientist) Czulius(I-scientist) ,(O) Kurt(B-scientist) Diebner(I-scientist) ,(O) Georg(B-scientist) Hartwig(I-scientist) ,(O) Karl-Heinz(B-scientist) Hcker(I-scientist) ,(O) Walter(B-scientist) Herrmann(I-scientist) ,(O) and(O) Heinz(B-scientist) Pose(I-scientist) ,(O) compared(O) the(O) effectiveness(O) of(O) neutron(O) production(O) in(O) a(O) paraffin-moderated(O) reactor(O) using(O) uranium(B-chemical element) plates(O) ,(O) rods(O) ,(O) and(O) cubes(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, chemical compound, person, event, country, academic journal, university, theory, scientist, protein, astronomical object, organization, award, location, discipline, chemical element and O.\nSentence: The testing station is where Rexer , F. Berkei , W. Borrmann , W. Czulius , Kurt Diebner , Georg Hartwig , Karl-Heinz Hcker , Walter Herrmann , and Heinz Pose , compared the effectiveness of neutron production in a paraffin-moderated reactor using uranium plates , rods , and cubes .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","testing","station","is","where","Rexer",",","F.","Berkei",",","W.","Borrmann",",","W.","Czulius",",","Kurt","Diebner",",","Georg","Hartwig",",","Karl-Heinz","Hcker",",","Walter","Herrmann",",","and","Heinz","Pose",",","compared","the","effectiveness","of","neutron","production","in","a","paraffin-moderated","reactor","using","uranium","plates",",","rods",",","and","cubes","."],"labels":["O","O","O","O","O","B-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical element","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["enzyme","chemical_compound","person","event","country","academic_journal","university","theory","scientist","protein","astronomical_object","organization","award","location","discipline","chemical_element"]}
{"id":"94","dataset":"crossner_science","split":"dev","instance":{"id":"94","prompt_labels":"Some(O) ligninolytic(O) enzymes(O) include(O) Haem(B-enzyme) peroxidase(I-enzyme) such(O) as(O) lignin(B-enzyme) peroxidase(I-enzyme) s(O) ,(O) manganese(B-enzyme) peroxidase(I-enzyme) s(O) ,(O) versatile(B-enzyme) peroxidase(I-enzyme) s(O) ,(O) and(O) Dye(B-enzyme) decolorizing(I-enzyme) peroxidase(I-enzyme) as(O) well(O) as(O) copper-based(O) laccase(B-enzyme) s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, theory, country, discipline, enzyme, person, location, organization, academic journal, protein, scientist, chemical compound, university, event, award, astronomical object and O.\nSentence: Some ligninolytic enzymes include Haem peroxidase such as lignin peroxidase s , manganese peroxidase s , versatile peroxidase s , and Dye decolorizing peroxidase as well as copper-based laccase s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","ligninolytic","enzymes","include","Haem","peroxidase","such","as","lignin","peroxidase","s",",","manganese","peroxidase","s",",","versatile","peroxidase","s",",","and","Dye","decolorizing","peroxidase","as","well","as","copper-based","laccase","s","."],"labels":["O","O","O","O","B-enzyme","I-enzyme","O","O","B-enzyme","I-enzyme","O","O","B-enzyme","I-enzyme","O","O","B-enzyme","I-enzyme","O","O","O","B-enzyme","I-enzyme","I-enzyme","O","O","O","O","B-enzyme","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","theory","country","discipline","enzyme","person","location","organization","academic_journal","protein","scientist","chemical_compound","university","event","award","astronomical_object"]}
{"id":"96","dataset":"crossner_science","split":"dev","instance":{"id":"96","prompt_labels":"Gas(O) giants(O) with(O) a(O) large(O) radius(O) and(O) very(O) low(O) density(O) are(O) sometimes(O) called(O) puffy(O) planets(O) COROT-1b(B-astronomical object) ,(O) TrES-4(B-astronomical object) ,(O) WASP-12b(B-astronomical object) ,(O) WASP-17b(B-astronomical object) ,(O) and(O) Kepler-7b(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, astronomical object, organization, theory, event, chemical compound, award, location, discipline, protein, person, academic journal, university, enzyme, scientist, country and O.\nSentence: Gas giants with a large radius and very low density are sometimes called puffy planets COROT-1b , TrES-4 , WASP-12b , WASP-17b , and Kepler-7b .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Gas","giants","with","a","large","radius","and","very","low","density","are","sometimes","called","puffy","planets","COROT-1b",",","TrES-4",",","WASP-12b",",","WASP-17b",",","and","Kepler-7b","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","astronomical_object","organization","theory","event","chemical_compound","award","location","discipline","protein","person","academic_journal","university","enzyme","scientist","country"]}
{"id":"97","dataset":"crossner_science","split":"dev","instance":{"id":"97","prompt_labels":"ATX-II(B-chemical compound) slows(O) down(O) the(O) inactivation(O) of(O) different(O) Voltage-gated(O) ion(O) channel(O) ,(O) including(O) Nasubv(B-protein) /(I-protein) sub1.1(I-protein) and(O) Nasubv(B-protein) /(I-protein) sub1.2(I-protein) ,(O) thus(O) prolonging(O) action(O) potentials(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, country, discipline, location, person, organization, enzyme, protein, chemical element, award, theory, event, scientist, astronomical object, chemical compound, university and O.\nSentence: ATX-II slows down the inactivation of different Voltage-gated ion channel , including Nasubv / sub1.1 and Nasubv / sub1.2 , thus prolonging action potentials .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["ATX-II","slows","down","the","inactivation","of","different","Voltage-gated","ion","channel",",","including","Nasubv","/","sub1.1","and","Nasubv","/","sub1.2",",","thus","prolonging","action","potentials","."],"labels":["B-chemical compound","O","O","O","O","O","O","O","O","O","O","O","B-protein","I-protein","I-protein","O","B-protein","I-protein","I-protein","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","country","discipline","location","person","organization","enzyme","protein","chemical_element","award","theory","event","scientist","astronomical_object","chemical_compound","university"]}
{"id":"101","dataset":"crossner_science","split":"dev","instance":{"id":"101","prompt_labels":"Eggleton(B-scientist) is(O) the(O) author(O) and(O) coauthor(O) of(O) more(O) than(O) 480(O) journal(O) publications(O) ,(O) including(O) articles(O) in(O) Nature(B-academic journal) Photonics(I-academic journal) ,(O) Nature(B-academic journal) Physics(I-academic journal) ,(O) Nature(B-academic journal) Communications(I-academic journal) ,(O) Physical(B-academic journal) Review(I-academic journal) Letters(I-academic journal) and(O) Optica(B-academic journal) and(O) over(O) 200(O) invited(O) presentations(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, theory, university, country, chemical element, academic journal, chemical compound, person, award, protein, location, enzyme, discipline, astronomical object, scientist and O.\nSentence: Eggleton is the author and coauthor of more than 480 journal publications , including articles in Nature Photonics , Nature Physics , Nature Communications , Physical Review Letters and Optica and over 200 invited presentations .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Eggleton","is","the","author","and","coauthor","of","more","than","480","journal","publications",",","including","articles","in","Nature","Photonics",",","Nature","Physics",",","Nature","Communications",",","Physical","Review","Letters","and","Optica","and","over","200","invited","presentations","."],"labels":["B-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","organization","theory","university","country","chemical_element","academic_journal","chemical_compound","person","award","protein","location","enzyme","discipline","astronomical_object","scientist"]}
{"id":"102","dataset":"crossner_science","split":"dev","instance":{"id":"102","prompt_labels":"It(O) is(O) operated(O) by(O) the(O) National(B-organization) Centre(I-organization) for(I-organization) Radio(I-organization) Astrophysics(I-organization) ,(O) a(O) part(O) of(O) the(O) Tata(B-organization) Institute(I-organization) of(I-organization) Fundamental(I-organization) Research(I-organization) ,(O) Mumbai(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, academic journal, astronomical object, scientist, enzyme, discipline, event, organization, protein, country, university, theory, location, chemical element, person, award and O.\nSentence: It is operated by the National Centre for Radio Astrophysics , a part of the Tata Institute of Fundamental Research , Mumbai .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","is","operated","by","the","National","Centre","for","Radio","Astrophysics",",","a","part","of","the","Tata","Institute","of","Fundamental","Research",",","Mumbai","."],"labels":["O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","academic_journal","astronomical_object","scientist","enzyme","discipline","event","organization","protein","country","university","theory","location","chemical_element","person","award"]}
{"id":"103","dataset":"crossner_science","split":"dev","instance":{"id":"103","prompt_labels":"The(O) Council(B-organization) of(I-organization) Europe(I-organization) also(O) has(O) a(O) Congress(B-organization) of(I-organization) the(I-organization) Council(I-organization) of(I-organization) Europe(I-organization) ,(O) similar(O) to(O) the(O) EU(B-organization) 's(I-organization) Committee(I-organization) of(I-organization) the(I-organization) Regions(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, enzyme, chemical compound, academic journal, event, location, award, theory, university, scientist, discipline, astronomical object, chemical element, protein, organization, person and O.\nSentence: The Council of Europe also has a Congress of the Council of Europe , similar to the EU 's Committee of the Regions .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Council","of","Europe","also","has","a","Congress","of","the","Council","of","Europe",",","similar","to","the","EU","'s","Committee","of","the","Regions","."],"labels":["O","B-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["country","enzyme","chemical_compound","academic_journal","event","location","award","theory","university","scientist","discipline","astronomical_object","chemical_element","protein","organization","person"]}
{"id":"106","dataset":"crossner_science","split":"dev","instance":{"id":"106","prompt_labels":"Cotton(B-scientist) served(O) on(O) various(O) editorial(O) boards(O) of(O) scientific(O) journals(O) ,(O) including(O) those(O) of(O) the(O) Journal(B-academic journal) of(I-academic journal) the(I-academic journal) American(I-academic journal) Chemical(I-academic journal) Society(I-academic journal) ,(O) Inorganic(B-academic journal) Chemistry(I-academic journal) ,(O) and(O) Organometallics(B-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, organization, chemical element, academic journal, event, enzyme, person, country, discipline, university, award, astronomical object, chemical compound, location, scientist, protein and O.\nSentence: Cotton served on various editorial boards of scientific journals , including those of the Journal of the American Chemical Society , Inorganic Chemistry , and Organometallics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Cotton","served","on","various","editorial","boards","of","scientific","journals",",","including","those","of","the","Journal","of","the","American","Chemical","Society",",","Inorganic","Chemistry",",","and","Organometallics","."],"labels":["B-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","O","B-academic journal","O"],"target_index":null,"target_label":null},"label_list":["theory","organization","chemical_element","academic_journal","event","enzyme","person","country","discipline","university","award","astronomical_object","chemical_compound","location","scientist","protein"]}
{"id":"108","dataset":"crossner_science","split":"dev","instance":{"id":"108","prompt_labels":"The(O) pyrimidines(B-chemical compound) ,(O) thymine(B-chemical compound) ,(O) cytosine(B-chemical compound) and(O) uracil(B-chemical compound) ,(O) form(O) the(O) complementary(O) bases(O) to(O) the(O) purine(B-chemical compound) bases(O) in(O) DNA(O) and(O) RNA(O) ,(O) and(O) are(O) also(O) components(O) of(O) Cytidine(B-chemical compound) triphosphate(I-chemical compound) ,(O) Uridine(B-chemical compound) monophosphate(I-chemical compound) ,(O) Uridine(B-chemical compound) diphosphate(I-chemical compound) and(O) Uridine(B-chemical compound) triphosphate(I-chemical compound) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, university, chemical element, location, academic journal, organization, award, country, discipline, event, scientist, person, astronomical object, protein, chemical compound, enzyme and O.\nSentence: The pyrimidines , thymine , cytosine and uracil , form the complementary bases to the purine bases in DNA and RNA , and are also components of Cytidine triphosphate , Uridine monophosphate , Uridine diphosphate and Uridine triphosphate .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","pyrimidines",",","thymine",",","cytosine","and","uracil",",","form","the","complementary","bases","to","the","purine","bases","in","DNA","and","RNA",",","and","are","also","components","of","Cytidine","triphosphate",",","Uridine","monophosphate",",","Uridine","diphosphate","and","Uridine","triphosphate","."],"labels":["O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","O","O","O","O","O","O","B-chemical compound","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","O"],"target_index":null,"target_label":null},"label_list":["theory","university","chemical_element","location","academic_journal","organization","award","country","discipline","event","scientist","person","astronomical_object","protein","chemical_compound","enzyme"]}
{"id":"109","dataset":"crossner_science","split":"dev","instance":{"id":"109","prompt_labels":"He(O) has(O) received(O) Academy(B-award) Awards(I-award) ,(O) Grammy(B-award) Award(I-award) ,(O) and(O) Golden(B-award) Globe(I-award) Award(I-award) s(O) ,(O) and(O) he(O) is(O) an(O) inductee(O) to(O) the(O) Rock(B-organization) and(I-organization) Roll(I-organization) Hall(I-organization) of(I-organization) Fame(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, discipline, university, country, event, chemical element, organization, award, academic journal, theory, location, scientist, chemical compound, person, enzyme, astronomical object and O.\nSentence: He has received Academy Awards , Grammy Award , and Golden Globe Award s , and he is an inductee to the Rock and Roll Hall of Fame .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","has","received","Academy","Awards",",","Grammy","Award",",","and","Golden","Globe","Award","s",",","and","he","is","an","inductee","to","the","Rock","and","Roll","Hall","of","Fame","."],"labels":["O","O","O","B-award","I-award","O","B-award","I-award","O","O","B-award","I-award","I-award","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["protein","discipline","university","country","event","chemical_element","organization","award","academic_journal","theory","location","scientist","chemical_compound","person","enzyme","astronomical_object"]}
{"id":"110","dataset":"crossner_science","split":"dev","instance":{"id":"110","prompt_labels":"He(O) was(O) also(O) awarded(O) the(O) Eddington(B-award) Medal(I-award) of(O) the(O) Royal(B-organization) Astronomical(I-organization) Society(I-organization) in(O) 1969(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, enzyme, location, astronomical object, country, scientist, organization, chemical element, discipline, event, person, award, academic journal, protein, university, chemical compound and O.\nSentence: He was also awarded the Eddington Medal of the Royal Astronomical Society in 1969 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","also","awarded","the","Eddington","Medal","of","the","Royal","Astronomical","Society","in","1969","."],"labels":["O","O","O","O","O","B-award","I-award","O","O","B-organization","I-organization","I-organization","O","O","O"],"target_index":null,"target_label":null},"label_list":["theory","enzyme","location","astronomical_object","country","scientist","organization","chemical_element","discipline","event","person","award","academic_journal","protein","university","chemical_compound"]}
{"id":"113","dataset":"crossner_science","split":"dev","instance":{"id":"113","prompt_labels":"Major(O) species(O) assessors(O) include(O) BirdLife(B-organization) International(I-organization) ,(O) the(O) Institute(B-organization) of(I-organization) Zoology(I-organization) ((O) the(O) research(O) division(O) of(O) the(O) Zoological(B-organization) Society(I-organization) of(I-organization) London(I-organization) )(O) ,(O) the(O) World(B-organization) Conservation(I-organization) Monitoring(I-organization) Centre(I-organization) ,(O) and(O) many(O) Specialist(O) Groups(O) within(O) the(O) IUCN(B-organization) Species(I-organization) Survival(I-organization) Commission(I-organization) ((O) SSC(B-organization) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, chemical element, country, discipline, person, university, award, academic journal, astronomical object, protein, location, scientist, chemical compound, organization, event, theory and O.\nSentence: Major species assessors include BirdLife International , the Institute of Zoology ( the research division of the Zoological Society of London ) , the World Conservation Monitoring Centre , and many Specialist Groups within the IUCN Species Survival Commission ( SSC ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Major","species","assessors","include","BirdLife","International",",","the","Institute","of","Zoology","(","the","research","division","of","the","Zoological","Society","of","London",")",",","the","World","Conservation","Monitoring","Centre",",","and","many","Specialist","Groups","within","the","IUCN","Species","Survival","Commission","(","SSC",")","."],"labels":["O","O","O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","O","O"],"target_index":null,"target_label":null},"label_list":["enzyme","chemical_element","country","discipline","person","university","award","academic_journal","astronomical_object","protein","location","scientist","chemical_compound","organization","event","theory"]}
{"id":"114","dataset":"crossner_science","split":"dev","instance":{"id":"114","prompt_labels":"Uranus(B-astronomical object) is(O) similar(O) in(O) composition(O) to(O) Neptune(B-astronomical object) ,(O) and(O) both(O) have(O) bulk(O) chemical(O) compositions(O) which(O) differ(O) from(O) that(O) of(O) the(O) larger(O) gas(O) giant(O) s(O) Jupiter(B-astronomical object) and(O) Saturn(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, university, organization, person, protein, enzyme, scientist, chemical element, astronomical object, discipline, chemical compound, theory, country, location, academic journal and O.\nSentence: Uranus is similar in composition to Neptune , and both have bulk chemical compositions which differ from that of the larger gas giant s Jupiter and Saturn .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Uranus","is","similar","in","composition","to","Neptune",",","and","both","have","bulk","chemical","compositions","which","differ","from","that","of","the","larger","gas","giant","s","Jupiter","and","Saturn","."],"labels":["B-astronomical object","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["event","award","university","organization","person","protein","enzyme","scientist","chemical_element","astronomical_object","discipline","chemical_compound","theory","country","location","academic_journal"]}
{"id":"115","dataset":"crossner_science","split":"dev","instance":{"id":"115","prompt_labels":"In(O) 1999(O) ,(O) Haraway(B-scientist) received(O) the(O) Society(B-organization) for(I-organization) Social(I-organization) Studies(I-organization) of(I-organization) Science(I-organization) '(O) s(O) ((O) 4S(B-organization) )(O) Ludwik(B-award) Fleck(I-award) Prize(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, chemical compound, discipline, chemical element, academic journal, person, university, location, organization, scientist, award, astronomical object, enzyme, protein, theory and O.\nSentence: In 1999 , Haraway received the Society for Social Studies of Science ' s ( 4S ) Ludwik Fleck Prize .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1999",",","Haraway","received","the","Society","for","Social","Studies","of","Science","'","s","(","4S",")","Ludwik","Fleck","Prize","."],"labels":["O","O","O","B-scientist","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","O","B-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["event","country","chemical_compound","discipline","chemical_element","academic_journal","person","university","location","organization","scientist","award","astronomical_object","enzyme","protein","theory"]}
{"id":"116","dataset":"crossner_science","split":"dev","instance":{"id":"116","prompt_labels":"Knowles(B-person) collaborated(O) with(O) several(O) studio(O) personalities(O) ,(O) including(O) Jack(B-person) Splash(I-person) ,(O) Shea(B-person) Taylor(I-person) ,(O) Mr.(B-person) Familiar(I-person) ,(O) Lamont(B-person) Dozier(I-person) ,(O) production(O) teams(O) Soulshock(B-organization) &(I-organization) Karlin(I-organization) and(O) Bama(B-organization) Boyz(I-organization) ,(O) as(O) well(O) as(O) singers(O) and(O) rappers(O) Pharrell(B-person) Williams(I-person) ,(O) Bilal(B-person) ,(O) Q-Tip(B-person) and(O) Lil(B-person) Wayne(I-person) respectively(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, protein, chemical element, chemical compound, academic journal, discipline, event, country, astronomical object, scientist, person, organization, enzyme, university, theory, award and O.\nSentence: Knowles collaborated with several studio personalities , including Jack Splash , Shea Taylor , Mr. Familiar , Lamont Dozier , production teams Soulshock & Karlin and Bama Boyz , as well as singers and rappers Pharrell Williams , Bilal , Q-Tip and Lil Wayne respectively .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Knowles","collaborated","with","several","studio","personalities",",","including","Jack","Splash",",","Shea","Taylor",",","Mr.","Familiar",",","Lamont","Dozier",",","production","teams","Soulshock","&","Karlin","and","Bama","Boyz",",","as","well","as","singers","and","rappers","Pharrell","Williams",",","Bilal",",","Q-Tip","and","Lil","Wayne","respectively","."],"labels":["B-person","O","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","O","O","O","O","O","O","B-person","I-person","O","B-person","O","B-person","O","B-person","I-person","O","O"],"target_index":null,"target_label":null},"label_list":["location","protein","chemical_element","chemical_compound","academic_journal","discipline","event","country","astronomical_object","scientist","person","organization","enzyme","university","theory","award"]}
{"id":"119","dataset":"crossner_science","split":"dev","instance":{"id":"119","prompt_labels":"Thus(O) ,(O) Jupiter(B-astronomical object) and(O) Saturn(B-astronomical object) are(O) gas(O) giants(O) ,(O) and(O) Uranus(B-astronomical object) and(O) Neptune(B-astronomical object) are(O) ice(O) giant(O) s(O) ,(O) even(O) though(O) the(O) vast(O) majority(O) of(O) the(O) gas(O) and(O) ice(O) in(O) their(O) interiors(O) is(O) a(O) hot(O) ,(O) highly(O) dense(O) fluid(O) that(O) gets(O) denser(O) as(O) the(O) center(O) of(O) the(O) planet(O) is(O) approached(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, chemical compound, scientist, academic journal, event, astronomical object, country, protein, award, enzyme, theory, university, chemical element, location, organization, discipline and O.\nSentence: Thus , Jupiter and Saturn are gas giants , and Uranus and Neptune are ice giant s , even though the vast majority of the gas and ice in their interiors is a hot , highly dense fluid that gets denser as the center of the planet is approached .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Thus",",","Jupiter","and","Saturn","are","gas","giants",",","and","Uranus","and","Neptune","are","ice","giant","s",",","even","though","the","vast","majority","of","the","gas","and","ice","in","their","interiors","is","a","hot",",","highly","dense","fluid","that","gets","denser","as","the","center","of","the","planet","is","approached","."],"labels":["O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","chemical_compound","scientist","academic_journal","event","astronomical_object","country","protein","award","enzyme","theory","university","chemical_element","location","organization","discipline"]}
{"id":"121","dataset":"crossner_science","split":"dev","instance":{"id":"121","prompt_labels":"This(O) subcategory(O) includes(O) Pluto(B-astronomical object) ,(O) Haumea(B-astronomical object) ,(O) Makemake(B-astronomical object) and(O) Eris(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, country, event, chemical element, protein, university, discipline, theory, organization, award, enzyme, astronomical object, location, academic journal, scientist, person and O.\nSentence: This subcategory includes Pluto , Haumea , Makemake and Eris .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","subcategory","includes","Pluto",",","Haumea",",","Makemake","and","Eris","."],"labels":["O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","country","event","chemical_element","protein","university","discipline","theory","organization","award","enzyme","astronomical_object","location","academic_journal","scientist","person"]}
{"id":"122","dataset":"crossner_science","split":"dev","instance":{"id":"122","prompt_labels":"Since(O) then(O) ,(O) names(O) have(O) been(O) given(O) to(O) 134(O) additional(O) satellites(O) :(O) 57(O) satellites(O) of(O) Jupiter(B-astronomical object) ,(O) 43(O) of(O) Saturn(B-astronomical object) ,(O) 22(O) of(O) Uranus(B-astronomical object) ,(O) 12(O) of(O) Neptune(B-astronomical object) ,(O) 5(O) of(O) Pluto(B-astronomical object) ,(O) 1(O) of(O) Eris(B-astronomical object) ,(O) and(O) 2(O) of(O) Haumea(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, protein, discipline, location, country, astronomical object, chemical element, scientist, chemical compound, event, academic journal, person, theory, award, organization, university and O.\nSentence: Since then , names have been given to 134 additional satellites : 57 satellites of Jupiter , 43 of Saturn , 22 of Uranus , 12 of Neptune , 5 of Pluto , 1 of Eris , and 2 of Haumea .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Since","then",",","names","have","been","given","to","134","additional","satellites",":","57","satellites","of","Jupiter",",","43","of","Saturn",",","22","of","Uranus",",","12","of","Neptune",",","5","of","Pluto",",","1","of","Eris",",","and","2","of","Haumea","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","B-astronomical object","O","O","O","B-astronomical object","O","O","O","B-astronomical object","O","O","O","B-astronomical object","O","O","O","B-astronomical object","O","O","O","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["enzyme","protein","discipline","location","country","astronomical_object","chemical_element","scientist","chemical_compound","event","academic_journal","person","theory","award","organization","university"]}
{"id":"123","dataset":"crossner_science","split":"dev","instance":{"id":"123","prompt_labels":"A(O) super-Earth(O) is(O) an(O) extrasolar(O) planet(O) with(O) a(O) mass(O) higher(O) than(O) Earth(B-astronomical object) '(O) s(O) ,(O) but(O) substantially(O) below(O) those(O) of(O) the(O) Solar(O) System(O) 's(O) ice(O) giant(O) s(O) ,(O) Uranus(B-astronomical object) and(O) Neptune(B-astronomical object) ,(O) which(O) are(O) 14.5(O) and(O) 17(O) times(O) Earth(B-astronomical object) 's(O) ,(O) respectively(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, chemical element, country, location, theory, organization, chemical compound, scientist, university, protein, discipline, enzyme, event, astronomical object, academic journal and O.\nSentence: A super-Earth is an extrasolar planet with a mass higher than Earth ' s , but substantially below those of the Solar System 's ice giant s , Uranus and Neptune , which are 14.5 and 17 times Earth 's , respectively .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","super-Earth","is","an","extrasolar","planet","with","a","mass","higher","than","Earth","'","s",",","but","substantially","below","those","of","the","Solar","System","'s","ice","giant","s",",","Uranus","and","Neptune",",","which","are","14.5","and","17","times","Earth","'s",",","respectively","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","person","chemical_element","country","location","theory","organization","chemical_compound","scientist","university","protein","discipline","enzyme","event","astronomical_object","academic_journal"]}
{"id":"124","dataset":"crossner_science","split":"dev","instance":{"id":"124","prompt_labels":"Cooper(B-person) was(O) a(O) member(O) of(O) the(O) Society(B-organization) of(I-organization) Experimental(I-organization) Test(I-organization) Pilots(I-organization) ,(O) the(O) American(B-organization) Institute(I-organization) of(I-organization) Aeronautics(I-organization) and(I-organization) Astronautics(I-organization) ,(O) the(O) American(B-organization) Astronautical(I-organization) Society(I-organization) ,(O) Scottish(O) Rite(O) and(O) York(O) Rite(O) Masons(O) ,(O) Shriners(B-organization) ,(O) the(O) Royal(B-organization) Order(I-organization) of(I-organization) Jesters(I-organization) ,(O) the(O) Rotary(B-organization) Club(I-organization) ,(O) Order(B-organization) of(I-organization) Daedalians(I-organization) ,(O) Confederate(B-organization) Air(I-organization) Force(I-organization) ,(O) Adventurers(B-organization) '(I-organization) Club(I-organization) of(O) Los(B-location) Angeles(I-location) ,(O) and(O) Boy(B-organization) Scouts(I-organization) of(I-organization) America(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, discipline, academic journal, event, theory, chemical element, person, astronomical object, country, location, award, organization, scientist, enzyme, university, chemical compound and O.\nSentence: Cooper was a member of the Society of Experimental Test Pilots , the American Institute of Aeronautics and Astronautics , the American Astronautical Society , Scottish Rite and York Rite Masons , Shriners , the Royal Order of Jesters , the Rotary Club , Order of Daedalians , Confederate Air Force , Adventurers ' Club of Los Angeles , and Boy Scouts of America .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Cooper","was","a","member","of","the","Society","of","Experimental","Test","Pilots",",","the","American","Institute","of","Aeronautics","and","Astronautics",",","the","American","Astronautical","Society",",","Scottish","Rite","and","York","Rite","Masons",",","Shriners",",","the","Royal","Order","of","Jesters",",","the","Rotary","Club",",","Order","of","Daedalians",",","Confederate","Air","Force",",","Adventurers","'","Club","of","Los","Angeles",",","and","Boy","Scouts","of","America","."],"labels":["B-person","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","B-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-location","I-location","O","O","B-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["protein","discipline","academic_journal","event","theory","chemical_element","person","astronomical_object","country","location","award","organization","scientist","enzyme","university","chemical_compound"]}
{"id":"127","dataset":"crossner_science","split":"dev","instance":{"id":"127","prompt_labels":"Portions(O) of(O) Galveston(B-location) County(I-location) are(O) served(O) by(O) College(B-university) of(I-university) the(I-university) Mainland(I-university) and(O) Galveston(B-university) College(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, discipline, person, academic journal, theory, chemical element, astronomical object, university, location, protein, event, organization, award, enzyme, chemical compound, scientist and O.\nSentence: Portions of Galveston County are served by College of the Mainland and Galveston College .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Portions","of","Galveston","County","are","served","by","College","of","the","Mainland","and","Galveston","College","."],"labels":["O","O","B-location","I-location","O","O","O","B-university","I-university","I-university","I-university","O","B-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["country","discipline","person","academic_journal","theory","chemical_element","astronomical_object","university","location","protein","event","organization","award","enzyme","chemical_compound","scientist"]}
{"id":"128","dataset":"crossner_science","split":"dev","instance":{"id":"128","prompt_labels":"This(O) range(O) ,(O) as(O) well(O) as(O) the(O) relative(O) speeds(O) between(O) the(O) planets(O) ,(O) led(O) Kepler(B-scientist) to(O) conclude(O) that(O) the(O) Solar(O) System(O) was(O) composed(O) of(O) two(O) basses(O) ((O) Saturn(B-astronomical object) and(O) Jupiter(B-astronomical object) )(O) ,(O) a(O) tenor(O) ((O) Mars(B-astronomical object) )(O) ,(O) two(O) altos(O) ((O) Venus(B-astronomical object) and(O) Earth(B-astronomical object) )(O) ,(O) and(O) a(O) soprano(O) ((O) Mercury(B-astronomical object) )(O) ,(O) which(O) had(O) sung(O) in(O) perfect(O) concord(O) ,(O) at(O) the(O) beginning(O) of(O) time(O) ,(O) and(O) could(O) potentially(O) arrange(O) themselves(O) to(O) do(O) so(O) again(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, chemical compound, award, event, country, scientist, organization, theory, chemical element, astronomical object, location, university, discipline, protein, enzyme, academic journal and O.\nSentence: This range , as well as the relative speeds between the planets , led Kepler to conclude that the Solar System was composed of two basses ( Saturn and Jupiter ) , a tenor ( Mars ) , two altos ( Venus and Earth ) , and a soprano ( Mercury ) , which had sung in perfect concord , at the beginning of time , and could potentially arrange themselves to do so again .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","range",",","as","well","as","the","relative","speeds","between","the","planets",",","led","Kepler","to","conclude","that","the","Solar","System","was","composed","of","two","basses","(","Saturn","and","Jupiter",")",",","a","tenor","(","Mars",")",",","two","altos","(","Venus","and","Earth",")",",","and","a","soprano","(","Mercury",")",",","which","had","sung","in","perfect","concord",",","at","the","beginning","of","time",",","and","could","potentially","arrange","themselves","to","do","so","again","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","B-astronomical object","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","chemical_compound","award","event","country","scientist","organization","theory","chemical_element","astronomical_object","location","university","discipline","protein","enzyme","academic_journal"]}
{"id":"129","dataset":"crossner_science","split":"dev","instance":{"id":"129","prompt_labels":"Many(O) families(O) of(O) proteins(O) act(O) as(O) negative(O) regulators(O) categorized(O) into(O) either(O) antiapoptotic(O) factors(O) ,(O) such(O) as(O) IAP(B-protein) nowiki(O) /(O) s(O) and(O) Bcl-2(B-protein) family(I-protein) proteins(O) or(O) prosurvival(O) factors(O) like(O) cFLIP(B-protein) ,(O) BNIP3(B-protein) ,(O) FADD(B-protein) ,(O) Protein(B-protein) kinase(I-protein) B(I-protein) ,(O) and(O) NF-B(B-protein) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, award, chemical element, astronomical object, organization, person, scientist, location, event, academic journal, theory, chemical compound, protein, university, country, enzyme and O.\nSentence: Many families of proteins act as negative regulators categorized into either antiapoptotic factors , such as IAP nowiki / s and Bcl-2 family proteins or prosurvival factors like cFLIP , BNIP3 , FADD , Protein kinase B , and NF-B .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Many","families","of","proteins","act","as","negative","regulators","categorized","into","either","antiapoptotic","factors",",","such","as","IAP","nowiki","/","s","and","Bcl-2","family","proteins","or","prosurvival","factors","like","cFLIP",",","BNIP3",",","FADD",",","Protein","kinase","B",",","and","NF-B","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-protein","O","O","O","O","B-protein","I-protein","O","O","O","O","O","B-protein","O","B-protein","O","B-protein","O","B-protein","I-protein","I-protein","O","O","B-protein","O"],"target_index":null,"target_label":null},"label_list":["discipline","award","chemical_element","astronomical_object","organization","person","scientist","location","event","academic_journal","theory","chemical_compound","protein","university","country","enzyme"]}
{"id":"130","dataset":"crossner_science","split":"dev","instance":{"id":"130","prompt_labels":"Seven(O) other(O) objects(O) are(O) classified(O) as(O) both(O) periodic(O) comets(O) and(O) numbered(O) asteroids(O) :(O) 2060(B-astronomical object) Chiron(I-astronomical object) ((O) 95P(B-astronomical object) /(I-astronomical object) Chiron(I-astronomical object) )(O) ,(O) 4015(B-astronomical object) Wilson-Harrington(I-astronomical object) ((O) 107P(B-astronomical object) /(I-astronomical object) Wilson-Harrington(I-astronomical object) )(O) ,(O) 7968(B-astronomical object) Elst-Pizarro(I-astronomical object) ((O) 133P(B-astronomical object) /(I-astronomical object) Elst-Pizarro(I-astronomical object) )(O) ,(O) 60558(B-astronomical object) Echeclus(I-astronomical object) ((O) 174P(B-astronomical object) /(I-astronomical object) Echeclus(I-astronomical object) )(O) ,(O) ((O) 362P(B-astronomical object) /(I-astronomical object) 2008(I-astronomical object) GOsub98(I-astronomical object) /(I-astronomical object) sub(I-astronomical object) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, event, chemical compound, award, university, location, organization, scientist, person, astronomical object, academic journal, enzyme, discipline, theory, country, protein and O.\nSentence: Seven other objects are classified as both periodic comets and numbered asteroids : 2060 Chiron ( 95P / Chiron ) , 4015 Wilson-Harrington ( 107P / Wilson-Harrington ) , 7968 Elst-Pizarro ( 133P / Elst-Pizarro ) , 60558 Echeclus ( 174P / Echeclus ) , ( 362P / 2008 GOsub98 / sub ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Seven","other","objects","are","classified","as","both","periodic","comets","and","numbered","asteroids",":","2060","Chiron","(","95P","/","Chiron",")",",","4015","Wilson-Harrington","(","107P","/","Wilson-Harrington",")",",","7968","Elst-Pizarro","(","133P","/","Elst-Pizarro",")",",","60558","Echeclus","(","174P","/","Echeclus",")",",","(","362P","/","2008","GOsub98","/","sub",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","I-astronomical object","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","I-astronomical object","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","I-astronomical object","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","I-astronomical object","O","O","O","B-astronomical object","I-astronomical object","I-astronomical object","I-astronomical object","I-astronomical object","I-astronomical object","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","event","chemical_compound","award","university","location","organization","scientist","person","astronomical_object","academic_journal","enzyme","discipline","theory","country","protein"]}
{"id":"134","dataset":"crossner_science","split":"dev","instance":{"id":"134","prompt_labels":"The(O) Montreal(B-organization) Neurological(I-organization) Institute(I-organization) ,(O) the(O) former(O) Royal(B-organization) Victoria(I-organization) Hospital(I-organization) ,(O) Allan(B-organization) Memorial(I-organization) Institute(I-organization) and(O) the(O) Montreal(B-organization) General(I-organization) Hospital(I-organization) of(I-organization) McGill(I-organization) University(I-organization) are(O) on(O) Pine(B-location) Avenue(I-location) ,(O) as(O) is(O) Cormier(B-location) House(I-location) ,(O) the(O) former(O) residence(O) of(O) Pierre(B-person) Elliott(I-person) Trudeau(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, award, theory, chemical element, enzyme, country, location, astronomical object, event, scientist, organization, university, discipline, academic journal, person, chemical compound and O.\nSentence: The Montreal Neurological Institute , the former Royal Victoria Hospital , Allan Memorial Institute and the Montreal General Hospital of McGill University are on Pine Avenue , as is Cormier House , the former residence of Pierre Elliott Trudeau .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Montreal","Neurological","Institute",",","the","former","Royal","Victoria","Hospital",",","Allan","Memorial","Institute","and","the","Montreal","General","Hospital","of","McGill","University","are","on","Pine","Avenue",",","as","is","Cormier","House",",","the","former","residence","of","Pierre","Elliott","Trudeau","."],"labels":["O","B-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-location","I-location","O","O","O","B-location","I-location","O","O","O","O","O","B-person","I-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["protein","award","theory","chemical_element","enzyme","country","location","astronomical_object","event","scientist","organization","university","discipline","academic_journal","person","chemical_compound"]}
{"id":"137","dataset":"crossner_science","split":"dev","instance":{"id":"137","prompt_labels":"Garca-Sastre(B-scientist) is(O) an(O) editor(O) for(O) the(O) Journal(B-academic journal) of(I-academic journal) Experimental(I-academic journal) Medicine(I-academic journal) and(O) PLOS(B-academic journal) Pathogens(I-academic journal) ,(O) and(O) he(O) sits(O) on(O) the(O) editorial(O) boards(O) of(O) the(O) Journal(B-academic journal) of(I-academic journal) Virology(I-academic journal) ,(O) Virology(B-academic journal) ,(O) Virus(B-academic journal) Research(I-academic journal) and(O) the(O) Journal(B-academic journal) of(I-academic journal) General(I-academic journal) Virology(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, chemical element, university, astronomical object, theory, event, country, discipline, chemical compound, award, scientist, organization, enzyme, location, protein, academic journal and O.\nSentence: Garca-Sastre is an editor for the Journal of Experimental Medicine and PLOS Pathogens , and he sits on the editorial boards of the Journal of Virology , Virology , Virus Research and the Journal of General Virology .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Garca-Sastre","is","an","editor","for","the","Journal","of","Experimental","Medicine","and","PLOS","Pathogens",",","and","he","sits","on","the","editorial","boards","of","the","Journal","of","Virology",",","Virology",",","Virus","Research","and","the","Journal","of","General","Virology","."],"labels":["B-scientist","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","O","B-academic journal","I-academic journal","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["person","chemical_element","university","astronomical_object","theory","event","country","discipline","chemical_compound","award","scientist","organization","enzyme","location","protein","academic_journal"]}
{"id":"139","dataset":"crossner_science","split":"dev","instance":{"id":"139","prompt_labels":"Some(O) of(O) these(O) mechanisms(O) include(O) ATP-dependent(O) chromatin(O) remodeling(O) ,(O) LINE1(B-protein) ,(O) and(O) prion(B-protein) protein-based(O) modifications(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, country, enzyme, scientist, theory, chemical compound, discipline, chemical element, university, organization, event, award, person, location, astronomical object, academic journal and O.\nSentence: Some of these mechanisms include ATP-dependent chromatin remodeling , LINE1 , and prion protein-based modifications .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","of","these","mechanisms","include","ATP-dependent","chromatin","remodeling",",","LINE1",",","and","prion","protein-based","modifications","."],"labels":["O","O","O","O","O","O","O","O","O","B-protein","O","O","B-protein","O","O","O"],"target_index":null,"target_label":null},"label_list":["protein","country","enzyme","scientist","theory","chemical_compound","discipline","chemical_element","university","organization","event","award","person","location","astronomical_object","academic_journal"]}
{"id":"141","dataset":"crossner_science","split":"dev","instance":{"id":"141","prompt_labels":"A(O) stretch(O) of(O) road(O) in(O) the(O) natural(O) park(O) is(O) notable(O) for(O) being(O) the(O) scene(O) in(O) the(O) 1969(O) James(B-person) Bond(I-person) film(O) On(O) Her(O) Majesty(O) 's(O) Secret(O) Service(O) where(O) Tracy(B-person) Bond(I-person) ((O) played(O) by(O) Diana(B-person) Rigg(I-person) )(O) is(O) shot(O) dead(O) by(O) Irma(O) Bunt(O) ((O) Ilse(B-person) Steppat(I-person) )(O) in(O) a(O) drive-by(B-person) shooting(I-person) at(O) the(O) end(O) of(O) the(O) film.(O) at(O) the(O) Internet(O) Movie(O) Database(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, person, organization, university, enzyme, astronomical object, discipline, chemical element, protein, chemical compound, country, award, academic journal, event, location, scientist and O.\nSentence: A stretch of road in the natural park is notable for being the scene in the 1969 James Bond film On Her Majesty 's Secret Service where Tracy Bond ( played by Diana Rigg ) is shot dead by Irma Bunt ( Ilse Steppat ) in a drive-by shooting at the end of the film. at the Internet Movie Database","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","stretch","of","road","in","the","natural","park","is","notable","for","being","the","scene","in","the","1969","James","Bond","film","On","Her","Majesty","'s","Secret","Service","where","Tracy","Bond","(","played","by","Diana","Rigg",")","is","shot","dead","by","Irma","Bunt","(","Ilse","Steppat",")","in","a","drive-by","shooting","at","the","end","of","the","film.","at","the","Internet","Movie","Database"],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["theory","person","organization","university","enzyme","astronomical_object","discipline","chemical_element","protein","chemical_compound","country","award","academic_journal","event","location","scientist"]}
{"id":"144","dataset":"crossner_science","split":"dev","instance":{"id":"144","prompt_labels":"DNA(O) cytosine(O) methylation(O) is(O) catalyzed(O) by(O) DNA(B-enzyme) methyltransferase(I-enzyme) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, scientist, chemical element, enzyme, university, protein, location, organization, chemical compound, theory, discipline, award, person, country, astronomical object, academic journal and O.\nSentence: DNA cytosine methylation is catalyzed by DNA methyltransferase .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["DNA","cytosine","methylation","is","catalyzed","by","DNA","methyltransferase","."],"labels":["O","O","O","O","O","O","B-enzyme","I-enzyme","O"],"target_index":null,"target_label":null},"label_list":["event","scientist","chemical_element","enzyme","university","protein","location","organization","chemical_compound","theory","discipline","award","person","country","astronomical_object","academic_journal"]}
{"id":"146","dataset":"crossner_science","split":"dev","instance":{"id":"146","prompt_labels":"Chiron(B-astronomical object) 's(O) orbit(O) was(O) found(O) to(O) be(O) highly(O) eccentric(O) ((O) 0.37(O) )(O) ,(O) with(O) perihelion(O) just(O) inside(O) the(O) orbit(O) of(O) Saturn(B-astronomical object) and(O) aphelion(O) just(O) outside(O) the(O) perihelion(O) of(O) Uranus(B-astronomical object) ((O) it(O) does(O) not(O) reach(O) the(O) average(O) distance(O) of(O) Uranus(B-astronomical object) ,(O) however(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, discipline, organization, chemical element, theory, country, protein, astronomical object, university, academic journal, chemical compound, award, enzyme, location, event, person and O.\nSentence: Chiron 's orbit was found to be highly eccentric ( 0.37 ) , with perihelion just inside the orbit of Saturn and aphelion just outside the perihelion of Uranus ( it does not reach the average distance of Uranus , however ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Chiron","'s","orbit","was","found","to","be","highly","eccentric","(","0.37",")",",","with","perihelion","just","inside","the","orbit","of","Saturn","and","aphelion","just","outside","the","perihelion","of","Uranus","(","it","does","not","reach","the","average","distance","of","Uranus",",","however",")","."],"labels":["B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","discipline","organization","chemical_element","theory","country","protein","astronomical_object","university","academic_journal","chemical_compound","award","enzyme","location","event","person"]}
{"id":"147","dataset":"crossner_science","split":"dev","instance":{"id":"147","prompt_labels":"In(O) fact(O) ,(O) it(O) is(O) the(O) third(O) dimmest(O) of(O) the(O) first(O) twenty-three(O) asteroids(O) discovered(O) ,(O) with(O) only(O) 13(B-astronomical object) Egeria(I-astronomical object) and(O) 17(B-astronomical object) Thetis(I-astronomical object) having(O) lower(O) mean(O) opposition(O) magnitude(O) s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, organization, protein, chemical element, location, country, astronomical object, university, award, discipline, person, chemical compound, academic journal, theory, event, scientist and O.\nSentence: In fact , it is the third dimmest of the first twenty-three asteroids discovered , with only 13 Egeria and 17 Thetis having lower mean opposition magnitude s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","fact",",","it","is","the","third","dimmest","of","the","first","twenty-three","asteroids","discovered",",","with","only","13","Egeria","and","17","Thetis","having","lower","mean","opposition","magnitude","s","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["enzyme","organization","protein","chemical_element","location","country","astronomical_object","university","award","discipline","person","chemical_compound","academic_journal","theory","event","scientist"]}
{"id":"148","dataset":"crossner_science","split":"dev","instance":{"id":"148","prompt_labels":"The(O) amino(O) acid(O) sequence(O) of(O) arginine(B-protein) vasopressin(I-protein) ((O) argipressin(B-protein) )(O) is(O) Cys(B-chemical compound) -(O) Tyr(B-chemical compound) -(O) Phenylalanine(B-chemical compound) -(O) Gln(B-chemical compound) -(O) Asn(B-chemical compound) -(O) Cysteine(B-chemical compound) -(O) Pro(B-chemical compound) -(O) Arg(B-chemical compound) -(O) Gly(B-chemical compound) -NHsub2(B-chemical compound) /(I-chemical compound) sub(I-chemical compound) ,(O) with(O) the(O) cysteine(O) residues(O) forming(O) a(O) disulfide(B-chemical compound) bond(I-chemical compound) and(O) the(O) C(O) -terminus(O) of(O) the(O) sequence(O) converted(O) to(O) a(O) primary(B-chemical compound) amide(I-chemical compound) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, discipline, enzyme, person, astronomical object, academic journal, university, award, scientist, organization, protein, location, country, theory, chemical element, event and O.\nSentence: The amino acid sequence of arginine vasopressin ( argipressin ) is Cys - Tyr - Phenylalanine - Gln - Asn - Cysteine - Pro - Arg - Gly -NHsub2 / sub , with the cysteine residues forming a disulfide bond and the C -terminus of the sequence converted to a primary amide .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","amino","acid","sequence","of","arginine","vasopressin","(","argipressin",")","is","Cys","-","Tyr","-","Phenylalanine","-","Gln","-","Asn","-","Cysteine","-","Pro","-","Arg","-","Gly","-NHsub2","/","sub",",","with","the","cysteine","residues","forming","a","disulfide","bond","and","the","C","-terminus","of","the","sequence","converted","to","a","primary","amide","."],"labels":["O","O","O","O","O","B-protein","I-protein","O","B-protein","O","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","B-chemical compound","I-chemical compound","I-chemical compound","O","O","O","O","O","O","O","B-chemical compound","I-chemical compound","O","O","O","O","O","O","O","O","O","O","B-chemical compound","I-chemical compound","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","discipline","enzyme","person","astronomical_object","academic_journal","university","award","scientist","organization","protein","location","country","theory","chemical_element","event"]}
{"id":"150","dataset":"crossner_science","split":"dev","instance":{"id":"150","prompt_labels":"Eduard(B-scientist) Otto(I-scientist) Emil(I-scientist) Karl(I-scientist) Adam(I-scientist) Freiherr(I-scientist) von(I-scientist) Stackelberg(I-scientist) ((O) 6(O) November(O) 1867(O) in(O) Sillame(B-location) ,(O) Estonia(B-country) -(O) 7(O) April(O) 1943(O) in(O) Munich(B-location) ,(O) Nazi(B-country) Germany(I-country) )(O) was(O) an(O) Estonian(O) chemist(O) ,(O) landowner(O) and(O) politician(O) who(O) belonged(O) to(O) the(O) Stackelberg(O) family(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, theory, country, chemical element, discipline, scientist, academic journal, person, enzyme, astronomical object, chemical compound, award, university, location, protein, organization and O.\nSentence: Eduard Otto Emil Karl Adam Freiherr von Stackelberg ( 6 November 1867 in Sillame , Estonia - 7 April 1943 in Munich , Nazi Germany ) was an Estonian chemist , landowner and politician who belonged to the Stackelberg family .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Eduard","Otto","Emil","Karl","Adam","Freiherr","von","Stackelberg","(","6","November","1867","in","Sillame",",","Estonia","-","7","April","1943","in","Munich",",","Nazi","Germany",")","was","an","Estonian","chemist",",","landowner","and","politician","who","belonged","to","the","Stackelberg","family","."],"labels":["B-scientist","I-scientist","I-scientist","I-scientist","I-scientist","I-scientist","I-scientist","I-scientist","O","O","O","O","O","B-location","O","B-country","O","O","O","O","O","B-location","O","B-country","I-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","theory","country","chemical_element","discipline","scientist","academic_journal","person","enzyme","astronomical_object","chemical_compound","award","university","location","protein","organization"]}
{"id":"151","dataset":"crossner_science","split":"dev","instance":{"id":"151","prompt_labels":"The(O) scientists(O) found(O) that(O) while(O) CRISPR(O) could(O) effectively(O) cleave(O) the(O) -globin(O) gene(O) ((O) HBB(B-protein) )(O) ,(O) the(O) efficiency(O) of(O) homologous(O) recombination(O) directed(O) repair(O) of(O) HBB(B-protein) was(O) highly(O) inefficient(O) and(O) did(O) not(O) do(O) so(O) in(O) a(O) majority(O) of(O) the(O) trials(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, protein, event, university, theory, discipline, chemical compound, location, country, chemical element, scientist, organization, enzyme, academic journal, person, astronomical object and O.\nSentence: The scientists found that while CRISPR could effectively cleave the -globin gene ( HBB ) , the efficiency of homologous recombination directed repair of HBB was highly inefficient and did not do so in a majority of the trials .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","scientists","found","that","while","CRISPR","could","effectively","cleave","the","-globin","gene","(","HBB",")",",","the","efficiency","of","homologous","recombination","directed","repair","of","HBB","was","highly","inefficient","and","did","not","do","so","in","a","majority","of","the","trials","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-protein","O","O","O","O","O","O","O","O","O","O","B-protein","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","protein","event","university","theory","discipline","chemical_compound","location","country","chemical_element","scientist","organization","enzyme","academic_journal","person","astronomical_object"]}
{"id":"153","dataset":"crossner_science","split":"dev","instance":{"id":"153","prompt_labels":"In(O) this(O) respect(O) he(O) was(O) the(O) equivalent(O) of(O) Mars(B-astronomical object) ,(O) Janus(B-astronomical object) ,(O) Saturn(B-astronomical object) and(O) even(O) Jupiter(B-astronomical object) among(O) Latin(O) tribes(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, discipline, award, protein, country, enzyme, chemical compound, chemical element, astronomical object, university, location, event, scientist, theory, academic journal and O.\nSentence: In this respect he was the equivalent of Mars , Janus , Saturn and even Jupiter among Latin tribes .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","this","respect","he","was","the","equivalent","of","Mars",",","Janus",",","Saturn","and","even","Jupiter","among","Latin","tribes","."],"labels":["O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","B-astronomical object","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","person","discipline","award","protein","country","enzyme","chemical_compound","chemical_element","astronomical_object","university","location","event","scientist","theory","academic_journal"]}
{"id":"154","dataset":"crossner_science","split":"dev","instance":{"id":"154","prompt_labels":"His(O) work(O) has(O) been(O) published(O) in(O) international(O) refereed(O) journals(O) ,(O) including(O) American(B-academic journal) Economic(I-academic journal) Review(I-academic journal) ,(O) Journal(O) of(O) European(B-academic journal) Economic(I-academic journal) Association(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Economic(I-academic journal) Perspectives(I-academic journal) ,(O) Economic(B-academic journal) Journal(I-academic journal) and(O) American(B-academic journal) Political(I-academic journal) Science(I-academic journal) Review(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, chemical compound, discipline, enzyme, scientist, country, theory, location, protein, astronomical object, award, organization, event, academic journal, chemical element, university and O.\nSentence: His work has been published in international refereed journals , including American Economic Review , Journal of European Economic Association , Journal of Economic Perspectives , Economic Journal and American Political Science Review .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","work","has","been","published","in","international","refereed","journals",",","including","American","Economic","Review",",","Journal","of","European","Economic","Association",",","Journal","of","Economic","Perspectives",",","Economic","Journal","and","American","Political","Science","Review","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["person","chemical_compound","discipline","enzyme","scientist","country","theory","location","protein","astronomical_object","award","organization","event","academic_journal","chemical_element","university"]}
{"id":"155","dataset":"crossner_science","split":"dev","instance":{"id":"155","prompt_labels":"Known(O) for(O) his(O) research(O) on(O) Mitogen-activated(B-enzyme) protein(I-enzyme) kinase(I-enzyme) ((O) MAPK(B-enzyme) )(O) cascade(O) in(O) plants(O) ,(O) he(O) is(O) a(O) three-time(O) Alexander(B-award) von(I-award) Humboldt(I-award) Fellow(I-award) and(O) an(O) elected(O) fellow(B-award) of(I-award) the(I-award) National(I-award) Academy(I-award) of(I-award) Sciences(I-award) ,(O) India(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, scientist, enzyme, location, academic journal, astronomical object, protein, event, theory, chemical compound, discipline, country, university, award, chemical element, person and O.\nSentence: Known for his research on Mitogen-activated protein kinase ( MAPK ) cascade in plants , he is a three-time Alexander von Humboldt Fellow and an elected fellow of the National Academy of Sciences , India .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Known","for","his","research","on","Mitogen-activated","protein","kinase","(","MAPK",")","cascade","in","plants",",","he","is","a","three-time","Alexander","von","Humboldt","Fellow","and","an","elected","fellow","of","the","National","Academy","of","Sciences",",","India","."],"labels":["O","O","O","O","O","B-enzyme","I-enzyme","I-enzyme","O","B-enzyme","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["organization","scientist","enzyme","location","academic_journal","astronomical_object","protein","event","theory","chemical_compound","discipline","country","university","award","chemical_element","person"]}
{"id":"156","dataset":"crossner_science","split":"dev","instance":{"id":"156","prompt_labels":"An(O) approved(O) residency(O) program(O) and(O) certification(O) ((O) in(O) the(O) U.S.(B-country) ,(O) the(O) American(B-organization) Board(I-organization) of(I-organization) Pathology(I-organization) or(O) the(O) American(B-organization) Osteopathic(I-organization) Board(I-organization) of(I-organization) Pathology(I-organization) )(O) is(O) usually(O) required(O) to(O) obtain(O) employment(O) or(O) hospital(O) privileges(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, organization, location, protein, person, university, scientist, country, event, chemical element, discipline, astronomical object, award, academic journal, enzyme, theory and O.\nSentence: An approved residency program and certification ( in the U.S. , the American Board of Pathology or the American Osteopathic Board of Pathology ) is usually required to obtain employment or hospital privileges .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["An","approved","residency","program","and","certification","(","in","the","U.S.",",","the","American","Board","of","Pathology","or","the","American","Osteopathic","Board","of","Pathology",")","is","usually","required","to","obtain","employment","or","hospital","privileges","."],"labels":["O","O","O","O","O","O","O","O","O","B-country","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","organization","location","protein","person","university","scientist","country","event","chemical_element","discipline","astronomical_object","award","academic_journal","enzyme","theory"]}
{"id":"157","dataset":"crossner_science","split":"dev","instance":{"id":"157","prompt_labels":"Singer(B-scientist) is(O) a(O) member(O) of(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) and(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, theory, organization, chemical element, location, scientist, academic journal, award, discipline, enzyme, event, university, chemical compound, astronomical object, person, country and O.\nSentence: Singer is a member of the National Academy of Sciences and the American Academy of Arts and Sciences .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Singer","is","a","member","of","the","National","Academy","of","Sciences","and","the","American","Academy","of","Arts","and","Sciences","."],"labels":["B-scientist","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["protein","theory","organization","chemical_element","location","scientist","academic_journal","award","discipline","enzyme","event","university","chemical_compound","astronomical_object","person","country"]}
{"id":"161","dataset":"crossner_science","split":"dev","instance":{"id":"161","prompt_labels":"The(O) Perturb-seq(O) protocol(O) uses(O) CRISPR(O) technology(O) to(O) inactivate(O) specific(O) genes(O) and(O) DNA(O) barcoding(O) of(O) each(O) guide(O) RNA(O) to(O) allow(O) for(O) all(O) perturbations(O) to(O) be(O) pooled(O) together(O) and(O) later(O) deconvoluted(O) ,(O) with(O) assignment(O) of(O) each(O) phenotype(O) to(O) a(O) specific(O) guide(O) RNA(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, astronomical object, discipline, enzyme, chemical element, protein, theory, scientist, location, university, award, person, organization, chemical compound, country, academic journal and O.\nSentence: The Perturb-seq protocol uses CRISPR technology to inactivate specific genes and DNA barcoding of each guide RNA to allow for all perturbations to be pooled together and later deconvoluted , with assignment of each phenotype to a specific guide RNA .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Perturb-seq","protocol","uses","CRISPR","technology","to","inactivate","specific","genes","and","DNA","barcoding","of","each","guide","RNA","to","allow","for","all","perturbations","to","be","pooled","together","and","later","deconvoluted",",","with","assignment","of","each","phenotype","to","a","specific","guide","RNA","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","astronomical_object","discipline","enzyme","chemical_element","protein","theory","scientist","location","university","award","person","organization","chemical_compound","country","academic_journal"]}
{"id":"164","dataset":"crossner_science","split":"dev","instance":{"id":"164","prompt_labels":"Published(O) in(O) 1993(O) ,(O) it(O) won(O) the(O) 1994(O) Nebula(B-award) Award(I-award) for(I-award) Best(I-award) Novel(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, enzyme, scientist, discipline, award, university, country, location, astronomical object, protein, chemical compound, theory, academic journal, chemical element, event, person and O.\nSentence: Published in 1993 , it won the 1994 Nebula Award for Best Novel .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Published","in","1993",",","it","won","the","1994","Nebula","Award","for","Best","Novel","."],"labels":["O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["organization","enzyme","scientist","discipline","award","university","country","location","astronomical_object","protein","chemical_compound","theory","academic_journal","chemical_element","event","person"]}
{"id":"166","dataset":"crossner_science","split":"dev","instance":{"id":"166","prompt_labels":"19126(B-astronomical object) Ottohahn(I-astronomical object) named(O) in(O) his(O) honor(O) ,(O) as(O) were(O) the(O) Otto(B-award) Hahn(I-award) Prize(I-award) of(I-award) both(I-award) the(I-award) German(I-award) Chemical(I-award) and(I-award) Physical(I-award) Societies(I-award) and(O) the(O) city(O) of(O) Frankfurt(B-location) /(O) Main(O) ,(O) the(O) Otto(B-award) Hahn(I-award) Medal(I-award) ,(O) and(O) the(O) Otto(B-award) Hahn(I-award) Award(I-award) of(I-award) the(I-award) Max(I-award) Planck(I-award) Society(I-award) and(O) ,(O) since(O) 1988(O) ,(O) the(O) Otto(B-award) Hahn(I-award) Peace(I-award) Medal(I-award) in(I-award) Gold(I-award) of(I-award) the(I-award) United(I-award) Nations(I-award) Association(I-award) of(I-award) Germany(I-award) ((O) DGVN(B-organization) )(O) in(O) Berlin(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, chemical compound, discipline, protein, chemical element, person, theory, astronomical object, location, award, country, scientist, university, academic journal, organization, event and O.\nSentence: 19126 Ottohahn named in his honor , as were the Otto Hahn Prize of both the German Chemical and Physical Societies and the city of Frankfurt / Main , the Otto Hahn Medal , and the Otto Hahn Award of the Max Planck Society and , since 1988 , the Otto Hahn Peace Medal in Gold of the United Nations Association of Germany ( DGVN ) in Berlin .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["19126","Ottohahn","named","in","his","honor",",","as","were","the","Otto","Hahn","Prize","of","both","the","German","Chemical","and","Physical","Societies","and","the","city","of","Frankfurt","/","Main",",","the","Otto","Hahn","Medal",",","and","the","Otto","Hahn","Award","of","the","Max","Planck","Society","and",",","since","1988",",","the","Otto","Hahn","Peace","Medal","in","Gold","of","the","United","Nations","Association","of","Germany","(","DGVN",")","in","Berlin","."],"labels":["B-astronomical object","I-astronomical object","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","B-location","O","O","O","O","B-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","B-organization","O","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["enzyme","chemical_compound","discipline","protein","chemical_element","person","theory","astronomical_object","location","award","country","scientist","university","academic_journal","organization","event"]}
{"id":"168","dataset":"crossner_science","split":"dev","instance":{"id":"168","prompt_labels":"A(O) physician(O) and(O) professor(O) of(O) public(O) health(O) ,(O) he(O) worked(O) first(O) in(O) social(B-discipline) medicine(I-discipline) at(O) the(O) University(B-university) of(I-university) Sassari(I-university) ((O) 1969-1974(O) )(O) and(O) then(O) in(O) occupational(O) health(O) at(O) the(O) Sapienza(B-university) University(I-university) of(I-university) Rome(I-university) ((O) 1975-1999(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, event, protein, country, scientist, discipline, university, person, chemical compound, location, enzyme, organization, astronomical object, award, chemical element, theory and O.\nSentence: A physician and professor of public health , he worked first in social medicine at the University of Sassari ( 1969-1974 ) and then in occupational health at the Sapienza University of Rome ( 1975-1999 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","physician","and","professor","of","public","health",",","he","worked","first","in","social","medicine","at","the","University","of","Sassari","(","1969-1974",")","and","then","in","occupational","health","at","the","Sapienza","University","of","Rome","(","1975-1999",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-discipline","I-discipline","O","O","B-university","I-university","I-university","O","O","O","O","O","O","O","O","O","O","B-university","I-university","I-university","I-university","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","event","protein","country","scientist","discipline","university","person","chemical_compound","location","enzyme","organization","astronomical_object","award","chemical_element","theory"]}
{"id":"169","dataset":"crossner_science","split":"dev","instance":{"id":"169","prompt_labels":"Segment(O) 7(O) encodes(O) the(O) M1(B-protein) protein(I-protein) and(O) the(O) smaller(O) M2(B-protein) proton(I-protein) channel(I-protein) protein(I-protein) ,(O) which(O) is(O) produced(O) by(O) RNA(O) splicing(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, astronomical object, academic journal, enzyme, discipline, organization, location, scientist, chemical compound, university, event, country, theory, person, protein, chemical element and O.\nSentence: Segment 7 encodes the M1 protein and the smaller M2 proton channel protein , which is produced by RNA splicing .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Segment","7","encodes","the","M1","protein","and","the","smaller","M2","proton","channel","protein",",","which","is","produced","by","RNA","splicing","."],"labels":["O","O","O","O","B-protein","I-protein","O","O","O","B-protein","I-protein","I-protein","I-protein","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","astronomical_object","academic_journal","enzyme","discipline","organization","location","scientist","chemical_compound","university","event","country","theory","person","protein","chemical_element"]}
{"id":"173","dataset":"crossner_science","split":"dev","instance":{"id":"173","prompt_labels":"Among(O) the(O) researchers(O) who(O) laid(O) the(O) foundations(O) of(O) AI(O) were(O) Alan(B-scientist) Turing(I-scientist) ,(O) John(B-scientist) von(I-scientist) Neumann(I-scientist) ,(O) Norbert(B-scientist) Wiener(I-scientist) ,(O) Claude(B-scientist) Shannon(I-scientist) ,(O) Warren(B-scientist) McCullough(I-scientist) ,(O) Walter(B-scientist) Pitts(I-scientist) and(O) Donald(B-scientist) Hebb(I-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, academic journal, theory, chemical compound, protein, scientist, award, university, organization, enzyme, astronomical object, location, discipline, country, chemical element and O.\nSentence: Among the researchers who laid the foundations of AI were Alan Turing , John von Neumann , Norbert Wiener , Claude Shannon , Warren McCullough , Walter Pitts and Donald Hebb .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Among","the","researchers","who","laid","the","foundations","of","AI","were","Alan","Turing",",","John","von","Neumann",",","Norbert","Wiener",",","Claude","Shannon",",","Warren","McCullough",",","Walter","Pitts","and","Donald","Hebb","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O"],"target_index":null,"target_label":null},"label_list":["person","event","academic_journal","theory","chemical_compound","protein","scientist","award","university","organization","enzyme","astronomical_object","location","discipline","country","chemical_element"]}
{"id":"174","dataset":"crossner_science","split":"dev","instance":{"id":"174","prompt_labels":"The(O) two(O) factions(O) ,(O) Lawrence(B-person) Murphy(I-person) -(O) Dolan(B-person) and(O) John(B-person) Tunstall(I-person) -(O) McSween(B-person) ,(O) fought(O) a(O) series(O) of(O) escalating(O) battles(O) with(O) such(O) murderous(O) ferocity(O) that(O) the(O) repercussions(O) were(O) felt(O) as(O) far(O) away(O) as(O) the(O) state(O) capital(O) Santa(B-location) Fe(I-location) and(O) even(O) in(O) Washington(B-location) ,(I-location) D.C.(I-location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, university, location, award, scientist, event, country, astronomical object, organization, enzyme, theory, chemical compound, protein, person, academic journal, chemical element and O.\nSentence: The two factions , Lawrence Murphy - Dolan and John Tunstall - McSween , fought a series of escalating battles with such murderous ferocity that the repercussions were felt as far away as the state capital Santa Fe and even in Washington , D.C.","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","two","factions",",","Lawrence","Murphy","-","Dolan","and","John","Tunstall","-","McSween",",","fought","a","series","of","escalating","battles","with","such","murderous","ferocity","that","the","repercussions","were","felt","as","far","away","as","the","state","capital","Santa","Fe","and","even","in","Washington",",","D.C."],"labels":["O","O","O","O","B-person","I-person","O","B-person","O","B-person","I-person","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","B-location","I-location","I-location"],"target_index":null,"target_label":null},"label_list":["discipline","university","location","award","scientist","event","country","astronomical_object","organization","enzyme","theory","chemical_compound","protein","person","academic_journal","chemical_element"]}
{"id":"176","dataset":"crossner_science","split":"dev","instance":{"id":"176","prompt_labels":"It(O) was(O) discovered(O) on(O) 24(O) September(O) 1960(O) ,(O) by(O) Dutch(O) astronomer(O) couple(O) Ingrid(B-scientist) van(I-scientist) Houten-Groeneveld(I-scientist) and(O) Cornelis(B-scientist) van(I-scientist) Houten(I-scientist) in(O) collaboration(O) with(O) Dutch-American(O) astronomer(O) Tom(B-scientist) Gehrels(I-scientist) at(O) the(O) U.S.(B-location) Palomar(I-location) Observatory(I-location) in(O) California(B-location) ,(O) and(O) named(O) after(O) Dutch(O) astronomer(O) Gerard(B-scientist) Kuiper(I-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, theory, enzyme, academic journal, location, discipline, country, university, protein, organization, event, astronomical object, award, chemical compound, scientist, chemical element and O.\nSentence: It was discovered on 24 September 1960 , by Dutch astronomer couple Ingrid van Houten-Groeneveld and Cornelis van Houten in collaboration with Dutch-American astronomer Tom Gehrels at the U.S. Palomar Observatory in California , and named after Dutch astronomer Gerard Kuiper .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","discovered","on","24","September","1960",",","by","Dutch","astronomer","couple","Ingrid","van","Houten-Groeneveld","and","Cornelis","van","Houten","in","collaboration","with","Dutch-American","astronomer","Tom","Gehrels","at","the","U.S.","Palomar","Observatory","in","California",",","and","named","after","Dutch","astronomer","Gerard","Kuiper","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","O","O","O","O","B-scientist","I-scientist","O","O","B-location","I-location","I-location","O","B-location","O","O","O","O","O","O","B-scientist","I-scientist","O"],"target_index":null,"target_label":null},"label_list":["person","theory","enzyme","academic_journal","location","discipline","country","university","protein","organization","event","astronomical_object","award","chemical_compound","scientist","chemical_element"]}
{"id":"177","dataset":"crossner_science","split":"dev","instance":{"id":"177","prompt_labels":"He(O) was(O) internationally(O) recognized(O) with(O) membership(O) in(O) the(O) Japan(B-organization) Academy(I-organization) and(O) the(O) Brazilian(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) and(O) in(O) 1959(O) was(O) appointed(O) a(O) member(O) of(O) the(O) Board(O) of(O) Governors(O) of(O) the(O) Weizmann(B-university) Institute(I-university) of(I-university) Science(I-university) in(O) Israel(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, discipline, enzyme, academic journal, award, scientist, protein, theory, organization, astronomical object, event, country, person, location, chemical element, university and O.\nSentence: He was internationally recognized with membership in the Japan Academy and the Brazilian Academy of Sciences , and in 1959 was appointed a member of the Board of Governors of the Weizmann Institute of Science in Israel .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","internationally","recognized","with","membership","in","the","Japan","Academy","and","the","Brazilian","Academy","of","Sciences",",","and","in","1959","was","appointed","a","member","of","the","Board","of","Governors","of","the","Weizmann","Institute","of","Science","in","Israel","."],"labels":["O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-university","I-university","I-university","I-university","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","discipline","enzyme","academic_journal","award","scientist","protein","theory","organization","astronomical_object","event","country","person","location","chemical_element","university"]}
{"id":"178","dataset":"crossner_science","split":"dev","instance":{"id":"178","prompt_labels":"Among(O) Einstein(B-scientist) 's(O) well-known(O) friends(O) were(O) Michele(B-person) Besso(I-person) ,(O) Paul(B-scientist) Ehrenfest(I-scientist) ,(O) Marcel(B-scientist) Grossmann(I-scientist) ,(O) Jnos(B-scientist) Plesch(I-scientist) ,(O) Daniel(B-scientist) Q.(I-scientist) Posin(I-scientist) ,(O) Maurice(B-scientist) Solovine(I-scientist) ,(O) and(O) Stephen(B-person) Samuel(I-person) Wise(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, theory, location, chemical element, chemical compound, person, scientist, academic journal, protein, discipline, university, astronomical object, event, country, enzyme, organization and O.\nSentence: Among Einstein 's well-known friends were Michele Besso , Paul Ehrenfest , Marcel Grossmann , Jnos Plesch , Daniel Q. Posin , Maurice Solovine , and Stephen Samuel Wise .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Among","Einstein","'s","well-known","friends","were","Michele","Besso",",","Paul","Ehrenfest",",","Marcel","Grossmann",",","Jnos","Plesch",",","Daniel","Q.","Posin",",","Maurice","Solovine",",","and","Stephen","Samuel","Wise","."],"labels":["O","B-scientist","O","O","O","O","B-person","I-person","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-person","I-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["award","theory","location","chemical_element","chemical_compound","person","scientist","academic_journal","protein","discipline","university","astronomical_object","event","country","enzyme","organization"]}
{"id":"180","dataset":"crossner_science","split":"dev","instance":{"id":"180","prompt_labels":"The(O) following(O) asteroids(O) were(O) also(O) named(O) in(O) memory(O) of(O) the(O) other(O) six(O) members(O) of(O) STS-107(B-event) :(O) 51823(B-astronomical object) Rickhusband(I-astronomical object) ,(O) 51824(B-astronomical object) Mikeanderson(I-astronomical object) ,(O) 51826(B-astronomical object) Kalpanachawla(I-astronomical object) ,(O) 51827(B-astronomical object) Laurelclark(I-astronomical object) ,(O) 51828(B-astronomical object) Ilanramon(I-astronomical object) and(O) 51829(B-astronomical object) Williemccool(I-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, astronomical object, academic journal, event, enzyme, chemical compound, university, discipline, location, chemical element, organization, award, protein, theory, scientist and O.\nSentence: The following asteroids were also named in memory of the other six members of STS-107 : 51823 Rickhusband , 51824 Mikeanderson , 51826 Kalpanachawla , 51827 Laurelclark , 51828 Ilanramon and 51829 Williemccool .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","following","asteroids","were","also","named","in","memory","of","the","other","six","members","of","STS-107",":","51823","Rickhusband",",","51824","Mikeanderson",",","51826","Kalpanachawla",",","51827","Laurelclark",",","51828","Ilanramon","and","51829","Williemccool","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["person","country","astronomical_object","academic_journal","event","enzyme","chemical_compound","university","discipline","location","chemical_element","organization","award","protein","theory","scientist"]}
{"id":"182","dataset":"crossner_science","split":"dev","instance":{"id":"182","prompt_labels":"Lysostaphin(B-enzyme) can(O) lyse(O) Staphylococcus(O) ,(O) but(O) Micrococcus(O) bacteria(O) are(O) resistant(O) to(O) the(O) chemical(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, university, astronomical object, event, chemical compound, organization, chemical element, discipline, award, academic journal, scientist, enzyme, location, theory, protein and O.\nSentence: Lysostaphin can lyse Staphylococcus , but Micrococcus bacteria are resistant to the chemical .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Lysostaphin","can","lyse","Staphylococcus",",","but","Micrococcus","bacteria","are","resistant","to","the","chemical","."],"labels":["B-enzyme","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","country","university","astronomical_object","event","chemical_compound","organization","chemical_element","discipline","award","academic_journal","scientist","enzyme","location","theory","protein"]}
{"id":"183","dataset":"crossner_science","split":"dev","instance":{"id":"183","prompt_labels":"Other(O) notable(O) German(O) scientists(O) ,(O) who(O) worked(O) on(O) the(O) Soviet(O) atomic(O) bomb(O) project(O) and(O) joined(O) Schintlmeister(B-scientist) at(O) the(O) Technische(B-university) Hochschule(I-university) Dresden(I-university) were(O) the(O) physicists(O) Heinz(B-scientist) Barwich(I-scientist) and(O) Werner(B-scientist) Hartmann(I-scientist) from(O) Institute(B-organization) G(I-organization) in(O) Agudzery(B-location) and(O) Heinz(B-scientist) Pose(I-scientist) and(O) Ernst(B-scientist) Rexer(I-scientist) from(O) Laboratory(B-organization) V(I-organization) in(O) Obninsk(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, chemical compound, academic journal, theory, scientist, enzyme, event, university, location, chemical element, country, organization, award, protein, discipline, person and O.\nSentence: Other notable German scientists , who worked on the Soviet atomic bomb project and joined Schintlmeister at the Technische Hochschule Dresden were the physicists Heinz Barwich and Werner Hartmann from Institute G in Agudzery and Heinz Pose and Ernst Rexer from Laboratory V in Obninsk .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","notable","German","scientists",",","who","worked","on","the","Soviet","atomic","bomb","project","and","joined","Schintlmeister","at","the","Technische","Hochschule","Dresden","were","the","physicists","Heinz","Barwich","and","Werner","Hartmann","from","Institute","G","in","Agudzery","and","Heinz","Pose","and","Ernst","Rexer","from","Laboratory","V","in","Obninsk","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","O","O","B-university","I-university","I-university","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-organization","I-organization","O","B-location","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-organization","I-organization","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","chemical_compound","academic_journal","theory","scientist","enzyme","event","university","location","chemical_element","country","organization","award","protein","discipline","person"]}
{"id":"184","dataset":"crossner_science","split":"dev","instance":{"id":"184","prompt_labels":"Starting(O) with(O) 1972(O) ,(O) the(O) Review(O) no(O) longer(O) appear(O) exclusively(O) in(O) Reviews(B-academic journal) of(I-academic journal) Modern(I-academic journal) Physics(I-academic journal) ,(O) but(O) also(O) in(O) Physics(B-academic journal) Letters(I-academic journal) B(I-academic journal) ,(O) European(B-academic journal) Physical(I-academic journal) Journal(I-academic journal) C(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Physics(I-academic journal) G(I-academic journal) ,(O) Physical(B-academic journal) Review(I-academic journal) D(I-academic journal) ,(O) and(O) Chinese(B-academic journal) Physics(I-academic journal) C(I-academic journal) ((O) depending(O) on(O) the(O) year(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, chemical element, astronomical object, discipline, university, enzyme, organization, protein, theory, scientist, country, location, award, academic journal, person, chemical compound and O.\nSentence: Starting with 1972 , the Review no longer appear exclusively in Reviews of Modern Physics , but also in Physics Letters B , European Physical Journal C , Journal of Physics G , Physical Review D , and Chinese Physics C ( depending on the year ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Starting","with","1972",",","the","Review","no","longer","appear","exclusively","in","Reviews","of","Modern","Physics",",","but","also","in","Physics","Letters","B",",","European","Physical","Journal","C",",","Journal","of","Physics","G",",","Physical","Review","D",",","and","Chinese","Physics","C","(","depending","on","the","year",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O","O","B-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","chemical_element","astronomical_object","discipline","university","enzyme","organization","protein","theory","scientist","country","location","award","academic_journal","person","chemical_compound"]}
{"id":"188","dataset":"crossner_science","split":"dev","instance":{"id":"188","prompt_labels":"Michel(B-scientist) Adanson(I-scientist) ((O) 1763(O) )(O) ,(O) Antoine(B-scientist) Laurent(I-scientist) de(I-scientist) Jussieu(I-scientist) ((O) 1789(O) )(O) ,(O) and(O) Augustin(B-scientist) Pyramus(I-scientist) de(I-scientist) Candolle(I-scientist) ((O) 1819(O) )(O) all(O) proposed(O) various(O) alternative(O) natural(O) systems(O) of(O) classification(O) that(O) grouped(O) plants(O) using(O) a(O) wider(O) range(O) of(O) shared(O) characters(O) and(O) were(O) widely(O) followed(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, chemical element, astronomical object, protein, event, university, discipline, enzyme, country, person, academic journal, location, chemical compound, scientist, award, organization and O.\nSentence: Michel Adanson ( 1763 ) , Antoine Laurent de Jussieu ( 1789 ) , and Augustin Pyramus de Candolle ( 1819 ) all proposed various alternative natural systems of classification that grouped plants using a wider range of shared characters and were widely followed .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Michel","Adanson","(","1763",")",",","Antoine","Laurent","de","Jussieu","(","1789",")",",","and","Augustin","Pyramus","de","Candolle","(","1819",")","all","proposed","various","alternative","natural","systems","of","classification","that","grouped","plants","using","a","wider","range","of","shared","characters","and","were","widely","followed","."],"labels":["B-scientist","I-scientist","O","O","O","O","B-scientist","I-scientist","I-scientist","I-scientist","O","O","O","O","O","B-scientist","I-scientist","I-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["theory","chemical_element","astronomical_object","protein","event","university","discipline","enzyme","country","person","academic_journal","location","chemical_compound","scientist","award","organization"]}
{"id":"193","dataset":"crossner_science","split":"dev","instance":{"id":"193","prompt_labels":"He(O) was(O) educated(O) at(O) the(O) Technical(B-university) University(I-university) of(I-university) Munich(I-university) from(O) 1925(O) to(O) 1927(O) and(O) then(O) entered(O) the(O) Technical(B-university) University(I-university) of(I-university) Berlin(I-university) ,(O) where(O) he(O) posited(O) that(O) microscope(O) s(O) using(O) electrons(O) ,(O) with(O) wavelengths(O) 1000(O) times(O) shorter(O) than(O) those(O) of(O) light(O) ,(O) could(O) provide(O) a(O) more(O) detailed(O) picture(O) of(O) an(O) object(O) than(O) a(O) microscope(O) utilizing(O) light(O) ,(O) in(O) which(O) magnification(O) is(O) limited(O) by(O) the(O) size(O) of(O) the(O) wavelengths(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, enzyme, event, protein, organization, academic journal, award, chemical compound, chemical element, theory, country, location, discipline, person, university, astronomical object and O.\nSentence: He was educated at the Technical University of Munich from 1925 to 1927 and then entered the Technical University of Berlin , where he posited that microscope s using electrons , with wavelengths 1000 times shorter than those of light , could provide a more detailed picture of an object than a microscope utilizing light , in which magnification is limited by the size of the wavelengths .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","educated","at","the","Technical","University","of","Munich","from","1925","to","1927","and","then","entered","the","Technical","University","of","Berlin",",","where","he","posited","that","microscope","s","using","electrons",",","with","wavelengths","1000","times","shorter","than","those","of","light",",","could","provide","a","more","detailed","picture","of","an","object","than","a","microscope","utilizing","light",",","in","which","magnification","is","limited","by","the","size","of","the","wavelengths","."],"labels":["O","O","O","O","O","B-university","I-university","I-university","I-university","O","O","O","O","O","O","O","O","B-university","I-university","I-university","I-university","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","enzyme","event","protein","organization","academic_journal","award","chemical_compound","chemical_element","theory","country","location","discipline","person","university","astronomical_object"]}
{"id":"195","dataset":"crossner_science","split":"dev","instance":{"id":"195","prompt_labels":"The(O) minor(O) planets(O) 105(B-astronomical object) Artemis(I-astronomical object) ,(O) 399(B-astronomical object) Persephone(I-astronomical object) ,(O) 1388(B-astronomical object) Aphrodite(I-astronomical object) and(O) 5731(B-astronomical object) Zeus(I-astronomical object) were(O) named(O) for(O) these(O) Greek(O) gods(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, university, chemical element, protein, award, country, event, academic journal, astronomical object, scientist, theory, discipline, location, enzyme, person, chemical compound and O.\nSentence: The minor planets 105 Artemis , 399 Persephone , 1388 Aphrodite and 5731 Zeus were named for these Greek gods .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","minor","planets","105","Artemis",",","399","Persephone",",","1388","Aphrodite","and","5731","Zeus","were","named","for","these","Greek","gods","."],"labels":["O","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","university","chemical_element","protein","award","country","event","academic_journal","astronomical_object","scientist","theory","discipline","location","enzyme","person","chemical_compound"]}
{"id":"197","dataset":"crossner_science","split":"dev","instance":{"id":"197","prompt_labels":"In(O) 2010(O) ,(O) Goodall(B-scientist) through(O) JGI(B-organization) formed(O) a(O) coalition(O) with(O) a(O) number(O) of(O) organizations(O) such(O) as(O) the(O) Wildlife(B-organization) Conservation(I-organization) Society(I-organization) ((O) WCS(B-organization) )(O) and(O) the(O) Humane(B-organization) Society(I-organization) of(I-organization) the(I-organization) United(I-organization) States(I-organization) ((O) HSUS(B-organization) )(O) and(O) petitioned(O) to(O) list(O) all(O) chimpanzees(O) including(O) those(O) that(O) are(O) captive(O) as(O) endangered(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, scientist, enzyme, chemical compound, discipline, organization, person, academic journal, event, astronomical object, chemical element, award, country, university, theory, location and O.\nSentence: In 2010 , Goodall through JGI formed a coalition with a number of organizations such as the Wildlife Conservation Society ( WCS ) and the Humane Society of the United States ( HSUS ) and petitioned to list all chimpanzees including those that are captive as endangered .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2010",",","Goodall","through","JGI","formed","a","coalition","with","a","number","of","organizations","such","as","the","Wildlife","Conservation","Society","(","WCS",")","and","the","Humane","Society","of","the","United","States","(","HSUS",")","and","petitioned","to","list","all","chimpanzees","including","those","that","are","captive","as","endangered","."],"labels":["O","O","O","B-scientist","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["protein","scientist","enzyme","chemical_compound","discipline","organization","person","academic_journal","event","astronomical_object","chemical_element","award","country","university","theory","location"]}
{"id":"199","dataset":"crossner_science","split":"dev","instance":{"id":"199","prompt_labels":"He(O) was(O) also(O) awarded(O) the(O) Davy(B-award) Medal(I-award) in(O) 1971(O) ,(O) the(O) Rumford(B-award) Medal(I-award) in(O) 1978(O) ,(O) the(O) Ellison-Cliffe(B-award) Medal(I-award) in(O) 1991(O) and(O) the(O) Copley(B-award) Medal(I-award) in(O) 1992(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, scientist, event, theory, organization, country, university, person, discipline, location, protein, award, chemical element, academic journal, enzyme, chemical compound and O.\nSentence: He was also awarded the Davy Medal in 1971 , the Rumford Medal in 1978 , the Ellison-Cliffe Medal in 1991 and the Copley Medal in 1992 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","also","awarded","the","Davy","Medal","in","1971",",","the","Rumford","Medal","in","1978",",","the","Ellison-Cliffe","Medal","in","1991","and","the","Copley","Medal","in","1992","."],"labels":["O","O","O","O","O","B-award","I-award","O","O","O","O","B-award","I-award","O","O","O","O","B-award","I-award","O","O","O","O","B-award","I-award","O","O","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","scientist","event","theory","organization","country","university","person","discipline","location","protein","award","chemical_element","academic_journal","enzyme","chemical_compound"]}
