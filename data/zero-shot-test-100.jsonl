{"id": "25", "dataset": "crossner_ai", "split": "test", "label_list": ["programming language", "conference", "organization", "university", "metric", "researcher", "algorithm", "product", "task", "field", "person", "location", "country"], "instance": {"id": "25", "words": ["The", "majority", "are", "results", "of", "the", "word2vec", "model", "developed", "by", "Mikolov", "et", "al", "or", "variants", "of", "word2vec", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-product", "I-product", "O", "O", "B-researcher", "O", "O", "O", "O", "O", "B-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, conference, organization, university, metric, researcher, algorithm, product, task, field, person, location, country and O.\nSentence: The majority are results of the word2vec model developed by Mikolov et al or variants of word2vec .", "prompt_labels": "The(O) majority(O) are(O) results(O) of(O) the(O) word2vec(B-product) model(I-product) developed(O) by(O) Mikolov(B-researcher) et(O) al(O) or(O) variants(O) of(O) word2vec(B-product) .(O)"}}
{"id": "130", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "algorithm", "task", "country", "programming language", "metric", "researcher", "person", "location", "university", "product", "field", "conference"], "instance": {"id": "130", "words": ["The", "construction", "of", "a", "rich", "lexicon", "with", "a", "suitable", "ontology", "requires", "significant", "effort", ",", "e.g.", ",", "Wordnet", "lexicon", "required", "many", "person-years", "of", "effort.", "G.", "A.", "Miller", ",", "R.", "Beckwith", ",", "C.", "D.", "Fellbaum", ",", "D.", "Gross", ",", "K.", "Miller", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, algorithm, task, country, programming language, metric, researcher, person, location, university, product, field, conference and O.\nSentence: The construction of a rich lexicon with a suitable ontology requires significant effort , e.g. , Wordnet lexicon required many person-years of effort. G. A. Miller , R. Beckwith , C. D. Fellbaum , D. Gross , K. Miller .", "prompt_labels": "The(O) construction(O) of(O) a(O) rich(O) lexicon(O) with(O) a(O) suitable(O) ontology(O) requires(O) significant(O) effort(O) ,(O) e.g.(O) ,(O) Wordnet(B-product) lexicon(O) required(O) many(O) person-years(O) of(O) effort.(O) G.(B-researcher) A.(I-researcher) Miller(I-researcher) ,(O) R.(B-researcher) Beckwith(I-researcher) ,(O) C.(B-researcher) D.(I-researcher) Fellbaum(I-researcher) ,(O) D.(B-researcher) Gross(I-researcher) ,(O) K.(B-researcher) Miller(I-researcher) .(O)"}}
{"id": "375", "dataset": "crossner_ai", "split": "test", "label_list": ["programming language", "university", "location", "product", "task", "researcher", "organization", "country", "conference", "algorithm", "field", "person", "metric"], "instance": {"id": "375", "words": ["A", "trial", "by", "RET", "in", "2011", "with", "Facial", "recognition", "system", "cameras", "mounted", "on", "trams", "made", "sure", "that", "people", "were", "banned", "from", "the", "city", "trams", "did", "not", "sneak", "on", "anyway", "."], "labels": ["O", "O", "O", "B-organization", "O", "O", "O", "B-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, university, location, product, task, researcher, organization, country, conference, algorithm, field, person, metric and O.\nSentence: A trial by RET in 2011 with Facial recognition system cameras mounted on trams made sure that people were banned from the city trams did not sneak on anyway .", "prompt_labels": "A(O) trial(O) by(O) RET(B-organization) in(O) 2011(O) with(O) Facial(B-product) recognition(I-product) system(I-product) cameras(O) mounted(O) on(O) trams(O) made(O) sure(O) that(O) people(O) were(O) banned(O) from(O) the(O) city(O) trams(O) did(O) not(O) sneak(O) on(O) anyway(O) .(O)"}}
{"id": "335", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "metric", "organization", "country", "location", "conference", "person", "task", "product", "researcher", "field", "algorithm", "programming language"], "instance": {"id": "335", "words": ["In", "the", "first", "published", "paper", "on", "CGs", ",", "John", "F.", "Sowa", "applied", "them", "to", "a", "wide", "range", "of", "topics", "in", "artificial", "intelligence", ",", "computer", "science", ",", "and", "cognitive", "science", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-field", "O", "B-researcher", "I-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "O", "B-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, metric, organization, country, location, conference, person, task, product, researcher, field, algorithm, programming language and O.\nSentence: In the first published paper on CGs , John F. Sowa applied them to a wide range of topics in artificial intelligence , computer science , and cognitive science .", "prompt_labels": "In(O) the(O) first(O) published(O) paper(O) on(O) CGs(B-field) ,(O) John(B-researcher) F.(I-researcher) Sowa(I-researcher) applied(O) them(O) to(O) a(O) wide(O) range(O) of(O) topics(O) in(O) artificial(B-field) intelligence(I-field) ,(O) computer(B-field) science(I-field) ,(O) and(O) cognitive(B-field) science(I-field) .(O)"}}
{"id": "312", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "person", "algorithm", "university", "conference", "organization", "researcher", "field", "country", "programming language", "location", "task", "product"], "instance": {"id": "312", "words": ["Logo", "is", "an", "educational", "programming", "language", ",", "designed", "in", "1967", "by", "Wally", "Feurzeig", ",", "Seymour", "Papert", ",", "and", "Cynthia", "Solomon", "."], "labels": ["B-programming language", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O", "B-researcher", "I-researcher", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, person, algorithm, university, conference, organization, researcher, field, country, programming language, location, task, product and O.\nSentence: Logo is an educational programming language , designed in 1967 by Wally Feurzeig , Seymour Papert , and Cynthia Solomon .", "prompt_labels": "Logo(B-programming language) is(O) an(O) educational(O) programming(O) language(O) ,(O) designed(O) in(O) 1967(O) by(O) Wally(B-researcher) Feurzeig(I-researcher) ,(O) Seymour(B-researcher) Papert(I-researcher) ,(O) and(O) Cynthia(B-researcher) Solomon(I-researcher) .(O)"}}
{"id": "160", "dataset": "crossner_ai", "split": "test", "label_list": ["programming language", "algorithm", "product", "field", "country", "researcher", "location", "conference", "organization", "task", "university", "metric", "person"], "instance": {"id": "160", "words": ["Important", "journals", "include", "the", "IEEE", "Transactions", "on", "Speech", "and", "Audio", "Processing", "(", "later", "renamed", "IEEE", "Transactions", "on", "Audio", ",", "Speech", "and", "Language", "Processing", "and", "since", "Sept", "2014", "renamed", "IEEE", "/", "ACM", "Transactions", "on", "Audio", ",", "Speech", "and", "Language", "Processing", "-", "after", "merging", "with", "an", "ACM", "publication", ")", ",", "Computer", "Speech", "and", "Language", ",", "and", "Speech", "Communication", "."], "labels": ["O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "O", "O", "B-conference", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "O", "O", "B-conference", "I-conference", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, algorithm, product, field, country, researcher, location, conference, organization, task, university, metric, person and O.\nSentence: Important journals include the IEEE Transactions on Speech and Audio Processing ( later renamed IEEE Transactions on Audio , Speech and Language Processing and since Sept 2014 renamed IEEE / ACM Transactions on Audio , Speech and Language Processing - after merging with an ACM publication ) , Computer Speech and Language , and Speech Communication .", "prompt_labels": "Important(O) journals(O) include(O) the(O) IEEE(B-conference) Transactions(I-conference) on(I-conference) Speech(I-conference) and(I-conference) Audio(I-conference) Processing(I-conference) ((O) later(O) renamed(O) IEEE(B-conference) Transactions(I-conference) on(I-conference) Audio(I-conference) ,(I-conference) Speech(I-conference) and(I-conference) Language(I-conference) Processing(I-conference) and(O) since(O) Sept(O) 2014(O) renamed(O) IEEE(B-conference) /(I-conference) ACM(I-conference) Transactions(I-conference) on(I-conference) Audio(I-conference) ,(I-conference) Speech(I-conference) and(I-conference) Language(I-conference) Processing(I-conference) -(O) after(O) merging(O) with(O) an(O) ACM(B-conference) publication(O) )(O) ,(O) Computer(B-conference) Speech(I-conference) and(I-conference) Language(I-conference) ,(O) and(O) Speech(B-conference) Communication(I-conference) .(O)"}}
{"id": "289", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "field", "conference", "location", "person", "researcher", "task", "programming language", "university", "country", "organization", "algorithm", "metric"], "instance": {"id": "289", "words": ["Machine", "vision", "as", "a", "systems", "engineering", "discipline", "can", "be", "considered", "distinct", "from", "computer", "vision", ",", "a", "form", "of", "computer", "science", "."], "labels": ["B-field", "I-field", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "B-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, field, conference, location, person, researcher, task, programming language, university, country, organization, algorithm, metric and O.\nSentence: Machine vision as a systems engineering discipline can be considered distinct from computer vision , a form of computer science .", "prompt_labels": "Machine(B-field) vision(I-field) as(O) a(O) systems(B-field) engineering(I-field) discipline(O) can(O) be(O) considered(O) distinct(O) from(O) computer(B-field) vision(I-field) ,(O) a(O) form(O) of(O) computer(B-field) science(I-field) .(O)"}}
{"id": "208", "dataset": "crossner_ai", "split": "test", "label_list": ["country", "location", "task", "conference", "programming language", "product", "university", "field", "researcher", "algorithm", "person", "organization", "metric"], "instance": {"id": "208", "words": ["Prior", "to", "joining", "the", "Penn", "faculty", "in", "2002", ",", "he", "spent", "a", "decade", "(", "1991-2001", ")", "in", "AT", "&", "T", "Labs", "and", "Bell", "Labs", ",", "including", "as", "head", "of", "the", "AI", "department", "with", "colleagues", "including", "Michael", "L.", "Littman", ",", "David", "A.", "McAllester", ",", "and", "Richard", "S.", "Sutton", ";", "Secure", "Systems", "Research", "department", ";", "and", "Machine", "Learning", "department", "with", "members", "such", "as", "Michael", "Collins", "and", "the", "leader", ")", "."], "labels": ["O", "O", "O", "O", "B-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-field", "O", "O", "O", "O", "B-researcher", "I-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "I-researcher", "O", "O", "B-researcher", "I-researcher", "I-researcher", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, task, conference, programming language, product, university, field, researcher, algorithm, person, organization, metric and O.\nSentence: Prior to joining the Penn faculty in 2002 , he spent a decade ( 1991-2001 ) in AT & T Labs and Bell Labs , including as head of the AI department with colleagues including Michael L. Littman , David A. McAllester , and Richard S. Sutton ; Secure Systems Research department ; and Machine Learning department with members such as Michael Collins and the leader ) .", "prompt_labels": "Prior(O) to(O) joining(O) the(O) Penn(B-university) faculty(O) in(O) 2002(O) ,(O) he(O) spent(O) a(O) decade(O) ((O) 1991-2001(O) )(O) in(O) AT(B-organization) &(I-organization) T(I-organization) Labs(I-organization) and(O) Bell(B-organization) Labs(I-organization) ,(O) including(O) as(O) head(O) of(O) the(O) AI(B-field) department(O) with(O) colleagues(O) including(O) Michael(B-researcher) L.(I-researcher) Littman(I-researcher) ,(O) David(B-researcher) A.(I-researcher) McAllester(I-researcher) ,(O) and(O) Richard(B-researcher) S.(I-researcher) Sutton(I-researcher) ;(O) Secure(B-organization) Systems(I-organization) Research(I-organization) department(I-organization) ;(O) and(O) Machine(B-field) Learning(I-field) department(O) with(O) members(O) such(O) as(O) Michael(B-researcher) Collins(I-researcher) and(O) the(O) leader(O) )(O) .(O)"}}
{"id": "97", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "conference", "task", "person", "field", "organization", "programming language", "product", "country", "researcher", "university", "location", "metric"], "instance": {"id": "97", "words": ["The", "start-up", "was", "founded", "by", "Demis", "Hassabis", ",", "Shane", "Legg", "and", "Mustafa", "Suleyman", "in", "2010", "."], "labels": ["O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-person", "I-person", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, conference, task, person, field, organization, programming language, product, country, researcher, university, location, metric and O.\nSentence: The start-up was founded by Demis Hassabis , Shane Legg and Mustafa Suleyman in 2010 .", "prompt_labels": "The(O) start-up(O) was(O) founded(O) by(O) Demis(B-researcher) Hassabis(I-researcher) ,(O) Shane(B-researcher) Legg(I-researcher) and(O) Mustafa(B-person) Suleyman(I-person) in(O) 2010(O) .(O)"}}
{"id": "93", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "algorithm", "metric", "programming language", "person", "field", "task", "country", "product", "researcher", "university", "organization", "location"], "instance": {"id": "93", "words": ["Since", "IBM", "proposed", "and", "realized", "the", "system", "of", "BLEU", "Papineni", "et", "al", "."], "labels": ["O", "B-organization", "O", "O", "O", "O", "O", "O", "B-metric", "B-researcher", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, algorithm, metric, programming language, person, field, task, country, product, researcher, university, organization, location and O.\nSentence: Since IBM proposed and realized the system of BLEU Papineni et al .", "prompt_labels": "Since(O) IBM(B-organization) proposed(O) and(O) realized(O) the(O) system(O) of(O) BLEU(B-metric) Papineni(B-researcher) et(O) al(O) .(O)"}}
{"id": "362", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "location", "conference", "country", "product", "task", "person", "field", "organization", "researcher", "university", "metric", "programming language"], "instance": {"id": "362", "words": ["The", "measured", "performance", "on", "test", "data", "of", "eight", "naive", "WSI", "across", "various", "tauopathies", "resulted", "in", "the", "recall", ",", "precision", ",", "and", "an", "F1", "score", "of", "0.92", ",", "0.72", ",", "and", "0.81", ",", "respectively", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "O", "O", "O", "O", "O", "O", "B-metric", "O", "B-metric", "O", "O", "O", "B-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, location, conference, country, product, task, person, field, organization, researcher, university, metric, programming language and O.\nSentence: The measured performance on test data of eight naive WSI across various tauopathies resulted in the recall , precision , and an F1 score of 0.92 , 0.72 , and 0.81 , respectively .", "prompt_labels": "The(O) measured(O) performance(O) on(O) test(O) data(O) of(O) eight(O) naive(O) WSI(B-task) across(O) various(O) tauopathies(O) resulted(O) in(O) the(O) recall(B-metric) ,(O) precision(B-metric) ,(O) and(O) an(O) F1(B-metric) score(I-metric) of(O) 0.92(O) ,(O) 0.72(O) ,(O) and(O) 0.81(O) ,(O) respectively(O) .(O)"}}
{"id": "267", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "product", "task", "location", "person", "organization", "university", "metric", "country", "conference", "programming language", "field", "researcher"], "instance": {"id": "267", "words": ["Typically", ",", "the", "process", "starts", "by", "terminology", "extraction", "and", "concepts", "or", "noun", "phrase", "s", "from", "plain", "text", "using", "linguistic", "processors", "such", "as", "part-of-speech", "tagging", "and", "phrase", "chunking", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, product, task, location, person, organization, university, metric, country, conference, programming language, field, researcher and O.\nSentence: Typically , the process starts by terminology extraction and concepts or noun phrase s from plain text using linguistic processors such as part-of-speech tagging and phrase chunking .", "prompt_labels": "Typically(O) ,(O) the(O) process(O) starts(O) by(O) terminology(B-task) extraction(I-task) and(O) concepts(O) or(O) noun(O) phrase(O) s(O) from(O) plain(O) text(O) using(O) linguistic(O) processors(O) such(O) as(O) part-of-speech(B-task) tagging(I-task) and(O) phrase(B-task) chunking(I-task) .(O)"}}
{"id": "124", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "conference", "metric", "task", "country", "product", "researcher", "location", "university", "programming language", "algorithm", "organization", "person"], "instance": {"id": "124", "words": ["Learning", "the", "parameters", "math", "\\", "theta", "/", "math", "is", "usually", "done", "by", "maximum", "likelihood", "learning", "for", "mathp", "(", "Y", "_", "i", "|", "X", "_", "i", ";", "\\", "theta", ")", "/", "math", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, conference, metric, task, country, product, researcher, location, university, programming language, algorithm, organization, person and O.\nSentence: Learning the parameters math \\ theta / math is usually done by maximum likelihood learning for mathp ( Y _ i | X _ i ; \\ theta ) / math .", "prompt_labels": "Learning(O) the(O) parameters(O) math(O) \\(O) theta(O) /(O) math(O) is(O) usually(O) done(O) by(O) maximum(B-algorithm) likelihood(I-algorithm) learning(I-algorithm) for(O) mathp(O) ((O) Y(O) _(O) i(O) |(O) X(O) _(O) i(O) ;(O) \\(O) theta(O) )(O) /(O) math(O) .(O)"}}
{"id": "249", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "researcher", "university", "algorithm", "organization", "field", "country", "task", "person", "product", "conference", "location", "programming language"], "instance": {"id": "249", "words": ["The", "NER", "model", "is", "one", "of", "a", "number", "of", "methods", "for", "determining", "the", "accuracy", "of", "live", "subtitles", "in", "television", "broadcasts", "and", "events", "that", "are", "produced", "using", "speech", "recognition", "."], "labels": ["O", "B-task", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, researcher, university, algorithm, organization, field, country, task, person, product, conference, location, programming language and O.\nSentence: The NER model is one of a number of methods for determining the accuracy of live subtitles in television broadcasts and events that are produced using speech recognition .", "prompt_labels": "The(O) NER(B-task) model(O) is(O) one(O) of(O) a(O) number(O) of(O) methods(O) for(O) determining(O) the(O) accuracy(B-metric) of(O) live(O) subtitles(O) in(O) television(O) broadcasts(O) and(O) events(O) that(O) are(O) produced(O) using(O) speech(B-task) recognition(I-task) .(O)"}}
{"id": "395", "dataset": "crossner_ai", "split": "test", "label_list": ["location", "university", "conference", "programming language", "product", "task", "researcher", "person", "algorithm", "country", "organization", "metric", "field"], "instance": {"id": "395", "words": ["In", "particular", ",", "they", "are", "used", "during", "the", "calculation", "of", "likelihood", "of", "a", "tree", "(", "in", "Bayesian", "and", "maximum", "likelihood", "approaches", "to", "tree", "estimation", ")", "and", "they", "are", "used", "to", "estimate", "the", "evolutionary", "distance", "between", "sequences", "from", "the", "observed", "differences", "between", "the", "sequences", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, university, conference, programming language, product, task, researcher, person, algorithm, country, organization, metric, field and O.\nSentence: In particular , they are used during the calculation of likelihood of a tree ( in Bayesian and maximum likelihood approaches to tree estimation ) and they are used to estimate the evolutionary distance between sequences from the observed differences between the sequences .", "prompt_labels": "In(O) particular(O) ,(O) they(O) are(O) used(O) during(O) the(O) calculation(O) of(O) likelihood(O) of(O) a(O) tree(O) ((O) in(O) Bayesian(B-algorithm) and(O) maximum(B-algorithm) likelihood(I-algorithm) approaches(O) to(O) tree(O) estimation(O) )(O) and(O) they(O) are(O) used(O) to(O) estimate(O) the(O) evolutionary(O) distance(O) between(O) sequences(O) from(O) the(O) observed(O) differences(O) between(O) the(O) sequences(O) .(O)"}}
{"id": "138", "dataset": "crossner_ai", "split": "test", "label_list": ["task", "algorithm", "programming language", "organization", "researcher", "conference", "location", "country", "person", "product", "university", "field", "metric"], "instance": {"id": "138", "words": ["Phidgets", "can", "be", "programmed", "using", "a", "variety", "of", "software", "and", "programming", "languages", ",", "ranging", "from", "Java", "to", "Microsoft", "Excel", "."], "labels": ["B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-programming language", "O", "B-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, algorithm, programming language, organization, researcher, conference, location, country, person, product, university, field, metric and O.\nSentence: Phidgets can be programmed using a variety of software and programming languages , ranging from Java to Microsoft Excel .", "prompt_labels": "Phidgets(B-product) can(O) be(O) programmed(O) using(O) a(O) variety(O) of(O) software(O) and(O) programming(O) languages(O) ,(O) ranging(O) from(O) Java(B-programming language) to(O) Microsoft(B-product) Excel(I-product) .(O)"}}
{"id": "16", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "location", "person", "field", "product", "metric", "task", "university", "country", "researcher", "algorithm", "programming language", "organization"], "instance": {"id": "16", "words": ["Engelberger", "'s", "most", "famous", "co-invention", ",", "the", "Unimate", "industrial", "robotic", "arm", ",", "was", "among", "the", "first", "inductees", "into", "the", "Robot", "Hall", "of", "Fame", "in", "2003", "."], "labels": ["B-researcher", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, location, person, field, product, metric, task, university, country, researcher, algorithm, programming language, organization and O.\nSentence: Engelberger 's most famous co-invention , the Unimate industrial robotic arm , was among the first inductees into the Robot Hall of Fame in 2003 .", "prompt_labels": "Engelberger(B-researcher) 's(O) most(O) famous(O) co-invention(O) ,(O) the(O) Unimate(B-product) industrial(I-product) robotic(I-product) arm(I-product) ,(O) was(O) among(O) the(O) first(O) inductees(O) into(O) the(O) Robot(B-location) Hall(I-location) of(I-location) Fame(I-location) in(O) 2003(O) .(O)"}}
{"id": "35", "dataset": "crossner_ai", "split": "test", "label_list": ["location", "metric", "organization", "task", "field", "conference", "product", "algorithm", "programming language", "university", "researcher", "country", "person"], "instance": {"id": "35", "words": ["BLEU", "uses", "a", "modified", "form", "of", "precision", "to", "compare", "a", "candidate", "translation", "against", "multiple", "reference", "translations", "."], "labels": ["B-metric", "O", "O", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, metric, organization, task, field, conference, product, algorithm, programming language, university, researcher, country, person and O.\nSentence: BLEU uses a modified form of precision to compare a candidate translation against multiple reference translations .", "prompt_labels": "BLEU(B-metric) uses(O) a(O) modified(O) form(O) of(O) precision(B-metric) to(O) compare(O) a(O) candidate(O) translation(O) against(O) multiple(O) reference(O) translations(O) .(O)"}}
{"id": "310", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "organization", "location", "conference", "product", "country", "person", "programming language", "task", "researcher", "university", "field", "metric"], "instance": {"id": "310", "words": ["The", "members", "went", "to", "the", "University", "of", "Debrecen", ",", "the", "Hungarian", "Academy", "of", "Sciences", ",", "Eötvös", "Loránd", "University", ",", "etc", "."], "labels": ["O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-university", "I-university", "I-university", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, organization, location, conference, product, country, person, programming language, task, researcher, university, field, metric and O.\nSentence: The members went to the University of Debrecen , the Hungarian Academy of Sciences , Eötvös Loránd University , etc .", "prompt_labels": "The(O) members(O) went(O) to(O) the(O) University(B-university) of(I-university) Debrecen(I-university) ,(O) the(O) Hungarian(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) Eötvös(B-university) Loránd(I-university) University(I-university) ,(O) etc(O) .(O)"}}
{"id": "266", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "person", "metric", "task", "algorithm", "organization", "conference", "country", "programming language", "field", "location", "researcher", "product"], "instance": {"id": "266", "words": ["In", "the", "film", "Westworld", ",", "female", "robots", "actually", "engaged", "in", "intercourse", "with", "human", "men", "as", "part", "of", "the", "make-believe", "vacation", "world", "human", "customers", "paid", "to", "attend", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, person, metric, task, algorithm, organization, conference, country, programming language, field, location, researcher, product and O.\nSentence: In the film Westworld , female robots actually engaged in intercourse with human men as part of the make-believe vacation world human customers paid to attend .", "prompt_labels": "In(O) the(O) film(O) Westworld(O) ,(O) female(O) robots(O) actually(O) engaged(O) in(O) intercourse(O) with(O) human(O) men(O) as(O) part(O) of(O) the(O) make-believe(O) vacation(O) world(O) human(O) customers(O) paid(O) to(O) attend(O) .(O)"}}
{"id": "207", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "programming language", "task", "algorithm", "product", "field", "researcher", "person", "location", "university", "country", "organization", "conference"], "instance": {"id": "207", "words": ["However", ",", "usage", "only", "became", "widespread", "in", "2005", "when", "Navneet", "Dalal", "and", "Bill", "Triggs", ",", "researchers", "for", "the", "French", "National", "Institute", "for", "Research", "in", "Computer", "Science", "and", "Automation", "(", "INRIA", ")", ",", "presented", "their", "supplementary", "work", "on", "HOG", "descriptors", "at", "the", "Conference", "on", "Computer", "Vision", "and", "Pattern", "Recognition", "(", "CVPR", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, programming language, task, algorithm, product, field, researcher, person, location, university, country, organization, conference and O.\nSentence: However , usage only became widespread in 2005 when Navneet Dalal and Bill Triggs , researchers for the French National Institute for Research in Computer Science and Automation ( INRIA ) , presented their supplementary work on HOG descriptors at the Conference on Computer Vision and Pattern Recognition ( CVPR ) .", "prompt_labels": "However(O) ,(O) usage(O) only(O) became(O) widespread(O) in(O) 2005(O) when(O) Navneet(B-researcher) Dalal(I-researcher) and(O) Bill(B-researcher) Triggs(I-researcher) ,(O) researchers(O) for(O) the(O) French(B-organization) National(I-organization) Institute(I-organization) for(I-organization) Research(I-organization) in(I-organization) Computer(I-organization) Science(I-organization) and(I-organization) Automation(I-organization) ((O) INRIA(B-organization) )(O) ,(O) presented(O) their(O) supplementary(O) work(O) on(O) HOG(B-algorithm) descriptors(I-algorithm) at(O) the(O) Conference(B-conference) on(I-conference) Computer(I-conference) Vision(I-conference) and(I-conference) Pattern(I-conference) Recognition(I-conference) ((O) CVPR(B-conference) )(O) .(O)"}}
{"id": "398", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "programming language", "organization", "location", "researcher", "conference", "university", "country", "field", "person", "metric", "algorithm", "task"], "instance": {"id": "398", "words": ["In", "red-green", "anaglyph", ",", "the", "audience", "was", "presented", "three", "reels", "of", "tests", ",", "which", "included", "rural", "scenes", ",", "test", "shots", "of", "Marie", "Doro", ",", "a", "segment", "of", "John", "B.", "Mason", "playing", "a", "number", "of", "passages", "from", "Jim", "the", "Penman", "(", "a", "film", "released", "by", "Famous", "Players-Lasky", "that", "year", ",", "but", "not", "in", "3D", ")", ",", "Oriental", "dancers", ",", "and", "a", "reel", "of", "footage", "of", "Niagara", "Falls", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, programming language, organization, location, researcher, conference, university, country, field, person, metric, algorithm, task and O.\nSentence: In red-green anaglyph , the audience was presented three reels of tests , which included rural scenes , test shots of Marie Doro , a segment of John B. Mason playing a number of passages from Jim the Penman ( a film released by Famous Players-Lasky that year , but not in 3D ) , Oriental dancers , and a reel of footage of Niagara Falls .", "prompt_labels": "In(O) red-green(O) anaglyph(O) ,(O) the(O) audience(O) was(O) presented(O) three(O) reels(O) of(O) tests(O) ,(O) which(O) included(O) rural(O) scenes(O) ,(O) test(O) shots(O) of(O) Marie(B-person) Doro(I-person) ,(O) a(O) segment(O) of(O) John(B-person) B.(I-person) Mason(I-person) playing(O) a(O) number(O) of(O) passages(O) from(O) Jim(B-person) the(I-person) Penman(I-person) ((O) a(O) film(O) released(O) by(O) Famous(B-organization) Players-Lasky(I-organization) that(O) year(O) ,(O) but(O) not(O) in(O) 3D(O) )(O) ,(O) Oriental(O) dancers(O) ,(O) and(O) a(O) reel(O) of(O) footage(O) of(O) Niagara(B-location) Falls(I-location) .(O)"}}
{"id": "220", "dataset": "crossner_ai", "split": "test", "label_list": ["researcher", "product", "person", "algorithm", "country", "university", "task", "field", "metric", "conference", "organization", "programming language", "location"], "instance": {"id": "220", "words": ["It", "forms", "one", "of", "the", "three", "main", "categories", "of", "machine", "learning", ",", "along", "with", "supervised", "learning", "and", "reinforcement", "learning", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, product, person, algorithm, country, university, task, field, metric, conference, organization, programming language, location and O.\nSentence: It forms one of the three main categories of machine learning , along with supervised learning and reinforcement learning .", "prompt_labels": "It(O) forms(O) one(O) of(O) the(O) three(O) main(O) categories(O) of(O) machine(B-field) learning(I-field) ,(O) along(O) with(O) supervised(B-field) learning(I-field) and(O) reinforcement(B-field) learning(I-field) .(O)"}}
{"id": "88", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "university", "person", "task", "field", "researcher", "conference", "location", "algorithm", "country", "product", "programming language", "metric"], "instance": {"id": "88", "words": ["WordNet", ",", "a", "freely", "available", "database", "originally", "designed", "as", "a", "semantic", "network", "based", "on", "psycholinguistic", "principles", ",", "was", "expanded", "by", "addition", "of", "definitions", "and", "is", "now", "also", "viewed", "as", "a", "dictionary", "."], "labels": ["B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, university, person, task, field, researcher, conference, location, algorithm, country, product, programming language, metric and O.\nSentence: WordNet , a freely available database originally designed as a semantic network based on psycholinguistic principles , was expanded by addition of definitions and is now also viewed as a dictionary .", "prompt_labels": "WordNet(B-product) ,(O) a(O) freely(O) available(O) database(O) originally(O) designed(O) as(O) a(O) semantic(O) network(O) based(O) on(O) psycholinguistic(O) principles(O) ,(O) was(O) expanded(O) by(O) addition(O) of(O) definitions(O) and(O) is(O) now(O) also(O) viewed(O) as(O) a(O) dictionary(O) .(O)"}}
{"id": "167", "dataset": "crossner_ai", "split": "test", "label_list": ["programming language", "researcher", "university", "conference", "country", "location", "field", "task", "metric", "algorithm", "person", "organization", "product"], "instance": {"id": "167", "words": ["However", ",", "in", "the", "version", "of", "the", "metric", "used", "by", "NIST", "evaluations", "prior", "to", "2009", ",", "the", "shortest", "reference", "sentence", "had", "been", "used", "instead", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, researcher, university, conference, country, location, field, task, metric, algorithm, person, organization, product and O.\nSentence: However , in the version of the metric used by NIST evaluations prior to 2009 , the shortest reference sentence had been used instead .", "prompt_labels": "However(O) ,(O) in(O) the(O) version(O) of(O) the(O) metric(O) used(O) by(O) NIST(B-metric) evaluations(O) prior(O) to(O) 2009(O) ,(O) the(O) shortest(O) reference(O) sentence(O) had(O) been(O) used(O) instead(O) .(O)"}}
{"id": "262", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "programming language", "organization", "country", "metric", "product", "task", "conference", "location", "person", "university", "field", "researcher"], "instance": {"id": "262", "words": ["A", "subset", "called", "Micro-Planner", "was", "implemented", "by", "Gerald", "Jay", "Sussman", ",", "Eugene", "Charniak", "and", "Terry", "Winograd", "Sussman", ",", ",", "and", "Winograd", "1971", "and", "was", "used", "in", "Winograd", "'s", "natural-language", "understanding", "program", "SHRDLU", ",", "Eugene", "Charniak", "'s", "story", "understanding", "work", ",", "Thorne", "McCarty", "'s", "work", "on", "legal", "reasoning", ",", "and", "some", "other", "projects", "."], "labels": ["O", "O", "O", "B-product", "O", "O", "O", "B-researcher", "I-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "B-researcher", "O", "O", "O", "B-researcher", "O", "O", "O", "O", "O", "B-researcher", "O", "B-task", "I-task", "O", "B-product", "O", "B-researcher", "I-researcher", "O", "B-task", "I-task", "O", "O", "B-researcher", "I-researcher", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, programming language, organization, country, metric, product, task, conference, location, person, university, field, researcher and O.\nSentence: A subset called Micro-Planner was implemented by Gerald Jay Sussman , Eugene Charniak and Terry Winograd Sussman , , and Winograd 1971 and was used in Winograd 's natural-language understanding program SHRDLU , Eugene Charniak 's story understanding work , Thorne McCarty 's work on legal reasoning , and some other projects .", "prompt_labels": "A(O) subset(O) called(O) Micro-Planner(B-product) was(O) implemented(O) by(O) Gerald(B-researcher) Jay(I-researcher) Sussman(I-researcher) ,(O) Eugene(B-researcher) Charniak(I-researcher) and(O) Terry(B-researcher) Winograd(I-researcher) Sussman(B-researcher) ,(O) ,(O) and(O) Winograd(B-researcher) 1971(O) and(O) was(O) used(O) in(O) Winograd(B-researcher) 's(O) natural-language(B-task) understanding(I-task) program(O) SHRDLU(B-product) ,(O) Eugene(B-researcher) Charniak(I-researcher) 's(O) story(B-task) understanding(I-task) work(O) ,(O) Thorne(B-researcher) McCarty(I-researcher) 's(O) work(O) on(O) legal(B-task) reasoning(I-task) ,(O) and(O) some(O) other(O) projects(O) .(O)"}}
{"id": "11", "dataset": "crossner_ai", "split": "test", "label_list": ["researcher", "metric", "country", "algorithm", "product", "location", "person", "field", "conference", "university", "programming language", "organization", "task"], "instance": {"id": "11", "words": ["Typical", "text", "mining", "tasks", "include", "text", "categorization", ",", "text", "clustering", ",", "concept", "/", "entity", "extraction", ",", "production", "of", "granular", "taxonomies", ",", "sentiment", "analysis", ",", "document", "summarization", ",", "and", "entity", "relation", "modeling", "(", "i.e.", ",", "learning", "relations", "between", "named", "entity", "recognition", ")", "."], "labels": ["O", "B-field", "I-field", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "I-task", "I-task", "O", "B-task", "I-task", "I-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "O", "B-task", "I-task", "I-task", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "I-task", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, metric, country, algorithm, product, location, person, field, conference, university, programming language, organization, task and O.\nSentence: Typical text mining tasks include text categorization , text clustering , concept / entity extraction , production of granular taxonomies , sentiment analysis , document summarization , and entity relation modeling ( i.e. , learning relations between named entity recognition ) .", "prompt_labels": "Typical(O) text(B-field) mining(I-field) tasks(O) include(O) text(B-task) categorization(I-task) ,(O) text(B-task) clustering(I-task) ,(O) concept(B-task) /(I-task) entity(I-task) extraction(I-task) ,(O) production(B-task) of(I-task) granular(I-task) taxonomies(I-task) ,(O) sentiment(B-task) analysis(I-task) ,(O) document(B-task) summarization(I-task) ,(O) and(O) entity(B-task) relation(I-task) modeling(I-task) ((O) i.e.(O) ,(O) learning(O) relations(O) between(O) named(B-task) entity(I-task) recognition(I-task) )(O) .(O)"}}
{"id": "63", "dataset": "crossner_ai", "split": "test", "label_list": ["person", "researcher", "metric", "programming language", "university", "field", "algorithm", "task", "location", "product", "country", "organization", "conference"], "instance": {"id": "63", "words": ["Notable", "former", "PhD", "students", "and", "postdoctoral", "researchers", "from", "his", "group", "include", "Richard", "Zemel", ",", "and", "Zoubin", "Ghahramani", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "O", "B-researcher", "I-researcher", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, researcher, metric, programming language, university, field, algorithm, task, location, product, country, organization, conference and O.\nSentence: Notable former PhD students and postdoctoral researchers from his group include Richard Zemel , and Zoubin Ghahramani .", "prompt_labels": "Notable(O) former(O) PhD(O) students(O) and(O) postdoctoral(O) researchers(O) from(O) his(O) group(O) include(O) Richard(B-researcher) Zemel(I-researcher) ,(O) and(O) Zoubin(B-researcher) Ghahramani(I-researcher) .(O)"}}
{"id": "394", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "conference", "researcher", "algorithm", "location", "university", "organization", "programming language", "metric", "person", "task", "country", "field"], "instance": {"id": "394", "words": ["Where", "Bilingual", "evaluation", "understudy", "simply", "calculates", "n-gram", "precision", "adding", "equal", "weight", "to", "each", "one", ",", "NIST", "also", "calculates", "how", "informative", "a", "particular", "n-gram", "is", "."], "labels": ["O", "B-metric", "I-metric", "I-metric", "O", "O", "B-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, conference, researcher, algorithm, location, university, organization, programming language, metric, person, task, country, field and O.\nSentence: Where Bilingual evaluation understudy simply calculates n-gram precision adding equal weight to each one , NIST also calculates how informative a particular n-gram is .", "prompt_labels": "Where(O) Bilingual(B-metric) evaluation(I-metric) understudy(I-metric) simply(O) calculates(O) n-gram(B-metric) precision(I-metric) adding(O) equal(O) weight(O) to(O) each(O) one(O) ,(O) NIST(B-metric) also(O) calculates(O) how(O) informative(O) a(O) particular(O) n-gram(O) is(O) .(O)"}}
{"id": "135", "dataset": "crossner_ai", "split": "test", "label_list": ["task", "algorithm", "organization", "location", "metric", "person", "conference", "country", "field", "product", "researcher", "university", "programming language"], "instance": {"id": "135", "words": ["Two", "shallow", "approaches", "used", "to", "train", "and", "then", "disambiguate", "are", "Naive", "Bayes", "classifier", "and", "decision", "trees", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, algorithm, organization, location, metric, person, conference, country, field, product, researcher, university, programming language and O.\nSentence: Two shallow approaches used to train and then disambiguate are Naive Bayes classifier and decision trees .", "prompt_labels": "Two(O) shallow(O) approaches(O) used(O) to(O) train(O) and(O) then(O) disambiguate(O) are(O) Naive(B-algorithm) Bayes(I-algorithm) classifier(I-algorithm) and(O) decision(B-algorithm) trees(I-algorithm) .(O)"}}
{"id": "374", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "programming language", "location", "product", "researcher", "field", "university", "person", "task", "organization", "country", "algorithm", "metric"], "instance": {"id": "374", "words": ["He", "is", "a", "Fellow", "of", "the", "Association", "for", "Computing", "Machinery", ",", "IEEE", ",", "American", "Association", "for", "the", "Advancement", "of", "Science", ",", "IAPR", "and", "SPIE", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "O", "B-organization", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "B-conference", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, programming language, location, product, researcher, field, university, person, task, organization, country, algorithm, metric and O.\nSentence: He is a Fellow of the Association for Computing Machinery , IEEE , American Association for the Advancement of Science , IAPR and SPIE .", "prompt_labels": "He(O) is(O) a(O) Fellow(O) of(O) the(O) Association(B-conference) for(I-conference) Computing(I-conference) Machinery(I-conference) ,(O) IEEE(B-organization) ,(O) American(B-conference) Association(I-conference) for(I-conference) the(I-conference) Advancement(I-conference) of(I-conference) Science(I-conference) ,(O) IAPR(B-conference) and(O) SPIE(B-conference) .(O)"}}
{"id": "427", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "programming language", "task", "person", "metric", "conference", "algorithm", "university", "location", "product", "organization", "country", "researcher"], "instance": {"id": "427", "words": ["As", "part", "of", "the", "2006", "European", "Conference", "on", "Computer", "Vision", "(", "ECCV", ")", ",", "Dalal", "and", "Triggs", "teamed", "up", "with", "Cordelia", "Schmid", "to", "apply", "HOG", "detectors", "to", "the", "problem", "of", "human", "detection", "in", "films", "and", "videos", "."], "labels": ["O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "O", "B-researcher", "O", "B-researcher", "O", "O", "O", "B-researcher", "I-researcher", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "B-task", "I-task", "I-task", "I-task", "I-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, programming language, task, person, metric, conference, algorithm, university, location, product, organization, country, researcher and O.\nSentence: As part of the 2006 European Conference on Computer Vision ( ECCV ) , Dalal and Triggs teamed up with Cordelia Schmid to apply HOG detectors to the problem of human detection in films and videos .", "prompt_labels": "As(O) part(O) of(O) the(O) 2006(B-conference) European(I-conference) Conference(I-conference) on(I-conference) Computer(I-conference) Vision(I-conference) ((O) ECCV(B-conference) )(O) ,(O) Dalal(B-researcher) and(O) Triggs(B-researcher) teamed(O) up(O) with(O) Cordelia(B-researcher) Schmid(I-researcher) to(O) apply(O) HOG(B-algorithm) detectors(I-algorithm) to(O) the(O) problem(O) of(O) human(B-task) detection(I-task) in(I-task) films(I-task) and(I-task) videos(I-task) .(O)"}}
{"id": "371", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "location", "algorithm", "task", "researcher", "person", "programming language", "field", "product", "country", "conference", "organization", "metric"], "instance": {"id": "371", "words": ["OPeNDAP", "offers", "open-source", "libraries", "in", "C", "+", "+", "and", "Java", ",", "but", "many", "clients", "rely", "on", "community", "developed", "libraries", "such", "as", "libraries", "include", "embedded", "capabilities", "for", "retrieving", "(", "array-style", ")", "data", "from", "DAP", "servers", "."], "labels": ["B-organization", "O", "O", "O", "O", "B-programming language", "I-programming language", "I-programming language", "O", "B-programming language", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, location, algorithm, task, researcher, person, programming language, field, product, country, conference, organization, metric and O.\nSentence: OPeNDAP offers open-source libraries in C + + and Java , but many clients rely on community developed libraries such as libraries include embedded capabilities for retrieving ( array-style ) data from DAP servers .", "prompt_labels": "OPeNDAP(B-organization) offers(O) open-source(O) libraries(O) in(O) C(B-programming language) +(I-programming language) +(I-programming language) and(O) Java(B-programming language) ,(O) but(O) many(O) clients(O) rely(O) on(O) community(O) developed(O) libraries(O) such(O) as(O) libraries(O) include(O) embedded(O) capabilities(O) for(O) retrieving(O) ((O) array-style(O) )(O) data(O) from(O) DAP(O) servers(O) .(O)"}}
{"id": "142", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "location", "task", "country", "person", "conference", "university", "researcher", "algorithm", "metric", "product", "programming language", "organization"], "instance": {"id": "142", "words": ["An", "unrelated", "but", "commonly", "used", "combination", "of", "basic", "statistics", "from", "information", "retrieval", "is", "the", "F-score", ",", "being", "a", "(", "possibly", "weighted", ")", "harmonic", "mean", "of", "recall", "and", "precision", "where", "recall", "=", "sensitivity", "=", "TRUE", "positive", "rate", ",", "but", "specificity", "and", "precision", "are", "totally", "different", "measures", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "B-metric", "O", "B-metric", "O", "B-metric", "O", "B-metric", "I-metric", "I-metric", "O", "O", "B-metric", "O", "B-metric", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, location, task, country, person, conference, university, researcher, algorithm, metric, product, programming language, organization and O.\nSentence: An unrelated but commonly used combination of basic statistics from information retrieval is the F-score , being a ( possibly weighted ) harmonic mean of recall and precision where recall = sensitivity = TRUE positive rate , but specificity and precision are totally different measures .", "prompt_labels": "An(O) unrelated(O) but(O) commonly(O) used(O) combination(O) of(O) basic(O) statistics(O) from(O) information(B-task) retrieval(I-task) is(O) the(O) F-score(B-metric) ,(O) being(O) a(O) ((O) possibly(O) weighted(O) )(O) harmonic(O) mean(O) of(O) recall(B-metric) and(O) precision(B-metric) where(O) recall(B-metric) =(O) sensitivity(B-metric) =(O) TRUE(B-metric) positive(I-metric) rate(I-metric) ,(O) but(O) specificity(B-metric) and(O) precision(B-metric) are(O) totally(O) different(O) measures(O) .(O)"}}
{"id": "288", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "conference", "metric", "programming language", "field", "task", "country", "product", "algorithm", "location", "person", "university", "researcher"], "instance": {"id": "288", "words": ["Also", "known", "as", "parallel", "robots", ",", "or", "generalized", "Stewart", "platforms", "(", "in", "the", "Stewart", "platform", ",", "the", "actuators", "are", "paired", "together", "on", "both", "the", "basis", "and", "the", "platform", ")", ",", "these", "systems", "are", "articulated", "robot", "s", "that", "use", "similar", "mechanisms", "for", "the", "movement", "of", "either", "the", "robot", "on", "its", "base", ",", "or", "one", "or", "more", "manipulator", "arms", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O", "O", "O", "B-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, conference, metric, programming language, field, task, country, product, algorithm, location, person, university, researcher and O.\nSentence: Also known as parallel robots , or generalized Stewart platforms ( in the Stewart platform , the actuators are paired together on both the basis and the platform ) , these systems are articulated robot s that use similar mechanisms for the movement of either the robot on its base , or one or more manipulator arms .", "prompt_labels": "Also(O) known(O) as(O) parallel(O) robots(O) ,(O) or(O) generalized(O) Stewart(B-product) platforms(I-product) ((O) in(O) the(O) Stewart(B-product) platform(I-product) ,(O) the(O) actuators(O) are(O) paired(O) together(O) on(O) both(O) the(O) basis(O) and(O) the(O) platform(O) )(O) ,(O) these(O) systems(O) are(O) articulated(B-product) robot(I-product) s(O) that(O) use(O) similar(O) mechanisms(O) for(O) the(O) movement(O) of(O) either(O) the(O) robot(O) on(O) its(O) base(O) ,(O) or(O) one(O) or(O) more(O) manipulator(O) arms(O) .(O)"}}
{"id": "365", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "university", "country", "organization", "field", "location", "metric", "product", "programming language", "researcher", "algorithm", "person", "task"], "instance": {"id": "365", "words": ["Not", "only", "does", "this", "alter", "the", "performance", "of", "all", "subsequent", "tests", "on", "the", "retained", "explanatory", "model", ",", "it", "may", "introduce", "bias", "and", "alter", "mean", "square", "error", "in", "estimation", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, university, country, organization, field, location, metric, product, programming language, researcher, algorithm, person, task and O.\nSentence: Not only does this alter the performance of all subsequent tests on the retained explanatory model , it may introduce bias and alter mean square error in estimation .", "prompt_labels": "Not(O) only(O) does(O) this(O) alter(O) the(O) performance(O) of(O) all(O) subsequent(O) tests(O) on(O) the(O) retained(O) explanatory(O) model(O) ,(O) it(O) may(O) introduce(O) bias(O) and(O) alter(O) mean(B-metric) square(I-metric) error(I-metric) in(O) estimation(O) .(O)"}}
{"id": "217", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "country", "field", "university", "researcher", "location", "product", "conference", "programming language", "algorithm", "task", "person", "metric"], "instance": {"id": "217", "words": ["An", "early", "version", "of", "VMAF", "has", "been", "shown", "to", "outperform", "other", "image", "and", "video", "quality", "metrics", "such", "as", "SSIM", ",", "PSNR", "-HVS", "and", "VQM-VFD", "on", "three", "of", "four", "datasets", "in", "terms", "of", "prediction", "accuracy", ",", "when", "compared", "to", "subjective", "ratings", "."], "labels": ["O", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "B-metric", "I-metric", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, field, university, researcher, location, product, conference, programming language, algorithm, task, person, metric and O.\nSentence: An early version of VMAF has been shown to outperform other image and video quality metrics such as SSIM , PSNR -HVS and VQM-VFD on three of four datasets in terms of prediction accuracy , when compared to subjective ratings .", "prompt_labels": "An(O) early(O) version(O) of(O) VMAF(B-metric) has(O) been(O) shown(O) to(O) outperform(O) other(O) image(O) and(O) video(O) quality(O) metrics(O) such(O) as(O) SSIM(B-metric) ,(O) PSNR(B-metric) -HVS(I-metric) and(O) VQM-VFD(B-metric) on(O) three(O) of(O) four(O) datasets(O) in(O) terms(O) of(O) prediction(O) accuracy(B-metric) ,(O) when(O) compared(O) to(O) subjective(O) ratings(O) .(O)"}}
{"id": "273", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "country", "person", "algorithm", "field", "researcher", "metric", "product", "conference", "location", "organization", "programming language", "task"], "instance": {"id": "273", "words": ["Hidden", "Markov", "model", "s", "are", "the", "basis", "for", "most", "modern", "automatic", "speech", "recognition", "systems", "."], "labels": ["B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, country, person, algorithm, field, researcher, metric, product, conference, location, organization, programming language, task and O.\nSentence: Hidden Markov model s are the basis for most modern automatic speech recognition systems .", "prompt_labels": "Hidden(B-algorithm) Markov(I-algorithm) model(I-algorithm) s(O) are(O) the(O) basis(O) for(O) most(O) modern(O) automatic(B-product) speech(I-product) recognition(I-product) systems(I-product) .(O)"}}
{"id": "69", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "country", "task", "programming language", "algorithm", "product", "conference", "person", "field", "location", "researcher", "metric", "university"], "instance": {"id": "69", "words": ["There", "are", "many", "more", "recent", "algorithms", "such", "as", "LPBoost", ",", "TotalBoost", ",", "BrownBoost", ",", "xgboost", ",", "MadaBoost", ",", ",", "and", "others", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "O", "B-algorithm", "O", "B-algorithm", "O", "B-algorithm", "O", "B-algorithm", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, task, programming language, algorithm, product, conference, person, field, location, researcher, metric, university and O.\nSentence: There are many more recent algorithms such as LPBoost , TotalBoost , BrownBoost , xgboost , MadaBoost , , and others .", "prompt_labels": "There(O) are(O) many(O) more(O) recent(O) algorithms(O) such(O) as(O) LPBoost(B-algorithm) ,(O) TotalBoost(B-algorithm) ,(O) BrownBoost(B-algorithm) ,(O) xgboost(B-algorithm) ,(O) MadaBoost(B-algorithm) ,(O) ,(O) and(O) others(O) .(O)"}}
{"id": "121", "dataset": "crossner_ai", "split": "test", "label_list": ["programming language", "organization", "country", "university", "researcher", "algorithm", "location", "person", "product", "field", "metric", "task", "conference"], "instance": {"id": "121", "words": ["But", "even", "an", "official", "language", "with", "a", "regulating", "academy", ",", "such", "as", "Standard", "French", "with", "the", "Académie", "française", ",", "is", "classified", "as", "a", "natural", "language", "(", "for", "example", ",", "in", "the", "field", "of", "natural", "language", "processing", ")", ",", "as", "its", "prescriptive", "points", "do", "not", "make", "it", "either", "constructed", "enough", "to", "be", "classified", "as", "a", "constructed", "language", "or", "controlled", "enough", "to", "be", "classified", "as", "a", "controlled", "natural", "language", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, organization, country, university, researcher, algorithm, location, person, product, field, metric, task, conference and O.\nSentence: But even an official language with a regulating academy , such as Standard French with the Académie française , is classified as a natural language ( for example , in the field of natural language processing ) , as its prescriptive points do not make it either constructed enough to be classified as a constructed language or controlled enough to be classified as a controlled natural language .", "prompt_labels": "But(O) even(O) an(O) official(O) language(O) with(O) a(O) regulating(O) academy(O) ,(O) such(O) as(O) Standard(O) French(O) with(O) the(O) Académie(B-organization) française(I-organization) ,(O) is(O) classified(O) as(O) a(O) natural(O) language(O) ((O) for(O) example(O) ,(O) in(O) the(O) field(O) of(O) natural(B-field) language(I-field) processing(I-field) )(O) ,(O) as(O) its(O) prescriptive(O) points(O) do(O) not(O) make(O) it(O) either(O) constructed(O) enough(O) to(O) be(O) classified(O) as(O) a(O) constructed(O) language(O) or(O) controlled(O) enough(O) to(O) be(O) classified(O) as(O) a(O) controlled(O) natural(O) language(O) .(O)"}}
{"id": "250", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "organization", "location", "metric", "algorithm", "task", "person", "product", "country", "programming language", "researcher", "conference", "field"], "instance": {"id": "250", "words": ["Atran", "has", "taught", "at", "Cambridge", "University", ",", "Hebrew", "University", "in", "Jerusalem", ",", "the", "École", "pratique", "des", "hautes", "études", "and", "École", "Polytechnique", "in", "Paris", ",", "and", "John", "Jay", "College", "of", "Criminal", "Justice", "in", "New", "York", "City", "."], "labels": ["B-researcher", "O", "O", "O", "B-university", "I-university", "O", "B-university", "I-university", "O", "B-location", "O", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "O", "B-university", "I-university", "O", "B-location", "O", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "I-university", "O", "B-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, organization, location, metric, algorithm, task, person, product, country, programming language, researcher, conference, field and O.\nSentence: Atran has taught at Cambridge University , Hebrew University in Jerusalem , the École pratique des hautes études and École Polytechnique in Paris , and John Jay College of Criminal Justice in New York City .", "prompt_labels": "Atran(B-researcher) has(O) taught(O) at(O) Cambridge(B-university) University(I-university) ,(O) Hebrew(B-university) University(I-university) in(O) Jerusalem(B-location) ,(O) the(O) École(B-university) pratique(I-university) des(I-university) hautes(I-university) études(I-university) and(O) École(B-university) Polytechnique(I-university) in(O) Paris(B-location) ,(O) and(O) John(B-university) Jay(I-university) College(I-university) of(I-university) Criminal(I-university) Justice(I-university) in(O) New(B-location) York(I-location) City(I-location) .(O)"}}
{"id": "302", "dataset": "crossner_ai", "split": "test", "label_list": ["person", "location", "product", "task", "researcher", "university", "metric", "conference", "country", "organization", "algorithm", "field", "programming language"], "instance": {"id": "302", "words": ["Stephen", "H.", "Muggleton", "FBCS", ",", "FIET", ",", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", ","], "labels": ["B-researcher", "I-researcher", "I-researcher", "B-organization", "O", "B-organization", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, product, task, researcher, university, metric, conference, country, organization, algorithm, field, programming language and O.\nSentence: Stephen H. Muggleton FBCS , FIET , Association for the Advancement of Artificial Intelligence ,", "prompt_labels": "Stephen(B-researcher) H.(I-researcher) Muggleton(I-researcher) FBCS(B-organization) ,(O) FIET(B-organization) ,(O) Association(B-conference) for(I-conference) the(I-conference) Advancement(I-conference) of(I-conference) Artificial(I-conference) Intelligence(I-conference) ,(O)"}}
{"id": "420", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "conference", "organization", "person", "country", "task", "researcher", "field", "location", "algorithm", "programming language", "metric", "university"], "instance": {"id": "420", "words": ["The", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "has", "studied", "this", "topic", "in", "depth"], "labels": ["O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, conference, organization, person, country, task, researcher, field, location, algorithm, programming language, metric, university and O.\nSentence: The Association for the Advancement of Artificial Intelligence has studied this topic in depth", "prompt_labels": "The(O) Association(B-conference) for(I-conference) the(I-conference) Advancement(I-conference) of(I-conference) Artificial(I-conference) Intelligence(I-conference) has(O) studied(O) this(O) topic(O) in(O) depth(O)"}}
{"id": "12", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "conference", "product", "programming language", "country", "person", "algorithm", "organization", "task", "metric", "location", "field", "researcher"], "instance": {"id": "12", "words": ["Nonetheless", ",", "stemming", "reduces", "precision", ",", "or", "TRUE", "negative", "rate", ",", "for", "such", "systems", "."], "labels": ["O", "O", "O", "O", "B-metric", "O", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, conference, product, programming language, country, person, algorithm, organization, task, metric, location, field, researcher and O.\nSentence: Nonetheless , stemming reduces precision , or TRUE negative rate , for such systems .", "prompt_labels": "Nonetheless(O) ,(O) stemming(O) reduces(O) precision(B-metric) ,(O) or(O) TRUE(B-metric) negative(I-metric) rate(I-metric) ,(O) for(O) such(O) systems(O) .(O)"}}
{"id": "33", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "task", "field", "product", "metric", "conference", "algorithm", "researcher", "country", "programming language", "university", "location", "person"], "instance": {"id": "33", "words": ["Johnson-Laird", "is", "a", "Fellow", "of", "the", "American", "Philosophical", "Society", ",", "a", "Fellow", "of", "the", "Royal", "Society", ",", "a", "Fellow", "of", "the", "British", "Academy", ",", "a", "William", "James", "Fellow", "of", "the", "Association", "for", "Psychological", "Science", ",", "and", "a", "Fellow", "of", "the", "Cognitive", "Science", "Society", "."], "labels": ["B-researcher", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-researcher", "I-researcher", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, task, field, product, metric, conference, algorithm, researcher, country, programming language, university, location, person and O.\nSentence: Johnson-Laird is a Fellow of the American Philosophical Society , a Fellow of the Royal Society , a Fellow of the British Academy , a William James Fellow of the Association for Psychological Science , and a Fellow of the Cognitive Science Society .", "prompt_labels": "Johnson-Laird(B-researcher) is(O) a(O) Fellow(O) of(O) the(O) American(B-organization) Philosophical(I-organization) Society(I-organization) ,(O) a(O) Fellow(O) of(O) the(O) Royal(B-organization) Society(I-organization) ,(O) a(O) Fellow(O) of(O) the(O) British(B-organization) Academy(I-organization) ,(O) a(O) William(B-researcher) James(I-researcher) Fellow(O) of(O) the(O) Association(B-organization) for(I-organization) Psychological(I-organization) Science(I-organization) ,(O) and(O) a(O) Fellow(O) of(O) the(O) Cognitive(B-organization) Science(I-organization) Society(I-organization) .(O)"}}
{"id": "348", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "researcher", "location", "university", "product", "organization", "country", "metric", "task", "algorithm", "conference", "programming language", "person"], "instance": {"id": "348", "words": ["For", "multilayer", "perceptron", "s", ",", "where", "a", "hidden", "layer", "exists", ",", "more", "sophisticated", "algorithms", "such", "as", "backpropagation", "must", "be", "used", "."], "labels": ["O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, researcher, location, university, product, organization, country, metric, task, algorithm, conference, programming language, person and O.\nSentence: For multilayer perceptron s , where a hidden layer exists , more sophisticated algorithms such as backpropagation must be used .", "prompt_labels": "For(O) multilayer(B-algorithm) perceptron(I-algorithm) s(O) ,(O) where(O) a(O) hidden(O) layer(O) exists(O) ,(O) more(O) sophisticated(O) algorithms(O) such(O) as(O) backpropagation(B-algorithm) must(O) be(O) used(O) .(O)"}}
{"id": "405", "dataset": "crossner_ai", "split": "test", "label_list": ["programming language", "metric", "task", "conference", "algorithm", "university", "person", "country", "field", "researcher", "product", "organization", "location"], "instance": {"id": "405", "words": ["Examples", "include", "Salford", "Systems", "CART", "(", "which", "licensed", "the", "proprietary", "code", "of", "the", "original", "CART", "authors", ")", ",", "IBM", "SPSS", "Modeler", ",", "RapidMiner", ",", "SAS", "Enterprise", "Miner", ",", "Matlab", ",", "R", "(", "an", "open-source", "software", "environment", "for", "statistical", "computing", ",", "which", "includes", "several", "CART", "implementations", "such", "as", "rpart", ",", "party", "and", "randomForest", "packages", ")", ",", "Weka", "(", "a", "free", "and", "open-source", "data-mining", "suite", ",", "contains", "many", "decision", "tree", "algorithms", ")", ",", "Orange", ",", "KNIME", ",", "Microsoft", "SQL", "Server", "programming", "language", ")", "."], "labels": ["O", "O", "B-organization", "I-organization", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O", "O", "O", "B-organization", "B-product", "I-product", "O", "B-product", "O", "B-product", "I-product", "I-product", "O", "B-product", "O", "B-programming language", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "B-product", "O", "O", "O", "B-algorithm", "O", "B-algorithm", "O", "B-algorithm", "O", "O", "O", "B-product", "O", "O", "O", "O", "O", "B-task", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "B-product", "O", "B-product", "O", "B-product", "I-product", "I-product", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, metric, task, conference, algorithm, university, person, country, field, researcher, product, organization, location and O.\nSentence: Examples include Salford Systems CART ( which licensed the proprietary code of the original CART authors ) , IBM SPSS Modeler , RapidMiner , SAS Enterprise Miner , Matlab , R ( an open-source software environment for statistical computing , which includes several CART implementations such as rpart , party and randomForest packages ) , Weka ( a free and open-source data-mining suite , contains many decision tree algorithms ) , Orange , KNIME , Microsoft SQL Server programming language ) .", "prompt_labels": "Examples(O) include(O) Salford(B-organization) Systems(I-organization) CART(B-product) ((O) which(O) licensed(O) the(O) proprietary(O) code(O) of(O) the(O) original(O) CART(B-product) authors(O) )(O) ,(O) IBM(B-organization) SPSS(B-product) Modeler(I-product) ,(O) RapidMiner(B-product) ,(O) SAS(B-product) Enterprise(I-product) Miner(I-product) ,(O) Matlab(B-product) ,(O) R(B-programming language) ((O) an(O) open-source(O) software(O) environment(O) for(O) statistical(B-field) computing(I-field) ,(O) which(O) includes(O) several(O) CART(B-product) implementations(O) such(O) as(O) rpart(B-algorithm) ,(O) party(B-algorithm) and(O) randomForest(B-algorithm) packages(O) )(O) ,(O) Weka(B-product) ((O) a(O) free(O) and(O) open-source(O) data-mining(B-task) suite(O) ,(O) contains(O) many(O) decision(B-algorithm) tree(I-algorithm) algorithms(O) )(O) ,(O) Orange(B-product) ,(O) KNIME(B-product) ,(O) Microsoft(B-product) SQL(I-product) Server(I-product) programming(O) language(O) )(O) .(O)"}}
{"id": "18", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "location", "researcher", "conference", "university", "task", "programming language", "algorithm", "country", "person", "organization", "product", "metric"], "instance": {"id": "18", "words": ["The", "first", "publication", "about", "the", "LMF", "specification", "as", "it", "has", "been", "ratified", "by", "ISO", "(", "this", "paper", "became", "(", "in", "2015", ")", "the", "9th", "most", "cited", "paper", "within", "the", "LREC", "conferences", "from", "LREC", "papers", ")", ":"], "labels": ["O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "O", "O", "B-conference", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, location, researcher, conference, university, task, programming language, algorithm, country, person, organization, product, metric and O.\nSentence: The first publication about the LMF specification as it has been ratified by ISO ( this paper became ( in 2015 ) the 9th most cited paper within the LREC conferences from LREC papers ) :", "prompt_labels": "The(O) first(O) publication(O) about(O) the(O) LMF(B-task) specification(I-task) as(O) it(O) has(O) been(O) ratified(O) by(O) ISO(B-organization) ((O) this(O) paper(O) became(O) ((O) in(O) 2015(O) )(O) the(O) 9th(O) most(O) cited(O) paper(O) within(O) the(O) LREC(B-conference) conferences(O) from(O) LREC(B-conference) papers(O) )(O) :(O)"}}
{"id": "317", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "metric", "product", "programming language", "location", "field", "organization", "country", "researcher", "task", "conference", "person", "university"], "instance": {"id": "317", "words": ["Other", "linear", "classification", "algorithms", "include", "Winnow", ",", "support", "vector", "machine", "and", "logistic", "regression", "."], "labels": ["O", "O", "O", "O", "O", "B-algorithm", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, metric, product, programming language, location, field, organization, country, researcher, task, conference, person, university and O.\nSentence: Other linear classification algorithms include Winnow , support vector machine and logistic regression .", "prompt_labels": "Other(O) linear(O) classification(O) algorithms(O) include(O) Winnow(B-algorithm) ,(O) support(B-algorithm) vector(I-algorithm) machine(I-algorithm) and(O) logistic(B-algorithm) regression(I-algorithm) .(O)"}}
{"id": "400", "dataset": "crossner_ai", "split": "test", "label_list": ["person", "organization", "country", "location", "university", "field", "conference", "task", "product", "programming language", "researcher", "metric", "algorithm"], "instance": {"id": "400", "words": ["Crawler-friendly", "Web", "Servers", ",", "and", "it", "integrates", "the", "features", "of", "sitemaps", "and", "RSS", "feeds", "into", "a", "decentralized", "mechanism", "for", "computational", "biologists", "and", "bio-informaticians", "to", "openly", "broadcast", "and", "retrieve", "meta-data", "about", "biomedical", "resources", "."], "labels": ["B-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, country, location, university, field, conference, task, product, programming language, researcher, metric, algorithm and O.\nSentence: Crawler-friendly Web Servers , and it integrates the features of sitemaps and RSS feeds into a decentralized mechanism for computational biologists and bio-informaticians to openly broadcast and retrieve meta-data about biomedical resources .", "prompt_labels": "Crawler-friendly(B-product) Web(I-product) Servers(I-product) ,(O) and(O) it(O) integrates(O) the(O) features(O) of(O) sitemaps(O) and(O) RSS(O) feeds(O) into(O) a(O) decentralized(O) mechanism(O) for(O) computational(O) biologists(O) and(O) bio-informaticians(O) to(O) openly(O) broadcast(O) and(O) retrieve(O) meta-data(O) about(O) biomedical(O) resources(O) .(O)"}}
{"id": "253", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "country", "algorithm", "programming language", "organization", "person", "researcher", "location", "metric", "field", "conference", "university", "task"], "instance": {"id": "253", "words": ["Accuracy", "is", "usually", "rated", "with", "word", "error", "rate", "(", "WER", ")", ",", "whereas", "speed", "is", "measured", "with", "the", "real", "time", "factor", "."], "labels": ["O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, country, algorithm, programming language, organization, person, researcher, location, metric, field, conference, university, task and O.\nSentence: Accuracy is usually rated with word error rate ( WER ) , whereas speed is measured with the real time factor .", "prompt_labels": "Accuracy(O) is(O) usually(O) rated(O) with(O) word(B-metric) error(I-metric) rate(I-metric) ((O) WER(B-metric) )(O) ,(O) whereas(O) speed(O) is(O) measured(O) with(O) the(O) real(B-metric) time(I-metric) factor(I-metric) .(O)"}}
{"id": "370", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "location", "task", "programming language", "algorithm", "metric", "conference", "organization", "product", "researcher", "country", "university", "person"], "instance": {"id": "370", "words": ["Hyponymy", "is", "the", "most", "frequently", "encoded", "relation", "among", "synsets", "used", "in", "lexical", "databases", "such", "as", "WordNet", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, location, task, programming language, algorithm, metric, conference, organization, product, researcher, country, university, person and O.\nSentence: Hyponymy is the most frequently encoded relation among synsets used in lexical databases such as WordNet .", "prompt_labels": "Hyponymy(O) is(O) the(O) most(O) frequently(O) encoded(O) relation(O) among(O) synsets(O) used(O) in(O) lexical(O) databases(O) such(O) as(O) WordNet(B-product) .(O)"}}
{"id": "393", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "university", "person", "organization", "country", "conference", "location", "product", "researcher", "task", "field", "programming language", "metric"], "instance": {"id": "393", "words": ["Denso", "Wave", "is", "a", "subsidiary", "that", "produces", "automatic", "identification", "products", "(", "bar-code", "reader", "s", "and", "related", "products", ")", ",", "industrial", "robot", "s", "and", "programmable", "logic", "controller", "s", "."], "labels": ["B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O", "O", "B-product", "I-product", "I-product", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, university, person, organization, country, conference, location, product, researcher, task, field, programming language, metric and O.\nSentence: Denso Wave is a subsidiary that produces automatic identification products ( bar-code reader s and related products ) , industrial robot s and programmable logic controller s .", "prompt_labels": "Denso(B-organization) Wave(I-organization) is(O) a(O) subsidiary(O) that(O) produces(O) automatic(O) identification(O) products(O) ((O) bar-code(B-product) reader(I-product) s(O) and(O) related(O) products(O) )(O) ,(O) industrial(B-product) robot(I-product) s(O) and(O) programmable(B-product) logic(I-product) controller(I-product) s(O) .(O)"}}
{"id": "146", "dataset": "crossner_ai", "split": "test", "label_list": ["researcher", "metric", "location", "programming language", "country", "field", "algorithm", "university", "organization", "person", "conference", "product", "task"], "instance": {"id": "146", "words": ["The", "MCC", "can", "be", "calculated", "directly", "from", "the", "confusion", "matrix", "using", "the", "formula", ":"], "labels": ["O", "B-metric", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, metric, location, programming language, country, field, algorithm, university, organization, person, conference, product, task and O.\nSentence: The MCC can be calculated directly from the confusion matrix using the formula :", "prompt_labels": "The(O) MCC(B-metric) can(O) be(O) calculated(O) directly(O) from(O) the(O) confusion(B-metric) matrix(I-metric) using(O) the(O) formula(O) :(O)"}}
{"id": "342", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "country", "task", "metric", "university", "person", "programming language", "conference", "field", "researcher", "organization", "location", "algorithm"], "instance": {"id": "342", "words": ["In", "data", "mining", "and", "statistics", ",", "hierarchical", "clustering", "(", "also", "called", "hierarchical", "cluster", "analysis", "or", "HCA", ")", "is", "a", "method", "of", "cluster", "analysis", "which", "seeks", "to", "build", "a", "hierarchy", "of", "clusters", "."], "labels": ["O", "B-field", "I-field", "O", "B-field", "O", "B-task", "I-task", "O", "O", "O", "B-task", "I-task", "I-task", "O", "B-task", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, country, task, metric, university, person, programming language, conference, field, researcher, organization, location, algorithm and O.\nSentence: In data mining and statistics , hierarchical clustering ( also called hierarchical cluster analysis or HCA ) is a method of cluster analysis which seeks to build a hierarchy of clusters .", "prompt_labels": "In(O) data(B-field) mining(I-field) and(O) statistics(B-field) ,(O) hierarchical(B-task) clustering(I-task) ((O) also(O) called(O) hierarchical(B-task) cluster(I-task) analysis(I-task) or(O) HCA(B-task) )(O) is(O) a(O) method(O) of(O) cluster(B-task) analysis(I-task) which(O) seeks(O) to(O) build(O) a(O) hierarchy(O) of(O) clusters(O) .(O)"}}
{"id": "42", "dataset": "crossner_ai", "split": "test", "label_list": ["country", "field", "metric", "task", "person", "product", "conference", "algorithm", "location", "programming language", "researcher", "organization", "university"], "instance": {"id": "42", "words": ["Google", "Translate", "is", "a", "free", "multilingual", "statistical", "machine", "translation", "and", "neural", "machine", "translation", "service", "developed", "by", "Google", ",", "to", "translate", "text", "and", "websites", "from", "one", "language", "into", "another", "."], "labels": ["B-product", "I-product", "O", "O", "O", "B-task", "I-task", "I-task", "I-task", "O", "B-task", "I-task", "I-task", "O", "O", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, field, metric, task, person, product, conference, algorithm, location, programming language, researcher, organization, university and O.\nSentence: Google Translate is a free multilingual statistical machine translation and neural machine translation service developed by Google , to translate text and websites from one language into another .", "prompt_labels": "Google(B-product) Translate(I-product) is(O) a(O) free(O) multilingual(B-task) statistical(I-task) machine(I-task) translation(I-task) and(O) neural(B-task) machine(I-task) translation(I-task) service(O) developed(O) by(O) Google(B-product) ,(O) to(O) translate(O) text(O) and(O) websites(O) from(O) one(O) language(O) into(O) another(O) .(O)"}}
{"id": "430", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "university", "organization", "programming language", "researcher", "field", "task", "person", "algorithm", "metric", "location", "country", "conference"], "instance": {"id": "430", "words": ["Further", ",", "in", "the", "case", "of", "estimation", "based", "on", "a", "single", "sample", ",", "it", "demonstrates", "philosophical", "issues", "and", "possible", "misunderstandings", "in", "the", "use", "of", "maximum", "likelihood", "estimators", "and", "likelihood", "functions", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "I-metric", "I-metric", "I-metric", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, university, organization, programming language, researcher, field, task, person, algorithm, metric, location, country, conference and O.\nSentence: Further , in the case of estimation based on a single sample , it demonstrates philosophical issues and possible misunderstandings in the use of maximum likelihood estimators and likelihood functions .", "prompt_labels": "Further(O) ,(O) in(O) the(O) case(O) of(O) estimation(O) based(O) on(O) a(O) single(O) sample(O) ,(O) it(O) demonstrates(O) philosophical(O) issues(O) and(O) possible(O) misunderstandings(O) in(O) the(O) use(O) of(O) maximum(B-metric) likelihood(I-metric) estimators(I-metric) and(I-metric) likelihood(I-metric) functions(I-metric) .(O)"}}
{"id": "247", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "programming language", "location", "person", "organization", "algorithm", "metric", "field", "task", "conference", "country", "product", "researcher"], "instance": {"id": "247", "words": ["If", "the", "modeling", "is", "done", "by", "an", "artificial", "neural", "network", "or", "other", "machine", "learning", ",", "the", "optimization", "of", "parameters", "is", "called", "training", ",", "while", "the", "optimization", "of", "model", "hyperparameters", "is", "called", "tuning", "and", "often", "uses", "cross-validation", ".."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, programming language, location, person, organization, algorithm, metric, field, task, conference, country, product, researcher and O.\nSentence: If the modeling is done by an artificial neural network or other machine learning , the optimization of parameters is called training , while the optimization of model hyperparameters is called tuning and often uses cross-validation ..", "prompt_labels": "If(O) the(O) modeling(O) is(O) done(O) by(O) an(O) artificial(B-algorithm) neural(I-algorithm) network(I-algorithm) or(O) other(O) machine(B-field) learning(I-field) ,(O) the(O) optimization(O) of(O) parameters(O) is(O) called(O) training(O) ,(O) while(O) the(O) optimization(O) of(O) model(O) hyperparameters(O) is(O) called(O) tuning(O) and(O) often(O) uses(O) cross-validation(B-algorithm) ..(O)"}}
{"id": "178", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "university", "researcher", "conference", "product", "organization", "person", "location", "task", "field", "programming language", "metric", "country"], "instance": {"id": "178", "words": ["Popular", "speech", "recognition", "conferences", "held", "each", "year", "or", "two", "include", "SpeechTEK", "and", "SpeechTEK", "Europe", ",", "ICASSP", ",", "Interspeech", "/", "Eurospeech", ",", "and", "the", "IEEE", "ASRU", "."], "labels": ["O", "B-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "B-conference", "O", "B-conference", "I-conference", "O", "B-conference", "O", "B-conference", "O", "B-conference", "O", "O", "O", "B-conference", "I-conference", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, university, researcher, conference, product, organization, person, location, task, field, programming language, metric, country and O.\nSentence: Popular speech recognition conferences held each year or two include SpeechTEK and SpeechTEK Europe , ICASSP , Interspeech / Eurospeech , and the IEEE ASRU .", "prompt_labels": "Popular(O) speech(B-task) recognition(I-task) conferences(O) held(O) each(O) year(O) or(O) two(O) include(O) SpeechTEK(B-conference) and(O) SpeechTEK(B-conference) Europe(I-conference) ,(O) ICASSP(B-conference) ,(O) Interspeech(B-conference) /(O) Eurospeech(B-conference) ,(O) and(O) the(O) IEEE(B-conference) ASRU(I-conference) .(O)"}}
{"id": "368", "dataset": "crossner_ai", "split": "test", "label_list": ["location", "university", "country", "programming language", "algorithm", "conference", "product", "task", "organization", "researcher", "field", "metric", "person"], "instance": {"id": "368", "words": ["An", "eigenface", "(", "The", "approach", "of", "using", "eigenfaces", "for", "Facial", "recognition", "system", "was", "developed", "by", "Sirovich", "and", "Kirby", "(", "1987", ")", "and", "used", "by", "Matthew", "Turk", "and", "Alex", "Pentland", "in", "face", "classification", ".", "Turk", ",", "Matthew", "A", "and", "Pentland", ",", "Alex", "P.", "Face", "recognition", "using", "eigenfaces", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "O", "O", "O", "B-researcher", "O", "B-researcher", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-task", "I-task", "O", "B-researcher", "I-researcher", "I-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "I-researcher", "I-researcher", "B-task", "I-task", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, university, country, programming language, algorithm, conference, product, task, organization, researcher, field, metric, person and O.\nSentence: An eigenface ( The approach of using eigenfaces for Facial recognition system was developed by Sirovich and Kirby ( 1987 ) and used by Matthew Turk and Alex Pentland in face classification . Turk , Matthew A and Pentland , Alex P. Face recognition using eigenfaces .", "prompt_labels": "An(O) eigenface(O) ((O) The(O) approach(O) of(O) using(O) eigenfaces(O) for(O) Facial(B-product) recognition(I-product) system(I-product) was(O) developed(O) by(O) Sirovich(B-researcher) and(O) Kirby(B-researcher) ((O) 1987(O) )(O) and(O) used(O) by(O) Matthew(B-researcher) Turk(I-researcher) and(O) Alex(B-researcher) Pentland(I-researcher) in(O) face(B-task) classification(I-task) .(O) Turk(B-researcher) ,(I-researcher) Matthew(I-researcher) A(I-researcher) and(O) Pentland(B-researcher) ,(I-researcher) Alex(I-researcher) P.(I-researcher) Face(B-task) recognition(I-task) using(O) eigenfaces(O) .(O)"}}
{"id": "363", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "metric", "programming language", "product", "field", "person", "conference", "researcher", "task", "organization", "location", "country", "university"], "instance": {"id": "363", "words": ["With", "the", "help", "of", "advanced", "AR", "technologies", "(", "e.g.", "adding", "computer", "vision", ",", "incorporating", "AR", "cameras", "into", "smartphone", "and", "object", "recognition", ")", "the", "information", "about", "the", "surrounding", "real", "world", "of", "the", "user", "becomes", "interactive", "and", "digitally", "manipulated", "."], "labels": ["O", "O", "O", "O", "O", "B-field", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "B-field", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, metric, programming language, product, field, person, conference, researcher, task, organization, location, country, university and O.\nSentence: With the help of advanced AR technologies ( e.g. adding computer vision , incorporating AR cameras into smartphone and object recognition ) the information about the surrounding real world of the user becomes interactive and digitally manipulated .", "prompt_labels": "With(O) the(O) help(O) of(O) advanced(O) AR(B-field) technologies(O) ((O) e.g.(O) adding(O) computer(B-field) vision(I-field) ,(O) incorporating(O) AR(B-field) cameras(O) into(O) smartphone(O) and(O) object(B-task) recognition(I-task) )(O) the(O) information(O) about(O) the(O) surrounding(O) real(O) world(O) of(O) the(O) user(O) becomes(O) interactive(O) and(O) digitally(O) manipulated(O) .(O)"}}
{"id": "177", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "task", "location", "metric", "country", "algorithm", "field", "programming language", "researcher", "organization", "product", "person", "conference"], "instance": {"id": "177", "words": ["Sensitivity", "or", "TRUE", "Positive", "Rate", "(", "TPR", ")", ",", "also", "known", "as", "recall", ",", "is", "the", "proportion", "of", "people", "that", "tested", "positive", "and", "are", "positive", "(", "TRUE", "Positive", ",", "TP", ")", "of", "all", "the", "people", "that", "actually", "are", "positive", "(", "Condition", "Positive", ",", "CP", "=", "TP", "+", "FN", ")", "."], "labels": ["B-metric", "O", "B-metric", "I-metric", "I-metric", "O", "B-metric", "O", "O", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O", "B-metric", "O", "B-metric", "I-metric", "I-metric", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, task, location, metric, country, algorithm, field, programming language, researcher, organization, product, person, conference and O.\nSentence: Sensitivity or TRUE Positive Rate ( TPR ) , also known as recall , is the proportion of people that tested positive and are positive ( TRUE Positive , TP ) of all the people that actually are positive ( Condition Positive , CP = TP + FN ) .", "prompt_labels": "Sensitivity(B-metric) or(O) TRUE(B-metric) Positive(I-metric) Rate(I-metric) ((O) TPR(B-metric) )(O) ,(O) also(O) known(O) as(O) recall(B-metric) ,(O) is(O) the(O) proportion(O) of(O) people(O) that(O) tested(O) positive(O) and(O) are(O) positive(O) ((O) TRUE(B-metric) Positive(I-metric) ,(O) TP(B-metric) )(O) of(O) all(O) the(O) people(O) that(O) actually(O) are(O) positive(O) ((O) Condition(B-metric) Positive(I-metric) ,(O) CP(B-metric) =(O) TP(B-metric) +(I-metric) FN(I-metric) )(O) .(O)"}}
{"id": "331", "dataset": "crossner_ai", "split": "test", "label_list": ["person", "product", "programming language", "university", "country", "conference", "task", "location", "researcher", "field", "algorithm", "metric", "organization"], "instance": {"id": "331", "words": ["In", "1984", "he", "moved", "to", "the", "University", "of", "Konstanz", "and", "in", "1990", "to", "the", "University", "of", "Salzburg", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, product, programming language, university, country, conference, task, location, researcher, field, algorithm, metric, organization and O.\nSentence: In 1984 he moved to the University of Konstanz and in 1990 to the University of Salzburg .", "prompt_labels": "In(O) 1984(O) he(O) moved(O) to(O) the(O) University(B-university) of(I-university) Konstanz(I-university) and(O) in(O) 1990(O) to(O) the(O) University(B-university) of(I-university) Salzburg(I-university) .(O)"}}
{"id": "104", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "researcher", "task", "programming language", "algorithm", "metric", "conference", "country", "person", "product", "location", "organization", "field"], "instance": {"id": "104", "words": [",", "Ltd.", "in", "Thailand", ";", "Komatsu", "(", "Shanghai", ")", "Ltd.", "in", "1996", "in", "Shanghai", ",", "China", ";", "Industrial", "Power", "Alliance", "Ltd.", "in", "Japan", ",", "a", "joint", "venture", "with", "Cummins", ",", "in", "1998", ";", "L", "&", "T-Komatsu", "Limited", "in", "India", "in", "1998", "(", "shares", "sold", "in", "2013", ")", ";", "and", "Komatsu", "Brasil", "International", "Ltda.", "in", "Brazil", "in", "1998", "."], "labels": ["O", "O", "O", "B-country", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-location", "O", "B-country", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-country", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-country", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, researcher, task, programming language, algorithm, metric, conference, country, person, product, location, organization, field and O.\nSentence: , Ltd. in Thailand ; Komatsu ( Shanghai ) Ltd. in 1996 in Shanghai , China ; Industrial Power Alliance Ltd. in Japan , a joint venture with Cummins , in 1998 ; L & T-Komatsu Limited in India in 1998 ( shares sold in 2013 ) ; and Komatsu Brasil International Ltda. in Brazil in 1998 .", "prompt_labels": ",(O) Ltd.(O) in(O) Thailand(B-country) ;(O) Komatsu(B-organization) ((I-organization) Shanghai(I-organization) )(I-organization) Ltd.(I-organization) in(O) 1996(O) in(O) Shanghai(B-location) ,(O) China(B-country) ;(O) Industrial(B-organization) Power(I-organization) Alliance(I-organization) Ltd.(I-organization) in(O) Japan(B-country) ,(O) a(O) joint(O) venture(O) with(O) Cummins(B-organization) ,(O) in(O) 1998(O) ;(O) L(B-organization) &(I-organization) T-Komatsu(I-organization) Limited(I-organization) in(O) India(B-country) in(O) 1998(O) ((O) shares(O) sold(O) in(O) 2013(O) )(O) ;(O) and(O) Komatsu(B-organization) Brasil(I-organization) International(I-organization) Ltda.(I-organization) in(O) Brazil(B-country) in(O) 1998(O) .(O)"}}
{"id": "95", "dataset": "crossner_ai", "split": "test", "label_list": ["country", "metric", "location", "university", "algorithm", "researcher", "programming language", "organization", "conference", "field", "task", "product", "person"], "instance": {"id": "95", "words": ["After", "boosting", ",", "a", "classifier", "constructed", "from", "200", "features", "could", "yield", "a", "95", "%", "detection", "rate", "under", "a", "^", "{", "-5", "}", "/", "math", "FALSE", "positive", "rate", ".P.", "Viola", ",", "M.", "Jones", ",", "Robust", "Real-time", "Object", "Detection", ",", "2001", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-task", "I-task", "I-task", "I-task", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, metric, location, university, algorithm, researcher, programming language, organization, conference, field, task, product, person and O.\nSentence: After boosting , a classifier constructed from 200 features could yield a 95 % detection rate under a ^ { -5 } / math FALSE positive rate .P. Viola , M. Jones , Robust Real-time Object Detection , 2001 .", "prompt_labels": "After(O) boosting(O) ,(O) a(O) classifier(O) constructed(O) from(O) 200(O) features(O) could(O) yield(O) a(O) 95(O) %(O) detection(O) rate(O) under(O) a(O) ^(O) {(O) -5(O) }(O) /(O) math(O) FALSE(B-metric) positive(I-metric) rate(I-metric) .P.(B-researcher) Viola(I-researcher) ,(O) M.(B-researcher) Jones(I-researcher) ,(O) Robust(B-task) Real-time(I-task) Object(I-task) Detection(I-task) ,(O) 2001(O) .(O)"}}
{"id": "183", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "researcher", "university", "location", "country", "programming language", "field", "conference", "organization", "product", "metric", "task", "person"], "instance": {"id": "183", "words": ["As", "in", "factor", "analysis", ",", "the", "LCA", "can", "also", "be", "used", "to", "classify", "case", "according", "to", "their", "maximum", "likelihood", "class", "membership", "."], "labels": ["O", "O", "B-task", "I-task", "O", "O", "B-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, researcher, university, location, country, programming language, field, conference, organization, product, metric, task, person and O.\nSentence: As in factor analysis , the LCA can also be used to classify case according to their maximum likelihood class membership .", "prompt_labels": "As(O) in(O) factor(B-task) analysis(I-task) ,(O) the(O) LCA(B-algorithm) can(O) also(O) be(O) used(O) to(O) classify(O) case(O) according(O) to(O) their(O) maximum(B-algorithm) likelihood(I-algorithm) class(O) membership(O) .(O)"}}
{"id": "39", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "programming language", "algorithm", "person", "conference", "location", "university", "researcher", "country", "organization", "product", "field", "task"], "instance": {"id": "39", "words": ["The", "2009", "Loebner", "Prize", "Competition", "was", "held", "September", "6", ",", "2009", "at", "the", "Brighton", "Centre", ",", "Brighton", "UK", "in", "conjunction", "with", "the", "Interspeech", "2009", "conference", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "B-country", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, programming language, algorithm, person, conference, location, university, researcher, country, organization, product, field, task and O.\nSentence: The 2009 Loebner Prize Competition was held September 6 , 2009 at the Brighton Centre , Brighton UK in conjunction with the Interspeech 2009 conference .", "prompt_labels": "The(O) 2009(O) Loebner(O) Prize(O) Competition(O) was(O) held(O) September(O) 6(O) ,(O) 2009(O) at(O) the(O) Brighton(B-location) Centre(I-location) ,(O) Brighton(B-location) UK(B-country) in(O) conjunction(O) with(O) the(O) Interspeech(B-conference) 2009(I-conference) conference(I-conference) .(O)"}}
{"id": "372", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "conference", "programming language", "field", "product", "task", "algorithm", "researcher", "person", "country", "organization", "location", "metric"], "instance": {"id": "372", "words": ["In", "that", "page", ",", "Samurai", "Damashii", "exaggerated", "the", "Senkousha", "as", "the", "crystallization", "of", "China", "'s", "four", "thousand", "years", "of", "scientific", "knowledge", ",", "commented", "on", "the", "crude", "design", "(", "e.g.", "the", "Chinese", "Cannon", "on", "its", "crotch", ")", ",", "and", "put", "its", "image", "among", "images", "of", "Honda", "'", "s", "ASIMO", "and", "Sony", "'", "s", "QRIO", "SDR-3X", "for", "juxtaposition", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "B-product", "O", "B-organization", "O", "O", "B-product", "I-product", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, conference, programming language, field, product, task, algorithm, researcher, person, country, organization, location, metric and O.\nSentence: In that page , Samurai Damashii exaggerated the Senkousha as the crystallization of China 's four thousand years of scientific knowledge , commented on the crude design ( e.g. the Chinese Cannon on its crotch ) , and put its image among images of Honda ' s ASIMO and Sony ' s QRIO SDR-3X for juxtaposition .", "prompt_labels": "In(O) that(O) page(O) ,(O) Samurai(O) Damashii(O) exaggerated(O) the(O) Senkousha(B-product) as(O) the(O) crystallization(O) of(O) China(B-country) 's(O) four(O) thousand(O) years(O) of(O) scientific(O) knowledge(O) ,(O) commented(O) on(O) the(O) crude(O) design(O) ((O) e.g.(O) the(O) Chinese(O) Cannon(O) on(O) its(O) crotch(O) )(O) ,(O) and(O) put(O) its(O) image(O) among(O) images(O) of(O) Honda(B-organization) '(O) s(O) ASIMO(B-product) and(O) Sony(B-organization) '(O) s(O) QRIO(B-product) SDR-3X(I-product) for(O) juxtaposition(O) .(O)"}}
{"id": "255", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "researcher", "programming language", "metric", "product", "algorithm", "organization", "person", "task", "country", "location", "field", "university"], "instance": {"id": "255", "words": ["In", "artificial", "intelligence", ",", "Marvin", "Minsky", ",", "Herbert", "A.", "Simon", ",", "and", "Allen", "Newell", "are", "prominent", "."], "labels": ["O", "B-field", "I-field", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "I-researcher", "O", "O", "B-researcher", "I-researcher", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, researcher, programming language, metric, product, algorithm, organization, person, task, country, location, field, university and O.\nSentence: In artificial intelligence , Marvin Minsky , Herbert A. Simon , and Allen Newell are prominent .", "prompt_labels": "In(O) artificial(B-field) intelligence(I-field) ,(O) Marvin(B-researcher) Minsky(I-researcher) ,(O) Herbert(B-researcher) A.(I-researcher) Simon(I-researcher) ,(O) and(O) Allen(B-researcher) Newell(I-researcher) are(O) prominent(O) .(O)"}}
{"id": "176", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "metric", "field", "country", "researcher", "task", "person", "conference", "algorithm", "programming language", "product", "organization", "location"], "instance": {"id": "176", "words": ["It", "is", "commonly", "used", "to", "generate", "representations", "for", "speech", "recognition", "(", "ASR", ")", ",", "e.g.", "the", "CMU", "Sphinx", "system", ",", "and", "speech", "synthesis", "(", "TTS", ")", ",", "e.g.", "the", "Festival", "system", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "B-task", "O", "O", "O", "O", "B-product", "I-product", "I-product", "O", "O", "B-task", "I-task", "O", "B-task", "O", "O", "O", "O", "B-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, metric, field, country, researcher, task, person, conference, algorithm, programming language, product, organization, location and O.\nSentence: It is commonly used to generate representations for speech recognition ( ASR ) , e.g. the CMU Sphinx system , and speech synthesis ( TTS ) , e.g. the Festival system .", "prompt_labels": "It(O) is(O) commonly(O) used(O) to(O) generate(O) representations(O) for(O) speech(B-task) recognition(I-task) ((O) ASR(B-task) )(O) ,(O) e.g.(O) the(O) CMU(B-product) Sphinx(I-product) system(I-product) ,(O) and(O) speech(B-task) synthesis(I-task) ((O) TTS(B-task) )(O) ,(O) e.g.(O) the(O) Festival(B-product) system(I-product) .(O)"}}
{"id": "87", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "person", "field", "task", "algorithm", "country", "researcher", "university", "conference", "location", "organization", "programming language", "metric"], "instance": {"id": "87", "words": ["The", "theory", "is", "based", "in", "philosophical", "foundations", ",", "and", "was", "founded", "by", "Ray", "Solomonoff", "around", "1960", ".", "Samuel", "Rathmanner", "and", "Marcus", "Hutter", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, person, field, task, algorithm, country, researcher, university, conference, location, organization, programming language, metric and O.\nSentence: The theory is based in philosophical foundations , and was founded by Ray Solomonoff around 1960 . Samuel Rathmanner and Marcus Hutter .", "prompt_labels": "The(O) theory(O) is(O) based(O) in(O) philosophical(O) foundations(O) ,(O) and(O) was(O) founded(O) by(O) Ray(B-researcher) Solomonoff(I-researcher) around(O) 1960(O) .(O) Samuel(B-researcher) Rathmanner(I-researcher) and(O) Marcus(B-researcher) Hutter(I-researcher) .(O)"}}
{"id": "2", "dataset": "crossner_ai", "split": "test", "label_list": ["task", "organization", "field", "country", "conference", "researcher", "person", "product", "metric", "location", "programming language", "university", "algorithm"], "instance": {"id": "2", "words": ["The", "task", "is", "usually", "to", "derive", "the", "maximum", "likelihood", "estimate", "of", "the", "parameters", "of", "the", "HMM", "given", "the", "of", "output", "sequences", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "B-algorithm", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, organization, field, country, conference, researcher, person, product, metric, location, programming language, university, algorithm and O.\nSentence: The task is usually to derive the maximum likelihood estimate of the parameters of the HMM given the of output sequences .", "prompt_labels": "The(O) task(O) is(O) usually(O) to(O) derive(O) the(O) maximum(B-algorithm) likelihood(I-algorithm) estimate(I-algorithm) of(O) the(O) parameters(O) of(O) the(O) HMM(B-algorithm) given(O) the(O) of(O) output(O) sequences(O) .(O)"}}
{"id": "399", "dataset": "crossner_ai", "split": "test", "label_list": ["task", "programming language", "university", "organization", "conference", "person", "algorithm", "country", "metric", "field", "researcher", "product", "location"], "instance": {"id": "399", "words": ["This", "is", "a", "particular", "way", "of", "implementing", "maximum", "likelihood", "estimation", "for", "this", "problem", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, programming language, university, organization, conference, person, algorithm, country, metric, field, researcher, product, location and O.\nSentence: This is a particular way of implementing maximum likelihood estimation for this problem .", "prompt_labels": "This(O) is(O) a(O) particular(O) way(O) of(O) implementing(O) maximum(B-metric) likelihood(I-metric) estimation(I-metric) for(O) this(O) problem(O) .(O)"}}
{"id": "246", "dataset": "crossner_ai", "split": "test", "label_list": ["location", "country", "researcher", "conference", "metric", "field", "task", "algorithm", "university", "organization", "programming language", "person", "product"], "instance": {"id": "246", "words": ["The", "incident", "strained", "relations", "between", "the", "United", "States", "and", "Japan", ",", "and", "resulted", "in", "the", "arrest", "and", "prosecution", "two", "senior", "executives", ",", "as", "well", "as", "the", "imposition", "of", "sanctions", "on", "the", "company", "by", "both", "countries", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, researcher, conference, metric, field, task, algorithm, university, organization, programming language, person, product and O.\nSentence: The incident strained relations between the United States and Japan , and resulted in the arrest and prosecution two senior executives , as well as the imposition of sanctions on the company by both countries .", "prompt_labels": "The(O) incident(O) strained(O) relations(O) between(O) the(O) United(B-country) States(I-country) and(O) Japan(B-country) ,(O) and(O) resulted(O) in(O) the(O) arrest(O) and(O) prosecution(O) two(O) senior(O) executives(O) ,(O) as(O) well(O) as(O) the(O) imposition(O) of(O) sanctions(O) on(O) the(O) company(O) by(O) both(O) countries(O) .(O)"}}
{"id": "152", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "programming language", "location", "person", "task", "field", "university", "product", "researcher", "algorithm", "country", "metric", "organization"], "instance": {"id": "152", "words": ["Hidden", "Markov", "models", "are", "known", "for", "their", "applications", "to", "reinforcement", "learning", "and", "temporal", "pattern", "recognition", "such", "as", "speech", ",", "handwriting", "recognition", ",", "gesture", "recognition", ",", "Thad", "Starner", ",", "Alex", "Pentland", "."], "labels": ["B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "I-field", "O", "O", "B-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, programming language, location, person, task, field, university, product, researcher, algorithm, country, metric, organization and O.\nSentence: Hidden Markov models are known for their applications to reinforcement learning and temporal pattern recognition such as speech , handwriting recognition , gesture recognition , Thad Starner , Alex Pentland .", "prompt_labels": "Hidden(B-algorithm) Markov(I-algorithm) models(I-algorithm) are(O) known(O) for(O) their(O) applications(O) to(O) reinforcement(B-field) learning(I-field) and(O) temporal(B-field) pattern(I-field) recognition(I-field) such(O) as(O) speech(B-task) ,(O) handwriting(B-task) recognition(I-task) ,(O) gesture(B-task) recognition(I-task) ,(O) Thad(B-researcher) Starner(I-researcher) ,(O) Alex(B-researcher) Pentland(I-researcher) .(O)"}}
{"id": "77", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "programming language", "metric", "location", "researcher", "task", "product", "person", "organization", "conference", "field", "country", "algorithm"], "instance": {"id": "77", "words": ["In", "statistics", ",", "an", "expectation-maximization", "(", "EM", ")", "algorithm", "is", "an", "iterative", "method", "to", "find", "maximum", "likelihood", "or", "maximum", "a", "posteriori", "(", "MAP", ")", "estimates", "of", "parameter", "s", "in", "statistical", "model", "s", ",", "where", "the", "model", "depends", "on", "unobserved", "latent", "variable", "s", "."], "labels": ["O", "B-field", "O", "O", "B-algorithm", "O", "B-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O", "B-metric", "I-metric", "I-metric", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, programming language, metric, location, researcher, task, product, person, organization, conference, field, country, algorithm and O.\nSentence: In statistics , an expectation-maximization ( EM ) algorithm is an iterative method to find maximum likelihood or maximum a posteriori ( MAP ) estimates of parameter s in statistical model s , where the model depends on unobserved latent variable s .", "prompt_labels": "In(O) statistics(B-field) ,(O) an(O) expectation-maximization(B-algorithm) ((O) EM(B-algorithm) )(O) algorithm(O) is(O) an(O) iterative(O) method(O) to(O) find(O) maximum(B-metric) likelihood(I-metric) or(O) maximum(B-metric) a(I-metric) posteriori(I-metric) ((O) MAP(B-metric) )(O) estimates(O) of(O) parameter(O) s(O) in(O) statistical(O) model(O) s(O) ,(O) where(O) the(O) model(O) depends(O) on(O) unobserved(O) latent(O) variable(O) s(O) .(O)"}}
{"id": "328", "dataset": "crossner_ai", "split": "test", "label_list": ["researcher", "programming language", "university", "organization", "algorithm", "conference", "person", "task", "product", "field", "location", "metric", "country"], "instance": {"id": "328", "words": ["Dr.", "Julesz", "emigrated", "from", "Hungary", "to", "the", "United", "States", "following", "the", "1956", "Soviet", "invasion", "."], "labels": ["B-researcher", "I-researcher", "O", "O", "B-country", "O", "B-country", "I-country", "I-country", "O", "O", "O", "B-country", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, programming language, university, organization, algorithm, conference, person, task, product, field, location, metric, country and O.\nSentence: Dr. Julesz emigrated from Hungary to the United States following the 1956 Soviet invasion .", "prompt_labels": "Dr.(B-researcher) Julesz(I-researcher) emigrated(O) from(O) Hungary(B-country) to(O) the(B-country) United(I-country) States(I-country) following(O) the(O) 1956(O) Soviet(B-country) invasion(O) .(O)"}}
{"id": "291", "dataset": "crossner_ai", "split": "test", "label_list": ["location", "task", "programming language", "field", "person", "university", "researcher", "country", "product", "organization", "metric", "conference", "algorithm"], "instance": {"id": "291", "words": ["In", "other", "words", ",", "the", "sample", "mean", "is", "the", "(", "necessarily", "unique", ")", "efficient", "estimator", ",", "and", "thus", "also", "the", "minimum", "variance", "unbiased", "estimator", "(", "MVUE", ")", ",", "in", "addition", "to", "being", "the", "maximum", "likelihood", "estimator", "."], "labels": ["O", "O", "O", "O", "O", "B-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "I-metric", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, task, programming language, field, person, university, researcher, country, product, organization, metric, conference, algorithm and O.\nSentence: In other words , the sample mean is the ( necessarily unique ) efficient estimator , and thus also the minimum variance unbiased estimator ( MVUE ) , in addition to being the maximum likelihood estimator .", "prompt_labels": "In(O) other(O) words(O) ,(O) the(O) sample(B-metric) mean(I-metric) is(O) the(O) ((O) necessarily(O) unique(O) )(O) efficient(O) estimator(O) ,(O) and(O) thus(O) also(O) the(O) minimum(B-metric) variance(I-metric) unbiased(I-metric) estimator(I-metric) ((O) MVUE(B-metric) )(O) ,(O) in(O) addition(O) to(O) being(O) the(O) maximum(B-metric) likelihood(I-metric) estimator(I-metric) .(O)"}}
{"id": "334", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "programming language", "researcher", "location", "conference", "person", "product", "task", "algorithm", "country", "field", "university", "metric"], "instance": {"id": "334", "words": ["Industrial", "robots", "have", "been", "implemented", "to", "collaborate", "with", "humans", "to", "perform", "industrial", "manufacturing", "tasks", "."], "labels": ["B-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, programming language, researcher, location, conference, person, product, task, algorithm, country, field, university, metric and O.\nSentence: Industrial robots have been implemented to collaborate with humans to perform industrial manufacturing tasks .", "prompt_labels": "Industrial(B-product) robots(I-product) have(O) been(O) implemented(O) to(O) collaborate(O) with(O) humans(O) to(O) perform(O) industrial(O) manufacturing(O) tasks(O) .(O)"}}
{"id": "415", "dataset": "crossner_ai", "split": "test", "label_list": ["person", "location", "task", "metric", "university", "organization", "algorithm", "programming language", "product", "field", "conference", "country", "researcher"], "instance": {"id": "415", "words": ["The", "actual", "data", "mining", "task", "is", "the", "semi-automatic", "or", "automatic", "analysis", "of", "large", "quantities", "of", "data", "to", "extract", "unknown", ",", "interesting", "patterns", "such", "as", "groups", "of", "data", "records", "(", "cluster", "analysis", ")", ",", "unusual", "records", "(", "anomaly", "detection", ")", ",", "and", "dependencies", "(", "association", "rule", "mining", ",", "sequential", "pattern", "mining", ")", "."], "labels": ["O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "B-task", "I-task", "I-task", "O", "B-task", "I-task", "I-task", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, task, metric, university, organization, algorithm, programming language, product, field, conference, country, researcher and O.\nSentence: The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract unknown , interesting patterns such as groups of data records ( cluster analysis ) , unusual records ( anomaly detection ) , and dependencies ( association rule mining , sequential pattern mining ) .", "prompt_labels": "The(O) actual(O) data(B-field) mining(I-field) task(O) is(O) the(O) semi-automatic(O) or(O) automatic(O) analysis(O) of(O) large(O) quantities(O) of(O) data(O) to(O) extract(O) unknown(O) ,(O) interesting(O) patterns(O) such(O) as(O) groups(O) of(O) data(O) records(O) ((O) cluster(B-task) analysis(I-task) )(O) ,(O) unusual(O) records(O) ((O) anomaly(B-task) detection(I-task) )(O) ,(O) and(O) dependencies(O) ((O) association(B-task) rule(I-task) mining(I-task) ,(O) sequential(B-task) pattern(I-task) mining(I-task) )(O) .(O)"}}
{"id": "26", "dataset": "crossner_ai", "split": "test", "label_list": ["location", "metric", "conference", "field", "organization", "person", "country", "researcher", "task", "algorithm", "university", "programming language", "product"], "instance": {"id": "26", "words": ["It", "was", "during", "this", "time", "that", "a", "total", "of", "43", "publications", "were", "recognized", "by", "the", "CVPR", "and", "the", "International", "Conference", "on", "Computer", "Vision", "(", "ICCV", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, metric, conference, field, organization, person, country, researcher, task, algorithm, university, programming language, product and O.\nSentence: It was during this time that a total of 43 publications were recognized by the CVPR and the International Conference on Computer Vision ( ICCV ) .", "prompt_labels": "It(O) was(O) during(O) this(O) time(O) that(O) a(O) total(O) of(O) 43(O) publications(O) were(O) recognized(O) by(O) the(O) CVPR(B-conference) and(O) the(O) International(B-conference) Conference(I-conference) on(I-conference) Computer(I-conference) Vision(I-conference) ((O) ICCV(B-conference) )(O) .(O)"}}
{"id": "76", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "field", "university", "product", "algorithm", "conference", "location", "metric", "programming language", "task", "researcher", "person", "country"], "instance": {"id": "76", "words": ["Scientific", "conferences", "where", "vision", "based", "activity", "recognition", "work", "often", "appears", "are", "ICCV", "and", "CVPR", "."], "labels": ["O", "O", "O", "B-task", "I-task", "I-task", "I-task", "O", "O", "O", "O", "B-conference", "O", "B-conference", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, field, university, product, algorithm, conference, location, metric, programming language, task, researcher, person, country and O.\nSentence: Scientific conferences where vision based activity recognition work often appears are ICCV and CVPR .", "prompt_labels": "Scientific(O) conferences(O) where(O) vision(B-task) based(I-task) activity(I-task) recognition(I-task) work(O) often(O) appears(O) are(O) ICCV(B-conference) and(O) CVPR(B-conference) .(O)"}}
{"id": "204", "dataset": "crossner_ai", "split": "test", "label_list": ["programming language", "researcher", "task", "field", "country", "product", "organization", "university", "location", "algorithm", "metric", "person", "conference"], "instance": {"id": "204", "words": ["Both", "rely", "on", "speech", "act", "theory", "developed", "by", "John", "Searle", "in", "the", "1960s", "and", "enhanced", "by", "Terry", "Winograd", "and", "Flores", "in", "the", "1970s", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, researcher, task, field, country, product, organization, university, location, algorithm, metric, person, conference and O.\nSentence: Both rely on speech act theory developed by John Searle in the 1960s and enhanced by Terry Winograd and Flores in the 1970s .", "prompt_labels": "Both(O) rely(O) on(O) speech(O) act(O) theory(O) developed(O) by(O) John(B-researcher) Searle(I-researcher) in(O) the(O) 1960s(O) and(O) enhanced(O) by(O) Terry(B-researcher) Winograd(I-researcher) and(O) Flores(B-researcher) in(O) the(O) 1970s(O) .(O)"}}
{"id": "78", "dataset": "crossner_ai", "split": "test", "label_list": ["location", "metric", "product", "university", "field", "person", "task", "programming language", "researcher", "conference", "country", "organization", "algorithm"], "instance": {"id": "78", "words": ["Similarly", ",", "investigators", "sometimes", "report", "the", "FALSE", "Positive", "Rate", "(", "FPR", ")", "as", "well", "as", "the", "FALSE", "Negative", "Rate", "(", "FNR", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "B-metric", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "B-metric", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, metric, product, university, field, person, task, programming language, researcher, conference, country, organization, algorithm and O.\nSentence: Similarly , investigators sometimes report the FALSE Positive Rate ( FPR ) as well as the FALSE Negative Rate ( FNR ) .", "prompt_labels": "Similarly(O) ,(O) investigators(O) sometimes(O) report(O) the(O) FALSE(B-metric) Positive(I-metric) Rate(I-metric) ((O) FPR(B-metric) )(O) as(O) well(O) as(O) the(O) FALSE(B-metric) Negative(I-metric) Rate(I-metric) ((O) FNR(B-metric) )(O) .(O)"}}
{"id": "195", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "programming language", "metric", "algorithm", "researcher", "task", "country", "conference", "university", "product", "person", "organization", "location"], "instance": {"id": "195", "words": ["As", "of", "2017", ",", "he", "is", "a", "professor", "at", "the", "Collège", "de", "France", "and", ",", "since", "1989", ",", "the", "director", "of", "INSERM", "Unit", "562", ",", "Cognitive", "Neuroimaging", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, programming language, metric, algorithm, researcher, task, country, conference, university, product, person, organization, location and O.\nSentence: As of 2017 , he is a professor at the Collège de France and , since 1989 , the director of INSERM Unit 562 , Cognitive Neuroimaging .", "prompt_labels": "As(O) of(O) 2017(O) ,(O) he(O) is(O) a(O) professor(O) at(O) the(O) Collège(B-university) de(I-university) France(I-university) and(O) ,(O) since(O) 1989(O) ,(O) the(O) director(O) of(O) INSERM(B-organization) Unit(I-organization) 562(I-organization) ,(O) Cognitive(B-field) Neuroimaging(I-field) .(O)"}}
{"id": "84", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "field", "person", "algorithm", "country", "conference", "researcher", "programming language", "task", "location", "organization", "product", "university"], "instance": {"id": "84", "words": ["The", "WaveNet", "model", "proposed", "in", "2016", "achieves", "great", "performance", "on", "speech", "quality", "."], "labels": ["O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, field, person, algorithm, country, conference, researcher, programming language, task, location, organization, product, university and O.\nSentence: The WaveNet model proposed in 2016 achieves great performance on speech quality .", "prompt_labels": "The(O) WaveNet(B-product) model(O) proposed(O) in(O) 2016(O) achieves(O) great(O) performance(O) on(O) speech(O) quality(O) .(O)"}}
{"id": "259", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "conference", "task", "country", "field", "person", "university", "algorithm", "location", "metric", "organization", "researcher", "programming language"], "instance": {"id": "259", "words": ["In", "this", "approach", ",", "models", "are", "developed", "using", "different", "data", "mining", ",", "machine", "learning", "algorithms", "to", "predict", "users", "'", "rating", "of", "unrated", "items", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, conference, task, country, field, person, university, algorithm, location, metric, organization, researcher, programming language and O.\nSentence: In this approach , models are developed using different data mining , machine learning algorithms to predict users ' rating of unrated items .", "prompt_labels": "In(O) this(O) approach(O) ,(O) models(O) are(O) developed(O) using(O) different(O) data(B-field) mining(I-field) ,(O) machine(B-field) learning(I-field) algorithms(O) to(O) predict(O) users(O) '(O) rating(O) of(O) unrated(O) items(O) .(O)"}}
{"id": "402", "dataset": "crossner_ai", "split": "test", "label_list": ["country", "algorithm", "field", "metric", "university", "organization", "programming language", "task", "location", "person", "researcher", "conference", "product"], "instance": {"id": "402", "words": ["The", "encoder", "and", "decoder", "are", "trained", "to", "take", "a", "phrase", "and", "reproduce", "the", "one-hot", "distribution", "of", "a", "corresponding", "paraphrase", "by", "minimizing", "perplexity", "using", "simple", "stochastic", "gradient", "descent", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, algorithm, field, metric, university, organization, programming language, task, location, person, researcher, conference, product and O.\nSentence: The encoder and decoder are trained to take a phrase and reproduce the one-hot distribution of a corresponding paraphrase by minimizing perplexity using simple stochastic gradient descent .", "prompt_labels": "The(O) encoder(O) and(O) decoder(O) are(O) trained(O) to(O) take(O) a(O) phrase(O) and(O) reproduce(O) the(O) one-hot(O) distribution(O) of(O) a(O) corresponding(O) paraphrase(O) by(O) minimizing(O) perplexity(B-metric) using(O) simple(O) stochastic(B-algorithm) gradient(I-algorithm) descent(I-algorithm) .(O)"}}
{"id": "92", "dataset": "crossner_ai", "split": "test", "label_list": ["task", "person", "algorithm", "researcher", "location", "programming language", "metric", "conference", "product", "university", "organization", "field", "country"], "instance": {"id": "92", "words": ["Neuroevolution", ",", "or", "neuro-evolution", ",", "is", "a", "form", "of", "artificial", "intelligence", "that", "uses", "evolutionary", "algorithm", "s", "to", "generate", "artificial", "neural", "network", "s", "(", "ANN", ")", ",", "parameters", ",", "topology", "and", "rules.", "and", "evolutionary", "robotics", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "B-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, person, algorithm, researcher, location, programming language, metric, conference, product, university, organization, field, country and O.\nSentence: Neuroevolution , or neuro-evolution , is a form of artificial intelligence that uses evolutionary algorithm s to generate artificial neural network s ( ANN ) , parameters , topology and rules. and evolutionary robotics .", "prompt_labels": "Neuroevolution(O) ,(O) or(O) neuro-evolution(O) ,(O) is(O) a(O) form(O) of(O) artificial(B-field) intelligence(I-field) that(O) uses(O) evolutionary(B-algorithm) algorithm(I-algorithm) s(O) to(O) generate(O) artificial(B-algorithm) neural(I-algorithm) network(I-algorithm) s(O) ((O) ANN(B-algorithm) )(O) ,(O) parameters(O) ,(O) topology(O) and(O) rules.(O) and(O) evolutionary(B-algorithm) robotics(I-algorithm) .(O)"}}
{"id": "173", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "task", "conference", "programming language", "university", "country", "field", "organization", "researcher", "product", "person", "algorithm", "location"], "instance": {"id": "173", "words": ["The", "Association", "for", "Computational", "Linguistics", "defines", "computational", "linguistics", "as", ":"], "labels": ["O", "B-conference", "I-conference", "I-conference", "I-conference", "O", "B-field", "I-field", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, task, conference, programming language, university, country, field, organization, researcher, product, person, algorithm, location and O.\nSentence: The Association for Computational Linguistics defines computational linguistics as :", "prompt_labels": "The(O) Association(B-conference) for(I-conference) Computational(I-conference) Linguistics(I-conference) defines(O) computational(B-field) linguistics(I-field) as(O) :(O)"}}
{"id": "248", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "country", "task", "conference", "field", "metric", "product", "location", "organization", "researcher", "programming language", "person", "university"], "instance": {"id": "248", "words": ["Localized", "versions", "of", "the", "site", "available", "in", "the", "United", "Kingdom", ",", "India", ",", "and", "Australia", "were", "discontinued", "following", "the", "acquisition", "of", "Rotten", "Tomatoes", "by", "Fandango", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "B-country", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, country, task, conference, field, metric, product, location, organization, researcher, programming language, person, university and O.\nSentence: Localized versions of the site available in the United Kingdom , India , and Australia were discontinued following the acquisition of Rotten Tomatoes by Fandango .", "prompt_labels": "Localized(O) versions(O) of(O) the(O) site(O) available(O) in(O) the(O) United(B-country) Kingdom(I-country) ,(O) India(B-country) ,(O) and(O) Australia(B-country) were(O) discontinued(O) following(O) the(O) acquisition(O) of(O) Rotten(B-organization) Tomatoes(I-organization) by(O) Fandango(B-organization) .(O)"}}
{"id": "21", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "university", "conference", "task", "location", "person", "metric", "country", "algorithm", "programming language", "researcher", "product", "organization"], "instance": {"id": "21", "words": ["At", "runtime", ",", "the", "target", "prosody", "of", "a", "sentence", "is", "superimposed", "on", "these", "minimal", "units", "by", "means", "of", "signal", "processing", "techniques", "such", "as", "linear", "predictive", "coding", ",", "PSOLA"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, university, conference, task, location, person, metric, country, algorithm, programming language, researcher, product, organization and O.\nSentence: At runtime , the target prosody of a sentence is superimposed on these minimal units by means of signal processing techniques such as linear predictive coding , PSOLA", "prompt_labels": "At(O) runtime(O) ,(O) the(O) target(O) prosody(O) of(O) a(O) sentence(O) is(O) superimposed(O) on(O) these(O) minimal(O) units(O) by(O) means(O) of(O) signal(B-field) processing(I-field) techniques(O) such(O) as(O) linear(B-algorithm) predictive(I-algorithm) coding(I-algorithm) ,(O) PSOLA(B-algorithm)"}}
{"id": "346", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "location", "country", "algorithm", "organization", "university", "product", "researcher", "task", "programming language", "person", "field", "conference"], "instance": {"id": "346", "words": ["A", "voice-user", "interface", "(", "VUI", ")", "makes", "spoken", "human", "interaction", "with", "computers", "possible", ",", "using", "speech", "recognition", "to", "understand", "spoken", "commands", "and", "Question", "answering", ",", "and", "typically", "text", "to", "speech", "to", "play", "a", "reply", "."], "labels": ["O", "B-product", "I-product", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "B-task", "I-task", "I-task", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, location, country, algorithm, organization, university, product, researcher, task, programming language, person, field, conference and O.\nSentence: A voice-user interface ( VUI ) makes spoken human interaction with computers possible , using speech recognition to understand spoken commands and Question answering , and typically text to speech to play a reply .", "prompt_labels": "A(O) voice-user(B-product) interface(I-product) ((O) VUI(B-product) )(O) makes(O) spoken(O) human(O) interaction(O) with(O) computers(O) possible(O) ,(O) using(O) speech(B-task) recognition(I-task) to(O) understand(O) spoken(O) commands(O) and(O) Question(B-task) answering(I-task) ,(O) and(O) typically(O) text(B-task) to(I-task) speech(I-task) to(O) play(O) a(O) reply(O) .(O)"}}
{"id": "330", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "researcher", "field", "location", "algorithm", "organization", "university", "metric", "programming language", "country", "task", "person", "product"], "instance": {"id": "330", "words": ["These", "probabilities", "are", "used", "to", "determine", "what", "the", "target", "is", "using", "a", "maximum", "likelihood", "decision", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, researcher, field, location, algorithm, organization, university, metric, programming language, country, task, person, product and O.\nSentence: These probabilities are used to determine what the target is using a maximum likelihood decision .", "prompt_labels": "These(O) probabilities(O) are(O) used(O) to(O) determine(O) what(O) the(O) target(O) is(O) using(O) a(O) maximum(B-algorithm) likelihood(I-algorithm) decision(I-algorithm) .(O)"}}
{"id": "295", "dataset": "crossner_ai", "split": "test", "label_list": ["programming language", "task", "person", "product", "location", "researcher", "field", "university", "algorithm", "conference", "metric", "organization", "country"], "instance": {"id": "295", "words": ["Walter", "'s", "work", "inspired", "subsequent", "generations", "of", "robotics", "researchers", "such", "as", "Rodney", "Brooks", ",", "Hans", "Moravec", "and", "Mark", "Tilden", "."], "labels": ["B-researcher", "O", "O", "O", "O", "O", "O", "B-field", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, task, person, product, location, researcher, field, university, algorithm, conference, metric, organization, country and O.\nSentence: Walter 's work inspired subsequent generations of robotics researchers such as Rodney Brooks , Hans Moravec and Mark Tilden .", "prompt_labels": "Walter(B-researcher) 's(O) work(O) inspired(O) subsequent(O) generations(O) of(O) robotics(B-field) researchers(O) such(O) as(O) Rodney(B-researcher) Brooks(I-researcher) ,(O) Hans(B-researcher) Moravec(I-researcher) and(O) Mark(B-researcher) Tilden(I-researcher) .(O)"}}
{"id": "326", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "metric", "researcher", "programming language", "conference", "person", "algorithm", "country", "field", "task", "university", "location", "product"], "instance": {"id": "326", "words": ["Some", "successful", "applications", "of", "deep", "learning", "are", "computer", "vision", "and", "speech", "recognition", ".", "Honglak", "Lee", ",", "Roger", "Grosse", ",", "Rajesh", "Ranganath", ",", "Andrew", "Y.", "Ng", "."], "labels": ["O", "O", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "B-task", "I-task", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "I-researcher", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, metric, researcher, programming language, conference, person, algorithm, country, field, task, university, location, product and O.\nSentence: Some successful applications of deep learning are computer vision and speech recognition . Honglak Lee , Roger Grosse , Rajesh Ranganath , Andrew Y. Ng .", "prompt_labels": "Some(O) successful(O) applications(O) of(O) deep(B-field) learning(I-field) are(O) computer(B-field) vision(I-field) and(O) speech(B-task) recognition(I-task) .(O) Honglak(B-researcher) Lee(I-researcher) ,(O) Roger(B-researcher) Grosse(I-researcher) ,(O) Rajesh(B-researcher) Ranganath(I-researcher) ,(O) Andrew(B-researcher) Y.(I-researcher) Ng(I-researcher) .(O)"}}
{"id": "366", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "researcher", "person", "programming language", "algorithm", "university", "country", "field", "location", "task", "conference", "metric", "organization"], "instance": {"id": "366", "words": ["Bigrams", "are", "used", "in", "most", "successful", "language", "model", "s", "for", "speech", "recognition", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, researcher, person, programming language, algorithm, university, country, field, location, task, conference, metric, organization and O.\nSentence: Bigrams are used in most successful language model s for speech recognition .", "prompt_labels": "Bigrams(O) are(O) used(O) in(O) most(O) successful(O) language(B-algorithm) model(I-algorithm) s(O) for(O) speech(B-task) recognition(I-task) .(O)"}}
{"id": "418", "dataset": "crossner_ai", "split": "test", "label_list": ["person", "algorithm", "university", "task", "country", "conference", "field", "programming language", "metric", "researcher", "product", "location", "organization"], "instance": {"id": "418", "words": ["The", "four", "outcomes", "can", "be", "formulated", "in", "a", "2", "×", "2", "contingency", "table", "or", "confusion", "matrix", ",", "as", "follows", ":"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "I-metric", "I-metric", "O", "B-metric", "I-metric", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, algorithm, university, task, country, conference, field, programming language, metric, researcher, product, location, organization and O.\nSentence: The four outcomes can be formulated in a 2 × 2 contingency table or confusion matrix , as follows :", "prompt_labels": "The(O) four(O) outcomes(O) can(O) be(O) formulated(O) in(O) a(O) 2(B-metric) ×(I-metric) 2(I-metric) contingency(I-metric) table(I-metric) or(O) confusion(B-metric) matrix(I-metric) ,(O) as(O) follows(O) :(O)"}}
{"id": "260", "dataset": "crossner_ai", "split": "test", "label_list": ["researcher", "university", "field", "country", "programming language", "task", "organization", "algorithm", "person", "location", "product", "metric", "conference"], "instance": {"id": "260", "words": ["In", "light", "of", "the", "above", "discussion", ",", "we", "see", "that", "the", "SVM", "technique", "is", "equivalent", "to", "empirical", "risk", "with", "Tikhonov", "regularization", ",", "where", "in", "this", "case", "the", "loss", "function", "is", "the", "hinge", "loss"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, university, field, country, programming language, task, organization, algorithm, person, location, product, metric, conference and O.\nSentence: In light of the above discussion , we see that the SVM technique is equivalent to empirical risk with Tikhonov regularization , where in this case the loss function is the hinge loss", "prompt_labels": "In(O) light(O) of(O) the(O) above(O) discussion(O) ,(O) we(O) see(O) that(O) the(O) SVM(B-algorithm) technique(O) is(O) equivalent(O) to(O) empirical(B-algorithm) risk(I-algorithm) with(O) Tikhonov(B-algorithm) regularization(I-algorithm) ,(O) where(O) in(O) this(O) case(O) the(O) loss(O) function(O) is(O) the(O) hinge(B-metric) loss(I-metric)"}}
{"id": "404", "dataset": "crossner_ai", "split": "test", "label_list": ["country", "task", "location", "university", "researcher", "metric", "field", "programming language", "organization", "person", "algorithm", "conference", "product"], "instance": {"id": "404", "words": ["Artificial", "neural", "networks", "have", "been", "used", "on", "a", "variety", "of", "tasks", ",", "including", "computer", "vision", ",", "speech", "recognition", ",", "machine", "translation", ",", "social", "network", "filtering", ",", "playing", "board", "and", "video", "games", "and", "medical", "diagnosis", "."], "labels": ["B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "I-task", "O", "B-task", "I-task", "I-task", "I-task", "I-task", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, task, location, university, researcher, metric, field, programming language, organization, person, algorithm, conference, product and O.\nSentence: Artificial neural networks have been used on a variety of tasks , including computer vision , speech recognition , machine translation , social network filtering , playing board and video games and medical diagnosis .", "prompt_labels": "Artificial(B-algorithm) neural(I-algorithm) networks(I-algorithm) have(O) been(O) used(O) on(O) a(O) variety(O) of(O) tasks(O) ,(O) including(O) computer(B-field) vision(I-field) ,(O) speech(B-task) recognition(I-task) ,(O) machine(B-task) translation(I-task) ,(O) social(B-task) network(I-task) filtering(I-task) ,(O) playing(B-task) board(I-task) and(I-task) video(I-task) games(I-task) and(O) medical(B-task) diagnosis(I-task) .(O)"}}
{"id": "226", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "person", "magazine", "award", "writer", "book", "poem", "event", "location", "country", "organization"], "instance": {"id": "226", "words": ["He", "directed", "Mac", "(", "1992", ")", ",", "which", "won", "the", "Golden", "Camera", "Award", "at", "the", "Cannes", "Film", "Festival", ",", "Illuminata", "(", "1998", ")", ",", "and", "Romance", "and", "Cigarettes", "(", "2005", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, person, magazine, award, writer, book, poem, event, location, country, organization and O.\nSentence: He directed Mac ( 1992 ) , which won the Golden Camera Award at the Cannes Film Festival , Illuminata ( 1998 ) , and Romance and Cigarettes ( 2005 ) .", "prompt_labels": "He(O) directed(O) Mac(O) ((O) 1992(O) )(O) ,(O) which(O) won(O) the(O) Golden(B-award) Camera(I-award) Award(I-award) at(O) the(O) Cannes(B-event) Film(I-event) Festival(I-event) ,(O) Illuminata(O) ((O) 1998(O) )(O) ,(O) and(O) Romance(O) and(O) Cigarettes(O) ((O) 2005(O) )(O) .(O)"}}
{"id": "177", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "magazine", "award", "literary genre", "poem", "location", "organization", "person", "event", "book", "country"], "instance": {"id": "177", "words": ["They", "published", "numerous", "criticisms", "in", "the", "1950s", "and", "1960s", "by", "Whittaker", "Chambers", ",", "Garry", "Wills", ",", "and", "M.", "Stanton", "Evans", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "B-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, magazine, award, literary genre, poem, location, organization, person, event, book, country and O.\nSentence: They published numerous criticisms in the 1950s and 1960s by Whittaker Chambers , Garry Wills , and M. Stanton Evans .", "prompt_labels": "They(O) published(O) numerous(O) criticisms(O) in(O) the(O) 1950s(O) and(O) 1960s(O) by(O) Whittaker(B-writer) Chambers(I-writer) ,(O) Garry(B-writer) Wills(I-writer) ,(O) and(O) M.(B-writer) Stanton(I-writer) Evans(I-writer) .(O)"}}
{"id": "136", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "award", "writer", "location", "organization", "country", "poem", "book", "literary genre", "magazine", "person"], "instance": {"id": "136", "words": ["Paradoxically", ",", "one", "of", "his", "most", "famous", "works", ",", "a", "book", "called", "Safahat", ",", "was", "not", "widely", "read", "or", "published", "until", "recently", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, writer, location, organization, country, poem, book, literary genre, magazine, person and O.\nSentence: Paradoxically , one of his most famous works , a book called Safahat , was not widely read or published until recently .", "prompt_labels": "Paradoxically(O) ,(O) one(O) of(O) his(O) most(O) famous(O) works(O) ,(O) a(O) book(O) called(O) Safahat(B-poem) ,(O) was(O) not(O) widely(O) read(O) or(O) published(O) until(O) recently(O) .(O)"}}
{"id": "3", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "country", "magazine", "poem", "organization", "literary genre", "person", "writer", "event", "award", "location"], "instance": {"id": "3", "words": ["Stoker", "'s", "inspirations", "for", "the", "story", ",", "in", "addition", "to", "Whitby", ",", "may", "have", "included", "a", "visit", "to", "Slains", "Castle", "in", "Aberdeenshire", ",", "a", "visit", "to", "the", "crypts", "of", "St.", "Michan", "'s", "Church", "in", "Dublin", ",", "and", "the", "novella", "Carmilla", "by", "Sheridan", "Le", "Fanu", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "B-location", "O", "O", "O", "B-literary genre", "B-book", "O", "B-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, country, magazine, poem, organization, literary genre, person, writer, event, award, location and O.\nSentence: Stoker 's inspirations for the story , in addition to Whitby , may have included a visit to Slains Castle in Aberdeenshire , a visit to the crypts of St. Michan 's Church in Dublin , and the novella Carmilla by Sheridan Le Fanu .", "prompt_labels": "Stoker(B-writer) 's(O) inspirations(O) for(O) the(O) story(O) ,(O) in(O) addition(O) to(O) Whitby(B-location) ,(O) may(O) have(O) included(O) a(O) visit(O) to(O) Slains(B-location) Castle(I-location) in(O) Aberdeenshire(B-location) ,(O) a(O) visit(O) to(O) the(O) crypts(O) of(O) St.(B-location) Michan(I-location) 's(I-location) Church(I-location) in(O) Dublin(B-location) ,(O) and(O) the(O) novella(B-literary genre) Carmilla(B-book) by(O) Sheridan(B-writer) Le(I-writer) Fanu(I-writer) .(O)"}}
{"id": "107", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "location", "event", "person", "literary genre", "writer", "magazine", "book", "organization", "country", "award"], "instance": {"id": "107", "words": ["In", "2013", ",", "WordFire", "acquired", "the", "reprint", "rights", "to", "the", "works", "of", "Allen", "Drury", ",", "including", "his", "1959", "Pulitzer", "Prize", "-winning", "political", "novel", "Advise", "and", "Consent", "."], "labels": ["O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-award", "I-award", "O", "B-literary genre", "I-literary genre", "B-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, location, event, person, literary genre, writer, magazine, book, organization, country, award and O.\nSentence: In 2013 , WordFire acquired the reprint rights to the works of Allen Drury , including his 1959 Pulitzer Prize -winning political novel Advise and Consent .", "prompt_labels": "In(O) 2013(O) ,(O) WordFire(B-organization) acquired(O) the(O) reprint(O) rights(O) to(O) the(O) works(O) of(O) Allen(B-writer) Drury(I-writer) ,(O) including(O) his(O) 1959(O) Pulitzer(B-award) Prize(I-award) -winning(O) political(B-literary genre) novel(I-literary genre) Advise(B-book) and(I-book) Consent(I-book) .(O)"}}
{"id": "310", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "country", "poem", "location", "event", "writer", "magazine", "organization", "literary genre", "book", "award"], "instance": {"id": "310", "words": ["Al", "Capp", "Admits", "One", "Morals", "Count", ";", "Pays", "$", "500", "Fine", ";", "The", "Capital", "Times", ",", "February", "12", ",", "1972", "In", "a", "December", "1992", "article", "for", "The", "New", "Yorker", ",", "Seymour", "Hersh", "reported", "that", "President", "Richard", "Nixon", "and", "Charles", "Colson", "had", "repeatedly", "discussed", "the", "Capp", "case", "in", "Oval", "Office", "recordings", "that", "had", "recently", "been", "made", "available", "by", "the", "National", "Archives", "."], "labels": ["B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "O", "B-writer", "I-writer", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "B-writer", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, poem, location, event, writer, magazine, organization, literary genre, book, award and O.\nSentence: Al Capp Admits One Morals Count ; Pays $ 500 Fine ; The Capital Times , February 12 , 1972 In a December 1992 article for The New Yorker , Seymour Hersh reported that President Richard Nixon and Charles Colson had repeatedly discussed the Capp case in Oval Office recordings that had recently been made available by the National Archives .", "prompt_labels": "Al(B-writer) Capp(I-writer) Admits(O) One(O) Morals(O) Count(O) ;(O) Pays(O) $(O) 500(O) Fine(O) ;(O) The(B-organization) Capital(I-organization) Times(I-organization) ,(O) February(O) 12(O) ,(O) 1972(O) In(O) a(O) December(O) 1992(O) article(O) for(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) ,(O) Seymour(B-writer) Hersh(I-writer) reported(O) that(O) President(O) Richard(B-person) Nixon(I-person) and(O) Charles(B-person) Colson(I-person) had(O) repeatedly(O) discussed(O) the(O) Capp(B-writer) case(O) in(O) Oval(B-location) Office(I-location) recordings(O) that(O) had(O) recently(O) been(O) made(O) available(O) by(O) the(B-organization) National(I-organization) Archives(I-organization) .(O)"}}
{"id": "22", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "writer", "magazine", "location", "poem", "event", "literary genre", "book", "organization", "country", "award"], "instance": {"id": "22", "words": ["In", "The", "Nation", ",", "reviewer", "Katha", "Pollitt", "said", ",", "Gibson", "has", "violated", "just", "about", "every", "precept", "of", "the", "(", "United", "States", "Conference", "of", "Catholic", "Bishops", ")", "conference", "'s", "own", "1988", "'", "Criteria", "'", "for", "the", "portrayal", "of", "Jews", "in", "dramatizations", "of", "the", "Passion", "(", "no", "bloodthirsty", "Jews", ",", "no", "rabble", ",", "no", "use", "of", "Scripture", "that", "reinforces", "negative", "stereotypes", "of", "Jews", ",", "etc", "."], "labels": ["O", "B-magazine", "I-magazine", "O", "O", "B-writer", "I-writer", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, writer, magazine, location, poem, event, literary genre, book, organization, country, award and O.\nSentence: In The Nation , reviewer Katha Pollitt said , Gibson has violated just about every precept of the ( United States Conference of Catholic Bishops ) conference 's own 1988 ' Criteria ' for the portrayal of Jews in dramatizations of the Passion ( no bloodthirsty Jews , no rabble , no use of Scripture that reinforces negative stereotypes of Jews , etc .", "prompt_labels": "In(O) The(B-magazine) Nation(I-magazine) ,(O) reviewer(O) Katha(B-writer) Pollitt(I-writer) said(O) ,(O) Gibson(B-writer) has(O) violated(O) just(O) about(O) every(O) precept(O) of(O) the(O) ((O) United(B-organization) States(I-organization) Conference(I-organization) of(I-organization) Catholic(I-organization) Bishops(I-organization) )(O) conference(O) 's(O) own(O) 1988(O) '(O) Criteria(O) '(O) for(O) the(O) portrayal(O) of(O) Jews(O) in(O) dramatizations(O) of(O) the(O) Passion(O) ((O) no(O) bloodthirsty(O) Jews(O) ,(O) no(O) rabble(O) ,(O) no(O) use(O) of(O) Scripture(O) that(O) reinforces(O) negative(O) stereotypes(O) of(O) Jews(O) ,(O) etc(O) .(O)"}}
{"id": "201", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "award", "writer", "book", "country", "magazine", "person", "poem", "organization", "location", "event"], "instance": {"id": "201", "words": ["Early", "collections", "of", "English", "ballads", "were", "made", "by", "Samuel", "Pepys", "(", "1633-1703", ")", "and", "in", "the", "Roxburghe", "Ballads", "collected", "by", "Robert", "Harley", ",", "(", "1661", "&", "ndash", ";", "1724", ")", ",", "which", "paralleled", "the", "work", "in", "Scotland", "by", "Walter", "Scott", "and", "Robert", "Burns", "."], "labels": ["O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, award, writer, book, country, magazine, person, poem, organization, location, event and O.\nSentence: Early collections of English ballads were made by Samuel Pepys ( 1633-1703 ) and in the Roxburghe Ballads collected by Robert Harley , ( 1661 & ndash ; 1724 ) , which paralleled the work in Scotland by Walter Scott and Robert Burns .", "prompt_labels": "Early(O) collections(O) of(O) English(B-literary genre) ballads(I-literary genre) were(O) made(O) by(O) Samuel(B-person) Pepys(I-person) ((O) 1633-1703(O) )(O) and(O) in(O) the(O) Roxburghe(B-book) Ballads(I-book) collected(O) by(O) Robert(B-person) Harley(I-person) ,(O) ((O) 1661(O) &(O) ndash(O) ;(O) 1724(O) )(O) ,(O) which(O) paralleled(O) the(O) work(O) in(O) Scotland(B-country) by(O) Walter(B-writer) Scott(I-writer) and(O) Robert(B-writer) Burns(I-writer) .(O)"}}
{"id": "4", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "magazine", "poem", "event", "award", "writer", "literary genre", "person", "organization", "location", "book"], "instance": {"id": "4", "words": ["The", "film", "was", "completed", "in", "1979", "and", "won", "the", "Prize", "of", "the", "Ecumenical", "Jury", "at", "the", "Cannes", "Film", "Festival", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, magazine, poem, event, award, writer, literary genre, person, organization, location, book and O.\nSentence: The film was completed in 1979 and won the Prize of the Ecumenical Jury at the Cannes Film Festival .", "prompt_labels": "The(O) film(O) was(O) completed(O) in(O) 1979(O) and(O) won(O) the(O) Prize(B-award) of(I-award) the(I-award) Ecumenical(I-award) Jury(I-award) at(O) the(O) Cannes(B-event) Film(I-event) Festival(I-event) .(O)"}}
{"id": "151", "dataset": "crossner_literature", "split": "test", "label_list": ["location", "award", "writer", "magazine", "book", "literary genre", "event", "person", "country", "poem", "organization"], "instance": {"id": "151", "words": ["Novels", "such", "as", "Kipps", "and", "The", "History", "of", "Mr", "Polly", ",", "which", "describe", "lower-middle-class", "life", ",", "led", "to", "the", "suggestion", "that", "he", "was", "a", "worthy", "successor", "to", "Charles", "Dickens", ",", "Vincent", "Brome", ",", "H.", "G.", "Wells", ":", "A", "Biography", "(", "London", ",", "New", "York", ",", "and", "Toronto", ":", "Longmans", ",", "Green", ",", "1951", ")", ",", "p", "."], "labels": ["B-literary genre", "O", "O", "B-book", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "B-location", "O", "B-location", "I-location", "O", "O", "B-location", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, writer, magazine, book, literary genre, event, person, country, poem, organization and O.\nSentence: Novels such as Kipps and The History of Mr Polly , which describe lower-middle-class life , led to the suggestion that he was a worthy successor to Charles Dickens , Vincent Brome , H. G. Wells : A Biography ( London , New York , and Toronto : Longmans , Green , 1951 ) , p .", "prompt_labels": "Novels(B-literary genre) such(O) as(O) Kipps(B-book) and(O) The(B-book) History(I-book) of(I-book) Mr(I-book) Polly(I-book) ,(O) which(O) describe(O) lower-middle-class(O) life(O) ,(O) led(O) to(O) the(O) suggestion(O) that(O) he(O) was(O) a(O) worthy(O) successor(O) to(O) Charles(B-writer) Dickens(I-writer) ,(O) Vincent(B-writer) Brome(I-writer) ,(O) H.(B-book) G.(I-book) Wells(I-book) :(I-book) A(I-book) Biography(I-book) ((O) London(B-location) ,(O) New(B-location) York(I-location) ,(O) and(O) Toronto(B-location) :(O) Longmans(B-organization) ,(I-organization) Green(I-organization) ,(O) 1951(O) )(O) ,(O) p(O) .(O)"}}
{"id": "89", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "organization", "location", "country", "literary genre", "book", "magazine", "poem", "event", "person", "writer"], "instance": {"id": "89", "words": ["His", "name", "had", "sometimes", "been", "mentioned", "as", "a", "contender", "for", "the", "Nobel", "Prize", "in", "Literature", ",", "but", "in", "1971", ",", "after", "losing", "to", "Aleksandr", "Solzhenitsyn", ",", "he", "wrote", "to", "a", "friend", ":", "That", "Nobel", "Prize", "!", "I", "hope", "I", "never", "hear", "it", "mentioned", "again", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, organization, location, country, literary genre, book, magazine, poem, event, person, writer and O.\nSentence: His name had sometimes been mentioned as a contender for the Nobel Prize in Literature , but in 1971 , after losing to Aleksandr Solzhenitsyn , he wrote to a friend : That Nobel Prize ! I hope I never hear it mentioned again .", "prompt_labels": "His(O) name(O) had(O) sometimes(O) been(O) mentioned(O) as(O) a(O) contender(O) for(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) ,(O) but(O) in(O) 1971(O) ,(O) after(O) losing(O) to(O) Aleksandr(B-writer) Solzhenitsyn(I-writer) ,(O) he(O) wrote(O) to(O) a(O) friend(O) :(O) That(O) Nobel(B-award) Prize(I-award) !(O) I(O) hope(O) I(O) never(O) hear(O) it(O) mentioned(O) again(O) .(O)"}}
{"id": "230", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "award", "organization", "writer", "event", "magazine", "poem", "location", "book", "literary genre", "country"], "instance": {"id": "230", "words": ["He", "was", "a", "high-ranking", "Song", "dynasty", "scholar-official", "and", "historian", "who", "authored", "the", "monumental", "history", "book", "Zizhi", "Tongjian", "."], "labels": ["O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "B-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, organization, writer, event, magazine, poem, location, book, literary genre, country and O.\nSentence: He was a high-ranking Song dynasty scholar-official and historian who authored the monumental history book Zizhi Tongjian .", "prompt_labels": "He(O) was(O) a(O) high-ranking(O) Song(B-country) dynasty(I-country) scholar-official(O) and(O) historian(O) who(O) authored(O) the(O) monumental(B-literary genre) history(I-literary genre) book(O) Zizhi(B-book) Tongjian(I-book) .(O)"}}
{"id": "293", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "organization", "location", "award", "book", "magazine", "event", "person", "poem", "literary genre", "writer"], "instance": {"id": "293", "words": ["197", ",", "Oxford", "University", "Press", ",", "1969", "and", "gave", "lectures", "on", "Human", "Potentialities", "both", "at", "the", "UCSF", "Medical", "Center", "and", "at", "the", "Esalen", "Institute", "."], "labels": ["O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, location, award, book, magazine, event, person, poem, literary genre, writer and O.\nSentence: 197 , Oxford University Press , 1969 and gave lectures on Human Potentialities both at the UCSF Medical Center and at the Esalen Institute .", "prompt_labels": "197(O) ,(O) Oxford(B-organization) University(I-organization) Press(I-organization) ,(O) 1969(O) and(O) gave(O) lectures(O) on(O) Human(B-book) Potentialities(I-book) both(O) at(O) the(O) UCSF(B-location) Medical(I-location) Center(I-location) and(O) at(O) the(O) Esalen(B-location) Institute(I-location) .(O)"}}
{"id": "325", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "location", "poem", "country", "writer", "organization", "event", "literary genre", "person", "book", "award"], "instance": {"id": "325", "words": ["Tallulah", "premiered", "at", "the", "2016", "Sundance", "Film", "Festival", "on", "January", "23", ",", "2016", "and", "was", "released", "on", "Netflix", "on", "July", "29", ",", "2016", "."], "labels": ["O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, location, poem, country, writer, organization, event, literary genre, person, book, award and O.\nSentence: Tallulah premiered at the 2016 Sundance Film Festival on January 23 , 2016 and was released on Netflix on July 29 , 2016 .", "prompt_labels": "Tallulah(O) premiered(O) at(O) the(O) 2016(B-event) Sundance(I-event) Film(I-event) Festival(I-event) on(O) January(O) 23(O) ,(O) 2016(O) and(O) was(O) released(O) on(O) Netflix(B-organization) on(O) July(O) 29(O) ,(O) 2016(O) .(O)"}}
{"id": "306", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "book", "poem", "person", "location", "literary genre", "event", "country", "organization", "award", "magazine"], "instance": {"id": "306", "words": ["Modern", "readers", "are", "more", "often", "introduced", "to", "Orwell", "as", "a", "novelist", ",", "particularly", "through", "his", "enormously", "successful", "titles", "Animal", "Farm", "and", "Nineteen", "Eighty-Four", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "B-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, book, poem, person, location, literary genre, event, country, organization, award, magazine and O.\nSentence: Modern readers are more often introduced to Orwell as a novelist , particularly through his enormously successful titles Animal Farm and Nineteen Eighty-Four .", "prompt_labels": "Modern(O) readers(O) are(O) more(O) often(O) introduced(O) to(O) Orwell(B-writer) as(O) a(O) novelist(O) ,(O) particularly(O) through(O) his(O) enormously(O) successful(O) titles(O) Animal(B-book) Farm(I-book) and(O) Nineteen(B-book) Eighty-Four(I-book) .(O)"}}
{"id": "45", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "event", "location", "book", "award", "organization", "literary genre", "country", "person", "poem", "magazine"], "instance": {"id": "45", "words": ["In", "the", "second", "quarter", "of", "the", "13th", "century", ",", "a", "version", "in", "Latin", "verse", ",", "the", "Gesta", "Regum", "Britanniae", ",", "was", "produced", "by", "William", "of", "Rennes", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "B-poem", "I-poem", "I-poem", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, event, location, book, award, organization, literary genre, country, person, poem, magazine and O.\nSentence: In the second quarter of the 13th century , a version in Latin verse , the Gesta Regum Britanniae , was produced by William of Rennes .", "prompt_labels": "In(O) the(O) second(O) quarter(O) of(O) the(O) 13th(O) century(O) ,(O) a(O) version(O) in(O) Latin(B-literary genre) verse(I-literary genre) ,(O) the(O) Gesta(B-poem) Regum(I-poem) Britanniae(I-poem) ,(O) was(O) produced(O) by(O) William(B-writer) of(I-writer) Rennes(I-writer) .(O)"}}
{"id": "303", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "book", "person", "poem", "country", "event", "award", "organization", "magazine", "literary genre", "location"], "instance": {"id": "303", "words": ["Chaucer", "'s", "sources", "for", "the", "legends", "include", ":", "Virgil", "'", "s", "Aeneid", ",", "Vincent", "of", "Beauvais", ",", "Guido", "delle", "Colonne", "'", "s", "Historia", "destructionis", "Troiae", ",", "Gaius", "Julius", "Hyginus", "'", "Fabulae", "and", "Ovid", "'", "s", "Metamorphoses", "and", "Heroides", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "B-poem", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-poem", "I-poem", "I-poem", "O", "B-writer", "I-writer", "I-writer", "O", "B-poem", "O", "B-writer", "O", "O", "B-poem", "O", "B-poem", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, book, person, poem, country, event, award, organization, magazine, literary genre, location and O.\nSentence: Chaucer 's sources for the legends include : Virgil ' s Aeneid , Vincent of Beauvais , Guido delle Colonne ' s Historia destructionis Troiae , Gaius Julius Hyginus ' Fabulae and Ovid ' s Metamorphoses and Heroides .", "prompt_labels": "Chaucer(B-writer) 's(O) sources(O) for(O) the(O) legends(O) include(O) :(O) Virgil(B-writer) '(O) s(O) Aeneid(B-poem) ,(O) Vincent(B-writer) of(I-writer) Beauvais(I-writer) ,(O) Guido(B-writer) delle(I-writer) Colonne(I-writer) '(O) s(O) Historia(B-poem) destructionis(I-poem) Troiae(I-poem) ,(O) Gaius(B-writer) Julius(I-writer) Hyginus(I-writer) '(O) Fabulae(B-poem) and(O) Ovid(B-writer) '(O) s(O) Metamorphoses(B-poem) and(O) Heroides(B-poem) .(O)"}}
{"id": "8", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "award", "writer", "person", "country", "literary genre", "organization", "location", "event", "poem", "magazine"], "instance": {"id": "8", "words": ["His", "supernatural", "thriller", "film", "Djinn", "premiered", "at", "the", "2013", "Abu", "Dhabi", "Film", "Festival", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, award, writer, person, country, literary genre, organization, location, event, poem, magazine and O.\nSentence: His supernatural thriller film Djinn premiered at the 2013 Abu Dhabi Film Festival .", "prompt_labels": "His(O) supernatural(O) thriller(O) film(O) Djinn(O) premiered(O) at(O) the(O) 2013(O) Abu(B-event) Dhabi(I-event) Film(I-event) Festival(I-event) .(O)"}}
{"id": "340", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "book", "poem", "location", "event", "writer", "country", "award", "magazine", "person", "organization"], "instance": {"id": "340", "words": ["Tarkovsky", "studied", "film", "at", "Moscow", "'s", "State", "Institute", "of", "Cinematography", "under", "filmmaker", "Mikhail", "Romm", ",", "and", "subsequently", "directed", "his", "first", "five", "feature", "film", "s", "in", "the", "Soviet", "Union", ":", "Ivan", "'s", "Childhood", "(", "1962", ")", ",", "Andrei", "Rublev", "(", "1966", ")", ",", "Solaris", "(", "1972", ")", ",", "Mirror", "(", "1975", ")", ",", "and", "Stalker", "(", "1979", ")", "."], "labels": ["B-writer", "O", "O", "O", "B-location", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, book, poem, location, event, writer, country, award, magazine, person, organization and O.\nSentence: Tarkovsky studied film at Moscow 's State Institute of Cinematography under filmmaker Mikhail Romm , and subsequently directed his first five feature film s in the Soviet Union : Ivan 's Childhood ( 1962 ) , Andrei Rublev ( 1966 ) , Solaris ( 1972 ) , Mirror ( 1975 ) , and Stalker ( 1979 ) .", "prompt_labels": "Tarkovsky(B-writer) studied(O) film(O) at(O) Moscow(B-location) 's(O) State(B-organization) Institute(I-organization) of(I-organization) Cinematography(I-organization) under(O) filmmaker(O) Mikhail(B-person) Romm(I-person) ,(O) and(O) subsequently(O) directed(O) his(O) first(O) five(O) feature(O) film(O) s(O) in(O) the(O) Soviet(B-country) Union(I-country) :(O) Ivan(O) 's(O) Childhood(O) ((O) 1962(O) )(O) ,(O) Andrei(O) Rublev(O) ((O) 1966(O) )(O) ,(O) Solaris(O) ((O) 1972(O) )(O) ,(O) Mirror(O) ((O) 1975(O) )(O) ,(O) and(O) Stalker(O) ((O) 1979(O) )(O) .(O)"}}
{"id": "122", "dataset": "crossner_literature", "split": "test", "label_list": ["location", "poem", "literary genre", "organization", "magazine", "writer", "event", "book", "award", "person", "country"], "instance": {"id": "122", "words": ["In", "1929", ",", "Mann", "had", "a", "cottage", "built", "in", "the", "fishing", "village", "of", "Nidden", ",", "Memel", "Territory", "(", "now", "Nida", ",", "Lithuania", ")", "on", "the", "Curonian", "Spit", ",", "where", "there", "was", "a", "German", "art", "colony", "and", "where", "he", "spent", "the", "summers", "of", "1930-1932", "working", "on", "Joseph", "and", "His", "Brothers", "."], "labels": ["O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "I-location", "O", "O", "B-location", "O", "B-country", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, poem, literary genre, organization, magazine, writer, event, book, award, person, country and O.\nSentence: In 1929 , Mann had a cottage built in the fishing village of Nidden , Memel Territory ( now Nida , Lithuania ) on the Curonian Spit , where there was a German art colony and where he spent the summers of 1930-1932 working on Joseph and His Brothers .", "prompt_labels": "In(O) 1929(O) ,(O) Mann(B-writer) had(O) a(O) cottage(O) built(O) in(O) the(O) fishing(O) village(O) of(O) Nidden(B-location) ,(O) Memel(B-location) Territory(I-location) ((O) now(O) Nida(B-location) ,(O) Lithuania(B-country) )(O) on(O) the(O) Curonian(B-location) Spit(I-location) ,(O) where(O) there(O) was(O) a(O) German(O) art(O) colony(O) and(O) where(O) he(O) spent(O) the(O) summers(O) of(O) 1930-1932(O) working(O) on(O) Joseph(B-book) and(I-book) His(I-book) Brothers(I-book) .(O)"}}
{"id": "223", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "literary genre", "poem", "country", "writer", "award", "location", "person", "magazine", "book", "event"], "instance": {"id": "223", "words": ["In", "the", "Swahili", "and", "Indonesian", "culture", ",", "many", "of", "his", "stories", "are", "being", "told", "under", "the", "name", "of", "Abunuwasi", "or", "Abunawas", ",", "though", "this", "confuses", "Nasreddin", "with", "an", "entirely", "different", "man", "-", "the", "poet", "Abu", "Nuwas", ",", "known", "for", "homoerotic", "verse", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "B-person", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "B-literary genre", "I-literary genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, literary genre, poem, country, writer, award, location, person, magazine, book, event and O.\nSentence: In the Swahili and Indonesian culture , many of his stories are being told under the name of Abunuwasi or Abunawas , though this confuses Nasreddin with an entirely different man - the poet Abu Nuwas , known for homoerotic verse .", "prompt_labels": "In(O) the(O) Swahili(O) and(O) Indonesian(O) culture(O) ,(O) many(O) of(O) his(O) stories(O) are(O) being(O) told(O) under(O) the(O) name(O) of(O) Abunuwasi(B-person) or(O) Abunawas(B-person) ,(O) though(O) this(O) confuses(O) Nasreddin(B-person) with(O) an(O) entirely(O) different(O) man(O) -(O) the(O) poet(O) Abu(B-writer) Nuwas(I-writer) ,(O) known(O) for(O) homoerotic(B-literary genre) verse(I-literary genre) .(O)"}}
{"id": "125", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "writer", "country", "organization", "magazine", "book", "literary genre", "location", "event", "person", "poem"], "instance": {"id": "125", "words": ["But", ",", "in", "the", "1970s", ",", "new", "arguments", "concerning", "Israel", "'s", "past", "and", "the", "biblical", "texts", "challenged", "these", "views", ";", "these", "arguments", "can", "be", "found", "in", "Thomas", "L.", "Thompson", "'", "s", "The", "Historicity", "of", "the", "Patriarchal", "Narratives", "(", "1974", ")", ",", "and", "John", "Van", "Seters", "'", "Abraham", "in", "History", "and", "Tradition", "(", "1975", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, writer, country, organization, magazine, book, literary genre, location, event, person, poem and O.\nSentence: But , in the 1970s , new arguments concerning Israel 's past and the biblical texts challenged these views ; these arguments can be found in Thomas L. Thompson ' s The Historicity of the Patriarchal Narratives ( 1974 ) , and John Van Seters ' Abraham in History and Tradition ( 1975 ) .", "prompt_labels": "But(O) ,(O) in(O) the(O) 1970s(O) ,(O) new(O) arguments(O) concerning(O) Israel(B-country) 's(O) past(O) and(O) the(O) biblical(O) texts(O) challenged(O) these(O) views(O) ;(O) these(O) arguments(O) can(O) be(O) found(O) in(O) Thomas(B-writer) L.(I-writer) Thompson(I-writer) '(O) s(O) The(B-book) Historicity(I-book) of(I-book) the(I-book) Patriarchal(I-book) Narratives(I-book) ((O) 1974(O) )(O) ,(O) and(O) John(B-writer) Van(I-writer) Seters(I-writer) '(O) Abraham(B-book) in(I-book) History(I-book) and(I-book) Tradition(I-book) ((O) 1975(O) )(O) .(O)"}}
{"id": "88", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "literary genre", "writer", "location", "organization", "magazine", "award", "country", "person", "book", "poem"], "instance": {"id": "88", "words": ["These", "were", "later", "collected", "and", "published", "in", "book", "form", "as", "My", "Disillusionment", "in", "Russia", "(", "1923", ")", "and", "My", "Further", "Disillusionment", "in", "Russia", "(", "1924", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, literary genre, writer, location, organization, magazine, award, country, person, book, poem and O.\nSentence: These were later collected and published in book form as My Disillusionment in Russia ( 1923 ) and My Further Disillusionment in Russia ( 1924 ) .", "prompt_labels": "These(O) were(O) later(O) collected(O) and(O) published(O) in(O) book(O) form(O) as(O) My(B-book) Disillusionment(I-book) in(I-book) Russia(I-book) ((O) 1923(O) )(O) and(O) My(B-book) Further(I-book) Disillusionment(I-book) in(I-book) Russia(I-book) ((O) 1924(O) )(O) .(O)"}}
{"id": "398", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "organization", "magazine", "event", "literary genre", "award", "person", "poem", "book", "location", "country"], "instance": {"id": "398", "words": ["Other", "ancient", "epic", "poetry", "includes", "the", "Greek", "epics", ",", "the", "Iliad", "and", "the", "Odyssey", ";", "the", "Avestan", "books", ",", "the", "Gathic", "Avesta", "and", "the", "Yasna", ";", "the", "Ancient", "Rome", "national", "epic", ",", "Virgil", "'", "s", "Aeneid", "(", "written", "between", "29", "and", "19", "BCE", ")", ";", "and", "the", "Indian", "epics", ",", "the", "Ramayana", "and", "the", "Mahabharata", "."], "labels": ["O", "B-literary genre", "I-literary genre", "I-literary genre", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "B-book", "O", "O", "B-book", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "B-book", "O", "O", "B-country", "I-country", "O", "O", "O", "B-writer", "O", "O", "B-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "B-book", "O", "O", "B-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, organization, magazine, event, literary genre, award, person, poem, book, location, country and O.\nSentence: Other ancient epic poetry includes the Greek epics , the Iliad and the Odyssey ; the Avestan books , the Gathic Avesta and the Yasna ; the Ancient Rome national epic , Virgil ' s Aeneid ( written between 29 and 19 BCE ) ; and the Indian epics , the Ramayana and the Mahabharata .", "prompt_labels": "Other(O) ancient(B-literary genre) epic(I-literary genre) poetry(I-literary genre) includes(O) the(O) Greek(B-literary genre) epics(I-literary genre) ,(O) the(O) Iliad(B-book) and(O) the(O) Odyssey(B-book) ;(O) the(O) Avestan(O) books(O) ,(O) the(O) Gathic(B-book) Avesta(I-book) and(O) the(O) Yasna(B-book) ;(O) the(O) Ancient(B-country) Rome(I-country) national(O) epic(O) ,(O) Virgil(B-writer) '(O) s(O) Aeneid(B-book) ((O) written(O) between(O) 29(O) and(O) 19(O) BCE(O) )(O) ;(O) and(O) the(O) Indian(B-literary genre) epics(I-literary genre) ,(O) the(O) Ramayana(B-book) and(O) the(O) Mahabharata(B-book) .(O)"}}
{"id": "202", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "award", "location", "writer", "event", "magazine", "organization", "country", "poem", "person", "literary genre"], "instance": {"id": "202", "words": ["In", "her", "later", "years", ",", "she", "won", "her", "third", "Academy", "Award", ",", "this", "one", "for", "Academy", "Award", "for", "Best", "Supporting", "Actress", ",", "for", "her", "small", "performance", "in", "Murder", "on", "the", "Orient", "Express", "(", "1974", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, award, location, writer, event, magazine, organization, country, poem, person, literary genre and O.\nSentence: In her later years , she won her third Academy Award , this one for Academy Award for Best Supporting Actress , for her small performance in Murder on the Orient Express ( 1974 ) .", "prompt_labels": "In(O) her(O) later(O) years(O) ,(O) she(O) won(O) her(O) third(O) Academy(B-award) Award(I-award) ,(O) this(O) one(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Supporting(I-award) Actress(I-award) ,(O) for(O) her(O) small(O) performance(O) in(O) Murder(B-book) on(I-book) the(I-book) Orient(I-book) Express(I-book) ((O) 1974(O) )(O) .(O)"}}
{"id": "245", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "organization", "country", "literary genre", "person", "event", "poem", "magazine", "location", "book", "writer"], "instance": {"id": "245", "words": ["In", "2005", ",", "McEntire", "starred", "as", "Nellie", "Forbush", "in", "the", "Carnegie", "Hall", "concert", "production", "of", "the", "Broadway", "musical", "South", "Pacific", "with", "Alec", "Baldwin", "as", "Luther", "Billis", "and", "Brian", "Stokes", "Mitchell", "as", "Emile", "de", "Becque", ",", "directed", "by", "Walter", "Bobbie", "and", "with", "an", "adapted", "script", "by", "David", "Ives", "."], "labels": ["O", "O", "O", "B-person", "O", "O", "B-person", "I-person", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "B-organization", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, organization, country, literary genre, person, event, poem, magazine, location, book, writer and O.\nSentence: In 2005 , McEntire starred as Nellie Forbush in the Carnegie Hall concert production of the Broadway musical South Pacific with Alec Baldwin as Luther Billis and Brian Stokes Mitchell as Emile de Becque , directed by Walter Bobbie and with an adapted script by David Ives .", "prompt_labels": "In(O) 2005(O) ,(O) McEntire(B-person) starred(O) as(O) Nellie(B-person) Forbush(I-person) in(O) the(O) Carnegie(B-location) Hall(I-location) concert(I-location) production(O) of(O) the(O) Broadway(B-organization) musical(O) South(O) Pacific(O) with(O) Alec(B-person) Baldwin(I-person) as(O) Luther(B-person) Billis(I-person) and(O) Brian(B-person) Stokes(I-person) Mitchell(I-person) as(O) Emile(B-person) de(I-person) Becque(I-person) ,(O) directed(O) by(O) Walter(B-person) Bobbie(I-person) and(O) with(O) an(O) adapted(O) script(O) by(O) David(B-writer) Ives(I-writer) .(O)"}}
{"id": "248", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "event", "award", "country", "writer", "book", "organization", "person", "literary genre", "poem", "location"], "instance": {"id": "248", "words": ["A", "few", "Saint", "stories", "crossed", "into", "science", "fiction", "and", "fantasy", ",", "The", "Man", "Who", "Liked", "Ants", "and", "the", "early", "novel", "The", "Last", "Hero", "being", "examples", ";", "one", "Saint", "short", "story", ",", "The", "Darker", "Drink", ",", "was", "even", "published", "in", "the", "October", "1952", "issue", "of", "The", "Magazine", "of", "Fantasy", "&", "Science", "Fiction", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "B-literary genre", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "B-literary genre", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, event, award, country, writer, book, organization, person, literary genre, poem, location and O.\nSentence: A few Saint stories crossed into science fiction and fantasy , The Man Who Liked Ants and the early novel The Last Hero being examples ; one Saint short story , The Darker Drink , was even published in the October 1952 issue of The Magazine of Fantasy & Science Fiction .", "prompt_labels": "A(O) few(O) Saint(O) stories(O) crossed(O) into(O) science(B-literary genre) fiction(I-literary genre) and(O) fantasy(B-literary genre) ,(O) The(B-book) Man(I-book) Who(I-book) Liked(I-book) Ants(I-book) and(O) the(O) early(O) novel(B-literary genre) The(B-book) Last(I-book) Hero(I-book) being(O) examples(O) ;(O) one(O) Saint(O) short(B-literary genre) story(I-literary genre) ,(O) The(B-book) Darker(I-book) Drink(I-book) ,(O) was(O) even(O) published(O) in(O) the(O) October(O) 1952(O) issue(O) of(O) The(B-magazine) Magazine(I-magazine) of(I-magazine) Fantasy(I-magazine) &(I-magazine) Science(I-magazine) Fiction(I-magazine) .(O)"}}
{"id": "191", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "event", "organization", "poem", "country", "person", "award", "literary genre", "magazine", "location", "writer"], "instance": {"id": "191", "words": ["This", "group", "(", "jokingly", "designated", "The", "Collective", ")", "included", "future", "Federal", "Reserve", "Chairman", "Alan", "Greenspan", ",", "a", "young", "psychology", "student", "named", "Nathan", "Blumenthal", "(", "later", "Nathaniel", "Branden", ")", "and", "his", "wife", "Barbara", "Branden", "and", "Barbara", "'s", "cousin", "Leonard", "Peikoff", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, event, organization, poem, country, person, award, literary genre, magazine, location, writer and O.\nSentence: This group ( jokingly designated The Collective ) included future Federal Reserve Chairman Alan Greenspan , a young psychology student named Nathan Blumenthal ( later Nathaniel Branden ) and his wife Barbara Branden and Barbara 's cousin Leonard Peikoff .", "prompt_labels": "This(O) group(O) ((O) jokingly(O) designated(O) The(B-organization) Collective(I-organization) )(O) included(O) future(O) Federal(B-organization) Reserve(I-organization) Chairman(O) Alan(B-person) Greenspan(I-person) ,(O) a(O) young(O) psychology(O) student(O) named(O) Nathan(B-person) Blumenthal(I-person) ((O) later(O) Nathaniel(B-person) Branden(I-person) )(O) and(O) his(O) wife(O) Barbara(B-writer) Branden(I-writer) and(O) Barbara(B-writer) 's(O) cousin(O) Leonard(B-person) Peikoff(I-person) .(O)"}}
{"id": "273", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "poem", "event", "organization", "location", "country", "writer", "book", "literary genre", "person", "magazine"], "instance": {"id": "273", "words": ["His", "first", "story", "to", "attract", "major", "attention", "was", "A", "Rose", "for", "Ecclesiastes", ",", "published", "in", "The", "Magazine", "of", "Fantasy", "&", "Science", "Fiction", ",", "with", "cover", "art", "by", "Hannes", "Bok", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, poem, event, organization, location, country, writer, book, literary genre, person, magazine and O.\nSentence: His first story to attract major attention was A Rose for Ecclesiastes , published in The Magazine of Fantasy & Science Fiction , with cover art by Hannes Bok .", "prompt_labels": "His(O) first(O) story(O) to(O) attract(O) major(O) attention(O) was(O) A(B-book) Rose(I-book) for(I-book) Ecclesiastes(I-book) ,(O) published(O) in(O) The(B-magazine) Magazine(I-magazine) of(I-magazine) Fantasy(I-magazine) &(I-magazine) Science(I-magazine) Fiction(I-magazine) ,(O) with(O) cover(O) art(O) by(O) Hannes(B-writer) Bok(I-writer) .(O)"}}
{"id": "156", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "poem", "writer", "organization", "award", "person", "literary genre", "country", "location", "event", "book"], "instance": {"id": "156", "words": ["Two", "other", "early", "works", "were", "Anelida", "and", "Arcite", "and", "The", "House", "of", "Fame", "."], "labels": ["O", "O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "O", "B-poem", "I-poem", "I-poem", "I-poem", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, poem, writer, organization, award, person, literary genre, country, location, event, book and O.\nSentence: Two other early works were Anelida and Arcite and The House of Fame .", "prompt_labels": "Two(O) other(O) early(O) works(O) were(O) Anelida(B-poem) and(I-poem) Arcite(I-poem) and(O) The(B-poem) House(I-poem) of(I-poem) Fame(I-poem) .(O)"}}
{"id": "91", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "event", "book", "organization", "writer", "person", "location", "poem", "country", "award", "magazine"], "instance": {"id": "91", "words": ["With", "William", "Butler", "Yeats", "and", "Edward", "Martyn", ",", "she", "co-founded", "the", "Irish", "Literary", "Theatre", "and", "the", "Abbey", "Theatre", ",", "and", "wrote", "numerous", "short", "works", "for", "both", "companies", "."], "labels": ["O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, event, book, organization, writer, person, location, poem, country, award, magazine and O.\nSentence: With William Butler Yeats and Edward Martyn , she co-founded the Irish Literary Theatre and the Abbey Theatre , and wrote numerous short works for both companies .", "prompt_labels": "With(O) William(B-writer) Butler(I-writer) Yeats(I-writer) and(O) Edward(B-writer) Martyn(I-writer) ,(O) she(O) co-founded(O) the(O) Irish(B-organization) Literary(I-organization) Theatre(I-organization) and(O) the(O) Abbey(B-organization) Theatre(I-organization) ,(O) and(O) wrote(O) numerous(O) short(O) works(O) for(O) both(O) companies(O) .(O)"}}
{"id": "357", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "literary genre", "location", "poem", "event", "book", "person", "country", "organization", "writer", "magazine"], "instance": {"id": "357", "words": ["For", "example", ",", "Holmes", "falls", "in", "love", "and", "marries", "in", "Laurie", "R.", "King", "'", "s", "Mary", "Russell", "series", ",", "is", "re-animated", "after", "his", "death", "to", "fight", "future", "crime", "in", "the", "animated", "series", "Sherlock", "Holmes", "in", "the", "22nd", "Century", ",", "and", "is", "meshed", "with", "the", "setting", "of", "H.", "P.", "Lovecraft", "'", "s", "Cthulhu", "Mythos", "in", "Neil", "Gaiman", "'", "s", "A", "Study", "in", "Emerald", "(", "which", "won", "the", "2004", "Hugo", "Award", "for", "Best", "Short", "Story", ")", "."], "labels": ["O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-book", "I-book", "O", "B-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, literary genre, location, poem, event, book, person, country, organization, writer, magazine and O.\nSentence: For example , Holmes falls in love and marries in Laurie R. King ' s Mary Russell series , is re-animated after his death to fight future crime in the animated series Sherlock Holmes in the 22nd Century , and is meshed with the setting of H. P. Lovecraft ' s Cthulhu Mythos in Neil Gaiman ' s A Study in Emerald ( which won the 2004 Hugo Award for Best Short Story ) .", "prompt_labels": "For(O) example(O) ,(O) Holmes(B-person) falls(O) in(O) love(O) and(O) marries(O) in(O) Laurie(B-writer) R.(I-writer) King(I-writer) '(O) s(O) Mary(O) Russell(O) series(O) ,(O) is(O) re-animated(O) after(O) his(O) death(O) to(O) fight(O) future(O) crime(O) in(O) the(O) animated(O) series(O) Sherlock(O) Holmes(O) in(O) the(O) 22nd(O) Century(O) ,(O) and(O) is(O) meshed(O) with(O) the(O) setting(O) of(O) H.(B-writer) P.(I-writer) Lovecraft(I-writer) '(O) s(O) Cthulhu(B-book) Mythos(I-book) in(O) Neil(B-writer) Gaiman(I-writer) '(O) s(O) A(B-book) Study(I-book) in(I-book) Emerald(I-book) ((O) which(O) won(O) the(O) 2004(O) Hugo(B-award) Award(I-award) for(I-award) Best(I-award) Short(I-award) Story(I-award) )(O) .(O)"}}
{"id": "137", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "award", "person", "location", "country", "magazine", "literary genre", "event", "book", "poem", "organization"], "instance": {"id": "137", "words": ["It", "is", "partially", "a", "science-fictional", "pastiche", "of", "Moby-Dick", "by", "Herman", "Melville", "."], "labels": ["O", "O", "O", "O", "B-literary genre", "O", "O", "B-book", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, award, person, location, country, magazine, literary genre, event, book, poem, organization and O.\nSentence: It is partially a science-fictional pastiche of Moby-Dick by Herman Melville .", "prompt_labels": "It(O) is(O) partially(O) a(O) science-fictional(B-literary genre) pastiche(O) of(O) Moby-Dick(B-book) by(O) Herman(B-writer) Melville(I-writer) .(O)"}}
{"id": "393", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "event", "country", "person", "location", "book", "magazine", "organization", "award", "literary genre", "writer"], "instance": {"id": "393", "words": ["The", "Knighthood", "(", "i.e.", "the", "title", "of", "'", "Sir", "'", ")", "was", "conferred", "on", "him", "by", "the", "same", "King", "George", "V", "after", "receiving", "the", "Nobel", "Prize", "in", "Literature", "for", "Gitanjali", "from", "the", "government", "of", "Sweden", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "B-book", "O", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, event, country, person, location, book, magazine, organization, award, literary genre, writer and O.\nSentence: The Knighthood ( i.e. the title of ' Sir ' ) was conferred on him by the same King George V after receiving the Nobel Prize in Literature for Gitanjali from the government of Sweden .", "prompt_labels": "The(O) Knighthood(O) ((O) i.e.(O) the(O) title(O) of(O) '(O) Sir(O) '(O) )(O) was(O) conferred(O) on(O) him(O) by(O) the(O) same(O) King(B-person) George(I-person) V(I-person) after(O) receiving(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) for(O) Gitanjali(B-book) from(O) the(O) government(O) of(O) Sweden(B-country) .(O)"}}
{"id": "407", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "organization", "event", "country", "book", "magazine", "person", "award", "literary genre", "writer", "location"], "instance": {"id": "407", "words": ["Germany", "'", "s", "national", "poet", ",", "Johann", "Wolfgang", "von", "Goethe", ",", "also", "wrote", "many", "sonnets", ",", "using", "a", "rhyme", "scheme", "derived", "from", "Italian", "poetry", "."], "labels": ["B-country", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "I-writer", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, organization, event, country, book, magazine, person, award, literary genre, writer, location and O.\nSentence: Germany ' s national poet , Johann Wolfgang von Goethe , also wrote many sonnets , using a rhyme scheme derived from Italian poetry .", "prompt_labels": "Germany(B-country) '(O) s(O) national(O) poet(O) ,(O) Johann(B-writer) Wolfgang(I-writer) von(I-writer) Goethe(I-writer) ,(O) also(O) wrote(O) many(O) sonnets(B-literary genre) ,(O) using(O) a(O) rhyme(O) scheme(O) derived(O) from(O) Italian(O) poetry(B-literary genre) .(O)"}}
{"id": "315", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "writer", "book", "country", "organization", "poem", "location", "event", "award", "literary genre", "person"], "instance": {"id": "315", "words": ["The", "phrase", ",", "from", "Ode", "on", "a", "Distant", "Prospect", "of", "Eton", "College", ",", "is", "possibly", "one", "of", "the", "most", "misconstrued", "phrases", "in", "English", "literature", "."], "labels": ["O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, writer, book, country, organization, poem, location, event, award, literary genre, person and O.\nSentence: The phrase , from Ode on a Distant Prospect of Eton College , is possibly one of the most misconstrued phrases in English literature .", "prompt_labels": "The(O) phrase(O) ,(O) from(O) Ode(B-poem) on(I-poem) a(I-poem) Distant(I-poem) Prospect(I-poem) of(I-poem) Eton(I-poem) College(I-poem) ,(O) is(O) possibly(O) one(O) of(O) the(O) most(O) misconstrued(O) phrases(O) in(O) English(O) literature(O) .(O)"}}
{"id": "299", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "magazine", "writer", "location", "literary genre", "person", "event", "country", "book", "poem", "organization"], "instance": {"id": "299", "words": ["His", "poem", ",", "Jai", "Jai", "Garavi", "Gujarat", "(", "1873", ")", ",", "is", "used", "as", "a", "de", "facto", "state", "song", "for", "Gujarat", "."], "labels": ["O", "B-literary genre", "O", "B-poem", "I-poem", "I-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-poem", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, magazine, writer, location, literary genre, person, event, country, book, poem, organization and O.\nSentence: His poem , Jai Jai Garavi Gujarat ( 1873 ) , is used as a de facto state song for Gujarat .", "prompt_labels": "His(O) poem(B-literary genre) ,(O) Jai(B-poem) Jai(I-poem) Garavi(I-poem) Gujarat(I-poem) ((O) 1873(O) )(O) ,(O) is(O) used(O) as(O) a(O) de(O) facto(O) state(O) song(O) for(O) Gujarat(B-poem) .(O)"}}
{"id": "178", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "writer", "location", "award", "poem", "magazine", "organization", "country", "person", "event", "book"], "instance": {"id": "178", "words": ["Three", "of", "his", "films", "-", "Andrei", "Rublev", ",", "Mirror", ",", "and", "Stalker", "-", "featured", "in", "Sight", "&", "Sound", "s", "2012", "poll", "of", "the", "50", "greatest", "films", "of", "all", "time", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, writer, location, award, poem, magazine, organization, country, person, event, book and O.\nSentence: Three of his films - Andrei Rublev , Mirror , and Stalker - featured in Sight & Sound s 2012 poll of the 50 greatest films of all time .", "prompt_labels": "Three(O) of(O) his(O) films(O) -(O) Andrei(O) Rublev(O) ,(O) Mirror(O) ,(O) and(O) Stalker(O) -(O) featured(O) in(O) Sight(B-magazine) &(I-magazine) Sound(I-magazine) s(O) 2012(B-award) poll(I-award) of(I-award) the(I-award) 50(I-award) greatest(I-award) films(I-award) of(I-award) all(I-award) time(I-award) .(O)"}}
{"id": "194", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "magazine", "literary genre", "book", "writer", "event", "poem", "person", "location", "country", "award"], "instance": {"id": "194", "words": ["He", "associated", "with", "a", "variety", "of", "figures", "in", "Britain", "'s", "intelligence", "community", "at", "the", "time", ",", "including", "Dennis", "Wheatley", ",", "Roald", "Dahl", ",", "Ian", "Fleming", ",", "and", "Maxwell", "Knight", ","], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, magazine, literary genre, book, writer, event, poem, person, location, country, award and O.\nSentence: He associated with a variety of figures in Britain 's intelligence community at the time , including Dennis Wheatley , Roald Dahl , Ian Fleming , and Maxwell Knight ,", "prompt_labels": "He(O) associated(O) with(O) a(O) variety(O) of(O) figures(O) in(O) Britain(B-country) 's(O) intelligence(O) community(O) at(O) the(O) time(O) ,(O) including(O) Dennis(B-writer) Wheatley(I-writer) ,(O) Roald(B-writer) Dahl(I-writer) ,(O) Ian(B-writer) Fleming(I-writer) ,(O) and(O) Maxwell(B-person) Knight(I-person) ,(O)"}}
{"id": "372", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "organization", "event", "award", "writer", "poem", "literary genre", "location", "country", "book", "magazine"], "instance": {"id": "372", "words": ["He", "is", "best", "known", "for", "his", "works", "of", "fiction", ",", "especially", "The", "Screwtape", "Letters", ",", "The", "Chronicles", "of", "Narnia", ",", "and", "The", "Space", "Trilogy", ",", "and", "for", "his", "non-fiction", "Christian", "apologetics", ",", "such", "as", "Mere", "Christianity", ",", "Miracles", ",", "and", "The", "Problem", "of", "Pain", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "B-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "I-literary genre", "O", "O", "O", "B-book", "I-book", "O", "B-book", "O", "O", "B-book", "I-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, event, award, writer, poem, literary genre, location, country, book, magazine and O.\nSentence: He is best known for his works of fiction , especially The Screwtape Letters , The Chronicles of Narnia , and The Space Trilogy , and for his non-fiction Christian apologetics , such as Mere Christianity , Miracles , and The Problem of Pain .", "prompt_labels": "He(O) is(O) best(O) known(O) for(O) his(O) works(O) of(O) fiction(B-literary genre) ,(O) especially(O) The(B-book) Screwtape(I-book) Letters(I-book) ,(O) The(B-book) Chronicles(I-book) of(I-book) Narnia(I-book) ,(O) and(O) The(B-book) Space(I-book) Trilogy(I-book) ,(O) and(O) for(O) his(O) non-fiction(B-literary genre) Christian(I-literary genre) apologetics(I-literary genre) ,(O) such(O) as(O) Mere(B-book) Christianity(I-book) ,(O) Miracles(B-book) ,(O) and(O) The(B-book) Problem(I-book) of(I-book) Pain(I-book) .(O)"}}
{"id": "327", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "poem", "country", "event", "writer", "award", "person", "book", "magazine", "organization", "location"], "instance": {"id": "327", "words": ["The", "title", "is", "taken", "from", "his", "poem", "The", "Negro", "Speaks", "of", "Rivers", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-literary genre", "B-poem", "I-poem", "I-poem", "I-poem", "I-poem", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, poem, country, event, writer, award, person, book, magazine, organization, location and O.\nSentence: The title is taken from his poem The Negro Speaks of Rivers .", "prompt_labels": "The(O) title(O) is(O) taken(O) from(O) his(O) poem(B-literary genre) The(B-poem) Negro(I-poem) Speaks(I-poem) of(I-poem) Rivers(I-poem) .(O)"}}
{"id": "183", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "book", "organization", "country", "writer", "event", "award", "poem", "literary genre", "person", "location"], "instance": {"id": "183", "words": ["It", "was", "followed", "the", "next", "year", "by", "The", "Tale", "of", "Squirrel", "Nutkin", "and", "The", "Tailor", "of", "Gloucester", ",", "which", "had", "also", "first", "been", "written", "as", "picture", "letters", "to", "the", "Moore", "children", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, book, organization, country, writer, event, award, poem, literary genre, person, location and O.\nSentence: It was followed the next year by The Tale of Squirrel Nutkin and The Tailor of Gloucester , which had also first been written as picture letters to the Moore children .", "prompt_labels": "It(O) was(O) followed(O) the(O) next(O) year(O) by(O) The(B-book) Tale(I-book) of(I-book) Squirrel(I-book) Nutkin(I-book) and(O) The(B-book) Tailor(I-book) of(I-book) Gloucester(I-book) ,(O) which(O) had(O) also(O) first(O) been(O) written(O) as(O) picture(O) letters(O) to(O) the(O) Moore(O) children(O) .(O)"}}
{"id": "14", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "person", "award", "book", "event", "writer", "poem", "country", "organization", "location", "literary genre"], "instance": {"id": "14", "words": ["In", "a", "letter", "to", "P.", "Schuyler", "Miller", "and", "John", "Drury", "Clark", "in", "1936", ",", "only", "three", "months", "before", "Howard", "'s", "death", ",", "Conan", "is", "described", "as", "standing", "6", "ft", "/", "183", "cm", "and", "weighing", "when", "he", "takes", "part", "in", "an", "attack", "on", "Venarium", "at", "only", "14", "years", "old", ",", "though", "being", "far", "from", "fully", "grown", "."], "labels": ["O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, person, award, book, event, writer, poem, country, organization, location, literary genre and O.\nSentence: In a letter to P. Schuyler Miller and John Drury Clark in 1936 , only three months before Howard 's death , Conan is described as standing 6 ft / 183 cm and weighing when he takes part in an attack on Venarium at only 14 years old , though being far from fully grown .", "prompt_labels": "In(O) a(O) letter(O) to(O) P.(B-writer) Schuyler(I-writer) Miller(I-writer) and(O) John(B-writer) Drury(I-writer) Clark(I-writer) in(O) 1936(O) ,(O) only(O) three(O) months(O) before(O) Howard(B-writer) 's(O) death(O) ,(O) Conan(B-person) is(O) described(O) as(O) standing(O) 6(O) ft(O) /(O) 183(O) cm(O) and(O) weighing(O) when(O) he(O) takes(O) part(O) in(O) an(O) attack(O) on(O) Venarium(O) at(O) only(O) 14(O) years(O) old(O) ,(O) though(O) being(O) far(O) from(O) fully(O) grown(O) .(O)"}}
{"id": "167", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "book", "event", "person", "literary genre", "location", "organization", "award", "country", "poem", "writer"], "instance": {"id": "167", "words": ["Cuarón", "shared", "an", "Academy", "Awards", "nomination", "for", "Academy", "Award", "for", "Best", "Original", "Screenplay", "with", "co-writer", "and", "brother", "Carlos", "Cuarón", "."], "labels": ["B-writer", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, book, event, person, literary genre, location, organization, award, country, poem, writer and O.\nSentence: Cuarón shared an Academy Awards nomination for Academy Award for Best Original Screenplay with co-writer and brother Carlos Cuarón .", "prompt_labels": "Cuarón(B-writer) shared(O) an(O) Academy(B-award) Awards(I-award) nomination(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Screenplay(I-award) with(O) co-writer(O) and(O) brother(O) Carlos(B-writer) Cuarón(I-writer) .(O)"}}
{"id": "152", "dataset": "crossner_literature", "split": "test", "label_list": ["location", "poem", "book", "country", "magazine", "writer", "person", "literary genre", "organization", "award", "event"], "instance": {"id": "152", "words": ["During", "this", "time", ",", "he", "made", "the", "short", "film", "I", "Am", "Joaquin", "based", "on", "the", "legendary", "poem", "by", "Rodolfo", "Corky", "Gonzáles", "(", "it", "was", "later", "inducted", "into", "the", "National", "Film", "Registry", "in", "2010", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "O", "O", "O", "O", "B-literary genre", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, poem, book, country, magazine, writer, person, literary genre, organization, award, event and O.\nSentence: During this time , he made the short film I Am Joaquin based on the legendary poem by Rodolfo Corky Gonzáles ( it was later inducted into the National Film Registry in 2010 ) .", "prompt_labels": "During(O) this(O) time(O) ,(O) he(O) made(O) the(O) short(O) film(O) I(B-poem) Am(I-poem) Joaquin(I-poem) based(O) on(O) the(O) legendary(O) poem(B-literary genre) by(O) Rodolfo(B-writer) Corky(I-writer) Gonzáles(I-writer) ((O) it(O) was(O) later(O) inducted(O) into(O) the(O) National(B-organization) Film(I-organization) Registry(I-organization) in(O) 2010(O) )(O) .(O)"}}
{"id": "186", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "organization", "location", "person", "magazine", "book", "event", "poem", "literary genre", "country", "award"], "instance": {"id": "186", "words": ["We", "can", "see", "its", "echo", "in", "the", "works", "of", "such", "very", "different", "writers", "as", "Daniel", "Defoe", ",", "William", "Makepeace", "Thackeray", ",", "the", "Brontës", ",", "Samuel", "Taylor", "Coleridge", ",", "T.", "S.", "Eliot", "and", "even", "Dorothy", "L.", "Sayers", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-writer", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, organization, location, person, magazine, book, event, poem, literary genre, country, award and O.\nSentence: We can see its echo in the works of such very different writers as Daniel Defoe , William Makepeace Thackeray , the Brontës , Samuel Taylor Coleridge , T. S. Eliot and even Dorothy L. Sayers .", "prompt_labels": "We(O) can(O) see(O) its(O) echo(O) in(O) the(O) works(O) of(O) such(O) very(O) different(O) writers(O) as(O) Daniel(B-writer) Defoe(I-writer) ,(O) William(B-writer) Makepeace(I-writer) Thackeray(I-writer) ,(O) the(O) Brontës(B-writer) ,(O) Samuel(B-writer) Taylor(I-writer) Coleridge(I-writer) ,(O) T.(B-writer) S.(I-writer) Eliot(I-writer) and(O) even(O) Dorothy(B-writer) L.(I-writer) Sayers(I-writer) .(O)"}}
{"id": "392", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "organization", "literary genre", "location", "magazine", "writer", "book", "country", "person", "award", "event"], "instance": {"id": "392", "words": ["Christie", "had", "long", "been", "a", "fan", "of", "detective", "novels", ",", "having", "enjoyed", "Wilkie", "Collins", "'", "s", "The", "Woman", "in", "White", "and", "The", "Moonstone", ",", "as", "well", "as", "Sir", "Arthur", "Conan", "Doyle", "'", "s", "early", "Sherlock", "Holmes", "stories", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, organization, literary genre, location, magazine, writer, book, country, person, award, event and O.\nSentence: Christie had long been a fan of detective novels , having enjoyed Wilkie Collins ' s The Woman in White and The Moonstone , as well as Sir Arthur Conan Doyle ' s early Sherlock Holmes stories .", "prompt_labels": "Christie(B-writer) had(O) long(O) been(O) a(O) fan(O) of(O) detective(B-literary genre) novels(I-literary genre) ,(O) having(O) enjoyed(O) Wilkie(B-writer) Collins(I-writer) '(O) s(O) The(B-book) Woman(I-book) in(I-book) White(I-book) and(O) The(B-book) Moonstone(I-book) ,(O) as(O) well(O) as(O) Sir(B-writer) Arthur(I-writer) Conan(I-writer) Doyle(I-writer) '(O) s(O) early(B-book) Sherlock(I-book) Holmes(I-book) stories(I-book) .(O)"}}
{"id": "355", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "location", "book", "person", "country", "writer", "award", "poem", "literary genre", "organization", "magazine"], "instance": {"id": "355", "words": ["She", "wrote", "seven", "novels", ",", "Adam", "Bede", "(", "1859", ")", ",", "The", "Mill", "on", "the", "Floss", "(", "1860", ")", ",", "Silas", "Marner", "(", "1861", ")", ",", "Romola", "(", "1862-63", ")", ",", "Felix", "Holt", ",", "the", "Radical", "(", "1866", ")", ",", "Middlemarch", "(", "1871-72", ")", "and", "Daniel", "Deronda", "(", "1876", ")", ",", "most", "of", "which", "are", "set", "in", "provincial", "England", "and", "known", "for", "their", "realism", "and", "psychological", "insight", "."], "labels": ["O", "O", "O", "B-literary genre", "O", "B-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "B-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, book, person, country, writer, award, poem, literary genre, organization, magazine and O.\nSentence: She wrote seven novels , Adam Bede ( 1859 ) , The Mill on the Floss ( 1860 ) , Silas Marner ( 1861 ) , Romola ( 1862-63 ) , Felix Holt , the Radical ( 1866 ) , Middlemarch ( 1871-72 ) and Daniel Deronda ( 1876 ) , most of which are set in provincial England and known for their realism and psychological insight .", "prompt_labels": "She(O) wrote(O) seven(O) novels(B-literary genre) ,(O) Adam(B-book) Bede(I-book) ((O) 1859(O) )(O) ,(O) The(B-book) Mill(I-book) on(I-book) the(I-book) Floss(I-book) ((O) 1860(O) )(O) ,(O) Silas(B-book) Marner(I-book) ((O) 1861(O) )(O) ,(O) Romola(B-book) ((O) 1862-63(O) )(O) ,(O) Felix(B-book) Holt(I-book) ,(I-book) the(I-book) Radical(I-book) ((O) 1866(O) )(O) ,(O) Middlemarch(B-book) ((O) 1871-72(O) )(O) and(O) Daniel(B-book) Deronda(I-book) ((O) 1876(O) )(O) ,(O) most(O) of(O) which(O) are(O) set(O) in(O) provincial(O) England(B-country) and(O) known(O) for(O) their(O) realism(O) and(O) psychological(O) insight(O) .(O)"}}
{"id": "247", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "organization", "book", "person", "writer", "location", "magazine", "event", "award", "country", "poem"], "instance": {"id": "247", "words": ["Another", "well-known", "version", "of", "the", "play", "is", "Jedermann", "by", "the", "Austrian", "playwright", "Hugo", "von", "Hofmannsthal", ",", "which", "has", "been", "performed", "annually", "at", "the", "Salzburg", "Festival", "since", "1920", ",", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-book", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, organization, book, person, writer, location, magazine, event, award, country, poem and O.\nSentence: Another well-known version of the play is Jedermann by the Austrian playwright Hugo von Hofmannsthal , which has been performed annually at the Salzburg Festival since 1920 , .", "prompt_labels": "Another(O) well-known(O) version(O) of(O) the(O) play(O) is(O) Jedermann(B-book) by(O) the(O) Austrian(O) playwright(O) Hugo(B-writer) von(I-writer) Hofmannsthal(I-writer) ,(O) which(O) has(O) been(O) performed(O) annually(O) at(O) the(O) Salzburg(B-event) Festival(I-event) since(O) 1920(O) ,(O) .(O)"}}
{"id": "408", "dataset": "crossner_literature", "split": "test", "label_list": ["location", "magazine", "organization", "country", "event", "award", "literary genre", "book", "writer", "poem", "person"], "instance": {"id": "408", "words": ["Ride", "with", "the", "Devil", "received", "its", "world", "premiere", "at", "the", "25th", "Deauville", "American", "Film", "Festival", "in", "France", "on", "September", "9", ",", "1999", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "B-country", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, magazine, organization, country, event, award, literary genre, book, writer, poem, person and O.\nSentence: Ride with the Devil received its world premiere at the 25th Deauville American Film Festival in France on September 9 , 1999 .", "prompt_labels": "Ride(O) with(O) the(O) Devil(O) received(O) its(O) world(O) premiere(O) at(O) the(O) 25th(O) Deauville(B-event) American(I-event) Film(I-event) Festival(I-event) in(O) France(B-country) on(O) September(O) 9(O) ,(O) 1999(O) .(O)"}}
{"id": "280", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "country", "magazine", "organization", "person", "poem", "event", "location", "award", "book", "literary genre"], "instance": {"id": "280", "words": ["Gaiman", "'s", "2009", "Newbery", "Medal", "winning", "book", "The", "Graveyard", "Book", "will", "be", "made", "into", "a", "movie", ",", "with", "Ron", "Howard", "as", "the", "director", "."], "labels": ["B-writer", "O", "O", "B-award", "I-award", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, country, magazine, organization, person, poem, event, location, award, book, literary genre and O.\nSentence: Gaiman 's 2009 Newbery Medal winning book The Graveyard Book will be made into a movie , with Ron Howard as the director .", "prompt_labels": "Gaiman(B-writer) 's(O) 2009(O) Newbery(B-award) Medal(I-award) winning(O) book(O) The(B-book) Graveyard(I-book) Book(I-book) will(O) be(O) made(O) into(O) a(O) movie(O) ,(O) with(O) Ron(B-person) Howard(I-person) as(O) the(O) director(O) .(O)"}}
{"id": "285", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "country", "location", "poem", "event", "book", "magazine", "award", "writer", "organization", "literary genre"], "instance": {"id": "285", "words": ["In", "1947", ",", "Miller", "'s", "play", "All", "My", "Sons", ",", "the", "writing", "of", "which", "had", "commenced", "in", "1941", ",", "was", "a", "success", "on", "Broadway", "(", "earning", "him", "his", "first", "Tony", "Award", ",", "Tony", "Award", "for", "Best", "Author", ")", "and", "his", "reputation", "as", "a", "playwright", "was", "established", "."], "labels": ["O", "O", "O", "B-person", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, location, poem, event, book, magazine, award, writer, organization, literary genre and O.\nSentence: In 1947 , Miller 's play All My Sons , the writing of which had commenced in 1941 , was a success on Broadway ( earning him his first Tony Award , Tony Award for Best Author ) and his reputation as a playwright was established .", "prompt_labels": "In(O) 1947(O) ,(O) Miller(B-person) 's(O) play(O) All(B-book) My(I-book) Sons(I-book) ,(O) the(O) writing(O) of(O) which(O) had(O) commenced(O) in(O) 1941(O) ,(O) was(O) a(O) success(O) on(O) Broadway(B-organization) ((O) earning(O) him(O) his(O) first(O) Tony(B-award) Award(I-award) ,(O) Tony(B-award) Award(I-award) for(I-award) Best(I-award) Author(I-award) )(O) and(O) his(O) reputation(O) as(O) a(O) playwright(O) was(O) established(O) .(O)"}}
{"id": "259", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "event", "organization", "country", "magazine", "location", "book", "person", "writer", "poem", "literary genre"], "instance": {"id": "259", "words": ["Director", "Anatole", "Litvak", ",", "unhappy", "with", "the", "script", "submitted", "by", "Frank", "Partos", "and", "Millen", "Brand", "for", "The", "Snake", "Pit", "(", "1948", ")", ",", "hired", "Laurents", "to", "rewrite", "it", "."], "labels": ["O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, organization, country, magazine, location, book, person, writer, poem, literary genre and O.\nSentence: Director Anatole Litvak , unhappy with the script submitted by Frank Partos and Millen Brand for The Snake Pit ( 1948 ) , hired Laurents to rewrite it .", "prompt_labels": "Director(O) Anatole(B-person) Litvak(I-person) ,(O) unhappy(O) with(O) the(O) script(O) submitted(O) by(O) Frank(B-writer) Partos(I-writer) and(O) Millen(B-writer) Brand(I-writer) for(O) The(O) Snake(O) Pit(O) ((O) 1948(O) )(O) ,(O) hired(O) Laurents(B-writer) to(O) rewrite(O) it(O) .(O)"}}
{"id": "87", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "book", "person", "poem", "magazine", "event", "country", "award", "writer", "organization", "location"], "instance": {"id": "87", "words": ["Soon", "after", "his", "return", "to", "England", ",", "Dickens", "began", "work", "on", "the", "first", "of", "his", "Christmas", "stories", ",", "A", "Christmas", "Carol", ",", "written", "in", "1843", ",", "which", "was", "followed", "by", "The", "Chimes", "in", "1844", "and", "The", "Cricket", "on", "the", "Hearth", "in", "1845", "."], "labels": ["O", "O", "O", "O", "O", "B-country", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "B-event", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, book, person, poem, magazine, event, country, award, writer, organization, location and O.\nSentence: Soon after his return to England , Dickens began work on the first of his Christmas stories , A Christmas Carol , written in 1843 , which was followed by The Chimes in 1844 and The Cricket on the Hearth in 1845 .", "prompt_labels": "Soon(O) after(O) his(O) return(O) to(O) England(B-country) ,(O) Dickens(B-writer) began(O) work(O) on(O) the(O) first(O) of(O) his(O) Christmas(B-event) stories(O) ,(O) A(B-book) Christmas(I-book) Carol(I-book) ,(O) written(O) in(O) 1843(O) ,(O) which(O) was(O) followed(O) by(O) The(B-book) Chimes(I-book) in(O) 1844(O) and(O) The(B-book) Cricket(I-book) on(I-book) the(I-book) Hearth(I-book) in(O) 1845(O) .(O)"}}
{"id": "46", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "location", "award", "country", "literary genre", "writer", "person", "magazine", "organization", "poem", "event"], "instance": {"id": "46", "words": ["His", "best", "known", "works", "include", "The", "Day", "of", "the", "Triffids", "(", "1951", ")", "and", "The", "Midwich", "Cuckoos", "(", "1957", ")", ",", "the", "latter", "filmed", "twice", "as", "Village", "of", "the", "Damned", "."], "labels": ["O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, location, award, country, literary genre, writer, person, magazine, organization, poem, event and O.\nSentence: His best known works include The Day of the Triffids ( 1951 ) and The Midwich Cuckoos ( 1957 ) , the latter filmed twice as Village of the Damned .", "prompt_labels": "His(O) best(O) known(O) works(O) include(O) The(B-book) Day(I-book) of(I-book) the(I-book) Triffids(I-book) ((O) 1951(O) )(O) and(O) The(B-book) Midwich(I-book) Cuckoos(I-book) ((O) 1957(O) )(O) ,(O) the(O) latter(O) filmed(O) twice(O) as(O) Village(O) of(O) the(O) Damned(O) .(O)"}}
{"id": "336", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "organization", "poem", "writer", "magazine", "location", "event", "literary genre", "award", "person", "book"], "instance": {"id": "336", "words": ["His", "works", "have", "been", "short", "listed", "for", "the", "Booker", "Prize", "five", "times", ",", "in", "1981", "for", "Midnight", "'s", "Children", ",", "1983", "for", "Shame", ",", "1988", "for", "The", "Satanic", "Verses", ",", "1995", "for", "The", "Moor", "'s", "Last", "Sigh", ",", "2019", "for", "Quichotte", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "B-book", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "B-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, poem, writer, magazine, location, event, literary genre, award, person, book and O.\nSentence: His works have been short listed for the Booker Prize five times , in 1981 for Midnight 's Children , 1983 for Shame , 1988 for The Satanic Verses , 1995 for The Moor 's Last Sigh , 2019 for Quichotte .", "prompt_labels": "His(O) works(O) have(O) been(O) short(O) listed(O) for(O) the(O) Booker(B-award) Prize(I-award) five(O) times(O) ,(O) in(O) 1981(O) for(O) Midnight(B-book) 's(I-book) Children(I-book) ,(O) 1983(O) for(O) Shame(B-book) ,(O) 1988(O) for(O) The(B-book) Satanic(I-book) Verses(I-book) ,(O) 1995(O) for(O) The(B-book) Moor(I-book) 's(I-book) Last(I-book) Sigh(I-book) ,(O) 2019(O) for(O) Quichotte(B-book) .(O)"}}
{"id": "262", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "magazine", "literary genre", "writer", "person", "event", "location", "award", "country", "organization", "book"], "instance": {"id": "262", "words": ["Romanticism", "began", "in", "Portugal", "with", "the", "publication", "of", "the", "poem", "Camões", "(", "1825", ")", ",", "by", "Almeida", "Garrett", ",", "who", "was", "raised", "by", "his", "uncle", "D.", "Alexandre", ",", "bishop", "of", "Angra", ",", "in", "the", "precepts", "of", "Neoclassicism", ",", "which", "can", "be", "observed", "in", "his", "early", "work", "."], "labels": ["B-literary genre", "O", "O", "B-country", "O", "O", "O", "O", "O", "B-literary genre", "B-book", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, magazine, literary genre, writer, person, event, location, award, country, organization, book and O.\nSentence: Romanticism began in Portugal with the publication of the poem Camões ( 1825 ) , by Almeida Garrett , who was raised by his uncle D. Alexandre , bishop of Angra , in the precepts of Neoclassicism , which can be observed in his early work .", "prompt_labels": "Romanticism(B-literary genre) began(O) in(O) Portugal(B-country) with(O) the(O) publication(O) of(O) the(O) poem(B-literary genre) Camões(B-book) ((O) 1825(O) )(O) ,(O) by(O) Almeida(B-writer) Garrett(I-writer) ,(O) who(O) was(O) raised(O) by(O) his(O) uncle(O) D.(B-person) Alexandre(I-person) ,(O) bishop(O) of(O) Angra(O) ,(O) in(O) the(O) precepts(O) of(O) Neoclassicism(B-literary genre) ,(O) which(O) can(O) be(O) observed(O) in(O) his(O) early(O) work(O) .(O)"}}
{"id": "242", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "literary genre", "poem", "person", "writer", "location", "organization", "award", "country", "magazine", "event"], "instance": {"id": "242", "words": ["This", "heartbreak", "was", "reflected", "in", "her", "early", "poetry", "and", "earned", "Mistral", "her", "first", "recognized", "literary", "work", "in", "1914", "with", "Sonetos", "de", "la", "Muerte", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "I-poem", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, literary genre, poem, person, writer, location, organization, award, country, magazine, event and O.\nSentence: This heartbreak was reflected in her early poetry and earned Mistral her first recognized literary work in 1914 with Sonetos de la Muerte .", "prompt_labels": "This(O) heartbreak(O) was(O) reflected(O) in(O) her(O) early(O) poetry(B-literary genre) and(O) earned(O) Mistral(B-writer) her(O) first(O) recognized(O) literary(O) work(O) in(O) 1914(O) with(O) Sonetos(B-poem) de(I-poem) la(I-poem) Muerte(I-poem) .(O)"}}
{"id": "40", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "literary genre", "award", "event", "poem", "writer", "location", "magazine", "book", "organization", "person"], "instance": {"id": "40", "words": ["Carmilla", "is", "an", "1872", "Gothic", "novel", "la", "by", "Irish", "author", "Sheridan", "Le", "Fanu", "and", "one", "of", "the", "early", "works", "of", "vampire", "fiction", ",", "predating", "Bram", "Stoker", "'", "s", "Dracula", "(", "1897", ")", "by", "26", "years", "."], "labels": ["B-book", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "B-writer", "I-writer", "O", "O", "B-book", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, literary genre, award, event, poem, writer, location, magazine, book, organization, person and O.\nSentence: Carmilla is an 1872 Gothic novel la by Irish author Sheridan Le Fanu and one of the early works of vampire fiction , predating Bram Stoker ' s Dracula ( 1897 ) by 26 years .", "prompt_labels": "Carmilla(B-book) is(O) an(O) 1872(O) Gothic(B-literary genre) novel(I-literary genre) la(O) by(O) Irish(O) author(O) Sheridan(B-writer) Le(I-writer) Fanu(I-writer) and(O) one(O) of(O) the(O) early(O) works(O) of(O) vampire(B-literary genre) fiction(I-literary genre) ,(O) predating(O) Bram(B-writer) Stoker(I-writer) '(O) s(O) Dracula(B-book) ((O) 1897(O) )(O) by(O) 26(O) years(O) .(O)"}}
{"id": "339", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "award", "person", "literary genre", "book", "location", "poem", "country", "writer", "magazine", "organization"], "instance": {"id": "339", "words": ["Cuban-American", "writer", "Daína", "Chaviano", "paid", "homage", "to", "Anaïs", "Nin", "and", "Henry", "Miller", "in", "her", "novel", "Gata", "encerrada", "(", "2001", ")", ",", "where", "both", "characters", "are", "portraited", "as", "disembodied", "spirits", "whose", "previous", "lives", "they", "shared", "with", "Melisa", ",", "the", "main", "character", "-", "and", "presumably", "Chaviano", "'s", "alter", "ego", "--", ",", "a", "young", "Cuban", "obsessed", "with", "Anaïs", "Nin", "."], "labels": ["O", "O", "B-writer", "I-writer", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "B-literary genre", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, person, literary genre, book, location, poem, country, writer, magazine, organization and O.\nSentence: Cuban-American writer Daína Chaviano paid homage to Anaïs Nin and Henry Miller in her novel Gata encerrada ( 2001 ) , where both characters are portraited as disembodied spirits whose previous lives they shared with Melisa , the main character - and presumably Chaviano 's alter ego -- , a young Cuban obsessed with Anaïs Nin .", "prompt_labels": "Cuban-American(O) writer(O) Daína(B-writer) Chaviano(I-writer) paid(O) homage(O) to(O) Anaïs(B-writer) Nin(I-writer) and(O) Henry(B-writer) Miller(I-writer) in(O) her(O) novel(B-literary genre) Gata(B-book) encerrada(I-book) ((O) 2001(O) )(O) ,(O) where(O) both(O) characters(O) are(O) portraited(O) as(O) disembodied(O) spirits(O) whose(O) previous(O) lives(O) they(O) shared(O) with(O) Melisa(B-person) ,(O) the(O) main(O) character(O) -(O) and(O) presumably(O) Chaviano(B-person) 's(O) alter(O) ego(O) --(O) ,(O) a(O) young(O) Cuban(O) obsessed(O) with(O) Anaïs(B-writer) Nin(I-writer) .(O)"}}
{"id": "267", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "magazine", "country", "event", "person", "location", "award", "organization", "poem", "book", "literary genre"], "instance": {"id": "267", "words": ["The", "episode", "depicts", "the", "attempted", "assassination", "of", "Warhol", "by", "Valerie", "Solanas", "(", "Lena", "Dunham", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "B-writer", "I-writer", "O", "B-person", "I-person", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, magazine, country, event, person, location, award, organization, poem, book, literary genre and O.\nSentence: The episode depicts the attempted assassination of Warhol by Valerie Solanas ( Lena Dunham ) .", "prompt_labels": "The(O) episode(O) depicts(O) the(O) attempted(O) assassination(O) of(O) Warhol(B-writer) by(O) Valerie(B-writer) Solanas(I-writer) ((O) Lena(B-person) Dunham(I-person) )(O) .(O)"}}
{"id": "60", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "organization", "literary genre", "writer", "person", "book", "poem", "location", "award", "magazine", "country"], "instance": {"id": "60", "words": ["As", "a", "writer", ",", "Orwell", "produced", "literary", "criticism", "and", "poetry", ",", "fiction", "and", "polemic", "al", "journalism", ";", "and", "is", "best", "known", "for", "the", "allegorical", "novella", "Animal", "Farm", "(", "1945", ")", "and", "the", "dystopian", "novel", "Nineteen", "Eighty-Four", "(", "1949", ")", "."], "labels": ["O", "O", "O", "O", "B-writer", "O", "B-literary genre", "I-literary genre", "O", "B-literary genre", "O", "B-literary genre", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "B-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, literary genre, writer, person, book, poem, location, award, magazine, country and O.\nSentence: As a writer , Orwell produced literary criticism and poetry , fiction and polemic al journalism ; and is best known for the allegorical novella Animal Farm ( 1945 ) and the dystopian novel Nineteen Eighty-Four ( 1949 ) .", "prompt_labels": "As(O) a(O) writer(O) ,(O) Orwell(B-writer) produced(O) literary(B-literary genre) criticism(I-literary genre) and(O) poetry(B-literary genre) ,(O) fiction(B-literary genre) and(O) polemic(B-literary genre) al(O) journalism(O) ;(O) and(O) is(O) best(O) known(O) for(O) the(O) allegorical(O) novella(O) Animal(B-book) Farm(I-book) ((O) 1945(O) )(O) and(O) the(O) dystopian(B-literary genre) novel(I-literary genre) Nineteen(B-book) Eighty-Four(I-book) ((O) 1949(O) )(O) .(O)"}}
{"id": "390", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "poem", "literary genre", "location", "person", "event", "country", "organization", "book", "magazine", "writer"], "instance": {"id": "390", "words": ["Aung", "San", "Suu", "Kyi", "(", ";", "born", "19", "June", "1945", ")", "is", "a", "Myanmar", "politician", ",", "diplomat", ",", "author", ",", "and", "a", "1991", "Nobel", "Peace", "Prize", "laureate", "."], "labels": ["B-writer", "I-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, poem, literary genre, location, person, event, country, organization, book, magazine, writer and O.\nSentence: Aung San Suu Kyi ( ; born 19 June 1945 ) is a Myanmar politician , diplomat , author , and a 1991 Nobel Peace Prize laureate .", "prompt_labels": "Aung(B-writer) San(I-writer) Suu(I-writer) Kyi(I-writer) ((O) ;(O) born(O) 19(O) June(O) 1945(O) )(O) is(O) a(O) Myanmar(B-country) politician(O) ,(O) diplomat(O) ,(O) author(O) ,(O) and(O) a(O) 1991(O) Nobel(B-award) Peace(I-award) Prize(I-award) laureate(O) .(O)"}}
{"id": "253", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "poem", "magazine", "event", "location", "book", "award", "organization", "literary genre", "person", "country"], "instance": {"id": "253", "words": ["Nurturing", "an", "image", "of", "her", "own", "country", "as", "a", "haven", "of", "social", "peace", "and", "prosperity", ",", "she", "threatened", "to", "boycott", "the", "1955", "Venice", "Film", "Festival", "if", "the", "American", "juvenile", "delinquent", "film", "Blackboard", "Jungle", "was", "shown", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, poem, magazine, event, location, book, award, organization, literary genre, person, country and O.\nSentence: Nurturing an image of her own country as a haven of social peace and prosperity , she threatened to boycott the 1955 Venice Film Festival if the American juvenile delinquent film Blackboard Jungle was shown .", "prompt_labels": "Nurturing(O) an(O) image(O) of(O) her(O) own(O) country(O) as(O) a(O) haven(O) of(O) social(O) peace(O) and(O) prosperity(O) ,(O) she(O) threatened(O) to(O) boycott(O) the(O) 1955(O) Venice(B-event) Film(I-event) Festival(I-event) if(O) the(O) American(O) juvenile(O) delinquent(O) film(O) Blackboard(O) Jungle(O) was(O) shown(O) .(O)"}}
{"id": "180", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "location", "book", "award", "event", "organization", "literary genre", "person", "poem", "magazine", "country"], "instance": {"id": "180", "words": ["The", "English", "Patient", "(", "1992", ")", "won", "the", "Booker", "Prize", ",", "the", "Canada", "Australia", "Prize", ",", "and", "the", "Governor", "General", "'s", "Award", "."], "labels": ["B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, location, book, award, event, organization, literary genre, person, poem, magazine, country and O.\nSentence: The English Patient ( 1992 ) won the Booker Prize , the Canada Australia Prize , and the Governor General 's Award .", "prompt_labels": "The(B-book) English(I-book) Patient(I-book) ((O) 1992(O) )(O) won(O) the(O) Booker(B-award) Prize(I-award) ,(O) the(O) Canada(B-award) Australia(I-award) Prize(I-award) ,(O) and(O) the(O) Governor(B-award) General(I-award) 's(I-award) Award(I-award) .(O)"}}
{"id": "412", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "country", "literary genre", "poem", "book", "writer", "location", "magazine", "organization", "person", "award"], "instance": {"id": "412", "words": ["Oliver", "Twist", "and", "Great", "Expectations", "are", "also", "frequently", "adapted", "and", ",", "like", "many", "of", "his", "novels", ",", "evoke", "images", "of", "early", "Victorian", "London", "."], "labels": ["B-book", "I-book", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, literary genre, poem, book, writer, location, magazine, organization, person, award and O.\nSentence: Oliver Twist and Great Expectations are also frequently adapted and , like many of his novels , evoke images of early Victorian London .", "prompt_labels": "Oliver(B-book) Twist(I-book) and(O) Great(B-book) Expectations(I-book) are(O) also(O) frequently(O) adapted(O) and(O) ,(O) like(O) many(O) of(O) his(O) novels(B-literary genre) ,(O) evoke(O) images(O) of(O) early(O) Victorian(B-location) London(I-location) .(O)"}}
{"id": "363", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "event", "award", "magazine", "location", "organization", "literary genre", "country", "book", "writer", "poem"], "instance": {"id": "363", "words": ["Gathering", "widespread", "critical", "acclaim", "at", "the", "Toronto", "and", "New", "York", "Film", "Festival", "film", "festivals", ",", "the", "film", "also", "became", "a", "favorite", "when", "Academy", "Awards", "nominations", "were", "announced", "in", "2001", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, award, magazine, location, organization, literary genre, country, book, writer, poem and O.\nSentence: Gathering widespread critical acclaim at the Toronto and New York Film Festival film festivals , the film also became a favorite when Academy Awards nominations were announced in 2001 .", "prompt_labels": "Gathering(O) widespread(O) critical(O) acclaim(O) at(O) the(O) Toronto(B-event) and(I-event) New(I-event) York(I-event) Film(I-event) Festival(I-event) film(O) festivals(O) ,(O) the(O) film(O) also(O) became(O) a(O) favorite(O) when(O) Academy(B-award) Awards(I-award) nominations(O) were(O) announced(O) in(O) 2001(O) .(O)"}}
{"id": "123", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "magazine", "literary genre", "writer", "award", "event", "country", "poem", "organization", "person", "location"], "instance": {"id": "123", "words": ["However", ",", "in", "late", "1970", ",", "Pauline", "Kael", ",", "in", "her", "negative", "The", "New", "Yorker", "review", "of", "the", "Maysles", "'", "subsequent", "documentary", "Gimme", "Shelter", ",", "alleged", "that", "Salesman", "was", "set", "up", "and", "acted", "by", "its", "principals", ",", "rather", "than", "actually", "being", "direct", "cinema", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, magazine, literary genre, writer, award, event, country, poem, organization, person, location and O.\nSentence: However , in late 1970 , Pauline Kael , in her negative The New Yorker review of the Maysles ' subsequent documentary Gimme Shelter , alleged that Salesman was set up and acted by its principals , rather than actually being direct cinema .", "prompt_labels": "However(O) ,(O) in(O) late(O) 1970(O) ,(O) Pauline(B-writer) Kael(I-writer) ,(O) in(O) her(O) negative(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) review(O) of(O) the(O) Maysles(B-person) '(O) subsequent(O) documentary(O) Gimme(O) Shelter(O) ,(O) alleged(O) that(O) Salesman(O) was(O) set(O) up(O) and(O) acted(O) by(O) its(O) principals(O) ,(O) rather(O) than(O) actually(O) being(O) direct(O) cinema(O) .(O)"}}
{"id": "52", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "book", "magazine", "poem", "award", "literary genre", "location", "event", "writer", "person", "country"], "instance": {"id": "52", "words": ["Powers", "'", "first", "major", "novel", "was", "The", "Drawing", "of", "the", "Dark", "(", "1979", ")", ",", "but", "the", "novel", "that", "earned", "him", "wide", "praise", "was", "The", "Anubis", "Gates", ",", "which", "won", "the", "Philip", "K.", "Dick", "Award", ",", "and", "has", "since", "been", "published", "in", "many", "other", "languages", "."], "labels": ["B-writer", "O", "O", "O", "B-literary genre", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, book, magazine, poem, award, literary genre, location, event, writer, person, country and O.\nSentence: Powers ' first major novel was The Drawing of the Dark ( 1979 ) , but the novel that earned him wide praise was The Anubis Gates , which won the Philip K. Dick Award , and has since been published in many other languages .", "prompt_labels": "Powers(B-writer) '(O) first(O) major(O) novel(B-literary genre) was(O) The(B-book) Drawing(I-book) of(I-book) the(I-book) Dark(I-book) ((O) 1979(O) )(O) ,(O) but(O) the(O) novel(B-literary genre) that(O) earned(O) him(O) wide(O) praise(O) was(O) The(B-book) Anubis(I-book) Gates(I-book) ,(O) which(O) won(O) the(O) Philip(B-award) K.(I-award) Dick(I-award) Award(I-award) ,(O) and(O) has(O) since(O) been(O) published(O) in(O) many(O) other(O) languages(O) .(O)"}}
{"id": "381", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "location", "literary genre", "poem", "organization", "writer", "event", "person", "country", "book", "magazine"], "instance": {"id": "381", "words": ["His", "first", "published", "novels", "were", "social", "satires", ",", "Crome", "Yellow", "(", "1921", ")", ",", "Antic", "Hay", "(", "1923", ")", ",", "Those", "Barren", "Leaves", "(", "1925", ")", ",", "and", "Point", "Counter", "Point", "(", "1928", ")", "."], "labels": ["O", "O", "O", "B-literary genre", "O", "B-literary genre", "I-literary genre", "O", "B-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, literary genre, poem, organization, writer, event, person, country, book, magazine and O.\nSentence: His first published novels were social satires , Crome Yellow ( 1921 ) , Antic Hay ( 1923 ) , Those Barren Leaves ( 1925 ) , and Point Counter Point ( 1928 ) .", "prompt_labels": "His(O) first(O) published(O) novels(B-literary genre) were(O) social(B-literary genre) satires(I-literary genre) ,(O) Crome(B-book) Yellow(I-book) ((O) 1921(O) )(O) ,(O) Antic(B-book) Hay(I-book) ((O) 1923(O) )(O) ,(O) Those(B-book) Barren(I-book) Leaves(I-book) ((O) 1925(O) )(O) ,(O) and(O) Point(B-book) Counter(I-book) Point(I-book) ((O) 1928(O) )(O) .(O)"}}
{"id": "236", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "book", "event", "award", "person", "location", "country", "writer", "poem", "magazine", "literary genre"], "instance": {"id": "236", "words": ["Shot", "in", "January", ",", "it", "premièred", "at", "the", "Cannes", "Film", "Festival", "on", "27", "May", "2007", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, book, event, award, person, location, country, writer, poem, magazine, literary genre and O.\nSentence: Shot in January , it premièred at the Cannes Film Festival on 27 May 2007 .", "prompt_labels": "Shot(O) in(O) January(O) ,(O) it(O) premièred(O) at(O) the(O) Cannes(B-event) Film(I-event) Festival(I-event) on(O) 27(O) May(O) 2007(O) .(O)"}}
{"id": "394", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "location", "organization", "person", "literary genre", "writer", "book", "award", "country", "poem", "magazine"], "instance": {"id": "394", "words": ["1969", ",", "Crichton", "also", "wrote", "a", "review", "for", "The", "New", "Republic", "(", "as", "J.", "Michael", "Crichton", ")", ",", "critiquing", "Slaughterhouse", "Five", "by", "Kurt", "Vonnegut", "."], "labels": ["O", "O", "B-writer", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "B-book", "I-book", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, organization, person, literary genre, writer, book, award, country, poem, magazine and O.\nSentence: 1969 , Crichton also wrote a review for The New Republic ( as J. Michael Crichton ) , critiquing Slaughterhouse Five by Kurt Vonnegut .", "prompt_labels": "1969(O) ,(O) Crichton(B-writer) also(O) wrote(O) a(O) review(O) for(O) The(B-magazine) New(I-magazine) Republic(I-magazine) ((O) as(O) J.(B-writer) Michael(I-writer) Crichton(I-writer) )(O) ,(O) critiquing(O) Slaughterhouse(B-book) Five(I-book) by(O) Kurt(B-writer) Vonnegut(I-writer) .(O)"}}
{"id": "140", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "event", "country", "person", "award", "poem", "writer", "book", "magazine", "location", "organization"], "instance": {"id": "140", "words": ["Akhundzade", "'s", "first", "published", "work", "was", "Eastern", "poem", "on", "the", "death", "of", "Pushkin", "(", "1837", ")", ",", "written", "to", "lament", "the", "death", "of", "the", "great", "Russian", "poet", "Alexander", "Pushkin", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, event, country, person, award, poem, writer, book, magazine, location, organization and O.\nSentence: Akhundzade 's first published work was Eastern poem on the death of Pushkin ( 1837 ) , written to lament the death of the great Russian poet Alexander Pushkin .", "prompt_labels": "Akhundzade(B-writer) 's(O) first(O) published(O) work(O) was(O) Eastern(B-poem) poem(I-poem) on(I-poem) the(I-poem) death(I-poem) of(I-poem) Pushkin(I-poem) ((O) 1837(O) )(O) ,(O) written(O) to(O) lament(O) the(O) death(O) of(O) the(O) great(O) Russian(O) poet(O) Alexander(B-writer) Pushkin(I-writer) .(O)"}}
{"id": "406", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "organization", "book", "magazine", "person", "event", "location", "poem", "literary genre", "country", "award"], "instance": {"id": "406", "words": ["Other", "signers", "and", "RESIST", "members", "included", "Mitchell", "Goodman", ",", "Henry", "Braun", ",", "Denise", "Levertov", ",", "Noam", "Chomsky", ",", "William", "Sloane", "Coffin", ",", "Dwight", "Macdonald", ",", "Robert", "Lowell", ",", "and", "Norman", "Mailer", ".", "Barsky", ",", "Robert", "F.", "(", "1998", ")", "as", "a", "form", "of", "anti-war", "protest", "."], "labels": ["O", "O", "O", "B-organization", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "B-writer", "I-writer", "O", "B-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, organization, book, magazine, person, event, location, poem, literary genre, country, award and O.\nSentence: Other signers and RESIST members included Mitchell Goodman , Henry Braun , Denise Levertov , Noam Chomsky , William Sloane Coffin , Dwight Macdonald , Robert Lowell , and Norman Mailer . Barsky , Robert F. ( 1998 ) as a form of anti-war protest .", "prompt_labels": "Other(O) signers(O) and(O) RESIST(B-organization) members(O) included(O) Mitchell(B-writer) Goodman(I-writer) ,(O) Henry(B-writer) Braun(I-writer) ,(O) Denise(B-writer) Levertov(I-writer) ,(O) Noam(B-writer) Chomsky(I-writer) ,(O) William(B-writer) Sloane(I-writer) Coffin(I-writer) ,(O) Dwight(B-writer) Macdonald(I-writer) ,(O) Robert(B-writer) Lowell(I-writer) ,(O) and(O) Norman(B-writer) Mailer(I-writer) .(O) Barsky(B-writer) ,(O) Robert(B-writer) F.(I-writer) ((O) 1998(O) )(O) as(O) a(O) form(O) of(O) anti-war(B-event) protest(I-event) .(O)"}}
{"id": "373", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "country", "writer", "award", "organization", "poem", "literary genre", "location", "book", "person", "event"], "instance": {"id": "373", "words": ["His", "fictional", "work", ",", "Redburn", "(", "1849", ")", ",", "and", "his", "non-fiction", "White-Jacket", "(", "1850", ")", "were", "given", "better", "reviews", "but", "did", "not", "provide", "financial", "security", "."], "labels": ["O", "B-literary genre", "I-literary genre", "O", "B-book", "O", "O", "O", "O", "O", "O", "O", "B-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, country, writer, award, organization, poem, literary genre, location, book, person, event and O.\nSentence: His fictional work , Redburn ( 1849 ) , and his non-fiction White-Jacket ( 1850 ) were given better reviews but did not provide financial security .", "prompt_labels": "His(O) fictional(B-literary genre) work(I-literary genre) ,(O) Redburn(B-book) ((O) 1849(O) )(O) ,(O) and(O) his(O) non-fiction(O) White-Jacket(B-book) ((O) 1850(O) )(O) were(O) given(O) better(O) reviews(O) but(O) did(O) not(O) provide(O) financial(O) security(O) .(O)"}}
{"id": "26", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "literary genre", "event", "location", "country", "book", "award", "person", "organization", "writer", "poem"], "instance": {"id": "26", "words": ["Kim", "Stanley", "Robinson", "'", "s", "novel", ",", "The", "Years", "of", "Rice", "and", "Salt", "(", "2002", ")", ",", "starts", "at", "the", "point", "of", "divergence", "with", "Timur", "turning", "his", "army", "away", "from", "Europe", ",", "and", "the", "Black", "Death", "has", "killed", "99", "%", "of", "Europe", "'s", "population", ",", "instead", "of", "only", "a", "third", "."], "labels": ["B-writer", "I-writer", "I-writer", "O", "O", "B-literary genre", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, literary genre, event, location, country, book, award, person, organization, writer, poem and O.\nSentence: Kim Stanley Robinson ' s novel , The Years of Rice and Salt ( 2002 ) , starts at the point of divergence with Timur turning his army away from Europe , and the Black Death has killed 99 % of Europe 's population , instead of only a third .", "prompt_labels": "Kim(B-writer) Stanley(I-writer) Robinson(I-writer) '(O) s(O) novel(B-literary genre) ,(O) The(B-book) Years(I-book) of(I-book) Rice(I-book) and(I-book) Salt(I-book) ((O) 2002(O) )(O) ,(O) starts(O) at(O) the(O) point(O) of(O) divergence(O) with(O) Timur(B-person) turning(O) his(O) army(O) away(O) from(O) Europe(B-location) ,(O) and(O) the(O) Black(O) Death(O) has(O) killed(O) 99(O) %(O) of(O) Europe(B-location) 's(O) population(O) ,(O) instead(O) of(O) only(O) a(O) third(O) .(O)"}}
{"id": "115", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "location", "literary genre", "writer", "magazine", "person", "award", "event", "poem", "country", "book"], "instance": {"id": "115", "words": ["Adapted", "by", "Harry", "Behn", "from", "the", "autobiographical", "novel", "Plumes", "by", "Laurence", "Stallings", ",", "with", "titles", "by", "Joseph", "W.", "Farnham", ",", "the", "film", "is", "about", "an", "idle", "rich", "boy", "who", "joins", "the", "US", "Army", "'", "s", "Rainbow", "Division", "and", "is", "sent", "to", "France", "to", "fight", "in", "World", "War", "I", ",", "becomes", "a", "friend", "of", "two", "working", "class", "men", ",", "experiences", "the", "horrors", "of", "trench", "warfare", ",", "and", "finds", "love", "with", "a", "French", "girl", "."], "labels": ["O", "O", "B-writer", "I-writer", "O", "O", "B-literary genre", "I-literary genre", "B-book", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, literary genre, writer, magazine, person, award, event, poem, country, book and O.\nSentence: Adapted by Harry Behn from the autobiographical novel Plumes by Laurence Stallings , with titles by Joseph W. Farnham , the film is about an idle rich boy who joins the US Army ' s Rainbow Division and is sent to France to fight in World War I , becomes a friend of two working class men , experiences the horrors of trench warfare , and finds love with a French girl .", "prompt_labels": "Adapted(O) by(O) Harry(B-writer) Behn(I-writer) from(O) the(O) autobiographical(B-literary genre) novel(I-literary genre) Plumes(B-book) by(O) Laurence(B-writer) Stallings(I-writer) ,(O) with(O) titles(O) by(O) Joseph(B-writer) W.(I-writer) Farnham(I-writer) ,(O) the(O) film(O) is(O) about(O) an(O) idle(O) rich(O) boy(O) who(O) joins(O) the(O) US(O) Army(O) '(O) s(O) Rainbow(O) Division(O) and(O) is(O) sent(O) to(O) France(B-country) to(O) fight(O) in(O) World(B-event) War(I-event) I(I-event) ,(O) becomes(O) a(O) friend(O) of(O) two(O) working(O) class(O) men(O) ,(O) experiences(O) the(O) horrors(O) of(O) trench(O) warfare(O) ,(O) and(O) finds(O) love(O) with(O) a(O) French(O) girl(O) .(O)"}}
{"id": "367", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "literary genre", "location", "organization", "magazine", "person", "writer", "award", "country", "event", "book"], "instance": {"id": "367", "words": ["Some", "historians", "such", "as", "John", "Julius", "Norwich", ",", "despite", "their", "admiration", "for", "his", "furthering", "of", "historical", "methodology", ",", "consider", "Gibbon", "'s", "hostile", "views", "on", "the", "Byzantine", "Empire", "flawed", "and", "blame", "him", "somewhat", "for", "the", "lack", "of", "interest", "shown", "in", "the", "subject", "throughout", "the", "19th", "and", "early", "20th", "centuries.", "John", "Julius", "Norwich", ",", "Byzantium", "(", "New", "York", ":", "Knopf", ",", "1989", ")", ";", "Byzantium", ":", "the", "apogee", "(", "London", "and", "New", "York", ":", "Viking", "Press", ",", "1991", ")", "."], "labels": ["O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "B-location", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "B-location", "O", "B-location", "I-location", "O", "B-organization", "I-organization", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, literary genre, location, organization, magazine, person, writer, award, country, event, book and O.\nSentence: Some historians such as John Julius Norwich , despite their admiration for his furthering of historical methodology , consider Gibbon 's hostile views on the Byzantine Empire flawed and blame him somewhat for the lack of interest shown in the subject throughout the 19th and early 20th centuries. John Julius Norwich , Byzantium ( New York : Knopf , 1989 ) ; Byzantium : the apogee ( London and New York : Viking Press , 1991 ) .", "prompt_labels": "Some(O) historians(O) such(O) as(O) John(B-writer) Julius(I-writer) Norwich(I-writer) ,(O) despite(O) their(O) admiration(O) for(O) his(O) furthering(O) of(O) historical(O) methodology(O) ,(O) consider(O) Gibbon(B-writer) 's(O) hostile(O) views(O) on(O) the(O) Byzantine(B-country) Empire(I-country) flawed(O) and(O) blame(O) him(O) somewhat(O) for(O) the(O) lack(O) of(O) interest(O) shown(O) in(O) the(O) subject(O) throughout(O) the(O) 19th(O) and(O) early(O) 20th(O) centuries.(O) John(B-writer) Julius(I-writer) Norwich(I-writer) ,(O) Byzantium(B-location) ((O) New(B-location) York(I-location) :(O) Knopf(O) ,(O) 1989(O) )(O) ;(O) Byzantium(B-location) :(O) the(O) apogee(O) ((O) London(B-location) and(O) New(B-location) York(I-location) :(O) Viking(B-organization) Press(I-organization) ,(O) 1991(O) )(O) .(O)"}}
{"id": "63", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "award", "magazine", "poem", "organization", "event", "location", "literary genre", "book", "person", "country"], "instance": {"id": "63", "words": ["The", "existing", "repertoire", "of", "Scottish-themed", "plays", "included", "John", "Home", "'", "s", "Douglas", "(", "1756", ")", "and", "Allan", "Ramsay", "'", "s", "The", "Gentle", "Shepherd", "(", "1725", ")", ",", "with", "the", "last", "two", "being", "the", "most", "popular", "plays", "among", "amateur", "groups", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-poem", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-poem", "I-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, award, magazine, poem, organization, event, location, literary genre, book, person, country and O.\nSentence: The existing repertoire of Scottish-themed plays included John Home ' s Douglas ( 1756 ) and Allan Ramsay ' s The Gentle Shepherd ( 1725 ) , with the last two being the most popular plays among amateur groups .", "prompt_labels": "The(O) existing(O) repertoire(O) of(O) Scottish-themed(O) plays(O) included(O) John(B-writer) Home(I-writer) '(O) s(O) Douglas(B-poem) ((O) 1756(O) )(O) and(O) Allan(B-writer) Ramsay(I-writer) '(O) s(O) The(B-poem) Gentle(I-poem) Shepherd(I-poem) ((O) 1725(O) )(O) ,(O) with(O) the(O) last(O) two(O) being(O) the(O) most(O) popular(O) plays(O) among(O) amateur(O) groups(O) .(O)"}}
{"id": "27", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "writer", "organization", "poem", "literary genre", "award", "magazine", "event", "location", "country", "person"], "instance": {"id": "27", "words": ["H.", "G.", "Wells", "praised", "Growth", "of", "the", "Soil", "(", "1917", ")", "for", "which", "Hamsun", "was", "awarded", "the", "Nobel", "Prize", "in", "Literature", "."], "labels": ["B-writer", "I-writer", "I-writer", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, writer, organization, poem, literary genre, award, magazine, event, location, country, person and O.\nSentence: H. G. Wells praised Growth of the Soil ( 1917 ) for which Hamsun was awarded the Nobel Prize in Literature .", "prompt_labels": "H.(B-writer) G.(I-writer) Wells(I-writer) praised(O) Growth(B-book) of(I-book) the(I-book) Soil(I-book) ((O) 1917(O) )(O) for(O) which(O) Hamsun(B-writer) was(O) awarded(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) .(O)"}}
{"id": "224", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "literary genre", "person", "event", "location", "magazine", "book", "writer", "poem", "organization", "country"], "instance": {"id": "224", "words": ["In", "1998", ",", "he", "was", "a", "member", "of", "the", "jury", "at", "the", "48th", "Berlin", "International", "Film", "Festival", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, literary genre, person, event, location, magazine, book, writer, poem, organization, country and O.\nSentence: In 1998 , he was a member of the jury at the 48th Berlin International Film Festival .", "prompt_labels": "In(O) 1998(O) ,(O) he(O) was(O) a(O) member(O) of(O) the(O) jury(O) at(O) the(O) 48th(B-event) Berlin(I-event) International(I-event) Film(I-event) Festival(I-event) .(O)"}}
{"id": "232", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "person", "writer", "book", "literary genre", "poem", "event", "location", "award", "organization", "magazine"], "instance": {"id": "232", "words": ["During", "his", "life", ",", "Cocteau", "was", "commander", "of", "the", "Legion", "of", "Honor", ",", "Member", "of", "the", "Mallarmé", "Academy", ",", "German", "Academy", "(", "Berlin", ")", ",", "American", "Academy", ",", "Mark", "Twain", "(", "U.S.A", ")", "Academy", ",", "Honorary", "President", "of", "the", "Cannes", "Film", "Festival", ",", "Honorary", "President", "of", "the", "France-Hungary", "Association", "and", "President", "of", "the", "Jazz", "Academy", "and", "of", "the", "Academy", "of", "the", "Disc", "."], "labels": ["O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-location", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, writer, book, literary genre, poem, event, location, award, organization, magazine and O.\nSentence: During his life , Cocteau was commander of the Legion of Honor , Member of the Mallarmé Academy , German Academy ( Berlin ) , American Academy , Mark Twain ( U.S.A ) Academy , Honorary President of the Cannes Film Festival , Honorary President of the France-Hungary Association and President of the Jazz Academy and of the Academy of the Disc .", "prompt_labels": "During(O) his(O) life(O) ,(O) Cocteau(B-writer) was(O) commander(O) of(O) the(O) Legion(B-organization) of(I-organization) Honor(I-organization) ,(O) Member(O) of(O) the(O) Mallarmé(B-organization) Academy(I-organization) ,(O) German(B-organization) Academy(I-organization) ((O) Berlin(B-location) )(O) ,(O) American(B-organization) Academy(I-organization) ,(O) Mark(B-organization) Twain(I-organization) ((I-organization) U.S.A(I-organization) )(I-organization) Academy(I-organization) ,(O) Honorary(O) President(O) of(O) the(O) Cannes(B-event) Film(I-event) Festival(I-event) ,(O) Honorary(O) President(O) of(O) the(O) France-Hungary(B-organization) Association(I-organization) and(O) President(O) of(O) the(O) Jazz(B-organization) Academy(I-organization) and(O) of(O) the(O) Academy(B-organization) of(I-organization) the(I-organization) Disc(I-organization) .(O)"}}
{"id": "74", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "person", "award", "literary genre", "location", "writer", "country", "organization", "event", "magazine", "poem"], "instance": {"id": "74", "words": ["Alternate", "history", "has", "long", "been", "a", "staple", "of", "Japanese", "speculative", "fiction", "with", "such", "authors", "as", "Futaro", "Yamada", "and", "Ryō", "Hanmura", "writing", "novels", "set", "in", "recognizable", "historical", "settings", "with", "supernatural", "or", "science", "fiction", "elements", "present", "."], "labels": ["B-literary genre", "I-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "B-literary genre", "I-literary genre", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, person, award, literary genre, location, writer, country, organization, event, magazine, poem and O.\nSentence: Alternate history has long been a staple of Japanese speculative fiction with such authors as Futaro Yamada and Ryō Hanmura writing novels set in recognizable historical settings with supernatural or science fiction elements present .", "prompt_labels": "Alternate(B-literary genre) history(I-literary genre) has(O) long(O) been(O) a(O) staple(O) of(O) Japanese(O) speculative(O) fiction(B-literary genre) with(O) such(O) authors(O) as(O) Futaro(B-writer) Yamada(I-writer) and(O) Ryō(B-writer) Hanmura(I-writer) writing(O) novels(B-literary genre) set(O) in(O) recognizable(O) historical(O) settings(O) with(O) supernatural(B-literary genre) or(O) science(B-literary genre) fiction(I-literary genre) elements(O) present(O) .(O)"}}
{"id": "233", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "writer", "literary genre", "event", "magazine", "organization", "location", "book", "person", "country", "award"], "instance": {"id": "233", "words": ["After", "making", "a", "series", "of", "westerns", "and", "comedies", ",", "Dwan", "directed", "fellow", "Canadian-American", "Mary", "Pickford", "in", "several", "very", "successful", "movies", "as", "well", "as", "her", "husband", ",", "Douglas", "Fairbanks", ",", "notably", "in", "the", "acclaimed", "1922", "Robin", "Hood", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, writer, literary genre, event, magazine, organization, location, book, person, country, award and O.\nSentence: After making a series of westerns and comedies , Dwan directed fellow Canadian-American Mary Pickford in several very successful movies as well as her husband , Douglas Fairbanks , notably in the acclaimed 1922 Robin Hood .", "prompt_labels": "After(O) making(O) a(O) series(O) of(O) westerns(O) and(O) comedies(O) ,(O) Dwan(B-person) directed(O) fellow(O) Canadian-American(O) Mary(B-person) Pickford(I-person) in(O) several(O) very(O) successful(O) movies(O) as(O) well(O) as(O) her(O) husband(O) ,(O) Douglas(B-person) Fairbanks(I-person) ,(O) notably(O) in(O) the(O) acclaimed(O) 1922(O) Robin(O) Hood(O) .(O)"}}
{"id": "371", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "book", "poem", "person", "location", "event", "writer", "country", "organization", "magazine", "award"], "instance": {"id": "371", "words": ["Celebrated", "Puerto", "Rican", "novelists", "who", "write", "in", "English", "and", "Spanish", "include", "Giannina", "Braschi", ",", "author", "of", "the", "Spanglish", "classic", "Yo-Yo", "Boing", "!", "and", "Rosario", "Ferré", ",", "best", "known", "for", "Eccentric", "Neighborhoods", "Puerto", "Rico", "has", "also", "produced", "important", "playwrights", "such", "as", "René", "Marqués", ",", "Luis", "Rafael", "Sánchez", ",", "and", "José", "Rivera", "and", "New", "York", "based", "poets", "such", "as", "Julia", "de", "Burgos", ",", "Giannina", "Braschi", "and", "Pedro", "Pietri", ",", "as", "well", "as", "various", "members", "of", "the", "Nuyorican", "Poets", "Café", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-book", "I-book", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-writer", "I-writer", "O", "B-location", "I-location", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, book, poem, person, location, event, writer, country, organization, magazine, award and O.\nSentence: Celebrated Puerto Rican novelists who write in English and Spanish include Giannina Braschi , author of the Spanglish classic Yo-Yo Boing ! and Rosario Ferré , best known for Eccentric Neighborhoods Puerto Rico has also produced important playwrights such as René Marqués , Luis Rafael Sánchez , and José Rivera and New York based poets such as Julia de Burgos , Giannina Braschi and Pedro Pietri , as well as various members of the Nuyorican Poets Café .", "prompt_labels": "Celebrated(O) Puerto(O) Rican(O) novelists(O) who(O) write(O) in(O) English(O) and(O) Spanish(O) include(O) Giannina(B-writer) Braschi(I-writer) ,(O) author(O) of(O) the(O) Spanglish(O) classic(O) Yo-Yo(B-book) Boing(I-book) !(I-book) and(O) Rosario(B-writer) Ferré(I-writer) ,(O) best(O) known(O) for(O) Eccentric(B-book) Neighborhoods(I-book) Puerto(B-location) Rico(I-location) has(O) also(O) produced(O) important(O) playwrights(O) such(O) as(O) René(B-writer) Marqués(I-writer) ,(O) Luis(B-writer) Rafael(I-writer) Sánchez(I-writer) ,(O) and(O) José(B-writer) Rivera(I-writer) and(O) New(B-location) York(I-location) based(O) poets(O) such(O) as(O) Julia(B-writer) de(I-writer) Burgos(I-writer) ,(O) Giannina(B-writer) Braschi(I-writer) and(O) Pedro(B-writer) Pietri(I-writer) ,(O) as(O) well(O) as(O) various(O) members(O) of(O) the(O) Nuyorican(B-location) Poets(I-location) Café(I-location) .(O)"}}
{"id": "235", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "person", "award", "writer", "country", "location", "book", "magazine", "event", "literary genre", "organization"], "instance": {"id": "235", "words": ["On", "the", "DVD", "commentary", "for", "the", "1969", "movie", ",", "Guthrie", "stated", "that", "the", "events", "presented", "in", "the", "song", "all", "actually", "happened", "(", "others", ",", "such", "as", "the", "arresting", "officer", ",", "William", "Obanhein", ",", "disputed", "some", "of", "the", "song", "'s", "details", ",", "Saul", "Braun", ",", "Alice", "&", "Ray", "&", "Yesterday", "'s", "Flowers", ",", "in", "Playboy", "'", "s", "Music", "Scene", ",", "Chicago", ",", "IL", ",", "1972", ",", "pp.", "122-125", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, person, award, writer, country, location, book, magazine, event, literary genre, organization and O.\nSentence: On the DVD commentary for the 1969 movie , Guthrie stated that the events presented in the song all actually happened ( others , such as the arresting officer , William Obanhein , disputed some of the song 's details , Saul Braun , Alice & Ray & Yesterday 's Flowers , in Playboy ' s Music Scene , Chicago , IL , 1972 , pp. 122-125 .", "prompt_labels": "On(O) the(O) DVD(O) commentary(O) for(O) the(O) 1969(O) movie(O) ,(O) Guthrie(B-writer) stated(O) that(O) the(O) events(O) presented(O) in(O) the(O) song(O) all(O) actually(O) happened(O) ((O) others(O) ,(O) such(O) as(O) the(O) arresting(O) officer(O) ,(O) William(B-person) Obanhein(I-person) ,(O) disputed(O) some(O) of(O) the(O) song(O) 's(O) details(O) ,(O) Saul(B-person) Braun(I-person) ,(O) Alice(O) &(O) Ray(O) &(O) Yesterday(O) 's(O) Flowers(O) ,(O) in(O) Playboy(B-magazine) '(O) s(O) Music(O) Scene(O) ,(O) Chicago(B-location) ,(O) IL(B-location) ,(O) 1972(O) ,(O) pp.(O) 122-125(O) .(O)"}}
{"id": "261", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "country", "writer", "person", "organization", "poem", "book", "award", "literary genre", "event", "location"], "instance": {"id": "261", "words": ["The", "novels", "The", "Book", "of", "Ptath", "and", "The", "Weapon", "Makers", "both", "appeared", "in", "magazines", "in", "serial", "form", "during", "this", "era", ";", "they", "were", "later", "published", "in", "book", "form", "after", "World", "War", "II", "."], "labels": ["O", "B-literary genre", "B-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, country, writer, person, organization, poem, book, award, literary genre, event, location and O.\nSentence: The novels The Book of Ptath and The Weapon Makers both appeared in magazines in serial form during this era ; they were later published in book form after World War II .", "prompt_labels": "The(O) novels(B-literary genre) The(B-book) Book(I-book) of(I-book) Ptath(I-book) and(O) The(B-book) Weapon(I-book) Makers(I-book) both(O) appeared(O) in(O) magazines(O) in(O) serial(O) form(O) during(O) this(O) era(O) ;(O) they(O) were(O) later(O) published(O) in(O) book(O) form(O) after(O) World(B-event) War(I-event) II(I-event) .(O)"}}
{"id": "383", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "literary genre", "location", "book", "event", "award", "poem", "organization", "country", "person", "magazine"], "instance": {"id": "383", "words": ["Vernor", "Steffen", "Vinge", "(", "He", "has", "won", "the", "Hugo", "Award", "for", "his", "novels", "and", "novellas", "A", "Fire", "Upon", "the", "Deep", "(", "1992", ")", ",", "A", "Deepness", "in", "the", "Sky", "(", "1999", ")", ",", "Rainbows", "End", "(", "2006", ")", ",", "Fast", "Times", "at", "Fairmont", "High", "(", "2002", ")", ",", "and", "The", "Cookie", "Monster", "(", "2004", ")", "."], "labels": ["B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-literary genre", "O", "B-literary genre", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, literary genre, location, book, event, award, poem, organization, country, person, magazine and O.\nSentence: Vernor Steffen Vinge ( He has won the Hugo Award for his novels and novellas A Fire Upon the Deep ( 1992 ) , A Deepness in the Sky ( 1999 ) , Rainbows End ( 2006 ) , Fast Times at Fairmont High ( 2002 ) , and The Cookie Monster ( 2004 ) .", "prompt_labels": "Vernor(B-writer) Steffen(I-writer) Vinge(I-writer) ((O) He(O) has(O) won(O) the(O) Hugo(B-award) Award(I-award) for(O) his(O) novels(B-literary genre) and(O) novellas(B-literary genre) A(B-book) Fire(I-book) Upon(I-book) the(I-book) Deep(I-book) ((O) 1992(O) )(O) ,(O) A(B-book) Deepness(I-book) in(I-book) the(I-book) Sky(I-book) ((O) 1999(O) )(O) ,(O) Rainbows(B-book) End(I-book) ((O) 2006(O) )(O) ,(O) Fast(B-book) Times(I-book) at(I-book) Fairmont(I-book) High(I-book) ((O) 2002(O) )(O) ,(O) and(O) The(B-book) Cookie(I-book) Monster(I-book) ((O) 2004(O) )(O) .(O)"}}
{"id": "29", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "location", "event", "person", "poem", "organization", "magazine", "writer", "country", "book", "award"], "instance": {"id": "29", "words": ["Later", ",", "he", "was", "best", "known", "for", "his", "setting", "the", "Draumkvedet", "(", "1905", ")", "and", "the", "Poetic", "Edda", "(", "1908", ")", "into", "modern", "Norwegian", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-poem", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, location, event, person, poem, organization, magazine, writer, country, book, award and O.\nSentence: Later , he was best known for his setting the Draumkvedet ( 1905 ) and the Poetic Edda ( 1908 ) into modern Norwegian .", "prompt_labels": "Later(O) ,(O) he(O) was(O) best(O) known(O) for(O) his(O) setting(O) the(O) Draumkvedet(B-poem) ((O) 1905(O) )(O) and(O) the(O) Poetic(B-book) Edda(I-book) ((O) 1908(O) )(O) into(O) modern(O) Norwegian(O) .(O)"}}
{"id": "249", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "location", "magazine", "award", "event", "organization", "book", "literary genre", "country", "person", "poem"], "instance": {"id": "249", "words": ["On", "November", "17", ",", "2010", ",", "Smith", "won", "the", "National", "Book", "Award", "for", "her", "memoir", "Just", "Kids", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "B-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, location, magazine, award, event, organization, book, literary genre, country, person, poem and O.\nSentence: On November 17 , 2010 , Smith won the National Book Award for her memoir Just Kids .", "prompt_labels": "On(O) November(O) 17(O) ,(O) 2010(O) ,(O) Smith(B-writer) won(O) the(O) National(B-award) Book(I-award) Award(I-award) for(O) her(O) memoir(O) Just(B-book) Kids(I-book) .(O)"}}
{"id": "172", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "event", "person", "magazine", "writer", "literary genre", "country", "location", "award", "organization", "book"], "instance": {"id": "172", "words": ["However", ",", "in", "the", "parallel", "poem", "The", "Greene", "Knight", ",", "the", "lace", "is", "white", ",", "not", "green", ",", "and", "is", "considered", "the", "origin", "of", "the", "collar", "worn", "by", "the", "knights", "of", "the", "Bath", ",", "not", "the", "Order", "of", "the", "Garter", "."], "labels": ["O", "O", "O", "O", "B-literary genre", "I-literary genre", "B-poem", "I-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, event, person, magazine, writer, literary genre, country, location, award, organization, book and O.\nSentence: However , in the parallel poem The Greene Knight , the lace is white , not green , and is considered the origin of the collar worn by the knights of the Bath , not the Order of the Garter .", "prompt_labels": "However(O) ,(O) in(O) the(O) parallel(B-literary genre) poem(I-literary genre) The(B-poem) Greene(I-poem) Knight(I-poem) ,(O) the(O) lace(O) is(O) white(O) ,(O) not(O) green(O) ,(O) and(O) is(O) considered(O) the(O) origin(O) of(O) the(O) collar(O) worn(O) by(O) the(O) knights(O) of(O) the(O) Bath(O) ,(O) not(O) the(O) Order(O) of(O) the(O) Garter(O) .(O)"}}
{"id": "148", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "location", "writer", "country", "award", "organization", "book", "event", "person", "poem", "literary genre"], "instance": {"id": "148", "words": ["All", "the", "Light", "We", "Cannot", "See", ",", "a", "Pulitzer", "prize", "winning", "novel", "by", "Anthony", "Doerr", ",", "tells", "the", "story", "of", "Marie-Laure", "LeBlanc", ",", "a", "young", "girl", "who", "has", "gone", "completely", "blind", "due", "to", "cataract", "s", "at", "the", "age", "of", "6", "."], "labels": ["B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "B-award", "I-award", "O", "B-literary genre", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, location, writer, country, award, organization, book, event, person, poem, literary genre and O.\nSentence: All the Light We Cannot See , a Pulitzer prize winning novel by Anthony Doerr , tells the story of Marie-Laure LeBlanc , a young girl who has gone completely blind due to cataract s at the age of 6 .", "prompt_labels": "All(B-book) the(I-book) Light(I-book) We(I-book) Cannot(I-book) See(I-book) ,(O) a(O) Pulitzer(B-award) prize(I-award) winning(O) novel(B-literary genre) by(O) Anthony(B-writer) Doerr(I-writer) ,(O) tells(O) the(O) story(O) of(O) Marie-Laure(B-person) LeBlanc(I-person) ,(O) a(O) young(O) girl(O) who(O) has(O) gone(O) completely(O) blind(O) due(O) to(O) cataract(O) s(O) at(O) the(O) age(O) of(O) 6(O) .(O)"}}
{"id": "244", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "writer", "magazine", "organization", "literary genre", "country", "location", "book", "person", "award", "poem"], "instance": {"id": "244", "words": ["It", "received", "good", "notices", "and", "was", "nominated", "for", "the", "National", "Book", "Award", "in", "March", "1962", ",", "though", "Walker", "Percy", "'", "s", "The", "Moviegoer", "won", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-book", "I-book", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, writer, magazine, organization, literary genre, country, location, book, person, award, poem and O.\nSentence: It received good notices and was nominated for the National Book Award in March 1962 , though Walker Percy ' s The Moviegoer won .", "prompt_labels": "It(O) received(O) good(O) notices(O) and(O) was(O) nominated(O) for(O) the(O) National(B-award) Book(I-award) Award(I-award) in(O) March(O) 1962(O) ,(O) though(O) Walker(B-writer) Percy(I-writer) '(O) s(O) The(B-book) Moviegoer(I-book) won(O) .(O)"}}
{"id": "198", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "literary genre", "writer", "location", "country", "organization", "poem", "person", "book", "magazine", "event"], "instance": {"id": "198", "words": ["Robert", "B.", "Parker", ",", "The", "Godwulf", "Manuscript", ",", "Dell", "Books", ",", "1987", ",", "page", "163", ":", "I", "looked", "at", "them", "obliquely", "as", "I", "'d", "learned", "to", "do", "a", "long", "time", "ago", "in", "Korea", "."], "labels": ["B-writer", "I-writer", "I-writer", "O", "B-book", "I-book", "I-book", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, literary genre, writer, location, country, organization, poem, person, book, magazine, event and O.\nSentence: Robert B. Parker , The Godwulf Manuscript , Dell Books , 1987 , page 163 : I looked at them obliquely as I 'd learned to do a long time ago in Korea .", "prompt_labels": "Robert(B-writer) B.(I-writer) Parker(I-writer) ,(O) The(B-book) Godwulf(I-book) Manuscript(I-book) ,(O) Dell(B-organization) Books(I-organization) ,(O) 1987(O) ,(O) page(O) 163(O) :(O) I(O) looked(O) at(O) them(O) obliquely(O) as(O) I(O) 'd(O) learned(O) to(O) do(O) a(O) long(O) time(O) ago(O) in(O) Korea(B-country) .(O)"}}
{"id": "361", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "award", "book", "location", "magazine", "event", "organization", "person", "literary genre", "poem", "writer"], "instance": {"id": "361", "words": ["Although", "Williams", "attracted", "the", "attention", "and", "admiration", "of", "some", "of", "the", "most", "notable", "writers", "of", "his", "day", ",", "including", "T.", "S.", "Eliot", "and", "W.", "H.", "Auden", ",", "his", "greatest", "admirer", "was", "probably", "C.", "S.", "Lewis", ",", "whose", "novel", "That", "Hideous", "Strength", "(", "1945", ")", "has", "been", "regarded", "as", "partially", "inspired", "by", "his", "acquaintance", "with", "both", "the", "man", "and", "his", "novels", "and", "poems", "."], "labels": ["O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-literary genre", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "B-literary genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, book, location, magazine, event, organization, person, literary genre, poem, writer and O.\nSentence: Although Williams attracted the attention and admiration of some of the most notable writers of his day , including T. S. Eliot and W. H. Auden , his greatest admirer was probably C. S. Lewis , whose novel That Hideous Strength ( 1945 ) has been regarded as partially inspired by his acquaintance with both the man and his novels and poems .", "prompt_labels": "Although(O) Williams(B-writer) attracted(O) the(O) attention(O) and(O) admiration(O) of(O) some(O) of(O) the(O) most(O) notable(O) writers(O) of(O) his(O) day(O) ,(O) including(O) T.(B-writer) S.(I-writer) Eliot(I-writer) and(O) W.(B-writer) H.(I-writer) Auden(I-writer) ,(O) his(O) greatest(O) admirer(O) was(O) probably(O) C.(B-writer) S.(I-writer) Lewis(I-writer) ,(O) whose(O) novel(B-literary genre) That(B-book) Hideous(I-book) Strength(I-book) ((O) 1945(O) )(O) has(O) been(O) regarded(O) as(O) partially(O) inspired(O) by(O) his(O) acquaintance(O) with(O) both(O) the(O) man(O) and(O) his(O) novels(B-literary genre) and(O) poems(B-literary genre) .(O)"}}
{"id": "287", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "writer", "country", "literary genre", "book", "award", "poem", "person", "location", "event", "organization"], "instance": {"id": "287", "words": ["It", "is", "based", "on", "the", "novel", "Schindler", "'s", "Ark", "by", "Australian", "novelist", "Thomas", "Keneally", "."], "labels": ["O", "O", "O", "O", "O", "B-literary genre", "B-book", "I-book", "I-book", "O", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, writer, country, literary genre, book, award, poem, person, location, event, organization and O.\nSentence: It is based on the novel Schindler 's Ark by Australian novelist Thomas Keneally .", "prompt_labels": "It(O) is(O) based(O) on(O) the(O) novel(B-literary genre) Schindler(B-book) 's(I-book) Ark(I-book) by(O) Australian(O) novelist(O) Thomas(B-writer) Keneally(I-writer) .(O)"}}
{"id": "35", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "literary genre", "organization", "magazine", "poem", "country", "person", "book", "writer", "location", "award"], "instance": {"id": "35", "words": ["In", "March", "1938", ",", "Huxley", "'s", "friend", "Anita", "Loos", ",", "a", "novelist", "and", "screenwriter", ",", "put", "him", "in", "touch", "with", "Metro-Goldwyn-Mayer", "(", "MGM", ")", ",", "which", "hired", "him", "for", "Madame", "Curie", "which", "was", "originally", "to", "star", "Greta", "Garbo", "and", "be", "directed", "by", "George", "Cukor", "."], "labels": ["O", "O", "O", "O", "B-writer", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, literary genre, organization, magazine, poem, country, person, book, writer, location, award and O.\nSentence: In March 1938 , Huxley 's friend Anita Loos , a novelist and screenwriter , put him in touch with Metro-Goldwyn-Mayer ( MGM ) , which hired him for Madame Curie which was originally to star Greta Garbo and be directed by George Cukor .", "prompt_labels": "In(O) March(O) 1938(O) ,(O) Huxley(B-writer) 's(O) friend(O) Anita(B-writer) Loos(I-writer) ,(O) a(O) novelist(O) and(O) screenwriter(O) ,(O) put(O) him(O) in(O) touch(O) with(O) Metro-Goldwyn-Mayer(B-organization) ((O) MGM(B-organization) )(O) ,(O) which(O) hired(O) him(O) for(O) Madame(O) Curie(O) which(O) was(O) originally(O) to(O) star(O) Greta(B-person) Garbo(I-person) and(O) be(O) directed(O) by(O) George(B-person) Cukor(I-person) .(O)"}}
{"id": "276", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "event", "writer", "award", "country", "organization", "literary genre", "poem", "book", "magazine", "location"], "instance": {"id": "276", "words": ["A", "documentary", "was", "also", "made", "for", "BBC", "Radio", "4", "entitled", "The", "Story", "of", "O", ":", "The", "Vice", "Francaise", ",", "presented", "by", "Rowan", "Pelling", ",", "former", "editor", "of", "the", "Erotic", "Review", ",", "which", "looked", "at", "the", "history", "of", "the", "book", "and", "its", "author", "Anne", "Desclos", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, writer, award, country, organization, literary genre, poem, book, magazine, location and O.\nSentence: A documentary was also made for BBC Radio 4 entitled The Story of O : The Vice Francaise , presented by Rowan Pelling , former editor of the Erotic Review , which looked at the history of the book and its author Anne Desclos .", "prompt_labels": "A(O) documentary(O) was(O) also(O) made(O) for(O) BBC(B-organization) Radio(I-organization) 4(I-organization) entitled(O) The(O) Story(O) of(O) O(O) :(O) The(O) Vice(O) Francaise(O) ,(O) presented(O) by(O) Rowan(B-writer) Pelling(I-writer) ,(O) former(O) editor(O) of(O) the(O) Erotic(B-magazine) Review(I-magazine) ,(O) which(O) looked(O) at(O) the(O) history(O) of(O) the(O) book(O) and(O) its(O) author(O) Anne(B-writer) Desclos(I-writer) .(O)"}}
{"id": "207", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "organization", "location", "literary genre", "magazine", "award", "poem", "writer", "country", "book", "person"], "instance": {"id": "207", "words": ["Hein", "was", "a", "close", "associate", "of", "Martin", "Gardner", "and", "his", "work", "was", "frequently", "featured", "in", "Gardner", "'s", "Mathematical", "Games", "column", "in", "Scientific", "American", ".", "The", "game", "of", "Hex", "(", "July", "1957", ")", ",", "the", "Soma", "cube", "(", "Sep", "1958", ")", ",", "the", "game", "of", "Tangloids", "(", "Dec", "1959", ")", "and", "The", "Superellipse", "(", "Sep", "1965", ")", "At", "the", "age", "of", "95", "Gardner", "wrote", "his", "autobiography", "and", "titled", "it", "Undiluted", "Hocus-Pocus", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, location, literary genre, magazine, award, poem, writer, country, book, person and O.\nSentence: Hein was a close associate of Martin Gardner and his work was frequently featured in Gardner 's Mathematical Games column in Scientific American . The game of Hex ( July 1957 ) , the Soma cube ( Sep 1958 ) , the game of Tangloids ( Dec 1959 ) and The Superellipse ( Sep 1965 ) At the age of 95 Gardner wrote his autobiography and titled it Undiluted Hocus-Pocus .", "prompt_labels": "Hein(B-writer) was(O) a(O) close(O) associate(O) of(O) Martin(B-writer) Gardner(I-writer) and(O) his(O) work(O) was(O) frequently(O) featured(O) in(O) Gardner(B-writer) 's(O) Mathematical(O) Games(O) column(O) in(O) Scientific(B-magazine) American(I-magazine) .(O) The(O) game(O) of(O) Hex(O) ((O) July(O) 1957(O) )(O) ,(O) the(O) Soma(O) cube(O) ((O) Sep(O) 1958(O) )(O) ,(O) the(O) game(O) of(O) Tangloids(O) ((O) Dec(O) 1959(O) )(O) and(O) The(O) Superellipse(O) ((O) Sep(O) 1965(O) )(O) At(O) the(O) age(O) of(O) 95(O) Gardner(B-writer) wrote(O) his(O) autobiography(O) and(O) titled(O) it(O) Undiluted(B-book) Hocus-Pocus(I-book) .(O)"}}
{"id": "246", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "organization", "location", "poem", "magazine", "literary genre", "event", "award", "book", "writer", "person"], "instance": {"id": "246", "words": ["In", "1987", ",", "the", "British", "Academy", "of", "Film", "and", "Television", "Arts", "awarded", "the", "BAFTA", "Award", "for", "Best", "Foreign", "Language", "Film", "to", "The", "Sacrifice", "."], "labels": ["O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, location, poem, magazine, literary genre, event, award, book, writer, person and O.\nSentence: In 1987 , the British Academy of Film and Television Arts awarded the BAFTA Award for Best Foreign Language Film to The Sacrifice .", "prompt_labels": "In(O) 1987(O) ,(O) the(O) British(B-organization) Academy(I-organization) of(I-organization) Film(I-organization) and(I-organization) Television(I-organization) Arts(I-organization) awarded(O) the(O) BAFTA(B-award) Award(I-award) for(I-award) Best(I-award) Foreign(I-award) Language(I-award) Film(I-award) to(O) The(O) Sacrifice(O) .(O)"}}
{"id": "7", "dataset": "crossner_music", "split": "test", "label_list": ["organization", "award", "event", "location", "song", "musical instrument", "album", "music genre", "person", "band", "musical artist", "country"], "instance": {"id": "7", "words": ["Often", "termed", "soul", "blues", "or", "Southern", "soul", ",", "the", "music", "at", "the", "heart", "of", "this", "movement", "was", "given", "new", "life", "by", "the", "unexpected", "success", "of", "two", "particular", "recordings", "on", "the", "Jackson-based", "Malaco", "label", ":", "Z.", "Z.", "Hill", "'", "s", "Down", "Home", "Blues", "(", "1982", ")", "and", "Little", "Milton", "'", "s", "The", "Blues", "is", "Alright", "(", "1984", ")", "."], "labels": ["O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "O", "B-song", "I-song", "I-song", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, event, location, song, musical instrument, album, music genre, person, band, musical artist, country and O.\nSentence: Often termed soul blues or Southern soul , the music at the heart of this movement was given new life by the unexpected success of two particular recordings on the Jackson-based Malaco label : Z. Z. Hill ' s Down Home Blues ( 1982 ) and Little Milton ' s The Blues is Alright ( 1984 ) .", "prompt_labels": "Often(O) termed(O) soul(B-music genre) blues(I-music genre) or(O) Southern(B-music genre) soul(I-music genre) ,(O) the(O) music(O) at(O) the(O) heart(O) of(O) this(O) movement(O) was(O) given(O) new(O) life(O) by(O) the(O) unexpected(O) success(O) of(O) two(O) particular(O) recordings(O) on(O) the(O) Jackson-based(O) Malaco(B-organization) label(I-organization) :(O) Z.(B-musical artist) Z.(I-musical artist) Hill(I-musical artist) '(O) s(O) Down(B-song) Home(I-song) Blues(I-song) ((O) 1982(O) )(O) and(O) Little(B-musical artist) Milton(I-musical artist) '(O) s(O) The(B-song) Blues(I-song) is(I-song) Alright(I-song) ((O) 1984(O) )(O) .(O)"}}
{"id": "410", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "song", "music genre", "event", "musical instrument", "country", "person", "band", "award", "organization", "location", "album"], "instance": {"id": "410", "words": ["Goodness", "toured", "extensively", "all", "over", "the", "world", ",", "supporting", "such", "acts", "as", "Pearl", "Jam", ",", "Cheap", "Trick", ",", "and", "Oasis", "."], "labels": ["B-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "O", "B-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, song, music genre, event, musical instrument, country, person, band, award, organization, location, album and O.\nSentence: Goodness toured extensively all over the world , supporting such acts as Pearl Jam , Cheap Trick , and Oasis .", "prompt_labels": "Goodness(B-band) toured(O) extensively(O) all(O) over(O) the(O) world(O) ,(O) supporting(O) such(O) acts(O) as(O) Pearl(B-band) Jam(I-band) ,(O) Cheap(B-band) Trick(I-band) ,(O) and(O) Oasis(B-band) .(O)"}}
{"id": "310", "dataset": "crossner_music", "split": "test", "label_list": ["person", "country", "musical instrument", "band", "organization", "musical artist", "song", "album", "event", "award", "location", "music genre"], "instance": {"id": "310", "words": ["Pink", "is", "also", "involved", "with", "several", "charities", ",", "including", "Human", "Rights", "Campaign", ",", "ONE", "Campaign", ",", "Prince", "'s", "Trust", ",", "New", "York", "Restoration", "Project", ",", "Run", "for", "the", "Cure", "Foundation", ",", "Save", "the", "Children", ",", "Take", "Back", "the", "Night", ",", "UNICEF", "and", "World", "Animal", "Protection", ".", "Look", "to", "the", "Stars", ":", "that", "swept", "through", "the", "Australian", "state", "of", "Victoria", "earlier", "that", "month", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, musical instrument, band, organization, musical artist, song, album, event, award, location, music genre and O.\nSentence: Pink is also involved with several charities , including Human Rights Campaign , ONE Campaign , Prince 's Trust , New York Restoration Project , Run for the Cure Foundation , Save the Children , Take Back the Night , UNICEF and World Animal Protection . Look to the Stars : that swept through the Australian state of Victoria earlier that month .", "prompt_labels": "Pink(B-musical artist) is(O) also(O) involved(O) with(O) several(O) charities(O) ,(O) including(O) Human(B-organization) Rights(I-organization) Campaign(I-organization) ,(O) ONE(B-organization) Campaign(I-organization) ,(O) Prince(B-organization) 's(I-organization) Trust(I-organization) ,(O) New(B-organization) York(I-organization) Restoration(I-organization) Project(I-organization) ,(O) Run(B-organization) for(I-organization) the(I-organization) Cure(I-organization) Foundation(I-organization) ,(O) Save(B-organization) the(I-organization) Children(I-organization) ,(O) Take(B-organization) Back(I-organization) the(I-organization) Night(I-organization) ,(O) UNICEF(B-organization) and(O) World(B-organization) Animal(I-organization) Protection(I-organization) .(O) Look(O) to(O) the(O) Stars(O) :(O) that(O) swept(O) through(O) the(O) Australian(O) state(O) of(O) Victoria(B-location) earlier(O) that(O) month(O) .(O)"}}
{"id": "296", "dataset": "crossner_music", "split": "test", "label_list": ["location", "musical instrument", "organization", "song", "person", "award", "country", "event", "band", "album", "musical artist", "music genre"], "instance": {"id": "296", "words": ["Deep", "Purple", "and", "Whitesnake", "'", "s", "David", "Coverdale", ",", "Samson", "'", "s", "Nicky", "Moore", "and", "Lone", "Star", "'", "s", "John", "Sloman", "were", "all", "considered", "and", "Iommi", "states", "in", "his", "autobiography", "that", "Michael", "Bolton", "auditioned", "."], "labels": ["B-band", "I-band", "O", "B-band", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "B-musical artist", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, musical instrument, organization, song, person, award, country, event, band, album, musical artist, music genre and O.\nSentence: Deep Purple and Whitesnake ' s David Coverdale , Samson ' s Nicky Moore and Lone Star ' s John Sloman were all considered and Iommi states in his autobiography that Michael Bolton auditioned .", "prompt_labels": "Deep(B-band) Purple(I-band) and(O) Whitesnake(B-band) '(O) s(O) David(B-musical artist) Coverdale(I-musical artist) ,(O) Samson(B-band) '(O) s(O) Nicky(B-musical artist) Moore(I-musical artist) and(O) Lone(B-band) Star(I-band) '(O) s(O) John(B-musical artist) Sloman(I-musical artist) were(O) all(O) considered(O) and(O) Iommi(B-musical artist) states(O) in(O) his(O) autobiography(O) that(O) Michael(B-musical artist) Bolton(I-musical artist) auditioned(O) .(O)"}}
{"id": "321", "dataset": "crossner_music", "split": "test", "label_list": ["band", "award", "organization", "country", "song", "music genre", "musical instrument", "event", "location", "person", "musical artist", "album"], "instance": {"id": "321", "words": ["In", "conjunction", "with", "the", "book", "and", "the", "play", "that", "also", "paid", "tribute", "to", "his", "uncle", ",", "Milt", "Gabler", ",", "Crystal", "produced", "two", "CD", "compilations", ":", "Billy", "Crystal", "Presents", ":", "The", "Milt", "Gabler", "Story", ",", "which", "featured", "his", "uncle", "'s", "most", "influential", "recordings", "from", "Billie", "Holiday", "'", "s", "Strange", "Fruit", "to", "Rock", "Around", "the", "Clock", "by", "Bill", "Haley", "&", "His", "Comets", ";", "and", "Billy", "Remembers", "Billie", "featuring", "Crystal", "'s", "favorite", "Holiday", "recordings", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-musical artist", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "B-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-band", "I-band", "I-band", "I-band", "I-band", "O", "O", "B-album", "I-album", "I-album", "O", "B-musical artist", "O", "O", "B-musical artist", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, award, organization, country, song, music genre, musical instrument, event, location, person, musical artist, album and O.\nSentence: In conjunction with the book and the play that also paid tribute to his uncle , Milt Gabler , Crystal produced two CD compilations : Billy Crystal Presents : The Milt Gabler Story , which featured his uncle 's most influential recordings from Billie Holiday ' s Strange Fruit to Rock Around the Clock by Bill Haley & His Comets ; and Billy Remembers Billie featuring Crystal 's favorite Holiday recordings .", "prompt_labels": "In(O) conjunction(O) with(O) the(O) book(O) and(O) the(O) play(O) that(O) also(O) paid(O) tribute(O) to(O) his(O) uncle(O) ,(O) Milt(B-person) Gabler(I-person) ,(O) Crystal(B-musical artist) produced(O) two(O) CD(O) compilations(O) :(O) Billy(B-musical artist) Crystal(I-musical artist) Presents(O) :(O) The(B-album) Milt(I-album) Gabler(I-album) Story(I-album) ,(O) which(O) featured(O) his(O) uncle(O) 's(O) most(O) influential(O) recordings(O) from(O) Billie(B-musical artist) Holiday(I-musical artist) '(O) s(O) Strange(B-song) Fruit(I-song) to(O) Rock(B-song) Around(I-song) the(I-song) Clock(I-song) by(O) Bill(B-band) Haley(I-band) &(I-band) His(I-band) Comets(I-band) ;(O) and(O) Billy(B-album) Remembers(I-album) Billie(I-album) featuring(O) Crystal(B-musical artist) 's(O) favorite(O) Holiday(B-musical artist) recordings(O) .(O)"}}
{"id": "253", "dataset": "crossner_music", "split": "test", "label_list": ["band", "award", "song", "music genre", "country", "musical instrument", "musical artist", "location", "person", "album", "organization", "event"], "instance": {"id": "253", "words": ["Franklin", "continued", "to", "record", "acclaimed", "albums", "such", "as", "I", "Never", "Loved", "a", "Man", "the", "Way", "I", "Love", "You", "(", "1967", ")", ",", "Lady", "Soul", "(", "1968", ")", ",", "Spirit", "in", "the", "Dark", "(", "1970", ")", ",", "Young", ",", "Gifted", "and", "Black", "(", "1972", ")", ",", "Amazing", "Grace", "(", "1972", ")", ",", "and", "Sparkle", "(", "1976", ")", "before", "experiencing", "problems", "with", "her", "record", "company", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "B-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, award, song, music genre, country, musical instrument, musical artist, location, person, album, organization, event and O.\nSentence: Franklin continued to record acclaimed albums such as I Never Loved a Man the Way I Love You ( 1967 ) , Lady Soul ( 1968 ) , Spirit in the Dark ( 1970 ) , Young , Gifted and Black ( 1972 ) , Amazing Grace ( 1972 ) , and Sparkle ( 1976 ) before experiencing problems with her record company .", "prompt_labels": "Franklin(B-musical artist) continued(O) to(O) record(O) acclaimed(O) albums(O) such(O) as(O) I(B-album) Never(I-album) Loved(I-album) a(I-album) Man(I-album) the(I-album) Way(I-album) I(I-album) Love(I-album) You(I-album) ((O) 1967(O) )(O) ,(O) Lady(B-album) Soul(I-album) ((O) 1968(O) )(O) ,(O) Spirit(B-album) in(I-album) the(I-album) Dark(I-album) ((O) 1970(O) )(O) ,(O) Young(B-album) ,(I-album) Gifted(I-album) and(I-album) Black(I-album) ((O) 1972(O) )(O) ,(O) Amazing(B-album) Grace(I-album) ((O) 1972(O) )(O) ,(O) and(O) Sparkle(B-album) ((O) 1976(O) )(O) before(O) experiencing(O) problems(O) with(O) her(O) record(O) company(O) .(O)"}}
{"id": "282", "dataset": "crossner_music", "split": "test", "label_list": ["song", "event", "person", "country", "album", "musical artist", "organization", "band", "location", "award", "musical instrument", "music genre"], "instance": {"id": "282", "words": ["Since", "2001", ",", "the", "dominance", "of", "traditional", "boy", "bands", "on", "pop", "charts", "began", "to", "fade", "in", "the", "western", "hemisphere", ",", "although", "Gil", "Kaufman", "of", "MTV", "has", "described", "new", "boy", "bands", "that", "are", "more", "likely", "to", "resemble", "My", "Chemical", "Romance", ",", "Sum", "41", ",", "and", "Simple", "Plan", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, event, person, country, album, musical artist, organization, band, location, award, musical instrument, music genre and O.\nSentence: Since 2001 , the dominance of traditional boy bands on pop charts began to fade in the western hemisphere , although Gil Kaufman of MTV has described new boy bands that are more likely to resemble My Chemical Romance , Sum 41 , and Simple Plan .", "prompt_labels": "Since(O) 2001(O) ,(O) the(O) dominance(O) of(O) traditional(O) boy(O) bands(O) on(O) pop(B-music genre) charts(O) began(O) to(O) fade(O) in(O) the(O) western(O) hemisphere(O) ,(O) although(O) Gil(B-person) Kaufman(I-person) of(O) MTV(B-organization) has(O) described(O) new(O) boy(O) bands(O) that(O) are(O) more(O) likely(O) to(O) resemble(O) My(B-band) Chemical(I-band) Romance(I-band) ,(O) Sum(B-band) 41(I-band) ,(O) and(O) Simple(B-band) Plan(I-band) .(O)"}}
{"id": "197", "dataset": "crossner_music", "split": "test", "label_list": ["organization", "event", "album", "band", "musical artist", "musical instrument", "award", "country", "location", "song", "music genre", "person"], "instance": {"id": "197", "words": ["Among", "the", "most", "prominent", "of", "these", "were", "Metallica", "'", "s", "Master", "of", "Puppets", ",", "Slayer", "'", "s", "Reign", "in", "Blood", ",", "Anthrax", "'", "s", "Among", "the", "Living", "and", "Megadeth", "'", "s", "Peace", "Sells", "...", "but", "Who", "'s", "Buying", "?", "beginning", "a", "search", "for", "his", "replacement", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-band", "O", "O", "B-album", "I-album", "I-album", "O", "B-band", "O", "O", "B-album", "I-album", "I-album", "O", "B-band", "O", "O", "B-album", "I-album", "I-album", "O", "B-band", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, album, band, musical artist, musical instrument, award, country, location, song, music genre, person and O.\nSentence: Among the most prominent of these were Metallica ' s Master of Puppets , Slayer ' s Reign in Blood , Anthrax ' s Among the Living and Megadeth ' s Peace Sells ... but Who 's Buying ? beginning a search for his replacement .", "prompt_labels": "Among(O) the(O) most(O) prominent(O) of(O) these(O) were(O) Metallica(B-band) '(O) s(O) Master(B-album) of(I-album) Puppets(I-album) ,(O) Slayer(B-band) '(O) s(O) Reign(B-album) in(I-album) Blood(I-album) ,(O) Anthrax(B-band) '(O) s(O) Among(B-album) the(I-album) Living(I-album) and(O) Megadeth(B-band) '(O) s(O) Peace(B-album) Sells(I-album) ...(I-album) but(I-album) Who(I-album) 's(I-album) Buying(I-album) ?(I-album) beginning(O) a(O) search(O) for(O) his(O) replacement(O) .(O)"}}
{"id": "36", "dataset": "crossner_music", "split": "test", "label_list": ["band", "musical instrument", "event", "album", "musical artist", "person", "country", "award", "location", "organization", "song", "music genre"], "instance": {"id": "36", "words": ["Duke", "turned", "65", "in", "the", "spring", "of", "1964", "but", "showed", "no", "signs", "of", "slowing", "down", "as", "he", "continued", "to", "make", "vital", "and", "innovative", "recordings", ",", "including", "The", "Far", "East", "Suite", "(", "1966", ")", ",", "New", "Orleans", "Suite", "(", "1970", ")", ",", "Latin", "American", "Suite", "(", "1972", ")", "and", "The", "Afro-Eurasian", "Eclipse", "(", "1971", ")", ",", "much", "of", "it", "inspired", "by", "his", "world", "tours", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical instrument, event, album, musical artist, person, country, award, location, organization, song, music genre and O.\nSentence: Duke turned 65 in the spring of 1964 but showed no signs of slowing down as he continued to make vital and innovative recordings , including The Far East Suite ( 1966 ) , New Orleans Suite ( 1970 ) , Latin American Suite ( 1972 ) and The Afro-Eurasian Eclipse ( 1971 ) , much of it inspired by his world tours .", "prompt_labels": "Duke(B-musical artist) turned(O) 65(O) in(O) the(O) spring(O) of(O) 1964(O) but(O) showed(O) no(O) signs(O) of(O) slowing(O) down(O) as(O) he(O) continued(O) to(O) make(O) vital(O) and(O) innovative(O) recordings(O) ,(O) including(O) The(B-album) Far(I-album) East(I-album) Suite(I-album) ((O) 1966(O) )(O) ,(O) New(B-album) Orleans(I-album) Suite(I-album) ((O) 1970(O) )(O) ,(O) Latin(B-album) American(I-album) Suite(I-album) ((O) 1972(O) )(O) and(O) The(B-album) Afro-Eurasian(I-album) Eclipse(I-album) ((O) 1971(O) )(O) ,(O) much(O) of(O) it(O) inspired(O) by(O) his(O) world(O) tours(O) .(O)"}}
{"id": "244", "dataset": "crossner_music", "split": "test", "label_list": ["location", "musical instrument", "album", "person", "country", "organization", "award", "music genre", "musical artist", "song", "band", "event"], "instance": {"id": "244", "words": ["He", "also", "produced", "tracks", "for", "a", "number", "of", "other", "acts", "on", "Ruthless", "Records", ",", "including", "Eazy-E", "'s", "1988", "solo", "debut", "Eazy-Duz-It", ",", "Above", "the", "Law", "'", "s", "1990", "debut", "Livin", "'", "Like", "Hustlers", ",", "Michel", "'le", "'", "s", "1989", "self-titled", "debut", ",", "The", "D.O.C.", "'", "s", "1989", "debut", "No", "One", "Can", "Do", "It", "Better", ",", "J.J.", "Fad", "'", "s", "1988", "debut", "Supersonic", "and", "funk", "rock", "musician", "Jimmy", "Z", "'", "s", "1991", "album", "Muzical", "Madness"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-musical artist", "O", "O", "O", "O", "B-album", "O", "B-band", "I-band", "I-band", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O", "B-band", "I-band", "O", "O", "O", "O", "B-album", "O", "B-music genre", "I-music genre", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "B-album", "I-album"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, musical instrument, album, person, country, organization, award, music genre, musical artist, song, band, event and O.\nSentence: He also produced tracks for a number of other acts on Ruthless Records , including Eazy-E 's 1988 solo debut Eazy-Duz-It , Above the Law ' s 1990 debut Livin ' Like Hustlers , Michel 'le ' s 1989 self-titled debut , The D.O.C. ' s 1989 debut No One Can Do It Better , J.J. Fad ' s 1988 debut Supersonic and funk rock musician Jimmy Z ' s 1991 album Muzical Madness", "prompt_labels": "He(O) also(O) produced(O) tracks(O) for(O) a(O) number(O) of(O) other(O) acts(O) on(O) Ruthless(B-organization) Records(I-organization) ,(O) including(O) Eazy-E(B-musical artist) 's(O) 1988(O) solo(O) debut(O) Eazy-Duz-It(B-album) ,(O) Above(B-band) the(I-band) Law(I-band) '(O) s(O) 1990(O) debut(O) Livin(B-album) '(I-album) Like(I-album) Hustlers(I-album) ,(O) Michel(B-musical artist) 'le(I-musical artist) '(O) s(O) 1989(O) self-titled(O) debut(O) ,(O) The(B-musical artist) D.O.C.(I-musical artist) '(O) s(O) 1989(O) debut(O) No(B-album) One(I-album) Can(I-album) Do(I-album) It(I-album) Better(I-album) ,(O) J.J.(B-band) Fad(I-band) '(O) s(O) 1988(O) debut(O) Supersonic(B-album) and(O) funk(B-music genre) rock(I-music genre) musician(O) Jimmy(B-musical artist) Z(I-musical artist) '(O) s(O) 1991(O) album(O) Muzical(B-album) Madness(I-album)"}}
{"id": "423", "dataset": "crossner_music", "split": "test", "label_list": ["musical instrument", "song", "band", "award", "album", "music genre", "organization", "country", "location", "person", "musical artist", "event"], "instance": {"id": "423", "words": ["The", "musical", "was", "nominated", "for", "three", "Tony", "Awards", ":", "Tony", "Award", "for", "Best", "Musical", ",", "Tony", "Award", "for", "Best", "Scenic", "Design", ",", "and", "Tony", "Award", "for", "Best", "Lighting", "Design", ",", "winning", "the", "latter", "two", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, song, band, award, album, music genre, organization, country, location, person, musical artist, event and O.\nSentence: The musical was nominated for three Tony Awards : Tony Award for Best Musical , Tony Award for Best Scenic Design , and Tony Award for Best Lighting Design , winning the latter two .", "prompt_labels": "The(O) musical(O) was(O) nominated(O) for(O) three(O) Tony(B-award) Awards(I-award) :(O) Tony(B-award) Award(I-award) for(I-award) Best(I-award) Musical(I-award) ,(O) Tony(B-award) Award(I-award) for(I-award) Best(I-award) Scenic(I-award) Design(I-award) ,(O) and(O) Tony(B-award) Award(I-award) for(I-award) Best(I-award) Lighting(I-award) Design(I-award) ,(O) winning(O) the(O) latter(O) two(O) .(O)"}}
{"id": "389", "dataset": "crossner_music", "split": "test", "label_list": ["event", "album", "person", "award", "song", "music genre", "musical artist", "organization", "musical instrument", "country", "band", "location"], "instance": {"id": "389", "words": ["The", "music", "arose", "as", "a", "synthesis", "of", "traditional", "Creole", "music", ",", "some", "Cajun", "music", "influences", ",", "and", "African-American", "traditions", ",", "including", "Rhythm", "and", "blues", ",", "blues", ",", "jazz", ",", "and", "Gospel", "music", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "O", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, album, person, award, song, music genre, musical artist, organization, musical instrument, country, band, location and O.\nSentence: The music arose as a synthesis of traditional Creole music , some Cajun music influences , and African-American traditions , including Rhythm and blues , blues , jazz , and Gospel music .", "prompt_labels": "The(O) music(O) arose(O) as(O) a(O) synthesis(O) of(O) traditional(O) Creole(B-music genre) music(I-music genre) ,(O) some(O) Cajun(B-music genre) music(I-music genre) influences(O) ,(O) and(O) African-American(O) traditions(O) ,(O) including(O) Rhythm(B-music genre) and(I-music genre) blues(I-music genre) ,(O) blues(B-music genre) ,(O) jazz(B-music genre) ,(O) and(O) Gospel(B-music genre) music(I-music genre) .(O)"}}
{"id": "108", "dataset": "crossner_music", "split": "test", "label_list": ["organization", "album", "country", "music genre", "award", "musical artist", "event", "person", "musical instrument", "location", "band", "song"], "instance": {"id": "108", "words": ["Alt-country", ",", "in", "various", "iterations", "overlapped", "with", "other", "genres", ",", "including", "Red", "Dirt", "country", "music", "(", "Cross", "Canadian", "Ragweed", ")", ",", "jam", "band", "s", "(", "My", "Morning", "Jacket", "and", "The", "String", "Cheese", "Incident", ")", ",", "and", "indie", "folk", "(", "The", "Avett", "Brothers", ")", "."], "labels": ["B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "I-music genre", "O", "B-band", "I-band", "I-band", "O", "O", "B-band", "I-band", "O", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "I-band", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-band", "I-band", "I-band", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, album, country, music genre, award, musical artist, event, person, musical instrument, location, band, song and O.\nSentence: Alt-country , in various iterations overlapped with other genres , including Red Dirt country music ( Cross Canadian Ragweed ) , jam band s ( My Morning Jacket and The String Cheese Incident ) , and indie folk ( The Avett Brothers ) .", "prompt_labels": "Alt-country(B-music genre) ,(O) in(O) various(O) iterations(O) overlapped(O) with(O) other(O) genres(O) ,(O) including(O) Red(B-music genre) Dirt(I-music genre) country(I-music genre) music(I-music genre) ((O) Cross(B-band) Canadian(I-band) Ragweed(I-band) )(O) ,(O) jam(B-band) band(I-band) s(O) ((O) My(B-band) Morning(I-band) Jacket(I-band) and(O) The(B-band) String(I-band) Cheese(I-band) Incident(I-band) )(O) ,(O) and(O) indie(B-music genre) folk(I-music genre) ((O) The(B-band) Avett(I-band) Brothers(I-band) )(O) .(O)"}}
{"id": "356", "dataset": "crossner_music", "split": "test", "label_list": ["band", "country", "album", "organization", "song", "person", "musical instrument", "musical artist", "location", "music genre", "event", "award"], "instance": {"id": "356", "words": ["Traditional", "power", "metal", "bands", "like", "Sweden", "'s", "HammerFall", ",", "England", "'s", "DragonForce", ",", "and", "America", "'s", "Iced", "Earth", "have", "a", "sound", "clearly", "indebted", "to", "the", "classic", "NWOBHM", "style.See", ",", "e.g.", ",", "Reesman", "Bryan", "."], "labels": ["O", "B-music genre", "I-music genre", "O", "O", "B-country", "O", "B-band", "O", "B-country", "O", "B-band", "O", "O", "B-country", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, country, album, organization, song, person, musical instrument, musical artist, location, music genre, event, award and O.\nSentence: Traditional power metal bands like Sweden 's HammerFall , England 's DragonForce , and America 's Iced Earth have a sound clearly indebted to the classic NWOBHM style.See , e.g. , Reesman Bryan .", "prompt_labels": "Traditional(O) power(B-music genre) metal(I-music genre) bands(O) like(O) Sweden(B-country) 's(O) HammerFall(B-band) ,(O) England(B-country) 's(O) DragonForce(B-band) ,(O) and(O) America(B-country) 's(O) Iced(B-band) Earth(I-band) have(O) a(O) sound(O) clearly(O) indebted(O) to(O) the(O) classic(O) NWOBHM(B-music genre) style.See(O) ,(O) e.g.(O) ,(O) Reesman(B-person) Bryan(I-person) .(O)"}}
{"id": "309", "dataset": "crossner_music", "split": "test", "label_list": ["award", "location", "person", "album", "country", "music genre", "song", "musical instrument", "musical artist", "event", "band", "organization"], "instance": {"id": "309", "words": ["This", "was", "reflected", "in", "a", "series", "of", "albums", "released", "by", "Island", "Records", ",", "including", "Swordfishtrombones", "(", "1983", ")", ",", "Rain", "Dogs", "(", "1985", ")", ",", "and", "Franks", "Wild", "Years", "(", "1987", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-album", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, person, album, country, music genre, song, musical instrument, musical artist, event, band, organization and O.\nSentence: This was reflected in a series of albums released by Island Records , including Swordfishtrombones ( 1983 ) , Rain Dogs ( 1985 ) , and Franks Wild Years ( 1987 ) .", "prompt_labels": "This(O) was(O) reflected(O) in(O) a(O) series(O) of(O) albums(O) released(O) by(O) Island(B-organization) Records(I-organization) ,(O) including(O) Swordfishtrombones(B-album) ((O) 1983(O) )(O) ,(O) Rain(B-album) Dogs(I-album) ((O) 1985(O) )(O) ,(O) and(O) Franks(B-album) Wild(I-album) Years(I-album) ((O) 1987(O) )(O) .(O)"}}
{"id": "196", "dataset": "crossner_music", "split": "test", "label_list": ["musical instrument", "music genre", "event", "musical artist", "award", "song", "person", "album", "country", "organization", "location", "band"], "instance": {"id": "196", "words": ["Jagger", ",", "in", "the", "role", "of", "Turner", "in", "the", "1970", "film", "Performance", ",", "performed", "excerpts", "from", "Come", "On", "in", "My", "Kitchen", "and", "Me", "and", "the", "Devil", "Blues", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, music genre, event, musical artist, award, song, person, album, country, organization, location, band and O.\nSentence: Jagger , in the role of Turner in the 1970 film Performance , performed excerpts from Come On in My Kitchen and Me and the Devil Blues .", "prompt_labels": "Jagger(B-musical artist) ,(O) in(O) the(O) role(O) of(O) Turner(B-person) in(O) the(O) 1970(O) film(O) Performance(O) ,(O) performed(O) excerpts(O) from(O) Come(B-song) On(I-song) in(I-song) My(I-song) Kitchen(I-song) and(O) Me(B-song) and(I-song) the(I-song) Devil(I-song) Blues(I-song) .(O)"}}
{"id": "266", "dataset": "crossner_music", "split": "test", "label_list": ["event", "musical instrument", "song", "country", "music genre", "band", "award", "album", "location", "organization", "person", "musical artist"], "instance": {"id": "266", "words": ["The", "musical", "debuted", "in", "San", "Diego", "at", "the", "Old", "Globe", "Theatre", "in", "1986", "and", "premiered", "on", "Broadway", "on", "November", "5", ",", "1987", ",", "where", "it", "won", "several", "Tony", "Award", "s", ",", "including", "Tony", "Award", "for", "Best", "Original", "Score", ",", "Tony", "Award", "for", "Best", "Book", "of", "a", "Musical", ",", "and", "Best", "Actress", "in", "a", "Musical", "(", "Joanna", "Gleason", ")", ",", "in", "a", "year", "dominated", "by", "The", "Phantom", "of", "the", "Opera", "(", "1988", ")", "."], "labels": ["O", "O", "O", "O", "B-location", "I-location", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical instrument, song, country, music genre, band, award, album, location, organization, person, musical artist and O.\nSentence: The musical debuted in San Diego at the Old Globe Theatre in 1986 and premiered on Broadway on November 5 , 1987 , where it won several Tony Award s , including Tony Award for Best Original Score , Tony Award for Best Book of a Musical , and Best Actress in a Musical ( Joanna Gleason ) , in a year dominated by The Phantom of the Opera ( 1988 ) .", "prompt_labels": "The(O) musical(O) debuted(O) in(O) San(B-location) Diego(I-location) at(O) the(O) Old(B-organization) Globe(I-organization) Theatre(I-organization) in(O) 1986(O) and(O) premiered(O) on(O) Broadway(B-organization) on(O) November(O) 5(O) ,(O) 1987(O) ,(O) where(O) it(O) won(O) several(O) Tony(B-award) Award(I-award) s(O) ,(O) including(O) Tony(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Score(I-award) ,(O) Tony(B-award) Award(I-award) for(I-award) Best(I-award) Book(I-award) of(I-award) a(I-award) Musical(I-award) ,(O) and(O) Best(B-award) Actress(I-award) in(I-award) a(I-award) Musical(I-award) ((O) Joanna(B-musical artist) Gleason(I-musical artist) )(O) ,(O) in(O) a(O) year(O) dominated(O) by(O) The(O) Phantom(O) of(O) the(O) Opera(O) ((O) 1988(O) )(O) .(O)"}}
{"id": "395", "dataset": "crossner_music", "split": "test", "label_list": ["country", "person", "musical instrument", "organization", "band", "musical artist", "album", "song", "location", "event", "award", "music genre"], "instance": {"id": "395", "words": ["Deacon", "Blue", "also", "performed", "at", "the", "Glasgow", "2014", "Commonwealth", "Games", "closing", "ceremony", "on", "3", "August", "2014", ",", "performing", "their", "hit", ",", "Dignity", "."], "labels": ["B-band", "I-band", "O", "O", "O", "O", "B-location", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, musical instrument, organization, band, musical artist, album, song, location, event, award, music genre and O.\nSentence: Deacon Blue also performed at the Glasgow 2014 Commonwealth Games closing ceremony on 3 August 2014 , performing their hit , Dignity .", "prompt_labels": "Deacon(B-band) Blue(I-band) also(O) performed(O) at(O) the(O) Glasgow(B-location) 2014(B-event) Commonwealth(I-event) Games(I-event) closing(O) ceremony(O) on(O) 3(O) August(O) 2014(O) ,(O) performing(O) their(O) hit(O) ,(O) Dignity(B-song) .(O)"}}
{"id": "440", "dataset": "crossner_music", "split": "test", "label_list": ["award", "musical instrument", "album", "location", "song", "musical artist", "organization", "country", "event", "person", "music genre", "band"], "instance": {"id": "440", "words": ["Parton", "recorded", "a", "series", "of", "bluegrass", "-inspired", "albums", ",", "beginning", "with", "The", "Grass", "Is", "Blue", "(", "1999", ")", ",", "winning", "a", "Grammy", "Award", "for", "Best", "Bluegrass", "Album", ";", "and", "Little", "Sparrow", "(", "2001", ")", ",", "with", "its", "cover", "of", "Collective", "Soul", "'", "s", "Shine", "winning", "a", "Grammy", "Award", "for", "Best", "Female", "Country", "Vocal", "Performance", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "O", "B-song", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, musical instrument, album, location, song, musical artist, organization, country, event, person, music genre, band and O.\nSentence: Parton recorded a series of bluegrass -inspired albums , beginning with The Grass Is Blue ( 1999 ) , winning a Grammy Award for Best Bluegrass Album ; and Little Sparrow ( 2001 ) , with its cover of Collective Soul ' s Shine winning a Grammy Award for Best Female Country Vocal Performance .", "prompt_labels": "Parton(B-musical artist) recorded(O) a(O) series(O) of(O) bluegrass(B-music genre) -inspired(O) albums(O) ,(O) beginning(O) with(O) The(B-album) Grass(I-album) Is(I-album) Blue(I-album) ((O) 1999(O) )(O) ,(O) winning(O) a(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Bluegrass(I-award) Album(I-award) ;(O) and(O) Little(B-album) Sparrow(I-album) ((O) 2001(O) )(O) ,(O) with(O) its(O) cover(O) of(O) Collective(B-band) Soul(I-band) '(O) s(O) Shine(B-song) winning(O) a(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Female(I-award) Country(I-award) Vocal(I-award) Performance(I-award) .(O)"}}
{"id": "254", "dataset": "crossner_music", "split": "test", "label_list": ["band", "person", "organization", "album", "musical instrument", "event", "award", "country", "location", "music genre", "musical artist", "song"], "instance": {"id": "254", "words": ["The", "movie", "also", "featured", "a", "soundtrack", "of", "popular", "songs", ",", "including", "a", "cover", "version", "of", "The", "Troggs", "'", "Love", "Is", "All", "Around", "performed", "by", "Wet", "Wet", "Wet", "that", "remained", "at", "number", "1", "on", "the", "UK", "Singles", "Chart", "for", "fifteen", "weeks", "and", "was", "then", "the", "ninth", "(", "now", "twelfth", ")", "biggest", "selling", "single", "of", "all", "time", "in", "Britain", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "B-band", "I-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, person, organization, album, musical instrument, event, award, country, location, music genre, musical artist, song and O.\nSentence: The movie also featured a soundtrack of popular songs , including a cover version of The Troggs ' Love Is All Around performed by Wet Wet Wet that remained at number 1 on the UK Singles Chart for fifteen weeks and was then the ninth ( now twelfth ) biggest selling single of all time in Britain .", "prompt_labels": "The(O) movie(O) also(O) featured(O) a(O) soundtrack(O) of(O) popular(O) songs(O) ,(O) including(O) a(O) cover(O) version(O) of(O) The(B-band) Troggs(I-band) '(O) Love(B-song) Is(I-song) All(I-song) Around(I-song) performed(O) by(O) Wet(B-band) Wet(I-band) Wet(I-band) that(O) remained(O) at(O) number(O) 1(O) on(O) the(O) UK(O) Singles(O) Chart(O) for(O) fifteen(O) weeks(O) and(O) was(O) then(O) the(O) ninth(O) ((O) now(O) twelfth(O) )(O) biggest(O) selling(O) single(O) of(O) all(O) time(O) in(O) Britain(B-country) .(O)"}}
{"id": "245", "dataset": "crossner_music", "split": "test", "label_list": ["band", "musical instrument", "location", "organization", "award", "song", "person", "album", "country", "event", "musical artist", "music genre"], "instance": {"id": "245", "words": ["In", "2004", ",", "Grohl", "drummed", "on", "several", "tracks", "for", "Nine", "Inch", "Nails", "'", "2005", "album", "With", "Teeth", ",", "later", "returning", "to", "play", "drums", "on", "'", "The", "Idea", "of", "You", "'", "from", "their", "2016", "EP", "Not", "the", "Actual", "Events", "."], "labels": ["O", "O", "O", "B-musical artist", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "B-musical instrument", "O", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical instrument, location, organization, award, song, person, album, country, event, musical artist, music genre and O.\nSentence: In 2004 , Grohl drummed on several tracks for Nine Inch Nails ' 2005 album With Teeth , later returning to play drums on ' The Idea of You ' from their 2016 EP Not the Actual Events .", "prompt_labels": "In(O) 2004(O) ,(O) Grohl(B-musical artist) drummed(O) on(O) several(O) tracks(O) for(O) Nine(B-band) Inch(I-band) Nails(I-band) '(O) 2005(O) album(O) With(B-album) Teeth(I-album) ,(O) later(O) returning(O) to(O) play(O) drums(B-musical instrument) on(O) '(O) The(B-song) Idea(I-song) of(I-song) You(I-song) '(O) from(O) their(O) 2016(O) EP(O) Not(B-album) the(I-album) Actual(I-album) Events(I-album) .(O)"}}
{"id": "17", "dataset": "crossner_music", "split": "test", "label_list": ["organization", "band", "award", "music genre", "musical artist", "song", "musical instrument", "album", "country", "location", "person", "event"], "instance": {"id": "17", "words": ["In", "the", "early", "2010s", ",", "there", "was", "somewhat", "of", "a", "resurgence", "of", "boy", "band", "popularity", "in", "countries", "where", "the", "trend", "had", "not", "maintained", ",", "with", "the", "emergence", "of", "new", "boy", "bands", "like", "Big", "Time", "Rush", ",", "The", "Wanted", ",", "and", "One", "Direction", "and", "the", "formation", "of", "supergroup", "NKOTBSB", "which", "comprised", "members", "of", "New", "Kids", "on", "the", "Block", "and", "Backstreet", "Boys", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "B-band", "O", "O", "O", "O", "B-band", "I-band", "I-band", "I-band", "I-band", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, band, award, music genre, musical artist, song, musical instrument, album, country, location, person, event and O.\nSentence: In the early 2010s , there was somewhat of a resurgence of boy band popularity in countries where the trend had not maintained , with the emergence of new boy bands like Big Time Rush , The Wanted , and One Direction and the formation of supergroup NKOTBSB which comprised members of New Kids on the Block and Backstreet Boys .", "prompt_labels": "In(O) the(O) early(O) 2010s(O) ,(O) there(O) was(O) somewhat(O) of(O) a(O) resurgence(O) of(O) boy(O) band(O) popularity(O) in(O) countries(O) where(O) the(O) trend(O) had(O) not(O) maintained(O) ,(O) with(O) the(O) emergence(O) of(O) new(O) boy(O) bands(O) like(O) Big(B-band) Time(I-band) Rush(I-band) ,(O) The(B-band) Wanted(I-band) ,(O) and(O) One(B-band) Direction(I-band) and(O) the(O) formation(O) of(O) supergroup(O) NKOTBSB(B-band) which(O) comprised(O) members(O) of(O) New(B-band) Kids(I-band) on(I-band) the(I-band) Block(I-band) and(O) Backstreet(B-band) Boys(I-band) .(O)"}}
{"id": "79", "dataset": "crossner_music", "split": "test", "label_list": ["person", "album", "country", "organization", "music genre", "song", "musical instrument", "location", "event", "band", "award", "musical artist"], "instance": {"id": "79", "words": ["They", "play", "Avant-garde", "jazz", "versions", "of", "tradition", "American", "Folk", "music", "&", "amp", ";", "Blues", "songs", "with", "Ritchie", "'s", "shakuhachi", "playing", "as", "the", "focal", "point", "."], "labels": ["O", "O", "B-music genre", "I-music genre", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "O", "B-music genre", "O", "O", "B-musical artist", "O", "B-musical instrument", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, album, country, organization, music genre, song, musical instrument, location, event, band, award, musical artist and O.\nSentence: They play Avant-garde jazz versions of tradition American Folk music & amp ; Blues songs with Ritchie 's shakuhachi playing as the focal point .", "prompt_labels": "They(O) play(O) Avant-garde(B-music genre) jazz(I-music genre) versions(O) of(O) tradition(O) American(B-music genre) Folk(I-music genre) music(I-music genre) &(O) amp(O) ;(O) Blues(B-music genre) songs(O) with(O) Ritchie(B-musical artist) 's(O) shakuhachi(B-musical instrument) playing(O) as(O) the(O) focal(O) point(O) .(O)"}}
{"id": "0", "dataset": "crossner_music", "split": "test", "label_list": ["location", "musical instrument", "band", "organization", "award", "musical artist", "person", "album", "event", "song", "music genre", "country"], "instance": {"id": "0", "words": ["It", "is", "based", "on", "the", "classic", "song", "The", "Guns", "of", "Brixton", "on", "The", "Clash", "'", "s", "London", "Calling", "and", "has", "proven", "to", "be", "a", "success", "on", "the", "modern", "rock", "charts", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-band", "I-band", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, musical instrument, band, organization, award, musical artist, person, album, event, song, music genre, country and O.\nSentence: It is based on the classic song The Guns of Brixton on The Clash ' s London Calling and has proven to be a success on the modern rock charts .", "prompt_labels": "It(O) is(O) based(O) on(O) the(O) classic(O) song(O) The(B-song) Guns(I-song) of(I-song) Brixton(I-song) on(O) The(B-band) Clash(I-band) '(O) s(O) London(B-album) Calling(I-album) and(O) has(O) proven(O) to(O) be(O) a(O) success(O) on(O) the(O) modern(O) rock(B-music genre) charts(O) .(O)"}}
{"id": "357", "dataset": "crossner_music", "split": "test", "label_list": ["music genre", "person", "musical instrument", "award", "event", "band", "location", "organization", "album", "country", "musical artist", "song"], "instance": {"id": "357", "words": ["Other", "songs", "from", "2015", "like", "I", "Don", "'t", "Like", "It", ",", "I", "Love", "It", "by", "Flo", "Rida", ",", "Adventure", "of", "a", "Lifetime", "by", "Coldplay", ",", "Back", "Together", "by", "Robin", "Thicke", "and", "Levels", "by", "Nick", "Jonas", "feature", "disco", "elements", "as", "well", "."], "labels": ["O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "I-song", "I-song", "I-song", "I-song", "O", "B-musical artist", "I-musical artist", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-band", "O", "B-song", "I-song", "O", "B-musical artist", "I-musical artist", "O", "B-song", "O", "B-musical artist", "I-musical artist", "O", "B-music genre", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, person, musical instrument, award, event, band, location, organization, album, country, musical artist, song and O.\nSentence: Other songs from 2015 like I Don 't Like It , I Love It by Flo Rida , Adventure of a Lifetime by Coldplay , Back Together by Robin Thicke and Levels by Nick Jonas feature disco elements as well .", "prompt_labels": "Other(O) songs(O) from(O) 2015(O) like(O) I(B-song) Don(I-song) 't(I-song) Like(I-song) It(I-song) ,(I-song) I(I-song) Love(I-song) It(I-song) by(O) Flo(B-musical artist) Rida(I-musical artist) ,(O) Adventure(B-song) of(I-song) a(I-song) Lifetime(I-song) by(O) Coldplay(B-band) ,(O) Back(B-song) Together(I-song) by(O) Robin(B-musical artist) Thicke(I-musical artist) and(O) Levels(B-song) by(O) Nick(B-musical artist) Jonas(I-musical artist) feature(O) disco(B-music genre) elements(O) as(O) well(O) .(O)"}}
{"id": "129", "dataset": "crossner_music", "split": "test", "label_list": ["person", "music genre", "event", "award", "album", "location", "musical artist", "song", "band", "musical instrument", "country", "organization"], "instance": {"id": "129", "words": ["While", "they", "did", "not", "release", "any", "studio", "albums", "during", "this", "period", ",", "Devo", "sporadically", "reconvened", "to", "record", "a", "number", "of", "songs", "for", "various", "films", "and", "compilations", ",", "including", "a", "cover", "of", "the", "Nine", "Inch", "Nails", "hit", "Head", "Like", "a", "Hole", "for", "the", "1992", "film", "Police", "Story", "3", ":", "Super", "Cop", "and", "a", "new", "recording", "of", "Girl", "U", "Want", "on", "the", "soundtrack", "to", "the", "1995", "film", "Tank", "Girl", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, music genre, event, award, album, location, musical artist, song, band, musical instrument, country, organization and O.\nSentence: While they did not release any studio albums during this period , Devo sporadically reconvened to record a number of songs for various films and compilations , including a cover of the Nine Inch Nails hit Head Like a Hole for the 1992 film Police Story 3 : Super Cop and a new recording of Girl U Want on the soundtrack to the 1995 film Tank Girl .", "prompt_labels": "While(O) they(O) did(O) not(O) release(O) any(O) studio(O) albums(O) during(O) this(O) period(O) ,(O) Devo(B-band) sporadically(O) reconvened(O) to(O) record(O) a(O) number(O) of(O) songs(O) for(O) various(O) films(O) and(O) compilations(O) ,(O) including(O) a(O) cover(O) of(O) the(O) Nine(B-band) Inch(I-band) Nails(I-band) hit(O) Head(B-song) Like(I-song) a(I-song) Hole(I-song) for(O) the(O) 1992(O) film(O) Police(O) Story(O) 3(O) :(O) Super(O) Cop(O) and(O) a(O) new(O) recording(O) of(O) Girl(B-song) U(I-song) Want(I-song) on(O) the(O) soundtrack(O) to(O) the(O) 1995(O) film(O) Tank(O) Girl(O) .(O)"}}
{"id": "257", "dataset": "crossner_music", "split": "test", "label_list": ["song", "person", "musical artist", "band", "country", "event", "award", "album", "music genre", "organization", "musical instrument", "location"], "instance": {"id": "257", "words": ["The", "first", "known", "opera", "from", "Turkey", "(", "the", "Ottoman", "Empire", ")", "was", "Arshak", "II", ",", "which", "was", "an", "Armenia", "n", "opera", "composed", "by", "an", "ethnic", "Armenian", "composer", "Tigran", "Chukhajian", "in", "1868", "and", "partially", "performed", "in", "1873", "."], "labels": ["O", "O", "O", "O", "O", "B-country", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, person, musical artist, band, country, event, award, album, music genre, organization, musical instrument, location and O.\nSentence: The first known opera from Turkey ( the Ottoman Empire ) was Arshak II , which was an Armenia n opera composed by an ethnic Armenian composer Tigran Chukhajian in 1868 and partially performed in 1873 .", "prompt_labels": "The(O) first(O) known(O) opera(O) from(O) Turkey(B-country) ((O) the(O) Ottoman(B-country) Empire(I-country) )(O) was(O) Arshak(O) II(O) ,(O) which(O) was(O) an(O) Armenia(B-country) n(O) opera(O) composed(O) by(O) an(O) ethnic(O) Armenian(O) composer(O) Tigran(B-person) Chukhajian(I-person) in(O) 1868(O) and(O) partially(O) performed(O) in(O) 1873(O) .(O)"}}
{"id": "207", "dataset": "crossner_music", "split": "test", "label_list": ["location", "song", "music genre", "country", "award", "person", "organization", "musical instrument", "band", "event", "album", "musical artist"], "instance": {"id": "207", "words": ["For", "Revolver", "he", "selected", "five", "non-", "Heavy", "metal", "music", "records", "that", "influenced", "him", ":", "The", "Cure", "'s", "Pornography", ",", "Helium", "'s", "No", "Guitars", ",", "Mogwai", "EP", "+", "2", ",", "My", "Bloody", "Valentine", "'s", "Loveless", "and", "The", "Smashing", "Pumpkins", "'", "Siamese", "Dream", "."], "labels": ["O", "B-band", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "B-band", "O", "B-album", "O", "B-band", "O", "B-album", "I-album", "O", "B-band", "B-album", "I-album", "I-album", "O", "B-band", "I-band", "I-band", "O", "B-album", "O", "B-band", "I-band", "I-band", "O", "B-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, song, music genre, country, award, person, organization, musical instrument, band, event, album, musical artist and O.\nSentence: For Revolver he selected five non- Heavy metal music records that influenced him : The Cure 's Pornography , Helium 's No Guitars , Mogwai EP + 2 , My Bloody Valentine 's Loveless and The Smashing Pumpkins ' Siamese Dream .", "prompt_labels": "For(O) Revolver(B-band) he(O) selected(O) five(O) non-(O) Heavy(B-music genre) metal(I-music genre) music(I-music genre) records(O) that(O) influenced(O) him(O) :(O) The(O) Cure(B-band) 's(O) Pornography(B-album) ,(O) Helium(B-band) 's(O) No(B-album) Guitars(I-album) ,(O) Mogwai(B-band) EP(B-album) +(I-album) 2(I-album) ,(O) My(B-band) Bloody(I-band) Valentine(I-band) 's(O) Loveless(B-album) and(O) The(B-band) Smashing(I-band) Pumpkins(I-band) '(O) Siamese(B-album) Dream(I-album) .(O)"}}
{"id": "166", "dataset": "crossner_music", "split": "test", "label_list": ["award", "event", "song", "music genre", "country", "organization", "person", "band", "musical instrument", "location", "album", "musical artist"], "instance": {"id": "166", "words": ["For", "portraying", "both", "gunfighter", "Kid", "Shelleen", "and", "criminal", "Tim", "Strawn", ",", "he", "won", "the", "Academy", "Award", "for", "Best", "Actor", ",", "along", "with", "a", "BAFTA", "Award", "for", "Best", "Actor", "in", "a", "Leading", "Role", ",", "a", "Golden", "Globe", "Award", ",", "an", "National", "Board", "of", "Review", "Award", "for", "Best", "Actor", ",", "and", "the", "Silver", "Bear", "for", "Best", "Actor", "."], "labels": ["O", "O", "O", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, song, music genre, country, organization, person, band, musical instrument, location, album, musical artist and O.\nSentence: For portraying both gunfighter Kid Shelleen and criminal Tim Strawn , he won the Academy Award for Best Actor , along with a BAFTA Award for Best Actor in a Leading Role , a Golden Globe Award , an National Board of Review Award for Best Actor , and the Silver Bear for Best Actor .", "prompt_labels": "For(O) portraying(O) both(O) gunfighter(O) Kid(B-person) Shelleen(I-person) and(O) criminal(O) Tim(B-person) Strawn(I-person) ,(O) he(O) won(O) the(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) ,(O) along(O) with(O) a(O) BAFTA(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) in(I-award) a(I-award) Leading(I-award) Role(I-award) ,(O) a(O) Golden(B-award) Globe(I-award) Award(I-award) ,(O) an(O) National(B-award) Board(I-award) of(I-award) Review(I-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) ,(O) and(O) the(O) Silver(B-award) Bear(I-award) for(I-award) Best(I-award) Actor(I-award) .(O)"}}
{"id": "383", "dataset": "crossner_music", "split": "test", "label_list": ["musical instrument", "event", "organization", "country", "song", "band", "music genre", "award", "album", "musical artist", "person", "location"], "instance": {"id": "383", "words": ["A", "third", "UK", "tour", "for", "2017", "/", "2018", "opened", "at", "the", "Curve", "in", "Leicester", ",", "and", "also", "toured", "to", "the", "Birmingham", "Hippodrome", ",", "the", "Bord", "Gáis", "Energy", "Theatre", "in", "Dublin", ",", "the", "Wales", "Millennium", "Centre", "in", "Cardiff", ",", "the", "Edinburgh", "Festival", "Theatre", ",", "the", "Mayflower", "Theatre", "in", "Southampton", "and", "the", "Palace", "Theatre", "in", "Manchester", "."], "labels": ["O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "B-location", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "O", "O", "B-location", "I-location", "I-location", "O", "O", "B-location", "I-location", "O", "B-location", "O", "O", "B-location", "I-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, event, organization, country, song, band, music genre, award, album, musical artist, person, location and O.\nSentence: A third UK tour for 2017 / 2018 opened at the Curve in Leicester , and also toured to the Birmingham Hippodrome , the Bord Gáis Energy Theatre in Dublin , the Wales Millennium Centre in Cardiff , the Edinburgh Festival Theatre , the Mayflower Theatre in Southampton and the Palace Theatre in Manchester .", "prompt_labels": "A(O) third(O) UK(B-country) tour(O) for(O) 2017(O) /(O) 2018(O) opened(O) at(O) the(O) Curve(B-organization) in(O) Leicester(B-location) ,(O) and(O) also(O) toured(O) to(O) the(O) Birmingham(B-location) Hippodrome(I-location) ,(O) the(O) Bord(B-location) Gáis(I-location) Energy(I-location) Theatre(I-location) in(O) Dublin(B-location) ,(O) the(O) Wales(B-location) Millennium(I-location) Centre(I-location) in(O) Cardiff(B-location) ,(O) the(O) Edinburgh(B-location) Festival(I-location) Theatre(I-location) ,(O) the(O) Mayflower(B-location) Theatre(I-location) in(O) Southampton(B-location) and(O) the(O) Palace(B-location) Theatre(I-location) in(O) Manchester(B-location) .(O)"}}
{"id": "85", "dataset": "crossner_music", "split": "test", "label_list": ["organization", "musical instrument", "country", "song", "person", "location", "event", "award", "music genre", "band", "album", "musical artist"], "instance": {"id": "85", "words": ["Nathan", "Phillips", "Square", "was", "also", "a", "venue", "for", "Wheelchair", "tennis", "for", "the", "2017", "Invictus", "Games", "."], "labels": ["B-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, musical instrument, country, song, person, location, event, award, music genre, band, album, musical artist and O.\nSentence: Nathan Phillips Square was also a venue for Wheelchair tennis for the 2017 Invictus Games .", "prompt_labels": "Nathan(B-location) Phillips(I-location) Square(I-location) was(O) also(O) a(O) venue(O) for(O) Wheelchair(O) tennis(O) for(O) the(O) 2017(B-event) Invictus(I-event) Games(I-event) .(O)"}}
{"id": "318", "dataset": "crossner_music", "split": "test", "label_list": ["song", "organization", "award", "music genre", "album", "person", "location", "musical instrument", "musical artist", "band", "event", "country"], "instance": {"id": "318", "words": ["Shore", "has", "also", "been", "honored", "with", "awards", "from", "National", "Board", "of", "Review", ",", "Recording", "Academy", "Honors", ",", "Broadcast", "Film", "Critics", "Association", ",", "Chicago", "Film", "Critics", ",", "Genie", "Award", ",", "World", "Soundtrack", "Award", ",", "New", "York", "'s", "Gotham", "Award", ",", "and", "The", "Saturn", "Award", "for", "Science", "Fiction", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-award", "I-award", "I-award", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-award", "I-award", "O", "B-award", "I-award", "I-award", "O", "B-location", "I-location", "O", "B-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, organization, award, music genre, album, person, location, musical instrument, musical artist, band, event, country and O.\nSentence: Shore has also been honored with awards from National Board of Review , Recording Academy Honors , Broadcast Film Critics Association , Chicago Film Critics , Genie Award , World Soundtrack Award , New York 's Gotham Award , and The Saturn Award for Science Fiction .", "prompt_labels": "Shore(B-musical artist) has(O) also(O) been(O) honored(O) with(O) awards(O) from(O) National(B-organization) Board(I-organization) of(I-organization) Review(I-organization) ,(O) Recording(B-award) Academy(I-award) Honors(I-award) ,(O) Broadcast(B-organization) Film(I-organization) Critics(I-organization) Association(I-organization) ,(O) Chicago(B-organization) Film(I-organization) Critics(I-organization) ,(O) Genie(B-award) Award(I-award) ,(O) World(B-award) Soundtrack(I-award) Award(I-award) ,(O) New(B-location) York(I-location) 's(O) Gotham(B-award) Award(I-award) ,(O) and(O) The(O) Saturn(B-award) Award(I-award) for(I-award) Science(I-award) Fiction(I-award) .(O)"}}
{"id": "276", "dataset": "crossner_music", "split": "test", "label_list": ["person", "organization", "song", "band", "musical instrument", "award", "event", "album", "location", "music genre", "musical artist", "country"], "instance": {"id": "276", "words": ["Other", "acts", "who", "became", "prominent", "in", "the", "alt-country", "genre", "during", "the", "1990s", "and", "2000s", "included", "The", "Bottle", "Rockets", ",", "The", "Handsome", "Family", ",", "Blue", "Mountain", ",", "Robbie", "Fulks", ",", "Blood", "Oranges", ",", "Bright", "Eyes", ",", "Drive-By", "Truckers", ",", "Old", "97", "'s", ",", "Old", "Crow", "Medicine", "Show", ",", "Nickel", "Creek", ",", "Neko", "Case", ",", "and", "Whiskeytown", ",", "whose", "lead", "singer", "Ryan", "Adams", "later", "had", "a", "successful", "solo-career", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "O", "B-band", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, song, band, musical instrument, award, event, album, location, music genre, musical artist, country and O.\nSentence: Other acts who became prominent in the alt-country genre during the 1990s and 2000s included The Bottle Rockets , The Handsome Family , Blue Mountain , Robbie Fulks , Blood Oranges , Bright Eyes , Drive-By Truckers , Old 97 's , Old Crow Medicine Show , Nickel Creek , Neko Case , and Whiskeytown , whose lead singer Ryan Adams later had a successful solo-career .", "prompt_labels": "Other(O) acts(O) who(O) became(O) prominent(O) in(O) the(O) alt-country(B-music genre) genre(O) during(O) the(O) 1990s(O) and(O) 2000s(O) included(O) The(B-band) Bottle(I-band) Rockets(I-band) ,(O) The(B-band) Handsome(I-band) Family(I-band) ,(O) Blue(B-band) Mountain(I-band) ,(O) Robbie(B-band) Fulks(I-band) ,(O) Blood(B-band) Oranges(I-band) ,(O) Bright(B-band) Eyes(I-band) ,(O) Drive-By(B-band) Truckers(I-band) ,(O) Old(B-band) 97(I-band) 's(O) ,(O) Old(B-band) Crow(I-band) Medicine(I-band) Show(I-band) ,(O) Nickel(B-band) Creek(I-band) ,(O) Neko(B-musical artist) Case(I-musical artist) ,(O) and(O) Whiskeytown(B-band) ,(O) whose(O) lead(O) singer(O) Ryan(B-musical artist) Adams(I-musical artist) later(O) had(O) a(O) successful(O) solo-career(O) .(O)"}}
{"id": "436", "dataset": "crossner_music", "split": "test", "label_list": ["music genre", "country", "musical artist", "musical instrument", "person", "event", "song", "organization", "location", "band", "award", "album"], "instance": {"id": "436", "words": ["Foreman", "wrote", "Melbourne", "Girl", "for", "the", "Closing", "Ceremony", "of", "the", "2002", "Commonwealth", "Games", "in", "Manchester", ",", "UK", "which", "was", "performed", "by", "Vanessa", "Amorosi", "."], "labels": ["O", "O", "B-song", "I-song", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "B-location", "O", "B-country", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, country, musical artist, musical instrument, person, event, song, organization, location, band, award, album and O.\nSentence: Foreman wrote Melbourne Girl for the Closing Ceremony of the 2002 Commonwealth Games in Manchester , UK which was performed by Vanessa Amorosi .", "prompt_labels": "Foreman(O) wrote(O) Melbourne(B-song) Girl(I-song) for(O) the(O) Closing(O) Ceremony(O) of(O) the(O) 2002(B-event) Commonwealth(I-event) Games(I-event) in(O) Manchester(B-location) ,(O) UK(B-country) which(O) was(O) performed(O) by(O) Vanessa(B-musical artist) Amorosi(I-musical artist) .(O)"}}
{"id": "264", "dataset": "crossner_music", "split": "test", "label_list": ["album", "location", "organization", "musical instrument", "country", "band", "musical artist", "event", "person", "music genre", "song", "award"], "instance": {"id": "264", "words": ["Within", "months", ",", "Hendrix", "earned", "three", "UK", "top", "ten", "hits", "with", "the", "Jimi", "Hendrix", "Experience", ":", "Hey", "Joe", ",", "Purple", "Haze", ",", "and", "The", "Wind", "Cries", "Mary", "."], "labels": ["O", "O", "O", "B-musical artist", "O", "O", "B-country", "O", "O", "O", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-song", "I-song", "O", "B-song", "I-song", "O", "O", "B-song", "I-song", "I-song", "I-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, location, organization, musical instrument, country, band, musical artist, event, person, music genre, song, award and O.\nSentence: Within months , Hendrix earned three UK top ten hits with the Jimi Hendrix Experience : Hey Joe , Purple Haze , and The Wind Cries Mary .", "prompt_labels": "Within(O) months(O) ,(O) Hendrix(B-musical artist) earned(O) three(O) UK(B-country) top(O) ten(O) hits(O) with(O) the(B-band) Jimi(I-band) Hendrix(I-band) Experience(I-band) :(O) Hey(B-song) Joe(I-song) ,(O) Purple(B-song) Haze(I-song) ,(O) and(O) The(B-song) Wind(I-song) Cries(I-song) Mary(I-song) .(O)"}}
{"id": "302", "dataset": "crossner_music", "split": "test", "label_list": ["musical instrument", "location", "country", "organization", "event", "song", "award", "music genre", "person", "band", "album", "musical artist"], "instance": {"id": "302", "words": ["By", "the", "mid-1980s", ",", "bands", "began", "proliferating", "and", "became", "increasingly", "popular", ",", "including", "the", "Sisters", "of", "Mercy", ",", "the", "Mission", ",", "Alien", "Sex", "Fiend", ",", "the", "March", "Violets", ",", "Xmal", "Deutschland", ",", "the", "Membranes", ",", "and", "Fields", "of", "the", "Nephilim", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "O", "B-band", "I-band", "I-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, location, country, organization, event, song, award, music genre, person, band, album, musical artist and O.\nSentence: By the mid-1980s , bands began proliferating and became increasingly popular , including the Sisters of Mercy , the Mission , Alien Sex Fiend , the March Violets , Xmal Deutschland , the Membranes , and Fields of the Nephilim .", "prompt_labels": "By(O) the(O) mid-1980s(O) ,(O) bands(O) began(O) proliferating(O) and(O) became(O) increasingly(O) popular(O) ,(O) including(O) the(B-band) Sisters(I-band) of(I-band) Mercy(I-band) ,(O) the(B-band) Mission(I-band) ,(O) Alien(B-band) Sex(I-band) Fiend(I-band) ,(O) the(B-band) March(I-band) Violets(I-band) ,(O) Xmal(B-band) Deutschland(I-band) ,(O) the(B-band) Membranes(I-band) ,(O) and(O) Fields(B-band) of(I-band) the(I-band) Nephilim(I-band) .(O)"}}
{"id": "44", "dataset": "crossner_music", "split": "test", "label_list": ["person", "band", "organization", "music genre", "musical instrument", "location", "event", "award", "musical artist", "country", "album", "song"], "instance": {"id": "44", "words": ["The", "National", "Squash", "Centre", "and", "the", "National", "Cycling", "Centre", ",", "which", "includes", "both", "the", "Manchester", "Velodrome", "and", "the", "National", "Indoor", "BMX", "Arena", ",", "are", "all", "a", "short", "distance", "from", "the", "stadium", "."], "labels": ["O", "B-location", "I-location", "I-location", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, band, organization, music genre, musical instrument, location, event, award, musical artist, country, album, song and O.\nSentence: The National Squash Centre and the National Cycling Centre , which includes both the Manchester Velodrome and the National Indoor BMX Arena , are all a short distance from the stadium .", "prompt_labels": "The(O) National(B-location) Squash(I-location) Centre(I-location) and(O) the(O) National(B-location) Cycling(I-location) Centre(I-location) ,(O) which(O) includes(O) both(O) the(O) Manchester(B-location) Velodrome(I-location) and(O) the(O) National(B-location) Indoor(I-location) BMX(I-location) Arena(I-location) ,(O) are(O) all(O) a(O) short(O) distance(O) from(O) the(O) stadium(O) .(O)"}}
{"id": "235", "dataset": "crossner_music", "split": "test", "label_list": ["event", "song", "musical instrument", "musical artist", "country", "organization", "music genre", "album", "location", "band", "award", "person"], "instance": {"id": "235", "words": ["It", "includes", "folklore", "music", "of", "parts", "of", "Bolivia", ",", "Ecuador", ",", "Chile", ",", "Colombia", ",", "Peru", "and", "Venezuela", "."], "labels": ["O", "O", "B-music genre", "I-music genre", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, song, musical instrument, musical artist, country, organization, music genre, album, location, band, award, person and O.\nSentence: It includes folklore music of parts of Bolivia , Ecuador , Chile , Colombia , Peru and Venezuela .", "prompt_labels": "It(O) includes(O) folklore(B-music genre) music(I-music genre) of(O) parts(O) of(O) Bolivia(B-country) ,(O) Ecuador(B-country) ,(O) Chile(B-country) ,(O) Colombia(B-country) ,(O) Peru(B-country) and(O) Venezuela(B-country) .(O)"}}
{"id": "190", "dataset": "crossner_music", "split": "test", "label_list": ["country", "location", "song", "person", "musical artist", "album", "event", "organization", "music genre", "award", "musical instrument", "band"], "instance": {"id": "190", "words": ["She", "reprised", "her", "role", "in", "it", "the", "next", "year", ",", "playing", "the", "Edinburgh", "Playhouse", "from", "November", "19", "to", "December", "8", ",", "2007", "and", "the", "Wales", "Millennium", "Centre", "in", "the", "Donald", "Gordon", "Theatre", "from", "December", "13", ",", "2007", "through", "January", "12", ",", "2008", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, song, person, musical artist, album, event, organization, music genre, award, musical instrument, band and O.\nSentence: She reprised her role in it the next year , playing the Edinburgh Playhouse from November 19 to December 8 , 2007 and the Wales Millennium Centre in the Donald Gordon Theatre from December 13 , 2007 through January 12 , 2008 .", "prompt_labels": "She(O) reprised(O) her(O) role(O) in(O) it(O) the(O) next(O) year(O) ,(O) playing(O) the(O) Edinburgh(B-location) Playhouse(I-location) from(O) November(O) 19(O) to(O) December(O) 8(O) ,(O) 2007(O) and(O) the(O) Wales(B-location) Millennium(I-location) Centre(I-location) in(O) the(O) Donald(B-location) Gordon(I-location) Theatre(I-location) from(O) December(O) 13(O) ,(O) 2007(O) through(O) January(O) 12(O) ,(O) 2008(O) .(O)"}}
{"id": "260", "dataset": "crossner_music", "split": "test", "label_list": ["band", "album", "event", "musical instrument", "person", "award", "country", "music genre", "organization", "musical artist", "location", "song"], "instance": {"id": "260", "words": ["The", "stadium", "hosted", "the", "2003", "World", "Championships", "in", "Athletics", "and", "from", "1999", "to", "2016", "it", "hosted", "the", "annual", "Meeting", "Areva", "athletics", "meet", "."], "labels": ["O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, album, event, musical instrument, person, award, country, music genre, organization, musical artist, location, song and O.\nSentence: The stadium hosted the 2003 World Championships in Athletics and from 1999 to 2016 it hosted the annual Meeting Areva athletics meet .", "prompt_labels": "The(O) stadium(O) hosted(O) the(O) 2003(B-event) World(I-event) Championships(I-event) in(I-event) Athletics(I-event) and(O) from(O) 1999(O) to(O) 2016(O) it(O) hosted(O) the(O) annual(O) Meeting(B-event) Areva(I-event) athletics(O) meet(O) .(O)"}}
{"id": "96", "dataset": "crossner_music", "split": "test", "label_list": ["musical instrument", "song", "country", "person", "album", "organization", "band", "music genre", "musical artist", "event", "award", "location"], "instance": {"id": "96", "words": ["On", "February", "17", ",", "2020", ",", "a", "50th", "anniversary", "concert", "production", "of", "Joseph", "was", "staged", "at", "Lincoln", "Center", "'", "s", "David", "Geffen", "Hall", "in", "New", "York", "City", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, song, country, person, album, organization, band, music genre, musical artist, event, award, location and O.\nSentence: On February 17 , 2020 , a 50th anniversary concert production of Joseph was staged at Lincoln Center ' s David Geffen Hall in New York City .", "prompt_labels": "On(O) February(O) 17(O) ,(O) 2020(O) ,(O) a(O) 50th(O) anniversary(O) concert(O) production(O) of(O) Joseph(B-organization) was(O) staged(O) at(O) Lincoln(B-location) Center(I-location) '(O) s(O) David(B-location) Geffen(I-location) Hall(I-location) in(O) New(B-location) York(I-location) City(I-location) .(O)"}}
{"id": "87", "dataset": "crossner_music", "split": "test", "label_list": ["country", "organization", "band", "song", "album", "musical artist", "location", "event", "person", "music genre", "musical instrument", "award"], "instance": {"id": "87", "words": ["The", "third", "generation", "(", "1950s-1960s", ")", "started", "at", "the", "end", "of", "World", "War", "II", "with", "mountaineer", "string", "band", "music", "known", "as", "Bluegrass", "music", ",", "which", "emerged", "when", "Bill", "Monroe", ",", "along", "with", "Lester", "Flatt", "and", "Earl", "Scruggs", "were", "introduced", "by", "Roy", "Acuff", "at", "the", "Grand", "Ole", "Opry", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "B-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, band, song, album, musical artist, location, event, person, music genre, musical instrument, award and O.\nSentence: The third generation ( 1950s-1960s ) started at the end of World War II with mountaineer string band music known as Bluegrass music , which emerged when Bill Monroe , along with Lester Flatt and Earl Scruggs were introduced by Roy Acuff at the Grand Ole Opry .", "prompt_labels": "The(O) third(O) generation(O) ((O) 1950s-1960s(O) )(O) started(O) at(O) the(O) end(O) of(O) World(B-event) War(I-event) II(I-event) with(O) mountaineer(O) string(O) band(O) music(O) known(O) as(O) Bluegrass(B-music genre) music(I-music genre) ,(O) which(O) emerged(O) when(O) Bill(B-musical artist) Monroe(I-musical artist) ,(O) along(O) with(O) Lester(B-musical artist) Flatt(I-musical artist) and(O) Earl(B-musical artist) Scruggs(I-musical artist) were(O) introduced(O) by(O) Roy(B-musical artist) Acuff(I-musical artist) at(O) the(O) Grand(B-location) Ole(I-location) Opry(I-location) .(O)"}}
{"id": "236", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "band", "country", "musical instrument", "song", "event", "album", "award", "music genre", "person", "organization", "location"], "instance": {"id": "236", "words": ["Collette", "supports", "various", "charities", "including", "Médecins", "Sans", "Frontières", ",", "Amnesty", "International", "and", "Feeding", "America", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, band, country, musical instrument, song, event, album, award, music genre, person, organization, location and O.\nSentence: Collette supports various charities including Médecins Sans Frontières , Amnesty International and Feeding America .", "prompt_labels": "Collette(B-musical artist) supports(O) various(O) charities(O) including(O) Médecins(B-organization) Sans(I-organization) Frontières(I-organization) ,(O) Amnesty(B-organization) International(I-organization) and(O) Feeding(B-organization) America(I-organization) .(O)"}}
{"id": "112", "dataset": "crossner_music", "split": "test", "label_list": ["music genre", "location", "album", "award", "person", "musical instrument", "song", "organization", "musical artist", "country", "band", "event"], "instance": {"id": "112", "words": ["Govere", "first", "attracted", "media", "attention", "by", "being", "the", "first", "black", "female", "gymnast", "to", "represent", "Zimbabwe", "in", "artistic", "gymnastics", "at", "the", "1999", "All-Africa", "Games", ",", "and", "later", "by", "co-founding", "the", "Kijana", "Project", ",", "which", "provides", "relief", "for", "AIDS", "orphans", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, location, album, award, person, musical instrument, song, organization, musical artist, country, band, event and O.\nSentence: Govere first attracted media attention by being the first black female gymnast to represent Zimbabwe in artistic gymnastics at the 1999 All-Africa Games , and later by co-founding the Kijana Project , which provides relief for AIDS orphans .", "prompt_labels": "Govere(B-person) first(O) attracted(O) media(O) attention(O) by(O) being(O) the(O) first(O) black(O) female(O) gymnast(O) to(O) represent(O) Zimbabwe(B-country) in(O) artistic(O) gymnastics(O) at(O) the(O) 1999(B-event) All-Africa(I-event) Games(I-event) ,(O) and(O) later(O) by(O) co-founding(O) the(O) Kijana(B-organization) Project(I-organization) ,(O) which(O) provides(O) relief(O) for(O) AIDS(O) orphans(O) .(O)"}}
{"id": "382", "dataset": "crossner_music", "split": "test", "label_list": ["organization", "band", "location", "event", "musical instrument", "person", "song", "music genre", "award", "country", "album", "musical artist"], "instance": {"id": "382", "words": ["Pioneered", "by", "black-doom", "bands", "like", "Ophthalamia", ",", "Katatonia", ",", "Bethlehem", ",", "Forgotten", "Tomb", "and", "Shining", ",", "depressive", "suicidal", "black", "metal", ",", "also", "known", "as", "suicidal", "black", "metal", ",", "depressive", "black", "metal", "or", "DSBM", ",", "is", "a", "style", "that", "melds", "the", "second", "wave", "-style", "of", "black", "metal", "with", "doom", "metal", ","], "labels": ["O", "O", "B-music genre", "O", "O", "B-band", "O", "B-band", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "O", "B-music genre", "I-music genre", "I-music genre", "I-music genre", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, band, location, event, musical instrument, person, song, music genre, award, country, album, musical artist and O.\nSentence: Pioneered by black-doom bands like Ophthalamia , Katatonia , Bethlehem , Forgotten Tomb and Shining , depressive suicidal black metal , also known as suicidal black metal , depressive black metal or DSBM , is a style that melds the second wave -style of black metal with doom metal ,", "prompt_labels": "Pioneered(O) by(O) black-doom(B-music genre) bands(O) like(O) Ophthalamia(B-band) ,(O) Katatonia(B-band) ,(O) Bethlehem(B-band) ,(O) Forgotten(B-band) Tomb(I-band) and(O) Shining(B-band) ,(O) depressive(B-music genre) suicidal(I-music genre) black(I-music genre) metal(I-music genre) ,(O) also(O) known(O) as(O) suicidal(B-music genre) black(I-music genre) metal(I-music genre) ,(O) depressive(B-music genre) black(I-music genre) metal(I-music genre) or(O) DSBM(B-music genre) ,(O) is(O) a(O) style(O) that(O) melds(O) the(O) second(O) wave(O) -style(O) of(O) black(B-music genre) metal(I-music genre) with(O) doom(B-music genre) metal(I-music genre) ,(O)"}}
{"id": "60", "dataset": "crossner_music", "split": "test", "label_list": ["band", "musical artist", "organization", "country", "musical instrument", "song", "location", "person", "award", "event", "music genre", "album"], "instance": {"id": "60", "words": ["The", "New", "Orleans", "setting", "of", "the", "film", "played", "to", "Newman", "'s", "musical", "strengths", ",", "and", "his", "songs", "contained", "elements", "of", "Cajun", "music", ",", "zydeco", ",", "blues", "and", "Dixieland", "."], "labels": ["O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "O", "B-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical artist, organization, country, musical instrument, song, location, person, award, event, music genre, album and O.\nSentence: The New Orleans setting of the film played to Newman 's musical strengths , and his songs contained elements of Cajun music , zydeco , blues and Dixieland .", "prompt_labels": "The(O) New(B-music genre) Orleans(I-music genre) setting(O) of(O) the(O) film(O) played(O) to(O) Newman(B-musical artist) 's(O) musical(O) strengths(O) ,(O) and(O) his(O) songs(O) contained(O) elements(O) of(O) Cajun(B-music genre) music(I-music genre) ,(O) zydeco(B-music genre) ,(O) blues(B-music genre) and(O) Dixieland(B-music genre) .(O)"}}
{"id": "61", "dataset": "crossner_music", "split": "test", "label_list": ["country", "musical artist", "musical instrument", "album", "song", "music genre", "event", "band", "person", "organization", "location", "award"], "instance": {"id": "61", "words": ["They", "also", "released", "five", "singles", "to", "promote", "the", "album", ":", "Enter", "Sandman", ",", "The", "Unforgiven", ",", "Nothing", "Else", "Matters", ",", "Wherever", "I", "May", "Roam", ",", "and", "Sad", "but", "TRUE", ",", "all", "of", "which", "have", "been", "considered", "to", "be", "among", "the", "band", "'s", "best-known", "songs", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "O", "B-song", "I-song", "O", "B-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "B-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, musical artist, musical instrument, album, song, music genre, event, band, person, organization, location, award and O.\nSentence: They also released five singles to promote the album : Enter Sandman , The Unforgiven , Nothing Else Matters , Wherever I May Roam , and Sad but TRUE , all of which have been considered to be among the band 's best-known songs .", "prompt_labels": "They(O) also(O) released(O) five(O) singles(O) to(O) promote(O) the(O) album(O) :(O) Enter(B-song) Sandman(I-song) ,(O) The(B-song) Unforgiven(I-song) ,(O) Nothing(B-song) Else(I-song) Matters(I-song) ,(O) Wherever(B-song) I(I-song) May(I-song) Roam(I-song) ,(O) and(O) Sad(B-song) but(I-song) TRUE(I-song) ,(O) all(O) of(O) which(O) have(O) been(O) considered(O) to(O) be(O) among(O) the(O) band(O) 's(O) best-known(O) songs(O) .(O)"}}
{"id": "200", "dataset": "crossner_music", "split": "test", "label_list": ["song", "country", "organization", "event", "award", "music genre", "musical artist", "musical instrument", "band", "person", "location", "album"], "instance": {"id": "200", "words": ["Additionally", ",", "Grant", "won", "the", "Golden", "Globe", "Award", "for", "Best", "Actor", "-", "Motion", "Picture", "Musical", "or", "Comedy", "and", "the", "BAFTA", "Award", "for", "Best", "Actor", "in", "a", "Leading", "Role", ",", "and", "the", "film", "won", "the", "British", "Academy", "Film", "Awards", "BAFTA", "Award", "for", "Best", "Film", ",", "BAFTA", "Award", "for", "Best", "Direction", ",", "and", "BAFTA", "Award", "for", "Best", "Actress", "in", "a", "Supporting", "Role", "for", "Scott", "Thomas", "."], "labels": ["O", "O", "B-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, country, organization, event, award, music genre, musical artist, musical instrument, band, person, location, album and O.\nSentence: Additionally , Grant won the Golden Globe Award for Best Actor - Motion Picture Musical or Comedy and the BAFTA Award for Best Actor in a Leading Role , and the film won the British Academy Film Awards BAFTA Award for Best Film , BAFTA Award for Best Direction , and BAFTA Award for Best Actress in a Supporting Role for Scott Thomas .", "prompt_labels": "Additionally(O) ,(O) Grant(B-person) won(O) the(O) Golden(B-award) Globe(I-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) -(I-award) Motion(I-award) Picture(I-award) Musical(I-award) or(I-award) Comedy(I-award) and(O) the(O) BAFTA(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) in(I-award) a(I-award) Leading(I-award) Role(I-award) ,(O) and(O) the(O) film(O) won(O) the(O) British(B-award) Academy(I-award) Film(I-award) Awards(I-award) BAFTA(B-award) Award(I-award) for(I-award) Best(I-award) Film(I-award) ,(O) BAFTA(B-award) Award(I-award) for(I-award) Best(I-award) Direction(I-award) ,(O) and(O) BAFTA(B-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) in(I-award) a(I-award) Supporting(I-award) Role(I-award) for(O) Scott(B-person) Thomas(I-person) .(O)"}}
{"id": "293", "dataset": "crossner_music", "split": "test", "label_list": ["event", "award", "country", "location", "musical artist", "music genre", "song", "musical instrument", "album", "organization", "person", "band"], "instance": {"id": "293", "words": ["These", "have", "been", "used", "in", "clay", "shooting", "and", "were", "suggested", "for", "use", "in", "the", "Modern", "pentathlon", "at", "the", "2012", "Summer", "Olympics", "after", "a", "successful", "trial", "at", "the", "2010", "Summer", "Youth", "Olympics", "in", "Singapore", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, country, location, musical artist, music genre, song, musical instrument, album, organization, person, band and O.\nSentence: These have been used in clay shooting and were suggested for use in the Modern pentathlon at the 2012 Summer Olympics after a successful trial at the 2010 Summer Youth Olympics in Singapore .", "prompt_labels": "These(O) have(O) been(O) used(O) in(O) clay(O) shooting(O) and(O) were(O) suggested(O) for(O) use(O) in(O) the(O) Modern(B-event) pentathlon(I-event) at(I-event) the(I-event) 2012(I-event) Summer(I-event) Olympics(I-event) after(O) a(O) successful(O) trial(O) at(O) the(O) 2010(B-event) Summer(I-event) Youth(I-event) Olympics(I-event) in(O) Singapore(B-country) .(O)"}}
{"id": "163", "dataset": "crossner_music", "split": "test", "label_list": ["event", "band", "location", "person", "album", "musical instrument", "music genre", "country", "song", "musical artist", "organization", "award"], "instance": {"id": "163", "words": ["Baron", "Cohen", "was", "named", "Best", "Newcomer", "at", "the", "1999", "British", "Comedy", "Awards", "for", "The", "11", "O", "'Clock", "Show", ",", "and", "since", "then", ",", "he", "has", "received", "two", "British", "Academy", "of", "Film", "and", "Television", "Arts", "Awards", "for", "Da", "Ali", "G", "Show", ",", "several", "Emmy", "nominations", ",", "a", "nomination", "for", "an", "Academy", "Award", "for", "Best", "Adapted", "Screenplay", ",", "and", "a", "Golden", "Globe", "for", "Best", "Actor", "for", "his", "work", "in", "the", "feature", "film", "Borat", "."], "labels": ["B-person", "I-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "B-award", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, band, location, person, album, musical instrument, music genre, country, song, musical artist, organization, award and O.\nSentence: Baron Cohen was named Best Newcomer at the 1999 British Comedy Awards for The 11 O 'Clock Show , and since then , he has received two British Academy of Film and Television Arts Awards for Da Ali G Show , several Emmy nominations , a nomination for an Academy Award for Best Adapted Screenplay , and a Golden Globe for Best Actor for his work in the feature film Borat .", "prompt_labels": "Baron(B-person) Cohen(I-person) was(O) named(O) Best(B-award) Newcomer(I-award) at(I-award) the(I-award) 1999(I-award) British(I-award) Comedy(I-award) Awards(I-award) for(O) The(B-event) 11(I-event) O(I-event) 'Clock(I-event) Show(I-event) ,(O) and(O) since(O) then(O) ,(O) he(O) has(O) received(O) two(O) British(B-award) Academy(I-award) of(I-award) Film(I-award) and(I-award) Television(I-award) Arts(I-award) Awards(I-award) for(O) Da(B-event) Ali(I-event) G(I-event) Show(I-event) ,(O) several(O) Emmy(B-award) nominations(O) ,(O) a(O) nomination(O) for(O) an(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Adapted(I-award) Screenplay(I-award) ,(O) and(O) a(O) Golden(B-award) Globe(I-award) for(I-award) Best(I-award) Actor(I-award) for(O) his(O) work(O) in(O) the(O) feature(O) film(O) Borat(O) .(O)"}}
{"id": "396", "dataset": "crossner_music", "split": "test", "label_list": ["band", "music genre", "song", "organization", "country", "musical artist", "event", "location", "album", "award", "person", "musical instrument"], "instance": {"id": "396", "words": ["The", "band", "released", "seven", "studio", "albums", "between", "1994", "and", "2005", ":", "Sleater-Kinney", "(", "1995", ")", ",", "Call", "the", "Doctor", "(", "1996", ")", ",", "Dig", "Me", "Out", "(", "1997", ")", ",", "The", "Hot", "Rock", "(", "1999", ")", ",", "All", "Hands", "on", "the", "Bad", "One", "(", "2000", ")", ",", "One", "Beat", "(", "2002", ")", "and", "The", "Woods", "(", "2005", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, music genre, song, organization, country, musical artist, event, location, album, award, person, musical instrument and O.\nSentence: The band released seven studio albums between 1994 and 2005 : Sleater-Kinney ( 1995 ) , Call the Doctor ( 1996 ) , Dig Me Out ( 1997 ) , The Hot Rock ( 1999 ) , All Hands on the Bad One ( 2000 ) , One Beat ( 2002 ) and The Woods ( 2005 ) .", "prompt_labels": "The(O) band(O) released(O) seven(O) studio(O) albums(O) between(O) 1994(O) and(O) 2005(O) :(O) Sleater-Kinney(B-album) ((O) 1995(O) )(O) ,(O) Call(B-album) the(I-album) Doctor(I-album) ((O) 1996(O) )(O) ,(O) Dig(B-album) Me(I-album) Out(I-album) ((O) 1997(O) )(O) ,(O) The(B-album) Hot(I-album) Rock(I-album) ((O) 1999(O) )(O) ,(O) All(B-album) Hands(I-album) on(I-album) the(I-album) Bad(I-album) One(I-album) ((O) 2000(O) )(O) ,(O) One(B-album) Beat(I-album) ((O) 2002(O) )(O) and(O) The(B-album) Woods(I-album) ((O) 2005(O) )(O) .(O)"}}
{"id": "34", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "location", "event", "album", "country", "person", "band", "song", "musical instrument", "award", "organization", "music genre"], "instance": {"id": "34", "words": ["Other", "artists", "featured", "on", "the", "show", "include", "Michael", "Jackson", ",", "Barry", "White", ",", "Al", "Green", ",", "Tina", "Turner", ",", "Macy", "Gray", ",", "Gloria", "Gaynor", ",", "Chayanne", ",", "Barry", "Manilow", ",", "Anastacia", ",", "Elton", "John", ",", "Sting", "and", "Mariah", "Carey", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, location, event, album, country, person, band, song, musical instrument, award, organization, music genre and O.\nSentence: Other artists featured on the show include Michael Jackson , Barry White , Al Green , Tina Turner , Macy Gray , Gloria Gaynor , Chayanne , Barry Manilow , Anastacia , Elton John , Sting and Mariah Carey .", "prompt_labels": "Other(O) artists(O) featured(O) on(O) the(O) show(O) include(O) Michael(B-musical artist) Jackson(I-musical artist) ,(O) Barry(B-musical artist) White(I-musical artist) ,(O) Al(B-musical artist) Green(I-musical artist) ,(O) Tina(B-musical artist) Turner(I-musical artist) ,(O) Macy(B-musical artist) Gray(I-musical artist) ,(O) Gloria(B-musical artist) Gaynor(I-musical artist) ,(O) Chayanne(B-musical artist) ,(O) Barry(B-musical artist) Manilow(I-musical artist) ,(O) Anastacia(B-musical artist) ,(O) Elton(B-musical artist) John(I-musical artist) ,(O) Sting(B-musical artist) and(O) Mariah(B-musical artist) Carey(I-musical artist) .(O)"}}
{"id": "27", "dataset": "crossner_music", "split": "test", "label_list": ["organization", "song", "country", "award", "person", "event", "band", "musical instrument", "music genre", "musical artist", "album", "location"], "instance": {"id": "27", "words": ["He", "played", "on", "the", "live", "albums", "that", "would", "follow", "the", "release", "of", "Bitches", "Brew", ",", "taken", "from", "concerts", "at", "the", "Fillmore", "East", "in", "New", "York", "and", "Fillmore", "West", "in", "San", "Francisco", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, song, country, award, person, event, band, musical instrument, music genre, musical artist, album, location and O.\nSentence: He played on the live albums that would follow the release of Bitches Brew , taken from concerts at the Fillmore East in New York and Fillmore West in San Francisco .", "prompt_labels": "He(O) played(O) on(O) the(O) live(O) albums(O) that(O) would(O) follow(O) the(O) release(O) of(O) Bitches(B-album) Brew(I-album) ,(O) taken(O) from(O) concerts(O) at(O) the(O) Fillmore(B-location) East(I-location) in(O) New(B-location) York(I-location) and(O) Fillmore(B-location) West(I-location) in(O) San(B-location) Francisco(I-location) .(O)"}}
{"id": "183", "dataset": "crossner_music", "split": "test", "label_list": ["location", "organization", "music genre", "award", "event", "musical instrument", "album", "band", "person", "musical artist", "song", "country"], "instance": {"id": "183", "words": ["In", "addition", "to", "Lady", "Antebellum", ",", "groups", "such", "as", "Herrick", ",", "The", "Quebe", "Sisters", "Band", ",", "Little", "Big", "Town", ",", "The", "Band", "Perry", ",", "Gloriana", ",", "Thompson", "Square", ",", "Eli", "Young", "Band", ",", "Zac", "Brown", "Band", "and", "British", "duo", "The", "Shires", "have", "emerged", "to", "occupy", "a", "large", "portion", "of", "the", "new", "country", "artists", "in", "the", "popular", "scene", "along", "with", "solo", "singers", "Kacey", "Musgraves", "and", "Miranda", "Lambert", "."], "labels": ["O", "O", "O", "B-band", "I-band", "O", "O", "O", "O", "B-band", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "O", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, music genre, award, event, musical instrument, album, band, person, musical artist, song, country and O.\nSentence: In addition to Lady Antebellum , groups such as Herrick , The Quebe Sisters Band , Little Big Town , The Band Perry , Gloriana , Thompson Square , Eli Young Band , Zac Brown Band and British duo The Shires have emerged to occupy a large portion of the new country artists in the popular scene along with solo singers Kacey Musgraves and Miranda Lambert .", "prompt_labels": "In(O) addition(O) to(O) Lady(B-band) Antebellum(I-band) ,(O) groups(O) such(O) as(O) Herrick(B-band) ,(O) The(B-band) Quebe(I-band) Sisters(I-band) Band(I-band) ,(O) Little(B-band) Big(I-band) Town(I-band) ,(O) The(B-band) Band(I-band) Perry(I-band) ,(O) Gloriana(B-band) ,(O) Thompson(B-band) Square(I-band) ,(O) Eli(B-band) Young(I-band) Band(I-band) ,(O) Zac(B-band) Brown(I-band) Band(I-band) and(O) British(O) duo(O) The(B-band) Shires(I-band) have(O) emerged(O) to(O) occupy(O) a(O) large(O) portion(O) of(O) the(O) new(O) country(O) artists(O) in(O) the(O) popular(O) scene(O) along(O) with(O) solo(O) singers(O) Kacey(B-musical artist) Musgraves(I-musical artist) and(O) Miranda(B-musical artist) Lambert(I-musical artist) .(O)"}}
{"id": "277", "dataset": "crossner_music", "split": "test", "label_list": ["award", "musical artist", "album", "country", "event", "location", "organization", "music genre", "person", "song", "musical instrument", "band"], "instance": {"id": "277", "words": ["Respected", "for", "her", "versatility", ",", "she", "received", "an", "Academy", "Juvenile", "Award", ",", "a", "Golden", "Globe", "Award", ",", "a", "Special", "Tony", "Award", ",", "and", "was", "the", "first", "woman", "to", "win", "the", "Grammy", "Award", "for", "Album", "of", "the", "Year", "for", "her", "1961", "live", "recording", "Judy", "at", "Carnegie", "Hall", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, musical artist, album, country, event, location, organization, music genre, person, song, musical instrument, band and O.\nSentence: Respected for her versatility , she received an Academy Juvenile Award , a Golden Globe Award , a Special Tony Award , and was the first woman to win the Grammy Award for Album of the Year for her 1961 live recording Judy at Carnegie Hall .", "prompt_labels": "Respected(O) for(O) her(O) versatility(O) ,(O) she(O) received(O) an(O) Academy(B-award) Juvenile(I-award) Award(I-award) ,(O) a(O) Golden(B-award) Globe(I-award) Award(I-award) ,(O) a(O) Special(B-award) Tony(I-award) Award(I-award) ,(O) and(O) was(O) the(O) first(O) woman(O) to(O) win(O) the(O) Grammy(B-award) Award(I-award) for(I-award) Album(I-award) of(I-award) the(I-award) Year(I-award) for(O) her(O) 1961(O) live(O) recording(O) Judy(B-album) at(I-album) Carnegie(I-album) Hall(I-album) .(O)"}}
{"id": "405", "dataset": "crossner_music", "split": "test", "label_list": ["music genre", "event", "award", "musical artist", "location", "musical instrument", "country", "person", "organization", "song", "band", "album"], "instance": {"id": "405", "words": ["He", "worked", "with", "Berlin", "State", "Opera", ";", "La", "Scala", ",", "Milan", ";", "Royal", "Opera", "Stockholm", ";", "the", "Royal", "Opera", "House", "at", "Covent", "Garden", ",", "Chorégies", "d", "'Orange", "and", "Houston", "Grand", "Opera", "."], "labels": ["O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-location", "I-location", "O", "B-location", "O", "B-location", "I-location", "I-location", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "I-location", "O", "B-location", "I-location", "I-location", "O", "B-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, event, award, musical artist, location, musical instrument, country, person, organization, song, band, album and O.\nSentence: He worked with Berlin State Opera ; La Scala , Milan ; Royal Opera Stockholm ; the Royal Opera House at Covent Garden , Chorégies d 'Orange and Houston Grand Opera .", "prompt_labels": "He(O) worked(O) with(O) Berlin(B-organization) State(I-organization) Opera(I-organization) ;(O) La(B-location) Scala(I-location) ,(O) Milan(B-location) ;(O) Royal(B-location) Opera(I-location) Stockholm(I-location) ;(O) the(O) Royal(B-location) Opera(I-location) House(I-location) at(O) Covent(B-location) Garden(I-location) ,(O) Chorégies(B-location) d(I-location) 'Orange(I-location) and(O) Houston(B-location) Grand(I-location) Opera(I-location) .(O)"}}
{"id": "378", "dataset": "crossner_music", "split": "test", "label_list": ["music genre", "album", "band", "musical artist", "country", "song", "musical instrument", "award", "location", "event", "organization", "person"], "instance": {"id": "378", "words": ["By", "the", "time", "he", "released", "his", "debut", "album", "2Pacalypse", "Now", "in", "1991", ",", "he", "had", "become", "a", "central", "figure", "in", "West", "Coast", "hip", "hop", ",", "introducing", "social", "issues", "in", "the", "genre", "at", "a", "time", "when", "gangsta", "rap", "was", "dominant", "in", "the", "mainstream", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, album, band, musical artist, country, song, musical instrument, award, location, event, organization, person and O.\nSentence: By the time he released his debut album 2Pacalypse Now in 1991 , he had become a central figure in West Coast hip hop , introducing social issues in the genre at a time when gangsta rap was dominant in the mainstream .", "prompt_labels": "By(O) the(O) time(O) he(O) released(O) his(O) debut(O) album(O) 2Pacalypse(B-album) Now(I-album) in(O) 1991(O) ,(O) he(O) had(O) become(O) a(O) central(O) figure(O) in(O) West(B-music genre) Coast(I-music genre) hip(I-music genre) hop(I-music genre) ,(O) introducing(O) social(O) issues(O) in(O) the(O) genre(O) at(O) a(O) time(O) when(O) gangsta(B-music genre) rap(I-music genre) was(O) dominant(O) in(O) the(O) mainstream(O) .(O)"}}
{"id": "217", "dataset": "crossner_music", "split": "test", "label_list": ["song", "event", "person", "musical instrument", "location", "musical artist", "band", "album", "award", "organization", "country", "music genre"], "instance": {"id": "217", "words": ["Cliff", "Richard", "covered", "Lay", "All", "Your", "Love", "on", "Me", ",", "while", "Dionne", "Warwick", ",", "Peter", "Cetera", ",", "and", "Celebrity", "Skin", "recorded", "their", "versions", "of", "SOS", "."], "labels": ["B-musical artist", "I-musical artist", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "I-song", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, event, person, musical instrument, location, musical artist, band, album, award, organization, country, music genre and O.\nSentence: Cliff Richard covered Lay All Your Love on Me , while Dionne Warwick , Peter Cetera , and Celebrity Skin recorded their versions of SOS .", "prompt_labels": "Cliff(B-musical artist) Richard(I-musical artist) covered(O) Lay(B-song) All(I-song) Your(I-song) Love(I-song) on(I-song) Me(I-song) ,(O) while(O) Dionne(B-musical artist) Warwick(I-musical artist) ,(O) Peter(B-musical artist) Cetera(I-musical artist) ,(O) and(O) Celebrity(B-album) Skin(I-album) recorded(O) their(O) versions(O) of(O) SOS(B-song) .(O)"}}
{"id": "109", "dataset": "crossner_music", "split": "test", "label_list": ["album", "musical instrument", "musical artist", "event", "award", "organization", "music genre", "person", "song", "band", "location", "country"], "instance": {"id": "109", "words": ["Paul", "Gilbert", "composes", "music", "in", "a", "wide", "variety", "of", "styles", ",", "including", "Pop", "music", ",", "Rock", "and", "roll", ",", "metal", ",", "blues", ",", "and", "funk", "."], "labels": ["B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "O", "O", "B-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, musical instrument, musical artist, event, award, organization, music genre, person, song, band, location, country and O.\nSentence: Paul Gilbert composes music in a wide variety of styles , including Pop music , Rock and roll , metal , blues , and funk .", "prompt_labels": "Paul(B-musical artist) Gilbert(I-musical artist) composes(O) music(O) in(O) a(O) wide(O) variety(O) of(O) styles(O) ,(O) including(O) Pop(B-music genre) music(I-music genre) ,(O) Rock(B-music genre) and(I-music genre) roll(I-music genre) ,(O) metal(B-music genre) ,(O) blues(B-music genre) ,(O) and(O) funk(B-music genre) .(O)"}}
{"id": "118", "dataset": "crossner_music", "split": "test", "label_list": ["award", "country", "musical artist", "album", "musical instrument", "song", "organization", "event", "band", "music genre", "person", "location"], "instance": {"id": "118", "words": ["Johnston", "was", "involved", "in", "the", "1999", "All-Africa", "Games", ",", "and", "in", "South", "Africa", "'s", "bid", "for", "the", "2004", "Summer", "Olympics", "."], "labels": ["B-person", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, country, musical artist, album, musical instrument, song, organization, event, band, music genre, person, location and O.\nSentence: Johnston was involved in the 1999 All-Africa Games , and in South Africa 's bid for the 2004 Summer Olympics .", "prompt_labels": "Johnston(B-person) was(O) involved(O) in(O) the(O) 1999(B-event) All-Africa(I-event) Games(I-event) ,(O) and(O) in(O) South(B-country) Africa(I-country) 's(O) bid(O) for(O) the(O) 2004(B-event) Summer(I-event) Olympics(I-event) .(O)"}}
{"id": "415", "dataset": "crossner_music", "split": "test", "label_list": ["person", "organization", "country", "musical instrument", "song", "musical artist", "album", "award", "music genre", "event", "band", "location"], "instance": {"id": "415", "words": ["In", "addition", "to", "Parliament", "Funkadelic", ",", "artists", "like", "Sly", "and", "the", "Family", "Stone", ",", "Rufus", "&", "Chaka", "Khan", ",", "Bootsy", "'s", "Rubber", "Band", ",", "the", "Isley", "Brothers", ",", "Ohio", "Players", ",", "Con", "Funk", "Shun", ",", "Kool", "and", "the", "Gang", ",", "the", "Bar-Kays", ",", "Commodores", ",", "Roy", "Ayers", ",", "and", "Stevie", "Wonder", ",", "among", "others", ",", "were", "successful", "in", "getting", "radio", "play", "."], "labels": ["O", "O", "O", "B-band", "I-band", "O", "O", "O", "B-band", "I-band", "I-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "I-band", "O", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "O", "B-musical artist", "I-musical artist", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, country, musical instrument, song, musical artist, album, award, music genre, event, band, location and O.\nSentence: In addition to Parliament Funkadelic , artists like Sly and the Family Stone , Rufus & Chaka Khan , Bootsy 's Rubber Band , the Isley Brothers , Ohio Players , Con Funk Shun , Kool and the Gang , the Bar-Kays , Commodores , Roy Ayers , and Stevie Wonder , among others , were successful in getting radio play .", "prompt_labels": "In(O) addition(O) to(O) Parliament(B-band) Funkadelic(I-band) ,(O) artists(O) like(O) Sly(B-band) and(I-band) the(I-band) Family(I-band) Stone(I-band) ,(O) Rufus(B-band) &(I-band) Chaka(I-band) Khan(I-band) ,(O) Bootsy(B-band) 's(I-band) Rubber(I-band) Band(I-band) ,(O) the(O) Isley(B-band) Brothers(I-band) ,(O) Ohio(B-band) Players(I-band) ,(O) Con(B-band) Funk(I-band) Shun(I-band) ,(O) Kool(B-band) and(I-band) the(I-band) Gang(I-band) ,(O) the(B-band) Bar-Kays(I-band) ,(O) Commodores(B-band) ,(O) Roy(B-musical artist) Ayers(I-musical artist) ,(O) and(O) Stevie(B-musical artist) Wonder(I-musical artist) ,(O) among(O) others(O) ,(O) were(O) successful(O) in(O) getting(O) radio(O) play(O) .(O)"}}
{"id": "314", "dataset": "crossner_music", "split": "test", "label_list": ["location", "person", "album", "musical instrument", "event", "award", "organization", "musical artist", "country", "song", "band", "music genre"], "instance": {"id": "314", "words": ["The", "group", "'s", "evolution", "can", "be", "traced", "through", "the", "recordings", "The", "John", "Coltrane", "Quartet", "Plays", ",", "Living", "Space", "and", "Transition", "(", "both", "June", "1965", ")", ",", "New", "Thing", "at", "Newport", "(", "July", "1965", ")", ",", "Sun", "Ship", "(", "August", "1965", ")", ",", "and", "First", "Meditations", "(", "September", "1965", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "O", "B-album", "I-album", "O", "B-album", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, album, musical instrument, event, award, organization, musical artist, country, song, band, music genre and O.\nSentence: The group 's evolution can be traced through the recordings The John Coltrane Quartet Plays , Living Space and Transition ( both June 1965 ) , New Thing at Newport ( July 1965 ) , Sun Ship ( August 1965 ) , and First Meditations ( September 1965 ) .", "prompt_labels": "The(O) group(O) 's(O) evolution(O) can(O) be(O) traced(O) through(O) the(O) recordings(O) The(B-album) John(I-album) Coltrane(I-album) Quartet(I-album) Plays(I-album) ,(O) Living(B-album) Space(I-album) and(O) Transition(B-album) ((O) both(O) June(O) 1965(O) )(O) ,(O) New(B-album) Thing(I-album) at(I-album) Newport(I-album) ((O) July(O) 1965(O) )(O) ,(O) Sun(B-album) Ship(I-album) ((O) August(O) 1965(O) )(O) ,(O) and(O) First(B-album) Meditations(I-album) ((O) September(O) 1965(O) )(O) .(O)"}}
{"id": "308", "dataset": "crossner_music", "split": "test", "label_list": ["event", "album", "musical artist", "band", "musical instrument", "country", "organization", "award", "person", "location", "music genre", "song"], "instance": {"id": "308", "words": ["During", "their", "three", "years", "as", "a", "mainstream", "act", ",", "Nirvana", "was", "awarded", "an", "American", "Music", "Awards", ",", "Brit", "Awards", ",", "Grammy", "Award", ",", "seven", "MTV", "Video", "Music", "Award", "s", "and", "two", "NME", "Awards", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "O", "O", "O", "B-award", "I-award", "I-award", "O", "B-award", "I-award", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, album, musical artist, band, musical instrument, country, organization, award, person, location, music genre, song and O.\nSentence: During their three years as a mainstream act , Nirvana was awarded an American Music Awards , Brit Awards , Grammy Award , seven MTV Video Music Award s and two NME Awards .", "prompt_labels": "During(O) their(O) three(O) years(O) as(O) a(O) mainstream(O) act(O) ,(O) Nirvana(B-band) was(O) awarded(O) an(O) American(B-award) Music(I-award) Awards(I-award) ,(O) Brit(B-award) Awards(I-award) ,(O) Grammy(B-award) Award(I-award) ,(O) seven(O) MTV(B-award) Video(I-award) Music(I-award) Award(I-award) s(O) and(O) two(O) NME(B-award) Awards(I-award) .(O)"}}
{"id": "252", "dataset": "crossner_music", "split": "test", "label_list": ["person", "band", "musical artist", "song", "award", "musical instrument", "country", "music genre", "organization", "album", "event", "location"], "instance": {"id": "252", "words": ["Reed", "'s", "1984", "album", "New", "Sensations", "marked", "the", "first", "time", "that", "Reed", "had", "charted", "within", "the", "US", "Top", "100", "since", "1978", "'s", "Street", "Hassle", ",", "and", "the", "first", "time", "that", "Reed", "had", "charted", "in", "the", "UK", "altogether", "since", "1976", "'s", "Coney", "Island", "Baby", "."], "labels": ["B-musical artist", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, band, musical artist, song, award, musical instrument, country, music genre, organization, album, event, location and O.\nSentence: Reed 's 1984 album New Sensations marked the first time that Reed had charted within the US Top 100 since 1978 's Street Hassle , and the first time that Reed had charted in the UK altogether since 1976 's Coney Island Baby .", "prompt_labels": "Reed(B-musical artist) 's(O) 1984(O) album(O) New(B-album) Sensations(I-album) marked(O) the(O) first(O) time(O) that(O) Reed(B-musical artist) had(O) charted(O) within(O) the(O) US(B-country) Top(O) 100(O) since(O) 1978(O) 's(O) Street(B-album) Hassle(I-album) ,(O) and(O) the(O) first(O) time(O) that(O) Reed(B-musical artist) had(O) charted(O) in(O) the(O) UK(B-country) altogether(O) since(O) 1976(O) 's(O) Coney(B-album) Island(I-album) Baby(I-album) .(O)"}}
{"id": "354", "dataset": "crossner_music", "split": "test", "label_list": ["song", "musical artist", "award", "musical instrument", "person", "country", "album", "location", "music genre", "event", "organization", "band"], "instance": {"id": "354", "words": ["Other", "influential", "folk", "artists", "include", "Surinder", "Shinda", "-", "famous", "for", "his", "Putt", "Jattan", "De", "-", "Harbhajan", "Mann", ",", "Manmohan", "Waris", ",", "Meshi", "Eshara", ",", "Sarbjit", "Cheema", ",", "Hans", "Raj", "Hans", ",", "Sardool", "Sikander", ",", "Anakhi", "."], "labels": ["O", "O", "B-music genre", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, musical artist, award, musical instrument, person, country, album, location, music genre, event, organization, band and O.\nSentence: Other influential folk artists include Surinder Shinda - famous for his Putt Jattan De - Harbhajan Mann , Manmohan Waris , Meshi Eshara , Sarbjit Cheema , Hans Raj Hans , Sardool Sikander , Anakhi .", "prompt_labels": "Other(O) influential(O) folk(B-music genre) artists(O) include(O) Surinder(B-musical artist) Shinda(I-musical artist) -(O) famous(O) for(O) his(O) Putt(O) Jattan(O) De(O) -(O) Harbhajan(B-musical artist) Mann(I-musical artist) ,(O) Manmohan(B-musical artist) Waris(I-musical artist) ,(O) Meshi(B-musical artist) Eshara(I-musical artist) ,(O) Sarbjit(B-musical artist) Cheema(I-musical artist) ,(O) Hans(B-musical artist) Raj(I-musical artist) Hans(I-musical artist) ,(O) Sardool(B-musical artist) Sikander(I-musical artist) ,(O) Anakhi(B-band) .(O)"}}
{"id": "165", "dataset": "crossner_music", "split": "test", "label_list": ["music genre", "award", "musical artist", "country", "person", "organization", "band", "musical instrument", "album", "location", "song", "event"], "instance": {"id": "165", "words": ["This", "was", "the", "instrument", "of", "the", "early", "jazz", "great", "Johnny", "St.", "Cyr", ",", "jazzmen", "Django", "Reinhardt", ",", "Danny", "Barker", ",", "Papa", "Charlie", "Jackson", "and", "Clancy", "Hayes", ",", "as", "well", "as", "the", "blues", "and", "gospel", "singer", "The", "Reverend", "Gary", "Davis", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "B-music genre", "O", "B-music genre", "O", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, award, musical artist, country, person, organization, band, musical instrument, album, location, song, event and O.\nSentence: This was the instrument of the early jazz great Johnny St. Cyr , jazzmen Django Reinhardt , Danny Barker , Papa Charlie Jackson and Clancy Hayes , as well as the blues and gospel singer The Reverend Gary Davis .", "prompt_labels": "This(O) was(O) the(O) instrument(O) of(O) the(O) early(O) jazz(B-music genre) great(O) Johnny(B-musical artist) St.(I-musical artist) Cyr(I-musical artist) ,(O) jazzmen(O) Django(B-musical artist) Reinhardt(I-musical artist) ,(O) Danny(B-musical artist) Barker(I-musical artist) ,(O) Papa(B-musical artist) Charlie(I-musical artist) Jackson(I-musical artist) and(O) Clancy(B-musical artist) Hayes(I-musical artist) ,(O) as(O) well(O) as(O) the(O) blues(B-music genre) and(O) gospel(B-music genre) singer(O) The(O) Reverend(B-musical artist) Gary(I-musical artist) Davis(I-musical artist) .(O)"}}
{"id": "173", "dataset": "crossner_music", "split": "test", "label_list": ["location", "organization", "musical artist", "band", "person", "album", "event", "music genre", "award", "musical instrument", "song", "country"], "instance": {"id": "173", "words": ["In", "June", "2010", ",", "ASCAP", "sent", "letters", "to", "its", "members", "soliciting", "donations", "to", "fight", "entities", "that", "support", "weaker", "copyright", "restrictions", ",", "such", "as", "Public", "Knowledge", ",", "the", "Electronic", "Frontier", "Foundation", ",", "and", "Creative", "Commons", ","], "labels": ["O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, musical artist, band, person, album, event, music genre, award, musical instrument, song, country and O.\nSentence: In June 2010 , ASCAP sent letters to its members soliciting donations to fight entities that support weaker copyright restrictions , such as Public Knowledge , the Electronic Frontier Foundation , and Creative Commons ,", "prompt_labels": "In(O) June(O) 2010(O) ,(O) ASCAP(B-organization) sent(O) letters(O) to(O) its(O) members(O) soliciting(O) donations(O) to(O) fight(O) entities(O) that(O) support(O) weaker(O) copyright(O) restrictions(O) ,(O) such(O) as(O) Public(B-organization) Knowledge(I-organization) ,(O) the(O) Electronic(B-organization) Frontier(I-organization) Foundation(I-organization) ,(O) and(O) Creative(B-organization) Commons(I-organization) ,(O)"}}
{"id": "99", "dataset": "crossner_music", "split": "test", "label_list": ["location", "band", "musical instrument", "person", "album", "music genre", "musical artist", "organization", "country", "song", "award", "event"], "instance": {"id": "99", "words": ["Examples", "include", "Portuguese", "Fado", ",", "Spanish-speaking", "Mexican", "Regional", ",", "Reggaeton", "and", "Tejano", "music", ",", "French", "Cajun", "music", "(", "especially", "in", "French", "Louisiana", ")", ",", "Russian", "Shanson", ",", "and", "(", "since", "the", "late", "2000s", ")", "Korean", "K-pop", "."], "labels": ["O", "O", "O", "B-music genre", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, band, musical instrument, person, album, music genre, musical artist, organization, country, song, award, event and O.\nSentence: Examples include Portuguese Fado , Spanish-speaking Mexican Regional , Reggaeton and Tejano music , French Cajun music ( especially in French Louisiana ) , Russian Shanson , and ( since the late 2000s ) Korean K-pop .", "prompt_labels": "Examples(O) include(O) Portuguese(O) Fado(B-music genre) ,(O) Spanish-speaking(O) Mexican(B-music genre) Regional(I-music genre) ,(O) Reggaeton(B-music genre) and(O) Tejano(B-music genre) music(I-music genre) ,(O) French(O) Cajun(B-music genre) music(I-music genre) ((O) especially(O) in(O) French(B-location) Louisiana(I-location) )(O) ,(O) Russian(O) Shanson(B-music genre) ,(O) and(O) ((O) since(O) the(O) late(O) 2000s(O) )(O) Korean(O) K-pop(B-music genre) .(O)"}}
{"id": "305", "dataset": "crossner_music", "split": "test", "label_list": ["location", "album", "music genre", "organization", "country", "event", "musical instrument", "band", "award", "song", "person", "musical artist"], "instance": {"id": "305", "words": ["The", "viola", "is", "also", "an", "important", "accompaniment", "instrument", "in", "Slovakia", "n", ",", "Hungary", "and", "Romania", "n", "folk", "string", "band", "music", ",", "especially", "in", "Transylvania", "."], "labels": ["O", "B-musical instrument", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "B-country", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, album, music genre, organization, country, event, musical instrument, band, award, song, person, musical artist and O.\nSentence: The viola is also an important accompaniment instrument in Slovakia n , Hungary and Romania n folk string band music , especially in Transylvania .", "prompt_labels": "The(O) viola(B-musical instrument) is(O) also(O) an(O) important(O) accompaniment(O) instrument(O) in(O) Slovakia(B-country) n(O) ,(O) Hungary(B-country) and(O) Romania(B-country) n(O) folk(O) string(O) band(O) music(O) ,(O) especially(O) in(O) Transylvania(B-location) .(O)"}}
{"id": "102", "dataset": "crossner_music", "split": "test", "label_list": ["location", "award", "event", "organization", "musical artist", "country", "musical instrument", "band", "music genre", "person", "song", "album"], "instance": {"id": "102", "words": ["The", "banned", "monastic", "orders", ":", "Jesuits", ",", "Camaldolese", ",", "Order", "of", "Friars", "Minor", "Capuchin", ",", "Carmelites", ",", "Carthusians", ",", "Poor", "Clares", ",", "Order", "of", "Saint", "Benedict", ",", "Cistercians", ",", "Dominican", "Order", "(", "Order", "of", "Preachers", ")", ",", "Franciscans", ",", "Order", "of", "Saint", "Paul", "the", "First", "Hermit", "and", "Premonstratensians", ",", "and", "their", "wealth", "was", "taken", "over", "by", "the", "Religious", "Fund", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "O", "B-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "B-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, event, organization, musical artist, country, musical instrument, band, music genre, person, song, album and O.\nSentence: The banned monastic orders : Jesuits , Camaldolese , Order of Friars Minor Capuchin , Carmelites , Carthusians , Poor Clares , Order of Saint Benedict , Cistercians , Dominican Order ( Order of Preachers ) , Franciscans , Order of Saint Paul the First Hermit and Premonstratensians , and their wealth was taken over by the Religious Fund .", "prompt_labels": "The(O) banned(O) monastic(O) orders(O) :(O) Jesuits(B-organization) ,(O) Camaldolese(B-organization) ,(O) Order(B-organization) of(I-organization) Friars(I-organization) Minor(I-organization) Capuchin(I-organization) ,(O) Carmelites(B-organization) ,(O) Carthusians(B-organization) ,(O) Poor(B-organization) Clares(I-organization) ,(O) Order(B-organization) of(I-organization) Saint(I-organization) Benedict(I-organization) ,(O) Cistercians(B-organization) ,(O) Dominican(B-organization) Order(I-organization) ((O) Order(B-organization) of(I-organization) Preachers(I-organization) )(O) ,(O) Franciscans(B-organization) ,(O) Order(B-organization) of(I-organization) Saint(I-organization) Paul(I-organization) the(I-organization) First(I-organization) Hermit(I-organization) and(O) Premonstratensians(B-organization) ,(O) and(O) their(O) wealth(O) was(O) taken(O) over(O) by(O) the(O) Religious(B-organization) Fund(I-organization) .(O)"}}
{"id": "403", "dataset": "crossner_music", "split": "test", "label_list": ["country", "award", "band", "person", "event", "album", "organization", "song", "musical artist", "location", "musical instrument", "music genre"], "instance": {"id": "403", "words": ["Major", "artists", "of", "the", "Texas", "style", "are", "Johnny", "Winter", ",", "Stevie", "Ray", "Vaughan", ",", "the", "The", "Fabulous", "Thunderbirds", "(", "led", "by", "harmonica", "player", "and", "singer-songwriter", "Kim", "Wilson", ")", ",", "and", "ZZ", "Top", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "O", "B-band", "I-band", "I-band", "O", "O", "O", "B-musical instrument", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, band, person, event, album, organization, song, musical artist, location, musical instrument, music genre and O.\nSentence: Major artists of the Texas style are Johnny Winter , Stevie Ray Vaughan , the The Fabulous Thunderbirds ( led by harmonica player and singer-songwriter Kim Wilson ) , and ZZ Top .", "prompt_labels": "Major(O) artists(O) of(O) the(O) Texas(O) style(O) are(O) Johnny(B-musical artist) Winter(I-musical artist) ,(O) Stevie(B-musical artist) Ray(I-musical artist) Vaughan(I-musical artist) ,(O) the(O) The(B-band) Fabulous(I-band) Thunderbirds(I-band) ((O) led(O) by(O) harmonica(B-musical instrument) player(O) and(O) singer-songwriter(O) Kim(B-musical artist) Wilson(I-musical artist) )(O) ,(O) and(O) ZZ(B-band) Top(I-band) .(O)"}}
{"id": "63", "dataset": "crossner_music", "split": "test", "label_list": ["music genre", "event", "song", "organization", "location", "person", "album", "musical artist", "band", "award", "country", "musical instrument"], "instance": {"id": "63", "words": ["Cliff", "Bruner", ",", "Moon", "Mullican", ",", "Milton", "Brown", "and", "Adolph", "Hofner", "were", "other", "early", "Western", "swing", "pioneers", "."], "labels": ["B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "B-music genre", "I-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, event, song, organization, location, person, album, musical artist, band, award, country, musical instrument and O.\nSentence: Cliff Bruner , Moon Mullican , Milton Brown and Adolph Hofner were other early Western swing pioneers .", "prompt_labels": "Cliff(B-musical artist) Bruner(I-musical artist) ,(O) Moon(B-musical artist) Mullican(I-musical artist) ,(O) Milton(B-musical artist) Brown(I-musical artist) and(O) Adolph(B-musical artist) Hofner(I-musical artist) were(O) other(O) early(O) Western(B-music genre) swing(I-music genre) pioneers(O) .(O)"}}
{"id": "101", "dataset": "crossner_music", "split": "test", "label_list": ["person", "music genre", "musical instrument", "musical artist", "award", "location", "band", "event", "organization", "country", "song", "album"], "instance": {"id": "101", "words": ["The", "Godfather", "won", "three", "Academy", "Awards", ":", "Academy", "Award", "for", "Best", "Picture", ",", "Academy", "Award", "for", "Best", "Actor", ",", "and", "Academy", "Award", "for", "Best", "Adapted", "Screenplay", "(", "shared", "with", "Mario", "Puzo", ")", "."], "labels": ["B-band", "I-band", "O", "O", "B-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-person", "I-person", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, music genre, musical instrument, musical artist, award, location, band, event, organization, country, song, album and O.\nSentence: The Godfather won three Academy Awards : Academy Award for Best Picture , Academy Award for Best Actor , and Academy Award for Best Adapted Screenplay ( shared with Mario Puzo ) .", "prompt_labels": "The(B-band) Godfather(I-band) won(O) three(O) Academy(B-award) Awards(I-award) :(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) ,(O) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Adapted(I-award) Screenplay(I-award) ((O) shared(O) with(O) Mario(B-person) Puzo(I-person) )(O) .(O)"}}
{"id": "91", "dataset": "crossner_music", "split": "test", "label_list": ["award", "song", "musical instrument", "album", "country", "band", "event", "person", "musical artist", "music genre", "location", "organization"], "instance": {"id": "91", "words": ["Derived", "from", "the", "traditional", "Western", ",", "including", "Red", "Dirt", ",", "New", "Mexico", "music", ",", "Texas", "country", "music", ",", "Tejano", "music", ",", "and", "honky-tonk", "musical", "styles", "of", "the", "late", "1950s", "and", "1960s", "."], "labels": ["O", "O", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, song, musical instrument, album, country, band, event, person, musical artist, music genre, location, organization and O.\nSentence: Derived from the traditional Western , including Red Dirt , New Mexico music , Texas country music , Tejano music , and honky-tonk musical styles of the late 1950s and 1960s .", "prompt_labels": "Derived(O) from(O) the(O) traditional(B-music genre) Western(I-music genre) ,(O) including(O) Red(B-music genre) Dirt(I-music genre) ,(O) New(B-music genre) Mexico(I-music genre) music(I-music genre) ,(O) Texas(B-music genre) country(I-music genre) music(I-music genre) ,(O) Tejano(B-music genre) music(I-music genre) ,(O) and(O) honky-tonk(B-music genre) musical(O) styles(O) of(O) the(O) late(O) 1950s(O) and(O) 1960s(O) .(O)"}}
{"id": "107", "dataset": "crossner_music", "split": "test", "label_list": ["organization", "award", "musical instrument", "band", "music genre", "event", "person", "album", "location", "country", "musical artist", "song"], "instance": {"id": "107", "words": ["He", "also", "performed", "in", "The", "Rocky", "Horror", "Show", ",", "as", "the", "narrator", ",", "at", "the", "Churchill", "Theatre", "in", "Bromley", "and", "the", "New", "Wimbledon", "Theatre", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "O", "O", "B-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, musical instrument, band, music genre, event, person, album, location, country, musical artist, song and O.\nSentence: He also performed in The Rocky Horror Show , as the narrator , at the Churchill Theatre in Bromley and the New Wimbledon Theatre .", "prompt_labels": "He(O) also(O) performed(O) in(O) The(O) Rocky(O) Horror(O) Show(O) ,(O) as(O) the(O) narrator(O) ,(O) at(O) the(O) Churchill(B-location) Theatre(I-location) in(O) Bromley(B-location) and(O) the(O) New(B-location) Wimbledon(I-location) Theatre(I-location) .(O)"}}
{"id": "249", "dataset": "crossner_music", "split": "test", "label_list": ["location", "band", "music genre", "musical artist", "song", "event", "organization", "person", "country", "album", "musical instrument", "award"], "instance": {"id": "249", "words": ["Singers", "such", "as", "Blind", "Willie", "McTell", "and", "Blind", "Boy", "Fuller", "performed", "in", "the", "southeastern", "delicate", "and", "lyrical", "Piedmont", "blues", "tradition", ",", "which", "used", "an", "elaborate", "ragtime-based", "fingerpicking", "guitar", "technique", "."], "labels": ["O", "O", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, band, music genre, musical artist, song, event, organization, person, country, album, musical instrument, award and O.\nSentence: Singers such as Blind Willie McTell and Blind Boy Fuller performed in the southeastern delicate and lyrical Piedmont blues tradition , which used an elaborate ragtime-based fingerpicking guitar technique .", "prompt_labels": "Singers(O) such(O) as(O) Blind(B-musical artist) Willie(I-musical artist) McTell(I-musical artist) and(O) Blind(B-musical artist) Boy(I-musical artist) Fuller(I-musical artist) performed(O) in(O) the(O) southeastern(O) delicate(O) and(O) lyrical(O) Piedmont(B-music genre) blues(I-music genre) tradition(O) ,(O) which(O) used(O) an(O) elaborate(O) ragtime-based(O) fingerpicking(O) guitar(O) technique(O) .(O)"}}
{"id": "228", "dataset": "crossner_music", "split": "test", "label_list": ["album", "song", "musical instrument", "country", "musical artist", "person", "band", "event", "organization", "award", "music genre", "location"], "instance": {"id": "228", "words": ["In", "1995", "they", "supported", "Greenpeace", ",", "International", "Physicians", "for", "the", "Prevention", "of", "Nuclear", "War", ",", "Aktion", "Atomteststop", "(", "an", "initiative", "for", "a", "nuclear", "test", "ban", ")", ",", "the", "Bund", "für", "Umwelt", "und", "Naturschutz", "Deutschland", "(", "German", "Friends", "of", "the", "Earth", "chapter", ")", "and", "they", "were", "featured", "on", "the", "track", "Tout", "Pour", "Sauver", "L", "'Amour", "(", "Everything", "to", "save", "love", ")", "on", "the", "Stop", "Chirac", "compilation", "album", "."], "labels": ["O", "O", "O", "O", "B-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "O", "B-album", "I-album", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, song, musical instrument, country, musical artist, person, band, event, organization, award, music genre, location and O.\nSentence: In 1995 they supported Greenpeace , International Physicians for the Prevention of Nuclear War , Aktion Atomteststop ( an initiative for a nuclear test ban ) , the Bund für Umwelt und Naturschutz Deutschland ( German Friends of the Earth chapter ) and they were featured on the track Tout Pour Sauver L 'Amour ( Everything to save love ) on the Stop Chirac compilation album .", "prompt_labels": "In(O) 1995(O) they(O) supported(O) Greenpeace(B-organization) ,(O) International(B-organization) Physicians(I-organization) for(I-organization) the(I-organization) Prevention(I-organization) of(I-organization) Nuclear(I-organization) War(I-organization) ,(O) Aktion(B-organization) Atomteststop(I-organization) ((O) an(O) initiative(O) for(O) a(O) nuclear(O) test(O) ban(O) )(O) ,(O) the(O) Bund(B-organization) für(I-organization) Umwelt(I-organization) und(I-organization) Naturschutz(I-organization) Deutschland(I-organization) ((O) German(O) Friends(B-organization) of(I-organization) the(I-organization) Earth(I-organization) chapter(O) )(O) and(O) they(O) were(O) featured(O) on(O) the(O) track(O) Tout(B-song) Pour(I-song) Sauver(I-song) L(I-song) 'Amour(I-song) ((O) Everything(B-song) to(I-song) save(I-song) love(I-song) )(O) on(O) the(O) Stop(B-album) Chirac(I-album) compilation(O) album(O) .(O)"}}
{"id": "222", "dataset": "crossner_music", "split": "test", "label_list": ["musical instrument", "event", "musical artist", "music genre", "band", "song", "person", "location", "album", "award", "organization", "country"], "instance": {"id": "222", "words": ["Williams", "has", "received", "a", "record", "eighteen", "Brit", "Awards", "-", "winning", "Brit", "Award", "for", "British", "Male", "Solo", "Artist", "four", "times", ",", "two", "awards", "for", "Outstanding", "Contribution", "to", "Music", "and", "the", "2017", "Brits", "Icon", "for", "his", "lasting", "impact", "on", "British", "culture", ",", "eight", "German", "Echo", "Music", "Prize", ",", "and", "three", "MTV", "European", "Music", "Awards", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, event, musical artist, music genre, band, song, person, location, album, award, organization, country and O.\nSentence: Williams has received a record eighteen Brit Awards - winning Brit Award for British Male Solo Artist four times , two awards for Outstanding Contribution to Music and the 2017 Brits Icon for his lasting impact on British culture , eight German Echo Music Prize , and three MTV European Music Awards .", "prompt_labels": "Williams(B-musical artist) has(O) received(O) a(O) record(O) eighteen(O) Brit(B-award) Awards(I-award) -(O) winning(O) Brit(B-award) Award(I-award) for(I-award) British(I-award) Male(I-award) Solo(I-award) Artist(I-award) four(O) times(O) ,(O) two(O) awards(O) for(O) Outstanding(B-award) Contribution(I-award) to(I-award) Music(I-award) and(O) the(O) 2017(B-award) Brits(I-award) Icon(I-award) for(O) his(O) lasting(O) impact(O) on(O) British(O) culture(O) ,(O) eight(O) German(O) Echo(B-award) Music(I-award) Prize(I-award) ,(O) and(O) three(O) MTV(B-award) European(I-award) Music(I-award) Awards(I-award) .(O)"}}
{"id": "158", "dataset": "crossner_music", "split": "test", "label_list": ["band", "person", "musical instrument", "event", "location", "organization", "music genre", "song", "musical artist", "album", "country", "award"], "instance": {"id": "158", "words": ["The", "four-piece", "Indie", "rock", "band", "played", "gigs", "at", "pubs", "and", "festivals", "from", "2005", "to", "2007", "such", "as", "Knitting", "Factory", ",", "Bamboozle", "Left", ",", "The", "Roxy", ",", "Spaceland", ",", "and", "The", "Viper", "Room", "."], "labels": ["O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "B-location", "O", "O", "B-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, person, musical instrument, event, location, organization, music genre, song, musical artist, album, country, award and O.\nSentence: The four-piece Indie rock band played gigs at pubs and festivals from 2005 to 2007 such as Knitting Factory , Bamboozle Left , The Roxy , Spaceland , and The Viper Room .", "prompt_labels": "The(O) four-piece(O) Indie(B-music genre) rock(I-music genre) band(O) played(O) gigs(O) at(O) pubs(O) and(O) festivals(O) from(O) 2005(O) to(O) 2007(O) such(O) as(O) Knitting(B-location) Factory(I-location) ,(O) Bamboozle(B-location) Left(I-location) ,(O) The(B-location) Roxy(I-location) ,(O) Spaceland(B-location) ,(O) and(O) The(B-location) Viper(I-location) Room(I-location) .(O)"}}
{"id": "322", "dataset": "crossner_music", "split": "test", "label_list": ["song", "organization", "award", "location", "person", "band", "event", "musical artist", "country", "musical instrument", "music genre", "album"], "instance": {"id": "322", "words": ["She", "has", "won", "several", "awards", "throughout", "her", "career", ",", "including", "one", "Grammy", "Award", "from", "seven", "nominations", ",", "one", "Latin", "Grammy", "Award", ",", "ten", "Juno", "Awards", ",", "one", "BRIT", "Award", ",", "one", "Billboard", "Music", "Award", ",", "one", "MTV", "Europe", "Music", "Award", ",", "one", "World", "Music", "Award", ",", "and", "three", "Much", "Music", "Video", "Awards", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, organization, award, location, person, band, event, musical artist, country, musical instrument, music genre, album and O.\nSentence: She has won several awards throughout her career , including one Grammy Award from seven nominations , one Latin Grammy Award , ten Juno Awards , one BRIT Award , one Billboard Music Award , one MTV Europe Music Award , one World Music Award , and three Much Music Video Awards .", "prompt_labels": "She(O) has(O) won(O) several(O) awards(O) throughout(O) her(O) career(O) ,(O) including(O) one(O) Grammy(B-award) Award(I-award) from(O) seven(O) nominations(O) ,(O) one(O) Latin(B-award) Grammy(I-award) Award(I-award) ,(O) ten(O) Juno(B-award) Awards(I-award) ,(O) one(O) BRIT(B-award) Award(I-award) ,(O) one(O) Billboard(B-award) Music(I-award) Award(I-award) ,(O) one(O) MTV(B-award) Europe(I-award) Music(I-award) Award(I-award) ,(O) one(O) World(B-award) Music(I-award) Award(I-award) ,(O) and(O) three(O) Much(B-award) Music(I-award) Video(I-award) Awards(I-award) .(O)"}}
{"id": "426", "dataset": "crossner_music", "split": "test", "label_list": ["band", "album", "person", "award", "organization", "country", "event", "song", "music genre", "location", "musical artist", "musical instrument"], "instance": {"id": "426", "words": ["The", "1970", "releases", "by", "Black", "Sabbath", "(", "Black", "Sabbath", "and", "Paranoid", ")", "and", "Deep", "Purple", "(", "In", "Rock", ")", "were", "crucial", "in", "this", "regard", "."], "labels": ["O", "O", "O", "O", "B-band", "I-band", "O", "B-album", "I-album", "O", "B-album", "O", "O", "B-band", "I-band", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, album, person, award, organization, country, event, song, music genre, location, musical artist, musical instrument and O.\nSentence: The 1970 releases by Black Sabbath ( Black Sabbath and Paranoid ) and Deep Purple ( In Rock ) were crucial in this regard .", "prompt_labels": "The(O) 1970(O) releases(O) by(O) Black(B-band) Sabbath(I-band) ((O) Black(B-album) Sabbath(I-album) and(O) Paranoid(B-album) )(O) and(O) Deep(B-band) Purple(I-band) ((O) In(B-album) Rock(I-album) )(O) were(O) crucial(O) in(O) this(O) regard(O) .(O)"}}
{"id": "267", "dataset": "crossner_music", "split": "test", "label_list": ["musical instrument", "person", "song", "music genre", "organization", "country", "album", "musical artist", "location", "award", "band", "event"], "instance": {"id": "267", "words": ["He", "has", "won", "five", "Primetime", "Emmy", "Award", "s", ",", "four", "Golden", "Globe", "Awards", ",", "a", "Grammy", "Award", ",", "and", "two", "Screen", "Actors", "Guild", "Awards", "."], "labels": ["O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, person, song, music genre, organization, country, album, musical artist, location, award, band, event and O.\nSentence: He has won five Primetime Emmy Award s , four Golden Globe Awards , a Grammy Award , and two Screen Actors Guild Awards .", "prompt_labels": "He(O) has(O) won(O) five(O) Primetime(B-award) Emmy(I-award) Award(I-award) s(O) ,(O) four(O) Golden(B-award) Globe(I-award) Awards(I-award) ,(O) a(O) Grammy(B-award) Award(I-award) ,(O) and(O) two(O) Screen(B-award) Actors(I-award) Guild(I-award) Awards(I-award) .(O)"}}
{"id": "331", "dataset": "crossner_music", "split": "test", "label_list": ["country", "organization", "location", "person", "musical artist", "musical instrument", "award", "album", "band", "music genre", "event", "song"], "instance": {"id": "331", "words": ["His", "film", "Annie", "Hall", "(", "1977", ")", ",", "a", "romantic", "comedy", "featuring", "Allen", "and", "his", "frequent", "collaborator", "Diane", "Keaton", ",", "won", "four", "Academy", "Awards", ",", "including", "Academy", "Award", "for", "Best", "Picture", ",", "Academy", "Award", "for", "Best", "Director", ",", "Academy", "Award", "for", "Best", "Original", "Screenplay", ",", "and", "Academy", "Award", "for", "Best", "Actress", "for", "Keaton", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, location, person, musical artist, musical instrument, award, album, band, music genre, event, song and O.\nSentence: His film Annie Hall ( 1977 ) , a romantic comedy featuring Allen and his frequent collaborator Diane Keaton , won four Academy Awards , including Academy Award for Best Picture , Academy Award for Best Director , Academy Award for Best Original Screenplay , and Academy Award for Best Actress for Keaton .", "prompt_labels": "His(O) film(O) Annie(O) Hall(O) ((O) 1977(O) )(O) ,(O) a(O) romantic(O) comedy(O) featuring(O) Allen(B-person) and(O) his(O) frequent(O) collaborator(O) Diane(B-person) Keaton(I-person) ,(O) won(O) four(O) Academy(B-award) Awards(I-award) ,(O) including(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Screenplay(I-award) ,(O) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) for(O) Keaton(B-person) .(O)"}}
{"id": "182", "dataset": "crossner_music", "split": "test", "label_list": ["location", "musical artist", "song", "country", "album", "band", "musical instrument", "music genre", "person", "event", "organization", "award"], "instance": {"id": "182", "words": ["Steel-string", "guitars", "are", "also", "important", "in", "the", "world", "of", "flatpicking", ",", "as", "utilized", "by", "such", "artists", "as", "Clarence", "White", ",", "Tony", "Rice", ",", "Bryan", "Sutton", ",", "Doc", "Watson", "and", "David", "Grier", "."], "labels": ["B-musical instrument", "I-musical instrument", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, musical artist, song, country, album, band, musical instrument, music genre, person, event, organization, award and O.\nSentence: Steel-string guitars are also important in the world of flatpicking , as utilized by such artists as Clarence White , Tony Rice , Bryan Sutton , Doc Watson and David Grier .", "prompt_labels": "Steel-string(B-musical instrument) guitars(I-musical instrument) are(O) also(O) important(O) in(O) the(O) world(O) of(O) flatpicking(O) ,(O) as(O) utilized(O) by(O) such(O) artists(O) as(O) Clarence(B-musical artist) White(I-musical artist) ,(O) Tony(B-musical artist) Rice(I-musical artist) ,(O) Bryan(B-musical artist) Sutton(I-musical artist) ,(O) Doc(B-musical artist) Watson(I-musical artist) and(O) David(B-musical artist) Grier(I-musical artist) .(O)"}}
{"id": "246", "dataset": "crossner_music", "split": "test", "label_list": ["organization", "musical instrument", "country", "award", "event", "location", "person", "band", "album", "musical artist", "song", "music genre"], "instance": {"id": "246", "words": ["Love", "has", "been", "candid", "about", "her", "diverse", "musical", "influences", ",", "the", "earliest", "being", "Patti", "Smith", ",", "The", "Runaways", ",", "and", "The", "Pretenders", ",", "artists", "she", "discovered", "while", "in", "juvenile", "hall", "at", "age", "fifteen", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, musical instrument, country, award, event, location, person, band, album, musical artist, song, music genre and O.\nSentence: Love has been candid about her diverse musical influences , the earliest being Patti Smith , The Runaways , and The Pretenders , artists she discovered while in juvenile hall at age fifteen .", "prompt_labels": "Love(B-musical artist) has(O) been(O) candid(O) about(O) her(O) diverse(O) musical(O) influences(O) ,(O) the(O) earliest(O) being(O) Patti(B-musical artist) Smith(I-musical artist) ,(O) The(B-band) Runaways(I-band) ,(O) and(O) The(B-band) Pretenders(I-band) ,(O) artists(O) she(O) discovered(O) while(O) in(O) juvenile(O) hall(O) at(O) age(O) fifteen(O) .(O)"}}
{"id": "120", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "country", "event", "location", "song", "band", "music genre", "musical instrument", "album", "award", "organization", "person"], "instance": {"id": "120", "words": ["His", "songs", "for", "the", "group", "included", "Taxman", ",", "Within", "You", "Without", "You", ",", "While", "My", "Guitar", "Gently", "Weeps", ",", "Here", "Comes", "the", "Sun", "and", "Something", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, country, event, location, song, band, music genre, musical instrument, album, award, organization, person and O.\nSentence: His songs for the group included Taxman , Within You Without You , While My Guitar Gently Weeps , Here Comes the Sun and Something .", "prompt_labels": "His(O) songs(O) for(O) the(O) group(O) included(O) Taxman(B-song) ,(O) Within(B-song) You(I-song) Without(I-song) You(I-song) ,(O) While(B-song) My(I-song) Guitar(I-song) Gently(I-song) Weeps(I-song) ,(O) Here(B-song) Comes(I-song) the(I-song) Sun(I-song) and(O) Something(B-song) .(O)"}}
{"id": "447", "dataset": "crossner_music", "split": "test", "label_list": ["band", "musical instrument", "album", "person", "music genre", "location", "musical artist", "award", "event", "country", "organization", "song"], "instance": {"id": "447", "words": ["From", "the", "1970s", "through", "the", "1990s", ",", "Terry", "performed", "at", "Carnegie", "Hall", ",", "Town", "Hall", ",", "and", "Lincoln", "Center", ",", "toured", "with", "the", "Newport", "Jazz", "All", "Stars", "and", "Jazz", "at", "the", "Philharmonic", ",", "and", "was", "featured", "with", "Skitch", "Henderson", "'", "s", "New", "York", "Pops", "Orchestra", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "B-location", "I-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical instrument, album, person, music genre, location, musical artist, award, event, country, organization, song and O.\nSentence: From the 1970s through the 1990s , Terry performed at Carnegie Hall , Town Hall , and Lincoln Center , toured with the Newport Jazz All Stars and Jazz at the Philharmonic , and was featured with Skitch Henderson ' s New York Pops Orchestra .", "prompt_labels": "From(O) the(O) 1970s(O) through(O) the(O) 1990s(O) ,(O) Terry(B-musical artist) performed(O) at(O) Carnegie(B-location) Hall(I-location) ,(O) Town(B-location) Hall(I-location) ,(O) and(O) Lincoln(B-location) Center(I-location) ,(O) toured(O) with(O) the(O) Newport(B-event) Jazz(I-event) All(I-event) Stars(I-event) and(O) Jazz(B-event) at(I-event) the(I-event) Philharmonic(I-event) ,(O) and(O) was(O) featured(O) with(O) Skitch(B-musical artist) Henderson(I-musical artist) '(O) s(O) New(B-location) York(I-location) Pops(I-location) Orchestra(I-location) .(O)"}}
{"id": "287", "dataset": "crossner_music", "split": "test", "label_list": ["event", "organization", "location", "country", "music genre", "award", "musical instrument", "person", "song", "album", "musical artist", "band"], "instance": {"id": "287", "words": ["In", "1990", ",", "she", "contributed", "a", "cover", "of", "You", "Do", "Something", "to", "Me", "to", "the", "Cole", "Porter", "tribute", "/", "AIDS", "fundraising", "album", "Red", "Hot", "+", "Blue", "produced", "by", "the", "Red", "Hot", "Organization", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, location, country, music genre, award, musical instrument, person, song, album, musical artist, band and O.\nSentence: In 1990 , she contributed a cover of You Do Something to Me to the Cole Porter tribute / AIDS fundraising album Red Hot + Blue produced by the Red Hot Organization .", "prompt_labels": "In(O) 1990(O) ,(O) she(O) contributed(O) a(O) cover(O) of(O) You(B-album) Do(I-album) Something(I-album) to(I-album) Me(I-album) to(O) the(O) Cole(B-musical artist) Porter(I-musical artist) tribute(O) /(O) AIDS(O) fundraising(O) album(O) Red(B-album) Hot(I-album) +(I-album) Blue(I-album) produced(O) by(O) the(O) Red(B-organization) Hot(I-organization) Organization(I-organization) .(O)"}}
{"id": "234", "dataset": "crossner_music", "split": "test", "label_list": ["location", "country", "album", "person", "event", "award", "musical artist", "song", "organization", "band", "musical instrument", "music genre"], "instance": {"id": "234", "words": ["It", "was", "not", "long", "before", "Peña", "was", "touring", "the", "world", ",", "both", "as", "a", "soloist", "and", "an", "accompanist", "with", "performances", "at", "Carnegie", "Hall", "in", "New", "York", "City", ",", "the", "Royal", "Albert", "Hall", "in", "London", "and", "the", "Concertgebouw", "in", "Amsterdam", "."], "labels": ["O", "O", "O", "O", "O", "B-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "I-location", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "O", "O", "B-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, album, person, event, award, musical artist, song, organization, band, musical instrument, music genre and O.\nSentence: It was not long before Peña was touring the world , both as a soloist and an accompanist with performances at Carnegie Hall in New York City , the Royal Albert Hall in London and the Concertgebouw in Amsterdam .", "prompt_labels": "It(O) was(O) not(O) long(O) before(O) Peña(B-band) was(O) touring(O) the(O) world(O) ,(O) both(O) as(O) a(O) soloist(O) and(O) an(O) accompanist(O) with(O) performances(O) at(O) Carnegie(B-location) Hall(I-location) in(O) New(B-location) York(I-location) City(I-location) ,(O) the(O) Royal(B-location) Albert(I-location) Hall(I-location) in(O) London(B-location) and(O) the(O) Concertgebouw(B-location) in(O) Amsterdam(B-location) .(O)"}}
{"id": "221", "dataset": "crossner_music", "split": "test", "label_list": ["music genre", "band", "musical instrument", "song", "person", "award", "organization", "event", "album", "location", "musical artist", "country"], "instance": {"id": "221", "words": ["Many", "stations", "play", "primarily", "gospel", "music", ",", "including", "Urban", "contemporary", "gospel", "and", "Southern", "Gospel", ",", "or", "contemporary", "worship", "music", ",", "while", "others", "play", "all", "formats", "of", "contemporary", "Christian", "music", ",", "including", "Christian", "pop", ",", "Christian", "rock", ",", "Christian", "rap", ",", "Christian", "country", "music", ",", "and", "Christian", "alternative", "rock", "."], "labels": ["O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, band, musical instrument, song, person, award, organization, event, album, location, musical artist, country and O.\nSentence: Many stations play primarily gospel music , including Urban contemporary gospel and Southern Gospel , or contemporary worship music , while others play all formats of contemporary Christian music , including Christian pop , Christian rock , Christian rap , Christian country music , and Christian alternative rock .", "prompt_labels": "Many(O) stations(O) play(O) primarily(O) gospel(B-music genre) music(I-music genre) ,(O) including(O) Urban(B-music genre) contemporary(I-music genre) gospel(I-music genre) and(O) Southern(B-music genre) Gospel(I-music genre) ,(O) or(O) contemporary(B-music genre) worship(I-music genre) music(I-music genre) ,(O) while(O) others(O) play(O) all(O) formats(O) of(O) contemporary(B-music genre) Christian(I-music genre) music(I-music genre) ,(O) including(O) Christian(O) pop(O) ,(O) Christian(B-music genre) rock(I-music genre) ,(O) Christian(B-music genre) rap(I-music genre) ,(O) Christian(B-music genre) country(I-music genre) music(I-music genre) ,(O) and(O) Christian(B-music genre) alternative(I-music genre) rock(I-music genre) .(O)"}}
{"id": "115", "dataset": "crossner_music", "split": "test", "label_list": ["location", "band", "person", "organization", "musical artist", "music genre", "country", "event", "album", "song", "award", "musical instrument"], "instance": {"id": "115", "words": ["Looking", "to", "depart", "from", "the", "distorted", "production", "of", "their", "previous", "record", ",", "The", "Downward", "Spiral", "(", "1994", ")", ",", "the", "album", "features", "elements", "of", "Ambient", "music", "and", "Electronic", "music", "music", ",", "alongside", "the", "band", "'s", "traditional", "industrial", "rock", "sound", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, band, person, organization, musical artist, music genre, country, event, album, song, award, musical instrument and O.\nSentence: Looking to depart from the distorted production of their previous record , The Downward Spiral ( 1994 ) , the album features elements of Ambient music and Electronic music music , alongside the band 's traditional industrial rock sound .", "prompt_labels": "Looking(O) to(O) depart(O) from(O) the(O) distorted(O) production(O) of(O) their(O) previous(O) record(O) ,(O) The(B-album) Downward(I-album) Spiral(I-album) ((O) 1994(O) )(O) ,(O) the(O) album(O) features(O) elements(O) of(O) Ambient(B-music genre) music(I-music genre) and(O) Electronic(B-music genre) music(I-music genre) music(O) ,(O) alongside(O) the(O) band(O) 's(O) traditional(O) industrial(B-music genre) rock(I-music genre) sound(O) .(O)"}}
{"id": "297", "dataset": "crossner_music", "split": "test", "label_list": ["musical instrument", "location", "music genre", "country", "band", "album", "song", "event", "award", "person", "organization", "musical artist"], "instance": {"id": "297", "words": ["Indie", "band", "Rilo", "Kiley", ",", "in", "keeping", "with", "their", "tendency", "to", "explore", "a", "variety", "of", "rockish", "styles", ",", "incorporated", "funk", "into", "their", "song", "The", "Moneymaker", "on", "the", "album", "Under", "the", "Blacklight", "."], "labels": ["O", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "B-song", "I-song", "O", "O", "O", "B-album", "I-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, location, music genre, country, band, album, song, event, award, person, organization, musical artist and O.\nSentence: Indie band Rilo Kiley , in keeping with their tendency to explore a variety of rockish styles , incorporated funk into their song The Moneymaker on the album Under the Blacklight .", "prompt_labels": "Indie(O) band(O) Rilo(B-band) Kiley(I-band) ,(O) in(O) keeping(O) with(O) their(O) tendency(O) to(O) explore(O) a(O) variety(O) of(O) rockish(O) styles(O) ,(O) incorporated(O) funk(B-music genre) into(O) their(O) song(O) The(B-song) Moneymaker(I-song) on(O) the(O) album(O) Under(B-album) the(I-album) Blacklight(I-album) .(O)"}}
{"id": "292", "dataset": "crossner_music", "split": "test", "label_list": ["country", "organization", "musical instrument", "music genre", "album", "event", "person", "award", "song", "band", "location", "musical artist"], "instance": {"id": "292", "words": ["Rock", "and", "roll", "songs", "critical", "of", "disco", "included", "Bob", "Seger", "'", "s", "Old", "Time", "Rock", "and", "Roll", "and", ",", "especially", ",", "The", "Who", "'", "s", "Sister", "Disco", "(", "both", "1978", ")", "-", "although", "The", "Who", "'s", "Eminence", "Front", "(", "four", "years", "later", ")", "had", "a", "disco", "feel", "."], "labels": ["B-music genre", "I-music genre", "I-music genre", "O", "O", "O", "B-music genre", "O", "B-musical artist", "I-musical artist", "O", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "B-band", "I-band", "O", "O", "B-song", "I-song", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, musical instrument, music genre, album, event, person, award, song, band, location, musical artist and O.\nSentence: Rock and roll songs critical of disco included Bob Seger ' s Old Time Rock and Roll and , especially , The Who ' s Sister Disco ( both 1978 ) - although The Who 's Eminence Front ( four years later ) had a disco feel .", "prompt_labels": "Rock(B-music genre) and(I-music genre) roll(I-music genre) songs(O) critical(O) of(O) disco(B-music genre) included(O) Bob(B-musical artist) Seger(I-musical artist) '(O) s(O) Old(B-song) Time(I-song) Rock(I-song) and(I-song) Roll(I-song) and(O) ,(O) especially(O) ,(O) The(B-band) Who(I-band) '(O) s(O) Sister(B-song) Disco(I-song) ((O) both(O) 1978(O) )(O) -(O) although(O) The(B-band) Who(I-band) 's(O) Eminence(B-song) Front(I-song) ((O) four(O) years(O) later(O) )(O) had(O) a(O) disco(B-music genre) feel(O) .(O)"}}
{"id": "42", "dataset": "crossner_music", "split": "test", "label_list": ["music genre", "band", "organization", "album", "award", "song", "event", "location", "person", "musical artist", "country", "musical instrument"], "instance": {"id": "42", "words": ["By", "the", "end", "of", "the", "decade", ",", "Underwood", "had", "amassed", "eight", "No.", "1", "songs", "on", "the", "Billboard", "Hot", "Country", "Songs", "chart", ",", "along", "with", "numerous", "awards", "from", "the", "Country", "Music", "Association", ",", "Academy", "of", "Country", "Music", "and", "others", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, band, organization, album, award, song, event, location, person, musical artist, country, musical instrument and O.\nSentence: By the end of the decade , Underwood had amassed eight No. 1 songs on the Billboard Hot Country Songs chart , along with numerous awards from the Country Music Association , Academy of Country Music and others .", "prompt_labels": "By(O) the(O) end(O) of(O) the(O) decade(O) ,(O) Underwood(B-musical artist) had(O) amassed(O) eight(O) No.(O) 1(O) songs(O) on(O) the(O) Billboard(B-organization) Hot(O) Country(O) Songs(O) chart(O) ,(O) along(O) with(O) numerous(O) awards(O) from(O) the(O) Country(B-organization) Music(I-organization) Association(I-organization) ,(O) Academy(B-organization) of(I-organization) Country(I-organization) Music(I-organization) and(O) others(O) .(O)"}}
{"id": "351", "dataset": "crossner_music", "split": "test", "label_list": ["band", "country", "album", "musical instrument", "musical artist", "music genre", "organization", "song", "event", "person", "award", "location"], "instance": {"id": "351", "words": ["Byrne", "has", "contributed", "songs", "to", "five", "AIDS", "benefit", "compilation", "albums", "produced", "by", "the", "Red", "Hot", "Organization", ":", "Red", "Hot", "+", "Blue", ":", "A", "Tribute", "to", "Cole", "Porter", ",", "Red", "Hot", "+", "Rio", ",", "Silencio", "=", "Muerte", ":", "Red", "Hot", "+", "Latin", ",", "Onda", "Sonora", ":", "Red", "Hot", "+", "Lisbon", ",", "and", "Offbeat", ":", "A", "Red", "Hot", "Soundtrip", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O", "B-album", "I-album", "I-album", "I-album", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, country, album, musical instrument, musical artist, music genre, organization, song, event, person, award, location and O.\nSentence: Byrne has contributed songs to five AIDS benefit compilation albums produced by the Red Hot Organization : Red Hot + Blue : A Tribute to Cole Porter , Red Hot + Rio , Silencio = Muerte : Red Hot + Latin , Onda Sonora : Red Hot + Lisbon , and Offbeat : A Red Hot Soundtrip .", "prompt_labels": "Byrne(B-musical artist) has(O) contributed(O) songs(O) to(O) five(O) AIDS(O) benefit(O) compilation(O) albums(O) produced(O) by(O) the(O) Red(B-organization) Hot(I-organization) Organization(I-organization) :(O) Red(B-album) Hot(I-album) +(I-album) Blue(I-album) :(I-album) A(I-album) Tribute(I-album) to(I-album) Cole(I-album) Porter(I-album) ,(O) Red(B-album) Hot(I-album) +(I-album) Rio(I-album) ,(O) Silencio(B-album) =(I-album) Muerte(I-album) :(I-album) Red(I-album) Hot(I-album) +(I-album) Latin(I-album) ,(O) Onda(B-album) Sonora(I-album) :(I-album) Red(I-album) Hot(I-album) +(I-album) Lisbon(I-album) ,(O) and(O) Offbeat(B-album) :(I-album) A(I-album) Red(I-album) Hot(I-album) Soundtrip(I-album) .(O)"}}
{"id": "385", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "organization", "song", "location", "country", "music genre", "album", "band", "musical instrument", "event", "person", "award"], "instance": {"id": "385", "words": ["Van", "Bentum", "competed", "in", "three", "consecutive", "Summer", "Olympics", "for", "her", "native", "country", ",", "starting", "in", "Swimming", "at", "the", "1980", "Summer", "Olympics", "."], "labels": ["B-person", "I-person", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, organization, song, location, country, music genre, album, band, musical instrument, event, person, award and O.\nSentence: Van Bentum competed in three consecutive Summer Olympics for her native country , starting in Swimming at the 1980 Summer Olympics .", "prompt_labels": "Van(B-person) Bentum(I-person) competed(O) in(O) three(O) consecutive(O) Summer(B-event) Olympics(I-event) for(O) her(O) native(O) country(O) ,(O) starting(O) in(O) Swimming(O) at(O) the(O) 1980(B-event) Summer(I-event) Olympics(I-event) .(O)"}}
{"id": "178", "dataset": "crossner_music", "split": "test", "label_list": ["music genre", "musical instrument", "event", "country", "band", "song", "person", "award", "album", "musical artist", "organization", "location"], "instance": {"id": "178", "words": ["He", "traveled", "to", "Africa", "in", "1973", ",", "where", "he", "visited", "Ethiopia", ",", "Kenya", ",", "Tanzania", ",", "Malawi", ",", "and", "South", "Africa", "."], "labels": ["O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, musical instrument, event, country, band, song, person, award, album, musical artist, organization, location and O.\nSentence: He traveled to Africa in 1973 , where he visited Ethiopia , Kenya , Tanzania , Malawi , and South Africa .", "prompt_labels": "He(O) traveled(O) to(O) Africa(B-location) in(O) 1973(O) ,(O) where(O) he(O) visited(O) Ethiopia(B-country) ,(O) Kenya(B-country) ,(O) Tanzania(B-country) ,(O) Malawi(B-country) ,(O) and(O) South(B-country) Africa(I-country) .(O)"}}
{"id": "58", "dataset": "crossner_music", "split": "test", "label_list": ["song", "music genre", "musical instrument", "event", "location", "award", "musical artist", "organization", "band", "person", "album", "country"], "instance": {"id": "58", "words": ["It", "includes", "collaborations", "with", "Pete", "Seeger", ",", "Ivan", "Neville", ",", "Cyril", "Neville", ",", "Skerik", ",", "Adam", "Levy", ",", "Righteous", "Babe", "recording", "artist", "Anaïs", "Mitchell", ",", "CC", "Adcock", ",", "and", "a", "host", "of", "New", "Orleans-based", "horn", "players", "known", "for", "their", "work", "in", "such", "outfits", "as", "Galactic", ",", "Bonerama", ",", "and", "Rebirth", "Brass", "Band", "."], "labels": ["O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-organization", "I-organization", "I-organization", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "O", "B-band", "O", "O", "B-band", "I-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, music genre, musical instrument, event, location, award, musical artist, organization, band, person, album, country and O.\nSentence: It includes collaborations with Pete Seeger , Ivan Neville , Cyril Neville , Skerik , Adam Levy , Righteous Babe recording artist Anaïs Mitchell , CC Adcock , and a host of New Orleans-based horn players known for their work in such outfits as Galactic , Bonerama , and Rebirth Brass Band .", "prompt_labels": "It(O) includes(O) collaborations(O) with(O) Pete(B-musical artist) Seeger(I-musical artist) ,(O) Ivan(B-musical artist) Neville(I-musical artist) ,(O) Cyril(B-musical artist) Neville(I-musical artist) ,(O) Skerik(B-musical artist) ,(O) Adam(B-musical artist) Levy(I-musical artist) ,(O) Righteous(B-organization) Babe(I-organization) recording(I-organization) artist(O) Anaïs(B-musical artist) Mitchell(I-musical artist) ,(O) CC(B-musical artist) Adcock(I-musical artist) ,(O) and(O) a(O) host(O) of(O) New(O) Orleans-based(O) horn(O) players(O) known(O) for(O) their(O) work(O) in(O) such(O) outfits(O) as(O) Galactic(B-band) ,(O) Bonerama(B-band) ,(O) and(O) Rebirth(B-band) Brass(I-band) Band(I-band) .(O)"}}
{"id": "62", "dataset": "crossner_music", "split": "test", "label_list": ["musical instrument", "music genre", "band", "country", "award", "location", "person", "organization", "event", "song", "musical artist", "album"], "instance": {"id": "62", "words": ["He", "helped", "found", "the", "American", "Conservatory", "Theater", "in", "San", "Francisco", ",", "the", "Mark", "Taper", "Forum", "in", "Los", "Angeles", ",", "and", "the", "Brooklyn", "Academy", "of", "Music", "Repertory", "Company", "in", "New", "York", "City", "."], "labels": ["O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "I-location", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "B-location", "I-location", "O", "B-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, music genre, band, country, award, location, person, organization, event, song, musical artist, album and O.\nSentence: He helped found the American Conservatory Theater in San Francisco , the Mark Taper Forum in Los Angeles , and the Brooklyn Academy of Music Repertory Company in New York City .", "prompt_labels": "He(O) helped(O) found(O) the(O) American(B-location) Conservatory(I-location) Theater(I-location) in(O) San(B-location) Francisco(I-location) ,(O) the(O) Mark(B-location) Taper(I-location) Forum(I-location) in(O) Los(B-location) Angeles(I-location) ,(O) and(O) the(O) Brooklyn(B-location) Academy(I-location) of(I-location) Music(I-location) Repertory(B-location) Company(I-location) in(O) New(B-location) York(I-location) City(I-location) .(O)"}}
{"id": "407", "dataset": "crossner_music", "split": "test", "label_list": ["musical instrument", "song", "band", "musical artist", "location", "event", "country", "organization", "album", "music genre", "award", "person"], "instance": {"id": "407", "words": ["He", "is", "the", "recipient", "of", "numerous", "accolades", ",", "including", "a", "Cannes", "Film", "Festival", "Award", "for", "Best", "Actor", "and", "nominations", "for", "a", "Tony", "Award", ",", "an", "Academy", "Awards", ",", "two", "Primetime", "Emmy", "Award", "s", "and", "five", "Golden", "Globe", "Awards", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, song, band, musical artist, location, event, country, organization, album, music genre, award, person and O.\nSentence: He is the recipient of numerous accolades , including a Cannes Film Festival Award for Best Actor and nominations for a Tony Award , an Academy Awards , two Primetime Emmy Award s and five Golden Globe Awards .", "prompt_labels": "He(O) is(O) the(O) recipient(O) of(O) numerous(O) accolades(O) ,(O) including(O) a(O) Cannes(B-award) Film(I-award) Festival(I-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) and(O) nominations(O) for(O) a(O) Tony(B-award) Award(I-award) ,(O) an(O) Academy(B-award) Awards(I-award) ,(O) two(O) Primetime(B-award) Emmy(I-award) Award(I-award) s(O) and(O) five(O) Golden(B-award) Globe(I-award) Awards(I-award) .(O)"}}
{"id": "184", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "organization", "event", "politician", "country", "political party", "location", "election"], "instance": {"id": "184", "words": ["Some", "international", "routes", "operate", "from", "Chengdu", ",", "Chongqing", ",", "Dalian", ",", "Hangzhou", ",", "Kunming", "and", "Xiamen", "."], "labels": ["O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, event, politician, country, political party, location, election and O.\nSentence: Some international routes operate from Chengdu , Chongqing , Dalian , Hangzhou , Kunming and Xiamen .", "prompt_labels": "Some(O) international(O) routes(O) operate(O) from(O) Chengdu(B-location) ,(O) Chongqing(B-location) ,(O) Dalian(B-location) ,(O) Hangzhou(B-location) ,(O) Kunming(B-location) and(O) Xiamen(B-location) .(O)"}}
{"id": "188", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "organization", "event", "location", "political party", "election", "politician", "person"], "instance": {"id": "188", "words": ["After", "serving", "two", "terms", "in", "the", "New", "Hampshire", "Senate", ",", "Shaheen", "was", "elected", "governor", "in", "1996", "New", "Hampshire", "gubernatorial", "election", "and", "reelected", "in", "1998", "New", "Hampshire", "gubernatorial", "election", "and", "2000", "New", "Hampshire", "gubernatorial", "election", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-politician", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, event, location, political party, election, politician, person and O.\nSentence: After serving two terms in the New Hampshire Senate , Shaheen was elected governor in 1996 New Hampshire gubernatorial election and reelected in 1998 New Hampshire gubernatorial election and 2000 New Hampshire gubernatorial election .", "prompt_labels": "After(O) serving(O) two(O) terms(O) in(O) the(O) New(B-organization) Hampshire(I-organization) Senate(I-organization) ,(O) Shaheen(B-politician) was(O) elected(O) governor(O) in(O) 1996(B-election) New(I-election) Hampshire(I-election) gubernatorial(I-election) election(I-election) and(O) reelected(O) in(O) 1998(B-election) New(I-election) Hampshire(I-election) gubernatorial(I-election) election(I-election) and(O) 2000(B-election) New(I-election) Hampshire(I-election) gubernatorial(I-election) election(I-election) .(O)"}}
{"id": "579", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "organization", "event", "election", "politician", "location", "country", "person"], "instance": {"id": "579", "words": ["During", "his", "premiership", ",", "Renzi", "faced", "several", "challenging", "foreign", "policy", "situations", ",", "such", "as", "the", "European", "debt", "crisis", ",", "the", "civil", "war", "in", "Libya", ",", "the", "Ukrainian", "Crisis", "and", "the", "insurgency", "of", "the", "Islamic", "State", "(", "IS", ")", "in", "the", "Middle", "East", "."], "labels": ["O", "O", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "B-event", "I-event", "O", "B-country", "O", "O", "B-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O", "B-country", "O", "O", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, event, election, politician, location, country, person and O.\nSentence: During his premiership , Renzi faced several challenging foreign policy situations , such as the European debt crisis , the civil war in Libya , the Ukrainian Crisis and the insurgency of the Islamic State ( IS ) in the Middle East .", "prompt_labels": "During(O) his(O) premiership(O) ,(O) Renzi(B-politician) faced(O) several(O) challenging(O) foreign(O) policy(O) situations(O) ,(O) such(O) as(O) the(O) European(B-event) debt(I-event) crisis(I-event) ,(O) the(O) civil(B-event) war(I-event) in(O) Libya(B-country) ,(O) the(O) Ukrainian(B-event) Crisis(I-event) and(O) the(O) insurgency(B-event) of(I-event) the(I-event) Islamic(I-event) State(I-event) ((O) IS(B-country) )(O) in(O) the(O) Middle(B-location) East(I-location) .(O)"}}
{"id": "238", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "location", "organization", "politician", "country", "person", "political party", "event"], "instance": {"id": "238", "words": ["The", "Workers", "'", "Party", "had", "its", "best", "performance", "at", "the", "polls", "in", "1989", "when", "it", "won", "seven", "seats", "in", "the", "1989", "Irish", "general", "election", "and", "party", "president", "Proinsias", "De", "Rossa", "won", "a", "seat", "in", "Dublin", "in", "the", "1989", "European", "Parliament", "election", "in", "Ireland", "held", "on", "the", "same", "day", ",", "sitting", "with", "the", "communist", "Left", "Unity", "group", "."], "labels": ["O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "B-location", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, location, organization, politician, country, person, political party, event and O.\nSentence: The Workers ' Party had its best performance at the polls in 1989 when it won seven seats in the 1989 Irish general election and party president Proinsias De Rossa won a seat in Dublin in the 1989 European Parliament election in Ireland held on the same day , sitting with the communist Left Unity group .", "prompt_labels": "The(O) Workers(B-political party) '(I-political party) Party(I-political party) had(O) its(O) best(O) performance(O) at(O) the(O) polls(O) in(O) 1989(O) when(O) it(O) won(O) seven(O) seats(O) in(O) the(O) 1989(B-election) Irish(I-election) general(I-election) election(I-election) and(O) party(O) president(O) Proinsias(B-politician) De(I-politician) Rossa(I-politician) won(O) a(O) seat(O) in(O) Dublin(B-location) in(O) the(O) 1989(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) Ireland(I-election) held(O) on(O) the(O) same(O) day(O) ,(O) sitting(O) with(O) the(O) communist(O) Left(B-political party) Unity(I-political party) group(O) .(O)"}}
{"id": "273", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "political party", "country", "election", "person", "politician", "location", "organization"], "instance": {"id": "273", "words": ["Levin", "was", "re-elected", "in", "1984", "United", "States", "Senate", "election", "in", "Michigan", ",", "1990", "United", "States", "Senate", "election", "in", "Michigan", ",", "1996", "United", "States", "Senate", "election", "in", "Michigan", ",", "2002", "United", "States", "Senate", "election", "in", "Michigan", "and", "2008", "United", "States", "Senate", "election", "in", "Michigan", "."], "labels": ["B-politician", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, country, election, person, politician, location, organization and O.\nSentence: Levin was re-elected in 1984 United States Senate election in Michigan , 1990 United States Senate election in Michigan , 1996 United States Senate election in Michigan , 2002 United States Senate election in Michigan and 2008 United States Senate election in Michigan .", "prompt_labels": "Levin(B-politician) was(O) re-elected(O) in(O) 1984(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Michigan(I-election) ,(O) 1990(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Michigan(I-election) ,(O) 1996(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Michigan(I-election) ,(O) 2002(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Michigan(I-election) and(O) 2008(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Michigan(I-election) .(O)"}}
{"id": "586", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "person", "political party", "location", "organization", "election", "country", "event"], "instance": {"id": "586", "words": ["On", "the", "second", "day", "of", "the", "Summit", ",", "President", "Obama", "announced", "that", "the", "next", "summit", "meeting", "about", "this", "subject", "will", "be", "in", "South", "Korea", ";", "see", "2012", "Nuclear", "Security", "Summit", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-event", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "B-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, political party, location, organization, election, country, event and O.\nSentence: On the second day of the Summit , President Obama announced that the next summit meeting about this subject will be in South Korea ; see 2012 Nuclear Security Summit .", "prompt_labels": "On(O) the(O) second(O) day(O) of(O) the(O) Summit(B-event) ,(O) President(O) Obama(B-politician) announced(O) that(O) the(O) next(O) summit(O) meeting(O) about(O) this(O) subject(O) will(O) be(O) in(O) South(B-country) Korea(I-country) ;(O) see(O) 2012(B-event) Nuclear(I-event) Security(I-event) Summit(I-event) .(O)"}}
{"id": "546", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "organization", "event", "country", "election", "political party", "person", "politician"], "instance": {"id": "546", "words": ["The", "rest", "of", "the", "film", "'s", "cast", "consists", "of", "Cherie", "Gil", ",", "Lou", "Veloso", ",", "Nonie", "Buencamino", ",", "Anna", "Luna", ",", "Allan", "Paule", ",", "Richard", "Quan", ",", "Victor", "Neri", ",", "Ruby", "Ruiz", ",", "Nanding", "Josef", "and", "Dina", "Bonnevie", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, event, country, election, political party, person, politician and O.\nSentence: The rest of the film 's cast consists of Cherie Gil , Lou Veloso , Nonie Buencamino , Anna Luna , Allan Paule , Richard Quan , Victor Neri , Ruby Ruiz , Nanding Josef and Dina Bonnevie .", "prompt_labels": "The(O) rest(O) of(O) the(O) film(O) 's(O) cast(O) consists(O) of(O) Cherie(B-person) Gil(I-person) ,(O) Lou(B-person) Veloso(I-person) ,(O) Nonie(B-person) Buencamino(I-person) ,(O) Anna(B-person) Luna(I-person) ,(O) Allan(B-person) Paule(I-person) ,(O) Richard(B-person) Quan(I-person) ,(O) Victor(B-person) Neri(I-person) ,(O) Ruby(B-person) Ruiz(I-person) ,(O) Nanding(B-person) Josef(I-person) and(O) Dina(B-person) Bonnevie(I-person) .(O)"}}
{"id": "47", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "location", "election", "organization", "person", "politician", "political party", "country"], "instance": {"id": "47", "words": ["The", "Scythians", "and", "Cimmerians", "took", "advantage", "of", "the", "bitter", "fighting", "among", "the", "Assyrians", "to", "raid", "Neo-Assyrian", "Empire", "colonies", ",", "with", "hordes", "of", "horse-borne", "marauders", "ravaging", "parts", "of", "Asia", "Minor", "and", "the", "Caucasus", ",", "where", "the", "vassal", "kings", "of", "Urartu", "and", "Lydia", "begged", "their", "Neo-Assyrian", "Empire", "overlord", "for", "help", "in", "vain", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, election, organization, person, politician, political party, country and O.\nSentence: The Scythians and Cimmerians took advantage of the bitter fighting among the Assyrians to raid Neo-Assyrian Empire colonies , with hordes of horse-borne marauders ravaging parts of Asia Minor and the Caucasus , where the vassal kings of Urartu and Lydia begged their Neo-Assyrian Empire overlord for help in vain .", "prompt_labels": "The(O) Scythians(O) and(O) Cimmerians(O) took(O) advantage(O) of(O) the(O) bitter(O) fighting(O) among(O) the(O) Assyrians(O) to(O) raid(O) Neo-Assyrian(B-country) Empire(I-country) colonies(O) ,(O) with(O) hordes(O) of(O) horse-borne(O) marauders(O) ravaging(O) parts(O) of(O) Asia(B-location) Minor(I-location) and(O) the(O) Caucasus(B-location) ,(O) where(O) the(O) vassal(O) kings(O) of(O) Urartu(B-country) and(O) Lydia(B-country) begged(O) their(O) Neo-Assyrian(B-country) Empire(I-country) overlord(O) for(O) help(O) in(O) vain(O) .(O)"}}
{"id": "403", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "election", "event", "person", "politician", "political party", "location", "organization"], "instance": {"id": "403", "words": ["After", "standing", "unsuccessfully", "at", "West", "Derbyshire", "in", "1966", "United", "Kingdom", "general", "election", ",", "he", "represented", "Derby", "North", "as", "a", "Labour", "MP", "from", "1970", "United", "Kingdom", "general", "election", "to", "1983", "United", "Kingdom", "general", "election", ",", "when", "he", "was", "defeated", "by", "the", "Conservative", "Greg", "Knight", "."], "labels": ["O", "O", "O", "O", "B-location", "I-location", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "B-location", "I-location", "O", "O", "B-politician", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, event, person, politician, political party, location, organization and O.\nSentence: After standing unsuccessfully at West Derbyshire in 1966 United Kingdom general election , he represented Derby North as a Labour MP from 1970 United Kingdom general election to 1983 United Kingdom general election , when he was defeated by the Conservative Greg Knight .", "prompt_labels": "After(O) standing(O) unsuccessfully(O) at(O) West(B-location) Derbyshire(I-location) in(O) 1966(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) he(O) represented(O) Derby(B-location) North(I-location) as(O) a(O) Labour(B-politician) MP(O) from(O) 1970(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) to(O) 1983(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) when(O) he(O) was(O) defeated(O) by(O) the(O) Conservative(O) Greg(B-politician) Knight(I-politician) .(O)"}}
{"id": "57", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "election", "location", "organization", "political party", "person", "politician", "event"], "instance": {"id": "57", "words": ["In", "2019", "Widdecombe", "appeared", "on", "the", "celebrity", "version", "of", "The", "Crystal", "Maze", ",", "where", "alongside", "Sunetra", "Sarker", ",", "Wes", "Nelson", ",", "Matthew", "Wright", "and", "Nikki", "Sanderson", ",", "she", "won", "money", "for", "the", "Stand", "Up", "to", "Cancer", "initiative", "."], "labels": ["O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, location, organization, political party, person, politician, event and O.\nSentence: In 2019 Widdecombe appeared on the celebrity version of The Crystal Maze , where alongside Sunetra Sarker , Wes Nelson , Matthew Wright and Nikki Sanderson , she won money for the Stand Up to Cancer initiative .", "prompt_labels": "In(O) 2019(O) Widdecombe(B-politician) appeared(O) on(O) the(O) celebrity(O) version(O) of(O) The(O) Crystal(O) Maze(O) ,(O) where(O) alongside(O) Sunetra(B-person) Sarker(I-person) ,(O) Wes(B-person) Nelson(I-person) ,(O) Matthew(B-person) Wright(I-person) and(O) Nikki(B-person) Sanderson(I-person) ,(O) she(O) won(O) money(O) for(O) the(O) Stand(O) Up(O) to(O) Cancer(O) initiative(O) .(O)"}}
{"id": "25", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "event", "country", "politician", "location", "political party", "election", "organization"], "instance": {"id": "25", "words": ["Jackson", "'s", "state", "department", "was", "active", "and", "successful", "at", "making", "trade", "agreements", "with", "Russia", ",", "Spain", ",", "Turkey", ",", "United", "Kingdom", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, country, politician, location, political party, election, organization and O.\nSentence: Jackson 's state department was active and successful at making trade agreements with Russia , Spain , Turkey , United Kingdom .", "prompt_labels": "Jackson(B-politician) 's(O) state(O) department(O) was(O) active(O) and(O) successful(O) at(O) making(O) trade(O) agreements(O) with(O) Russia(B-country) ,(O) Spain(B-country) ,(O) Turkey(B-country) ,(O) United(B-country) Kingdom(I-country) .(O)"}}
{"id": "638", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "organization", "political party", "election", "location", "country", "person", "event"], "instance": {"id": "638", "words": ["In", "the", "summer", "of", "1914", ",", "he", "took", "part", "in", "the", "meetings", "of", "the", "Crown", "Council", "held", "at", "Sinaia", ",", "arguing", "for", "Romania", "'s", "neutrality", "in", "World", "War", "I.", "Two", "years", "later", ",", "he", "took", "part", "in", "the", "Crown", "Council", "meeting", "at", "Cotroceni", "Palace", ",", "voting", "for", "Romania", "'s", "entry", "into", "the", "war", "on", "the", "side", "of", "the", "Allies", "of", "World", "War", "I", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-location", "O", "O", "O", "B-country", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-location", "I-location", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, political party, election, location, country, person, event and O.\nSentence: In the summer of 1914 , he took part in the meetings of the Crown Council held at Sinaia , arguing for Romania 's neutrality in World War I. Two years later , he took part in the Crown Council meeting at Cotroceni Palace , voting for Romania 's entry into the war on the side of the Allies of World War I .", "prompt_labels": "In(O) the(O) summer(O) of(O) 1914(O) ,(O) he(O) took(O) part(O) in(O) the(O) meetings(O) of(O) the(O) Crown(B-organization) Council(I-organization) held(O) at(O) Sinaia(B-location) ,(O) arguing(O) for(O) Romania(B-country) 's(O) neutrality(O) in(O) World(B-event) War(I-event) I.(I-event) Two(O) years(O) later(O) ,(O) he(O) took(O) part(O) in(O) the(O) Crown(B-organization) Council(I-organization) meeting(O) at(O) Cotroceni(B-location) Palace(I-location) ,(O) voting(O) for(O) Romania(B-country) 's(O) entry(O) into(O) the(O) war(O) on(O) the(O) side(O) of(O) the(O) Allies(B-organization) of(I-organization) World(I-organization) War(I-organization) I(I-organization) .(O)"}}
{"id": "460", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "election", "political party", "politician", "country", "organization", "event", "location"], "instance": {"id": "460", "words": ["Following", "the", "1988", "Canadian", "federal", "election", ",", "he", "co-authored", "Election", ":", "the", "issues", ",", "the", "strategies", ",", "the", "aftermath", "with", "Liberal", "Party", "of", "Canada", "strategiest", "Michael", "J.", "L.", "Kirby", "and", "Conservative", "Party", "of", "Canada", "strategist", "Hugh", "Segal", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "I-politician", "I-politician", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, election, political party, politician, country, organization, event, location and O.\nSentence: Following the 1988 Canadian federal election , he co-authored Election : the issues , the strategies , the aftermath with Liberal Party of Canada strategiest Michael J. L. Kirby and Conservative Party of Canada strategist Hugh Segal .", "prompt_labels": "Following(O) the(O) 1988(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) he(O) co-authored(O) Election(O) :(O) the(O) issues(O) ,(O) the(O) strategies(O) ,(O) the(O) aftermath(O) with(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) strategiest(O) Michael(B-politician) J.(I-politician) L.(I-politician) Kirby(I-politician) and(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) strategist(O) Hugh(B-politician) Segal(I-politician) .(O)"}}
{"id": "458", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "person", "country", "political party", "location", "organization", "event", "election"], "instance": {"id": "458", "words": ["At", "the", "1989", "European", "Parliament", "election", "in", "Italy", "there", "were", "two", "competing", "green", "parties", ":", "the", "LV", "and", "the", "Rainbow", "Greens", "(", "VA", ")", ",", "formed", "mainly", "by", "Radicals", ",", "including", "Adelaide", "Aglietta", ",", "Franco", "Corleone", ",", "Adele", "Faccio", ",", "Marco", "Taradash", "and", "Francesco", "Rutelli", ",", "as", "well", "as", "splinters", "from", "Proletarian", "Democracy", ",", "including", "Mario", "Capanna", ",", "Guido", "Pollice", ",", "Gianni", "Tamino", "and", "Edo", "Ronchi", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "O", "B-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, country, political party, location, organization, event, election and O.\nSentence: At the 1989 European Parliament election in Italy there were two competing green parties : the LV and the Rainbow Greens ( VA ) , formed mainly by Radicals , including Adelaide Aglietta , Franco Corleone , Adele Faccio , Marco Taradash and Francesco Rutelli , as well as splinters from Proletarian Democracy , including Mario Capanna , Guido Pollice , Gianni Tamino and Edo Ronchi .", "prompt_labels": "At(O) the(O) 1989(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) Italy(I-election) there(O) were(O) two(O) competing(O) green(O) parties(O) :(O) the(O) LV(B-political party) and(O) the(O) Rainbow(B-political party) Greens(I-political party) ((O) VA(B-political party) )(O) ,(O) formed(O) mainly(O) by(O) Radicals(O) ,(O) including(O) Adelaide(B-politician) Aglietta(I-politician) ,(O) Franco(B-politician) Corleone(I-politician) ,(O) Adele(B-politician) Faccio(I-politician) ,(O) Marco(B-politician) Taradash(I-politician) and(O) Francesco(B-politician) Rutelli(I-politician) ,(O) as(O) well(O) as(O) splinters(O) from(O) Proletarian(B-political party) Democracy(I-political party) ,(O) including(O) Mario(B-politician) Capanna(I-politician) ,(O) Guido(B-politician) Pollice(I-politician) ,(O) Gianni(B-politician) Tamino(I-politician) and(O) Edo(B-politician) Ronchi(I-politician) .(O)"}}
{"id": "345", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "country", "location", "politician", "political party", "person", "election", "event"], "instance": {"id": "345", "words": ["Throughout", "his", "political", "career", "he", "has", "been", "variously", "associated", "with", "conservative", "groups", ",", "including", "the", "Association", "of", "Christian", "Parent", "Controlled", "Schools", ",", "Salt", "Shakers", ",", "Focus", "on", "the", "Family", ",", "Lyons", "Forum", ",", "Endeavour", "Forum", ",", "Family", "Council", "of", "Victoria", ",", "Fatherhood", "Foundation", ",", "Australian", "Christian", "Lobby", ",", "Australian", "Family", "Association", "and", "Right", "to", "Life", "Australia", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, location, politician, political party, person, election, event and O.\nSentence: Throughout his political career he has been variously associated with conservative groups , including the Association of Christian Parent Controlled Schools , Salt Shakers , Focus on the Family , Lyons Forum , Endeavour Forum , Family Council of Victoria , Fatherhood Foundation , Australian Christian Lobby , Australian Family Association and Right to Life Australia .", "prompt_labels": "Throughout(O) his(O) political(O) career(O) he(O) has(O) been(O) variously(O) associated(O) with(O) conservative(O) groups(O) ,(O) including(O) the(O) Association(B-organization) of(I-organization) Christian(I-organization) Parent(I-organization) Controlled(I-organization) Schools(I-organization) ,(O) Salt(B-organization) Shakers(I-organization) ,(O) Focus(B-organization) on(I-organization) the(I-organization) Family(I-organization) ,(O) Lyons(B-organization) Forum(I-organization) ,(O) Endeavour(B-organization) Forum(I-organization) ,(O) Family(B-organization) Council(I-organization) of(I-organization) Victoria(I-organization) ,(O) Fatherhood(B-organization) Foundation(I-organization) ,(O) Australian(B-organization) Christian(I-organization) Lobby(I-organization) ,(O) Australian(B-organization) Family(I-organization) Association(I-organization) and(O) Right(B-organization) to(I-organization) Life(I-organization) Australia(I-organization) .(O)"}}
{"id": "512", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "country", "person", "political party", "location", "politician", "organization", "event"], "instance": {"id": "512", "words": ["Incumbent", "John", "Melcher", ",", "who", "was", "first", "elected", "to", "the", "Senate", "in", "1976", "United", "States", "Senate", "election", "in", "Montana", "and", "was", "re-elected", "in", "1982", "United", "States", "Senate", "election", "in", "Montana", ",", "ran", "for", "re-election", "."], "labels": ["O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, country, person, political party, location, politician, organization, event and O.\nSentence: Incumbent John Melcher , who was first elected to the Senate in 1976 United States Senate election in Montana and was re-elected in 1982 United States Senate election in Montana , ran for re-election .", "prompt_labels": "Incumbent(O) John(B-politician) Melcher(I-politician) ,(O) who(O) was(O) first(O) elected(O) to(O) the(O) Senate(B-organization) in(O) 1976(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Montana(I-election) and(O) was(O) re-elected(O) in(O) 1982(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Montana(I-election) ,(O) ran(O) for(O) re-election(O) .(O)"}}
{"id": "250", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "election", "organization", "location", "politician", "person", "event", "country"], "instance": {"id": "250", "words": ["One", "of", "the", "main", "goals", "of", "the", "Dal", "is", "to", "build", "the", "Ramjanmabhoomi", "temple", "in", "Ayodhya", ",", "the", "Krishnajanmabhoomi", "temple", "in", "Mathura", "and", "the", "Kashi", "Vishwanath", "temple", "in", "Varanasi", ",", "which", "are", "currently", "disputed", "places", "of", "worship", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "O", "O", "B-location", "I-location", "O", "B-location", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, election, organization, location, politician, person, event, country and O.\nSentence: One of the main goals of the Dal is to build the Ramjanmabhoomi temple in Ayodhya , the Krishnajanmabhoomi temple in Mathura and the Kashi Vishwanath temple in Varanasi , which are currently disputed places of worship .", "prompt_labels": "One(O) of(O) the(O) main(O) goals(O) of(O) the(O) Dal(O) is(O) to(O) build(O) the(O) Ramjanmabhoomi(B-location) temple(I-location) in(O) Ayodhya(B-location) ,(O) the(O) Krishnajanmabhoomi(B-location) temple(I-location) in(O) Mathura(B-location) and(O) the(O) Kashi(B-location) Vishwanath(I-location) temple(I-location) in(O) Varanasi(B-location) ,(O) which(O) are(O) currently(O) disputed(O) places(O) of(O) worship(O) .(O)"}}
{"id": "37", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "election", "organization", "location", "event", "politician", "country", "political party"], "instance": {"id": "37", "words": ["Allies", "of", "the", "ACLU", "in", "legal", "actions", "have", "included", "the", "National", "Association", "for", "the", "Advancement", "of", "Colored", "People", ",", "the", "American", "Jewish", "Congress", ",", "People", "For", "the", "American", "Way", ",", "the", "National", "Rifle", "Association", ",", "the", "Electronic", "Frontier", "Foundation", ",", "Americans", "United", "for", "Separation", "of", "Church", "and", "State", ",", "and", "the", "National", "Organization", "for", "Women", "."], "labels": ["O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, election, organization, location, event, politician, country, political party and O.\nSentence: Allies of the ACLU in legal actions have included the National Association for the Advancement of Colored People , the American Jewish Congress , People For the American Way , the National Rifle Association , the Electronic Frontier Foundation , Americans United for Separation of Church and State , and the National Organization for Women .", "prompt_labels": "Allies(O) of(O) the(O) ACLU(B-organization) in(O) legal(O) actions(O) have(O) included(O) the(O) National(B-organization) Association(I-organization) for(I-organization) the(I-organization) Advancement(I-organization) of(I-organization) Colored(I-organization) People(I-organization) ,(O) the(O) American(B-organization) Jewish(I-organization) Congress(I-organization) ,(O) People(B-organization) For(I-organization) the(I-organization) American(I-organization) Way(I-organization) ,(O) the(O) National(B-organization) Rifle(I-organization) Association(I-organization) ,(O) the(O) Electronic(B-organization) Frontier(I-organization) Foundation(I-organization) ,(O) Americans(B-organization) United(I-organization) for(I-organization) Separation(I-organization) of(I-organization) Church(I-organization) and(I-organization) State(I-organization) ,(O) and(O) the(O) National(B-organization) Organization(I-organization) for(I-organization) Women(I-organization) .(O)"}}
{"id": "507", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "person", "event", "organization", "political party", "country", "election", "politician"], "instance": {"id": "507", "words": ["In", "the", "1990", "California", "gubernatorial", "election", ",", "Republican", "senator", "Pete", "Wilson", "had", "beaten", "Democrat", "Dianne", "Feinstein", "for", "governor", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, event, organization, political party, country, election, politician and O.\nSentence: In the 1990 California gubernatorial election , Republican senator Pete Wilson had beaten Democrat Dianne Feinstein for governor .", "prompt_labels": "In(O) the(O) 1990(B-election) California(I-election) gubernatorial(I-election) election(I-election) ,(O) Republican(O) senator(O) Pete(B-politician) Wilson(I-politician) had(O) beaten(O) Democrat(O) Dianne(B-politician) Feinstein(I-politician) for(O) governor(O) .(O)"}}
{"id": "347", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "politician", "country", "election", "location", "organization", "person", "event"], "instance": {"id": "347", "words": ["244", "Among", "the", "most", "important", "are", "World", "Vision", "International", "(", "1950", ")", ",", "Samaritan", "'s", "Purse", "(", "1970", ")", ",", "Mercy", "Ships", "(", "1978", ")", ",", "Prison", "Fellowship", "International", "(", "1979", ")", ",", "International", "Justice", "Mission", "(", "1997", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, country, election, location, organization, person, event and O.\nSentence: 244 Among the most important are World Vision International ( 1950 ) , Samaritan 's Purse ( 1970 ) , Mercy Ships ( 1978 ) , Prison Fellowship International ( 1979 ) , International Justice Mission ( 1997 ) .", "prompt_labels": "244(O) Among(O) the(O) most(O) important(O) are(O) World(B-organization) Vision(I-organization) International(I-organization) ((O) 1950(O) )(O) ,(O) Samaritan(B-organization) 's(I-organization) Purse(I-organization) ((O) 1970(O) )(O) ,(O) Mercy(B-organization) Ships(I-organization) ((O) 1978(O) )(O) ,(O) Prison(B-organization) Fellowship(I-organization) International(I-organization) ((O) 1979(O) )(O) ,(O) International(B-organization) Justice(I-organization) Mission(I-organization) ((O) 1997(O) )(O) .(O)"}}
{"id": "327", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "person", "politician", "country", "political party", "election", "organization", "location"], "instance": {"id": "327", "words": ["Goldstone", "serves", "on", "the", "Board", "of", "Directors", "of", "several", "nonprofit", "organisations", "that", "promote", "justice", ",", "including", "Physicians", "for", "Human", "Rights", ",", "the", "International", "Center", "for", "Transitional", "Justice", ",", "the", "Institute", "for", "Justice", "and", "Reconciliation", ",", "the", "South", "African", "Legal", "Services", "Foundation", ",", "the", "Brandeis", "University", "Center", "for", "Ethics", ",", "Justice", ",", "and", "Public", "Life", ",", "Human", "Rights", "Watch", ",", "and", "the", "Center", "for", "Economic", "and", "Social", "Rights", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, politician, country, political party, election, organization, location and O.\nSentence: Goldstone serves on the Board of Directors of several nonprofit organisations that promote justice , including Physicians for Human Rights , the International Center for Transitional Justice , the Institute for Justice and Reconciliation , the South African Legal Services Foundation , the Brandeis University Center for Ethics , Justice , and Public Life , Human Rights Watch , and the Center for Economic and Social Rights .", "prompt_labels": "Goldstone(B-politician) serves(O) on(O) the(O) Board(O) of(O) Directors(O) of(O) several(O) nonprofit(O) organisations(O) that(O) promote(O) justice(O) ,(O) including(O) Physicians(B-organization) for(I-organization) Human(I-organization) Rights(I-organization) ,(O) the(O) International(B-organization) Center(I-organization) for(I-organization) Transitional(I-organization) Justice(I-organization) ,(O) the(O) Institute(B-organization) for(I-organization) Justice(I-organization) and(I-organization) Reconciliation(I-organization) ,(O) the(O) South(B-organization) African(I-organization) Legal(I-organization) Services(I-organization) Foundation(I-organization) ,(O) the(O) Brandeis(B-organization) University(I-organization) Center(I-organization) for(I-organization) Ethics(I-organization) ,(I-organization) Justice(I-organization) ,(I-organization) and(I-organization) Public(I-organization) Life(I-organization) ,(O) Human(B-organization) Rights(I-organization) Watch(I-organization) ,(O) and(O) the(O) Center(B-organization) for(I-organization) Economic(I-organization) and(I-organization) Social(I-organization) Rights(I-organization) .(O)"}}
{"id": "429", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "organization", "country", "political party", "person", "politician", "event", "location"], "instance": {"id": "429", "words": ["For", "the", "2018", "Mexican", "general", "election", ",", "the", "National", "Regeneration", "Movement", "proposed", "having", "Senator", "Rabindranath", "Salazar", "Solorio", "as", "the", "candidate", "under", "the", "coalition", "Juntos", "Haremos", "Historia", "for", "the", "Governor", "of", "Morelos", "but", "PES", ",", "also", "part", "of", "the", "coalition", ",", "argued", "Blanco", "was", "the", "better", "choice", "for", "the", "coalition", "'s", "candidate", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, country, political party, person, politician, event, location and O.\nSentence: For the 2018 Mexican general election , the National Regeneration Movement proposed having Senator Rabindranath Salazar Solorio as the candidate under the coalition Juntos Haremos Historia for the Governor of Morelos but PES , also part of the coalition , argued Blanco was the better choice for the coalition 's candidate .", "prompt_labels": "For(O) the(O) 2018(B-election) Mexican(I-election) general(I-election) election(I-election) ,(O) the(O) National(B-political party) Regeneration(I-political party) Movement(I-political party) proposed(O) having(O) Senator(O) Rabindranath(B-politician) Salazar(I-politician) Solorio(I-politician) as(O) the(O) candidate(O) under(O) the(O) coalition(O) Juntos(B-political party) Haremos(I-political party) Historia(I-political party) for(O) the(O) Governor(B-political party) of(I-political party) Morelos(I-political party) but(O) PES(B-organization) ,(O) also(O) part(O) of(O) the(O) coalition(O) ,(O) argued(O) Blanco(B-politician) was(O) the(O) better(O) choice(O) for(O) the(O) coalition(O) 's(O) candidate(O) .(O)"}}
{"id": "380", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "person", "location", "organization", "political party", "election", "country", "politician"], "instance": {"id": "380", "words": ["Presidential", "elections", "have", "been", "held", "in", "1999", "Slovak", "presidential", "election", ",", "2004", "Slovak", "presidential", "election", ",", "2009", "Slovak", "presidential", "election", ",", "2014", "Slovak", "presidential", "election", "and", "2019", "Slovak", "presidential", "election", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, location, organization, political party, election, country, politician and O.\nSentence: Presidential elections have been held in 1999 Slovak presidential election , 2004 Slovak presidential election , 2009 Slovak presidential election , 2014 Slovak presidential election and 2019 Slovak presidential election .", "prompt_labels": "Presidential(O) elections(O) have(O) been(O) held(O) in(O) 1999(B-election) Slovak(I-election) presidential(I-election) election(I-election) ,(O) 2004(B-election) Slovak(I-election) presidential(I-election) election(I-election) ,(O) 2009(B-election) Slovak(I-election) presidential(I-election) election(I-election) ,(O) 2014(B-election) Slovak(I-election) presidential(I-election) election(I-election) and(O) 2019(B-election) Slovak(I-election) presidential(I-election) election(I-election) .(O)"}}
{"id": "252", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "event", "person", "organization", "location", "country", "politician", "election"], "instance": {"id": "252", "words": ["The", "agency", "also", "has", "offices", "in", "Ottawa", ",", "at", "the", "David", "Florida", "Laboratory", ",", "and", "small", "liaison", "offices", "in", "Houston", ";", "Washington", ",", "D.C.", ";", "and", "Paris", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "I-location", "I-location", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, person, organization, location, country, politician, election and O.\nSentence: The agency also has offices in Ottawa , at the David Florida Laboratory , and small liaison offices in Houston ; Washington , D.C. ; and Paris .", "prompt_labels": "The(O) agency(O) also(O) has(O) offices(O) in(O) Ottawa(B-location) ,(O) at(O) the(O) David(B-location) Florida(I-location) Laboratory(I-location) ,(O) and(O) small(O) liaison(O) offices(O) in(O) Houston(B-location) ;(O) Washington(B-location) ,(I-location) D.C.(I-location) ;(O) and(O) Paris(B-location) .(O)"}}
{"id": "272", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "person", "election", "event", "location", "politician", "organization", "political party"], "instance": {"id": "272", "words": ["She", "has", "received", "low", "scores", "from", "low-spending", "advocates", "(", "Club", "for", "Growth", ",", "2016", ",", "8", "%", ";", "Citizens", "Against", "Government", "Waste", ",", "2015", ",", "0", "%", ";", "National", "Taxpayers", "Union", ",", "2015", ",", "9", "%", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, election, event, location, politician, organization, political party and O.\nSentence: She has received low scores from low-spending advocates ( Club for Growth , 2016 , 8 % ; Citizens Against Government Waste , 2015 , 0 % ; National Taxpayers Union , 2015 , 9 % ) .", "prompt_labels": "She(O) has(O) received(O) low(O) scores(O) from(O) low-spending(O) advocates(O) ((O) Club(B-organization) for(I-organization) Growth(I-organization) ,(O) 2016(O) ,(O) 8(O) %(O) ;(O) Citizens(B-organization) Against(I-organization) Government(I-organization) Waste(I-organization) ,(O) 2015(O) ,(O) 0(O) %(O) ;(O) National(B-organization) Taxpayers(I-organization) Union(I-organization) ,(O) 2015(O) ,(O) 9(O) %(O) )(O) .(O)"}}
{"id": "521", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "election", "country", "event", "location", "person", "political party", "organization"], "instance": {"id": "521", "words": ["Green", "deputies", "Noël", "Mamère", ",", "Martine", "Billard", "and", "Yves", "Cochet", "on", "September", "10", ",", "2003", "requested", "a", "Parliamentary", "Commission", "on", "the", "role", "of", "France", "in", "the", "support", "of", "military", "regimes", "in", "Latin", "America", "from", "1973", "to", "1984", "before", "the", "Foreign", "Affairs", "Commission", "of", "the", "National", "Assembly", ",", "presided", "by", "Edouard", "Balladur", "."], "labels": ["O", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, country, event, location, person, political party, organization and O.\nSentence: Green deputies Noël Mamère , Martine Billard and Yves Cochet on September 10 , 2003 requested a Parliamentary Commission on the role of France in the support of military regimes in Latin America from 1973 to 1984 before the Foreign Affairs Commission of the National Assembly , presided by Edouard Balladur .", "prompt_labels": "Green(O) deputies(O) Noël(B-politician) Mamère(I-politician) ,(O) Martine(B-politician) Billard(I-politician) and(O) Yves(B-politician) Cochet(I-politician) on(O) September(O) 10(O) ,(O) 2003(O) requested(O) a(O) Parliamentary(O) Commission(O) on(O) the(O) role(O) of(O) France(B-country) in(O) the(O) support(O) of(O) military(O) regimes(O) in(O) Latin(B-location) America(I-location) from(O) 1973(O) to(O) 1984(O) before(O) the(O) Foreign(B-organization) Affairs(I-organization) Commission(I-organization) of(I-organization) the(I-organization) National(I-organization) Assembly(I-organization) ,(O) presided(O) by(O) Edouard(B-politician) Balladur(I-politician) .(O)"}}
{"id": "6", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "person", "location", "event", "political party", "country", "organization", "election"], "instance": {"id": "6", "words": ["He", "invited", "the", "chieftains", "of", "the", "former", "satrapy", "of", "Gandhara", "(", "a", "region", "presently", "straddling", "eastern", "Afghanistan", "and", "northern", "Pakistan", ")", ",", "to", "come", "to", "him", "and", "submit", "to", "his", "authority", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, location, event, political party, country, organization, election and O.\nSentence: He invited the chieftains of the former satrapy of Gandhara ( a region presently straddling eastern Afghanistan and northern Pakistan ) , to come to him and submit to his authority .", "prompt_labels": "He(O) invited(O) the(O) chieftains(O) of(O) the(O) former(O) satrapy(O) of(O) Gandhara(B-country) ((O) a(O) region(O) presently(O) straddling(O) eastern(O) Afghanistan(B-country) and(O) northern(O) Pakistan(B-country) )(O) ,(O) to(O) come(O) to(O) him(O) and(O) submit(O) to(O) his(O) authority(O) .(O)"}}
{"id": "275", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "event", "political party", "person", "organization", "country", "election", "location"], "instance": {"id": "275", "words": ["After", "his", "junior", "year", "at", "Harvard", "College", ",", "he", "spent", "three", "years", "studying", "Japanese", "at", "the", "International", "Christian", "University", "in", "Tokyo", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, event, political party, person, organization, country, election, location and O.\nSentence: After his junior year at Harvard College , he spent three years studying Japanese at the International Christian University in Tokyo .", "prompt_labels": "After(O) his(O) junior(O) year(O) at(O) Harvard(B-organization) College(I-organization) ,(O) he(O) spent(O) three(O) years(O) studying(O) Japanese(O) at(O) the(O) International(B-organization) Christian(I-organization) University(I-organization) in(O) Tokyo(B-location) .(O)"}}
{"id": "508", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "person", "country", "political party", "politician", "election", "organization", "location"], "instance": {"id": "508", "words": ["Braun", "(", "whose", "victory", "coincided", "with", "Bill", "Clinton", "'", "s", "win", "in", "the", "1992", "United", "States", "presidential", "election", "and", "1992", "United", "States", "presidential", "election", "in", "Illinois", ")", "made", "history", "in", "this", "election", "by", "becoming", "the", "first", "African-American", "woman", "ever", "elected", "to", "the", "U.S", "Senate", ",", "and", "also", "the", "first", "African-American", "elected", "to", "the", "U.S", "Senate", "as", "a", "Democrat", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, country, political party, politician, election, organization, location and O.\nSentence: Braun ( whose victory coincided with Bill Clinton ' s win in the 1992 United States presidential election and 1992 United States presidential election in Illinois ) made history in this election by becoming the first African-American woman ever elected to the U.S Senate , and also the first African-American elected to the U.S Senate as a Democrat .", "prompt_labels": "Braun(B-politician) ((O) whose(O) victory(O) coincided(O) with(O) Bill(B-politician) Clinton(I-politician) '(O) s(O) win(O) in(O) the(O) 1992(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) and(O) 1992(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) in(I-election) Illinois(I-election) )(O) made(O) history(O) in(O) this(O) election(O) by(O) becoming(O) the(O) first(O) African-American(O) woman(O) ever(O) elected(O) to(O) the(O) U.S(B-organization) Senate(I-organization) ,(O) and(O) also(O) the(O) first(O) African-American(O) elected(O) to(O) the(O) U.S(B-organization) Senate(I-organization) as(O) a(O) Democrat(O) .(O)"}}
{"id": "217", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "country", "election", "location", "politician", "person", "event", "political party"], "instance": {"id": "217", "words": ["The", "NDP", "focused", "the", "campaign", "on", "winning", "ridings", "in", "Canada", "'s", "urban", "centres", ",", "hoping", "especially", "to", "win", "seats", "in", "central", "Toronto", ",", "Hamilton", ",", "Ottawa", "and", "Winnipeg", "."], "labels": ["O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, election, location, politician, person, event, political party and O.\nSentence: The NDP focused the campaign on winning ridings in Canada 's urban centres , hoping especially to win seats in central Toronto , Hamilton , Ottawa and Winnipeg .", "prompt_labels": "The(O) NDP(B-political party) focused(O) the(O) campaign(O) on(O) winning(O) ridings(O) in(O) Canada(B-country) 's(O) urban(O) centres(O) ,(O) hoping(O) especially(O) to(O) win(O) seats(O) in(O) central(O) Toronto(B-location) ,(O) Hamilton(B-location) ,(O) Ottawa(B-location) and(O) Winnipeg(B-location) .(O)"}}
{"id": "245", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "country", "person", "organization", "event", "election", "politician", "political party"], "instance": {"id": "245", "words": ["The", "regional", "office", "is", "located", "in", "Edmonton", "with", "three", "district", "offices", "located", "in", "Winnipeg", ",", "Regina", "and", "Calgary", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, person, organization, event, election, politician, political party and O.\nSentence: The regional office is located in Edmonton with three district offices located in Winnipeg , Regina and Calgary .", "prompt_labels": "The(O) regional(O) office(O) is(O) located(O) in(O) Edmonton(B-location) with(O) three(O) district(O) offices(O) located(O) in(O) Winnipeg(B-location) ,(O) Regina(B-location) and(O) Calgary(B-location) .(O)"}}
{"id": "333", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "organization", "event", "political party", "politician", "person", "country", "location"], "instance": {"id": "333", "words": ["The", "act", "led", "to", "a", "significant", "increase", "in", "the", "number", "of", "MEPs", "being", "returned", "from", "minor", "parties", "in", "the", "1999", "European", "Parliament", "election", "in", "the", "United", "Kingdom", ",", "with", "more", "Liberal", "Democrats", ",", "along", "with", "the", "first", "European", "representatives", "for", "Plaid", "Cymru", "and", "the", "first", "national", "representatives", "for", "both", "the", "Green", "Party", "of", "England", "and", "Wales", "and", "the", "United", "Kingdom", "Independence", "Party", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, event, political party, politician, person, country, location and O.\nSentence: The act led to a significant increase in the number of MEPs being returned from minor parties in the 1999 European Parliament election in the United Kingdom , with more Liberal Democrats , along with the first European representatives for Plaid Cymru and the first national representatives for both the Green Party of England and Wales and the United Kingdom Independence Party .", "prompt_labels": "The(O) act(O) led(O) to(O) a(O) significant(O) increase(O) in(O) the(O) number(O) of(O) MEPs(O) being(O) returned(O) from(O) minor(O) parties(O) in(O) the(O) 1999(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) the(I-election) United(I-election) Kingdom(I-election) ,(O) with(O) more(O) Liberal(B-political party) Democrats(I-political party) ,(O) along(O) with(O) the(O) first(O) European(O) representatives(O) for(O) Plaid(B-political party) Cymru(I-political party) and(O) the(O) first(O) national(O) representatives(O) for(O) both(O) the(O) Green(B-political party) Party(I-political party) of(I-political party) England(I-political party) and(I-political party) Wales(I-political party) and(O) the(O) United(B-political party) Kingdom(I-political party) Independence(I-political party) Party(I-political party) .(O)"}}
{"id": "647", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "political party", "politician", "event", "person", "country", "organization", "election"], "instance": {"id": "647", "words": ["The", "2019", "North", "Korea-United", "States", "Hanoi", "Summit", ",", "commonly", "known", "as", "the", "Hanoi", "Summit", ",", "was", "a", "two-day", "summit", "meeting", "between", "North", "Korea", "n", "Chairman", "Kim", "Jong-un", "and", "United", "States", "President", "Donald", "Trump", ",", "held", "at", "the", "Metropole", "Hotel", "in", "Hanoi", ",", "Vietnam", ",", "on", "February", "27-28", ",", "2019", "."], "labels": ["O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "B-politician", "I-politician", "O", "B-country", "I-country", "O", "B-politician", "I-politician", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, political party, politician, event, person, country, organization, election and O.\nSentence: The 2019 North Korea-United States Hanoi Summit , commonly known as the Hanoi Summit , was a two-day summit meeting between North Korea n Chairman Kim Jong-un and United States President Donald Trump , held at the Metropole Hotel in Hanoi , Vietnam , on February 27-28 , 2019 .", "prompt_labels": "The(O) 2019(B-event) North(I-event) Korea-United(I-event) States(I-event) Hanoi(I-event) Summit(I-event) ,(O) commonly(O) known(O) as(O) the(O) Hanoi(B-event) Summit(I-event) ,(O) was(O) a(O) two-day(O) summit(O) meeting(O) between(O) North(B-country) Korea(I-country) n(O) Chairman(O) Kim(B-politician) Jong-un(I-politician) and(O) United(B-country) States(I-country) President(O) Donald(B-politician) Trump(I-politician) ,(O) held(O) at(O) the(O) Metropole(B-location) Hotel(I-location) in(O) Hanoi(B-location) ,(O) Vietnam(B-location) ,(O) on(O) February(O) 27-28(O) ,(O) 2019(O) .(O)"}}
{"id": "633", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "person", "election", "country", "location", "organization", "political party", "politician"], "instance": {"id": "633", "words": ["Furthermore", ",", "Nigel", "Farage", "and", "long-term", "Eurosceptic", "party", "UK", "Independence", "Party", "used", "images", "from", "the", "refugee", "crisis", "during", "their", "campaign", "to", "increase", "the", "anxiety", "about", "immigration", "that", "the", "crisis", "caused", ",", "prompting", "criticism", "from", "some", "Leave", "and", "Remain", "supporters", "."], "labels": ["O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, election, country, location, organization, political party, politician and O.\nSentence: Furthermore , Nigel Farage and long-term Eurosceptic party UK Independence Party used images from the refugee crisis during their campaign to increase the anxiety about immigration that the crisis caused , prompting criticism from some Leave and Remain supporters .", "prompt_labels": "Furthermore(O) ,(O) Nigel(B-politician) Farage(I-politician) and(O) long-term(O) Eurosceptic(O) party(O) UK(B-political party) Independence(I-political party) Party(I-political party) used(O) images(O) from(O) the(O) refugee(O) crisis(O) during(O) their(O) campaign(O) to(O) increase(O) the(O) anxiety(O) about(O) immigration(O) that(O) the(O) crisis(O) caused(O) ,(O) prompting(O) criticism(O) from(O) some(O) Leave(O) and(O) Remain(O) supporters(O) .(O)"}}
{"id": "618", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "country", "political party", "organization", "location", "election", "person", "politician"], "instance": {"id": "618", "words": ["Under", "Lal", "Bahadur", "Shastri", ",", "the", "government", "'s", "popularity", "was", "boosted", "after", "India", "prevailed", "in", "the", "1965", "War", "with", "Pakistan", ",", "but", "this", "war", "(", "along", "with", "the", "previous", "1962", "War", "with", "China", ")", "had", "helped", "put", "a", "strain", "on", "the", "economy", "."], "labels": ["O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, political party, organization, location, election, person, politician and O.\nSentence: Under Lal Bahadur Shastri , the government 's popularity was boosted after India prevailed in the 1965 War with Pakistan , but this war ( along with the previous 1962 War with China ) had helped put a strain on the economy .", "prompt_labels": "Under(O) Lal(B-politician) Bahadur(I-politician) Shastri(I-politician) ,(O) the(O) government(O) 's(O) popularity(O) was(O) boosted(O) after(O) India(O) prevailed(O) in(O) the(O) 1965(B-event) War(I-event) with(I-event) Pakistan(I-event) ,(O) but(O) this(O) war(O) ((O) along(O) with(O) the(O) previous(O) 1962(B-event) War(I-event) with(I-event) China(I-event) )(O) had(O) helped(O) put(O) a(O) strain(O) on(O) the(O) economy(O) .(O)"}}
{"id": "532", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "country", "person", "political party", "politician", "location", "election", "event"], "instance": {"id": "532", "words": ["Panel", "guests", "have", "included", "comedians", "Sarah", "Silverman", ",", "Brett", "Gelman", ",", "Ira", "Madison", "III", "and", "Paul", "Scheer", ",", "screenwriter", "Wil", "Wheaton", "and", "journalists", "Lauren", "Duca", "and", "Katie", "Nolan", "."], "labels": ["O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, person, political party, politician, location, election, event and O.\nSentence: Panel guests have included comedians Sarah Silverman , Brett Gelman , Ira Madison III and Paul Scheer , screenwriter Wil Wheaton and journalists Lauren Duca and Katie Nolan .", "prompt_labels": "Panel(O) guests(O) have(O) included(O) comedians(O) Sarah(B-person) Silverman(I-person) ,(O) Brett(B-person) Gelman(I-person) ,(O) Ira(B-person) Madison(I-person) III(I-person) and(O) Paul(B-person) Scheer(I-person) ,(O) screenwriter(O) Wil(B-person) Wheaton(I-person) and(O) journalists(O) Lauren(B-person) Duca(I-person) and(O) Katie(B-person) Nolan(I-person) .(O)"}}
{"id": "383", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "organization", "person", "politician", "political party", "location", "event", "country"], "instance": {"id": "383", "words": ["At", "the", "The", "New", "Zealand", "National", "Party", "won", "the", "most", "seats", "and", "formed", "a", "minority", "government", "with", "the", "support", "of", "ACT", ",", "the", "Māori", "Party", "and", "United", "Future", "."], "labels": ["O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, person, politician, political party, location, event, country and O.\nSentence: At the The New Zealand National Party won the most seats and formed a minority government with the support of ACT , the Māori Party and United Future .", "prompt_labels": "At(O) the(O) The(O) New(B-political party) Zealand(I-political party) National(I-political party) Party(I-political party) won(O) the(O) most(O) seats(O) and(O) formed(O) a(O) minority(O) government(O) with(O) the(O) support(O) of(O) ACT(B-political party) ,(O) the(O) Māori(B-political party) Party(I-political party) and(O) United(B-political party) Future(I-political party) .(O)"}}
{"id": "173", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "event", "organization", "election", "politician", "country", "person", "location"], "instance": {"id": "173", "words": ["Six", "offices", "were", "set", "up", ":", "in", "Chang", "'an", ",", "Luoyang", ",", "Handan", ",", "Linzi", "(", "modern", "Zibo", ",", "Shandong", ")", ",", "Wancheng", "(", "modern", "Nanyang", ",", "Henan", ")", ",", "and", "Chengdu", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "O", "B-location", "O", "B-location", "O", "O", "B-location", "O", "O", "B-location", "O", "B-location", "O", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, organization, election, politician, country, person, location and O.\nSentence: Six offices were set up : in Chang 'an , Luoyang , Handan , Linzi ( modern Zibo , Shandong ) , Wancheng ( modern Nanyang , Henan ) , and Chengdu .", "prompt_labels": "Six(O) offices(O) were(O) set(O) up(O) :(O) in(O) Chang(B-location) 'an(I-location) ,(O) Luoyang(B-location) ,(O) Handan(B-location) ,(O) Linzi(B-location) ((O) modern(O) Zibo(B-location) ,(O) Shandong(B-location) )(O) ,(O) Wancheng(B-location) ((O) modern(O) Nanyang(B-location) ,(O) Henan(B-location) )(O) ,(O) and(O) Chengdu(B-location) .(O)"}}
{"id": "296", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "location", "country", "event", "political party", "organization", "politician", "election"], "instance": {"id": "296", "words": ["The", "strictness", "of", "these", "qualifications", "led", "to", "the", "1999", "Singaporean", "presidential", "election", ",", "2005", "Singaporean", "presidential", "election", ",", "and", "2017", "Singaporean", "presidential", "election", "being", "walkovers", "as", "only", "one", "candidate", "had", "qualified", "on", "nomination", "day", ".."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, country, event, political party, organization, politician, election and O.\nSentence: The strictness of these qualifications led to the 1999 Singaporean presidential election , 2005 Singaporean presidential election , and 2017 Singaporean presidential election being walkovers as only one candidate had qualified on nomination day ..", "prompt_labels": "The(O) strictness(O) of(O) these(O) qualifications(O) led(O) to(O) the(O) 1999(B-election) Singaporean(I-election) presidential(I-election) election(I-election) ,(O) 2005(B-election) Singaporean(I-election) presidential(I-election) election(I-election) ,(O) and(O) 2017(B-election) Singaporean(I-election) presidential(I-election) election(I-election) being(O) walkovers(O) as(O) only(O) one(O) candidate(O) had(O) qualified(O) on(O) nomination(O) day(O) ..(O)"}}
{"id": "629", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "political party", "event", "person", "election", "politician", "country", "location"], "instance": {"id": "629", "words": ["In", "August", "1914", ",", "World", "War", "I", "began", "when", "alliance", "obligations", "arising", "from", "the", "war", "between", "Serbia", "and", "Austria-Hungary", "brought", "Germany", "and", "Russia", "to", "war", ",", "while", "Germany", "'s", "invasion", "of", "Belgium", "directly", "triggered", "Britain", "'s", "entry", "."], "labels": ["O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "B-country", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, event, person, election, politician, country, location and O.\nSentence: In August 1914 , World War I began when alliance obligations arising from the war between Serbia and Austria-Hungary brought Germany and Russia to war , while Germany 's invasion of Belgium directly triggered Britain 's entry .", "prompt_labels": "In(O) August(O) 1914(O) ,(O) World(B-event) War(I-event) I(I-event) began(O) when(O) alliance(O) obligations(O) arising(O) from(O) the(O) war(O) between(O) Serbia(B-country) and(O) Austria-Hungary(B-country) brought(O) Germany(B-country) and(O) Russia(B-country) to(O) war(O) ,(O) while(O) Germany(B-event) 's(I-event) invasion(I-event) of(I-event) Belgium(I-event) directly(O) triggered(O) Britain(B-country) 's(O) entry(O) .(O)"}}
{"id": "14", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "country", "organization", "political party", "location", "event", "election", "person"], "instance": {"id": "14", "words": ["The", "provinces", "ceded", "to", "Augustus", "for", "that", "ten-year", "period", "comprised", "much", "of", "the", "conquered", "Roman", "world", ",", "including", "all", "of", "Hispania", "and", "Gaul", ",", "Syria", ",", "Cilicia", ",", "Cyprus", ",", "and", "Egypt", "."], "labels": ["O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-country", "O", "B-location", "O", "B-country", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, organization, political party, location, event, election, person and O.\nSentence: The provinces ceded to Augustus for that ten-year period comprised much of the conquered Roman world , including all of Hispania and Gaul , Syria , Cilicia , Cyprus , and Egypt .", "prompt_labels": "The(O) provinces(O) ceded(O) to(O) Augustus(B-person) for(O) that(O) ten-year(O) period(O) comprised(O) much(O) of(O) the(O) conquered(O) Roman(O) world(O) ,(O) including(O) all(O) of(O) Hispania(B-location) and(O) Gaul(B-location) ,(O) Syria(B-country) ,(O) Cilicia(B-location) ,(O) Cyprus(B-country) ,(O) and(O) Egypt(B-country) .(O)"}}
{"id": "352", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "election", "political party", "person", "politician", "event", "location", "organization"], "instance": {"id": "352", "words": ["The", "Opposition", "Australian", "Labor", "Party", "announced", "they", "would", "not", "support", "the", "bill", ";", "nor", "would", "the", "Australian", "Greens", ",", "Australian", "Democrats", "or", "independent", "Senator", "Brian", "Harradine", "."], "labels": ["O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "O", "O", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, political party, person, politician, event, location, organization and O.\nSentence: The Opposition Australian Labor Party announced they would not support the bill ; nor would the Australian Greens , Australian Democrats or independent Senator Brian Harradine .", "prompt_labels": "The(O) Opposition(O) Australian(B-political party) Labor(I-political party) Party(I-political party) announced(O) they(O) would(O) not(O) support(O) the(O) bill(O) ;(O) nor(O) would(O) the(O) Australian(B-political party) Greens(I-political party) ,(O) Australian(B-political party) Democrats(I-political party) or(O) independent(O) Senator(O) Brian(B-politician) Harradine(I-politician) .(O)"}}
{"id": "43", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "location", "person", "organization", "event", "politician", "country", "political party"], "instance": {"id": "43", "words": ["The", "region", "of", "Assyria", "fell", "under", "the", "successive", "control", "of", "the", "Median", "Empire", "of", "678", "to", "549", "BC", ",", "the", "Achaemenid", "Empire", "of", "550", "to", "330", "BC", ",", "the", "Macedonian", "Empire", "(", "late", "4th", "century", "BC", ")", ",", "the", "Seleucid", "Empire", "of", "312", "to", "63", "BC", ",", "the", "Parthian", "Empire", "of", "247", "BC", "to", "224", "AD", ",", "the", "Roman", "Empire", "(", "from", "116", "to", "118", "AD", ")", "and", "the", "Sasanian", "Empire", "of", "224", "to", "651", "AD", "."], "labels": ["O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, location, person, organization, event, politician, country, political party and O.\nSentence: The region of Assyria fell under the successive control of the Median Empire of 678 to 549 BC , the Achaemenid Empire of 550 to 330 BC , the Macedonian Empire ( late 4th century BC ) , the Seleucid Empire of 312 to 63 BC , the Parthian Empire of 247 BC to 224 AD , the Roman Empire ( from 116 to 118 AD ) and the Sasanian Empire of 224 to 651 AD .", "prompt_labels": "The(O) region(O) of(O) Assyria(B-country) fell(O) under(O) the(O) successive(O) control(O) of(O) the(O) Median(B-country) Empire(I-country) of(O) 678(O) to(O) 549(O) BC(O) ,(O) the(O) Achaemenid(B-country) Empire(I-country) of(O) 550(O) to(O) 330(O) BC(O) ,(O) the(O) Macedonian(B-country) Empire(I-country) ((O) late(O) 4th(O) century(O) BC(O) )(O) ,(O) the(O) Seleucid(B-country) Empire(I-country) of(O) 312(O) to(O) 63(O) BC(O) ,(O) the(O) Parthian(B-country) Empire(I-country) of(O) 247(O) BC(O) to(O) 224(O) AD(O) ,(O) the(O) Roman(B-country) Empire(I-country) ((O) from(O) 116(O) to(O) 118(O) AD(O) )(O) and(O) the(O) Sasanian(B-country) Empire(I-country) of(O) 224(O) to(O) 651(O) AD(O) .(O)"}}
{"id": "583", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "person", "country", "election", "organization", "location", "event", "political party"], "instance": {"id": "583", "words": ["Röttgen", ",", "in", "his", "capacity", "as", "environment", "minister", ",", "led", "the", "German", "delegations", "to", "the", "2009", "United", "Nations", "Climate", "Change", "Conference", "in", "Copenhagen", ",", "the", "2010", "United", "Nations", "Climate", "Change", "Conference", "in", "Cancún", "and", "the", "2011", "United", "Nations", "Climate", "Change", "Conference", "in", "Durban", ",", "respectively", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "B-location", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "B-location", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "B-location", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, country, election, organization, location, event, political party and O.\nSentence: Röttgen , in his capacity as environment minister , led the German delegations to the 2009 United Nations Climate Change Conference in Copenhagen , the 2010 United Nations Climate Change Conference in Cancún and the 2011 United Nations Climate Change Conference in Durban , respectively .", "prompt_labels": "Röttgen(B-politician) ,(O) in(O) his(O) capacity(O) as(O) environment(O) minister(O) ,(O) led(O) the(O) German(O) delegations(O) to(O) the(O) 2009(B-event) United(I-event) Nations(I-event) Climate(I-event) Change(I-event) Conference(I-event) in(O) Copenhagen(B-location) ,(O) the(O) 2010(B-event) United(I-event) Nations(I-event) Climate(I-event) Change(I-event) Conference(I-event) in(O) Cancún(B-location) and(O) the(O) 2011(B-event) United(I-event) Nations(I-event) Climate(I-event) Change(I-event) Conference(I-event) in(O) Durban(B-location) ,(O) respectively(O) .(O)"}}
{"id": "379", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "country", "election", "person", "location", "politician", "event", "organization"], "instance": {"id": "379", "words": ["he", "was", "returned", "as", "MP", "for", "Reigate", "where", "he", "was", "returned", "again", "in", "1727", "British", "general", "election", "and", "1734", "British", "general", "election", ".", "He", "sponsored", "the", "Mortmain", "Act", "and", "the", "Gin", "Act", "1736", ",", "and", "was", "noted", "for", "his", "opposition", "to", "intoxication", ",", "which", "annoyed", "the", "public", "so", "much", "that", "he", "was", "forced", "to", "have", "a", "guard", "at", "his", "house", "at", "all", "times", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, country, election, person, location, politician, event, organization and O.\nSentence: he was returned as MP for Reigate where he was returned again in 1727 British general election and 1734 British general election . He sponsored the Mortmain Act and the Gin Act 1736 , and was noted for his opposition to intoxication , which annoyed the public so much that he was forced to have a guard at his house at all times .", "prompt_labels": "he(O) was(O) returned(O) as(O) MP(O) for(O) Reigate(B-location) where(O) he(O) was(O) returned(O) again(O) in(O) 1727(B-election) British(I-election) general(I-election) election(I-election) and(O) 1734(B-election) British(I-election) general(I-election) election(I-election) .(O) He(O) sponsored(O) the(O) Mortmain(O) Act(O) and(O) the(O) Gin(O) Act(O) 1736(O) ,(O) and(O) was(O) noted(O) for(O) his(O) opposition(O) to(O) intoxication(O) ,(O) which(O) annoyed(O) the(O) public(O) so(O) much(O) that(O) he(O) was(O) forced(O) to(O) have(O) a(O) guard(O) at(O) his(O) house(O) at(O) all(O) times(O) .(O)"}}
{"id": "117", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "organization", "politician", "election", "location", "person", "country", "political party"], "instance": {"id": "117", "words": ["He", "studied", "law", "and", "political", "science", "at", "the", "universities", "of", "Goethe", "University", "Frankfurt", ",", "University", "of", "Burgundy", ",", "Ludwig", "Maximilian", "University", "of", "Munich", ",", "University", "of", "Königsberg", ",", "and", "University", "of", "Hamburg", "from", "1933", "to", "1936", ",", "gaining", "a", "doctorate", "in", "1938", "and", "taking", "the", "Second", "Staatsexamen", "degree", "in", "1939", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, politician, election, location, person, country, political party and O.\nSentence: He studied law and political science at the universities of Goethe University Frankfurt , University of Burgundy , Ludwig Maximilian University of Munich , University of Königsberg , and University of Hamburg from 1933 to 1936 , gaining a doctorate in 1938 and taking the Second Staatsexamen degree in 1939 .", "prompt_labels": "He(O) studied(O) law(O) and(O) political(O) science(O) at(O) the(O) universities(O) of(O) Goethe(B-organization) University(I-organization) Frankfurt(I-organization) ,(O) University(B-organization) of(I-organization) Burgundy(I-organization) ,(O) Ludwig(B-organization) Maximilian(I-organization) University(I-organization) of(I-organization) Munich(I-organization) ,(O) University(B-organization) of(I-organization) Königsberg(I-organization) ,(O) and(O) University(B-organization) of(I-organization) Hamburg(I-organization) from(O) 1933(O) to(O) 1936(O) ,(O) gaining(O) a(O) doctorate(O) in(O) 1938(O) and(O) taking(O) the(O) Second(O) Staatsexamen(O) degree(O) in(O) 1939(O) .(O)"}}
{"id": "632", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "location", "political party", "election", "organization", "country", "politician", "person"], "instance": {"id": "632", "words": ["It", "later", "reappeared", "during", "EU", "membership", "referendum", "campaigning", "ahead", "of", "the", "2016", "UK", "referendum", "on", "EU", "membership", ",", "being", "used", "to", "criticise", "the", "campaign", "being", "run", "by", "Britain", "Stronger", "in", "Europe", ",", "supporters", "of", "the", "UK", "remaining", "in", "the", "European", "Union", "."], "labels": ["O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-country", "O", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, political party, election, organization, country, politician, person and O.\nSentence: It later reappeared during EU membership referendum campaigning ahead of the 2016 UK referendum on EU membership , being used to criticise the campaign being run by Britain Stronger in Europe , supporters of the UK remaining in the European Union .", "prompt_labels": "It(O) later(O) reappeared(O) during(O) EU(B-event) membership(I-event) referendum(I-event) campaigning(I-event) ahead(O) of(O) the(O) 2016(B-event) UK(I-event) referendum(I-event) on(I-event) EU(I-event) membership(I-event) ,(O) being(O) used(O) to(O) criticise(O) the(O) campaign(O) being(O) run(O) by(O) Britain(B-organization) Stronger(I-organization) in(I-organization) Europe(I-organization) ,(O) supporters(O) of(O) the(O) UK(B-country) remaining(O) in(O) the(O) European(B-organization) Union(I-organization) .(O)"}}
{"id": "340", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "politician", "event", "person", "political party", "country", "location", "election"], "instance": {"id": "340", "words": ["In", "the", "2004", "Canadian", "federal", "election", ",", "the", "Conservative", "Party", "of", "Canada", "selected", "Adam", "Richardson", ",", "who", "had", "run", "for", "the", "Canadian", "Alliance", "in", "the", "2000", "Canadian", "federal", "election", ",", "but", "national", "head", "office", "refused", "to", "sign", "his", "nomination", "papers", ",", "apparently", "because", "of", "Richardson", "'s", "demands", "that", "Stephen", "Harper", "apologize", "for", "comments", "about", "Atlantic", "Canadians", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, event, person, political party, country, location, election and O.\nSentence: In the 2004 Canadian federal election , the Conservative Party of Canada selected Adam Richardson , who had run for the Canadian Alliance in the 2000 Canadian federal election , but national head office refused to sign his nomination papers , apparently because of Richardson 's demands that Stephen Harper apologize for comments about Atlantic Canadians .", "prompt_labels": "In(O) the(O) 2004(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) the(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) selected(O) Adam(B-politician) Richardson(I-politician) ,(O) who(O) had(O) run(O) for(O) the(O) Canadian(B-political party) Alliance(I-political party) in(O) the(O) 2000(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) but(O) national(O) head(O) office(O) refused(O) to(O) sign(O) his(O) nomination(O) papers(O) ,(O) apparently(O) because(O) of(O) Richardson(B-politician) 's(O) demands(O) that(O) Stephen(B-politician) Harper(I-politician) apologize(O) for(O) comments(O) about(O) Atlantic(O) Canadians(O) .(O)"}}
{"id": "236", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "country", "person", "politician", "location", "political party", "election", "organization"], "instance": {"id": "236", "words": ["Gazprom", "is", "listed", "on", "the", "stock", "markets", "of", "Moscow", ",", "London", ",", "Karachi", ",", "Berlin", ",", "Frankfurt", "and", "Singapore", "."], "labels": ["B-country", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, person, politician, location, political party, election, organization and O.\nSentence: Gazprom is listed on the stock markets of Moscow , London , Karachi , Berlin , Frankfurt and Singapore .", "prompt_labels": "Gazprom(B-country) is(O) listed(O) on(O) the(O) stock(O) markets(O) of(O) Moscow(B-location) ,(O) London(B-location) ,(O) Karachi(B-location) ,(O) Berlin(B-location) ,(O) Frankfurt(B-location) and(O) Singapore(B-country) .(O)"}}
{"id": "279", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "event", "political party", "person", "organization", "politician", "location", "country"], "instance": {"id": "279", "words": ["On", "24", "February", "2016", ",", "the", "DPJ", "announced", "an", "agreement", "to", "merge", "with", "the", "smaller", "Japan", "Innovation", "Party", "(", "JIP", ")", "and", "Vision", "of", "Reform", "ahead", "of", "the", "2016", "Japanese", "House", "of", "Councillors", "election", "in", "the", "summer", ","], "labels": ["O", "O", "O", "O", "O", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, political party, person, organization, politician, location, country and O.\nSentence: On 24 February 2016 , the DPJ announced an agreement to merge with the smaller Japan Innovation Party ( JIP ) and Vision of Reform ahead of the 2016 Japanese House of Councillors election in the summer ,", "prompt_labels": "On(O) 24(O) February(O) 2016(O) ,(O) the(O) DPJ(B-political party) announced(O) an(O) agreement(O) to(O) merge(O) with(O) the(O) smaller(O) Japan(B-political party) Innovation(I-political party) Party(I-political party) ((O) JIP(B-political party) )(O) and(O) Vision(B-political party) of(I-political party) Reform(I-political party) ahead(O) of(O) the(O) 2016(B-election) Japanese(I-election) House(I-election) of(I-election) Councillors(I-election) election(I-election) in(O) the(O) summer(O) ,(O)"}}
{"id": "243", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "organization", "person", "country", "event", "political party", "location", "election"], "instance": {"id": "243", "words": ["During", "campaigning", "for", "the", "1978", "French", "legislative", "election", ",", "in", "his", "Verdun-sur-le-Doubs", "speech", ",", "President", "Giscard", "d", "'Estaing", "noted", "that", "the", "political", "leanings", "of", "the", "French", "people", "were", "divided", "among", "four", "groups", ":", "the", "Communists", "(", "French", "Communist", "Party", ")", ",", "the", "Socialists", "(", "PS", ")", ",", "the", "Neo-Gaullists", "(", "Rally", "for", "the", "Republic", ")", "and", "his", "own", "followers", "."], "labels": ["O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "B-location", "O", "O", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, person, country, event, political party, location, election and O.\nSentence: During campaigning for the 1978 French legislative election , in his Verdun-sur-le-Doubs speech , President Giscard d 'Estaing noted that the political leanings of the French people were divided among four groups : the Communists ( French Communist Party ) , the Socialists ( PS ) , the Neo-Gaullists ( Rally for the Republic ) and his own followers .", "prompt_labels": "During(O) campaigning(O) for(O) the(O) 1978(B-election) French(I-election) legislative(I-election) election(I-election) ,(O) in(O) his(O) Verdun-sur-le-Doubs(B-location) speech(O) ,(O) President(O) Giscard(B-politician) d(I-politician) 'Estaing(I-politician) noted(O) that(O) the(O) political(O) leanings(O) of(O) the(O) French(O) people(O) were(O) divided(O) among(O) four(O) groups(O) :(O) the(O) Communists(O) ((O) French(B-political party) Communist(I-political party) Party(I-political party) )(O) ,(O) the(O) Socialists(O) ((O) PS(O) )(O) ,(O) the(O) Neo-Gaullists(O) ((O) Rally(B-political party) for(I-political party) the(I-political party) Republic(I-political party) )(O) and(O) his(O) own(O) followers(O) .(O)"}}
{"id": "459", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "person", "location", "political party", "organization", "election", "event", "country"], "instance": {"id": "459", "words": ["Edgar", "was", "recognized", "by", "several", "national", "organizations", "for", "his", "work", ",", "including", "by", "the", "American", "Legion", ",", "Vietnam", "Veterans", "of", "America", "and", "the", "National", "Taxpayers", "Union", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, location, political party, organization, election, event, country and O.\nSentence: Edgar was recognized by several national organizations for his work , including by the American Legion , Vietnam Veterans of America and the National Taxpayers Union .", "prompt_labels": "Edgar(B-person) was(O) recognized(O) by(O) several(O) national(O) organizations(O) for(O) his(O) work(O) ,(O) including(O) by(O) the(O) American(B-organization) Legion(I-organization) ,(O) Vietnam(B-organization) Veterans(I-organization) of(I-organization) America(I-organization) and(O) the(O) National(B-organization) Taxpayers(I-organization) Union(I-organization) .(O)"}}
{"id": "285", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "organization", "election", "event", "politician", "person", "political party", "location"], "instance": {"id": "285", "words": ["It", "did", "not", "stand", "candidates", "in", "the", "Māori", "electorates", "in", "the", "2002", "New", "Zealand", "general", "election", ",", "2005", "New", "Zealand", "general", "election", ",", "or", "2008", "New", "Zealand", "general", "elections", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, election, event, politician, person, political party, location and O.\nSentence: It did not stand candidates in the Māori electorates in the 2002 New Zealand general election , 2005 New Zealand general election , or 2008 New Zealand general elections .", "prompt_labels": "It(O) did(O) not(O) stand(O) candidates(O) in(O) the(O) Māori(O) electorates(O) in(O) the(O) 2002(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ,(O) 2005(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ,(O) or(O) 2008(B-election) New(I-election) Zealand(I-election) general(I-election) elections(I-election) .(O)"}}
{"id": "219", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "event", "organization", "country", "location", "election", "politician", "person"], "instance": {"id": "219", "words": ["In", "addition", ",", "students", "from", "Santa", "Ana", "College", ",", "Santiago", "Canyon", "College", ",", "Fullerton", "College", ",", "and", "Golden", "West", "College", "can", "local", "fixed", "route", "buses", "for", "free", "using", "their", "student", "ID", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, organization, country, location, election, politician, person and O.\nSentence: In addition , students from Santa Ana College , Santiago Canyon College , Fullerton College , and Golden West College can local fixed route buses for free using their student ID .", "prompt_labels": "In(O) addition(O) ,(O) students(O) from(O) Santa(B-organization) Ana(I-organization) College(I-organization) ,(O) Santiago(B-organization) Canyon(I-organization) College(I-organization) ,(O) Fullerton(B-organization) College(I-organization) ,(O) and(O) Golden(B-organization) West(I-organization) College(I-organization) can(O) local(O) fixed(O) route(O) buses(O) for(O) free(O) using(O) their(O) student(O) ID(O) .(O)"}}
{"id": "582", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "politician", "organization", "person", "event", "political party", "location", "country"], "instance": {"id": "582", "words": ["The", "campaign", "'s", "sustained", "role", "in", "Russian", "culture", "may", "be", "seen", "in", "Tolstoy", "'", "s", "War", "and", "Peace", ",", "Tchaikovsky", "'", "s", "1812", "Overture", ",", "and", "the", "identification", "of", "it", "with", "the", "German", "invasion", "of", "1941-45", ",", "which", "became", "known", "as", "the", "Great", "Patriotic", "War", "in", "the", "Soviet", "Union", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, organization, person, event, political party, location, country and O.\nSentence: The campaign 's sustained role in Russian culture may be seen in Tolstoy ' s War and Peace , Tchaikovsky ' s 1812 Overture , and the identification of it with the German invasion of 1941-45 , which became known as the Great Patriotic War in the Soviet Union .", "prompt_labels": "The(O) campaign(O) 's(O) sustained(O) role(O) in(O) Russian(O) culture(O) may(O) be(O) seen(O) in(O) Tolstoy(B-person) '(O) s(O) War(O) and(O) Peace(O) ,(O) Tchaikovsky(O) '(O) s(O) 1812(O) Overture(O) ,(O) and(O) the(O) identification(O) of(O) it(O) with(O) the(O) German(B-event) invasion(I-event) of(I-event) 1941-45(I-event) ,(O) which(O) became(O) known(O) as(O) the(O) Great(B-event) Patriotic(I-event) War(I-event) in(O) the(O) Soviet(B-country) Union(I-country) .(O)"}}
{"id": "146", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "person", "politician", "event", "location", "election", "political party", "organization"], "instance": {"id": "146", "words": ["At", "the", "four", "subsequent", "UK", "elections", "(", "2001", "United", "Kingdom", "general", "election", ",", "2005", "United", "Kingdom", "general", "election", ",", "2010", "United", "Kingdom", "general", "election", "and", "2015", "United", "Kingdom", "general", "election", ")", "the", "Conservatives", "won", "only", "one", "Scottish", "seat", "."], "labels": ["O", "O", "O", "O", "B-country", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, politician, event, location, election, political party, organization and O.\nSentence: At the four subsequent UK elections ( 2001 United Kingdom general election , 2005 United Kingdom general election , 2010 United Kingdom general election and 2015 United Kingdom general election ) the Conservatives won only one Scottish seat .", "prompt_labels": "At(O) the(O) four(O) subsequent(O) UK(B-country) elections(O) ((O) 2001(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) 2005(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) 2010(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 2015(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) )(O) the(O) Conservatives(O) won(O) only(O) one(O) Scottish(O) seat(O) .(O)"}}
{"id": "407", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "election", "person", "event", "location", "political party", "organization", "politician"], "instance": {"id": "407", "words": ["He", "stood", "for", "election", "to", "Parliament", "at", "the", "1987", "United", "Kingdom", "general", "election", ",", "the", "1992", "United", "Kingdom", "general", "election", "and", "1997", "United", "Kingdom", "general", "election", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, person, event, location, political party, organization, politician and O.\nSentence: He stood for election to Parliament at the 1987 United Kingdom general election , the 1992 United Kingdom general election and 1997 United Kingdom general election .", "prompt_labels": "He(O) stood(O) for(O) election(O) to(O) Parliament(O) at(O) the(O) 1987(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1992(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 1997(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)"}}
{"id": "588", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "politician", "election", "person", "location", "country", "organization", "political party"], "instance": {"id": "588", "words": ["The", "California", "Campaign", "(", "1846-1847", ")", ",", "colloquially", "the", "Conquest", "of", "California", "or", "Conquest", "of", "Alta", "California", "by", "the", "United", "States", ",", "was", "an", "early", "military", "campaign", "of", "the", "Mexican-American", "War", "that", "took", "place", "in", "the", "western", "part", "of", "Mexico", "'s", "Alta", "California", "Department", ",", "in", "the", "present-day", "state", "of", "California", "."], "labels": ["O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, politician, election, person, location, country, organization, political party and O.\nSentence: The California Campaign ( 1846-1847 ) , colloquially the Conquest of California or Conquest of Alta California by the United States , was an early military campaign of the Mexican-American War that took place in the western part of Mexico 's Alta California Department , in the present-day state of California .", "prompt_labels": "The(O) California(B-event) Campaign(I-event) ((O) 1846-1847(O) )(O) ,(O) colloquially(O) the(O) Conquest(B-event) of(I-event) California(I-event) or(O) Conquest(B-event) of(I-event) Alta(I-event) California(I-event) by(O) the(O) United(B-country) States(I-country) ,(O) was(O) an(O) early(O) military(O) campaign(O) of(O) the(O) Mexican-American(B-event) War(I-event) that(O) took(O) place(O) in(O) the(O) western(O) part(O) of(O) Mexico(B-country) 's(O) Alta(B-location) California(I-location) Department(O) ,(O) in(O) the(O) present-day(O) state(O) of(O) California(B-location) .(O)"}}
{"id": "446", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "country", "politician", "organization", "event", "person", "election", "political party"], "instance": {"id": "446", "words": ["In", "the", "2013", "Italian", "general", "election", "the", "UdC", "was", "part", "of", "With", "Monti", "for", "Italy", ",", "the", "coalition", "formed", "around", "Mario", "Monti", "'", "s", "Civic", "Choice", ",", "and", "obtained", "a", "mere", "1.8", "%", "of", "the", "vote", ",", "down", "from", "5.6", "%", "in", "2008", "Italian", "general", "election", "and", "6.8", "%", "in", "2006", "Italian", "general", "election", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, politician, organization, event, person, election, political party and O.\nSentence: In the 2013 Italian general election the UdC was part of With Monti for Italy , the coalition formed around Mario Monti ' s Civic Choice , and obtained a mere 1.8 % of the vote , down from 5.6 % in 2008 Italian general election and 6.8 % in 2006 Italian general election .", "prompt_labels": "In(O) the(O) 2013(B-election) Italian(I-election) general(I-election) election(I-election) the(O) UdC(B-political party) was(O) part(O) of(O) With(B-political party) Monti(I-political party) for(I-political party) Italy(I-political party) ,(O) the(O) coalition(O) formed(O) around(O) Mario(B-politician) Monti(I-politician) '(O) s(O) Civic(B-political party) Choice(I-political party) ,(O) and(O) obtained(O) a(O) mere(O) 1.8(O) %(O) of(O) the(O) vote(O) ,(O) down(O) from(O) 5.6(O) %(O) in(O) 2008(B-election) Italian(I-election) general(I-election) election(I-election) and(O) 6.8(O) %(O) in(O) 2006(B-election) Italian(I-election) general(I-election) election(I-election) .(O)"}}
{"id": "244", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "political party", "location", "election", "politician", "country", "organization", "event"], "instance": {"id": "244", "words": ["She", "has", "been", "a", "frequent", "candidate", "for", "the", "French", "presidency", ",", "starting", "with", "1974", "French", "presidential", "election", ",", "and", "continuing", "through", "those", "of", "1981", "French", "presidential", "election", ",", "1988", "French", "presidential", "election", ",", "1995", "French", "presidential", "election", ",", "2002", "French", "presidential", "election", ",", "and", "2007", "French", "presidential", "election", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, location, election, politician, country, organization, event and O.\nSentence: She has been a frequent candidate for the French presidency , starting with 1974 French presidential election , and continuing through those of 1981 French presidential election , 1988 French presidential election , 1995 French presidential election , 2002 French presidential election , and 2007 French presidential election .", "prompt_labels": "She(O) has(O) been(O) a(O) frequent(O) candidate(O) for(O) the(O) French(O) presidency(O) ,(O) starting(O) with(O) 1974(B-election) French(I-election) presidential(I-election) election(I-election) ,(O) and(O) continuing(O) through(O) those(O) of(O) 1981(B-election) French(I-election) presidential(I-election) election(I-election) ,(O) 1988(B-election) French(I-election) presidential(I-election) election(I-election) ,(O) 1995(B-election) French(I-election) presidential(I-election) election(I-election) ,(O) 2002(B-election) French(I-election) presidential(I-election) election(I-election) ,(O) and(O) 2007(B-election) French(I-election) presidential(I-election) election(I-election) .(O)"}}
{"id": "2", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "person", "location", "election", "politician", "event", "political party", "country"], "instance": {"id": "2", "words": ["Tamsin", "Greig", "narrated", ",", "and", "the", "cast", "included", "Nicky", "Henson", "as", "Napoleon", ",", "Toby", "Jones", "as", "the", "propagandist", "Squealer", ",", "and", "Ralph", "Ineson", "as", "Boxer", "."], "labels": ["B-person", "I-person", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, location, election, politician, event, political party, country and O.\nSentence: Tamsin Greig narrated , and the cast included Nicky Henson as Napoleon , Toby Jones as the propagandist Squealer , and Ralph Ineson as Boxer .", "prompt_labels": "Tamsin(B-person) Greig(I-person) narrated(O) ,(O) and(O) the(O) cast(O) included(O) Nicky(B-person) Henson(I-person) as(O) Napoleon(O) ,(O) Toby(B-person) Jones(I-person) as(O) the(O) propagandist(O) Squealer(O) ,(O) and(O) Ralph(B-person) Ineson(I-person) as(O) Boxer(O) .(O)"}}
{"id": "503", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "politician", "political party", "election", "event", "person", "organization", "location"], "instance": {"id": "503", "words": ["In", "1994", ",", "incumbent", "Democrat", "Ted", "Kennedy", "won", "re-election", "against", "businessman", "Mitt", "Romney", "with", "just", "58", "%", "of", "the", "vote", ",", "the", "lowest", "percentage", "since", "his", "first", "1962", "United", "States", "Senate", "special", "election", "in", "Massachusetts", "."], "labels": ["O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, politician, political party, election, event, person, organization, location and O.\nSentence: In 1994 , incumbent Democrat Ted Kennedy won re-election against businessman Mitt Romney with just 58 % of the vote , the lowest percentage since his first 1962 United States Senate special election in Massachusetts .", "prompt_labels": "In(O) 1994(O) ,(O) incumbent(O) Democrat(O) Ted(B-politician) Kennedy(I-politician) won(O) re-election(O) against(O) businessman(O) Mitt(B-politician) Romney(I-politician) with(O) just(O) 58(O) %(O) of(O) the(O) vote(O) ,(O) the(O) lowest(O) percentage(O) since(O) his(O) first(O) 1962(B-election) United(I-election) States(I-election) Senate(I-election) special(I-election) election(I-election) in(I-election) Massachusetts(I-election) .(O)"}}
{"id": "310", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "person", "event", "location", "organization", "election", "political party", "politician"], "instance": {"id": "310", "words": ["The", "party", "refused", "to", "admit", "Arab", "members", "until", "the", "late", "1960s", ",", "instead", "setting", "up", "a", "succession", "of", "satellite", "parties", "for", "Israeli", "Arabs", ",", "including", "the", "Democratic", "List", "of", "Nazareth", ",", "the", "Democratic", "List", "for", "Israeli", "Arabs", ",", "Agriculture", "and", "Development", ",", "Progress", "and", "Work", ",", "Cooperation", "and", "Brotherhood", ",", "Progress", "and", "Development", "and", "Cooperation", "and", "Development", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, event, location, organization, election, political party, politician and O.\nSentence: The party refused to admit Arab members until the late 1960s , instead setting up a succession of satellite parties for Israeli Arabs , including the Democratic List of Nazareth , the Democratic List for Israeli Arabs , Agriculture and Development , Progress and Work , Cooperation and Brotherhood , Progress and Development and Cooperation and Development .", "prompt_labels": "The(O) party(O) refused(O) to(O) admit(O) Arab(O) members(O) until(O) the(O) late(O) 1960s(O) ,(O) instead(O) setting(O) up(O) a(O) succession(O) of(O) satellite(O) parties(O) for(O) Israeli(O) Arabs(O) ,(O) including(O) the(O) Democratic(B-political party) List(I-political party) of(I-political party) Nazareth(I-political party) ,(O) the(O) Democratic(B-political party) List(I-political party) for(I-political party) Israeli(I-political party) Arabs(I-political party) ,(O) Agriculture(B-political party) and(I-political party) Development(I-political party) ,(O) Progress(B-political party) and(I-political party) Work(I-political party) ,(O) Cooperation(B-political party) and(I-political party) Brotherhood(I-political party) ,(O) Progress(B-political party) and(I-political party) Development(I-political party) and(O) Cooperation(B-political party) and(I-political party) Development(I-political party) .(O)"}}
{"id": "226", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "politician", "election", "location", "organization", "person", "political party", "event"], "instance": {"id": "226", "words": ["Shalala", "has", "been", "elected", "to", "the", "Council", "on", "Foreign", "Relations", ";", "National", "Academy", "of", "Education", ";", "the", "National", "Academy", "of", "Public", "Administration", ";", "the", "American", "Academy", "of", "Arts", "and", "Sciences", ";", "the", "American", "Philosophical", "Society", ";", "the", "National", "Academy", "of", "Social", "Insurance", ";", "the", "American", "Academy", "of", "Political", "and", "Social", "Science", ";", "and", "the", "National", "Academy", "of", "Medicine", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, politician, election, location, organization, person, political party, event and O.\nSentence: Shalala has been elected to the Council on Foreign Relations ; National Academy of Education ; the National Academy of Public Administration ; the American Academy of Arts and Sciences ; the American Philosophical Society ; the National Academy of Social Insurance ; the American Academy of Political and Social Science ; and the National Academy of Medicine .", "prompt_labels": "Shalala(B-person) has(O) been(O) elected(O) to(O) the(O) Council(B-organization) on(I-organization) Foreign(I-organization) Relations(I-organization) ;(O) National(B-organization) Academy(I-organization) of(I-organization) Education(I-organization) ;(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Public(I-organization) Administration(I-organization) ;(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) ;(O) the(O) American(B-organization) Philosophical(I-organization) Society(I-organization) ;(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Social(I-organization) Insurance(I-organization) ;(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Political(I-organization) and(I-organization) Social(I-organization) Science(I-organization) ;(O) and(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Medicine(I-organization) .(O)"}}
{"id": "372", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "political party", "organization", "election", "event", "person", "politician", "location"], "instance": {"id": "372", "words": ["The", "Eleventh", "Amendment", "was", "introduced", "by", "a", "Fianna", "Fáil", "-", "Progressive", "Democrats", "coalition", "government", "and", "was", "also", "supported", "by", "opposition", "parties", "Fine", "Gael", "and", "the", "Labour", "Party", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, organization, election, event, person, politician, location and O.\nSentence: The Eleventh Amendment was introduced by a Fianna Fáil - Progressive Democrats coalition government and was also supported by opposition parties Fine Gael and the Labour Party .", "prompt_labels": "The(O) Eleventh(O) Amendment(O) was(O) introduced(O) by(O) a(O) Fianna(B-political party) Fáil(I-political party) -(O) Progressive(B-political party) Democrats(I-political party) coalition(O) government(O) and(O) was(O) also(O) supported(O) by(O) opposition(O) parties(O) Fine(B-political party) Gael(I-political party) and(O) the(O) Labour(B-political party) Party(I-political party) .(O)"}}
{"id": "543", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "political party", "politician", "location", "event", "organization", "person", "country"], "instance": {"id": "543", "words": ["Film", "stars", "and", "celebrities", "such", "as", "Patrick", "Stewart", ",", "Susan", "Sarandon", ",", "Richard", "Curtis", ",", "Lindsay", "Duncan", ",", "Mark", "Rylance", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, politician, location, event, organization, person, country and O.\nSentence: Film stars and celebrities such as Patrick Stewart , Susan Sarandon , Richard Curtis , Lindsay Duncan , Mark Rylance .", "prompt_labels": "Film(O) stars(O) and(O) celebrities(O) such(O) as(O) Patrick(B-person) Stewart(I-person) ,(O) Susan(B-person) Sarandon(I-person) ,(O) Richard(B-person) Curtis(I-person) ,(O) Lindsay(B-person) Duncan(I-person) ,(O) Mark(B-person) Rylance(I-person) .(O)"}}
{"id": "42", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "political party", "country", "politician", "election", "organization", "person", "location"], "instance": {"id": "42", "words": ["At", "its", "peak", ",", "the", "Neo-Assyrian", "Empire", "of", "911", "to", "609", "BC", "stretched", "from", "eastern", "Libya", "and", "Cyprus", "in", "the", "East", "Mediterranean", "to", "Iran", ",", "and", "from", "present-day", "Armenia", "and", "Azerbaijan", "in", "the", "Transcaucasia", "to", "the", "Arabian", "Peninsula", "."], "labels": ["O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "O", "B-location", "I-location", "O", "B-country", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "O", "B-location", "O", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, country, politician, election, organization, person, location and O.\nSentence: At its peak , the Neo-Assyrian Empire of 911 to 609 BC stretched from eastern Libya and Cyprus in the East Mediterranean to Iran , and from present-day Armenia and Azerbaijan in the Transcaucasia to the Arabian Peninsula .", "prompt_labels": "At(O) its(O) peak(O) ,(O) the(O) Neo-Assyrian(B-country) Empire(I-country) of(O) 911(O) to(O) 609(O) BC(O) stretched(O) from(O) eastern(O) Libya(B-country) and(O) Cyprus(B-country) in(O) the(O) East(B-location) Mediterranean(I-location) to(O) Iran(B-country) ,(O) and(O) from(O) present-day(O) Armenia(B-country) and(O) Azerbaijan(B-country) in(O) the(O) Transcaucasia(B-location) to(O) the(O) Arabian(B-location) Peninsula(I-location) .(O)"}}
{"id": "484", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "political party", "location", "organization", "country", "event", "person", "politician"], "instance": {"id": "484", "words": ["He", "joined", "Mel", "Hurtig", "'", "s", "National", "Party", "of", "Canada", "in", "1993", ",", "and", "later", "campaigned", "for", "the", "Green", "Party", "of", "Canada", "and", "the", "Alberta", "Greens", "."], "labels": ["O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, location, organization, country, event, person, politician and O.\nSentence: He joined Mel Hurtig ' s National Party of Canada in 1993 , and later campaigned for the Green Party of Canada and the Alberta Greens .", "prompt_labels": "He(O) joined(O) Mel(B-politician) Hurtig(I-politician) '(O) s(O) National(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) in(O) 1993(O) ,(O) and(O) later(O) campaigned(O) for(O) the(O) Green(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) and(O) the(O) Alberta(B-political party) Greens(I-political party) .(O)"}}
{"id": "40", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "country", "person", "election", "location", "organization", "event", "political party"], "instance": {"id": "40", "words": ["Celebrities", "donating", "included", "Chris", "Sacca", "(", "who", "offered", "to", "match", "other", "people", "'s", "donations", "and", "ultimately", "gave", "$", "150,000", ")", ",", "Rosie", "'Donnell", ",", "Judd", "Apatow", ",", "Sia", ",", "John", "Legend", ",", "and", "Adele", "Stelter", "The", "number", "of", "members", "of", "the", "ACLU", "doubled", "in", "the", "time", "from", "the", "election", "to", "end", "of", "January", "to", "1", "million", "."], "labels": ["O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, person, election, location, organization, event, political party and O.\nSentence: Celebrities donating included Chris Sacca ( who offered to match other people 's donations and ultimately gave $ 150,000 ) , Rosie 'Donnell , Judd Apatow , Sia , John Legend , and Adele Stelter The number of members of the ACLU doubled in the time from the election to end of January to 1 million .", "prompt_labels": "Celebrities(O) donating(O) included(O) Chris(B-person) Sacca(I-person) ((O) who(O) offered(O) to(O) match(O) other(O) people(O) 's(O) donations(O) and(O) ultimately(O) gave(O) $(O) 150,000(O) )(O) ,(O) Rosie(B-person) 'Donnell(I-person) ,(O) Judd(B-person) Apatow(I-person) ,(O) Sia(B-person) ,(O) John(B-person) Legend(I-person) ,(O) and(O) Adele(B-person) Stelter(I-person) The(O) number(O) of(O) members(O) of(O) the(O) ACLU(B-organization) doubled(O) in(O) the(O) time(O) from(O) the(O) election(O) to(O) end(O) of(O) January(O) to(O) 1(O) million(O) .(O)"}}
{"id": "199", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "politician", "location", "election", "event", "country", "organization", "political party"], "instance": {"id": "199", "words": ["Later", ",", "the", "Rainbow", "Coalition", "was", "joined", "nationwide", "by", "Students", "for", "a", "Democratic", "Society", "(", "SDS", ")", ",", "the", "Brown", "Berets", ",", "American", "Indian", "Movement", ",", "and", "the", "Red", "Guard", "Party", "."], "labels": ["O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "B-organization", "I-organization", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, location, election, event, country, organization, political party and O.\nSentence: Later , the Rainbow Coalition was joined nationwide by Students for a Democratic Society ( SDS ) , the Brown Berets , American Indian Movement , and the Red Guard Party .", "prompt_labels": "Later(O) ,(O) the(O) Rainbow(B-event) Coalition(I-event) was(O) joined(O) nationwide(O) by(O) Students(B-organization) for(I-organization) a(I-organization) Democratic(I-organization) Society(I-organization) ((O) SDS(B-organization) )(O) ,(O) the(O) Brown(B-organization) Berets(I-organization) ,(O) American(B-political party) Indian(I-political party) Movement(I-political party) ,(O) and(O) the(O) Red(B-political party) Guard(I-political party) Party(I-political party) .(O)"}}
{"id": "86", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "person", "country", "location", "political party", "election", "organization", "politician"], "instance": {"id": "86", "words": ["Fini", "was", "succeeded", "by", "Ignazio", "La", "Russa", ",", "who", "managed", "the", "merger", "of", "the", "party", "with", "Forza", "Italia", "(", "FI", ")", "into", "The", "People", "of", "Freedom", "(", "PdL", ")", "in", "2009", "."], "labels": ["B-politician", "O", "O", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, country, location, political party, election, organization, politician and O.\nSentence: Fini was succeeded by Ignazio La Russa , who managed the merger of the party with Forza Italia ( FI ) into The People of Freedom ( PdL ) in 2009 .", "prompt_labels": "Fini(B-politician) was(O) succeeded(O) by(O) Ignazio(B-politician) La(I-politician) Russa(I-politician) ,(O) who(O) managed(O) the(O) merger(O) of(O) the(O) party(O) with(O) Forza(B-political party) Italia(I-political party) ((O) FI(B-political party) )(O) into(O) The(B-political party) People(I-political party) of(I-political party) Freedom(I-political party) ((O) PdL(B-political party) )(O) in(O) 2009(O) .(O)"}}
{"id": "163", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "event", "organization", "location", "country", "election", "political party", "politician"], "instance": {"id": "163", "words": ["Satellite", "mints", "to", "aid", "in", "the", "re-coinage", "were", "established", "in", "Bristol", ",", "Chester", ",", "Exeter", ",", "Norwich", ",", "and", "York", ",", "with", "returned", "coins", "being", "valued", "by", "weight", ",", "not", "face", "value", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, organization, location, country, election, political party, politician and O.\nSentence: Satellite mints to aid in the re-coinage were established in Bristol , Chester , Exeter , Norwich , and York , with returned coins being valued by weight , not face value .", "prompt_labels": "Satellite(O) mints(O) to(O) aid(O) in(O) the(O) re-coinage(O) were(O) established(O) in(O) Bristol(B-location) ,(O) Chester(B-location) ,(O) Exeter(B-location) ,(O) Norwich(B-location) ,(O) and(O) York(B-location) ,(O) with(O) returned(O) coins(O) being(O) valued(O) by(O) weight(O) ,(O) not(O) face(O) value(O) .(O)"}}
{"id": "344", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "politician", "organization", "election", "country", "event", "location", "political party"], "instance": {"id": "344", "words": ["He", "was", "then", "chosen", "to", "fill", "the", "casual", "vacancy", "caused", "by", "the", "resignation", "of", "Brian", "Archer", "in", "1994", ",", "and", "was", "elected", "in", "his", "own", "right", "at", "the", "subsequent", "1998", "Australian", "federal", "election", "and", "re-elected", "in", "2004", "Australian", "federal", "election", "and", "2004", "Australian", "federal", "election", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, organization, election, country, event, location, political party and O.\nSentence: He was then chosen to fill the casual vacancy caused by the resignation of Brian Archer in 1994 , and was elected in his own right at the subsequent 1998 Australian federal election and re-elected in 2004 Australian federal election and 2004 Australian federal election .", "prompt_labels": "He(O) was(O) then(O) chosen(O) to(O) fill(O) the(O) casual(O) vacancy(O) caused(O) by(O) the(O) resignation(O) of(O) Brian(B-politician) Archer(I-politician) in(O) 1994(O) ,(O) and(O) was(O) elected(O) in(O) his(O) own(O) right(O) at(O) the(O) subsequent(O) 1998(B-election) Australian(I-election) federal(I-election) election(I-election) and(O) re-elected(O) in(O) 2004(B-election) Australian(I-election) federal(I-election) election(I-election) and(O) 2004(B-election) Australian(I-election) federal(I-election) election(I-election) .(O)"}}
{"id": "506", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "person", "politician", "election", "event", "political party", "country", "location"], "instance": {"id": "506", "words": ["Warner", "was", "endorsed", "by", "such", "notable", "figures", "as", "Bob", "Dole", ",", "George", "H.W.", "Bush", ",", "and", "Colin", "Powell", ",", "while", "Miller", "was", "endorsed", "by", "the", "National", "Rifle", "Association", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "I-politician", "O", "O", "B-politician", "I-politician", "O", "O", "B-politician", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, politician, election, event, political party, country, location and O.\nSentence: Warner was endorsed by such notable figures as Bob Dole , George H.W. Bush , and Colin Powell , while Miller was endorsed by the National Rifle Association .", "prompt_labels": "Warner(B-politician) was(O) endorsed(O) by(O) such(O) notable(O) figures(O) as(O) Bob(B-politician) Dole(I-politician) ,(O) George(B-politician) H.W.(I-politician) Bush(I-politician) ,(O) and(O) Colin(B-politician) Powell(I-politician) ,(O) while(O) Miller(B-politician) was(O) endorsed(O) by(O) the(O) National(B-organization) Rifle(I-organization) Association(I-organization) .(O)"}}
{"id": "501", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "country", "event", "election", "political party", "location", "person", "politician"], "instance": {"id": "501", "words": ["In", "1968", "United", "States", "presidential", "election", ",", "Alabama", "supported", "native", "son", "and", "American", "Independent", "Party", "candidate", "George", "Wallace", "over", "both", "Richard", "Nixon", "and", "Hubert", "Humphrey", "."], "labels": ["O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-location", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "O", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, event, election, political party, location, person, politician and O.\nSentence: In 1968 United States presidential election , Alabama supported native son and American Independent Party candidate George Wallace over both Richard Nixon and Hubert Humphrey .", "prompt_labels": "In(O) 1968(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) Alabama(B-location) supported(O) native(O) son(O) and(O) American(B-political party) Independent(I-political party) Party(I-political party) candidate(O) George(B-politician) Wallace(I-politician) over(O) both(O) Richard(B-politician) Nixon(I-politician) and(O) Hubert(B-politician) Humphrey(I-politician) .(O)"}}
{"id": "432", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "person", "political party", "country", "politician", "event", "election", "organization"], "instance": {"id": "432", "words": ["In", "the", "2008", "Canadian", "federal", "election", ",", "Di", "Ianni", "was", "the", "Liberal", "Party", "of", "Canada", "candidate", "in", "the", "federal", "riding", "of", "Hamilton", "East", "-", "Stoney", "Creek", ",", "losing", "the", "race", "to", "incumbent", "MP", "Wayne", "Marston", "of", "the", "New", "Democratic", "Party", "by", "a", "margin", "of", "6,464", "votes", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, political party, country, politician, event, election, organization and O.\nSentence: In the 2008 Canadian federal election , Di Ianni was the Liberal Party of Canada candidate in the federal riding of Hamilton East - Stoney Creek , losing the race to incumbent MP Wayne Marston of the New Democratic Party by a margin of 6,464 votes .", "prompt_labels": "In(O) the(O) 2008(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) Di(B-politician) Ianni(I-politician) was(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) candidate(O) in(O) the(O) federal(O) riding(O) of(O) Hamilton(B-location) East(I-location) -(O) Stoney(B-location) Creek(I-location) ,(O) losing(O) the(O) race(O) to(O) incumbent(O) MP(O) Wayne(B-politician) Marston(I-politician) of(O) the(O) New(B-political party) Democratic(I-political party) Party(I-political party) by(O) a(O) margin(O) of(O) 6,464(O) votes(O) .(O)"}}
{"id": "160", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "election", "location", "country", "organization", "event", "political party", "person"], "instance": {"id": "160", "words": ["The", "current", "prime", "minister", "of", "Denmark", "is", "Mette", "Frederiksen", "who", ",", "since", "27", "June", "2019", ",", "has", "led", "a", "one-party", "government", "consisting", "of", "the", "Social", "Democrats", "with", "parliamentary", "support", "from", "the", "Danish", "Social", "Liberal", "Party", ",", "Socialist", "People", "'s", "Party", ",", "Red-Green", "Alliance", ",", "the", "Faroese", "Social", "Democratic", "Party", "and", "Greenland", "'", "s", "Inuit", "Ataqatigiit", "and", "Siumut", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-location", "O", "O", "B-political party", "I-political party", "O", "B-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, location, country, organization, event, political party, person and O.\nSentence: The current prime minister of Denmark is Mette Frederiksen who , since 27 June 2019 , has led a one-party government consisting of the Social Democrats with parliamentary support from the Danish Social Liberal Party , Socialist People 's Party , Red-Green Alliance , the Faroese Social Democratic Party and Greenland ' s Inuit Ataqatigiit and Siumut .", "prompt_labels": "The(O) current(O) prime(O) minister(O) of(O) Denmark(O) is(O) Mette(B-politician) Frederiksen(I-politician) who(O) ,(O) since(O) 27(O) June(O) 2019(O) ,(O) has(O) led(O) a(O) one-party(O) government(O) consisting(O) of(O) the(O) Social(O) Democrats(O) with(O) parliamentary(O) support(O) from(O) the(O) Danish(B-political party) Social(I-political party) Liberal(I-political party) Party(I-political party) ,(O) Socialist(B-political party) People(I-political party) 's(I-political party) Party(I-political party) ,(O) Red-Green(B-political party) Alliance(I-political party) ,(O) the(O) Faroese(O) Social(B-political party) Democratic(I-political party) Party(I-political party) and(O) Greenland(B-location) '(O) s(O) Inuit(B-political party) Ataqatigiit(I-political party) and(O) Siumut(B-political party) .(O)"}}
{"id": "34", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "organization", "country", "election", "person", "politician", "location", "event"], "instance": {"id": "34", "words": ["He", "also", "recalled", "a", "past", "instance", "in", "which", "Osama", "bin", "Laden", "called", "on", "al-Qaeda", "members", "and", "supporters", "to", "give", "allegiance", "to", "Abu", "Omar", "al-Baghdadi", "when", "the", "group", "was", "still", "solely", "operating", "in", "Iraq", ",", "as", "the", "Islamic", "State", "of", "Iraq", ",", "and", "condemned", "Ayman", "al-Zawahiri", "for", "not", "making", "this", "same", "claim", "for", "Abu", "Bakr", "al-Baghdadi", ",", "and", "that", "Zawahiri", "was", "encouraging", "factionalism", "and", "division", "between", "former", "allies", "of", "ISIL", "such", "as", "the", "al-Nusra", "Front", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "I-politician", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, country, election, person, politician, location, event and O.\nSentence: He also recalled a past instance in which Osama bin Laden called on al-Qaeda members and supporters to give allegiance to Abu Omar al-Baghdadi when the group was still solely operating in Iraq , as the Islamic State of Iraq , and condemned Ayman al-Zawahiri for not making this same claim for Abu Bakr al-Baghdadi , and that Zawahiri was encouraging factionalism and division between former allies of ISIL such as the al-Nusra Front .", "prompt_labels": "He(O) also(O) recalled(O) a(O) past(O) instance(O) in(O) which(O) Osama(B-politician) bin(I-politician) Laden(I-politician) called(O) on(O) al-Qaeda(B-organization) members(O) and(O) supporters(O) to(O) give(O) allegiance(O) to(O) Abu(B-politician) Omar(I-politician) al-Baghdadi(I-politician) when(O) the(O) group(O) was(O) still(O) solely(O) operating(O) in(O) Iraq(B-country) ,(O) as(O) the(O) Islamic(B-organization) State(I-organization) of(I-organization) Iraq(I-organization) ,(O) and(O) condemned(O) Ayman(B-politician) al-Zawahiri(I-politician) for(O) not(O) making(O) this(O) same(O) claim(O) for(O) Abu(B-politician) Bakr(I-politician) al-Baghdadi(I-politician) ,(O) and(O) that(O) Zawahiri(B-politician) was(O) encouraging(O) factionalism(O) and(O) division(O) between(O) former(O) allies(O) of(O) ISIL(B-organization) such(O) as(O) the(O) al-Nusra(B-organization) Front(I-organization) .(O)"}}
{"id": "305", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "person", "organization", "political party", "country", "election", "location", "event"], "instance": {"id": "305", "words": ["The", "Union", "populaire", "was", "succeeded", "by", "the", "Parti", "nationaliste", "du", "Québec", "in", "the", "1984", "Canadian", "federal", "election", ",", "and", "subsequently", "by", "the", "Bloc", "Québécois", "."], "labels": ["O", "B-political party", "I-political party", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, organization, political party, country, election, location, event and O.\nSentence: The Union populaire was succeeded by the Parti nationaliste du Québec in the 1984 Canadian federal election , and subsequently by the Bloc Québécois .", "prompt_labels": "The(O) Union(B-political party) populaire(I-political party) was(O) succeeded(O) by(O) the(O) Parti(B-political party) nationaliste(I-political party) du(I-political party) Québec(I-political party) in(O) the(O) 1984(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) and(O) subsequently(O) by(O) the(O) Bloc(B-political party) Québécois(I-political party) .(O)"}}
{"id": "192", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "political party", "location", "event", "person", "country", "politician", "election"], "instance": {"id": "192", "words": ["ALP", "=", "Australian", "Labor", "Party", ",", "L", "+", "NP", "=", "grouping", "of", "Liberal", "Party", "of", "Australia", "/", "National", "Party", "of", "Australia", "/", "Liberal", "National", "Party", "of", "Queensland", "/", "Country", "Liberal", "Party", "Coalition", "parties", "(", "and", "predecessors", ")", "."], "labels": ["B-political party", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, location, event, person, country, politician, election and O.\nSentence: ALP = Australian Labor Party , L + NP = grouping of Liberal Party of Australia / National Party of Australia / Liberal National Party of Queensland / Country Liberal Party Coalition parties ( and predecessors ) .", "prompt_labels": "ALP(B-political party) =(O) Australian(B-political party) Labor(I-political party) Party(I-political party) ,(O) L(O) +(O) NP(O) =(O) grouping(O) of(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) /(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) /(O) Liberal(B-political party) National(I-political party) Party(I-political party) of(I-political party) Queensland(I-political party) /(O) Country(B-political party) Liberal(I-political party) Party(I-political party) Coalition(O) parties(O) ((O) and(O) predecessors(O) )(O) .(O)"}}
{"id": "315", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "politician", "location", "political party", "election", "person", "country", "event"], "instance": {"id": "315", "words": ["When", "this", "draft", "was", "discussed", "in", "1929", "by", "the", "judiciary", "committee", "of", "the", "Reichstag", ",", "the", "Social", "Democratic", "Party", "of", "Germany", ",", "the", "Communist", "Party", "of", "Germany", ",", "and", "the", "left-wing", "liberal", "German", "Democratic", "Party", "at", "first", "managed", "to", "mobilize", "a", "majority", "of", "15", "to", "13", "votes", "against", "Paragraph", "296", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, location, political party, election, person, country, event and O.\nSentence: When this draft was discussed in 1929 by the judiciary committee of the Reichstag , the Social Democratic Party of Germany , the Communist Party of Germany , and the left-wing liberal German Democratic Party at first managed to mobilize a majority of 15 to 13 votes against Paragraph 296 .", "prompt_labels": "When(O) this(O) draft(O) was(O) discussed(O) in(O) 1929(O) by(O) the(O) judiciary(O) committee(O) of(O) the(O) Reichstag(B-location) ,(O) the(O) Social(B-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Germany(I-political party) ,(O) the(O) Communist(B-political party) Party(I-political party) of(I-political party) Germany(I-political party) ,(O) and(O) the(O) left-wing(O) liberal(O) German(B-political party) Democratic(I-political party) Party(I-political party) at(O) first(O) managed(O) to(O) mobilize(O) a(O) majority(O) of(O) 15(O) to(O) 13(O) votes(O) against(O) Paragraph(O) 296(O) .(O)"}}
{"id": "536", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "country", "person", "election", "political party", "location", "event", "organization"], "instance": {"id": "536", "words": ["Jacqueline", "McKenzie", ",", "Daniel", "Wyllie", "and", "John", "Brumpton", "reprise", "their", "roles", "from", "the", "original", "film", "."], "labels": ["B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, person, election, political party, location, event, organization and O.\nSentence: Jacqueline McKenzie , Daniel Wyllie and John Brumpton reprise their roles from the original film .", "prompt_labels": "Jacqueline(B-person) McKenzie(I-person) ,(O) Daniel(B-person) Wyllie(I-person) and(O) John(B-person) Brumpton(I-person) reprise(O) their(O) roles(O) from(O) the(O) original(O) film(O) .(O)"}}
{"id": "19", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "election", "organization", "politician", "location", "country", "event", "person"], "instance": {"id": "19", "words": ["Various", "ideological", "beliefs", "were", "factionalised", "under", "reforms", "to", "the", "ALP", "under", "Gough", "Whitlam", ",", "resulting", "in", "what", "is", "now", "known", "as", "the", "Labor", "Left", "who", "tend", "to", "favour", "a", "more", "interventionist", "economic", "policy", "and", "more", "socially", "progressive", "ideals", ",", "and", "Labor", "Right", ",", "the", "now", "dominant", "faction", "that", "tends", "to", "be", "more", "economically", "liberal", "and", "focus", "to", "a", "lesser", "extent", "on", "social", "issues", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, election, organization, politician, location, country, event, person and O.\nSentence: Various ideological beliefs were factionalised under reforms to the ALP under Gough Whitlam , resulting in what is now known as the Labor Left who tend to favour a more interventionist economic policy and more socially progressive ideals , and Labor Right , the now dominant faction that tends to be more economically liberal and focus to a lesser extent on social issues .", "prompt_labels": "Various(O) ideological(O) beliefs(O) were(O) factionalised(O) under(O) reforms(O) to(O) the(O) ALP(O) under(O) Gough(B-politician) Whitlam(I-politician) ,(O) resulting(O) in(O) what(O) is(O) now(O) known(O) as(O) the(O) Labor(B-political party) Left(I-political party) who(O) tend(O) to(O) favour(O) a(O) more(O) interventionist(O) economic(O) policy(O) and(O) more(O) socially(O) progressive(O) ideals(O) ,(O) and(O) Labor(B-political party) Right(I-political party) ,(O) the(O) now(O) dominant(O) faction(O) that(O) tends(O) to(O) be(O) more(O) economically(O) liberal(O) and(O) focus(O) to(O) a(O) lesser(O) extent(O) on(O) social(O) issues(O) .(O)"}}
{"id": "558", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "political party", "politician", "person", "country", "election", "organization", "event"], "instance": {"id": "558", "words": ["Mary", "Blathwayt", "'", "s", "parents", "planted", "trees", "there", "between", "April", "1909", "and", "July", "1911", "to", "commemorate", "the", "achievements", "of", "suffragettes", "including", "Emmeline", "Pankhurst", ",", "Christabel", "Pankhurst", ",", "Annie", "Kenney", ",", "Charlotte", "Despard", ",", "Millicent", "Fawcett", "and", "Lady", "Lytton", "."], "labels": ["B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, political party, politician, person, country, election, organization, event and O.\nSentence: Mary Blathwayt ' s parents planted trees there between April 1909 and July 1911 to commemorate the achievements of suffragettes including Emmeline Pankhurst , Christabel Pankhurst , Annie Kenney , Charlotte Despard , Millicent Fawcett and Lady Lytton .", "prompt_labels": "Mary(B-person) Blathwayt(I-person) '(O) s(O) parents(O) planted(O) trees(O) there(O) between(O) April(O) 1909(O) and(O) July(O) 1911(O) to(O) commemorate(O) the(O) achievements(O) of(O) suffragettes(O) including(O) Emmeline(B-person) Pankhurst(I-person) ,(O) Christabel(B-person) Pankhurst(I-person) ,(O) Annie(B-person) Kenney(I-person) ,(O) Charlotte(B-person) Despard(I-person) ,(O) Millicent(B-person) Fawcett(I-person) and(O) Lady(B-person) Lytton(I-person) .(O)"}}
{"id": "135", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "organization", "country", "politician", "location", "election", "event", "political party"], "instance": {"id": "135", "words": ["He", "was", "the", "founding", "President", "of", "the", "Council", "on", "Foreign", "Relations", ",", "formed", "in", "1921", ",", "Chairman", "of", "the", "Carnegie", "Endowment", "for", "International", "Peace", ",", "and", "a", "trustee", "of", "the", "Rockefeller", "Foundation", "from", "1922", "to", "1939", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, country, politician, location, election, event, political party and O.\nSentence: He was the founding President of the Council on Foreign Relations , formed in 1921 , Chairman of the Carnegie Endowment for International Peace , and a trustee of the Rockefeller Foundation from 1922 to 1939 .", "prompt_labels": "He(O) was(O) the(O) founding(O) President(O) of(O) the(O) Council(B-organization) on(I-organization) Foreign(I-organization) Relations(I-organization) ,(O) formed(O) in(O) 1921(O) ,(O) Chairman(O) of(O) the(O) Carnegie(B-organization) Endowment(I-organization) for(I-organization) International(I-organization) Peace(I-organization) ,(O) and(O) a(O) trustee(O) of(O) the(O) Rockefeller(B-organization) Foundation(I-organization) from(O) 1922(O) to(O) 1939(O) .(O)"}}
{"id": "482", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "political party", "person", "location", "politician", "event", "organization", "country"], "instance": {"id": "482", "words": ["The", "British", "People", "'s", "Party", "was", "a", "neo-Nazi", "political", "party", "in", "the", "United", "Kingdom", ",", "launched", "in", "2005", "by", "Kevin", "Watmough", ",", "Eddy", "Morrison", ",", "John", "G.", "Wood", "and", "Sid", "Williamson", ",", "former", "members", "of", "Combat", "18", ",", "British", "National", "Party", "(", "BNP", ")", ",", "National", "Front", "(", "NF", ")", "and", "the", "White", "Nationalist", "Party", ",", "as", "a", "splinter", "group", "from", "the", "Nationalist", "Alliance", "."], "labels": ["O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "I-politician", "O", "B-politician", "I-politician", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, person, location, politician, event, organization, country and O.\nSentence: The British People 's Party was a neo-Nazi political party in the United Kingdom , launched in 2005 by Kevin Watmough , Eddy Morrison , John G. Wood and Sid Williamson , former members of Combat 18 , British National Party ( BNP ) , National Front ( NF ) and the White Nationalist Party , as a splinter group from the Nationalist Alliance .", "prompt_labels": "The(O) British(B-political party) People(I-political party) 's(I-political party) Party(I-political party) was(O) a(O) neo-Nazi(O) political(O) party(O) in(O) the(O) United(B-country) Kingdom(I-country) ,(O) launched(O) in(O) 2005(O) by(O) Kevin(B-politician) Watmough(I-politician) ,(O) Eddy(B-politician) Morrison(I-politician) ,(O) John(B-politician) G.(I-politician) Wood(I-politician) and(O) Sid(B-politician) Williamson(I-politician) ,(O) former(O) members(O) of(O) Combat(B-organization) 18(I-organization) ,(O) British(B-political party) National(I-political party) Party(I-political party) ((O) BNP(B-political party) )(O) ,(O) National(B-political party) Front(I-political party) ((O) NF(B-political party) )(O) and(O) the(O) White(B-political party) Nationalist(I-political party) Party(I-political party) ,(O) as(O) a(O) splinter(O) group(O) from(O) the(O) Nationalist(B-political party) Alliance(I-political party) .(O)"}}
{"id": "581", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "person", "election", "country", "organization", "political party", "location", "politician"], "instance": {"id": "581", "words": ["The", "Arakan", "Campaign", "of", "1942-43", "was", "the", "first", "tentative", "Allies", "of", "World", "War", "II", "attack", "into", "British", "rule", "in", "Burma", ",", "following", "the", "Japanese", "conquest", "of", "Burma", "earlier", "in", "1942", ",", "during", "the", "Second", "World", "War", "."], "labels": ["O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, election, country, organization, political party, location, politician and O.\nSentence: The Arakan Campaign of 1942-43 was the first tentative Allies of World War II attack into British rule in Burma , following the Japanese conquest of Burma earlier in 1942 , during the Second World War .", "prompt_labels": "The(O) Arakan(B-event) Campaign(I-event) of(O) 1942-43(O) was(O) the(O) first(O) tentative(O) Allies(B-country) of(O) World(B-event) War(I-event) II(I-event) attack(O) into(O) British(O) rule(O) in(O) Burma(B-country) ,(O) following(O) the(O) Japanese(B-event) conquest(I-event) of(I-event) Burma(I-event) earlier(O) in(O) 1942(O) ,(O) during(O) the(O) Second(B-event) World(I-event) War(I-event) .(O)"}}
{"id": "329", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "location", "event", "person", "organization", "political party", "country", "election"], "instance": {"id": "329", "words": ["Two", "members", "being", "appointed", "by", "each", "of", "the", "National", "Governors", "Association", ",", "the", "National", "Conference", "of", "State", "Legislatures", ",", "the", "National", "Association", "of", "Secretaries", "of", "State", ",", "the", "National", "Association", "of", "State", "Election", "Directors", ",", "the", "National", "Association", "of", "Counties", ",", "the", ")"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, event, person, organization, political party, country, election and O.\nSentence: Two members being appointed by each of the National Governors Association , the National Conference of State Legislatures , the National Association of Secretaries of State , the National Association of State Election Directors , the National Association of Counties , the )", "prompt_labels": "Two(O) members(O) being(O) appointed(O) by(O) each(O) of(O) the(O) National(B-political party) Governors(I-political party) Association(I-political party) ,(O) the(O) National(B-organization) Conference(I-organization) of(I-organization) State(I-organization) Legislatures(I-organization) ,(O) the(O) National(B-organization) Association(I-organization) of(I-organization) Secretaries(I-organization) of(I-organization) State(I-organization) ,(O) the(O) National(B-organization) Association(I-organization) of(I-organization) State(I-organization) Election(I-organization) Directors(I-organization) ,(O) the(O) National(B-organization) Association(I-organization) of(I-organization) Counties(I-organization) ,(O) the(O) )(O)"}}
{"id": "16", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "country", "political party", "politician", "election", "location", "person", "organization"], "instance": {"id": "16", "words": ["The", "twelve", "countries", "that", "had", "significant", "interests", "in", "Antarctica", "at", "the", "time", "were", ":", "Argentina", ",", "Australia", ",", "Belgium", ",", "Chile", ",", "France", ",", "Japan", ",", "New", "Zealand", ",", "Norway", ",", "South", "Africa", ",", "the", "Soviet", "Union", ",", "the", "United", "Kingdom", ",", "and", "the", "United", "States", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "I-country", "O", "B-country", "O", "B-country", "I-country", "O", "O", "B-country", "I-country", "O", "O", "B-country", "I-country", "O", "O", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, political party, politician, election, location, person, organization and O.\nSentence: The twelve countries that had significant interests in Antarctica at the time were : Argentina , Australia , Belgium , Chile , France , Japan , New Zealand , Norway , South Africa , the Soviet Union , the United Kingdom , and the United States .", "prompt_labels": "The(O) twelve(O) countries(O) that(O) had(O) significant(O) interests(O) in(O) Antarctica(B-location) at(O) the(O) time(O) were(O) :(O) Argentina(B-country) ,(O) Australia(B-country) ,(O) Belgium(B-country) ,(O) Chile(B-country) ,(O) France(B-country) ,(O) Japan(B-country) ,(O) New(B-country) Zealand(I-country) ,(O) Norway(B-country) ,(O) South(B-country) Africa(I-country) ,(O) the(O) Soviet(B-country) Union(I-country) ,(O) the(O) United(B-country) Kingdom(I-country) ,(O) and(O) the(O) United(B-country) States(I-country) .(O)"}}
{"id": "187", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "politician", "country", "organization", "location", "event", "election", "political party"], "instance": {"id": "187", "words": ["Some", "parts", "of", "the", "denomination", "belong", "to", "the", "National", "Association", "of", "Evangelicals", ",", "the", "Canadian", "Council", "of", "Churches", ",", "and", "the", "Evangelical", "Fellowship", "of", "Canada", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, country, organization, location, event, election, political party and O.\nSentence: Some parts of the denomination belong to the National Association of Evangelicals , the Canadian Council of Churches , and the Evangelical Fellowship of Canada .", "prompt_labels": "Some(O) parts(O) of(O) the(O) denomination(O) belong(O) to(O) the(O) National(B-organization) Association(I-organization) of(I-organization) Evangelicals(I-organization) ,(O) the(O) Canadian(B-organization) Council(I-organization) of(I-organization) Churches(I-organization) ,(O) and(O) the(O) Evangelical(B-organization) Fellowship(I-organization) of(I-organization) Canada(I-organization) .(O)"}}
{"id": "177", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "organization", "politician", "event", "political party", "location", "country", "person"], "instance": {"id": "177", "words": ["The", "organization", "has", "local", "chapters", "in", "Fredericton", ",", "Montreal", ",", "Ottawa", ",", "Toronto", ",", "Hamilton", ",", "Edmonton", ",", "Calgary", "and", "Vancouver", ",", "plus", "international", "chapters", "in", "the", "US", "and", "the", "UK", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, politician, event, political party, location, country, person and O.\nSentence: The organization has local chapters in Fredericton , Montreal , Ottawa , Toronto , Hamilton , Edmonton , Calgary and Vancouver , plus international chapters in the US and the UK .", "prompt_labels": "The(O) organization(O) has(O) local(O) chapters(O) in(O) Fredericton(B-location) ,(O) Montreal(B-location) ,(O) Ottawa(B-location) ,(O) Toronto(B-location) ,(O) Hamilton(B-location) ,(O) Edmonton(B-location) ,(O) Calgary(B-location) and(O) Vancouver(B-location) ,(O) plus(O) international(O) chapters(O) in(O) the(O) US(B-country) and(O) the(O) UK(B-country) .(O)"}}
{"id": "181", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "political party", "organization", "politician", "election", "country", "event", "person"], "instance": {"id": "181", "words": ["Taylor", "was", "a", "candidate", "for", "the", "social", "democratic", "New", "Democratic", "Party", "(", "NDP", ")", "in", "Mount", "Royal", "on", "three", "occasions", "in", "the", "1960s", ",", "beginning", "with", "the", "1962", "Canadian", "federal", "election", "when", "he", "came", "in", "third", "behind", "Liberal", "Party", "of", "Canada", "Alan", "MacNaughton", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, political party, organization, politician, election, country, event, person and O.\nSentence: Taylor was a candidate for the social democratic New Democratic Party ( NDP ) in Mount Royal on three occasions in the 1960s , beginning with the 1962 Canadian federal election when he came in third behind Liberal Party of Canada Alan MacNaughton .", "prompt_labels": "Taylor(B-politician) was(O) a(O) candidate(O) for(O) the(O) social(O) democratic(O) New(B-political party) Democratic(I-political party) Party(I-political party) ((O) NDP(B-political party) )(O) in(O) Mount(B-location) Royal(I-location) on(O) three(O) occasions(O) in(O) the(O) 1960s(O) ,(O) beginning(O) with(O) the(O) 1962(B-election) Canadian(I-election) federal(I-election) election(I-election) when(O) he(O) came(O) in(O) third(O) behind(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) Alan(B-politician) MacNaughton(I-politician) .(O)"}}
{"id": "440", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "person", "event", "organization", "location", "election", "country", "political party"], "instance": {"id": "440", "words": ["He", "was", "the", "Minister", "of", "Finance", "(", "1976-1984", ")", ",", "Minister", "of", "International", "Trade", "and", "Industry", "(", "1984-1987", ")", ",", "former", "chairman", "of", "Asian", "Development", "Bank", ",", "former", "chairman", "of", "Islamic", "Development", "Bank", ",", "founding", "Chairman", "and", "Chief", "Executive", "of", "Malaysian", "oil", "company", ",", "PETRONAS", ",", "and", "chairman", "of", "the", "33rd", "Board", "of", "Governors", "of", "the", "World", "Bank", "and", "IMF", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, event, organization, location, election, country, political party and O.\nSentence: He was the Minister of Finance ( 1976-1984 ) , Minister of International Trade and Industry ( 1984-1987 ) , former chairman of Asian Development Bank , former chairman of Islamic Development Bank , founding Chairman and Chief Executive of Malaysian oil company , PETRONAS , and chairman of the 33rd Board of Governors of the World Bank and IMF .", "prompt_labels": "He(O) was(O) the(O) Minister(O) of(O) Finance(O) ((O) 1976-1984(O) )(O) ,(O) Minister(O) of(O) International(O) Trade(O) and(O) Industry(O) ((O) 1984-1987(O) )(O) ,(O) former(O) chairman(O) of(O) Asian(B-organization) Development(I-organization) Bank(I-organization) ,(O) former(O) chairman(O) of(O) Islamic(B-organization) Development(I-organization) Bank(I-organization) ,(O) founding(O) Chairman(O) and(O) Chief(O) Executive(O) of(O) Malaysian(B-organization) oil(I-organization) company(I-organization) ,(O) PETRONAS(B-organization) ,(O) and(O) chairman(O) of(O) the(O) 33rd(O) Board(O) of(O) Governors(O) of(O) the(O) World(B-organization) Bank(I-organization) and(O) IMF(B-organization) .(O)"}}
{"id": "214", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "politician", "person", "location", "country", "election", "organization", "event"], "instance": {"id": "214", "words": ["Spivak", "has", "received", "11", "honorary", "doctorates", "from", "the", "University", "of", "Toronto", ",", "University", "of", "London", ",", "Oberlin", "College", ",", "Rovira", "i", "Virgili", "University", ",", "Rabindra", "Bharati", "University", ",", "National", "University", "of", "General", "San", "Martín", ",", "University", "of", "St", "Andrews", ",", "Université", "de", "Vincennes", "à", "Saint-Denis", ",", "Presidency", "University", ",", "Yale", "University", ",", "and", "University", "of", "Ghana", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, person, location, country, election, organization, event and O.\nSentence: Spivak has received 11 honorary doctorates from the University of Toronto , University of London , Oberlin College , Rovira i Virgili University , Rabindra Bharati University , National University of General San Martín , University of St Andrews , Université de Vincennes à Saint-Denis , Presidency University , Yale University , and University of Ghana .", "prompt_labels": "Spivak(B-person) has(O) received(O) 11(O) honorary(O) doctorates(O) from(O) the(O) University(B-organization) of(I-organization) Toronto(I-organization) ,(O) University(B-organization) of(I-organization) London(I-organization) ,(O) Oberlin(B-organization) College(I-organization) ,(O) Rovira(B-organization) i(I-organization) Virgili(I-organization) University(I-organization) ,(O) Rabindra(B-organization) Bharati(I-organization) University(I-organization) ,(O) National(B-organization) University(I-organization) of(I-organization) General(I-organization) San(I-organization) Martín(I-organization) ,(O) University(B-organization) of(I-organization) St(I-organization) Andrews(I-organization) ,(O) Université(B-organization) de(I-organization) Vincennes(I-organization) à(I-organization) Saint-Denis(I-organization) ,(O) Presidency(B-organization) University(I-organization) ,(O) Yale(B-organization) University(I-organization) ,(O) and(O) University(B-organization) of(I-organization) Ghana(I-organization) .(O)"}}
{"id": "41", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "location", "political party", "person", "organization", "event", "country", "election"], "instance": {"id": "41", "words": ["Assyria", "(", "This", "vast", "span", "of", "time", "is", "divided", "in", "Early", "Period", "(", "2500", "BCE-2025", "BCE", ")", ",", "Old", "Assyrian", "Empire", "(", "2025", "BCE", "-", "1378", "BCE", ")", ",", "Middle", "Assyrian", "Empire", "(", "1392", "BCE", "-", "934", "BCE", ")", "and", "Neo-Assyrian", "Empire", "(", "911", "BCE", "-", "609", "BCE", ")", "."], "labels": ["B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, political party, person, organization, event, country, election and O.\nSentence: Assyria ( This vast span of time is divided in Early Period ( 2500 BCE-2025 BCE ) , Old Assyrian Empire ( 2025 BCE - 1378 BCE ) , Middle Assyrian Empire ( 1392 BCE - 934 BCE ) and Neo-Assyrian Empire ( 911 BCE - 609 BCE ) .", "prompt_labels": "Assyria(B-country) ((O) This(O) vast(O) span(O) of(O) time(O) is(O) divided(O) in(O) Early(O) Period(O) ((O) 2500(O) BCE-2025(O) BCE(O) )(O) ,(O) Old(B-country) Assyrian(I-country) Empire(I-country) ((O) 2025(O) BCE(O) -(O) 1378(O) BCE(O) )(O) ,(O) Middle(B-country) Assyrian(I-country) Empire(I-country) ((O) 1392(O) BCE(O) -(O) 934(O) BCE(O) )(O) and(O) Neo-Assyrian(B-country) Empire(I-country) ((O) 911(O) BCE(O) -(O) 609(O) BCE(O) )(O) .(O)"}}
{"id": "640", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "election", "event", "politician", "location", "organization", "person", "political party"], "instance": {"id": "640", "words": ["This", "absolute", "loyalty", "began", "to", "break", "down", "during", "World", "War", "II", "when", "Vice-Presidents", "Henry", "A.", "Wallace", "and", "Harry", "S.", "Truman", "began", "to", "realize", "that", "a", "legacy", "of", "discrimination", "against", "blacks", "was", "a", "threat", "to", "the", "United", "States", "'", "image", "abroad", "and", "its", "ability", "to", "win", "the", "Cold", "War", "against", "the", "radically", "egalitarian", "rhetoric", "of", "Communism", ".Fredericksen", ",", "Karl", "A.", ";", "The", "Dixiecrat", "Revolt", "and", "the", "End", "of", "the", "Solid", "South", ",", "p", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "B-politician", "I-politician", "I-politician", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "B-person", "I-person", "O", "O", "B-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, event, politician, location, organization, person, political party and O.\nSentence: This absolute loyalty began to break down during World War II when Vice-Presidents Henry A. Wallace and Harry S. Truman began to realize that a legacy of discrimination against blacks was a threat to the United States ' image abroad and its ability to win the Cold War against the radically egalitarian rhetoric of Communism .Fredericksen , Karl A. ; The Dixiecrat Revolt and the End of the Solid South , p .", "prompt_labels": "This(O) absolute(O) loyalty(O) began(O) to(O) break(O) down(O) during(O) World(B-event) War(I-event) II(I-event) when(O) Vice-Presidents(O) Henry(B-politician) A.(I-politician) Wallace(I-politician) and(O) Harry(B-politician) S.(I-politician) Truman(I-politician) began(O) to(O) realize(O) that(O) a(O) legacy(O) of(O) discrimination(O) against(O) blacks(O) was(O) a(O) threat(O) to(O) the(O) United(B-country) States(I-country) '(O) image(O) abroad(O) and(O) its(O) ability(O) to(O) win(O) the(O) Cold(B-event) War(I-event) against(O) the(O) radically(O) egalitarian(O) rhetoric(O) of(O) Communism(O) .Fredericksen(B-person) ,(O) Karl(B-person) A.(I-person) ;(O) The(O) Dixiecrat(B-event) Revolt(I-event) and(O) the(O) End(B-event) of(I-event) the(I-event) Solid(I-event) South(I-event) ,(O) p(O) .(O)"}}
{"id": "54", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "election", "country", "political party", "politician", "location", "person", "event"], "instance": {"id": "54", "words": ["He", "initially", "supported", "the", "new", "government", "of", "Burhanuddin", "Rabbani", "in", "Kabul", "but", "in", "1994", "switched", "sides", "and", "allied", "with", "Gulbuddin", "Hekmatyar", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, election, country, political party, politician, location, person, event and O.\nSentence: He initially supported the new government of Burhanuddin Rabbani in Kabul but in 1994 switched sides and allied with Gulbuddin Hekmatyar .", "prompt_labels": "He(O) initially(O) supported(O) the(O) new(O) government(O) of(O) Burhanuddin(B-politician) Rabbani(I-politician) in(O) Kabul(B-location) but(O) in(O) 1994(O) switched(O) sides(O) and(O) allied(O) with(O) Gulbuddin(B-politician) Hekmatyar(I-politician) .(O)"}}
{"id": "576", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "election", "organization", "political party", "person", "location", "politician", "event"], "instance": {"id": "576", "words": ["The", "German", "city", "of", "Friedrichshafen", "was", "bombed", "during", "World", "War", "II", "as", "part", "of", "the", "Allied", "strategic", "bombing", "campaign", "against", "Nazi", "Germany", "war", "materiel", "industry", ",", "particularly", "in", "the", "targeting", "of", "German", "fighter", "aircraft", "production", "and", "long", "range", "missile", "development", "."], "labels": ["O", "O", "O", "O", "B-location", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, organization, political party, person, location, politician, event and O.\nSentence: The German city of Friedrichshafen was bombed during World War II as part of the Allied strategic bombing campaign against Nazi Germany war materiel industry , particularly in the targeting of German fighter aircraft production and long range missile development .", "prompt_labels": "The(O) German(O) city(O) of(O) Friedrichshafen(B-location) was(O) bombed(O) during(O) World(B-event) War(I-event) II(I-event) as(O) part(O) of(O) the(O) Allied(O) strategic(O) bombing(O) campaign(O) against(O) Nazi(B-country) Germany(I-country) war(O) materiel(O) industry(O) ,(O) particularly(O) in(O) the(O) targeting(O) of(O) German(O) fighter(O) aircraft(O) production(O) and(O) long(O) range(O) missile(O) development(O) .(O)"}}
{"id": "281", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "organization", "person", "election", "political party", "location", "country", "event"], "instance": {"id": "281", "words": ["Liebknecht", "was", "a", "member", "of", "the", "Independent", "Social", "Democratic", "Party", "of", "Germany", "(", "USPD", ")", ",", "opposed", "to", "the", "merger", "with", "the", "Communist", "Party", "of", "Germany", "and", "the", "joining", "of", "the", "Comintern", "but", "also", "to", "the", "reunification", "of", "the", "party", "with", "the", "Social", "Democratic", "Party", "of", "Germany", ",", "he", "continued", "the", "USPD", "as", "an", "independent", "party", "with", "Georg", "Ledebour", "until", "its", "merger", "into", "the", "Sozialistische", "Arbeiterpartei", "Deutschlands", "(", "SAPD", ",", "Socialist", "Worker", "'s", "Party", "of", "Germany", ")", "in", "1931", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-political party", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, person, election, political party, location, country, event and O.\nSentence: Liebknecht was a member of the Independent Social Democratic Party of Germany ( USPD ) , opposed to the merger with the Communist Party of Germany and the joining of the Comintern but also to the reunification of the party with the Social Democratic Party of Germany , he continued the USPD as an independent party with Georg Ledebour until its merger into the Sozialistische Arbeiterpartei Deutschlands ( SAPD , Socialist Worker 's Party of Germany ) in 1931 .", "prompt_labels": "Liebknecht(B-politician) was(O) a(O) member(O) of(O) the(O) Independent(B-political party) Social(I-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Germany(I-political party) ((O) USPD(B-political party) )(O) ,(O) opposed(O) to(O) the(O) merger(O) with(O) the(O) Communist(B-political party) Party(I-political party) of(I-political party) Germany(I-political party) and(O) the(O) joining(O) of(O) the(O) Comintern(B-organization) but(O) also(O) to(O) the(O) reunification(O) of(O) the(O) party(O) with(O) the(O) Social(B-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Germany(I-political party) ,(O) he(O) continued(O) the(O) USPD(B-political party) as(O) an(O) independent(O) party(O) with(O) Georg(B-politician) Ledebour(I-politician) until(O) its(O) merger(O) into(O) the(O) Sozialistische(B-political party) Arbeiterpartei(I-political party) Deutschlands(I-political party) ((O) SAPD(B-political party) ,(O) Socialist(B-political party) Worker(I-political party) 's(I-political party) Party(I-political party) of(I-political party) Germany(I-political party) )(O) in(O) 1931(O) .(O)"}}
{"id": "77", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "organization", "person", "location", "political party", "country", "election", "politician"], "instance": {"id": "77", "words": ["At", "the", "same", "time", ",", "Hungarian", "dominance", "faced", "challenges", "from", "the", "local", "majorities", "of", "Romanians", "in", "Transylvania", "and", "in", "the", "eastern", "Banat", ",", "Slovaks", "in", "today", "'s", "Slovakia", ",", "and", "Croats", "and", "Serbs", "in", "the", "crown", "lands", "of", "Croatia", "and", "of", "Dalmatia", "(", "today", "'s", "Croatia", ")", ",", "in", "Bosnia", "and", "Herzegovina", ",", "and", "in", "the", "provinces", "known", "as", "the", "Vojvodina", "(", "today", "'s", "northern", "Serbia", ")", "."], "labels": ["O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "B-location", "O", "B-country", "O", "O", "O", "B-country", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "B-location", "O", "O", "O", "B-country", "O", "O", "O", "B-country", "I-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "B-country", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, person, location, political party, country, election, politician and O.\nSentence: At the same time , Hungarian dominance faced challenges from the local majorities of Romanians in Transylvania and in the eastern Banat , Slovaks in today 's Slovakia , and Croats and Serbs in the crown lands of Croatia and of Dalmatia ( today 's Croatia ) , in Bosnia and Herzegovina , and in the provinces known as the Vojvodina ( today 's northern Serbia ) .", "prompt_labels": "At(O) the(O) same(O) time(O) ,(O) Hungarian(B-country) dominance(O) faced(O) challenges(O) from(O) the(O) local(O) majorities(O) of(O) Romanians(O) in(O) Transylvania(B-location) and(O) in(O) the(O) eastern(O) Banat(B-location) ,(O) Slovaks(B-country) in(O) today(O) 's(O) Slovakia(B-country) ,(O) and(O) Croats(B-country) and(O) Serbs(O) in(O) the(O) crown(O) lands(O) of(O) Croatia(B-country) and(O) of(O) Dalmatia(B-location) ((O) today(O) 's(O) Croatia(B-country) )(O) ,(O) in(O) Bosnia(B-country) and(I-country) Herzegovina(I-country) ,(O) and(O) in(O) the(O) provinces(O) known(O) as(O) the(O) Vojvodina(B-location) ((O) today(O) 's(O) northern(O) Serbia(B-country) )(O) .(O)"}}
{"id": "383", "dataset": "crossner_science", "split": "test", "label_list": ["country", "enzyme", "chemical element", "chemical compound", "discipline", "award", "university", "theory", "event", "organization", "person", "scientist", "academic journal", "location", "astronomical object", "protein"], "instance": {"id": "383", "words": ["After", "3", "years", "at", "King", "'s", "College", "London", "(", "contemporary", "with", "Rosalind", "Franklin", ")", "she", "moved", "to", "the", "University", "of", "Sheffield", "in", "1955", "as", "a", "demonstrator", "in", "the", "Biochemistry", "department", "(", "now", "Molecular", "Biology", "and", "Biotechnology", ")", ",", "obtaining", "an", "MRC", "grant", "to", "study", "the", "iron", "storage", "protein", "Ferritin", ",", "publishing", "preliminary", "X-ray", "diffraction", "data", "in", "the", "1st", "volume", "of", "the", "Journal", "of", "Molecular", "Biology", "in", "1959", "."], "labels": ["O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "B-discipline", "O", "O", "O", "B-discipline", "I-discipline", "O", "B-discipline", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, enzyme, chemical element, chemical compound, discipline, award, university, theory, event, organization, person, scientist, academic journal, location, astronomical object, protein and O.\nSentence: After 3 years at King 's College London ( contemporary with Rosalind Franklin ) she moved to the University of Sheffield in 1955 as a demonstrator in the Biochemistry department ( now Molecular Biology and Biotechnology ) , obtaining an MRC grant to study the iron storage protein Ferritin , publishing preliminary X-ray diffraction data in the 1st volume of the Journal of Molecular Biology in 1959 .", "prompt_labels": "After(O) 3(O) years(O) at(O) King(B-university) 's(I-university) College(I-university) London(I-university) ((O) contemporary(O) with(O) Rosalind(B-scientist) Franklin(I-scientist) )(O) she(O) moved(O) to(O) the(O) University(B-university) of(I-university) Sheffield(I-university) in(O) 1955(O) as(O) a(O) demonstrator(O) in(O) the(O) Biochemistry(B-discipline) department(O) ((O) now(O) Molecular(B-discipline) Biology(I-discipline) and(O) Biotechnology(B-discipline) )(O) ,(O) obtaining(O) an(O) MRC(B-organization) grant(O) to(O) study(O) the(O) iron(O) storage(O) protein(O) Ferritin(B-protein) ,(O) publishing(O) preliminary(O) X-ray(O) diffraction(O) data(O) in(O) the(O) 1st(O) volume(O) of(O) the(O) Journal(B-academic journal) of(I-academic journal) Molecular(I-academic journal) Biology(I-academic journal) in(O) 1959(O) .(O)"}}
{"id": "268", "dataset": "crossner_science", "split": "test", "label_list": ["scientist", "theory", "event", "person", "country", "chemical compound", "academic journal", "astronomical object", "university", "award", "protein", "location", "discipline", "organization", "chemical element", "enzyme"], "instance": {"id": "268", "words": ["Some", "known", "inhibitors", "of", "the", "caveolae", "pathway", "are", "Filipin", "III", ",", "Genistein", "and", "Nystatin", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, theory, event, person, country, chemical compound, academic journal, astronomical object, university, award, protein, location, discipline, organization, chemical element, enzyme and O.\nSentence: Some known inhibitors of the caveolae pathway are Filipin III , Genistein and Nystatin .", "prompt_labels": "Some(O) known(O) inhibitors(O) of(O) the(O) caveolae(O) pathway(O) are(O) Filipin(B-chemical compound) III(I-chemical compound) ,(O) Genistein(B-chemical compound) and(O) Nystatin(B-chemical compound) .(O)"}}
{"id": "385", "dataset": "crossner_science", "split": "test", "label_list": ["person", "award", "enzyme", "country", "chemical compound", "astronomical object", "university", "theory", "scientist", "organization", "chemical element", "location", "event", "academic journal", "protein", "discipline"], "instance": {"id": "385", "words": ["García", "studied", "law", ",", "first", "at", "the", "Pontifical", "Catholic", "University", "of", "Peru", "-although", "the", "official", "records", "of", "his", "tenure", "in", "this", "university", "were", "never", "found-", "and", "later", "earning", "a", "law", "degree", "from", "the", "National", "University", "of", "San", "Marcos", "in", "1971", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, enzyme, country, chemical compound, astronomical object, university, theory, scientist, organization, chemical element, location, event, academic journal, protein, discipline and O.\nSentence: García studied law , first at the Pontifical Catholic University of Peru -although the official records of his tenure in this university were never found- and later earning a law degree from the National University of San Marcos in 1971 .", "prompt_labels": "García(B-person) studied(O) law(O) ,(O) first(O) at(O) the(O) Pontifical(B-university) Catholic(I-university) University(I-university) of(I-university) Peru(I-university) -although(O) the(O) official(O) records(O) of(O) his(O) tenure(O) in(O) this(O) university(O) were(O) never(O) found-(O) and(O) later(O) earning(O) a(O) law(O) degree(O) from(O) the(O) National(B-university) University(I-university) of(I-university) San(I-university) Marcos(I-university) in(O) 1971(O) .(O)"}}
{"id": "181", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "location", "organization", "theory", "country", "chemical element", "university", "chemical compound", "enzyme", "protein", "award", "scientist", "person", "astronomical object", "event", "discipline"], "instance": {"id": "181", "words": ["Since", "uranium", "had", "been", "named", "after", "the", "planet", "Uranus", "and", "neptunium", "after", "the", "planet", "Neptune", ",", "element", "94", "was", "named", "after", "Pluto", ",", "which", "at", "the", "time", "was", "considered", "to", "be", "a", "planet", "as", "well", "."], "labels": ["O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, location, organization, theory, country, chemical element, university, chemical compound, enzyme, protein, award, scientist, person, astronomical object, event, discipline and O.\nSentence: Since uranium had been named after the planet Uranus and neptunium after the planet Neptune , element 94 was named after Pluto , which at the time was considered to be a planet as well .", "prompt_labels": "Since(O) uranium(B-astronomical object) had(O) been(O) named(O) after(O) the(O) planet(O) Uranus(B-astronomical object) and(O) neptunium(B-astronomical object) after(O) the(O) planet(O) Neptune(B-astronomical object) ,(O) element(O) 94(O) was(O) named(O) after(O) Pluto(B-astronomical object) ,(O) which(O) at(O) the(O) time(O) was(O) considered(O) to(O) be(O) a(O) planet(O) as(O) well(O) .(O)"}}
{"id": "103", "dataset": "crossner_science", "split": "test", "label_list": ["discipline", "event", "enzyme", "protein", "chemical compound", "scientist", "university", "country", "award", "astronomical object", "theory", "person", "academic journal", "organization", "chemical element", "location"], "instance": {"id": "103", "words": ["In", "this", "publication", "he", "proposed", "the", "names", "still", "used", "today", "for", "the", "seven", "then-known", "satellites", "of", "Saturn", ":", "Mimas", ",", "Enceladus", ",", "Tethys", ",", "Dione", ",", "Rhea", ",", "Titan", ",", "and", "Iapetus", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, event, enzyme, protein, chemical compound, scientist, university, country, award, astronomical object, theory, person, academic journal, organization, chemical element, location and O.\nSentence: In this publication he proposed the names still used today for the seven then-known satellites of Saturn : Mimas , Enceladus , Tethys , Dione , Rhea , Titan , and Iapetus .", "prompt_labels": "In(O) this(O) publication(O) he(O) proposed(O) the(O) names(O) still(O) used(O) today(O) for(O) the(O) seven(O) then-known(O) satellites(O) of(O) Saturn(B-astronomical object) :(O) Mimas(B-astronomical object) ,(O) Enceladus(B-astronomical object) ,(O) Tethys(B-astronomical object) ,(O) Dione(B-astronomical object) ,(O) Rhea(B-astronomical object) ,(O) Titan(B-astronomical object) ,(O) and(O) Iapetus(B-astronomical object) .(O)"}}
{"id": "311", "dataset": "crossner_science", "split": "test", "label_list": ["discipline", "astronomical object", "enzyme", "person", "chemical element", "event", "award", "academic journal", "location", "chemical compound", "protein", "theory", "scientist", "organization", "university", "country"], "instance": {"id": "311", "words": ["1", "The", "eight", "planets", "are", ":", "Mercury", ",", "Venus", ",", "Earth", ",", "Mars", ",", "Jupiter", ",", "Saturn", ",", "Uranus", ",", "and", "Neptune", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, astronomical object, enzyme, person, chemical element, event, award, academic journal, location, chemical compound, protein, theory, scientist, organization, university, country and O.\nSentence: 1 The eight planets are : Mercury , Venus , Earth , Mars , Jupiter , Saturn , Uranus , and Neptune .", "prompt_labels": "1(O) The(O) eight(O) planets(O) are(O) :(O) Mercury(B-astronomical object) ,(O) Venus(B-astronomical object) ,(O) Earth(B-astronomical object) ,(O) Mars(B-astronomical object) ,(O) Jupiter(B-astronomical object) ,(O) Saturn(B-astronomical object) ,(O) Uranus(B-astronomical object) ,(O) and(O) Neptune(B-astronomical object) .(O)"}}
{"id": "461", "dataset": "crossner_science", "split": "test", "label_list": ["location", "university", "event", "person", "chemical element", "country", "enzyme", "discipline", "astronomical object", "award", "academic journal", "scientist", "organization", "chemical compound", "protein", "theory"], "instance": {"id": "461", "words": ["A", "similar", "example", "is", "given", "by", "the", "Senegalese", "sole", "(", "Solea", "senegalensis", ")", ",", "which", ",", "when", "acclimated", "to", "temperatures", "of", "26", "°", "C", ",", "produced", "a", "significantly", "higher", "amount", "of", "taurine", ",", "Glutamic", "acid", ",", "Gamma-Aminobutyric", "acid", "and", "glycine", "compared", "to", "acclimation", "to", "12", "°", "C.", "This", "may", "mean", "that", "the", "aforementioned", "compounds", "aid", "in", "antioxidant", "defense", ",", "osmoregulatory", "processes", ",", "or", "energetic", "purposes", "at", "these", "temperatures", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-theory", "I-theory", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, university, event, person, chemical element, country, enzyme, discipline, astronomical object, award, academic journal, scientist, organization, chemical compound, protein, theory and O.\nSentence: A similar example is given by the Senegalese sole ( Solea senegalensis ) , which , when acclimated to temperatures of 26 ° C , produced a significantly higher amount of taurine , Glutamic acid , Gamma-Aminobutyric acid and glycine compared to acclimation to 12 ° C. This may mean that the aforementioned compounds aid in antioxidant defense , osmoregulatory processes , or energetic purposes at these temperatures .", "prompt_labels": "A(O) similar(O) example(O) is(O) given(O) by(O) the(O) Senegalese(O) sole(O) ((O) Solea(O) senegalensis(O) )(O) ,(O) which(O) ,(O) when(O) acclimated(O) to(O) temperatures(O) of(O) 26(O) °(O) C(O) ,(O) produced(O) a(O) significantly(O) higher(O) amount(O) of(O) taurine(B-chemical compound) ,(O) Glutamic(B-chemical compound) acid(I-chemical compound) ,(O) Gamma-Aminobutyric(B-chemical compound) acid(I-chemical compound) and(O) glycine(B-chemical compound) compared(O) to(O) acclimation(O) to(O) 12(O) °(O) C.(O) This(O) may(O) mean(O) that(O) the(O) aforementioned(O) compounds(O) aid(O) in(O) antioxidant(B-theory) defense(I-theory) ,(O) osmoregulatory(O) processes(O) ,(O) or(O) energetic(O) purposes(O) at(O) these(O) temperatures(O) .(O)"}}
{"id": "7", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "organization", "astronomical object", "university", "chemical compound", "person", "country", "scientist", "enzyme", "award", "theory", "event", "protein", "location", "chemical element", "discipline"], "instance": {"id": "7", "words": ["Research", "indicates", "that", "the", "majority", "of", "variation", "in", "coat", "growth", "pattern", ",", "length", "and", "curl", "can", "be", "attributed", "to", "mutations", "in", "four", "genes", ",", "the", "R-spondin-2", "gene", "or", "RSPO2", ",", "the", "fibroblast", "growth", "factor-5", "gene", "or", "FGF5", ",", "the", "KRT71", "gene", "or", "KRT71", "and", "the", "melanocortin", "5", "receptor", "gene", "(", "MC5R", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "O", "B-protein", "O", "O", "B-protein", "I-protein", "I-protein", "O", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "I-protein", "O", "O", "B-protein", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, organization, astronomical object, university, chemical compound, person, country, scientist, enzyme, award, theory, event, protein, location, chemical element, discipline and O.\nSentence: Research indicates that the majority of variation in coat growth pattern , length and curl can be attributed to mutations in four genes , the R-spondin-2 gene or RSPO2 , the fibroblast growth factor-5 gene or FGF5 , the KRT71 gene or KRT71 and the melanocortin 5 receptor gene ( MC5R ) .", "prompt_labels": "Research(O) indicates(O) that(O) the(O) majority(O) of(O) variation(O) in(O) coat(O) growth(O) pattern(O) ,(O) length(O) and(O) curl(O) can(O) be(O) attributed(O) to(O) mutations(O) in(O) four(O) genes(O) ,(O) the(O) R-spondin-2(B-protein) gene(O) or(O) RSPO2(B-protein) ,(O) the(O) fibroblast(B-protein) growth(I-protein) factor-5(I-protein) gene(O) or(O) FGF5(B-protein) ,(O) the(O) KRT71(O) gene(O) or(O) KRT71(O) and(O) the(O) melanocortin(B-protein) 5(I-protein) receptor(I-protein) gene(O) ((O) MC5R(B-protein) )(O) .(O)"}}
{"id": "84", "dataset": "crossner_science", "split": "test", "label_list": ["theory", "university", "country", "academic journal", "award", "organization", "scientist", "discipline", "chemical element", "protein", "enzyme", "person", "event", "chemical compound", "astronomical object", "location"], "instance": {"id": "84", "words": ["He", "competed", "in", "Swimming", "at", "the", "1988", "Summer", "Olympics", "at", "the", "1988", "Summer", "Olympics", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, university, country, academic journal, award, organization, scientist, discipline, chemical element, protein, enzyme, person, event, chemical compound, astronomical object, location and O.\nSentence: He competed in Swimming at the 1988 Summer Olympics at the 1988 Summer Olympics .", "prompt_labels": "He(O) competed(O) in(O) Swimming(O) at(O) the(O) 1988(B-event) Summer(I-event) Olympics(I-event) at(O) the(O) 1988(B-event) Summer(I-event) Olympics(I-event) .(O)"}}
{"id": "457", "dataset": "crossner_science", "split": "test", "label_list": ["astronomical object", "organization", "award", "discipline", "country", "protein", "chemical element", "location", "theory", "university", "person", "academic journal", "enzyme", "scientist", "chemical compound", "event"], "instance": {"id": "457", "words": ["Repeated", "cocaine", "administration", "in", "mice", "induces", "post-translational", "modifications", "including", "hyperacetylation", "of", "Histone", "H3", "or", "Histone", "H4", "at", "1,696", "genes", "in", "one", "brain", "reward", "region", "the", "nucleus", "accumbens", "and", "deacetylation", "at", "206", "genes", "."], "labels": ["O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "O", "B-protein", "I-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, organization, award, discipline, country, protein, chemical element, location, theory, university, person, academic journal, enzyme, scientist, chemical compound, event and O.\nSentence: Repeated cocaine administration in mice induces post-translational modifications including hyperacetylation of Histone H3 or Histone H4 at 1,696 genes in one brain reward region the nucleus accumbens and deacetylation at 206 genes .", "prompt_labels": "Repeated(O) cocaine(B-chemical compound) administration(O) in(O) mice(O) induces(O) post-translational(O) modifications(O) including(O) hyperacetylation(O) of(O) Histone(B-protein) H3(I-protein) or(O) Histone(B-protein) H4(I-protein) at(O) 1,696(O) genes(O) in(O) one(O) brain(O) reward(O) region(O) the(O) nucleus(O) accumbens(O) and(O) deacetylation(O) at(O) 206(O) genes(O) .(O)"}}
{"id": "226", "dataset": "crossner_science", "split": "test", "label_list": ["scientist", "protein", "enzyme", "astronomical object", "award", "location", "academic journal", "university", "person", "country", "discipline", "chemical element", "event", "chemical compound", "organization", "theory"], "instance": {"id": "226", "words": ["The", "recurring", "characters", "of", "Adele", "Webber", "(", "Loretta", "Devine", ")", ",", "Finn", "Dandrige", "(", "Chris", "O", "'Donnell", ")", ",", "Dr.", "Ellis", "Grey", "(", "Kate", "Burton", ")", "and", "Olivia", "Harper", "(", "Sarah", "Utterback", ")", "were", "portrayed", "with", "guest", "star", "billing", "."], "labels": ["O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, protein, enzyme, astronomical object, award, location, academic journal, university, person, country, discipline, chemical element, event, chemical compound, organization, theory and O.\nSentence: The recurring characters of Adele Webber ( Loretta Devine ) , Finn Dandrige ( Chris O 'Donnell ) , Dr. Ellis Grey ( Kate Burton ) and Olivia Harper ( Sarah Utterback ) were portrayed with guest star billing .", "prompt_labels": "The(O) recurring(O) characters(O) of(O) Adele(B-person) Webber(I-person) ((O) Loretta(B-person) Devine(I-person) )(O) ,(O) Finn(B-person) Dandrige(I-person) ((O) Chris(B-person) O(I-person) 'Donnell(I-person) )(O) ,(O) Dr.(B-person) Ellis(I-person) Grey(I-person) ((O) Kate(B-person) Burton(I-person) )(O) and(O) Olivia(B-person) Harper(I-person) ((O) Sarah(B-person) Utterback(I-person) )(O) were(O) portrayed(O) with(O) guest(O) star(O) billing(O) .(O)"}}
{"id": "245", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "theory", "enzyme", "discipline", "location", "event", "chemical element", "award", "chemical compound", "country", "protein", "organization", "person", "university", "astronomical object", "scientist"], "instance": {"id": "245", "words": ["On", "August", "27", ",", "Mercury", "and", "Venus", "were", "in", "conjunction", ",", "followed", "by", "a", "conjunction", "of", "Venus", "and", "Jupiter", ",", "meaning", "that", "the", "three", "planets", "were", "very", "close", "together", "in", "the", "evening", "sky", "."], "labels": ["O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, theory, enzyme, discipline, location, event, chemical element, award, chemical compound, country, protein, organization, person, university, astronomical object, scientist and O.\nSentence: On August 27 , Mercury and Venus were in conjunction , followed by a conjunction of Venus and Jupiter , meaning that the three planets were very close together in the evening sky .", "prompt_labels": "On(O) August(O) 27(O) ,(O) Mercury(B-astronomical object) and(O) Venus(B-astronomical object) were(O) in(O) conjunction(O) ,(O) followed(O) by(O) a(O) conjunction(O) of(O) Venus(B-astronomical object) and(O) Jupiter(B-astronomical object) ,(O) meaning(O) that(O) the(O) three(O) planets(O) were(O) very(O) close(O) together(O) in(O) the(O) evening(O) sky(O) .(O)"}}
{"id": "365", "dataset": "crossner_science", "split": "test", "label_list": ["country", "organization", "protein", "enzyme", "scientist", "chemical element", "award", "person", "event", "location", "astronomical object", "theory", "university", "discipline", "chemical compound", "academic journal"], "instance": {"id": "365", "words": ["In", "1987", ",", "he", "discovered", "the", "DNA", "sequence", "of", "CRISPR", ","], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, protein, enzyme, scientist, chemical element, award, person, event, location, astronomical object, theory, university, discipline, chemical compound, academic journal and O.\nSentence: In 1987 , he discovered the DNA sequence of CRISPR ,", "prompt_labels": "In(O) 1987(O) ,(O) he(O) discovered(O) the(O) DNA(O) sequence(O) of(O) CRISPR(O) ,(O)"}}
{"id": "39", "dataset": "crossner_science", "split": "test", "label_list": ["location", "protein", "theory", "scientist", "chemical compound", "organization", "astronomical object", "enzyme", "country", "event", "university", "academic journal", "discipline", "person", "chemical element", "award"], "instance": {"id": "39", "words": ["On", "screen", "she", "has", "starred", "in", "scores", "of", "films", "and", "is", "a", "six-time", "Oscar", "nominee", ",", "winning", "the", "Academy", "Award", "for", "Best", "Supporting", "Actress", "for", "the", "title", "role", "in", "the", "film", "Julia", "(", "1977", ")", ".", "1977", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, protein, theory, scientist, chemical compound, organization, astronomical object, enzyme, country, event, university, academic journal, discipline, person, chemical element, award and O.\nSentence: On screen she has starred in scores of films and is a six-time Oscar nominee , winning the Academy Award for Best Supporting Actress for the title role in the film Julia ( 1977 ) . 1977 ) .", "prompt_labels": "On(O) screen(O) she(O) has(O) starred(O) in(O) scores(O) of(O) films(O) and(O) is(O) a(O) six-time(O) Oscar(B-award) nominee(O) ,(O) winning(O) the(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Supporting(I-award) Actress(I-award) for(O) the(O) title(O) role(O) in(O) the(O) film(O) Julia(O) ((O) 1977(O) )(O) .(O) 1977(O) )(O) .(O)"}}
{"id": "427", "dataset": "crossner_science", "split": "test", "label_list": ["astronomical object", "academic journal", "award", "person", "protein", "event", "location", "country", "enzyme", "scientist", "chemical element", "theory", "discipline", "university", "chemical compound", "organization"], "instance": {"id": "427", "words": ["This", "includes", "Pluto", ",", "which", "is", "constrained", "in", "its", "orbit", "by", "the", "gravity", "of", "Neptune", "and", "shares", "its", "orbital", "neighbourhood", "with", "many", "Kuiper", "belt", "objects", "."], "labels": ["O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, academic journal, award, person, protein, event, location, country, enzyme, scientist, chemical element, theory, discipline, university, chemical compound, organization and O.\nSentence: This includes Pluto , which is constrained in its orbit by the gravity of Neptune and shares its orbital neighbourhood with many Kuiper belt objects .", "prompt_labels": "This(O) includes(O) Pluto(B-astronomical object) ,(O) which(O) is(O) constrained(O) in(O) its(O) orbit(O) by(O) the(O) gravity(O) of(O) Neptune(B-astronomical object) and(O) shares(O) its(O) orbital(O) neighbourhood(O) with(O) many(O) Kuiper(O) belt(O) objects(O) .(O)"}}
{"id": "471", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "astronomical object", "protein", "scientist", "organization", "university", "theory", "person", "chemical compound", "event", "country", "academic journal", "award", "discipline", "location", "enzyme"], "instance": {"id": "471", "words": ["Oxidants", "are", "usually", "chemical", "substances", "with", "elements", "in", "high", "oxidation", "states", ",", "or", "else", "highly", "electronegative", "elements", "(", "Osub2", "/", "sub", ",", "Fluorine", ",", "Chlorine", ",", "Bromine", ")", "that", "can", "gain", "extra", "electrons", "by", "oxidizing", "another", "substance", "."], "labels": ["B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "I-chemical compound", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, astronomical object, protein, scientist, organization, university, theory, person, chemical compound, event, country, academic journal, award, discipline, location, enzyme and O.\nSentence: Oxidants are usually chemical substances with elements in high oxidation states , or else highly electronegative elements ( Osub2 / sub , Fluorine , Chlorine , Bromine ) that can gain extra electrons by oxidizing another substance .", "prompt_labels": "Oxidants(B-chemical compound) are(O) usually(O) chemical(O) substances(O) with(O) elements(O) in(O) high(O) oxidation(O) states(O) ,(O) or(O) else(O) highly(O) electronegative(O) elements(O) ((O) Osub2(B-chemical compound) /(I-chemical compound) sub(I-chemical compound) ,(O) Fluorine(B-chemical element) ,(O) Chlorine(B-chemical element) ,(O) Bromine(B-chemical element) )(O) that(O) can(O) gain(O) extra(O) electrons(O) by(O) oxidizing(O) another(O) substance(O) .(O)"}}
{"id": "405", "dataset": "crossner_science", "split": "test", "label_list": ["enzyme", "organization", "discipline", "theory", "award", "academic journal", "chemical compound", "event", "protein", "location", "university", "astronomical object", "chemical element", "country", "person", "scientist"], "instance": {"id": "405", "words": ["The", "hydroxyl", "radical", "can", "damage", "virtually", "all", "types", "of", "macromolecules", ":", "carbohydrates", ",", "nucleic", "acids", "(", "mutation", "s", ")", ",", "lipids", "(", "lipid", "peroxidation", ")", ",", "and", "amino", "acids", "(", "e.g.", "conversion", "of", "Phenylalanine", "to", "m", "-", "Tyrosine", "and", "o", "-", "Tyrosine", ")", "."], "labels": ["O", "B-chemical compound", "I-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "O", "B-chemical compound", "I-chemical compound", "O", "O", "O", "O", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "O", "O", "O", "B-chemical compound", "O", "B-chemical compound", "I-chemical compound", "I-chemical compound", "O", "B-chemical compound", "I-chemical compound", "I-chemical compound", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, organization, discipline, theory, award, academic journal, chemical compound, event, protein, location, university, astronomical object, chemical element, country, person, scientist and O.\nSentence: The hydroxyl radical can damage virtually all types of macromolecules : carbohydrates , nucleic acids ( mutation s ) , lipids ( lipid peroxidation ) , and amino acids ( e.g. conversion of Phenylalanine to m - Tyrosine and o - Tyrosine ) .", "prompt_labels": "The(O) hydroxyl(B-chemical compound) radical(I-chemical compound) can(O) damage(O) virtually(O) all(O) types(O) of(O) macromolecules(O) :(O) carbohydrates(B-chemical compound) ,(O) nucleic(B-chemical compound) acids(I-chemical compound) ((O) mutation(O) s(O) )(O) ,(O) lipids(B-chemical compound) ((O) lipid(O) peroxidation(O) )(O) ,(O) and(O) amino(B-chemical compound) acids(I-chemical compound) ((O) e.g.(O) conversion(O) of(O) Phenylalanine(B-chemical compound) to(O) m(B-chemical compound) -(I-chemical compound) Tyrosine(I-chemical compound) and(O) o(B-chemical compound) -(I-chemical compound) Tyrosine(I-chemical compound) )(O) .(O)"}}
{"id": "31", "dataset": "crossner_science", "split": "test", "label_list": ["organization", "person", "chemical compound", "astronomical object", "event", "theory", "university", "location", "protein", "scientist", "chemical element", "discipline", "country", "enzyme", "academic journal", "award"], "instance": {"id": "31", "words": ["A", "second", "meeting", "was", "held", "soon", "thereafter", "and", "included", "Klaus", "Clusius", ",", "Robert", "Döpel", ",", "Werner", "Heisenberg", ",", "and", "Carl", "Friedrich", "von", "Weizsäcker", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, chemical compound, astronomical object, event, theory, university, location, protein, scientist, chemical element, discipline, country, enzyme, academic journal, award and O.\nSentence: A second meeting was held soon thereafter and included Klaus Clusius , Robert Döpel , Werner Heisenberg , and Carl Friedrich von Weizsäcker .", "prompt_labels": "A(O) second(O) meeting(O) was(O) held(O) soon(O) thereafter(O) and(O) included(O) Klaus(B-scientist) Clusius(I-scientist) ,(O) Robert(B-scientist) Döpel(I-scientist) ,(O) Werner(B-scientist) Heisenberg(I-scientist) ,(O) and(O) Carl(B-scientist) Friedrich(I-scientist) von(I-scientist) Weizsäcker(I-scientist) .(O)"}}
{"id": "194", "dataset": "crossner_science", "split": "test", "label_list": ["event", "astronomical object", "organization", "protein", "enzyme", "scientist", "discipline", "person", "location", "university", "award", "theory", "country", "chemical compound", "chemical element", "academic journal"], "instance": {"id": "194", "words": ["The", "Copenhagen", "interpretation", "is", "an", "expression", "of", "the", "meaning", "of", "quantum", "mechanics", "that", "was", "largely", "devised", "from", "1925", "to", "1927", "by", "Niels", "Bohr", "and", "Werner", "Heisenberg", "."], "labels": ["O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "B-discipline", "I-discipline", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, astronomical object, organization, protein, enzyme, scientist, discipline, person, location, university, award, theory, country, chemical compound, chemical element, academic journal and O.\nSentence: The Copenhagen interpretation is an expression of the meaning of quantum mechanics that was largely devised from 1925 to 1927 by Niels Bohr and Werner Heisenberg .", "prompt_labels": "The(O) Copenhagen(B-location) interpretation(O) is(O) an(O) expression(O) of(O) the(O) meaning(O) of(O) quantum(B-discipline) mechanics(I-discipline) that(O) was(O) largely(O) devised(O) from(O) 1925(O) to(O) 1927(O) by(O) Niels(B-scientist) Bohr(I-scientist) and(O) Werner(B-scientist) Heisenberg(I-scientist) .(O)"}}
{"id": "296", "dataset": "crossner_science", "split": "test", "label_list": ["location", "theory", "university", "academic journal", "country", "event", "person", "enzyme", "discipline", "scientist", "chemical element", "chemical compound", "award", "organization", "protein", "astronomical object"], "instance": {"id": "296", "words": ["Manchester", "hosted", "the", "2002", "Commonwealth", "Games", ",", "which", "left", "it", "a", "legacy", "of", "sporting", "facilities", "including", "the", "City", "of", "Manchester", "Stadium", ",", "Manchester", "Aquatics", "Centre", "and", "the", "National", "Cycling", "Centre", ",", "headquarters", "of", "British", "Cycling", "."], "labels": ["B-person", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "B-location", "I-location", "I-location", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, theory, university, academic journal, country, event, person, enzyme, discipline, scientist, chemical element, chemical compound, award, organization, protein, astronomical object and O.\nSentence: Manchester hosted the 2002 Commonwealth Games , which left it a legacy of sporting facilities including the City of Manchester Stadium , Manchester Aquatics Centre and the National Cycling Centre , headquarters of British Cycling .", "prompt_labels": "Manchester(B-person) hosted(O) the(O) 2002(B-event) Commonwealth(I-event) Games(I-event) ,(O) which(O) left(O) it(O) a(O) legacy(O) of(O) sporting(O) facilities(O) including(O) the(O) City(B-location) of(I-location) Manchester(I-location) Stadium(I-location) ,(O) Manchester(B-location) Aquatics(I-location) Centre(I-location) and(O) the(O) National(B-location) Cycling(I-location) Centre(I-location) ,(O) headquarters(O) of(O) British(B-organization) Cycling(I-organization) .(O)"}}
{"id": "189", "dataset": "crossner_science", "split": "test", "label_list": ["award", "scientist", "chemical compound", "person", "event", "university", "protein", "academic journal", "chemical element", "location", "theory", "organization", "country", "enzyme", "astronomical object", "discipline"], "instance": {"id": "189", "words": ["He", "was", "awarded", "the", "Louisa", "Gross", "Horwitz", "Prize", "from", "Columbia", "University", "in", "1991", ",", "the", "Louis-Jeantet", "Prize", "for", "Medicine", "in", "1993", ",", "the", "Otto", "Warburg", "Medal", "in", "1999", "and", "half", "of", "the", "Nobel", "Prize", "in", "Chemistry", "in", "2002", "for", "his", "development", "of", "nuclear", "magnetic", "resonance", "spectroscopy", "for", "determining", "the", "three-dimensional", "structure", "of", "biological", "macromolecules", "in", "solution", "."], "labels": ["O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "B-university", "I-university", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, scientist, chemical compound, person, event, university, protein, academic journal, chemical element, location, theory, organization, country, enzyme, astronomical object, discipline and O.\nSentence: He was awarded the Louisa Gross Horwitz Prize from Columbia University in 1991 , the Louis-Jeantet Prize for Medicine in 1993 , the Otto Warburg Medal in 1999 and half of the Nobel Prize in Chemistry in 2002 for his development of nuclear magnetic resonance spectroscopy for determining the three-dimensional structure of biological macromolecules in solution .", "prompt_labels": "He(O) was(O) awarded(O) the(O) Louisa(B-award) Gross(I-award) Horwitz(I-award) Prize(I-award) from(O) Columbia(B-university) University(I-university) in(O) 1991(O) ,(O) the(O) Louis-Jeantet(B-award) Prize(I-award) for(I-award) Medicine(I-award) in(O) 1993(O) ,(O) the(O) Otto(B-award) Warburg(I-award) Medal(I-award) in(O) 1999(O) and(O) half(O) of(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Chemistry(I-award) in(O) 2002(O) for(O) his(O) development(O) of(O) nuclear(B-award) magnetic(I-award) resonance(I-award) spectroscopy(I-award) for(O) determining(O) the(O) three-dimensional(O) structure(O) of(O) biological(O) macromolecules(O) in(O) solution(O) .(O)"}}
{"id": "47", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "university", "award", "chemical element", "discipline", "country", "organization", "astronomical object", "location", "chemical compound", "scientist", "person", "event", "theory", "enzyme", "protein"], "instance": {"id": "47", "words": ["The", "group", "included", "the", "physicists", "Walther", "Bothe", ",", "Robert", "Döpel", ",", "Hans", "Geiger", ",", "Wolfgang", "Gentner", "(", "probably", "sent", "by", "Walther", "Bothe", ")", ",", "Wilhelm", "Hanle", ",", "Gerhard", "Hoffmann", ",", "and", "Georg", "Joos", ";", "Peter", "Debye", "was", "invited", ",", "but", "he", "did", "not", "attend", "."], "labels": ["O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, university, award, chemical element, discipline, country, organization, astronomical object, location, chemical compound, scientist, person, event, theory, enzyme, protein and O.\nSentence: The group included the physicists Walther Bothe , Robert Döpel , Hans Geiger , Wolfgang Gentner ( probably sent by Walther Bothe ) , Wilhelm Hanle , Gerhard Hoffmann , and Georg Joos ; Peter Debye was invited , but he did not attend .", "prompt_labels": "The(O) group(O) included(O) the(O) physicists(O) Walther(B-scientist) Bothe(I-scientist) ,(O) Robert(B-scientist) Döpel(I-scientist) ,(O) Hans(B-scientist) Geiger(I-scientist) ,(O) Wolfgang(B-scientist) Gentner(I-scientist) ((O) probably(O) sent(O) by(O) Walther(B-scientist) Bothe(I-scientist) )(O) ,(O) Wilhelm(B-scientist) Hanle(I-scientist) ,(O) Gerhard(B-scientist) Hoffmann(I-scientist) ,(O) and(O) Georg(B-scientist) Joos(I-scientist) ;(O) Peter(B-scientist) Debye(I-scientist) was(O) invited(O) ,(O) but(O) he(O) did(O) not(O) attend(O) .(O)"}}
{"id": "350", "dataset": "crossner_science", "split": "test", "label_list": ["country", "discipline", "location", "event", "award", "academic journal", "enzyme", "university", "astronomical object", "scientist", "protein", "chemical element", "organization", "theory", "person", "chemical compound"], "instance": {"id": "350", "words": ["Chinese", "Physics", "Letters", "is", "a", "part", "of", "a", "small", "group", "of", "four", "journals", "from", "the", "Chinese", "Physical", "Society", ",", "the", "other", "three", "are", ":", "Communications", "in", "Theoretical", "Physics", "(", "in", "English", ",", "subtitled", "Chinese", "Physics", "A", ")", ",", "Chinese", "Physics", "B", "(", "in", "English", ")", ",", "and", "Chinese", "Physics", "C", "(", "in", "English", ")", "."], "labels": ["B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, discipline, location, event, award, academic journal, enzyme, university, astronomical object, scientist, protein, chemical element, organization, theory, person, chemical compound and O.\nSentence: Chinese Physics Letters is a part of a small group of four journals from the Chinese Physical Society , the other three are : Communications in Theoretical Physics ( in English , subtitled Chinese Physics A ) , Chinese Physics B ( in English ) , and Chinese Physics C ( in English ) .", "prompt_labels": "Chinese(B-academic journal) Physics(I-academic journal) Letters(I-academic journal) is(O) a(O) part(O) of(O) a(O) small(O) group(O) of(O) four(O) journals(O) from(O) the(O) Chinese(B-organization) Physical(I-organization) Society(I-organization) ,(O) the(O) other(O) three(O) are(O) :(O) Communications(B-academic journal) in(I-academic journal) Theoretical(I-academic journal) Physics(I-academic journal) ((O) in(O) English(O) ,(O) subtitled(O) Chinese(B-academic journal) Physics(I-academic journal) A(I-academic journal) )(O) ,(O) Chinese(B-academic journal) Physics(I-academic journal) B(I-academic journal) ((O) in(O) English(O) )(O) ,(O) and(O) Chinese(B-academic journal) Physics(I-academic journal) C(I-academic journal) ((O) in(O) English(O) )(O) .(O)"}}
{"id": "3", "dataset": "crossner_science", "split": "test", "label_list": ["chemical compound", "event", "academic journal", "location", "chemical element", "award", "protein", "astronomical object", "theory", "university", "person", "country", "enzyme", "discipline", "scientist", "organization"], "instance": {"id": "3", "words": ["Viral", "TK", "phosphorylates", "aciclovir", "into", "its", "monophosphate", "form", ",", "which", "is", "subsequently", "phosphorylated", "to", "active", "aciclovir", "triphoshate", "by", "cellular", "kinase", "s", ",", "thus", "selectively", "inhibiting", "viral", "DNA", "polymerase", "."], "labels": ["B-enzyme", "I-enzyme", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "B-enzyme", "I-enzyme", "O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, event, academic journal, location, chemical element, award, protein, astronomical object, theory, university, person, country, enzyme, discipline, scientist, organization and O.\nSentence: Viral TK phosphorylates aciclovir into its monophosphate form , which is subsequently phosphorylated to active aciclovir triphoshate by cellular kinase s , thus selectively inhibiting viral DNA polymerase .", "prompt_labels": "Viral(B-enzyme) TK(I-enzyme) phosphorylates(O) aciclovir(O) into(O) its(O) monophosphate(O) form(O) ,(O) which(O) is(O) subsequently(O) phosphorylated(O) to(O) active(O) aciclovir(B-chemical compound) triphoshate(I-chemical compound) by(O) cellular(B-enzyme) kinase(I-enzyme) s(O) ,(O) thus(O) selectively(O) inhibiting(O) viral(O) DNA(B-enzyme) polymerase(I-enzyme) .(O)"}}
{"id": "49", "dataset": "crossner_science", "split": "test", "label_list": ["astronomical object", "country", "discipline", "theory", "protein", "scientist", "academic journal", "award", "university", "chemical element", "location", "organization", "chemical compound", "enzyme", "person", "event"], "instance": {"id": "49", "words": ["During", "his", "academic", "career", "he", "served", "on", "the", "editorial", "boards", "of", "the", "Journal", "of", "Rational", "Mechanics", "and", "Analysis", ",", "Archive", "for", "Rational", "Mechanics", "and", "Analysis", ",", "Journal", "of", "Elasticity", ",", "and", "the", "International", "Journal", "of", "Solids", "and", "Structures", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, country, discipline, theory, protein, scientist, academic journal, award, university, chemical element, location, organization, chemical compound, enzyme, person, event and O.\nSentence: During his academic career he served on the editorial boards of the Journal of Rational Mechanics and Analysis , Archive for Rational Mechanics and Analysis , Journal of Elasticity , and the International Journal of Solids and Structures .", "prompt_labels": "During(O) his(O) academic(O) career(O) he(O) served(O) on(O) the(O) editorial(O) boards(O) of(O) the(O) Journal(B-academic journal) of(I-academic journal) Rational(I-academic journal) Mechanics(I-academic journal) and(I-academic journal) Analysis(I-academic journal) ,(O) Archive(B-academic journal) for(I-academic journal) Rational(I-academic journal) Mechanics(I-academic journal) and(I-academic journal) Analysis(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Elasticity(I-academic journal) ,(O) and(O) the(O) International(B-academic journal) Journal(I-academic journal) of(I-academic journal) Solids(I-academic journal) and(I-academic journal) Structures(I-academic journal) .(O)"}}
{"id": "447", "dataset": "crossner_science", "split": "test", "label_list": ["award", "person", "scientist", "discipline", "protein", "university", "chemical compound", "theory", "location", "enzyme", "organization", "astronomical object", "event", "academic journal", "chemical element", "country"], "instance": {"id": "447", "words": ["The", "group", "included", "the", "physicists", "Walther", "Bothe", ",", "Robert", "Döpel", ",", "Hans", "Geiger", ",", "Wolfgang", "Gentner", ",", "Wilhelm", "Hanle", ",", "Gerhard", "Hoffmann", ",", "and", "Joos", "."], "labels": ["O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, scientist, discipline, protein, university, chemical compound, theory, location, enzyme, organization, astronomical object, event, academic journal, chemical element, country and O.\nSentence: The group included the physicists Walther Bothe , Robert Döpel , Hans Geiger , Wolfgang Gentner , Wilhelm Hanle , Gerhard Hoffmann , and Joos .", "prompt_labels": "The(O) group(O) included(O) the(O) physicists(O) Walther(B-scientist) Bothe(I-scientist) ,(O) Robert(B-scientist) Döpel(I-scientist) ,(O) Hans(B-scientist) Geiger(I-scientist) ,(O) Wolfgang(B-scientist) Gentner(I-scientist) ,(O) Wilhelm(B-scientist) Hanle(I-scientist) ,(O) Gerhard(B-scientist) Hoffmann(I-scientist) ,(O) and(O) Joos(B-scientist) .(O)"}}
{"id": "398", "dataset": "crossner_science", "split": "test", "label_list": ["country", "theory", "astronomical object", "event", "organization", "protein", "scientist", "chemical compound", "academic journal", "discipline", "person", "enzyme", "award", "university", "chemical element", "location"], "instance": {"id": "398", "words": ["The", "boy", "was", "sent", "to", "school", "at", "the", "Lycée", "Louis-le-Grand", ",", "part", "of", "the", "University", "of", "Paris", ",", "and", "it", "was", "decided", "that", "he", "would", "carry", "on", "the", "family", "martial", "tradition.", "The", "comte", ",", "the", "boy", "'s", "great-grandfather", ",", "enrolled", "the", "boy", "in", "a", "program", "to", "train", "future", "Musketeers", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, theory, astronomical object, event, organization, protein, scientist, chemical compound, academic journal, discipline, person, enzyme, award, university, chemical element, location and O.\nSentence: The boy was sent to school at the Lycée Louis-le-Grand , part of the University of Paris , and it was decided that he would carry on the family martial tradition. The comte , the boy 's great-grandfather , enrolled the boy in a program to train future Musketeers .", "prompt_labels": "The(O) boy(O) was(O) sent(O) to(O) school(O) at(O) the(O) Lycée(B-organization) Louis-le-Grand(I-organization) ,(O) part(O) of(O) the(O) University(B-university) of(I-university) Paris(I-university) ,(O) and(O) it(O) was(O) decided(O) that(O) he(O) would(O) carry(O) on(O) the(O) family(O) martial(O) tradition.(O) The(O) comte(O) ,(O) the(O) boy(O) 's(O) great-grandfather(O) ,(O) enrolled(O) the(O) boy(O) in(O) a(O) program(O) to(O) train(O) future(O) Musketeers(O) .(O)"}}
{"id": "238", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "scientist", "protein", "enzyme", "discipline", "award", "country", "astronomical object", "event", "location", "academic journal", "person", "theory", "chemical compound", "organization", "university"], "instance": {"id": "238", "words": ["Directors", "of", "the", "botanical", "garden", "from", "this", "period", "included", "Karl", "Julius", "Perleb", ",", "Fridolin", "Karl", "Leopold", "Spenner", ",", "Alexander", "Braun", ",", "Carl", "Wilhelm", "von", "Nägeli", ",", "Heinrich", "Anton", "de", "Bary", "and", "Julius", "von", "Sachs", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, scientist, protein, enzyme, discipline, award, country, astronomical object, event, location, academic journal, person, theory, chemical compound, organization, university and O.\nSentence: Directors of the botanical garden from this period included Karl Julius Perleb , Fridolin Karl Leopold Spenner , Alexander Braun , Carl Wilhelm von Nägeli , Heinrich Anton de Bary and Julius von Sachs .", "prompt_labels": "Directors(O) of(O) the(O) botanical(O) garden(O) from(O) this(O) period(O) included(O) Karl(B-scientist) Julius(I-scientist) Perleb(I-scientist) ,(O) Fridolin(B-scientist) Karl(I-scientist) Leopold(I-scientist) Spenner(I-scientist) ,(O) Alexander(B-scientist) Braun(I-scientist) ,(O) Carl(B-scientist) Wilhelm(I-scientist) von(I-scientist) Nägeli(I-scientist) ,(O) Heinrich(B-scientist) Anton(I-scientist) de(I-scientist) Bary(I-scientist) and(O) Julius(B-scientist) von(I-scientist) Sachs(I-scientist) .(O)"}}
{"id": "73", "dataset": "crossner_science", "split": "test", "label_list": ["award", "organization", "chemical element", "enzyme", "academic journal", "protein", "chemical compound", "astronomical object", "country", "discipline", "theory", "event", "scientist", "location", "person", "university"], "instance": {"id": "73", "words": ["On", "11", "August", "2014", ",", "astronomers", "released", "studies", ",", "using", "the", "Atacama", "Large", "Millimeter", "/", "Submillimeter", "Array", "(", "ALMA", ")", "for", "the", "first", "time", ",", "that", "detailed", "the", "distribution", "of", "HCN", ",", "Hydrogen", "isocyanide", ",", "Formaldehyde", ",", "and", "dust", "inside", "the", "comae", "of", "comet", "s", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, organization, chemical element, enzyme, academic journal, protein, chemical compound, astronomical object, country, discipline, theory, event, scientist, location, person, university and O.\nSentence: On 11 August 2014 , astronomers released studies , using the Atacama Large Millimeter / Submillimeter Array ( ALMA ) for the first time , that detailed the distribution of HCN , Hydrogen isocyanide , Formaldehyde , and dust inside the comae of comet s .", "prompt_labels": "On(O) 11(O) August(O) 2014(O) ,(O) astronomers(O) released(O) studies(O) ,(O) using(O) the(O) Atacama(O) Large(O) Millimeter(O) /(O) Submillimeter(O) Array(O) ((O) ALMA(O) )(O) for(O) the(O) first(O) time(O) ,(O) that(O) detailed(O) the(O) distribution(O) of(O) HCN(O) ,(O) Hydrogen(B-chemical compound) isocyanide(I-chemical compound) ,(O) Formaldehyde(B-chemical compound) ,(O) and(O) dust(O) inside(O) the(O) comae(O) of(O) comet(O) s(O) .(O)"}}
{"id": "123", "dataset": "crossner_science", "split": "test", "label_list": ["astronomical object", "person", "theory", "enzyme", "university", "location", "organization", "protein", "country", "discipline", "scientist", "chemical compound", "academic journal", "event", "chemical element", "award"], "instance": {"id": "123", "words": ["The", "planets", "of", "the", "Solar", "System", "are", "divided", "into", "two", "groups", ":", "the", "four", "inner", "planets", "are", "the", "terrestrial", "planet", "s", "(", "Mercury", ",", "Venus", ",", "Earth", "and", "Mars", ")", ",", "with", "relatively", "small", "sizes", "and", "rocky", "surfaces", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, person, theory, enzyme, university, location, organization, protein, country, discipline, scientist, chemical compound, academic journal, event, chemical element, award and O.\nSentence: The planets of the Solar System are divided into two groups : the four inner planets are the terrestrial planet s ( Mercury , Venus , Earth and Mars ) , with relatively small sizes and rocky surfaces .", "prompt_labels": "The(O) planets(O) of(O) the(O) Solar(O) System(O) are(O) divided(O) into(O) two(O) groups(O) :(O) the(O) four(O) inner(O) planets(O) are(O) the(O) terrestrial(O) planet(O) s(O) ((O) Mercury(B-astronomical object) ,(O) Venus(B-astronomical object) ,(O) Earth(B-astronomical object) and(O) Mars(B-astronomical object) )(O) ,(O) with(O) relatively(O) small(O) sizes(O) and(O) rocky(O) surfaces(O) .(O)"}}
{"id": "224", "dataset": "crossner_science", "split": "test", "label_list": ["person", "academic journal", "scientist", "university", "award", "protein", "chemical compound", "enzyme", "event", "organization", "location", "chemical element", "country", "astronomical object", "discipline", "theory"], "instance": {"id": "224", "words": ["After", "receiving", "his", "D.", "Sc.", "in", "physics", "from", "Humboldt", "University", "of", "Berlin", "in", "Berlin", ",", "Germany", "in", "1938", ",", "Peterlin", "accepted", "in", "1939", "the", "chair", "as", "a", "professor", "of", "physics", "at", "the", "University", "of", "Ljubljana", ",", "where", "he", "remained", "for", "22", "years", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-discipline", "O", "B-university", "I-university", "I-university", "I-university", "O", "B-location", "O", "B-country", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-discipline", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, academic journal, scientist, university, award, protein, chemical compound, enzyme, event, organization, location, chemical element, country, astronomical object, discipline, theory and O.\nSentence: After receiving his D. Sc. in physics from Humboldt University of Berlin in Berlin , Germany in 1938 , Peterlin accepted in 1939 the chair as a professor of physics at the University of Ljubljana , where he remained for 22 years .", "prompt_labels": "After(O) receiving(O) his(O) D.(O) Sc.(O) in(O) physics(B-discipline) from(O) Humboldt(B-university) University(I-university) of(I-university) Berlin(I-university) in(O) Berlin(B-location) ,(O) Germany(B-country) in(O) 1938(O) ,(O) Peterlin(B-person) accepted(O) in(O) 1939(O) the(O) chair(O) as(O) a(O) professor(O) of(O) physics(B-discipline) at(O) the(O) University(B-university) of(I-university) Ljubljana(I-university) ,(O) where(O) he(O) remained(O) for(O) 22(O) years(O) .(O)"}}
{"id": "156", "dataset": "crossner_science", "split": "test", "label_list": ["organization", "location", "protein", "event", "chemical compound", "university", "astronomical object", "discipline", "academic journal", "scientist", "chemical element", "award", "country", "theory", "enzyme", "person"], "instance": {"id": "156", "words": ["Encke", "(", "1850", ")", "has", "further", "symbols", "for", "5", "Astraea", ",", "6", "Hebe", ",", "7", "Iris", ",", "8", "Flora", "and", "9", "Metis", "."], "labels": ["B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, protein, event, chemical compound, university, astronomical object, discipline, academic journal, scientist, chemical element, award, country, theory, enzyme, person and O.\nSentence: Encke ( 1850 ) has further symbols for 5 Astraea , 6 Hebe , 7 Iris , 8 Flora and 9 Metis .", "prompt_labels": "Encke(B-astronomical object) ((O) 1850(O) )(O) has(O) further(O) symbols(O) for(O) 5(B-astronomical object) Astraea(I-astronomical object) ,(O) 6(B-astronomical object) Hebe(I-astronomical object) ,(O) 7(B-astronomical object) Iris(I-astronomical object) ,(O) 8(B-astronomical object) Flora(I-astronomical object) and(O) 9(B-astronomical object) Metis(I-astronomical object) .(O)"}}
{"id": "373", "dataset": "crossner_science", "split": "test", "label_list": ["enzyme", "person", "astronomical object", "university", "event", "location", "award", "discipline", "theory", "chemical compound", "organization", "academic journal", "protein", "country", "chemical element", "scientist"], "instance": {"id": "373", "words": ["Ellen", "DeGeneres", "also", "became", "the", "first", "actress", "to", "win", "an", "award", "for", "voice", "acting", ",", "and", "the", "third", "person", "to", "do", "so", "after", "Robin", "Williams", "and", "Scott", "Weinger", "in", "1992", ",", "both", "for", "Aladdin", "."], "labels": ["B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, person, astronomical object, university, event, location, award, discipline, theory, chemical compound, organization, academic journal, protein, country, chemical element, scientist and O.\nSentence: Ellen DeGeneres also became the first actress to win an award for voice acting , and the third person to do so after Robin Williams and Scott Weinger in 1992 , both for Aladdin .", "prompt_labels": "Ellen(B-person) DeGeneres(I-person) also(O) became(O) the(O) first(O) actress(O) to(O) win(O) an(O) award(O) for(O) voice(O) acting(O) ,(O) and(O) the(O) third(O) person(O) to(O) do(O) so(O) after(O) Robin(B-person) Williams(I-person) and(O) Scott(B-person) Weinger(I-person) in(O) 1992(O) ,(O) both(O) for(O) Aladdin(O) .(O)"}}
{"id": "284", "dataset": "crossner_science", "split": "test", "label_list": ["protein", "theory", "enzyme", "academic journal", "astronomical object", "organization", "country", "university", "scientist", "event", "chemical compound", "award", "person", "chemical element", "location", "discipline"], "instance": {"id": "284", "words": ["The", "visitor", "starts", "with", "the", "sun", "on", "the", "eastern", "summit", "of", "the", "Hill", ",", "and", "following", "the", "trail", "to", "the", "west", "from", "the", "Sun", ",", "in", "the", "direction", "of", "Mills", "Observatory", ",", "he", "or", "she", "will", "encounter", "another", "eight", "rocks", "representing", "the", "planets", "Mercury", ",", "Venus", ",", "Earth", ",", "Mars", ",", "Jupiter", ",", "Saturn", ",", "Uranus", "and", "Neptune", "."], "labels": ["O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, theory, enzyme, academic journal, astronomical object, organization, country, university, scientist, event, chemical compound, award, person, chemical element, location, discipline and O.\nSentence: The visitor starts with the sun on the eastern summit of the Hill , and following the trail to the west from the Sun , in the direction of Mills Observatory , he or she will encounter another eight rocks representing the planets Mercury , Venus , Earth , Mars , Jupiter , Saturn , Uranus and Neptune .", "prompt_labels": "The(O) visitor(O) starts(O) with(O) the(O) sun(B-astronomical object) on(O) the(O) eastern(O) summit(O) of(O) the(O) Hill(B-location) ,(O) and(O) following(O) the(O) trail(O) to(O) the(O) west(O) from(O) the(O) Sun(B-astronomical object) ,(O) in(O) the(O) direction(O) of(O) Mills(B-location) Observatory(I-location) ,(O) he(O) or(O) she(O) will(O) encounter(O) another(O) eight(O) rocks(O) representing(O) the(O) planets(O) Mercury(B-astronomical object) ,(O) Venus(B-astronomical object) ,(O) Earth(B-astronomical object) ,(O) Mars(B-astronomical object) ,(O) Jupiter(B-astronomical object) ,(O) Saturn(B-astronomical object) ,(O) Uranus(B-astronomical object) and(O) Neptune(B-astronomical object) .(O)"}}
{"id": "108", "dataset": "crossner_science", "split": "test", "label_list": ["person", "university", "country", "discipline", "chemical element", "theory", "enzyme", "academic journal", "chemical compound", "location", "organization", "scientist", "protein", "astronomical object", "award", "event"], "instance": {"id": "108", "words": ["Additionally", ",", "she", "has", "received", "five", "Academy", "of", "Country", "Music", "Awards", ",", "six", "Country", "Music", "Association", "Awards", ",", "and", "a", "Grammy", "Award", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, university, country, discipline, chemical element, theory, enzyme, academic journal, chemical compound, location, organization, scientist, protein, astronomical object, award, event and O.\nSentence: Additionally , she has received five Academy of Country Music Awards , six Country Music Association Awards , and a Grammy Award .", "prompt_labels": "Additionally(O) ,(O) she(O) has(O) received(O) five(O) Academy(B-award) of(I-award) Country(I-award) Music(I-award) Awards(I-award) ,(O) six(O) Country(B-award) Music(I-award) Association(I-award) Awards(I-award) ,(O) and(O) a(O) Grammy(B-award) Award(I-award) .(O)"}}
{"id": "51", "dataset": "crossner_science", "split": "test", "label_list": ["enzyme", "person", "protein", "academic journal", "chemical compound", "event", "astronomical object", "university", "discipline", "location", "award", "theory", "country", "organization", "chemical element", "scientist"], "instance": {"id": "51", "words": ["The", "California", "portion", "of", "the", "desert", "also", "contains", "Edwards", "Air", "Force", "Base", "and", "Naval", "Air", "Weapons", "Station", "China", "Lake", ",", "noted", "for", "experimental", "aviation", "and", "weapons", "projects", ",", "and", "the", "Marine", "Corps", "Air", "Ground", "Combat", "Center", "Twentynine", "Palms", "in", "the", "world", "at", "Twentynine", "Palms", "."], "labels": ["O", "B-location", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "B-location", "I-location", "I-location", "I-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "I-location", "I-location", "I-location", "I-location", "O", "O", "O", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, person, protein, academic journal, chemical compound, event, astronomical object, university, discipline, location, award, theory, country, organization, chemical element, scientist and O.\nSentence: The California portion of the desert also contains Edwards Air Force Base and Naval Air Weapons Station China Lake , noted for experimental aviation and weapons projects , and the Marine Corps Air Ground Combat Center Twentynine Palms in the world at Twentynine Palms .", "prompt_labels": "The(O) California(B-location) portion(O) of(O) the(O) desert(O) also(O) contains(O) Edwards(B-location) Air(I-location) Force(I-location) Base(I-location) and(O) Naval(B-location) Air(I-location) Weapons(I-location) Station(I-location) China(I-location) Lake(I-location) ,(O) noted(O) for(O) experimental(O) aviation(O) and(O) weapons(O) projects(O) ,(O) and(O) the(O) Marine(B-location) Corps(I-location) Air(I-location) Ground(I-location) Combat(I-location) Center(I-location) Twentynine(I-location) Palms(I-location) in(O) the(O) world(O) at(O) Twentynine(B-location) Palms(I-location) .(O)"}}
{"id": "418", "dataset": "crossner_science", "split": "test", "label_list": ["event", "academic journal", "theory", "protein", "award", "country", "person", "university", "organization", "enzyme", "astronomical object", "chemical element", "chemical compound", "scientist", "discipline", "location"], "instance": {"id": "418", "words": ["She", "is", "an", "active", "voice", "in", "numerous", "committees", "of", "the", "National", "Academy", "of", "Sciences", ",", "the", "American", "Association", "for", "the", "Advancement", "of", "Science", "(", "AAAS", ")", ",", "and", "the", "National", "Science", "Foundation", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, academic journal, theory, protein, award, country, person, university, organization, enzyme, astronomical object, chemical element, chemical compound, scientist, discipline, location and O.\nSentence: She is an active voice in numerous committees of the National Academy of Sciences , the American Association for the Advancement of Science ( AAAS ) , and the National Science Foundation .", "prompt_labels": "She(O) is(O) an(O) active(O) voice(O) in(O) numerous(O) committees(O) of(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) the(O) American(B-organization) Association(I-organization) for(I-organization) the(I-organization) Advancement(I-organization) of(I-organization) Science(I-organization) ((O) AAAS(B-organization) )(O) ,(O) and(O) the(O) National(B-organization) Science(I-organization) Foundation(I-organization) .(O)"}}
{"id": "228", "dataset": "crossner_science", "split": "test", "label_list": ["chemical compound", "university", "country", "protein", "location", "organization", "academic journal", "enzyme", "award", "theory", "astronomical object", "person", "discipline", "scientist", "chemical element", "event"], "instance": {"id": "228", "words": ["Against", "a", "Crooked", "Sky", "is", "a", "1975", "American", "Western", "film", "directed", "by", "Earl", "Bellamy", "and", "starring", "Richard", "Boone", ",", "Stewart", "Petersen", ",", "and", "Henry", "Wilcoxon", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, university, country, protein, location, organization, academic journal, enzyme, award, theory, astronomical object, person, discipline, scientist, chemical element, event and O.\nSentence: Against a Crooked Sky is a 1975 American Western film directed by Earl Bellamy and starring Richard Boone , Stewart Petersen , and Henry Wilcoxon .", "prompt_labels": "Against(O) a(O) Crooked(O) Sky(O) is(O) a(O) 1975(O) American(O) Western(O) film(O) directed(O) by(O) Earl(B-person) Bellamy(I-person) and(O) starring(O) Richard(B-person) Boone(I-person) ,(O) Stewart(B-person) Petersen(I-person) ,(O) and(O) Henry(B-person) Wilcoxon(I-person) .(O)"}}
{"id": "336", "dataset": "crossner_science", "split": "test", "label_list": ["award", "chemical compound", "scientist", "chemical element", "location", "event", "university", "person", "country", "discipline", "astronomical object", "theory", "organization", "academic journal", "enzyme", "protein"], "instance": {"id": "336", "words": ["The", "conclusion", "of", "the", "scientists", "was", "that", "further", "effort", "was", "needed", "in", "to", "improve", "the", "precision", "and", "efficiency", "of", "CRISPR", "gene", "editing", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, chemical compound, scientist, chemical element, location, event, university, person, country, discipline, astronomical object, theory, organization, academic journal, enzyme, protein and O.\nSentence: The conclusion of the scientists was that further effort was needed in to improve the precision and efficiency of CRISPR gene editing .", "prompt_labels": "The(O) conclusion(O) of(O) the(O) scientists(O) was(O) that(O) further(O) effort(O) was(O) needed(O) in(O) to(O) improve(O) the(O) precision(O) and(O) efficiency(O) of(O) CRISPR(O) gene(O) editing(O) .(O)"}}
{"id": "369", "dataset": "crossner_science", "split": "test", "label_list": ["chemical compound", "location", "event", "theory", "academic journal", "chemical element", "award", "enzyme", "university", "country", "organization", "astronomical object", "person", "scientist", "protein", "discipline"], "instance": {"id": "369", "words": ["Juno", "can", "reach", "+", "7.5", "at", "a", "favourable", "opposition", ",", "which", "is", "brighter", "than", "Neptune", "or", "Titan", ",", "and", "is", "the", "reason", "for", "it", "being", "discovered", "before", "the", "larger", "asteroids", "10", "Hygiea", ",", "52", "Europa", ",", "511", "Davida", ",", "and", "704", "Interamnia", "."], "labels": ["B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "O", "B-astronomical object", "I-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, location, event, theory, academic journal, chemical element, award, enzyme, university, country, organization, astronomical object, person, scientist, protein, discipline and O.\nSentence: Juno can reach + 7.5 at a favourable opposition , which is brighter than Neptune or Titan , and is the reason for it being discovered before the larger asteroids 10 Hygiea , 52 Europa , 511 Davida , and 704 Interamnia .", "prompt_labels": "Juno(B-astronomical object) can(O) reach(O) +(O) 7.5(O) at(O) a(O) favourable(O) opposition(O) ,(O) which(O) is(O) brighter(O) than(O) Neptune(B-astronomical object) or(O) Titan(B-astronomical object) ,(O) and(O) is(O) the(O) reason(O) for(O) it(O) being(O) discovered(O) before(O) the(O) larger(O) asteroids(O) 10(B-astronomical object) Hygiea(I-astronomical object) ,(O) 52(B-astronomical object) Europa(I-astronomical object) ,(O) 511(B-astronomical object) Davida(I-astronomical object) ,(O) and(O) 704(B-astronomical object) Interamnia(I-astronomical object) .(O)"}}
{"id": "265", "dataset": "crossner_science", "split": "test", "label_list": ["university", "chemical compound", "protein", "theory", "event", "academic journal", "discipline", "enzyme", "award", "person", "organization", "scientist", "astronomical object", "chemical element", "country", "location"], "instance": {"id": "265", "words": ["There", "is", "some", "information", "about", "earthquakes", "in", "Aristotle", "'", "s", "Meteorology", ",", "in", "Naturalis", "Historia", "by", "Pliny", "the", "Elder", ",", "and", "in", "Strabo", "'", "s", "Geographica", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-scientist", "O", "O", "B-discipline", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "B-scientist", "O", "O", "B-discipline", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, chemical compound, protein, theory, event, academic journal, discipline, enzyme, award, person, organization, scientist, astronomical object, chemical element, country, location and O.\nSentence: There is some information about earthquakes in Aristotle ' s Meteorology , in Naturalis Historia by Pliny the Elder , and in Strabo ' s Geographica .", "prompt_labels": "There(O) is(O) some(O) information(O) about(O) earthquakes(O) in(O) Aristotle(B-scientist) '(O) s(O) Meteorology(B-discipline) ,(O) in(O) Naturalis(O) Historia(O) by(O) Pliny(B-person) the(I-person) Elder(I-person) ,(O) and(O) in(O) Strabo(B-scientist) '(O) s(O) Geographica(B-discipline) .(O)"}}
{"id": "4", "dataset": "crossner_science", "split": "test", "label_list": ["person", "discipline", "scientist", "university", "event", "location", "chemical element", "organization", "protein", "theory", "astronomical object", "award", "chemical compound", "enzyme", "academic journal", "country"], "instance": {"id": "4", "words": ["He", "also", "won", "the", "Copley", "Medal", "in", "1907", ",", "the", "Henry", "Draper", "Medal", "in", "1916", "and", "the", "Gold", "Medal", "of", "the", "Royal", "Astronomical", "Society", "in", "1923", "."], "labels": ["O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, discipline, scientist, university, event, location, chemical element, organization, protein, theory, astronomical object, award, chemical compound, enzyme, academic journal, country and O.\nSentence: He also won the Copley Medal in 1907 , the Henry Draper Medal in 1916 and the Gold Medal of the Royal Astronomical Society in 1923 .", "prompt_labels": "He(O) also(O) won(O) the(O) Copley(B-award) Medal(I-award) in(O) 1907(O) ,(O) the(O) Henry(B-award) Draper(I-award) Medal(I-award) in(O) 1916(O) and(O) the(O) Gold(B-award) Medal(I-award) of(I-award) the(I-award) Royal(I-award) Astronomical(I-award) Society(I-award) in(O) 1923(O) .(O)"}}
{"id": "199", "dataset": "crossner_science", "split": "test", "label_list": ["theory", "chemical compound", "event", "protein", "country", "award", "discipline", "organization", "person", "astronomical object", "location", "scientist", "university", "enzyme", "chemical element", "academic journal"], "instance": {"id": "199", "words": ["He", "was", "appointed", "as", "assistant", "to", "the", "astronomer", "Giovanni", "Battista", "Donati", "in", "1862", ",", "professor", "at", "the", "technological", "institute", "of", "Bologna", "in", "1864", ",", "professor", "of", "physics", "at", "the", "University", "of", "Cagliari", "in", "1873", ",", "and", ",", "finally", ",", "successor", "to", "his", "father", "in", "1881", "in", "the", "chair", "of", "technological", "physics", "at", "the", "University", "of", "Pisa", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "O", "O", "O", "O", "B-discipline", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-discipline", "I-discipline", "O", "O", "B-university", "I-university", "I-university", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, chemical compound, event, protein, country, award, discipline, organization, person, astronomical object, location, scientist, university, enzyme, chemical element, academic journal and O.\nSentence: He was appointed as assistant to the astronomer Giovanni Battista Donati in 1862 , professor at the technological institute of Bologna in 1864 , professor of physics at the University of Cagliari in 1873 , and , finally , successor to his father in 1881 in the chair of technological physics at the University of Pisa .", "prompt_labels": "He(O) was(O) appointed(O) as(O) assistant(O) to(O) the(O) astronomer(O) Giovanni(B-scientist) Battista(I-scientist) Donati(I-scientist) in(O) 1862(O) ,(O) professor(O) at(O) the(O) technological(B-location) institute(I-location) of(I-location) Bologna(I-location) in(O) 1864(O) ,(O) professor(O) of(O) physics(B-discipline) at(O) the(O) University(B-university) of(I-university) Cagliari(I-university) in(O) 1873(O) ,(O) and(O) ,(O) finally(O) ,(O) successor(O) to(O) his(O) father(O) in(O) 1881(O) in(O) the(O) chair(O) of(O) technological(B-discipline) physics(I-discipline) at(O) the(O) University(B-university) of(I-university) Pisa(I-university) .(O)"}}
{"id": "101", "dataset": "crossner_science", "split": "test", "label_list": ["country", "chemical element", "organization", "enzyme", "chemical compound", "discipline", "protein", "person", "academic journal", "astronomical object", "scientist", "university", "award", "event", "location", "theory"], "instance": {"id": "101", "words": ["The", "minor", "planets", "1062", "Ljuba", "and", "1086", "Nata", "were", "also", "named", "after", "Soviet", "female", "paratroopers", "Lyuba", "Berlin", "(", "1915-1936", ")", "and", "Nata", "Babushkina", "(", "1915-1936", ")", ",", "respectively", "."], "labels": ["O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, chemical element, organization, enzyme, chemical compound, discipline, protein, person, academic journal, astronomical object, scientist, university, award, event, location, theory and O.\nSentence: The minor planets 1062 Ljuba and 1086 Nata were also named after Soviet female paratroopers Lyuba Berlin ( 1915-1936 ) and Nata Babushkina ( 1915-1936 ) , respectively .", "prompt_labels": "The(O) minor(O) planets(O) 1062(B-astronomical object) Ljuba(I-astronomical object) and(O) 1086(B-astronomical object) Nata(I-astronomical object) were(O) also(O) named(O) after(O) Soviet(O) female(O) paratroopers(O) Lyuba(B-person) Berlin(I-person) ((O) 1915-1936(O) )(O) and(O) Nata(B-person) Babushkina(I-person) ((O) 1915-1936(O) )(O) ,(O) respectively(O) .(O)"}}
{"id": "215", "dataset": "crossner_science", "split": "test", "label_list": ["country", "location", "scientist", "person", "event", "theory", "discipline", "astronomical object", "chemical element", "university", "organization", "chemical compound", "enzyme", "award", "academic journal", "protein"], "instance": {"id": "215", "words": ["It", "was", "discovered", "on", "16", "October", "1977", ",", "by", "Ingrid", "van", "Houten-Groeneveld", "and", "Cornelis", "van", "Houten", "at", "Leiden", ",", "and", "Tom", "Gehrels", "at", "Palomar", "Observatory", "in", "California", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-location", "O", "O", "B-scientist", "I-scientist", "O", "B-location", "I-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, scientist, person, event, theory, discipline, astronomical object, chemical element, university, organization, chemical compound, enzyme, award, academic journal, protein and O.\nSentence: It was discovered on 16 October 1977 , by Ingrid van Houten-Groeneveld and Cornelis van Houten at Leiden , and Tom Gehrels at Palomar Observatory in California .", "prompt_labels": "It(O) was(O) discovered(O) on(O) 16(O) October(O) 1977(O) ,(O) by(O) Ingrid(B-scientist) van(I-scientist) Houten-Groeneveld(I-scientist) and(O) Cornelis(B-scientist) van(I-scientist) Houten(I-scientist) at(O) Leiden(B-location) ,(O) and(O) Tom(B-scientist) Gehrels(I-scientist) at(O) Palomar(B-location) Observatory(I-location) in(O) California(B-location) .(O)"}}
{"id": "258", "dataset": "crossner_science", "split": "test", "label_list": ["organization", "scientist", "country", "location", "enzyme", "astronomical object", "event", "theory", "award", "protein", "chemical compound", "chemical element", "university", "academic journal", "person", "discipline"], "instance": {"id": "258", "words": ["Botryosphaeran", "reduces", "hepatic", "steatosis", "and", "dyslipidaemia", ",", "and", "glucose", "intolerance", "in", "diet-induced", "obese", "rats", "through", "activation", "of", "AMP-activated", "protein", "kinase", "(", "AMPK", ")", "and", "the", "expression", "of", "the", "Forkhead", "transcription", "factor", ",", "FOXO3a", ",", "in", "adipose", "tissue", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "I-enzyme", "O", "B-enzyme", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, scientist, country, location, enzyme, astronomical object, event, theory, award, protein, chemical compound, chemical element, university, academic journal, person, discipline and O.\nSentence: Botryosphaeran reduces hepatic steatosis and dyslipidaemia , and glucose intolerance in diet-induced obese rats through activation of AMP-activated protein kinase ( AMPK ) and the expression of the Forkhead transcription factor , FOXO3a , in adipose tissue .", "prompt_labels": "Botryosphaeran(O) reduces(O) hepatic(O) steatosis(O) and(O) dyslipidaemia(O) ,(O) and(O) glucose(O) intolerance(O) in(O) diet-induced(O) obese(O) rats(O) through(O) activation(O) of(O) AMP-activated(B-enzyme) protein(I-enzyme) kinase(I-enzyme) ((O) AMPK(B-enzyme) )(O) and(O) the(O) expression(O) of(O) the(O) Forkhead(O) transcription(O) factor(O) ,(O) FOXO3a(B-protein) ,(O) in(O) adipose(O) tissue(O) .(O)"}}
{"id": "428", "dataset": "crossner_science", "split": "test", "label_list": ["location", "country", "astronomical object", "award", "discipline", "chemical compound", "person", "theory", "organization", "event", "protein", "scientist", "university", "academic journal", "enzyme", "chemical element"], "instance": {"id": "428", "words": ["The", "importance", "of", "the", "predicted", "return", "based", "on", "the", "calculation", "by", "Encke", "was", "rewarded", "by", "the", "Royal", "Astronomical", "Society", "in", "London", "by", "presenting", "their", "Gold", "Medal", "of", "the", "Royal", "Astronomical", "Society", "to", "him", "in", "1824", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-location", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, astronomical object, award, discipline, chemical compound, person, theory, organization, event, protein, scientist, university, academic journal, enzyme, chemical element and O.\nSentence: The importance of the predicted return based on the calculation by Encke was rewarded by the Royal Astronomical Society in London by presenting their Gold Medal of the Royal Astronomical Society to him in 1824 .", "prompt_labels": "The(O) importance(O) of(O) the(O) predicted(O) return(O) based(O) on(O) the(O) calculation(O) by(O) Encke(B-scientist) was(O) rewarded(O) by(O) the(O) Royal(B-organization) Astronomical(I-organization) Society(I-organization) in(O) London(B-location) by(O) presenting(O) their(O) Gold(B-award) Medal(I-award) of(I-award) the(I-award) Royal(I-award) Astronomical(I-award) Society(I-award) to(O) him(O) in(O) 1824(O) .(O)"}}
{"id": "139", "dataset": "crossner_science", "split": "test", "label_list": ["theory", "award", "organization", "discipline", "protein", "country", "chemical element", "academic journal", "scientist", "enzyme", "university", "event", "location", "chemical compound", "astronomical object", "person"], "instance": {"id": "139", "words": ["Some", "additional", "examples", "include", "990", "Yerkes", ",", "991", "McDonalda", ",", "and", "992", "Swasey", "around", "this", "time", ";", "many", "more", "minor", "planets", "would", "be", "discovered", "at", "the", "observatory", "in", "the", "following", "decades", "."], "labels": ["O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, award, organization, discipline, protein, country, chemical element, academic journal, scientist, enzyme, university, event, location, chemical compound, astronomical object, person and O.\nSentence: Some additional examples include 990 Yerkes , 991 McDonalda , and 992 Swasey around this time ; many more minor planets would be discovered at the observatory in the following decades .", "prompt_labels": "Some(O) additional(O) examples(O) include(O) 990(B-astronomical object) Yerkes(I-astronomical object) ,(O) 991(B-astronomical object) McDonalda(I-astronomical object) ,(O) and(O) 992(B-astronomical object) Swasey(I-astronomical object) around(O) this(O) time(O) ;(O) many(O) more(O) minor(O) planets(O) would(O) be(O) discovered(O) at(O) the(O) observatory(O) in(O) the(O) following(O) decades(O) .(O)"}}
{"id": "450", "dataset": "crossner_science", "split": "test", "label_list": ["discipline", "event", "person", "chemical compound", "location", "enzyme", "astronomical object", "award", "university", "organization", "country", "academic journal", "chemical element", "scientist", "theory", "protein"], "instance": {"id": "450", "words": ["Wilson", "won", "gold", "medals", "in", "the", "800", "metres", "at", "both", "the", "2011", "World", "Youth", "Championships", "in", "Athletics", "and", "the", "2012", "World", "Junior", "Championships", "in", "Athletics", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, event, person, chemical compound, location, enzyme, astronomical object, award, university, organization, country, academic journal, chemical element, scientist, theory, protein and O.\nSentence: Wilson won gold medals in the 800 metres at both the 2011 World Youth Championships in Athletics and the 2012 World Junior Championships in Athletics .", "prompt_labels": "Wilson(B-person) won(O) gold(O) medals(O) in(O) the(O) 800(O) metres(O) at(O) both(O) the(O) 2011(B-event) World(I-event) Youth(I-event) Championships(I-event) in(I-event) Athletics(I-event) and(O) the(O) 2012(B-event) World(I-event) Junior(I-event) Championships(I-event) in(I-event) Athletics(I-event) .(O)"}}
{"id": "182", "dataset": "crossner_science", "split": "test", "label_list": ["location", "university", "astronomical object", "award", "chemical element", "organization", "scientist", "enzyme", "country", "protein", "person", "academic journal", "theory", "chemical compound", "event", "discipline"], "instance": {"id": "182", "words": ["A", "pupil", "of", "Peter", "Gustav", "Lejeune", "Dirichlet", ",", "Gabriel", "Lamé", "and", "Augustin-Louis", "Cauchy", "Bjerknes", "worked", "for", "the", "rest", "of", "his", "life", "in", "the", "field", "of", "hydrodynamics", "."], "labels": ["O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-discipline", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, university, astronomical object, award, chemical element, organization, scientist, enzyme, country, protein, person, academic journal, theory, chemical compound, event, discipline and O.\nSentence: A pupil of Peter Gustav Lejeune Dirichlet , Gabriel Lamé and Augustin-Louis Cauchy Bjerknes worked for the rest of his life in the field of hydrodynamics .", "prompt_labels": "A(O) pupil(O) of(O) Peter(B-scientist) Gustav(I-scientist) Lejeune(I-scientist) Dirichlet(I-scientist) ,(O) Gabriel(B-scientist) Lamé(I-scientist) and(O) Augustin-Louis(B-scientist) Cauchy(I-scientist) Bjerknes(I-scientist) worked(O) for(O) the(O) rest(O) of(O) his(O) life(O) in(O) the(O) field(O) of(O) hydrodynamics(B-discipline) .(O)"}}
{"id": "193", "dataset": "crossner_science", "split": "test", "label_list": ["event", "location", "award", "person", "enzyme", "academic journal", "protein", "chemical compound", "country", "theory", "chemical element", "organization", "university", "discipline", "astronomical object", "scientist"], "instance": {"id": "193", "words": ["The", "alkaline", "earth", "metals", "are", "named", "after", "their", "oxide", "s", ",", "the", "alkaline", "earths", ",", "whose", "old-fashioned", "names", "were", "beryllia", ",", "Magnesium", "oxide", ",", "Calcium", "oxide", ",", "strontia", ",", "and", "baryta", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "O", "O", "B-chemical compound", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, award, person, enzyme, academic journal, protein, chemical compound, country, theory, chemical element, organization, university, discipline, astronomical object, scientist and O.\nSentence: The alkaline earth metals are named after their oxide s , the alkaline earths , whose old-fashioned names were beryllia , Magnesium oxide , Calcium oxide , strontia , and baryta .", "prompt_labels": "The(O) alkaline(O) earth(O) metals(O) are(O) named(O) after(O) their(O) oxide(B-chemical compound) s(O) ,(O) the(O) alkaline(O) earths(O) ,(O) whose(O) old-fashioned(O) names(O) were(O) beryllia(B-chemical compound) ,(O) Magnesium(B-chemical compound) oxide(I-chemical compound) ,(O) Calcium(B-chemical compound) oxide(I-chemical compound) ,(O) strontia(B-chemical compound) ,(O) and(O) baryta(B-chemical compound) .(O)"}}
{"id": "125", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "theory", "enzyme", "award", "scientist", "organization", "event", "protein", "academic journal", "astronomical object", "discipline", "person", "university", "location", "chemical compound", "country"], "instance": {"id": "125", "words": ["With", "a", "diameter", "of", "approximately", "10", "kilometers", ",", "Amundsenia", "is", "one", "of", "the", "largest", "mid-sized", "Mars-crossing", "asteroid", "s", "such", "as", "1139", "Atami", "(", "9.35", "km", ")", ",", "1474", "Beira", "(", "14.9", "km", ")", ",", "1011", "Laodamia", "(", "7.39", "km", ")", ",", "1727", "Mette", "(", "est", "9", "km", ")", ",", "1131", "Porzia", "(", "7.13", "km", ")", ",", "1235", "Schorria", "(", "est", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, theory, enzyme, award, scientist, organization, event, protein, academic journal, astronomical object, discipline, person, university, location, chemical compound, country and O.\nSentence: With a diameter of approximately 10 kilometers , Amundsenia is one of the largest mid-sized Mars-crossing asteroid s such as 1139 Atami ( 9.35 km ) , 1474 Beira ( 14.9 km ) , 1011 Laodamia ( 7.39 km ) , 1727 Mette ( est 9 km ) , 1131 Porzia ( 7.13 km ) , 1235 Schorria ( est .", "prompt_labels": "With(O) a(O) diameter(O) of(O) approximately(O) 10(O) kilometers(O) ,(O) Amundsenia(B-astronomical object) is(O) one(O) of(O) the(O) largest(O) mid-sized(O) Mars-crossing(O) asteroid(O) s(O) such(O) as(O) 1139(B-astronomical object) Atami(I-astronomical object) ((O) 9.35(O) km(O) )(O) ,(O) 1474(B-astronomical object) Beira(I-astronomical object) ((O) 14.9(O) km(O) )(O) ,(O) 1011(B-astronomical object) Laodamia(I-astronomical object) ((O) 7.39(O) km(O) )(O) ,(O) 1727(B-astronomical object) Mette(I-astronomical object) ((O) est(O) 9(O) km(O) )(O) ,(O) 1131(B-astronomical object) Porzia(I-astronomical object) ((O) 7.13(O) km(O) )(O) ,(O) 1235(B-astronomical object) Schorria(I-astronomical object) ((O) est(O) .(O)"}}
{"id": "520", "dataset": "crossner_science", "split": "test", "label_list": ["person", "country", "theory", "enzyme", "chemical compound", "academic journal", "chemical element", "scientist", "protein", "university", "organization", "location", "astronomical object", "event", "award", "discipline"], "instance": {"id": "520", "words": ["This", "interaction", "stabilizes", "both", "TOC1", "and", "PRR5", "and", "prevents", "their", "degradation", "by", "the", "F-box", "protein", "ZEITLUPE", "(", "ZTL", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "B-protein", "O", "B-protein", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, theory, enzyme, chemical compound, academic journal, chemical element, scientist, protein, university, organization, location, astronomical object, event, award, discipline and O.\nSentence: This interaction stabilizes both TOC1 and PRR5 and prevents their degradation by the F-box protein ZEITLUPE ( ZTL ) .", "prompt_labels": "This(O) interaction(O) stabilizes(O) both(O) TOC1(O) and(O) PRR5(O) and(O) prevents(O) their(O) degradation(O) by(O) the(O) F-box(B-protein) protein(I-protein) ZEITLUPE(B-protein) ((O) ZTL(B-protein) )(O) .(O)"}}
{"id": "78", "dataset": "crossner_science", "split": "test", "label_list": ["university", "event", "theory", "award", "enzyme", "academic journal", "country", "discipline", "protein", "astronomical object", "chemical element", "chemical compound", "location", "scientist", "organization", "person"], "instance": {"id": "78", "words": ["Young", "began", "to", "study", "medicine", "in", "London", "at", "St", "Bartholomew", "'s", "Hospital", "in", "1792", ",", "moved", "to", "the", "University", "of", "Edinburgh", "Medical", "School", "in", "1794", ",", "and", "a", "year", "later", "went", "to", "Göttingen", ",", "Lower", "Saxony", ",", "Germany", ",", "where", "he", "obtained", "the", "degree", "of", "doctor", "of", "medicine", "in", "1796", "from", "the", "University", "of", "Göttingen", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-location", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "I-location", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, event, theory, award, enzyme, academic journal, country, discipline, protein, astronomical object, chemical element, chemical compound, location, scientist, organization, person and O.\nSentence: Young began to study medicine in London at St Bartholomew 's Hospital in 1792 , moved to the University of Edinburgh Medical School in 1794 , and a year later went to Göttingen , Lower Saxony , Germany , where he obtained the degree of doctor of medicine in 1796 from the University of Göttingen .", "prompt_labels": "Young(O) began(O) to(O) study(O) medicine(O) in(O) London(B-location) at(O) St(B-organization) Bartholomew(I-organization) 's(I-organization) Hospital(I-organization) in(O) 1792(O) ,(O) moved(O) to(O) the(O) University(B-university) of(I-university) Edinburgh(I-university) Medical(I-university) School(I-university) in(O) 1794(O) ,(O) and(O) a(O) year(O) later(O) went(O) to(O) Göttingen(B-location) ,(O) Lower(B-location) Saxony(I-location) ,(O) Germany(B-country) ,(O) where(O) he(O) obtained(O) the(O) degree(O) of(O) doctor(O) of(O) medicine(O) in(O) 1796(O) from(O) the(O) University(B-university) of(I-university) Göttingen(I-university) .(O)"}}
{"id": "63", "dataset": "crossner_science", "split": "test", "label_list": ["scientist", "academic journal", "university", "organization", "award", "enzyme", "person", "discipline", "protein", "astronomical object", "chemical element", "location", "event", "country", "chemical compound", "theory"], "instance": {"id": "63", "words": ["Ran", "exists", "in", "the", "cell", "in", "two", "nucleotide-bound", "forms", ":", "Guanosine", "diphosphate", "-bound", "and", "Guanosine", "triphosphate", "-bound", "."], "labels": ["B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "O", "B-chemical compound", "I-chemical compound", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, academic journal, university, organization, award, enzyme, person, discipline, protein, astronomical object, chemical element, location, event, country, chemical compound, theory and O.\nSentence: Ran exists in the cell in two nucleotide-bound forms : Guanosine diphosphate -bound and Guanosine triphosphate -bound .", "prompt_labels": "Ran(B-protein) exists(O) in(O) the(O) cell(O) in(O) two(O) nucleotide-bound(O) forms(O) :(O) Guanosine(B-chemical compound) diphosphate(I-chemical compound) -bound(O) and(O) Guanosine(B-chemical compound) triphosphate(I-chemical compound) -bound(O) .(O)"}}
{"id": "421", "dataset": "crossner_science", "split": "test", "label_list": ["discipline", "scientist", "event", "person", "award", "chemical compound", "academic journal", "protein", "organization", "country", "theory", "university", "astronomical object", "enzyme", "chemical element", "location"], "instance": {"id": "421", "words": ["Interaction", "of", "PAMPs", "with", "PRPs", "(", "pattern-recognition", "proteins", ")", "activates", "a", "series", "of", "Serine", "protease", "and", "those", "proteolytically", "cleave", "the", "prophenoloxidase", "(", "proPO", ")", "zymogen", "and", "activate", "phenoxidase", "(", "PO", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, scientist, event, person, award, chemical compound, academic journal, protein, organization, country, theory, university, astronomical object, enzyme, chemical element, location and O.\nSentence: Interaction of PAMPs with PRPs ( pattern-recognition proteins ) activates a series of Serine protease and those proteolytically cleave the prophenoloxidase ( proPO ) zymogen and activate phenoxidase ( PO ) .", "prompt_labels": "Interaction(O) of(O) PAMPs(O) with(O) PRPs(O) ((O) pattern-recognition(O) proteins(O) )(O) activates(O) a(O) series(O) of(O) Serine(B-enzyme) protease(I-enzyme) and(O) those(O) proteolytically(O) cleave(O) the(O) prophenoloxidase(O) ((O) proPO(O) )(O) zymogen(O) and(O) activate(O) phenoxidase(O) ((O) PO(O) )(O) .(O)"}}
{"id": "180", "dataset": "crossner_science", "split": "test", "label_list": ["chemical compound", "discipline", "chemical element", "award", "protein", "astronomical object", "university", "organization", "country", "enzyme", "theory", "person", "academic journal", "location", "scientist", "event"], "instance": {"id": "180", "words": ["The", "International", "Astronomical", "Union", "names", "all", "colles", "(", "small", "hills", ")", "on", "Saturn", "'", "s", "moon", "Titan", "after", "characters", "in", "Tolkien", "'s", "work", "."], "labels": ["O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "B-astronomical object", "O", "O", "O", "B-scientist", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, discipline, chemical element, award, protein, astronomical object, university, organization, country, enzyme, theory, person, academic journal, location, scientist, event and O.\nSentence: The International Astronomical Union names all colles ( small hills ) on Saturn ' s moon Titan after characters in Tolkien 's work .", "prompt_labels": "The(O) International(B-organization) Astronomical(I-organization) Union(I-organization) names(O) all(O) colles(O) ((O) small(O) hills(O) )(O) on(O) Saturn(B-astronomical object) '(O) s(O) moon(O) Titan(B-astronomical object) after(O) characters(O) in(O) Tolkien(B-scientist) 's(O) work(O) .(O)"}}
{"id": "289", "dataset": "crossner_science", "split": "test", "label_list": ["organization", "university", "academic journal", "event", "location", "theory", "award", "chemical compound", "discipline", "enzyme", "scientist", "chemical element", "country", "protein", "astronomical object", "person"], "instance": {"id": "289", "words": ["Today", "we", "know", "that", "AHR", "/", "CYP1-signaling", "is", "involved", "in", "an", "amazing", "array", "of", "genetic", "networks", "and", "subcellular", "processes", "critical", "to", "lifesub", "production", ";", "crosstalk", "with", "hypoxia", "and", "Hypoxia-inducible", "factors", "(", "HIF", ")", "-signaling", "pathways", ";", "transforming", "growth", "factor", "-signaling", "pathways", ",", "as", "well", "as", "growth", "suppression", ";", "tumor", "initiation", ";", "tumor", "promotion", ";", "transgenerational", "inheritance", "and", "epigenetic", "effects", ";", "chromatin", "remodeling", ";", "histone", "modification", ";", "and", "aging-related", "and", "degenerative", "disease", "s", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, university, academic journal, event, location, theory, award, chemical compound, discipline, enzyme, scientist, chemical element, country, protein, astronomical object, person and O.\nSentence: Today we know that AHR / CYP1-signaling is involved in an amazing array of genetic networks and subcellular processes critical to lifesub production ; crosstalk with hypoxia and Hypoxia-inducible factors ( HIF ) -signaling pathways ; transforming growth factor -signaling pathways , as well as growth suppression ; tumor initiation ; tumor promotion ; transgenerational inheritance and epigenetic effects ; chromatin remodeling ; histone modification ; and aging-related and degenerative disease s .", "prompt_labels": "Today(O) we(O) know(O) that(O) AHR(O) /(O) CYP1-signaling(O) is(O) involved(O) in(O) an(O) amazing(O) array(O) of(O) genetic(O) networks(O) and(O) subcellular(O) processes(O) critical(O) to(O) lifesub(O) production(O) ;(O) crosstalk(O) with(O) hypoxia(O) and(O) Hypoxia-inducible(O) factors(O) ((O) HIF(O) )(O) -signaling(O) pathways(O) ;(O) transforming(O) growth(O) factor(O) -signaling(O) pathways(O) ,(O) as(O) well(O) as(O) growth(O) suppression(O) ;(O) tumor(O) initiation(O) ;(O) tumor(O) promotion(O) ;(O) transgenerational(O) inheritance(O) and(O) epigenetic(O) effects(O) ;(O) chromatin(O) remodeling(O) ;(O) histone(O) modification(O) ;(O) and(O) aging-related(O) and(O) degenerative(O) disease(O) s(O) .(O)"}}
{"id": "526", "dataset": "crossner_science", "split": "test", "label_list": ["theory", "person", "university", "event", "scientist", "country", "astronomical object", "enzyme", "chemical element", "organization", "discipline", "chemical compound", "protein", "award", "location", "academic journal"], "instance": {"id": "526", "words": ["Phosphoinositide", "signaling", "is", "also", "altered", ",", "with", "elevated", "levels", "of", "phospholipase", "C", ",", "protein", "kinase", "C", ",", "and", "Gq", "alpha", "subunit", "being", "reported", "in", "bipolar", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "O", "B-enzyme", "I-enzyme", "I-enzyme", "O", "O", "B-protein", "I-protein", "I-protein", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, person, university, event, scientist, country, astronomical object, enzyme, chemical element, organization, discipline, chemical compound, protein, award, location, academic journal and O.\nSentence: Phosphoinositide signaling is also altered , with elevated levels of phospholipase C , protein kinase C , and Gq alpha subunit being reported in bipolar .", "prompt_labels": "Phosphoinositide(O) signaling(O) is(O) also(O) altered(O) ,(O) with(O) elevated(O) levels(O) of(O) phospholipase(B-enzyme) C(I-enzyme) ,(O) protein(B-enzyme) kinase(I-enzyme) C(I-enzyme) ,(O) and(O) Gq(B-protein) alpha(I-protein) subunit(I-protein) being(O) reported(O) in(O) bipolar(O) .(O)"}}
{"id": "88", "dataset": "crossner_science", "split": "test", "label_list": ["award", "theory", "person", "discipline", "chemical compound", "university", "chemical element", "scientist", "organization", "enzyme", "protein", "astronomical object", "location", "event", "academic journal", "country"], "instance": {"id": "88", "words": ["Five", "out", "of", "Finland", "'s", "15", "universities", "ref", "group", "=", "note", "These", "are", "Aalto", "University", ",", "the", "University", "of", "Helsinki", ",", "the", "University", "of", "the", "Arts", "Helsinki", ",", "the", "Hanken", "School", "of", "Economics", ",", "and", "the", "National", "Defence", "University", "."], "labels": ["O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "O", "O", "B-university", "I-university", "I-university", "O", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "O", "O", "B-university", "I-university", "I-university", "I-university", "O", "O", "O", "B-university", "I-university", "I-university", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, theory, person, discipline, chemical compound, university, chemical element, scientist, organization, enzyme, protein, astronomical object, location, event, academic journal, country and O.\nSentence: Five out of Finland 's 15 universities ref group = note These are Aalto University , the University of Helsinki , the University of the Arts Helsinki , the Hanken School of Economics , and the National Defence University .", "prompt_labels": "Five(O) out(O) of(O) Finland(B-person) 's(O) 15(O) universities(O) ref(O) group(O) =(O) note(O) These(O) are(O) Aalto(B-university) University(I-university) ,(O) the(O) University(B-university) of(I-university) Helsinki(I-university) ,(O) the(O) University(B-university) of(I-university) the(I-university) Arts(I-university) Helsinki(I-university) ,(O) the(O) Hanken(B-university) School(I-university) of(I-university) Economics(I-university) ,(O) and(O) the(O) National(B-university) Defence(I-university) University(I-university) .(O)"}}
{"id": "482", "dataset": "crossner_science", "split": "test", "label_list": ["enzyme", "theory", "chemical element", "organization", "location", "person", "event", "astronomical object", "award", "country", "scientist", "chemical compound", "university", "discipline", "protein", "academic journal"], "instance": {"id": "482", "words": ["Toutatis", "is", "also", "a", "Mars-crosser", "asteroid", "with", "a", "chaotic", "orbit", "produced", "by", "a", "3", ":", "1", "resonance", "with", "the", "planet", "Jupiter", ",", "a", "1", ":", "4", "resonance", "with", "the", "planet", "Earth", ",", "and", "frequent", "close", "approaches", "to", "the", "terrestrial", "planet", "s", ",", "including", "Earth", "."], "labels": ["B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, theory, chemical element, organization, location, person, event, astronomical object, award, country, scientist, chemical compound, university, discipline, protein, academic journal and O.\nSentence: Toutatis is also a Mars-crosser asteroid with a chaotic orbit produced by a 3 : 1 resonance with the planet Jupiter , a 1 : 4 resonance with the planet Earth , and frequent close approaches to the terrestrial planet s , including Earth .", "prompt_labels": "Toutatis(B-astronomical object) is(O) also(O) a(O) Mars-crosser(O) asteroid(O) with(O) a(O) chaotic(O) orbit(O) produced(O) by(O) a(O) 3(O) :(O) 1(O) resonance(O) with(O) the(O) planet(O) Jupiter(B-astronomical object) ,(O) a(O) 1(O) :(O) 4(O) resonance(O) with(O) the(O) planet(O) Earth(B-astronomical object) ,(O) and(O) frequent(O) close(O) approaches(O) to(O) the(O) terrestrial(O) planet(O) s(O) ,(O) including(O) Earth(B-astronomical object) .(O)"}}
{"id": "404", "dataset": "crossner_science", "split": "test", "label_list": ["organization", "country", "university", "award", "enzyme", "person", "location", "chemical element", "academic journal", "chemical compound", "event", "protein", "theory", "discipline", "astronomical object", "scientist"], "instance": {"id": "404", "words": ["Fischer", "then", "attended", "the", "University", "of", "Bonn", "in", "1871", ",", "but", "switched", "to", "the", "University", "of", "Strasbourg", "in", "1872", "with", "his", "study", "of", "phthaleins", ",", "and", "was", "appointed", "to", "a", "position", "at", "the", "university", "."], "labels": ["B-person", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, university, award, enzyme, person, location, chemical element, academic journal, chemical compound, event, protein, theory, discipline, astronomical object, scientist and O.\nSentence: Fischer then attended the University of Bonn in 1871 , but switched to the University of Strasbourg in 1872 with his study of phthaleins , and was appointed to a position at the university .", "prompt_labels": "Fischer(B-person) then(O) attended(O) the(O) University(B-university) of(I-university) Bonn(I-university) in(O) 1871(O) ,(O) but(O) switched(O) to(O) the(O) University(B-university) of(I-university) Strasbourg(I-university) in(O) 1872(O) with(O) his(O) study(O) of(O) phthaleins(B-chemical compound) ,(O) and(O) was(O) appointed(O) to(O) a(O) position(O) at(O) the(O) university(O) .(O)"}}
{"id": "102", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "discipline", "location", "person", "award", "scientist", "university", "theory", "enzyme", "academic journal", "organization", "chemical compound", "event", "astronomical object", "protein", "country"], "instance": {"id": "102", "words": ["This", "minor", "planet", "was", "named", "for", "Aesculapius", ",", "the", "Greek", "and", "Roman", "demigod", "of", "medicine", "and", "healing", ",", "son", "of", "Apollo", "and", "Coronis", ",", "after", "whom", "the", "asteroids", "158", "Koronis", "and", "1862", "Apollo", "are", "named", ",", "respectively", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "B-discipline", "O", "O", "O", "O", "O", "B-person", "O", "B-person", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, discipline, location, person, award, scientist, university, theory, enzyme, academic journal, organization, chemical compound, event, astronomical object, protein, country and O.\nSentence: This minor planet was named for Aesculapius , the Greek and Roman demigod of medicine and healing , son of Apollo and Coronis , after whom the asteroids 158 Koronis and 1862 Apollo are named , respectively .", "prompt_labels": "This(O) minor(O) planet(O) was(O) named(O) for(O) Aesculapius(B-astronomical object) ,(O) the(O) Greek(O) and(O) Roman(O) demigod(O) of(O) medicine(B-discipline) and(O) healing(O) ,(O) son(O) of(O) Apollo(B-person) and(O) Coronis(B-person) ,(O) after(O) whom(O) the(O) asteroids(O) 158(B-astronomical object) Koronis(I-astronomical object) and(O) 1862(B-astronomical object) Apollo(I-astronomical object) are(O) named(O) ,(O) respectively(O) .(O)"}}
{"id": "496", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "discipline", "event", "astronomical object", "protein", "award", "country", "theory", "person", "scientist", "chemical element", "chemical compound", "enzyme", "university", "location", "organization"], "instance": {"id": "496", "words": ["Loss", "of", "function", "MLH1", ",", "MSH2", ",", "MSH6", "or", "PMS2", "genes", "cause", "defective", "DNA", "mismatch", "repair", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, discipline, event, astronomical object, protein, award, country, theory, person, scientist, chemical element, chemical compound, enzyme, university, location, organization and O.\nSentence: Loss of function MLH1 , MSH2 , MSH6 or PMS2 genes cause defective DNA mismatch repair .", "prompt_labels": "Loss(O) of(O) function(O) MLH1(O) ,(O) MSH2(O) ,(O) MSH6(O) or(O) PMS2(O) genes(O) cause(O) defective(O) DNA(O) mismatch(O) repair(O) .(O)"}}
{"id": "36", "dataset": "crossner_science", "split": "test", "label_list": ["enzyme", "award", "discipline", "location", "protein", "university", "theory", "chemical compound", "event", "chemical element", "academic journal", "scientist", "organization", "person", "astronomical object", "country"], "instance": {"id": "36", "words": ["This", "type", "of", "oxidative", "stress", "causes", "a", "loss", "of", "NF-κB", "signaling", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, award, discipline, location, protein, university, theory, chemical compound, event, chemical element, academic journal, scientist, organization, person, astronomical object, country and O.\nSentence: This type of oxidative stress causes a loss of NF-κB signaling .", "prompt_labels": "This(O) type(O) of(O) oxidative(O) stress(O) causes(O) a(O) loss(O) of(O) NF-κB(B-protein) signaling(O) .(O)"}}
{"id": "423", "dataset": "crossner_science", "split": "test", "label_list": ["theory", "discipline", "academic journal", "university", "astronomical object", "location", "chemical compound", "event", "enzyme", "person", "protein", "chemical element", "organization", "award", "scientist", "country"], "instance": {"id": "423", "words": ["The", "purification", "of", "penicillin", "was", "achieved", "by", "Ernst", "Chain", ",", "Norman", "Heatley", "and", "Edward", "Abraham", ",", "with", "Chain", "and", "Abraham", "eventually", "developing", "a", "theoretical", "model", "for", "its", "chemical", "structure", ",", "which", "was", "later", "confirmed", "by", "Dorothy", "Hodgkin", "using", "X-ray", "crystallography", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "O", "B-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, discipline, academic journal, university, astronomical object, location, chemical compound, event, enzyme, person, protein, chemical element, organization, award, scientist, country and O.\nSentence: The purification of penicillin was achieved by Ernst Chain , Norman Heatley and Edward Abraham , with Chain and Abraham eventually developing a theoretical model for its chemical structure , which was later confirmed by Dorothy Hodgkin using X-ray crystallography .", "prompt_labels": "The(O) purification(O) of(O) penicillin(O) was(O) achieved(O) by(O) Ernst(B-scientist) Chain(I-scientist) ,(O) Norman(B-scientist) Heatley(I-scientist) and(O) Edward(B-scientist) Abraham(I-scientist) ,(O) with(O) Chain(B-scientist) and(O) Abraham(B-scientist) eventually(O) developing(O) a(O) theoretical(O) model(O) for(O) its(O) chemical(O) structure(O) ,(O) which(O) was(O) later(O) confirmed(O) by(O) Dorothy(B-scientist) Hodgkin(I-scientist) using(O) X-ray(O) crystallography(O) .(O)"}}
{"id": "527", "dataset": "crossner_science", "split": "test", "label_list": ["location", "chemical compound", "country", "astronomical object", "person", "event", "chemical element", "organization", "discipline", "university", "enzyme", "academic journal", "theory", "award", "protein", "scientist"], "instance": {"id": "527", "words": ["The", "journal", "has", "been", "cited", "most", "often", "by", "the", "following", "journals", "Journal", "of", "Clinical", "Pathology", ",", "Histopathology", ",", "Archives", "of", "Pathology", ";", "Laboratory", "Medicine", ",", "Human", "Pathology", ",", "and", "the", "World", "Journal", "of", "Gastroenterology", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, chemical compound, country, astronomical object, person, event, chemical element, organization, discipline, university, enzyme, academic journal, theory, award, protein, scientist and O.\nSentence: The journal has been cited most often by the following journals Journal of Clinical Pathology , Histopathology , Archives of Pathology ; Laboratory Medicine , Human Pathology , and the World Journal of Gastroenterology .", "prompt_labels": "The(O) journal(O) has(O) been(O) cited(O) most(O) often(O) by(O) the(O) following(O) journals(O) Journal(B-academic journal) of(I-academic journal) Clinical(I-academic journal) Pathology(I-academic journal) ,(O) Histopathology(B-academic journal) ,(O) Archives(B-academic journal) of(I-academic journal) Pathology(I-academic journal) ;(O) Laboratory(B-academic journal) Medicine(I-academic journal) ,(O) Human(B-academic journal) Pathology(I-academic journal) ,(O) and(O) the(O) World(B-academic journal) Journal(I-academic journal) of(I-academic journal) Gastroenterology(I-academic journal) .(O)"}}
{"id": "231", "dataset": "crossner_science", "split": "test", "label_list": ["astronomical object", "enzyme", "event", "university", "academic journal", "chemical element", "person", "organization", "theory", "discipline", "scientist", "protein", "award", "country", "chemical compound", "location"], "instance": {"id": "231", "words": ["Her", "children", "were", "Jupiter", ",", "Neptune", ",", "Pluto", ",", "Juno", ",", "Ceres", ",", "and", "Vesta", "."], "labels": ["O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, enzyme, event, university, academic journal, chemical element, person, organization, theory, discipline, scientist, protein, award, country, chemical compound, location and O.\nSentence: Her children were Jupiter , Neptune , Pluto , Juno , Ceres , and Vesta .", "prompt_labels": "Her(O) children(O) were(O) Jupiter(B-astronomical object) ,(O) Neptune(B-astronomical object) ,(O) Pluto(B-astronomical object) ,(O) Juno(B-astronomical object) ,(O) Ceres(B-astronomical object) ,(O) and(O) Vesta(B-astronomical object) .(O)"}}
{"id": "115", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "country", "location", "organization", "university", "discipline", "protein", "scientist", "award", "theory", "enzyme", "chemical element", "astronomical object", "chemical compound", "person", "event"], "instance": {"id": "115", "words": ["Observations", "are", "currently", "being", "coordinated", "by", "the", "Association", "of", "Lunar", "and", "Planetary", "Observers", "and", "the", "British", "Astronomical", "Association", "to", "re-observe", "sites", "where", "transient", "lunar", "phenomena", "were", "reported", "in", "the", "past", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, country, location, organization, university, discipline, protein, scientist, award, theory, enzyme, chemical element, astronomical object, chemical compound, person, event and O.\nSentence: Observations are currently being coordinated by the Association of Lunar and Planetary Observers and the British Astronomical Association to re-observe sites where transient lunar phenomena were reported in the past .", "prompt_labels": "Observations(O) are(O) currently(O) being(O) coordinated(O) by(O) the(O) Association(B-organization) of(I-organization) Lunar(I-organization) and(I-organization) Planetary(I-organization) Observers(I-organization) and(O) the(O) British(B-organization) Astronomical(I-organization) Association(I-organization) to(O) re-observe(O) sites(O) where(O) transient(O) lunar(O) phenomena(O) were(O) reported(O) in(O) the(O) past(O) .(O)"}}
{"id": "145", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "location", "award", "protein", "country", "organization", "event", "theory", "chemical compound", "chemical element", "university", "discipline", "astronomical object", "person", "enzyme", "scientist"], "instance": {"id": "145", "words": ["Three", "of", "the", "newly", "confirmed", "exoplanets", "were", "found", "to", "orbit", "within", "habitable", "zones", "of", "their", "related", "star", "s", ":", "two", "of", "the", "three", ",", "Kepler-438b", "and", "Kepler-442b", ",", "are", "near-Earth-size", "and", "likely", "rocky", ";", "the", "third", ",", "Kepler-440b", ",", "is", "a", "super-Earth", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, location, award, protein, country, organization, event, theory, chemical compound, chemical element, university, discipline, astronomical object, person, enzyme, scientist and O.\nSentence: Three of the newly confirmed exoplanets were found to orbit within habitable zones of their related star s : two of the three , Kepler-438b and Kepler-442b , are near-Earth-size and likely rocky ; the third , Kepler-440b , is a super-Earth .", "prompt_labels": "Three(O) of(O) the(O) newly(O) confirmed(O) exoplanets(O) were(O) found(O) to(O) orbit(O) within(O) habitable(O) zones(O) of(O) their(O) related(O) star(O) s(O) :(O) two(O) of(O) the(O) three(O) ,(O) Kepler-438b(B-astronomical object) and(O) Kepler-442b(B-astronomical object) ,(O) are(O) near-Earth-size(O) and(O) likely(O) rocky(O) ;(O) the(O) third(O) ,(O) Kepler-440b(B-astronomical object) ,(O) is(O) a(O) super-Earth(O) .(O)"}}
{"id": "361", "dataset": "crossner_science", "split": "test", "label_list": ["enzyme", "university", "chemical compound", "location", "scientist", "academic journal", "discipline", "event", "award", "chemical element", "protein", "astronomical object", "organization", "theory", "person", "country"], "instance": {"id": "361", "words": ["He", "has", "been", "published", "in", "a", "range", "of", "Academic", "journal", "s", ",", "such", "as", "the", "Cambridge", "Journal", "of", "Economics", ",", "the", "Oxford", "Bulletin", "of", "Economics", "and", "Statistics", "and", "the", "Oxford", "Review", "of", "Economic", "Policy", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, university, chemical compound, location, scientist, academic journal, discipline, event, award, chemical element, protein, astronomical object, organization, theory, person, country and O.\nSentence: He has been published in a range of Academic journal s , such as the Cambridge Journal of Economics , the Oxford Bulletin of Economics and Statistics and the Oxford Review of Economic Policy .", "prompt_labels": "He(O) has(O) been(O) published(O) in(O) a(O) range(O) of(O) Academic(O) journal(O) s(O) ,(O) such(O) as(O) the(O) Cambridge(B-academic journal) Journal(I-academic journal) of(I-academic journal) Economics(I-academic journal) ,(O) the(O) Oxford(B-academic journal) Bulletin(I-academic journal) of(I-academic journal) Economics(I-academic journal) and(I-academic journal) Statistics(I-academic journal) and(O) the(O) Oxford(B-academic journal) Review(I-academic journal) of(I-academic journal) Economic(I-academic journal) Policy(I-academic journal) .(O)"}}
{"id": "445", "dataset": "crossner_science", "split": "test", "label_list": ["protein", "scientist", "location", "discipline", "theory", "chemical element", "person", "enzyme", "chemical compound", "organization", "university", "event", "academic journal", "award", "astronomical object", "country"], "instance": {"id": "445", "words": ["To", "occur", ",", "this", "lobopodia-based", "fibroblast", "migration", "needs", "Nesprin", ",", "Integrin", ",", "RhoA", ",", "ROCK", "and", "myosin", "II", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "I-protein", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, scientist, location, discipline, theory, chemical element, person, enzyme, chemical compound, organization, university, event, academic journal, award, astronomical object, country and O.\nSentence: To occur , this lobopodia-based fibroblast migration needs Nesprin , Integrin , RhoA , ROCK and myosin II .", "prompt_labels": "To(O) occur(O) ,(O) this(O) lobopodia-based(O) fibroblast(O) migration(O) needs(O) Nesprin(B-protein) ,(O) Integrin(B-protein) ,(O) RhoA(B-protein) ,(O) ROCK(B-protein) and(O) myosin(B-protein) II(I-protein) .(O)"}}
{"id": "431", "dataset": "crossner_science", "split": "test", "label_list": ["location", "country", "chemical element", "discipline", "theory", "chemical compound", "event", "enzyme", "person", "astronomical object", "academic journal", "scientist", "university", "organization", "protein", "award"], "instance": {"id": "431", "words": ["He", "was", "later", "awarded", "the", "Gold", "Medal", "of", "the", "Royal", "Astronomical", "Society", "and", "the", "Royal", "Medal", "of", "the", "Royal", "Society", "for", "this", "work", "."], "labels": ["O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, chemical element, discipline, theory, chemical compound, event, enzyme, person, astronomical object, academic journal, scientist, university, organization, protein, award and O.\nSentence: He was later awarded the Gold Medal of the Royal Astronomical Society and the Royal Medal of the Royal Society for this work .", "prompt_labels": "He(O) was(O) later(O) awarded(O) the(O) Gold(B-award) Medal(I-award) of(I-award) the(I-award) Royal(I-award) Astronomical(I-award) Society(I-award) and(O) the(O) Royal(B-award) Medal(I-award) of(I-award) the(I-award) Royal(I-award) Society(I-award) for(O) this(O) work(O) .(O)"}}
{"id": "485", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "location", "university", "scientist", "award", "person", "chemical compound", "astronomical object", "event", "discipline", "theory", "academic journal", "country", "enzyme", "protein", "organization"], "instance": {"id": "485", "words": ["Starling", "'s", "successor", ",", "Archibald", "Hill", ",", "fostered", "the", "career", "of", "Bernard", "Katz", ",", "whose", "long", "association", "with", "UCL", "began", "in", "1935", "."], "labels": ["B-scientist", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "B-university", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, location, university, scientist, award, person, chemical compound, astronomical object, event, discipline, theory, academic journal, country, enzyme, protein, organization and O.\nSentence: Starling 's successor , Archibald Hill , fostered the career of Bernard Katz , whose long association with UCL began in 1935 .", "prompt_labels": "Starling(B-scientist) 's(O) successor(O) ,(O) Archibald(B-scientist) Hill(I-scientist) ,(O) fostered(O) the(O) career(O) of(O) Bernard(B-scientist) Katz(I-scientist) ,(O) whose(O) long(O) association(O) with(O) UCL(B-university) began(O) in(O) 1935(O) .(O)"}}
{"id": "35", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "chemical element", "organization", "theory", "astronomical object", "enzyme", "discipline", "location", "chemical compound", "protein", "scientist", "award", "university", "country", "person", "event"], "instance": {"id": "35", "words": ["The", "specific", "ligand", "of", "this", "receptor", "is", "CCL25", "To", "note", ",", "the", "chemokine", "binding", "protein", "D6", "had", "previously", "been", "named", "CCR9", ",", "but", "this", "molecule", "is", "a", "scavenger", "receptor", "not", "a", "TRUE", "(", "signaling", ")", "chemokine", "receptor", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "O", "O", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, chemical element, organization, theory, astronomical object, enzyme, discipline, location, chemical compound, protein, scientist, award, university, country, person, event and O.\nSentence: The specific ligand of this receptor is CCL25 To note , the chemokine binding protein D6 had previously been named CCR9 , but this molecule is a scavenger receptor not a TRUE ( signaling ) chemokine receptor .", "prompt_labels": "The(O) specific(O) ligand(O) of(O) this(O) receptor(O) is(O) CCL25(B-protein) To(O) note(O) ,(O) the(O) chemokine(O) binding(O) protein(O) D6(B-protein) had(O) previously(O) been(O) named(O) CCR9(B-protein) ,(O) but(O) this(O) molecule(O) is(O) a(O) scavenger(O) receptor(O) not(O) a(O) TRUE(O) ((O) signaling(O) )(O) chemokine(O) receptor(O) .(O)"}}
{"id": "343", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "protein", "scientist", "chemical compound", "location", "discipline", "enzyme", "theory", "organization", "country", "astronomical object", "person", "award", "academic journal", "university", "event"], "instance": {"id": "343", "words": ["Declaring", "an", "arsenal", "of", "39,967", "tons", "of", "chemical", "weapons", "in", "1997", ",", "by", "far", "the", "largest", "arsenal", ",", "consisting", "of", "blister", "agents", ":", "Lewisite", ",", "Sulfur", "mustard", ",", "Lewisite-mustard", "mix", ",", "and", "nerve", "agents", ":", "Sarin", ",", "Soman", ",", "and", "VX", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "I-chemical compound", "O", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "O", "O", "B-chemical compound", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, protein, scientist, chemical compound, location, discipline, enzyme, theory, organization, country, astronomical object, person, award, academic journal, university, event and O.\nSentence: Declaring an arsenal of 39,967 tons of chemical weapons in 1997 , by far the largest arsenal , consisting of blister agents : Lewisite , Sulfur mustard , Lewisite-mustard mix , and nerve agents : Sarin , Soman , and VX .", "prompt_labels": "Declaring(O) an(O) arsenal(O) of(O) 39,967(O) tons(O) of(O) chemical(O) weapons(O) in(O) 1997(O) ,(O) by(O) far(O) the(O) largest(O) arsenal(O) ,(O) consisting(O) of(O) blister(B-chemical compound) agents(I-chemical compound) :(O) Lewisite(B-chemical compound) ,(O) Sulfur(B-chemical compound) mustard(I-chemical compound) ,(O) Lewisite-mustard(B-chemical compound) mix(I-chemical compound) ,(O) and(O) nerve(B-chemical compound) agents(I-chemical compound) :(O) Sarin(B-chemical compound) ,(O) Soman(B-chemical compound) ,(O) and(O) VX(B-chemical compound) .(O)"}}
{"id": "503", "dataset": "crossner_science", "split": "test", "label_list": ["university", "event", "organization", "country", "chemical element", "award", "discipline", "theory", "chemical compound", "enzyme", "astronomical object", "scientist", "person", "location", "protein", "academic journal"], "instance": {"id": "503", "words": ["The", "2013", "Nobel", "Peace", "Prize", "was", "awarded", "to", "the", "organization", "because", "it", "had", ",", "with", "the", "Chemical", "Weapons", "Convention", ",", "defined", "the", "use", "of", "chemical", "weapons", "as", "a", "taboo", "under", "international", "law", "according", "to", "Thorbjørn", "Jagland", ",", "Chairman", "of", "the", "Norwegian", "Nobel", "Committee", "."], "labels": ["O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, event, organization, country, chemical element, award, discipline, theory, chemical compound, enzyme, astronomical object, scientist, person, location, protein, academic journal and O.\nSentence: The 2013 Nobel Peace Prize was awarded to the organization because it had , with the Chemical Weapons Convention , defined the use of chemical weapons as a taboo under international law according to Thorbjørn Jagland , Chairman of the Norwegian Nobel Committee .", "prompt_labels": "The(O) 2013(B-award) Nobel(I-award) Peace(I-award) Prize(I-award) was(O) awarded(O) to(O) the(O) organization(O) because(O) it(O) had(O) ,(O) with(O) the(O) Chemical(O) Weapons(O) Convention(O) ,(O) defined(O) the(O) use(O) of(O) chemical(O) weapons(O) as(O) a(O) taboo(O) under(O) international(O) law(O) according(O) to(O) Thorbjørn(B-person) Jagland(I-person) ,(O) Chairman(O) of(O) the(O) Norwegian(B-organization) Nobel(I-organization) Committee(I-organization) .(O)"}}
{"id": "204", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "discipline", "enzyme", "scientist", "university", "person", "event", "academic journal", "country", "chemical compound", "organization", "award", "astronomical object", "theory", "location", "protein"], "instance": {"id": "204", "words": ["In", "addition", ",", "scientific", "results", "obtained", "using", "HAARP", "are", "routinely", "published", "in", "major", "research", "journals", "(", "such", "as", "Geophysical", "Research", "Letters", "and", "Journal", "of", "Geophysical", "Research", ")", ",", "written", "both", "by", "university", "scientists", "(", "American", "and", "foreign", ")", "and", "by", "U.S.", "Department", "of", "Defense", "research", "lab", "scientists", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, discipline, enzyme, scientist, university, person, event, academic journal, country, chemical compound, organization, award, astronomical object, theory, location, protein and O.\nSentence: In addition , scientific results obtained using HAARP are routinely published in major research journals ( such as Geophysical Research Letters and Journal of Geophysical Research ) , written both by university scientists ( American and foreign ) and by U.S. Department of Defense research lab scientists .", "prompt_labels": "In(O) addition(O) ,(O) scientific(O) results(O) obtained(O) using(O) HAARP(O) are(O) routinely(O) published(O) in(O) major(O) research(O) journals(O) ((O) such(O) as(O) Geophysical(B-academic journal) Research(I-academic journal) Letters(I-academic journal) and(O) Journal(B-academic journal) of(I-academic journal) Geophysical(I-academic journal) Research(I-academic journal) )(O) ,(O) written(O) both(O) by(O) university(O) scientists(O) ((O) American(O) and(O) foreign(O) )(O) and(O) by(O) U.S.(B-organization) Department(I-organization) of(I-organization) Defense(I-organization) research(I-organization) lab(I-organization) scientists(O) .(O)"}}
{"id": "161", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "country", "academic journal", "award", "discipline", "enzyme", "university", "chemical compound", "location", "astronomical object", "theory", "organization", "protein", "scientist", "person", "event"], "instance": {"id": "161", "words": ["In", "1896", ",", "he", "translated", ",", "from", "German", ",", "Heimat", ",", "a", "play", "in", "four", "acts", "by", "Hermann", "Sudermann", ",", "renamed", "Magda", "and", "played", "by", "Henry", "Stephenson", "and", "Charles", "Waldron", "in", "a", "Broadway", "theatre", "production", "in", "New", "York", "City", ",", "New", "York", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "B-person", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, country, academic journal, award, discipline, enzyme, university, chemical compound, location, astronomical object, theory, organization, protein, scientist, person, event and O.\nSentence: In 1896 , he translated , from German , Heimat , a play in four acts by Hermann Sudermann , renamed Magda and played by Henry Stephenson and Charles Waldron in a Broadway theatre production in New York City , New York .", "prompt_labels": "In(O) 1896(O) ,(O) he(O) translated(O) ,(O) from(O) German(O) ,(O) Heimat(O) ,(O) a(O) play(O) in(O) four(O) acts(O) by(O) Hermann(B-person) Sudermann(I-person) ,(O) renamed(O) Magda(B-person) and(O) played(O) by(O) Henry(B-person) Stephenson(I-person) and(O) Charles(B-person) Waldron(I-person) in(O) a(O) Broadway(O) theatre(O) production(O) in(O) New(B-location) York(I-location) City(I-location) ,(O) New(B-location) York(I-location) .(O)"}}
{"id": "468", "dataset": "crossner_science", "split": "test", "label_list": ["university", "event", "chemical compound", "person", "theory", "organization", "chemical element", "protein", "scientist", "astronomical object", "award", "location", "enzyme", "country", "discipline", "academic journal"], "instance": {"id": "468", "words": ["Three", "of", "the", "newly", "confirmed", "exoplanets", "were", "found", "to", "orbit", "within", "habitable", "zone", "s", "of", "their", "related", "star", "s", ":", "two", "of", "the", "three", ",", "Kepler-438b", "and", "Kepler-442b", ",", "are", "near-Earth-size", "and", "likely", "rocky", ";", "the", "third", ",", "Kepler-440b", ",", "is", "a", "super-Earth", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, event, chemical compound, person, theory, organization, chemical element, protein, scientist, astronomical object, award, location, enzyme, country, discipline, academic journal and O.\nSentence: Three of the newly confirmed exoplanets were found to orbit within habitable zone s of their related star s : two of the three , Kepler-438b and Kepler-442b , are near-Earth-size and likely rocky ; the third , Kepler-440b , is a super-Earth .", "prompt_labels": "Three(O) of(O) the(O) newly(O) confirmed(O) exoplanets(O) were(O) found(O) to(O) orbit(O) within(O) habitable(O) zone(O) s(O) of(O) their(O) related(O) star(O) s(O) :(O) two(O) of(O) the(O) three(O) ,(O) Kepler-438b(B-astronomical object) and(O) Kepler-442b(B-astronomical object) ,(O) are(O) near-Earth-size(O) and(O) likely(O) rocky(O) ;(O) the(O) third(O) ,(O) Kepler-440b(B-astronomical object) ,(O) is(O) a(O) super-Earth(O) .(O)"}}
{"id": "381", "dataset": "crossner_science", "split": "test", "label_list": ["location", "protein", "organization", "theory", "person", "academic journal", "enzyme", "award", "chemical compound", "scientist", "country", "astronomical object", "event", "university", "chemical element", "discipline"], "instance": {"id": "381", "words": ["In", "1855", ",", "he", "received", "the", "Copley", "Medal", "of", "the", "Royal", "Society", "for", "his", "'", "very", "remarkable", "experimental", "researches", "'", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, protein, organization, theory, person, academic journal, enzyme, award, chemical compound, scientist, country, astronomical object, event, university, chemical element, discipline and O.\nSentence: In 1855 , he received the Copley Medal of the Royal Society for his ' very remarkable experimental researches ' .", "prompt_labels": "In(O) 1855(O) ,(O) he(O) received(O) the(O) Copley(B-award) Medal(I-award) of(O) the(O) Royal(B-organization) Society(I-organization) for(O) his(O) '(O) very(O) remarkable(O) experimental(O) researches(O) '(O) .(O)"}}
{"id": "225", "dataset": "crossner_science", "split": "test", "label_list": ["chemical compound", "organization", "university", "discipline", "astronomical object", "theory", "event", "country", "academic journal", "protein", "chemical element", "person", "location", "scientist", "award", "enzyme"], "instance": {"id": "225", "words": ["Histone", "s", ",", "gene", "regulatory", "proteins", ",", "DNA", "polymerase", "and", "RNA", "polymerase", "s", ",", "and", "other", "substances", "essential", "for", "nuclear", "activities", "must", "be", "imported", "from", "the", "cytoplasm", "."], "labels": ["B-protein", "O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "O", "B-enzyme", "I-enzyme", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, organization, university, discipline, astronomical object, theory, event, country, academic journal, protein, chemical element, person, location, scientist, award, enzyme and O.\nSentence: Histone s , gene regulatory proteins , DNA polymerase and RNA polymerase s , and other substances essential for nuclear activities must be imported from the cytoplasm .", "prompt_labels": "Histone(B-protein) s(O) ,(O) gene(O) regulatory(O) proteins(O) ,(O) DNA(B-enzyme) polymerase(I-enzyme) and(O) RNA(B-enzyme) polymerase(I-enzyme) s(O) ,(O) and(O) other(O) substances(O) essential(O) for(O) nuclear(O) activities(O) must(O) be(O) imported(O) from(O) the(O) cytoplasm(O) .(O)"}}
{"id": "538", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "scientist", "astronomical object", "organization", "theory", "person", "academic journal", "award", "country", "chemical compound", "protein", "university", "location", "event", "discipline", "enzyme"], "instance": {"id": "538", "words": ["The", "staff", "also", "produce", "articles", "on", "human", "and", "animal", "genetics", "topics", "such", "as", "gene", "splicing", ",", "CRISPR", ",", "government", "regulation", ",", "bioethics", ",", "use", "of", "stem", "cells", ",", "transhumanism", ",", "nanotechnology", "and", "synthetic", "biology", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-discipline", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-discipline", "O", "O", "O", "O", "O", "O", "B-theory", "O", "B-discipline", "O", "B-discipline", "I-discipline", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, scientist, astronomical object, organization, theory, person, academic journal, award, country, chemical compound, protein, university, location, event, discipline, enzyme and O.\nSentence: The staff also produce articles on human and animal genetics topics such as gene splicing , CRISPR , government regulation , bioethics , use of stem cells , transhumanism , nanotechnology and synthetic biology .", "prompt_labels": "The(O) staff(O) also(O) produce(O) articles(O) on(O) human(O) and(O) animal(O) genetics(B-discipline) topics(O) such(O) as(O) gene(O) splicing(O) ,(O) CRISPR(O) ,(O) government(O) regulation(O) ,(O) bioethics(B-discipline) ,(O) use(O) of(O) stem(O) cells(O) ,(O) transhumanism(B-theory) ,(O) nanotechnology(B-discipline) and(O) synthetic(B-discipline) biology(I-discipline) .(O)"}}
{"id": "483", "dataset": "crossner_science", "split": "test", "label_list": ["award", "person", "chemical compound", "chemical element", "astronomical object", "country", "protein", "university", "organization", "academic journal", "scientist", "enzyme", "discipline", "event", "location", "theory"], "instance": {"id": "483", "words": ["It", "was", "discovered", "on", "22", "February", "1938", ",", "by", "Finnish", "astronomer", "Yrjö", "Väisälä", "at", "Turku", "Observatory", "in", "Southwest", "Finland", ",", "and", "later", "named", "for", "Johannes", "Gabriel", "Granö", ",", "rector", "of", "the", "University", "of", "Turku", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, chemical compound, chemical element, astronomical object, country, protein, university, organization, academic journal, scientist, enzyme, discipline, event, location, theory and O.\nSentence: It was discovered on 22 February 1938 , by Finnish astronomer Yrjö Väisälä at Turku Observatory in Southwest Finland , and later named for Johannes Gabriel Granö , rector of the University of Turku .", "prompt_labels": "It(O) was(O) discovered(O) on(O) 22(O) February(O) 1938(O) ,(O) by(O) Finnish(O) astronomer(O) Yrjö(B-scientist) Väisälä(I-scientist) at(O) Turku(B-scientist) Observatory(I-scientist) in(O) Southwest(B-location) Finland(I-location) ,(O) and(O) later(O) named(O) for(O) Johannes(B-scientist) Gabriel(I-scientist) Granö(I-scientist) ,(O) rector(O) of(O) the(O) University(B-university) of(I-university) Turku(I-university) .(O)"}}
{"id": "158", "dataset": "crossner_science", "split": "test", "label_list": ["astronomical object", "scientist", "person", "enzyme", "university", "country", "discipline", "academic journal", "event", "protein", "award", "location", "chemical compound", "theory", "organization", "chemical element"], "instance": {"id": "158", "words": ["The", "most", "common", "herbicide", "used", "was", "Herbicide", "Orange", ",", "more", "commonly", "referred", "to", "as", "Agent", "Orange", ":", "a", "fifty-fifty", "mixture", "of", "two", "herbicides", "2,4-Dichlorophenoxyacetic", "acid", "(", "2,4-dichlorophenoxyacetic", "acid", ")", "and", "2,4,5-Trichlorophenoxyacetic", "acid", "(", "2,4,5-trichlorophenoxyacetic", "acid", ")", "manufactured", "for", "the", "U.S.", "Department", "of", "Defense", "primarily", "by", "Monsanto", "Corporation", "and", "Dow", "Chemical", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "I-chemical compound", "O", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "I-chemical compound", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, scientist, person, enzyme, university, country, discipline, academic journal, event, protein, award, location, chemical compound, theory, organization, chemical element and O.\nSentence: The most common herbicide used was Herbicide Orange , more commonly referred to as Agent Orange : a fifty-fifty mixture of two herbicides 2,4-Dichlorophenoxyacetic acid ( 2,4-dichlorophenoxyacetic acid ) and 2,4,5-Trichlorophenoxyacetic acid ( 2,4,5-trichlorophenoxyacetic acid ) manufactured for the U.S. Department of Defense primarily by Monsanto Corporation and Dow Chemical .", "prompt_labels": "The(O) most(O) common(O) herbicide(O) used(O) was(O) Herbicide(B-chemical compound) Orange(I-chemical compound) ,(O) more(O) commonly(O) referred(O) to(O) as(O) Agent(B-chemical compound) Orange(I-chemical compound) :(O) a(O) fifty-fifty(O) mixture(O) of(O) two(O) herbicides(O) 2,4-Dichlorophenoxyacetic(B-chemical compound) acid(I-chemical compound) ((O) 2,4-dichlorophenoxyacetic(B-chemical compound) acid(I-chemical compound) )(O) and(O) 2,4,5-Trichlorophenoxyacetic(B-chemical compound) acid(I-chemical compound) ((O) 2,4,5-trichlorophenoxyacetic(B-chemical compound) acid(I-chemical compound) )(O) manufactured(O) for(O) the(O) U.S.(B-organization) Department(I-organization) of(I-organization) Defense(I-organization) primarily(O) by(O) Monsanto(B-organization) Corporation(I-organization) and(O) Dow(B-organization) Chemical(I-organization) .(O)"}}
{"id": "529", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "person", "protein", "enzyme", "scientist", "astronomical object", "theory", "country", "award", "event", "discipline", "chemical compound", "location", "organization", "university", "academic journal"], "instance": {"id": "529", "words": ["From", "1917", ",", "he", "worked", "a", "few", "years", "with", "Niels", "Bohr", "in", "the", "University", "of", "Copenhagen", "and", "received", "his", "doctoral", "degree", "at", "the", "University", "College", "of", "Stockholm", "(", "now", "Stockholm", "University", ")", "in", "1921", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "O", "O", "B-university", "I-university", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, person, protein, enzyme, scientist, astronomical object, theory, country, award, event, discipline, chemical compound, location, organization, university, academic journal and O.\nSentence: From 1917 , he worked a few years with Niels Bohr in the University of Copenhagen and received his doctoral degree at the University College of Stockholm ( now Stockholm University ) in 1921 .", "prompt_labels": "From(O) 1917(O) ,(O) he(O) worked(O) a(O) few(O) years(O) with(O) Niels(B-scientist) Bohr(I-scientist) in(O) the(O) University(B-university) of(I-university) Copenhagen(I-university) and(O) received(O) his(O) doctoral(O) degree(O) at(O) the(O) University(B-university) College(I-university) of(I-university) Stockholm(I-university) ((O) now(O) Stockholm(B-university) University(I-university) )(O) in(O) 1921(O) .(O)"}}
{"id": "68", "dataset": "crossner_science", "split": "test", "label_list": ["location", "person", "university", "country", "award", "scientist", "astronomical object", "protein", "theory", "event", "chemical compound", "discipline", "academic journal", "enzyme", "organization", "chemical element"], "instance": {"id": "68", "words": ["Neurexin", "-", "Neuroligin", "interactions", "help", "establish", "the", "trans-synaptic", "functional", "asymmetry", "essential", "for", "the", "stabilization", "and", "maintenance", "of", "proper", "synaptic", "transmission", "."], "labels": ["B-protein", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, university, country, award, scientist, astronomical object, protein, theory, event, chemical compound, discipline, academic journal, enzyme, organization, chemical element and O.\nSentence: Neurexin - Neuroligin interactions help establish the trans-synaptic functional asymmetry essential for the stabilization and maintenance of proper synaptic transmission .", "prompt_labels": "Neurexin(B-protein) -(O) Neuroligin(B-protein) interactions(O) help(O) establish(O) the(O) trans-synaptic(O) functional(O) asymmetry(O) essential(O) for(O) the(O) stabilization(O) and(O) maintenance(O) of(O) proper(O) synaptic(O) transmission(O) .(O)"}}
{"id": "465", "dataset": "crossner_science", "split": "test", "label_list": ["astronomical object", "enzyme", "person", "university", "country", "discipline", "location", "award", "theory", "chemical compound", "protein", "chemical element", "scientist", "event", "organization", "academic journal"], "instance": {"id": "465", "words": ["Minimum", "distances", "from", "the", "Earth", ",", "Venus", ",", "and", "Jupiter", ",", "are", "0.5", ",", "0.8", ",", "and", "3.5", "AU", ",", "respectively", "."], "labels": ["O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, enzyme, person, university, country, discipline, location, award, theory, chemical compound, protein, chemical element, scientist, event, organization, academic journal and O.\nSentence: Minimum distances from the Earth , Venus , and Jupiter , are 0.5 , 0.8 , and 3.5 AU , respectively .", "prompt_labels": "Minimum(O) distances(O) from(O) the(O) Earth(B-astronomical object) ,(O) Venus(B-astronomical object) ,(O) and(O) Jupiter(B-astronomical object) ,(O) are(O) 0.5(O) ,(O) 0.8(O) ,(O) and(O) 3.5(O) AU(O) ,(O) respectively(O) .(O)"}}
{"id": "248", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "organization", "location", "award", "university", "theory", "astronomical object", "event", "chemical element", "enzyme", "scientist", "person", "discipline", "protein", "country", "chemical compound"], "instance": {"id": "248", "words": ["This", "culminates", "in", "the", "translocation", "of", "the", "NF-κB", "transcription", "factor", "Relish", ",", "leading", "to", "production", "of", "antimicrobial", "peptides", "and", "other", "effectors", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, organization, location, award, university, theory, astronomical object, event, chemical element, enzyme, scientist, person, discipline, protein, country, chemical compound and O.\nSentence: This culminates in the translocation of the NF-κB transcription factor Relish , leading to production of antimicrobial peptides and other effectors .", "prompt_labels": "This(O) culminates(O) in(O) the(O) translocation(O) of(O) the(O) NF-κB(B-protein) transcription(O) factor(O) Relish(O) ,(O) leading(O) to(O) production(O) of(O) antimicrobial(O) peptides(O) and(O) other(O) effectors(O) .(O)"}}
{"id": "420", "dataset": "crossner_science", "split": "test", "label_list": ["scientist", "enzyme", "location", "university", "astronomical object", "academic journal", "country", "event", "protein", "discipline", "chemical compound", "chemical element", "award", "organization", "theory", "person"], "instance": {"id": "420", "words": ["He", "became", "an", "expert", "on", "the", "electrical", "conductivity", "of", "gases", ",", "the", "properties", "of", "ion", "s", ",", "and", "the", "behavior", "of", "atmospheric", "electricity", ",", "publishing", "in", "journals", "including", "the", "Physical", "Review", ",", "Journal", "of", "Applied", "Physics", ",", "Journal", "of", "Chemical", "Physics", ",", "and", "the", "Journal", "of", "Atmospheric", "Electricity", "and", "Terrestrial", "Magnetism", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, enzyme, location, university, astronomical object, academic journal, country, event, protein, discipline, chemical compound, chemical element, award, organization, theory, person and O.\nSentence: He became an expert on the electrical conductivity of gases , the properties of ion s , and the behavior of atmospheric electricity , publishing in journals including the Physical Review , Journal of Applied Physics , Journal of Chemical Physics , and the Journal of Atmospheric Electricity and Terrestrial Magnetism .", "prompt_labels": "He(O) became(O) an(O) expert(O) on(O) the(O) electrical(O) conductivity(O) of(O) gases(O) ,(O) the(O) properties(O) of(O) ion(O) s(O) ,(O) and(O) the(O) behavior(O) of(O) atmospheric(O) electricity(O) ,(O) publishing(O) in(O) journals(O) including(O) the(O) Physical(B-academic journal) Review(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Applied(I-academic journal) Physics(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Chemical(I-academic journal) Physics(I-academic journal) ,(O) and(O) the(O) Journal(B-academic journal) of(I-academic journal) Atmospheric(I-academic journal) Electricity(I-academic journal) and(I-academic journal) Terrestrial(I-academic journal) Magnetism(I-academic journal) .(O)"}}
{"id": "12", "dataset": "crossner_science", "split": "test", "label_list": ["protein", "person", "scientist", "chemical compound", "award", "country", "organization", "discipline", "event", "enzyme", "university", "chemical element", "location", "academic journal", "theory", "astronomical object"], "instance": {"id": "12", "words": ["Polyethylene", "terephthalate", "(", "PET", ")", "bottles", "are", "made", "from", "ethylene", "and", "P-Xylene", "."], "labels": ["B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "O", "O", "O", "O", "O", "B-chemical compound", "O", "B-chemical compound", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, person, scientist, chemical compound, award, country, organization, discipline, event, enzyme, university, chemical element, location, academic journal, theory, astronomical object and O.\nSentence: Polyethylene terephthalate ( PET ) bottles are made from ethylene and P-Xylene .", "prompt_labels": "Polyethylene(B-chemical compound) terephthalate(I-chemical compound) ((O) PET(B-chemical compound) )(O) bottles(O) are(O) made(O) from(O) ethylene(B-chemical compound) and(O) P-Xylene(B-chemical compound) .(O)"}}
{"id": "473", "dataset": "crossner_science", "split": "test", "label_list": ["country", "award", "chemical compound", "theory", "organization", "university", "person", "discipline", "enzyme", "astronomical object", "scientist", "protein", "academic journal", "location", "event", "chemical element"], "instance": {"id": "473", "words": ["Their", "engagement", "with", "specific", "ideas", "relating", "to", "feminism", ",", "technoscience", ",", "political", "consciousness", ",", "and", "other", "social", "issues", ",", "formed", "the", "images", "and", "narrative", "of", "Haraway", "'s", "book", "Modest", "_", "Witness", "for", "which", "she", "received", "the", "Society", "for", "Social", "Studies", "of", "Science", "'", "s", "(", "4S", ")", "Ludwik", "Fleck", "Prize", "in", "1999", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-organization", "O", "B-award", "I-award", "I-award", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, chemical compound, theory, organization, university, person, discipline, enzyme, astronomical object, scientist, protein, academic journal, location, event, chemical element and O.\nSentence: Their engagement with specific ideas relating to feminism , technoscience , political consciousness , and other social issues , formed the images and narrative of Haraway 's book Modest _ Witness for which she received the Society for Social Studies of Science ' s ( 4S ) Ludwik Fleck Prize in 1999 .", "prompt_labels": "Their(O) engagement(O) with(O) specific(O) ideas(O) relating(O) to(O) feminism(O) ,(O) technoscience(O) ,(O) political(O) consciousness(O) ,(O) and(O) other(O) social(O) issues(O) ,(O) formed(O) the(O) images(O) and(O) narrative(O) of(O) Haraway(B-scientist) 's(O) book(O) Modest(O) _(O) Witness(O) for(O) which(O) she(O) received(O) the(O) Society(B-organization) for(I-organization) Social(I-organization) Studies(I-organization) of(I-organization) Science(I-organization) '(O) s(O) ((O) 4S(B-organization) )(O) Ludwik(B-award) Fleck(I-award) Prize(I-award) in(O) 1999(O) .(O)"}}
{"id": "523", "dataset": "crossner_science", "split": "test", "label_list": ["protein", "award", "enzyme", "organization", "country", "theory", "discipline", "university", "astronomical object", "event", "scientist", "chemical element", "location", "person", "academic journal", "chemical compound"], "instance": {"id": "523", "words": ["It", "is", "one", "of", "the", "five", "Nobel", "Prize", "s", "established", "by", "the", "will", "of", "Alfred", "Nobel", "in", "1895", "and", "awarded", "since", "1901", ";", "the", "others", "being", "the", "Nobel", "Prize", "in", "Chemistry", ",", "Nobel", "Prize", "in", "Literature", ",", "Nobel", "Peace", "Prize", ",", "and", "Nobel", "Prize", "in", "Physiology", "or", "Medicine", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, award, enzyme, organization, country, theory, discipline, university, astronomical object, event, scientist, chemical element, location, person, academic journal, chemical compound and O.\nSentence: It is one of the five Nobel Prize s established by the will of Alfred Nobel in 1895 and awarded since 1901 ; the others being the Nobel Prize in Chemistry , Nobel Prize in Literature , Nobel Peace Prize , and Nobel Prize in Physiology or Medicine .", "prompt_labels": "It(O) is(O) one(O) of(O) the(O) five(O) Nobel(B-award) Prize(I-award) s(O) established(O) by(O) the(O) will(O) of(O) Alfred(B-scientist) Nobel(I-scientist) in(O) 1895(O) and(O) awarded(O) since(O) 1901(O) ;(O) the(O) others(O) being(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Chemistry(I-award) ,(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) ,(O) Nobel(B-award) Peace(I-award) Prize(I-award) ,(O) and(O) Nobel(B-award) Prize(I-award) in(I-award) Physiology(I-award) or(I-award) Medicine(I-award) .(O)"}}
{"id": "480", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "country", "organization", "scientist", "university", "person", "award", "enzyme", "event", "astronomical object", "theory", "chemical compound", "chemical element", "protein", "location", "discipline"], "instance": {"id": "480", "words": ["His", "father", "encouraged", "him", "to", "apply", "to", "United", "States", "Military", "Academy", ",", "but", "he", "decided", "to", "enroll", "in", "the", "United", "States", "Naval", "Academy", "instead", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, country, organization, scientist, university, person, award, enzyme, event, astronomical object, theory, chemical compound, chemical element, protein, location, discipline and O.\nSentence: His father encouraged him to apply to United States Military Academy , but he decided to enroll in the United States Naval Academy instead .", "prompt_labels": "His(O) father(O) encouraged(O) him(O) to(O) apply(O) to(O) United(B-university) States(I-university) Military(I-university) Academy(I-university) ,(O) but(O) he(O) decided(O) to(O) enroll(O) in(O) the(O) United(B-university) States(I-university) Naval(I-university) Academy(I-university) instead(O) .(O)"}}
{"id": "186", "dataset": "crossner_science", "split": "test", "label_list": ["theory", "country", "scientist", "event", "organization", "protein", "chemical element", "enzyme", "university", "person", "location", "astronomical object", "chemical compound", "award", "discipline", "academic journal"], "instance": {"id": "186", "words": ["From", "1930", "to", "1934", ",", "Wu", "studied", "at", "National", "Central", "University", "(", "later", "renamed", "Nanjing", "University", ")", ",", "first", "in", "mathematics", ",", "but", "later", "transferring", "to", "physics", "."], "labels": ["O", "O", "O", "O", "O", "B-scientist", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "B-university", "I-university", "O", "O", "O", "O", "B-discipline", "O", "O", "O", "O", "O", "B-discipline", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, country, scientist, event, organization, protein, chemical element, enzyme, university, person, location, astronomical object, chemical compound, award, discipline, academic journal and O.\nSentence: From 1930 to 1934 , Wu studied at National Central University ( later renamed Nanjing University ) , first in mathematics , but later transferring to physics .", "prompt_labels": "From(O) 1930(O) to(O) 1934(O) ,(O) Wu(B-scientist) studied(O) at(O) National(B-university) Central(I-university) University(I-university) ((O) later(O) renamed(O) Nanjing(B-university) University(I-university) )(O) ,(O) first(O) in(O) mathematics(B-discipline) ,(O) but(O) later(O) transferring(O) to(O) physics(B-discipline) .(O)"}}
{"id": "61", "dataset": "crossner_science", "split": "test", "label_list": ["person", "chemical compound", "location", "protein", "theory", "award", "academic journal", "chemical element", "organization", "astronomical object", "scientist", "university", "event", "discipline", "country", "enzyme"], "instance": {"id": "61", "words": ["He", "won", "the", "J.", "Lawrence", "Smith", "Medal", "from", "the", "National", "Academy", "of", "Sciences", "in", "1960", ",", "the", "Meteoritical", "Society", "Frederick", "C.", "Leonard", "Memorial", "Medal", "in", "1968", ",", "the", "Kepler", "Gold", "Medal", "from", "the", "American", "Association", "for", "the", "Advancement", "of", "Science", "&", "amp", ";", "Meteoritical", "Society", "in", "1972", ",", "the", "Gold", "Medal", "of", "the", "Royal", "Astronomical", "Society", "in", "1975", "and", "the", "Bruce", "Medal", "in", "1976", "."], "labels": ["O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, chemical compound, location, protein, theory, award, academic journal, chemical element, organization, astronomical object, scientist, university, event, discipline, country, enzyme and O.\nSentence: He won the J. Lawrence Smith Medal from the National Academy of Sciences in 1960 , the Meteoritical Society Frederick C. Leonard Memorial Medal in 1968 , the Kepler Gold Medal from the American Association for the Advancement of Science & amp ; Meteoritical Society in 1972 , the Gold Medal of the Royal Astronomical Society in 1975 and the Bruce Medal in 1976 .", "prompt_labels": "He(O) won(O) the(O) J.(B-award) Lawrence(I-award) Smith(I-award) Medal(I-award) from(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) in(O) 1960(O) ,(O) the(O) Meteoritical(B-award) Society(I-award) Frederick(I-award) C.(I-award) Leonard(I-award) Memorial(I-award) Medal(I-award) in(O) 1968(O) ,(O) the(O) Kepler(B-award) Gold(I-award) Medal(I-award) from(O) the(O) American(B-organization) Association(I-organization) for(O) the(O) Advancement(O) of(O) Science(O) &(O) amp(O) ;(O) Meteoritical(O) Society(O) in(O) 1972(O) ,(O) the(O) Gold(B-award) Medal(I-award) of(I-award) the(I-award) Royal(I-award) Astronomical(I-award) Society(I-award) in(O) 1975(O) and(O) the(O) Bruce(B-award) Medal(I-award) in(O) 1976(O) .(O)"}}
{"id": "207", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "location", "chemical compound", "enzyme", "university", "discipline", "chemical element", "theory", "scientist", "person", "protein", "country", "event", "organization", "astronomical object", "award"], "instance": {"id": "207", "words": ["Bangladesh", "is", "a", "member", "of", "the", "UN", ",", "World", "Trade", "Organization", ",", "International", "Monetary", "Fund", ",", "the", "World", "Bank", ",", "Asian", "Development", "Bank", ",", "OIC", ",", "Islamic", "Development", "Bank", ",", "SAARC", ",", "BIMSTEC", "and", "the", "Islamic", "Military", "Counter", "Terrorism", "Coalition", "."], "labels": ["B-country", "O", "O", "O", "O", "O", "B-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "B-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, location, chemical compound, enzyme, university, discipline, chemical element, theory, scientist, person, protein, country, event, organization, astronomical object, award and O.\nSentence: Bangladesh is a member of the UN , World Trade Organization , International Monetary Fund , the World Bank , Asian Development Bank , OIC , Islamic Development Bank , SAARC , BIMSTEC and the Islamic Military Counter Terrorism Coalition .", "prompt_labels": "Bangladesh(B-country) is(O) a(O) member(O) of(O) the(O) UN(B-organization) ,(O) World(B-organization) Trade(I-organization) Organization(I-organization) ,(O) International(B-organization) Monetary(I-organization) Fund(I-organization) ,(O) the(O) World(B-organization) Bank(I-organization) ,(O) Asian(B-organization) Development(I-organization) Bank(I-organization) ,(O) OIC(B-organization) ,(O) Islamic(B-organization) Development(I-organization) Bank(I-organization) ,(O) SAARC(B-organization) ,(O) BIMSTEC(B-organization) and(O) the(O) Islamic(B-organization) Military(I-organization) Counter(I-organization) Terrorism(I-organization) Coalition(I-organization) .(O)"}}
{"id": "505", "dataset": "crossner_science", "split": "test", "label_list": ["organization", "discipline", "enzyme", "location", "protein", "event", "chemical compound", "academic journal", "award", "scientist", "person", "country", "chemical element", "astronomical object", "university", "theory"], "instance": {"id": "505", "words": ["Some", "notable", "examples", "include", "Journal", "of", "Computational", "Biology", "and", "PLOS", "Computational", "Biology", "."], "labels": ["O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, discipline, enzyme, location, protein, event, chemical compound, academic journal, award, scientist, person, country, chemical element, astronomical object, university, theory and O.\nSentence: Some notable examples include Journal of Computational Biology and PLOS Computational Biology .", "prompt_labels": "Some(O) notable(O) examples(O) include(O) Journal(B-academic journal) of(I-academic journal) Computational(I-academic journal) Biology(I-academic journal) and(O) PLOS(B-academic journal) Computational(I-academic journal) Biology(I-academic journal) .(O)"}}
{"id": "438", "dataset": "crossner_science", "split": "test", "label_list": ["university", "academic journal", "person", "chemical compound", "astronomical object", "organization", "event", "location", "country", "enzyme", "theory", "chemical element", "discipline", "award", "protein", "scientist"], "instance": {"id": "438", "words": ["Among", "the", "honors", "he", "'s", "received", "are", "the", "Federation", "Aeronautique", "Internationale", "'", "s", "De", "la", "Vaulx", "Medal", "in", "1970", "for", "his", "Apollo", "9", "flight", ",", "both", "of", "NASA", "'s", "Distinguished", "Service", "and", "Exceptional", "Service", "medals", ",", "and", ",", "unusual", "for", "an", "astronaut", ",", "an", "Emmy", "Award", "from", "the", "U.S.", "National", "Academy", "of", "Television", "Arts", "and", "Sciences", "for", "transmitting", "the", "first", "live", "TV", "pictures", "from", "space", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "B-award", "I-award", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-country", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, academic journal, person, chemical compound, astronomical object, organization, event, location, country, enzyme, theory, chemical element, discipline, award, protein, scientist and O.\nSentence: Among the honors he 's received are the Federation Aeronautique Internationale ' s De la Vaulx Medal in 1970 for his Apollo 9 flight , both of NASA 's Distinguished Service and Exceptional Service medals , and , unusual for an astronaut , an Emmy Award from the U.S. National Academy of Television Arts and Sciences for transmitting the first live TV pictures from space .", "prompt_labels": "Among(O) the(O) honors(O) he(O) 's(O) received(O) are(O) the(O) Federation(B-organization) Aeronautique(I-organization) Internationale(I-organization) '(O) s(O) De(B-award) la(I-award) Vaulx(I-award) Medal(I-award) in(O) 1970(O) for(O) his(O) Apollo(O) 9(O) flight(O) ,(O) both(O) of(O) NASA(B-organization) 's(O) Distinguished(B-award) Service(I-award) and(O) Exceptional(B-award) Service(I-award) medals(I-award) ,(O) and(O) ,(O) unusual(O) for(O) an(O) astronaut(O) ,(O) an(O) Emmy(B-award) Award(I-award) from(O) the(O) U.S.(B-country) National(B-organization) Academy(I-organization) of(I-organization) Television(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) for(O) transmitting(O) the(O) first(O) live(O) TV(O) pictures(O) from(O) space(O) .(O)"}}
{"id": "322", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "organization", "chemical element", "astronomical object", "university", "scientist", "event", "location", "country", "enzyme", "person", "chemical compound", "theory", "protein", "award", "discipline"], "instance": {"id": "322", "words": ["They", "include", "the", "N-terminal", "scaffold", "attachment", "factor", "-A", "/", "B", ",", "acinus", "and", "PIAS", "(", "SAP", ")", "domain", ",", "the", "Proline", "-", "Isoleucine", "-", "Asparagine", "-Ile-", "Threonine", "(", "PINIT", ")", "motif", ",", "the", "RING", "finger", "domain", "-", "Zinc", "finger", "-like", "zinc", "-binding", "domain", "(", "RLD", ")", ",", "the", "highly", "acidic", "domain", "(", "AD", ")", ",", "the", "SUMO-interacting", "motif", "(", "SIM", ")", ",", "and", "the", "serine", "/", "threonine", "-rich", "C-terminal", "region", "(", "S", "/", "T", ")", "."], "labels": ["O", "O", "O", "B-protein", "I-protein", "I-protein", "I-protein", "I-protein", "I-protein", "I-protein", "O", "O", "O", "B-protein", "I-protein", "I-protein", "I-protein", "O", "O", "O", "B-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "O", "O", "O", "O", "B-protein", "I-protein", "I-protein", "O", "B-protein", "I-protein", "I-protein", "I-protein", "I-protein", "I-protein", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "O", "B-protein", "O", "O", "O", "O", "B-protein", "I-protein", "I-protein", "I-protein", "I-protein", "I-protein", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, organization, chemical element, astronomical object, university, scientist, event, location, country, enzyme, person, chemical compound, theory, protein, award, discipline and O.\nSentence: They include the N-terminal scaffold attachment factor -A / B , acinus and PIAS ( SAP ) domain , the Proline - Isoleucine - Asparagine -Ile- Threonine ( PINIT ) motif , the RING finger domain - Zinc finger -like zinc -binding domain ( RLD ) , the highly acidic domain ( AD ) , the SUMO-interacting motif ( SIM ) , and the serine / threonine -rich C-terminal region ( S / T ) .", "prompt_labels": "They(O) include(O) the(O) N-terminal(B-protein) scaffold(I-protein) attachment(I-protein) factor(I-protein) -A(I-protein) /(I-protein) B(I-protein) ,(O) acinus(O) and(O) PIAS(B-protein) ((I-protein) SAP(I-protein) )(I-protein) domain(O) ,(O) the(O) Proline(B-chemical compound) -(O) Isoleucine(B-chemical compound) -(O) Asparagine(B-chemical compound) -Ile-(O) Threonine(B-chemical compound) ((O) PINIT(B-chemical compound) )(O) motif(O) ,(O) the(O) RING(B-protein) finger(I-protein) domain(I-protein) -(O) Zinc(B-protein) finger(I-protein) -like(I-protein) zinc(I-protein) -binding(I-protein) domain(I-protein) ((O) RLD(B-protein) )(O) ,(O) the(O) highly(O) acidic(O) domain(O) ((O) AD(O) )(O) ,(O) the(O) SUMO-interacting(B-protein) motif(I-protein) ((O) SIM(B-protein) )(O) ,(O) and(O) the(O) serine(B-protein) /(I-protein) threonine(I-protein) -rich(I-protein) C-terminal(I-protein) region(I-protein) ((O) S(O) /(O) T(O) )(O) .(O)"}}
{"id": "340", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "song", "rating", "plot", "year", "title", "trailer", "character", "review", "actor", "director", "average ratings"], "instance": {"id": "340", "words": ["show", "me", "which", "movie", "won", "the", "academy", "award", "for", "best", "picture", "in", "2009"], "labels": ["O", "O", "O", "O", "O", "O", "B-review", "I-review", "I-review", "I-review", "I-review", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, song, rating, plot, year, title, trailer, character, review, actor, director, average ratings and O.\nSentence: show me which movie won the academy award for best picture in 2009", "prompt_labels": "show(O) me(O) which(O) movie(O) won(O) the(O) academy(B-review) award(I-review) for(I-review) best(I-review) picture(I-review) in(O) 2009(B-year)"}}
{"id": "685", "dataset": "mit-movie", "split": "test", "label_list": ["review", "genre", "director", "average ratings", "plot", "song", "character", "rating", "actor", "title", "year", "trailer"], "instance": {"id": "685", "words": ["show", "me", "a", "drama", "starring", "james", "woods"], "labels": ["O", "O", "O", "B-genre", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, genre, director, average ratings, plot, song, character, rating, actor, title, year, trailer and O.\nSentence: show me a drama starring james woods", "prompt_labels": "show(O) me(O) a(O) drama(B-genre) starring(O) james(B-actor) woods(I-actor)"}}
{"id": "1736", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "director", "song", "genre", "character", "title", "average ratings", "year", "review", "plot", "trailer", "actor"], "instance": {"id": "1736", "words": ["name", "all", "movies", "in", "the", "past", "ten", "decades", "that", "star", "jonathon", "saech"], "labels": ["O", "O", "O", "O", "O", "B-year", "I-year", "I-year", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, director, song, genre, character, title, average ratings, year, review, plot, trailer, actor and O.\nSentence: name all movies in the past ten decades that star jonathon saech", "prompt_labels": "name(O) all(O) movies(O) in(O) the(O) past(B-year) ten(I-year) decades(I-year) that(O) star(O) jonathon(B-actor) saech(I-actor)"}}
{"id": "2433", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "character", "song", "rating", "actor", "title", "review", "director", "year", "average ratings", "trailer", "plot"], "instance": {"id": "2433", "words": ["play", "the", "trailer"], "labels": ["O", "O", "B-trailer"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, character, song, rating, actor, title, review, director, year, average ratings, trailer, plot and O.\nSentence: play the trailer", "prompt_labels": "play(O) the(O) trailer(B-trailer)"}}
{"id": "759", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "character", "song", "plot", "average ratings", "year", "title", "rating", "director", "actor", "review", "genre"], "instance": {"id": "759", "words": ["show", "me", "a", "movie", "with", "lots", "of", "sky", "diving", "in", "it"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-plot", "I-plot", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, character, song, plot, average ratings, year, title, rating, director, actor, review, genre and O.\nSentence: show me a movie with lots of sky diving in it", "prompt_labels": "show(O) me(O) a(O) movie(O) with(O) lots(O) of(O) sky(B-plot) diving(I-plot) in(O) it(O)"}}
{"id": "1239", "dataset": "mit-movie", "split": "test", "label_list": ["title", "year", "trailer", "plot", "song", "average ratings", "rating", "director", "review", "character", "genre", "actor"], "instance": {"id": "1239", "words": ["find", "a", "film", "titled", "shut"], "labels": ["O", "O", "O", "O", "B-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, year, trailer, plot, song, average ratings, rating, director, review, character, genre, actor and O.\nSentence: find a film titled shut", "prompt_labels": "find(O) a(O) film(O) titled(O) shut(B-title)"}}
{"id": "1408", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "plot", "rating", "title", "trailer", "director", "actor", "genre", "character", "review", "song", "year"], "instance": {"id": "1408", "words": ["in", "the", "2000", "s", "what", "pg", "13", "was", "a", "well", "rated", "movie", "about", "drama", "and", "guilt"], "labels": ["O", "O", "B-year", "I-year", "O", "B-rating", "I-rating", "O", "O", "B-average ratings", "I-average ratings", "O", "O", "B-genre", "O", "B-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, plot, rating, title, trailer, director, actor, genre, character, review, song, year and O.\nSentence: in the 2000 s what pg 13 was a well rated movie about drama and guilt", "prompt_labels": "in(O) the(O) 2000(B-year) s(I-year) what(O) pg(B-rating) 13(I-rating) was(O) a(O) well(B-average ratings) rated(I-average ratings) movie(O) about(O) drama(B-genre) and(O) guilt(B-plot)"}}
{"id": "856", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "director", "year", "review", "rating", "title", "song", "plot", "average ratings", "genre", "character", "actor"], "instance": {"id": "856", "words": ["what", "movies", "has", "alfred", "hitchcock", "been", "in"], "labels": ["O", "O", "O", "B-actor", "I-actor", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, director, year, review, rating, title, song, plot, average ratings, genre, character, actor and O.\nSentence: what movies has alfred hitchcock been in", "prompt_labels": "what(O) movies(O) has(O) alfred(B-actor) hitchcock(I-actor) been(O) in(O)"}}
{"id": "1301", "dataset": "mit-movie", "split": "test", "label_list": ["review", "song", "director", "title", "character", "average ratings", "trailer", "plot", "year", "rating", "actor", "genre"], "instance": {"id": "1301", "words": ["how", "did", "you", "like", "men", "in", "black"], "labels": ["O", "O", "O", "O", "B-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, director, title, character, average ratings, trailer, plot, year, rating, actor, genre and O.\nSentence: how did you like men in black", "prompt_labels": "how(O) did(O) you(O) like(O) men(B-title) in(I-title) black(I-title)"}}
{"id": "2261", "dataset": "mit-movie", "split": "test", "label_list": ["review", "director", "actor", "plot", "rating", "trailer", "character", "song", "year", "genre", "title", "average ratings"], "instance": {"id": "2261", "words": ["when", "did", "gallants", "come", "out"], "labels": ["O", "O", "B-title", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, director, actor, plot, rating, trailer, character, song, year, genre, title, average ratings and O.\nSentence: when did gallants come out", "prompt_labels": "when(O) did(O) gallants(B-title) come(O) out(O)"}}
{"id": "1864", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "rating", "title", "review", "character", "actor", "plot", "director", "song", "trailer", "average ratings", "year"], "instance": {"id": "1864", "words": ["what", "pg", "13", "rated", "film", "directed", "by", "john", "huston", "had", "a", "prisoner", "of", "war", "plot", "and", "an", "average", "rating", "of", "seven", "stars"], "labels": ["O", "B-rating", "I-rating", "O", "O", "O", "O", "B-director", "I-director", "O", "O", "B-plot", "I-plot", "I-plot", "O", "O", "O", "O", "O", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, rating, title, review, character, actor, plot, director, song, trailer, average ratings, year and O.\nSentence: what pg 13 rated film directed by john huston had a prisoner of war plot and an average rating of seven stars", "prompt_labels": "what(O) pg(B-rating) 13(I-rating) rated(O) film(O) directed(O) by(O) john(B-director) huston(I-director) had(O) a(O) prisoner(B-plot) of(I-plot) war(I-plot) plot(O) and(O) an(O) average(O) rating(O) of(O) seven(B-average ratings) stars(I-average ratings)"}}
{"id": "200", "dataset": "mit-movie", "split": "test", "label_list": ["director", "title", "actor", "review", "genre", "plot", "rating", "year", "trailer", "song", "character", "average ratings"], "instance": {"id": "200", "words": ["who", "was", "the", "male", "lead", "in", "gone", "with", "the", "wind"], "labels": ["O", "O", "O", "O", "O", "O", "B-title", "I-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, title, actor, review, genre, plot, rating, year, trailer, song, character, average ratings and O.\nSentence: who was the male lead in gone with the wind", "prompt_labels": "who(O) was(O) the(O) male(O) lead(O) in(O) gone(B-title) with(I-title) the(I-title) wind(I-title)"}}
{"id": "644", "dataset": "mit-movie", "split": "test", "label_list": ["director", "actor", "title", "genre", "trailer", "review", "average ratings", "year", "character", "plot", "song", "rating"], "instance": {"id": "644", "words": ["whats", "a", "mainstream", "movie", "about", "super", "heros"], "labels": ["O", "O", "B-genre", "O", "O", "B-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, actor, title, genre, trailer, review, average ratings, year, character, plot, song, rating and O.\nSentence: whats a mainstream movie about super heros", "prompt_labels": "whats(O) a(O) mainstream(B-genre) movie(O) about(O) super(B-plot) heros(I-plot)"}}
{"id": "1381", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "trailer", "rating", "title", "actor", "song", "review", "year", "plot", "character", "average ratings", "director"], "instance": {"id": "1381", "words": ["im", "looking", "for", "that", "childrens", "movie", "starring", "sylvester", "stallone", "from", "the", "1980", "s"], "labels": ["O", "O", "O", "O", "B-genre", "O", "O", "B-director", "I-director", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, trailer, rating, title, actor, song, review, year, plot, character, average ratings, director and O.\nSentence: im looking for that childrens movie starring sylvester stallone from the 1980 s", "prompt_labels": "im(O) looking(O) for(O) that(O) childrens(B-genre) movie(O) starring(O) sylvester(B-director) stallone(I-director) from(O) the(O) 1980(B-year) s(I-year)"}}
{"id": "1271", "dataset": "mit-movie", "split": "test", "label_list": ["plot", "title", "average ratings", "year", "character", "trailer", "review", "song", "genre", "director", "rating", "actor"], "instance": {"id": "1271", "words": ["has", "jennifer", "connelly", "ever", "been", "in", "a", "critically", "acclaimed", "nc", "17", "movie"], "labels": ["O", "B-actor", "I-actor", "O", "O", "O", "O", "B-average ratings", "I-average ratings", "B-rating", "I-rating", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, title, average ratings, year, character, trailer, review, song, genre, director, rating, actor and O.\nSentence: has jennifer connelly ever been in a critically acclaimed nc 17 movie", "prompt_labels": "has(O) jennifer(B-actor) connelly(I-actor) ever(O) been(O) in(O) a(O) critically(B-average ratings) acclaimed(I-average ratings) nc(B-rating) 17(I-rating) movie(O)"}}
{"id": "321", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "average ratings", "plot", "rating", "year", "character", "director", "title", "actor", "review", "trailer", "song"], "instance": {"id": "321", "words": ["name", "a", "movie", "with", "joseph", "cotton", "and", "orson", "welles"], "labels": ["O", "O", "O", "O", "B-actor", "I-actor", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, average ratings, plot, rating, year, character, director, title, actor, review, trailer, song and O.\nSentence: name a movie with joseph cotton and orson welles", "prompt_labels": "name(O) a(O) movie(O) with(O) joseph(B-actor) cotton(I-actor) and(O) orson(B-actor) welles(I-actor)"}}
{"id": "1518", "dataset": "mit-movie", "split": "test", "label_list": ["actor", "average ratings", "trailer", "plot", "song", "genre", "character", "review", "rating", "director", "year", "title"], "instance": {"id": "1518", "words": ["is", "there", "a", "movie", "from", "2000", "that", "starred", "chris", "rock"], "labels": ["O", "O", "O", "O", "O", "B-year", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, average ratings, trailer, plot, song, genre, character, review, rating, director, year, title and O.\nSentence: is there a movie from 2000 that starred chris rock", "prompt_labels": "is(O) there(O) a(O) movie(O) from(O) 2000(B-year) that(O) starred(O) chris(B-actor) rock(I-actor)"}}
{"id": "699", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "year", "director", "title", "plot", "rating", "song", "review", "average ratings", "trailer", "character", "actor"], "instance": {"id": "699", "words": ["find", "the", "movie", "with", "robin", "williams", "as", "a", "dj", "in", "vietnam"], "labels": ["O", "O", "O", "O", "B-actor", "I-actor", "O", "O", "B-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, year, director, title, plot, rating, song, review, average ratings, trailer, character, actor and O.\nSentence: find the movie with robin williams as a dj in vietnam", "prompt_labels": "find(O) the(O) movie(O) with(O) robin(B-actor) williams(I-actor) as(O) a(O) dj(B-plot) in(I-plot) vietnam(I-plot)"}}
{"id": "103", "dataset": "mit-movie", "split": "test", "label_list": ["title", "review", "character", "actor", "average ratings", "song", "year", "genre", "trailer", "plot", "director", "rating"], "instance": {"id": "103", "words": ["did", "angelina", "jolie", "play", "a", "russian", "in", "salt"], "labels": ["O", "B-actor", "I-actor", "O", "O", "B-plot", "O", "B-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, review, character, actor, average ratings, song, year, genre, trailer, plot, director, rating and O.\nSentence: did angelina jolie play a russian in salt", "prompt_labels": "did(O) angelina(B-actor) jolie(I-actor) play(O) a(O) russian(B-plot) in(O) salt(B-title)"}}
{"id": "150", "dataset": "mit-movie", "split": "test", "label_list": ["year", "title", "average ratings", "director", "character", "trailer", "genre", "actor", "song", "plot", "review", "rating"], "instance": {"id": "150", "words": ["what", "are", "some", "batman", "movies", "from", "the", "1990s"], "labels": ["O", "O", "O", "B-title", "I-title", "O", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, title, average ratings, director, character, trailer, genre, actor, song, plot, review, rating and O.\nSentence: what are some batman movies from the 1990s", "prompt_labels": "what(O) are(O) some(O) batman(B-title) movies(I-title) from(O) the(O) 1990s(B-year)"}}
{"id": "1336", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "year", "average ratings", "review", "character", "genre", "title", "plot", "actor", "song", "rating", "director"], "instance": {"id": "1336", "words": ["i", "would", "like", "an", "emotional", "type", "movie"], "labels": ["O", "O", "O", "O", "B-genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, year, average ratings, review, character, genre, title, plot, actor, song, rating, director and O.\nSentence: i would like an emotional type movie", "prompt_labels": "i(O) would(O) like(O) an(O) emotional(B-genre) type(O) movie(O)"}}
{"id": "282", "dataset": "mit-movie", "split": "test", "label_list": ["review", "plot", "actor", "character", "year", "rating", "song", "director", "average ratings", "genre", "trailer", "title"], "instance": {"id": "282", "words": ["what", "year", "did", "the", "original", "black", "and", "white", "casablanca", "come", "out"], "labels": ["O", "O", "O", "O", "O", "B-genre", "I-genre", "I-genre", "B-title", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, plot, actor, character, year, rating, song, director, average ratings, genre, trailer, title and O.\nSentence: what year did the original black and white casablanca come out", "prompt_labels": "what(O) year(O) did(O) the(O) original(O) black(B-genre) and(I-genre) white(I-genre) casablanca(B-title) come(O) out(O)"}}
{"id": "620", "dataset": "mit-movie", "split": "test", "label_list": ["title", "director", "year", "character", "song", "plot", "average ratings", "review", "genre", "rating", "trailer", "actor"], "instance": {"id": "620", "words": ["was", "the", "character", "chucky", "in", "any", "films", "that", "were", "sci", "fi"], "labels": ["O", "O", "O", "B-character", "O", "O", "O", "O", "O", "B-genre", "I-genre"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, director, year, character, song, plot, average ratings, review, genre, rating, trailer, actor and O.\nSentence: was the character chucky in any films that were sci fi", "prompt_labels": "was(O) the(O) character(O) chucky(B-character) in(O) any(O) films(O) that(O) were(O) sci(B-genre) fi(I-genre)"}}
{"id": "1714", "dataset": "mit-movie", "split": "test", "label_list": ["plot", "rating", "title", "character", "review", "actor", "song", "year", "director", "trailer", "average ratings", "genre"], "instance": {"id": "1714", "words": ["movie", "information", "on", "south", "of", "the", "border"], "labels": ["O", "O", "O", "B-title", "I-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, rating, title, character, review, actor, song, year, director, trailer, average ratings, genre and O.\nSentence: movie information on south of the border", "prompt_labels": "movie(O) information(O) on(O) south(B-title) of(I-title) the(I-title) border(I-title)"}}
{"id": "1124", "dataset": "mit-movie", "split": "test", "label_list": ["review", "plot", "genre", "actor", "rating", "director", "trailer", "song", "title", "character", "average ratings", "year"], "instance": {"id": "1124", "words": ["details", "about", "scooby", "doo", "and", "the", "loch", "ness", "monster", "kids", "movie"], "labels": ["O", "O", "B-title", "I-title", "I-title", "I-title", "I-title", "I-title", "I-title", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, plot, genre, actor, rating, director, trailer, song, title, character, average ratings, year and O.\nSentence: details about scooby doo and the loch ness monster kids movie", "prompt_labels": "details(O) about(O) scooby(B-title) doo(I-title) and(I-title) the(I-title) loch(I-title) ness(I-title) monster(I-title) kids(O) movie(O)"}}
{"id": "1025", "dataset": "mit-movie", "split": "test", "label_list": ["review", "character", "genre", "actor", "rating", "title", "song", "plot", "year", "director", "trailer", "average ratings"], "instance": {"id": "1025", "words": ["tell", "me", "a", "movie", "made", "by", "peter", "jackson"], "labels": ["O", "O", "O", "O", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, character, genre, actor, rating, title, song, plot, year, director, trailer, average ratings and O.\nSentence: tell me a movie made by peter jackson", "prompt_labels": "tell(O) me(O) a(O) movie(O) made(O) by(O) peter(B-director) jackson(I-director)"}}
{"id": "1805", "dataset": "mit-movie", "split": "test", "label_list": ["year", "plot", "actor", "average ratings", "review", "character", "song", "title", "trailer", "director", "genre", "rating"], "instance": {"id": "1805", "words": ["was", "samuel", "l", "jackson", "in", "any", "crime", "movies", "in", "the", "1960", "s", "when", "he", "was", "a", "child"], "labels": ["O", "B-actor", "I-actor", "I-actor", "O", "O", "B-genre", "O", "O", "O", "B-year", "I-year", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, plot, actor, average ratings, review, character, song, title, trailer, director, genre, rating and O.\nSentence: was samuel l jackson in any crime movies in the 1960 s when he was a child", "prompt_labels": "was(O) samuel(B-actor) l(I-actor) jackson(I-actor) in(O) any(O) crime(B-genre) movies(O) in(O) the(O) 1960(B-year) s(I-year) when(O) he(O) was(O) a(O) child(O)"}}
{"id": "435", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "rating", "character", "review", "title", "actor", "song", "trailer", "year", "director", "genre", "plot"], "instance": {"id": "435", "words": ["show", "me", "action", "movies", "with", "vin", "diesel"], "labels": ["O", "O", "O", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, rating, character, review, title, actor, song, trailer, year, director, genre, plot and O.\nSentence: show me action movies with vin diesel", "prompt_labels": "show(O) me(O) action(O) movies(O) with(O) vin(B-actor) diesel(I-actor)"}}
{"id": "118", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "title", "song", "rating", "trailer", "director", "review", "plot", "character", "actor", "year", "average ratings"], "instance": {"id": "118", "words": ["show", "me", "a", "good", "spy", "movie", "based", "in", "england"], "labels": ["O", "O", "O", "O", "B-genre", "O", "O", "O", "B-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, title, song, rating, trailer, director, review, plot, character, actor, year, average ratings and O.\nSentence: show me a good spy movie based in england", "prompt_labels": "show(O) me(O) a(O) good(O) spy(B-genre) movie(O) based(O) in(O) england(B-plot)"}}
{"id": "1162", "dataset": "mit-movie", "split": "test", "label_list": ["song", "title", "year", "director", "average ratings", "plot", "trailer", "genre", "review", "rating", "character", "actor"], "instance": {"id": "1162", "words": ["did", "ron", "howard", "ever", "direct", "an", "independent", "film"], "labels": ["O", "B-director", "I-director", "O", "O", "O", "B-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, title, year, director, average ratings, plot, trailer, genre, review, rating, character, actor and O.\nSentence: did ron howard ever direct an independent film", "prompt_labels": "did(O) ron(B-director) howard(I-director) ever(O) direct(O) an(O) independent(B-genre) film(O)"}}
{"id": "887", "dataset": "mit-movie", "split": "test", "label_list": ["year", "rating", "song", "actor", "average ratings", "plot", "director", "trailer", "title", "review", "genre", "character"], "instance": {"id": "887", "words": ["who", "starred", "in", "thw", "gumball", "rally"], "labels": ["O", "O", "O", "B-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, rating, song, actor, average ratings, plot, director, trailer, title, review, genre, character and O.\nSentence: who starred in thw gumball rally", "prompt_labels": "who(O) starred(O) in(O) thw(B-title) gumball(I-title) rally(I-title)"}}
{"id": "1048", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "plot", "director", "trailer", "song", "title", "character", "year", "rating", "actor", "review", "genre"], "instance": {"id": "1048", "words": ["how", "many", "movies", "featured", "both", "kirk", "douglas", "and", "michael", "douglas"], "labels": ["O", "O", "O", "O", "O", "B-actor", "I-actor", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, plot, director, trailer, song, title, character, year, rating, actor, review, genre and O.\nSentence: how many movies featured both kirk douglas and michael douglas", "prompt_labels": "how(O) many(O) movies(O) featured(O) both(O) kirk(B-actor) douglas(I-actor) and(O) michael(B-actor) douglas(I-actor)"}}
{"id": "816", "dataset": "mit-movie", "split": "test", "label_list": ["actor", "character", "review", "year", "genre", "plot", "rating", "director", "title", "trailer", "song", "average ratings"], "instance": {"id": "816", "words": ["i", "want", "1980s", "romance", "movies"], "labels": ["O", "O", "B-year", "B-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, character, review, year, genre, plot, rating, director, title, trailer, song, average ratings and O.\nSentence: i want 1980s romance movies", "prompt_labels": "i(O) want(O) 1980s(B-year) romance(B-genre) movies(O)"}}
{"id": "2256", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "actor", "title", "trailer", "song", "plot", "average ratings", "review", "character", "year", "director", "genre"], "instance": {"id": "2256", "words": ["whats", "the", "science", "fiction", "movie", "in", "1990", "that", "was", "rated", "r", "averaged", "really", "good", "ratings", "directed", "by", "brett", "ratner"], "labels": ["O", "O", "B-genre", "I-genre", "O", "O", "B-year", "O", "O", "O", "B-rating", "O", "B-average ratings", "I-average ratings", "O", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, actor, title, trailer, song, plot, average ratings, review, character, year, director, genre and O.\nSentence: whats the science fiction movie in 1990 that was rated r averaged really good ratings directed by brett ratner", "prompt_labels": "whats(O) the(O) science(B-genre) fiction(I-genre) movie(O) in(O) 1990(B-year) that(O) was(O) rated(O) r(B-rating) averaged(O) really(B-average ratings) good(I-average ratings) ratings(O) directed(O) by(O) brett(B-director) ratner(I-director)"}}
{"id": "907", "dataset": "mit-movie", "split": "test", "label_list": ["year", "average ratings", "rating", "genre", "review", "director", "trailer", "title", "plot", "character", "actor", "song"], "instance": {"id": "907", "words": ["show", "me", "documentaries", "about", "birds"], "labels": ["O", "O", "B-genre", "B-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, average ratings, rating, genre, review, director, trailer, title, plot, character, actor, song and O.\nSentence: show me documentaries about birds", "prompt_labels": "show(O) me(O) documentaries(B-genre) about(B-plot) birds(I-plot)"}}
{"id": "1120", "dataset": "mit-movie", "split": "test", "label_list": ["song", "rating", "plot", "genre", "trailer", "character", "average ratings", "year", "actor", "director", "review", "title"], "instance": {"id": "1120", "words": ["could", "you", "help", "me", "find", "a", "movie", "starring", "larenz", "tate", "that", "was", "made", "in", "the", "past", "eight", "years", "that", "was", "rated", "well"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-actor", "I-actor", "O", "O", "O", "O", "O", "B-year", "I-year", "I-year", "O", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, rating, plot, genre, trailer, character, average ratings, year, actor, director, review, title and O.\nSentence: could you help me find a movie starring larenz tate that was made in the past eight years that was rated well", "prompt_labels": "could(O) you(O) help(O) me(O) find(O) a(O) movie(O) starring(O) larenz(B-actor) tate(I-actor) that(O) was(O) made(O) in(O) the(O) past(B-year) eight(I-year) years(I-year) that(O) was(O) rated(B-average ratings) well(I-average ratings)"}}
{"id": "300", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "character", "year", "director", "song", "genre", "review", "title", "actor", "plot", "rating", "average ratings"], "instance": {"id": "300", "words": ["did", "the", "director", "of", "pet", "shop", "make", "any", "other", "movies"], "labels": ["O", "O", "O", "O", "B-title", "I-title", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, character, year, director, song, genre, review, title, actor, plot, rating, average ratings and O.\nSentence: did the director of pet shop make any other movies", "prompt_labels": "did(O) the(O) director(O) of(O) pet(B-title) shop(I-title) make(O) any(O) other(O) movies(O)"}}
{"id": "641", "dataset": "mit-movie", "split": "test", "label_list": ["actor", "plot", "trailer", "rating", "year", "song", "average ratings", "genre", "title", "director", "review", "character"], "instance": {"id": "641", "words": ["find", "movies", "about", "the", "first", "gulf", "war"], "labels": ["O", "O", "O", "O", "B-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, plot, trailer, rating, year, song, average ratings, genre, title, director, review, character and O.\nSentence: find movies about the first gulf war", "prompt_labels": "find(O) movies(O) about(O) the(O) first(B-plot) gulf(I-plot) war(I-plot)"}}
{"id": "1918", "dataset": "mit-movie", "split": "test", "label_list": ["title", "actor", "director", "plot", "average ratings", "genre", "review", "year", "character", "rating", "trailer", "song"], "instance": {"id": "1918", "words": ["what", "are", "some", "titles", "of", "any", "spy", "films", "rated", "r", "that", "were", "directed", "by", "rob", "walker", "from", "the", "last", "two", "years", "and", "liked", "by", "many"], "labels": ["O", "O", "O", "O", "O", "O", "B-plot", "O", "O", "B-rating", "O", "O", "O", "O", "B-director", "I-director", "O", "O", "B-year", "I-year", "I-year", "O", "B-average ratings", "I-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, actor, director, plot, average ratings, genre, review, year, character, rating, trailer, song and O.\nSentence: what are some titles of any spy films rated r that were directed by rob walker from the last two years and liked by many", "prompt_labels": "what(O) are(O) some(O) titles(O) of(O) any(O) spy(B-plot) films(O) rated(O) r(B-rating) that(O) were(O) directed(O) by(O) rob(B-director) walker(I-director) from(O) the(O) last(B-year) two(I-year) years(I-year) and(O) liked(B-average ratings) by(I-average ratings) many(I-average ratings)"}}
{"id": "1761", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "director", "actor", "song", "review", "year", "genre", "character", "title", "plot", "trailer", "average ratings"], "instance": {"id": "1761", "words": ["show", "me", "all", "unrated", "scary", "moves", "about", "gore"], "labels": ["O", "O", "O", "B-rating", "B-genre", "O", "O", "B-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, director, actor, song, review, year, genre, character, title, plot, trailer, average ratings and O.\nSentence: show me all unrated scary moves about gore", "prompt_labels": "show(O) me(O) all(O) unrated(B-rating) scary(B-genre) moves(O) about(O) gore(B-plot)"}}
{"id": "417", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "actor", "plot", "rating", "review", "genre", "song", "director", "title", "trailer", "year", "character"], "instance": {"id": "417", "words": ["what", "is", "the", "worst", "viewer", "rated", "vampire", "film"], "labels": ["O", "O", "O", "B-average ratings", "I-average ratings", "I-average ratings", "B-plot", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, actor, plot, rating, review, genre, song, director, title, trailer, year, character and O.\nSentence: what is the worst viewer rated vampire film", "prompt_labels": "what(O) is(O) the(O) worst(B-average ratings) viewer(I-average ratings) rated(I-average ratings) vampire(B-plot) film(O)"}}
{"id": "2133", "dataset": "mit-movie", "split": "test", "label_list": ["actor", "song", "rating", "trailer", "average ratings", "character", "genre", "plot", "year", "title", "director", "review"], "instance": {"id": "2133", "words": ["what", "movie", "came", "out", "in", "the", "last", "four", "years", "and", "is", "about", "new", "york"], "labels": ["O", "O", "O", "O", "O", "O", "B-year", "I-year", "I-year", "O", "O", "O", "B-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, song, rating, trailer, average ratings, character, genre, plot, year, title, director, review and O.\nSentence: what movie came out in the last four years and is about new york", "prompt_labels": "what(O) movie(O) came(O) out(O) in(O) the(O) last(B-year) four(I-year) years(I-year) and(O) is(O) about(O) new(B-plot) york(I-plot)"}}
{"id": "2417", "dataset": "mit-movie", "split": "test", "label_list": ["title", "director", "genre", "actor", "year", "review", "plot", "average ratings", "trailer", "rating", "character", "song"], "instance": {"id": "2417", "words": ["the", "graduate"], "labels": ["B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, director, genre, actor, year, review, plot, average ratings, trailer, rating, character, song and O.\nSentence: the graduate", "prompt_labels": "the(B-title) graduate(I-title)"}}
{"id": "846", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "actor", "rating", "song", "character", "year", "genre", "director", "review", "title", "plot", "trailer"], "instance": {"id": "846", "words": ["find", "me", "a", "movie", "with", "pirates"], "labels": ["O", "O", "O", "O", "O", "B-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, actor, rating, song, character, year, genre, director, review, title, plot, trailer and O.\nSentence: find me a movie with pirates", "prompt_labels": "find(O) me(O) a(O) movie(O) with(O) pirates(B-plot)"}}
{"id": "1151", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "director", "genre", "year", "title", "trailer", "song", "review", "plot", "average ratings", "actor", "character"], "instance": {"id": "1151", "words": ["did", "leonardo", "dicaprio", "star", "in", "a", "film", "directed", "by", "brian", "de", "palma"], "labels": ["O", "B-actor", "I-actor", "O", "O", "O", "O", "O", "O", "B-director", "I-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, director, genre, year, title, trailer, song, review, plot, average ratings, actor, character and O.\nSentence: did leonardo dicaprio star in a film directed by brian de palma", "prompt_labels": "did(O) leonardo(B-actor) dicaprio(I-actor) star(O) in(O) a(O) film(O) directed(O) by(O) brian(B-director) de(I-director) palma(I-director)"}}
{"id": "59", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "song", "average ratings", "rating", "title", "year", "character", "plot", "trailer", "review", "director", "actor"], "instance": {"id": "59", "words": ["what", "was", "the", "most", "popular", "movie", "from", "2004"], "labels": ["O", "O", "O", "B-review", "I-review", "I-review", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, song, average ratings, rating, title, year, character, plot, trailer, review, director, actor and O.\nSentence: what was the most popular movie from 2004", "prompt_labels": "what(O) was(O) the(O) most(B-review) popular(I-review) movie(I-review) from(O) 2004(B-year)"}}
{"id": "519", "dataset": "mit-movie", "split": "test", "label_list": ["title", "trailer", "rating", "character", "genre", "review", "song", "average ratings", "plot", "year", "director", "actor"], "instance": {"id": "519", "words": ["what", "was", "the", "last", "terminator", "film", "to", "be", "released"], "labels": ["O", "O", "O", "O", "B-title", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, trailer, rating, character, genre, review, song, average ratings, plot, year, director, actor and O.\nSentence: what was the last terminator film to be released", "prompt_labels": "what(O) was(O) the(O) last(O) terminator(B-title) film(O) to(O) be(O) released(O)"}}
{"id": "1186", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "year", "character", "rating", "song", "title", "average ratings", "director", "genre", "review", "plot", "actor"], "instance": {"id": "1186", "words": ["do", "you", "carry", "the", "movie", "the", "a", "team"], "labels": ["O", "O", "O", "O", "O", "B-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, year, character, rating, song, title, average ratings, director, genre, review, plot, actor and O.\nSentence: do you carry the movie the a team", "prompt_labels": "do(O) you(O) carry(O) the(O) movie(O) the(B-title) a(I-title) team(I-title)"}}
{"id": "945", "dataset": "mit-movie", "split": "test", "label_list": ["plot", "average ratings", "title", "character", "song", "rating", "director", "year", "trailer", "review", "genre", "actor"], "instance": {"id": "945", "words": ["find", "the", "suspense", "movie", "starring", "anthony", "hopkins", "and", "ryan", "gosling"], "labels": ["O", "O", "B-genre", "O", "O", "B-actor", "I-actor", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, average ratings, title, character, song, rating, director, year, trailer, review, genre, actor and O.\nSentence: find the suspense movie starring anthony hopkins and ryan gosling", "prompt_labels": "find(O) the(O) suspense(B-genre) movie(O) starring(O) anthony(B-actor) hopkins(I-actor) and(O) ryan(B-actor) gosling(I-actor)"}}
{"id": "2344", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "review", "song", "trailer", "plot", "average ratings", "year", "actor", "director", "rating", "title", "character"], "instance": {"id": "2344", "words": ["did", "charles", "haid", "have", "a", "war", "movie", "in", "the", "last", "three", "years"], "labels": ["O", "B-director", "I-director", "O", "O", "B-genre", "O", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, review, song, trailer, plot, average ratings, year, actor, director, rating, title, character and O.\nSentence: did charles haid have a war movie in the last three years", "prompt_labels": "did(O) charles(B-director) haid(I-director) have(O) a(O) war(B-genre) movie(O) in(O) the(O) last(B-year) three(I-year) years(I-year)"}}
{"id": "34", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "trailer", "director", "genre", "rating", "actor", "character", "plot", "year", "review", "song", "title"], "instance": {"id": "34", "words": ["what", "was", "the", "last", "film", "elizabeth", "montgomery", "starred", "in"], "labels": ["O", "O", "O", "B-year", "O", "B-actor", "I-actor", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, trailer, director, genre, rating, actor, character, plot, year, review, song, title and O.\nSentence: what was the last film elizabeth montgomery starred in", "prompt_labels": "what(O) was(O) the(O) last(B-year) film(O) elizabeth(B-actor) montgomery(I-actor) starred(O) in(O)"}}
{"id": "276", "dataset": "mit-movie", "split": "test", "label_list": ["character", "review", "trailer", "rating", "average ratings", "year", "plot", "title", "genre", "song", "actor", "director"], "instance": {"id": "276", "words": ["show", "me", "listing", "that", "have", "micheal", "in", "them"], "labels": ["O", "O", "O", "O", "O", "B-actor", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, review, trailer, rating, average ratings, year, plot, title, genre, song, actor, director and O.\nSentence: show me listing that have micheal in them", "prompt_labels": "show(O) me(O) listing(O) that(O) have(O) micheal(B-actor) in(O) them(O)"}}
{"id": "1023", "dataset": "mit-movie", "split": "test", "label_list": ["plot", "average ratings", "trailer", "character", "actor", "genre", "director", "review", "song", "year", "title", "rating"], "instance": {"id": "1023", "words": ["help", "me", "find", "a", "movie", "with", "the", "song", "when", "you", "really", "love", "a", "woman"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "I-song"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, average ratings, trailer, character, actor, genre, director, review, song, year, title, rating and O.\nSentence: help me find a movie with the song when you really love a woman", "prompt_labels": "help(O) me(O) find(O) a(O) movie(O) with(O) the(O) song(O) when(B-song) you(I-song) really(I-song) love(I-song) a(I-song) woman(I-song)"}}
{"id": "938", "dataset": "mit-movie", "split": "test", "label_list": ["character", "actor", "title", "year", "review", "rating", "plot", "genre", "song", "trailer", "director", "average ratings"], "instance": {"id": "938", "words": ["what", "was", "the", "first", "dirty", "harry", "movie"], "labels": ["O", "O", "O", "B-year", "B-title", "I-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, actor, title, year, review, rating, plot, genre, song, trailer, director, average ratings and O.\nSentence: what was the first dirty harry movie", "prompt_labels": "what(O) was(O) the(O) first(B-year) dirty(B-title) harry(I-title) movie(O)"}}
{"id": "1739", "dataset": "mit-movie", "split": "test", "label_list": ["title", "song", "character", "trailer", "director", "average ratings", "plot", "rating", "actor", "genre", "review", "year"], "instance": {"id": "1739", "words": ["name", "an", "emotional", "film", "from", "2000", "that", "is", "highly", "liked"], "labels": ["O", "O", "B-genre", "O", "O", "B-year", "O", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, song, character, trailer, director, average ratings, plot, rating, actor, genre, review, year and O.\nSentence: name an emotional film from 2000 that is highly liked", "prompt_labels": "name(O) an(O) emotional(B-genre) film(O) from(O) 2000(B-year) that(O) is(O) highly(B-average ratings) liked(I-average ratings)"}}
{"id": "353", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "character", "review", "song", "average ratings", "plot", "year", "actor", "trailer", "rating", "director", "title"], "instance": {"id": "353", "words": ["how", "many", "pirates", "of", "the", "caribbean", "movies", "is", "johnny", "depp", "in"], "labels": ["O", "O", "B-title", "I-title", "I-title", "I-title", "O", "O", "B-actor", "I-actor", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, character, review, song, average ratings, plot, year, actor, trailer, rating, director, title and O.\nSentence: how many pirates of the caribbean movies is johnny depp in", "prompt_labels": "how(O) many(O) pirates(B-title) of(I-title) the(I-title) caribbean(I-title) movies(O) is(O) johnny(B-actor) depp(I-actor) in(O)"}}
{"id": "2276", "dataset": "mit-movie", "split": "test", "label_list": ["year", "plot", "director", "song", "average ratings", "genre", "actor", "trailer", "character", "rating", "review", "title"], "instance": {"id": "2276", "words": ["where", "can", "i", "find", "the", "movie", "boy"], "labels": ["O", "O", "O", "O", "O", "O", "B-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, plot, director, song, average ratings, genre, actor, trailer, character, rating, review, title and O.\nSentence: where can i find the movie boy", "prompt_labels": "where(O) can(O) i(O) find(O) the(O) movie(O) boy(B-title)"}}
{"id": "2414", "dataset": "mit-movie", "split": "test", "label_list": ["plot", "genre", "review", "character", "average ratings", "year", "title", "rating", "director", "song", "actor", "trailer"], "instance": {"id": "2414", "words": ["empire", "strikes", "back"], "labels": ["B-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, genre, review, character, average ratings, year, title, rating, director, song, actor, trailer and O.\nSentence: empire strikes back", "prompt_labels": "empire(B-title) strikes(I-title) back(I-title)"}}
{"id": "1571", "dataset": "mit-movie", "split": "test", "label_list": ["review", "director", "title", "average ratings", "year", "trailer", "character", "song", "actor", "genre", "plot", "rating"], "instance": {"id": "1571", "words": ["is", "there", "any", "western", "films", "that", "are", "rated", "g"], "labels": ["O", "O", "O", "B-genre", "O", "O", "O", "O", "B-rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, director, title, average ratings, year, trailer, character, song, actor, genre, plot, rating and O.\nSentence: is there any western films that are rated g", "prompt_labels": "is(O) there(O) any(O) western(B-genre) films(O) that(O) are(O) rated(O) g(B-rating)"}}
{"id": "705", "dataset": "mit-movie", "split": "test", "label_list": ["actor", "year", "plot", "rating", "genre", "director", "review", "song", "average ratings", "title", "character", "trailer"], "instance": {"id": "705", "words": ["who", "is", "the", "actor", "that", "stars", "in", "the", "new", "tv", "show", "awake"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, year, plot, rating, genre, director, review, song, average ratings, title, character, trailer and O.\nSentence: who is the actor that stars in the new tv show awake", "prompt_labels": "who(O) is(O) the(O) actor(O) that(O) stars(O) in(O) the(O) new(O) tv(O) show(O) awake(B-title)"}}
{"id": "1077", "dataset": "mit-movie", "split": "test", "label_list": ["actor", "plot", "year", "review", "genre", "title", "rating", "trailer", "director", "song", "average ratings", "character"], "instance": {"id": "1077", "words": ["any", "war", "type", "rated", "r", "movies", "set", "in", "the", "1940", "s", "that", "received", "five", "stars"], "labels": ["O", "B-genre", "O", "O", "B-rating", "O", "O", "O", "O", "B-year", "I-year", "O", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, plot, year, review, genre, title, rating, trailer, director, song, average ratings, character and O.\nSentence: any war type rated r movies set in the 1940 s that received five stars", "prompt_labels": "any(O) war(B-genre) type(O) rated(O) r(B-rating) movies(O) set(O) in(O) the(O) 1940(B-year) s(I-year) that(O) received(O) five(B-average ratings) stars(I-average ratings)"}}
{"id": "880", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "title", "review", "plot", "character", "average ratings", "actor", "year", "trailer", "song", "genre", "director"], "instance": {"id": "880", "words": ["what", "movies", "from", "the", "90s", "did", "jeremy", "irons", "star", "in"], "labels": ["O", "O", "O", "O", "B-year", "O", "B-actor", "I-actor", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, title, review, plot, character, average ratings, actor, year, trailer, song, genre, director and O.\nSentence: what movies from the 90s did jeremy irons star in", "prompt_labels": "what(O) movies(O) from(O) the(O) 90s(B-year) did(O) jeremy(B-actor) irons(I-actor) star(O) in(O)"}}
{"id": "508", "dataset": "mit-movie", "split": "test", "label_list": ["director", "actor", "character", "average ratings", "genre", "year", "plot", "song", "rating", "trailer", "review", "title"], "instance": {"id": "508", "words": ["what", "movie", "had", "the", "earth", "explode", "in", "the", "trailer"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, actor, character, average ratings, genre, year, plot, song, rating, trailer, review, title and O.\nSentence: what movie had the earth explode in the trailer", "prompt_labels": "what(O) movie(O) had(O) the(O) earth(O) explode(O) in(O) the(O) trailer(O)"}}
{"id": "757", "dataset": "mit-movie", "split": "test", "label_list": ["plot", "rating", "song", "actor", "character", "director", "trailer", "review", "genre", "title", "average ratings", "year"], "instance": {"id": "757", "words": ["find", "a", "romantic", "comedy", "starring", "daryl", "hannah"], "labels": ["O", "O", "B-genre", "I-genre", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, rating, song, actor, character, director, trailer, review, genre, title, average ratings, year and O.\nSentence: find a romantic comedy starring daryl hannah", "prompt_labels": "find(O) a(O) romantic(B-genre) comedy(I-genre) starring(O) daryl(B-actor) hannah(I-actor)"}}
{"id": "2027", "dataset": "mit-movie", "split": "test", "label_list": ["actor", "review", "title", "song", "character", "rating", "genre", "year", "average ratings", "director", "trailer", "plot"], "instance": {"id": "2027", "words": ["what", "is", "a", "movie", "withing", "the", "last", "three", "years", "with", "a", "six", "star", "rating", "about", "an", "arrest", "starring", "jeri", "ryan"], "labels": ["O", "O", "O", "O", "O", "O", "B-year", "I-year", "I-year", "O", "O", "B-average ratings", "I-average ratings", "O", "O", "O", "B-plot", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, review, title, song, character, rating, genre, year, average ratings, director, trailer, plot and O.\nSentence: what is a movie withing the last three years with a six star rating about an arrest starring jeri ryan", "prompt_labels": "what(O) is(O) a(O) movie(O) withing(O) the(O) last(B-year) three(I-year) years(I-year) with(O) a(O) six(B-average ratings) star(I-average ratings) rating(O) about(O) an(O) arrest(B-plot) starring(O) jeri(B-actor) ryan(I-actor)"}}
{"id": "1453", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "average ratings", "trailer", "year", "review", "rating", "plot", "director", "title", "actor", "song", "character"], "instance": {"id": "1453", "words": ["is", "there", "a", "1970", "s", "avant", "garde", "film"], "labels": ["O", "O", "O", "B-year", "I-year", "B-genre", "I-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, average ratings, trailer, year, review, rating, plot, director, title, actor, song, character and O.\nSentence: is there a 1970 s avant garde film", "prompt_labels": "is(O) there(O) a(O) 1970(B-year) s(I-year) avant(B-genre) garde(I-genre) film(O)"}}
{"id": "1933", "dataset": "mit-movie", "split": "test", "label_list": ["year", "rating", "average ratings", "character", "review", "actor", "director", "plot", "genre", "title", "trailer", "song"], "instance": {"id": "1933", "words": ["what", "are", "the", "worst", "sci", "fi", "movies", "rated", "pg", "13", "in", "the", "last", "seven", "decades"], "labels": ["O", "O", "O", "O", "B-genre", "I-genre", "O", "O", "B-rating", "I-rating", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, rating, average ratings, character, review, actor, director, plot, genre, title, trailer, song and O.\nSentence: what are the worst sci fi movies rated pg 13 in the last seven decades", "prompt_labels": "what(O) are(O) the(O) worst(O) sci(B-genre) fi(I-genre) movies(O) rated(O) pg(B-rating) 13(I-rating) in(O) the(O) last(B-year) seven(I-year) decades(I-year)"}}
{"id": "982", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "average ratings", "genre", "year", "director", "trailer", "plot", "character", "song", "title", "review", "actor"], "instance": {"id": "982", "words": ["show", "me", "a", "cameron", "diaz", "movie", "from", "the", "90s"], "labels": ["O", "O", "O", "B-actor", "I-actor", "O", "O", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, average ratings, genre, year, director, trailer, plot, character, song, title, review, actor and O.\nSentence: show me a cameron diaz movie from the 90s", "prompt_labels": "show(O) me(O) a(O) cameron(B-actor) diaz(I-actor) movie(O) from(O) the(O) 90s(B-year)"}}
{"id": "885", "dataset": "mit-movie", "split": "test", "label_list": ["song", "rating", "trailer", "genre", "year", "character", "review", "director", "actor", "title", "plot", "average ratings"], "instance": {"id": "885", "words": ["look", "for", "a", "movie", "with", "jessica", "alba"], "labels": ["O", "O", "O", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, rating, trailer, genre, year, character, review, director, actor, title, plot, average ratings and O.\nSentence: look for a movie with jessica alba", "prompt_labels": "look(O) for(O) a(O) movie(O) with(O) jessica(B-actor) alba(I-actor)"}}
{"id": "363", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "title", "review", "song", "average ratings", "genre", "director", "year", "plot", "actor", "character", "trailer"], "instance": {"id": "363", "words": ["show", "me", "the", "harry", "potter", "series"], "labels": ["O", "O", "O", "B-character", "I-character", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, title, review, song, average ratings, genre, director, year, plot, actor, character, trailer and O.\nSentence: show me the harry potter series", "prompt_labels": "show(O) me(O) the(O) harry(B-character) potter(I-character) series(O)"}}
{"id": "807", "dataset": "mit-movie", "split": "test", "label_list": ["actor", "plot", "rating", "year", "genre", "director", "trailer", "average ratings", "character", "review", "title", "song"], "instance": {"id": "807", "words": ["show", "me", "a", "linda", "hunt", "movie"], "labels": ["O", "O", "O", "B-actor", "I-actor", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, plot, rating, year, genre, director, trailer, average ratings, character, review, title, song and O.\nSentence: show me a linda hunt movie", "prompt_labels": "show(O) me(O) a(O) linda(B-actor) hunt(I-actor) movie(O)"}}
{"id": "676", "dataset": "mit-movie", "split": "test", "label_list": ["title", "song", "director", "average ratings", "genre", "actor", "plot", "rating", "review", "year", "character", "trailer"], "instance": {"id": "676", "words": ["is", "there", "a", "color", "farce", "tradgedy", "movie"], "labels": ["O", "O", "O", "B-genre", "I-genre", "I-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, song, director, average ratings, genre, actor, plot, rating, review, year, character, trailer and O.\nSentence: is there a color farce tradgedy movie", "prompt_labels": "is(O) there(O) a(O) color(B-genre) farce(I-genre) tradgedy(I-genre) movie(O)"}}
{"id": "1947", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "title", "review", "genre", "character", "year", "song", "average ratings", "director", "plot", "rating", "actor"], "instance": {"id": "1947", "words": ["what", "decent", "drama", "of", "the", "2000", "s", "featured", "a", "plot", "about", "a", "secret", "society"], "labels": ["O", "B-average ratings", "B-genre", "O", "O", "B-year", "I-year", "O", "O", "O", "O", "O", "B-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, title, review, genre, character, year, song, average ratings, director, plot, rating, actor and O.\nSentence: what decent drama of the 2000 s featured a plot about a secret society", "prompt_labels": "what(O) decent(B-average ratings) drama(B-genre) of(O) the(O) 2000(B-year) s(I-year) featured(O) a(O) plot(O) about(O) a(O) secret(B-plot) society(I-plot)"}}
{"id": "775", "dataset": "mit-movie", "split": "test", "label_list": ["review", "year", "character", "plot", "title", "rating", "average ratings", "song", "director", "actor", "trailer", "genre"], "instance": {"id": "775", "words": ["which", "1982", "science", "fiction", "film", "was", "directed", "by", "steven", "lisberger"], "labels": ["O", "B-year", "B-genre", "I-genre", "O", "O", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, year, character, plot, title, rating, average ratings, song, director, actor, trailer, genre and O.\nSentence: which 1982 science fiction film was directed by steven lisberger", "prompt_labels": "which(O) 1982(B-year) science(B-genre) fiction(I-genre) film(O) was(O) directed(O) by(O) steven(B-director) lisberger(I-director)"}}
{"id": "2332", "dataset": "mit-movie", "split": "test", "label_list": ["review", "song", "average ratings", "genre", "actor", "character", "plot", "year", "rating", "title", "director", "trailer"], "instance": {"id": "2332", "words": ["who", "stars", "in", "easy", "rider"], "labels": ["O", "O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, average ratings, genre, actor, character, plot, year, rating, title, director, trailer and O.\nSentence: who stars in easy rider", "prompt_labels": "who(O) stars(O) in(O) easy(B-title) rider(I-title)"}}
{"id": "1774", "dataset": "mit-movie", "split": "test", "label_list": ["year", "song", "average ratings", "title", "plot", "character", "review", "actor", "genre", "director", "trailer", "rating"], "instance": {"id": "1774", "words": ["the", "tree", "came", "out", "when"], "labels": ["B-title", "I-title", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, song, average ratings, title, plot, character, review, actor, genre, director, trailer, rating and O.\nSentence: the tree came out when", "prompt_labels": "the(B-title) tree(I-title) came(O) out(O) when(O)"}}
{"id": "1424", "dataset": "mit-movie", "split": "test", "label_list": ["year", "genre", "trailer", "plot", "average ratings", "title", "rating", "director", "review", "actor", "character", "song"], "instance": {"id": "1424", "words": ["in", "the", "past", "nine", "decades", "what", "movies", "have", "come", "out", "for", "children"], "labels": ["O", "O", "B-year", "I-year", "I-year", "O", "O", "O", "O", "O", "O", "B-genre"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, genre, trailer, plot, average ratings, title, rating, director, review, actor, character, song and O.\nSentence: in the past nine decades what movies have come out for children", "prompt_labels": "in(O) the(O) past(B-year) nine(I-year) decades(I-year) what(O) movies(O) have(O) come(O) out(O) for(O) children(B-genre)"}}
{"id": "199", "dataset": "mit-movie", "split": "test", "label_list": ["plot", "song", "average ratings", "rating", "review", "character", "title", "director", "year", "actor", "genre", "trailer"], "instance": {"id": "199", "words": ["who", "stars", "in", "the", "girl", "with", "the", "dragon", "tattoo"], "labels": ["O", "O", "O", "O", "B-title", "I-title", "I-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, song, average ratings, rating, review, character, title, director, year, actor, genre, trailer and O.\nSentence: who stars in the girl with the dragon tattoo", "prompt_labels": "who(O) stars(O) in(O) the(O) girl(B-title) with(I-title) the(I-title) dragon(I-title) tattoo(I-title)"}}
{"id": "643", "dataset": "mit-movie", "split": "test", "label_list": ["year", "plot", "character", "title", "rating", "song", "trailer", "actor", "review", "genre", "director", "average ratings"], "instance": {"id": "643", "words": ["is", "there", "a", "movie", "with", "kate", "hudson", "as", "a", "love", "struck", "columnist"], "labels": ["O", "O", "O", "O", "O", "B-actor", "I-actor", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, plot, character, title, rating, song, trailer, actor, review, genre, director, average ratings and O.\nSentence: is there a movie with kate hudson as a love struck columnist", "prompt_labels": "is(O) there(O) a(O) movie(O) with(O) kate(B-actor) hudson(I-actor) as(O) a(O) love(O) struck(O) columnist(O)"}}
{"id": "2119", "dataset": "mit-movie", "split": "test", "label_list": ["review", "rating", "year", "director", "title", "character", "song", "genre", "trailer", "actor", "plot", "average ratings"], "instance": {"id": "2119", "words": ["what", "is", "the", "title", "of", "an", "animated", "film", "by", "tim", "burton"], "labels": ["O", "O", "O", "O", "O", "O", "B-genre", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, rating, year, director, title, character, song, genre, trailer, actor, plot, average ratings and O.\nSentence: what is the title of an animated film by tim burton", "prompt_labels": "what(O) is(O) the(O) title(O) of(O) an(O) animated(B-genre) film(O) by(O) tim(B-director) burton(I-director)"}}
{"id": "468", "dataset": "mit-movie", "split": "test", "label_list": ["title", "trailer", "rating", "director", "actor", "genre", "song", "plot", "average ratings", "year", "character", "review"], "instance": {"id": "468", "words": ["show", "the", "blow", "soundtrack"], "labels": ["B-title", "I-title", "I-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, trailer, rating, director, actor, genre, song, plot, average ratings, year, character, review and O.\nSentence: show the blow soundtrack", "prompt_labels": "show(B-title) the(I-title) blow(I-title) soundtrack(O)"}}
{"id": "1142", "dataset": "mit-movie", "split": "test", "label_list": ["song", "actor", "year", "title", "trailer", "average ratings", "director", "genre", "plot", "character", "review", "rating"], "instance": {"id": "1142", "words": ["did", "george", "carlin", "star", "in", "any", "r", "rated", "or", "family", "films", "in", "the", "last", "decade"], "labels": ["O", "B-actor", "I-actor", "O", "O", "O", "B-rating", "O", "O", "B-genre", "O", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, actor, year, title, trailer, average ratings, director, genre, plot, character, review, rating and O.\nSentence: did george carlin star in any r rated or family films in the last decade", "prompt_labels": "did(O) george(B-actor) carlin(I-actor) star(O) in(O) any(O) r(B-rating) rated(O) or(O) family(B-genre) films(O) in(O) the(O) last(B-year) decade(I-year)"}}
{"id": "1891", "dataset": "mit-movie", "split": "test", "label_list": ["year", "trailer", "director", "title", "character", "song", "review", "average ratings", "actor", "plot", "rating", "genre"], "instance": {"id": "1891", "words": ["what", "are", "some", "r", "rated", "drama", "films", "that", "were", "directed", "by", "john", "geddes"], "labels": ["O", "O", "O", "B-rating", "O", "B-genre", "O", "O", "O", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, trailer, director, title, character, song, review, average ratings, actor, plot, rating, genre and O.\nSentence: what are some r rated drama films that were directed by john geddes", "prompt_labels": "what(O) are(O) some(O) r(B-rating) rated(O) drama(B-genre) films(O) that(O) were(O) directed(O) by(O) john(B-director) geddes(I-director)"}}
{"id": "2397", "dataset": "mit-movie", "split": "test", "label_list": ["plot", "actor", "title", "review", "genre", "trailer", "rating", "director", "average ratings", "song", "year", "character"], "instance": {"id": "2397", "words": ["can", "i", "see", "the", "trailer", "for", "kiss", "me", "kate"], "labels": ["O", "O", "O", "O", "B-trailer", "O", "B-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, actor, title, review, genre, trailer, rating, director, average ratings, song, year, character and O.\nSentence: can i see the trailer for kiss me kate", "prompt_labels": "can(O) i(O) see(O) the(O) trailer(B-trailer) for(O) kiss(B-title) me(I-title) kate(I-title)"}}
{"id": "541", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "song", "director", "year", "review", "plot", "title", "actor", "genre", "trailer", "rating", "character"], "instance": {"id": "541", "words": ["find", "the", "movie", "with", "the", "yellow", "brick", "road"], "labels": ["O", "O", "O", "O", "O", "B-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, song, director, year, review, plot, title, actor, genre, trailer, rating, character and O.\nSentence: find the movie with the yellow brick road", "prompt_labels": "find(O) the(O) movie(O) with(O) the(O) yellow(B-plot) brick(I-plot) road(I-plot)"}}
{"id": "1951", "dataset": "mit-movie", "split": "test", "label_list": ["character", "rating", "song", "plot", "year", "director", "title", "average ratings", "trailer", "genre", "review", "actor"], "instance": {"id": "1951", "words": ["what", "family", "movie", "directed", "by", "jefery", "levy", "is", "rated", "pg"], "labels": ["O", "B-genre", "O", "O", "O", "B-director", "I-director", "O", "O", "B-rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, rating, song, plot, year, director, title, average ratings, trailer, genre, review, actor and O.\nSentence: what family movie directed by jefery levy is rated pg", "prompt_labels": "what(O) family(B-genre) movie(O) directed(O) by(O) jefery(B-director) levy(I-director) is(O) rated(O) pg(B-rating)"}}
{"id": "664", "dataset": "mit-movie", "split": "test", "label_list": ["song", "plot", "average ratings", "character", "trailer", "director", "rating", "review", "year", "genre", "title", "actor"], "instance": {"id": "664", "words": ["is", "there", "a", "science", "fiction", "film", "that", "starts", "on", "a", "tower"], "labels": ["O", "O", "O", "B-genre", "I-genre", "O", "O", "B-plot", "I-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, plot, average ratings, character, trailer, director, rating, review, year, genre, title, actor and O.\nSentence: is there a science fiction film that starts on a tower", "prompt_labels": "is(O) there(O) a(O) science(B-genre) fiction(I-genre) film(O) that(O) starts(B-plot) on(I-plot) a(I-plot) tower(I-plot)"}}
{"id": "60", "dataset": "mit-movie", "split": "test", "label_list": ["title", "trailer", "actor", "character", "director", "plot", "average ratings", "genre", "year", "rating", "song", "review"], "instance": {"id": "60", "words": ["what", "is", "the", "g", "rated", "movie", "about", "rabbits", "looking", "for", "a", "new", "home"], "labels": ["O", "O", "O", "B-rating", "I-rating", "O", "O", "B-plot", "I-plot", "I-plot", "I-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, trailer, actor, character, director, plot, average ratings, genre, year, rating, song, review and O.\nSentence: what is the g rated movie about rabbits looking for a new home", "prompt_labels": "what(O) is(O) the(O) g(B-rating) rated(I-rating) movie(O) about(O) rabbits(B-plot) looking(I-plot) for(I-plot) a(I-plot) new(I-plot) home(I-plot)"}}
{"id": "167", "dataset": "mit-movie", "split": "test", "label_list": ["review", "genre", "trailer", "title", "rating", "plot", "song", "actor", "character", "year", "average ratings", "director"], "instance": {"id": "167", "words": ["what", "was", "goodfellas", "rated"], "labels": ["O", "O", "B-title", "B-rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, genre, trailer, title, rating, plot, song, actor, character, year, average ratings, director and O.\nSentence: what was goodfellas rated", "prompt_labels": "what(O) was(O) goodfellas(B-title) rated(B-rating)"}}
{"id": "1729", "dataset": "mit-movie", "split": "test", "label_list": ["song", "title", "actor", "genre", "average ratings", "director", "review", "plot", "rating", "character", "trailer", "year"], "instance": {"id": "1729", "words": ["name", "a", "highly", "liked", "r", "rated", "funny", "film", "released", "in", "the", "last", "five", "decades"], "labels": ["O", "O", "B-average ratings", "I-average ratings", "B-rating", "O", "B-genre", "O", "O", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, title, actor, genre, average ratings, director, review, plot, rating, character, trailer, year and O.\nSentence: name a highly liked r rated funny film released in the last five decades", "prompt_labels": "name(O) a(O) highly(B-average ratings) liked(I-average ratings) r(B-rating) rated(O) funny(B-genre) film(O) released(O) in(O) the(O) last(B-year) five(I-year) decades(I-year)"}}
{"id": "258", "dataset": "mit-movie", "split": "test", "label_list": ["song", "average ratings", "director", "rating", "actor", "genre", "title", "trailer", "year", "plot", "character", "review"], "instance": {"id": "258", "words": ["what", "is", "a", "good", "romance", "movie", "with", "jennifer", "aniston"], "labels": ["O", "O", "O", "B-review", "B-genre", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, average ratings, director, rating, actor, genre, title, trailer, year, plot, character, review and O.\nSentence: what is a good romance movie with jennifer aniston", "prompt_labels": "what(O) is(O) a(O) good(B-review) romance(B-genre) movie(O) with(O) jennifer(B-actor) aniston(I-actor)"}}
{"id": "254", "dataset": "mit-movie", "split": "test", "label_list": ["director", "rating", "average ratings", "title", "trailer", "review", "actor", "genre", "plot", "song", "character", "year"], "instance": {"id": "254", "words": ["what", "were", "the", "reviews", "like", "for", "the", "man", "who", "shot", "liberty", "valance"], "labels": ["O", "O", "O", "O", "O", "O", "B-title", "I-title", "I-title", "I-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, rating, average ratings, title, trailer, review, actor, genre, plot, song, character, year and O.\nSentence: what were the reviews like for the man who shot liberty valance", "prompt_labels": "what(O) were(O) the(O) reviews(O) like(O) for(O) the(B-title) man(I-title) who(I-title) shot(I-title) liberty(I-title) valance(I-title)"}}
{"id": "2311", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "director", "genre", "actor", "character", "plot", "trailer", "title", "average ratings", "review", "year", "song"], "instance": {"id": "2311", "words": ["who", "is", "in", "the", "movie", "spanglish"], "labels": ["O", "O", "O", "O", "O", "B-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, director, genre, actor, character, plot, trailer, title, average ratings, review, year, song and O.\nSentence: who is in the movie spanglish", "prompt_labels": "who(O) is(O) in(O) the(O) movie(O) spanglish(B-title)"}}
{"id": "1772", "dataset": "mit-movie", "split": "test", "label_list": ["character", "plot", "trailer", "average ratings", "song", "year", "title", "director", "review", "genre", "actor", "rating"], "instance": {"id": "1772", "words": ["tell", "me", "the", "name", "of", "a", "susan", "seidelman", "tale", "movie", "that", "came", "out", "in", "the", "past", "two", "decades", "and", "was", "about", "a", "warthog"], "labels": ["O", "O", "O", "O", "O", "O", "B-director", "I-director", "B-genre", "O", "O", "O", "O", "O", "O", "B-year", "I-year", "I-year", "O", "O", "O", "O", "B-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, plot, trailer, average ratings, song, year, title, director, review, genre, actor, rating and O.\nSentence: tell me the name of a susan seidelman tale movie that came out in the past two decades and was about a warthog", "prompt_labels": "tell(O) me(O) the(O) name(O) of(O) a(O) susan(B-director) seidelman(I-director) tale(B-genre) movie(O) that(O) came(O) out(O) in(O) the(O) past(B-year) two(I-year) decades(I-year) and(O) was(O) about(O) a(O) warthog(B-plot)"}}
{"id": "269", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "title", "review", "song", "plot", "actor", "director", "trailer", "year", "character", "genre", "rating"], "instance": {"id": "269", "words": ["find", "me", "the", "number", "of", "samurai", "films", "made", "in", "the", "1960s"], "labels": ["O", "O", "O", "O", "O", "B-plot", "O", "O", "O", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, title, review, song, plot, actor, director, trailer, year, character, genre, rating and O.\nSentence: find me the number of samurai films made in the 1960s", "prompt_labels": "find(O) me(O) the(O) number(O) of(O) samurai(B-plot) films(O) made(O) in(O) the(O) 1960s(B-year)"}}
{"id": "2063", "dataset": "mit-movie", "split": "test", "label_list": ["plot", "actor", "review", "genre", "director", "title", "song", "average ratings", "trailer", "year", "character", "rating"], "instance": {"id": "2063", "words": ["what", "is", "the", "best", "mockumentary"], "labels": ["O", "O", "O", "O", "B-genre"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, actor, review, genre, director, title, song, average ratings, trailer, year, character, rating and O.\nSentence: what is the best mockumentary", "prompt_labels": "what(O) is(O) the(O) best(O) mockumentary(B-genre)"}}
{"id": "1364", "dataset": "mit-movie", "split": "test", "label_list": ["plot", "trailer", "title", "actor", "director", "average ratings", "year", "character", "genre", "song", "rating", "review"], "instance": {"id": "1364", "words": ["im", "looking", "for", "a", "movie", "with", "actor", "paul", "reiser", "in", "it"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-actor", "I-actor", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, trailer, title, actor, director, average ratings, year, character, genre, song, rating, review and O.\nSentence: im looking for a movie with actor paul reiser in it", "prompt_labels": "im(O) looking(O) for(O) a(O) movie(O) with(O) actor(O) paul(B-actor) reiser(I-actor) in(O) it(O)"}}
{"id": "165", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "genre", "title", "plot", "director", "character", "year", "review", "average ratings", "actor", "song", "rating"], "instance": {"id": "165", "words": ["what", "was", "the", "fog", "rated"], "labels": ["O", "O", "B-title", "I-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, genre, title, plot, director, character, year, review, average ratings, actor, song, rating and O.\nSentence: what was the fog rated", "prompt_labels": "what(O) was(O) the(B-title) fog(I-title) rated(O)"}}
{"id": "1041", "dataset": "mit-movie", "split": "test", "label_list": ["character", "song", "average ratings", "director", "rating", "genre", "year", "actor", "trailer", "title", "plot", "review"], "instance": {"id": "1041", "words": ["what", "film", "featured", "mel", "gibson", "danny", "glover", "and", "jet", "li"], "labels": ["O", "O", "O", "B-actor", "I-actor", "I-actor", "I-actor", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, song, average ratings, director, rating, genre, year, actor, trailer, title, plot, review and O.\nSentence: what film featured mel gibson danny glover and jet li", "prompt_labels": "what(O) film(O) featured(O) mel(B-actor) gibson(I-actor) danny(I-actor) glover(I-actor) and(O) jet(B-actor) li(I-actor)"}}
{"id": "531", "dataset": "mit-movie", "split": "test", "label_list": ["character", "actor", "title", "song", "plot", "genre", "review", "average ratings", "director", "rating", "year", "trailer"], "instance": {"id": "531", "words": ["find", "a", "chuck", "norris", "movie", "with", "a", "lot", "of", "martial", "arts", "fighting", "in", "it"], "labels": ["O", "O", "B-actor", "I-actor", "O", "O", "O", "O", "O", "B-plot", "I-plot", "I-plot", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, actor, title, song, plot, genre, review, average ratings, director, rating, year, trailer and O.\nSentence: find a chuck norris movie with a lot of martial arts fighting in it", "prompt_labels": "find(O) a(O) chuck(B-actor) norris(I-actor) movie(O) with(O) a(O) lot(O) of(O) martial(B-plot) arts(I-plot) fighting(I-plot) in(O) it(O)"}}
{"id": "34", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Location", "Dish", "Rating", "Hours", "Restaurant Name", "Price", "Amenity"], "instance": {"id": "34", "words": ["are", "there", "any", "eatery", "at", "the", "hotel", "downtown"], "labels": ["O", "O", "O", "O", "O", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Location, Dish, Rating, Hours, Restaurant Name, Price, Amenity and O.\nSentence: are there any eatery at the hotel downtown", "prompt_labels": "are(O) there(O) any(O) eatery(O) at(O) the(O) hotel(B-Location) downtown(I-Location)"}}
{"id": "403", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Restaurant Name", "Dish", "Price", "Location", "Rating", "Hours", "Amenity"], "instance": {"id": "403", "words": ["find", "me", "a", "romantic", "restaurant", "that", "has", "an", "open", "table"], "labels": ["O", "O", "O", "B-Amenity", "O", "O", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Restaurant Name, Dish, Price, Location, Rating, Hours, Amenity and O.\nSentence: find me a romantic restaurant that has an open table", "prompt_labels": "find(O) me(O) a(O) romantic(B-Amenity) restaurant(O) that(O) has(O) an(O) open(B-Amenity) table(I-Amenity)"}}
{"id": "425", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Hours", "Restaurant Name", "Price", "Amenity", "Location", "Cuisine", "Rating"], "instance": {"id": "425", "words": ["find", "me", "fast", "food", "that", "serves", "salad"], "labels": ["O", "O", "B-Cuisine", "I-Cuisine", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Hours, Restaurant Name, Price, Amenity, Location, Cuisine, Rating and O.\nSentence: find me fast food that serves salad", "prompt_labels": "find(O) me(O) fast(B-Cuisine) food(I-Cuisine) that(O) serves(O) salad(B-Dish)"}}
{"id": "372", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Price", "Cuisine", "Location", "Rating", "Dish", "Hours", "Restaurant Name"], "instance": {"id": "372", "words": ["find", "me", "a", "good", "burrito", "truck", "in", "the", "mission"], "labels": ["O", "O", "O", "B-Rating", "B-Dish", "I-Dish", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Price, Cuisine, Location, Rating, Dish, Hours, Restaurant Name and O.\nSentence: find me a good burrito truck in the mission", "prompt_labels": "find(O) me(O) a(O) good(B-Rating) burrito(B-Dish) truck(I-Dish) in(B-Location) the(I-Location) mission(I-Location)"}}
{"id": "78", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Hours", "Dish", "Restaurant Name", "Cuisine", "Location", "Amenity", "Rating"], "instance": {"id": "78", "words": ["are", "there", "any", "restaurants", "on", "the", "way", "to", "my", "destination", "that", "have", "a", "fireplace", "inside"], "labels": ["O", "O", "O", "O", "O", "O", "B-Location", "I-Location", "I-Location", "I-Location", "O", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Hours, Dish, Restaurant Name, Cuisine, Location, Amenity, Rating and O.\nSentence: are there any restaurants on the way to my destination that have a fireplace inside", "prompt_labels": "are(O) there(O) any(O) restaurants(O) on(O) the(O) way(B-Location) to(I-Location) my(I-Location) destination(I-Location) that(O) have(O) a(O) fireplace(B-Amenity) inside(I-Amenity)"}}
{"id": "743", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Restaurant Name", "Price", "Rating", "Location", "Cuisine", "Hours", "Dish"], "instance": {"id": "743", "words": ["im", "looking", "for", "cheap", "spanish", "cuisine"], "labels": ["O", "O", "O", "B-Price", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Restaurant Name, Price, Rating, Location, Cuisine, Hours, Dish and O.\nSentence: im looking for cheap spanish cuisine", "prompt_labels": "im(O) looking(O) for(O) cheap(B-Price) spanish(B-Cuisine) cuisine(O)"}}
{"id": "1251", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Restaurant Name", "Rating", "Hours", "Amenity", "Cuisine", "Location", "Price"], "instance": {"id": "1251", "words": ["when", "does", "white", "castle", "close"], "labels": ["O", "O", "B-Restaurant Name", "I-Restaurant Name", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Restaurant Name, Rating, Hours, Amenity, Cuisine, Location, Price and O.\nSentence: when does white castle close", "prompt_labels": "when(O) does(O) white(B-Restaurant Name) castle(I-Restaurant Name) close(B-Location)"}}
{"id": "880", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Price", "Restaurant Name", "Dish", "Amenity", "Hours", "Location", "Rating"], "instance": {"id": "880", "words": ["is", "there", "a", "vegan", "restaurant", "within", "5", "miles"], "labels": ["O", "O", "O", "B-Cuisine", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Restaurant Name, Dish, Amenity, Hours, Location, Rating and O.\nSentence: is there a vegan restaurant within 5 miles", "prompt_labels": "is(O) there(O) a(O) vegan(B-Cuisine) restaurant(O) within(B-Location) 5(I-Location) miles(I-Location)"}}
{"id": "246", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Price", "Rating", "Restaurant Name", "Cuisine", "Location", "Amenity", "Dish"], "instance": {"id": "246", "words": ["do", "they", "have", "a", "smoking", "area"], "labels": ["O", "O", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Price, Rating, Restaurant Name, Cuisine, Location, Amenity, Dish and O.\nSentence: do they have a smoking area", "prompt_labels": "do(O) they(O) have(O) a(O) smoking(B-Amenity) area(I-Amenity)"}}
{"id": "937", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Dish", "Location", "Price", "Hours", "Cuisine", "Restaurant Name", "Rating"], "instance": {"id": "937", "words": ["lets", "find", "the", "fanciest", "french", "cuisine", "in", "the", "city"], "labels": ["O", "O", "O", "B-Amenity", "B-Cuisine", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Location, Price, Hours, Cuisine, Restaurant Name, Rating and O.\nSentence: lets find the fanciest french cuisine in the city", "prompt_labels": "lets(O) find(O) the(O) fanciest(B-Amenity) french(B-Cuisine) cuisine(O) in(B-Location) the(I-Location) city(I-Location)"}}
{"id": "463", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Amenity", "Restaurant Name", "Dish", "Price", "Cuisine", "Location", "Hours"], "instance": {"id": "463", "words": ["give", "me", "a", "list", "of", "restaurants", "that", "have", "seafood", "on", "the", "menu"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-Cuisine", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Restaurant Name, Dish, Price, Cuisine, Location, Hours and O.\nSentence: give me a list of restaurants that have seafood on the menu", "prompt_labels": "give(O) me(O) a(O) list(O) of(O) restaurants(O) that(O) have(O) seafood(B-Cuisine) on(O) the(O) menu(O)"}}
{"id": "726", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Location", "Dish", "Rating", "Price", "Cuisine", "Hours", "Restaurant Name"], "instance": {"id": "726", "words": ["im", "in", "the", "mood", "for", "chinese", "food"], "labels": ["O", "O", "O", "O", "O", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Location, Dish, Rating, Price, Cuisine, Hours, Restaurant Name and O.\nSentence: im in the mood for chinese food", "prompt_labels": "im(O) in(O) the(O) mood(O) for(O) chinese(B-Cuisine) food(O)"}}
{"id": "1416", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Restaurant Name", "Hours", "Amenity", "Cuisine", "Dish", "Location", "Price"], "instance": {"id": "1416", "words": ["where", "is", "the", "closest", "pizza", "hut"], "labels": ["O", "O", "O", "B-Location", "B-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Hours, Amenity, Cuisine, Dish, Location, Price and O.\nSentence: where is the closest pizza hut", "prompt_labels": "where(O) is(O) the(O) closest(B-Location) pizza(B-Restaurant Name) hut(I-Restaurant Name)"}}
{"id": "52", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Cuisine", "Hours", "Dish", "Amenity", "Rating", "Restaurant Name", "Location"], "instance": {"id": "52", "words": ["are", "there", "any", "japanese", "restaurants", "in", "town", "that", "do", "discounts", "for", "bulk", "orders", "of", "sushi"], "labels": ["O", "O", "O", "B-Cuisine", "O", "B-Location", "I-Location", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity", "I-Amenity", "O", "B-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Cuisine, Hours, Dish, Amenity, Rating, Restaurant Name, Location and O.\nSentence: are there any japanese restaurants in town that do discounts for bulk orders of sushi", "prompt_labels": "are(O) there(O) any(O) japanese(B-Cuisine) restaurants(O) in(B-Location) town(I-Location) that(O) do(O) discounts(B-Amenity) for(I-Amenity) bulk(I-Amenity) orders(I-Amenity) of(O) sushi(B-Amenity)"}}
{"id": "569", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Amenity", "Cuisine", "Location", "Hours", "Dish", "Rating", "Price"], "instance": {"id": "569", "words": ["i", "need", "a", "4", "star", "rated", "subway", "nearby"], "labels": ["O", "O", "O", "B-Rating", "I-Rating", "I-Rating", "B-Restaurant Name", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Cuisine, Location, Hours, Dish, Rating, Price and O.\nSentence: i need a 4 star rated subway nearby", "prompt_labels": "i(O) need(O) a(O) 4(B-Rating) star(I-Rating) rated(I-Rating) subway(B-Restaurant Name) nearby(B-Location)"}}
{"id": "635", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Hours", "Rating", "Location", "Amenity", "Cuisine", "Price", "Restaurant Name"], "instance": {"id": "635", "words": ["i", "want", "pizza", "im", "looking", "for", "the", "best", "pizza", "restaurant", "that", "is", "kid", "friendly", "and", "has", "carry", "out"], "labels": ["O", "O", "B-Dish", "O", "O", "O", "O", "B-Rating", "B-Dish", "O", "O", "O", "B-Amenity", "I-Amenity", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Hours, Rating, Location, Amenity, Cuisine, Price, Restaurant Name and O.\nSentence: i want pizza im looking for the best pizza restaurant that is kid friendly and has carry out", "prompt_labels": "i(O) want(O) pizza(B-Dish) im(O) looking(O) for(O) the(O) best(B-Rating) pizza(B-Dish) restaurant(O) that(O) is(O) kid(B-Amenity) friendly(I-Amenity) and(O) has(O) carry(B-Amenity) out(I-Amenity)"}}
{"id": "295", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Rating", "Dish", "Price", "Location", "Restaurant Name", "Hours", "Amenity"], "instance": {"id": "295", "words": ["does", "olive", "garden", "serve", "wine"], "labels": ["O", "B-Restaurant Name", "I-Restaurant Name", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Dish, Price, Location, Restaurant Name, Hours, Amenity and O.\nSentence: does olive garden serve wine", "prompt_labels": "does(O) olive(B-Restaurant Name) garden(I-Restaurant Name) serve(O) wine(B-Dish)"}}
{"id": "645", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Rating", "Dish", "Price", "Hours", "Cuisine", "Amenity", "Location"], "instance": {"id": "645", "words": ["i", "want", "to", "eat", "at", "a", "very", "classy", "restaurant"], "labels": ["O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Rating, Dish, Price, Hours, Cuisine, Amenity, Location and O.\nSentence: i want to eat at a very classy restaurant", "prompt_labels": "i(O) want(O) to(O) eat(O) at(O) a(O) very(B-Amenity) classy(I-Amenity) restaurant(O)"}}
{"id": "10", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Location", "Amenity", "Hours", "Rating", "Cuisine", "Price", "Restaurant Name"], "instance": {"id": "10", "words": ["any", "places", "around", "here", "that", "has", "a", "nice", "view"], "labels": ["O", "O", "B-Location", "I-Location", "O", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Amenity, Hours, Rating, Cuisine, Price, Restaurant Name and O.\nSentence: any places around here that has a nice view", "prompt_labels": "any(O) places(O) around(B-Location) here(I-Location) that(O) has(O) a(O) nice(B-Amenity) view(I-Amenity)"}}
{"id": "0", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Dish", "Price", "Rating", "Location", "Amenity", "Cuisine", "Hours"], "instance": {"id": "0", "words": ["a", "four", "star", "restaurant", "with", "a", "bar"], "labels": ["O", "B-Rating", "I-Rating", "O", "B-Location", "I-Location", "B-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Dish, Price, Rating, Location, Amenity, Cuisine, Hours and O.\nSentence: a four star restaurant with a bar", "prompt_labels": "a(O) four(B-Rating) star(I-Rating) restaurant(O) with(B-Location) a(I-Location) bar(B-Amenity)"}}
{"id": "661", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Dish", "Amenity", "Rating", "Restaurant Name", "Price", "Hours", "Location"], "instance": {"id": "661", "words": ["i", "want", "to", "find", "a", "restaurant", "that", "has", "a", "diet", "friendly", "menu"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Dish, Amenity, Rating, Restaurant Name, Price, Hours, Location and O.\nSentence: i want to find a restaurant that has a diet friendly menu", "prompt_labels": "i(O) want(O) to(O) find(O) a(O) restaurant(O) that(O) has(O) a(O) diet(B-Amenity) friendly(I-Amenity) menu(O)"}}
{"id": "400", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Location", "Cuisine", "Rating", "Restaurant Name", "Hours", "Amenity", "Price"], "instance": {"id": "400", "words": ["find", "me", "a", "restaurant", "within", "5", "miles", "of", "here", "that", "is", "open", "at", "1", "am"], "labels": ["O", "O", "O", "O", "B-Location", "I-Location", "I-Location", "I-Location", "I-Location", "O", "O", "B-Hours", "I-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Cuisine, Rating, Restaurant Name, Hours, Amenity, Price and O.\nSentence: find me a restaurant within 5 miles of here that is open at 1 am", "prompt_labels": "find(O) me(O) a(O) restaurant(O) within(B-Location) 5(I-Location) miles(I-Location) of(I-Location) here(I-Location) that(O) is(O) open(B-Hours) at(I-Hours) 1(I-Hours) am(I-Hours)"}}
{"id": "1190", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Dish", "Hours", "Rating", "Restaurant Name", "Amenity", "Location", "Price"], "instance": {"id": "1190", "words": ["what", "restaurants", "are", "open", "past", "midnight"], "labels": ["O", "O", "O", "B-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Dish, Hours, Rating, Restaurant Name, Amenity, Location, Price and O.\nSentence: what restaurants are open past midnight", "prompt_labels": "what(O) restaurants(O) are(O) open(B-Hours) past(I-Hours) midnight(I-Hours)"}}
{"id": "913", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Dish", "Cuisine", "Rating", "Hours", "Location", "Price", "Restaurant Name"], "instance": {"id": "913", "words": ["is", "there", "any", "restaurant", "in", "the", "city", "that", "serves", "congee"], "labels": ["O", "O", "O", "O", "B-Location", "I-Location", "I-Location", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Cuisine, Rating, Hours, Location, Price, Restaurant Name and O.\nSentence: is there any restaurant in the city that serves congee", "prompt_labels": "is(O) there(O) any(O) restaurant(O) in(B-Location) the(I-Location) city(I-Location) that(O) serves(O) congee(B-Dish)"}}
{"id": "1075", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Rating", "Cuisine", "Restaurant Name", "Location", "Hours", "Price", "Dish"], "instance": {"id": "1075", "words": ["take", "me", "to", "a", "restaurant", "i", "would", "like"], "labels": ["O", "O", "O", "O", "O", "O", "B-Rating", "I-Rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Cuisine, Restaurant Name, Location, Hours, Price, Dish and O.\nSentence: take me to a restaurant i would like", "prompt_labels": "take(O) me(O) to(O) a(O) restaurant(O) i(O) would(B-Rating) like(I-Rating)"}}
{"id": "1059", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Location", "Rating", "Dish", "Price", "Cuisine", "Hours", "Amenity"], "instance": {"id": "1059", "words": ["show", "me", "some", "mexican", "restaurants", "that", "arent", "too", "far"], "labels": ["O", "O", "O", "B-Cuisine", "O", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Location, Rating, Dish, Price, Cuisine, Hours, Amenity and O.\nSentence: show me some mexican restaurants that arent too far", "prompt_labels": "show(O) me(O) some(O) mexican(B-Cuisine) restaurants(O) that(O) arent(B-Location) too(I-Location) far(I-Location)"}}
{"id": "786", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Location", "Price", "Cuisine", "Amenity", "Dish", "Hours", "Rating"], "instance": {"id": "786", "words": ["is", "the", "patio", "at", "the", "butcher", "and", "the", "boar", "dog", "friendly"], "labels": ["O", "O", "B-Amenity", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Location, Price, Cuisine, Amenity, Dish, Hours, Rating and O.\nSentence: is the patio at the butcher and the boar dog friendly", "prompt_labels": "is(O) the(O) patio(B-Amenity) at(O) the(O) butcher(B-Restaurant Name) and(I-Restaurant Name) the(I-Restaurant Name) boar(I-Restaurant Name) dog(B-Amenity) friendly(I-Amenity)"}}
{"id": "328", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Price", "Hours", "Amenity", "Restaurant Name", "Location", "Cuisine", "Dish"], "instance": {"id": "328", "words": ["dominos", "pizza", "joint", "near", "my", "location"], "labels": ["B-Restaurant Name", "I-Restaurant Name", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Price, Hours, Amenity, Restaurant Name, Location, Cuisine, Dish and O.\nSentence: dominos pizza joint near my location", "prompt_labels": "dominos(B-Restaurant Name) pizza(I-Restaurant Name) joint(O) near(B-Location) my(I-Location) location(I-Location)"}}
{"id": "910", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Price", "Dish", "Cuisine", "Location", "Rating", "Amenity", "Restaurant Name"], "instance": {"id": "910", "words": ["is", "there", "any", "place", "where", "i", "can", "take", "my", "pet"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Price, Dish, Cuisine, Location, Rating, Amenity, Restaurant Name and O.\nSentence: is there any place where i can take my pet", "prompt_labels": "is(O) there(O) any(O) place(O) where(O) i(O) can(O) take(O) my(O) pet(B-Amenity)"}}
{"id": "1369", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Cuisine", "Dish", "Hours", "Price", "Rating", "Location", "Restaurant Name"], "instance": {"id": "1369", "words": ["where", "i", "can", "get", "seafood", "and", "bring", "my", "own", "drinks"], "labels": ["O", "O", "O", "O", "B-Cuisine", "O", "B-Amenity", "I-Amenity", "I-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Cuisine, Dish, Hours, Price, Rating, Location, Restaurant Name and O.\nSentence: where i can get seafood and bring my own drinks", "prompt_labels": "where(O) i(O) can(O) get(O) seafood(B-Cuisine) and(O) bring(B-Amenity) my(I-Amenity) own(I-Amenity) drinks(I-Amenity)"}}
{"id": "368", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Rating", "Hours", "Amenity", "Location", "Dish", "Price", "Restaurant Name"], "instance": {"id": "368", "words": ["find", "me", "a", "fancy", "place", "to", "eat"], "labels": ["O", "O", "O", "B-Amenity", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Hours, Amenity, Location, Dish, Price, Restaurant Name and O.\nSentence: find me a fancy place to eat", "prompt_labels": "find(O) me(O) a(O) fancy(B-Amenity) place(O) to(O) eat(O)"}}
{"id": "1119", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Amenity", "Cuisine", "Location", "Hours", "Price", "Rating", "Dish"], "instance": {"id": "1119", "words": ["what", "is", "the", "best", "ice", "cream", "parlor"], "labels": ["O", "O", "O", "B-Rating", "B-Cuisine", "I-Cuisine", "I-Cuisine"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Cuisine, Location, Hours, Price, Rating, Dish and O.\nSentence: what is the best ice cream parlor", "prompt_labels": "what(O) is(O) the(O) best(B-Rating) ice(B-Cuisine) cream(I-Cuisine) parlor(I-Cuisine)"}}
{"id": "653", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Price", "Hours", "Amenity", "Restaurant Name", "Dish", "Rating", "Location"], "instance": {"id": "653", "words": ["i", "want", "to", "find", "a", "german", "restaurant", "on", "the", "lower", "east", "side", "that", "serves", "brunch", "on", "saturday", "and", "or", "sunday"], "labels": ["O", "O", "O", "O", "O", "B-Cuisine", "O", "O", "O", "B-Location", "I-Location", "I-Location", "O", "O", "B-Hours", "O", "B-Hours", "I-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Hours, Amenity, Restaurant Name, Dish, Rating, Location and O.\nSentence: i want to find a german restaurant on the lower east side that serves brunch on saturday and or sunday", "prompt_labels": "i(O) want(O) to(O) find(O) a(O) german(B-Cuisine) restaurant(O) on(O) the(O) lower(B-Location) east(I-Location) side(I-Location) that(O) serves(O) brunch(B-Hours) on(O) saturday(B-Hours) and(I-Hours) or(I-Hours) sunday(I-Hours)"}}
{"id": "1453", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Amenity", "Dish", "Restaurant Name", "Cuisine", "Price", "Location", "Rating"], "instance": {"id": "1453", "words": ["where", "is", "the", "next", "mcdonald"], "labels": ["O", "O", "O", "B-Location", "B-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Amenity, Dish, Restaurant Name, Cuisine, Price, Location, Rating and O.\nSentence: where is the next mcdonald", "prompt_labels": "where(O) is(O) the(O) next(B-Location) mcdonald(B-Restaurant Name)"}}
{"id": "1355", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Amenity", "Hours", "Location", "Cuisine", "Restaurant Name", "Dish", "Rating"], "instance": {"id": "1355", "words": ["where", "can", "i", "take", "a", "date", "nearby", "for", "enchiladas"], "labels": ["O", "O", "O", "O", "O", "B-Amenity", "B-Location", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Hours, Location, Cuisine, Restaurant Name, Dish, Rating and O.\nSentence: where can i take a date nearby for enchiladas", "prompt_labels": "where(O) can(O) i(O) take(O) a(O) date(B-Amenity) nearby(B-Location) for(O) enchiladas(B-Dish)"}}
{"id": "287", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Amenity", "Cuisine", "Price", "Location", "Rating", "Dish", "Hours"], "instance": {"id": "287", "words": ["does", "johnsons", "steakhouse", "require", "formal", "attire"], "labels": ["O", "B-Restaurant Name", "I-Restaurant Name", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Cuisine, Price, Location, Rating, Dish, Hours and O.\nSentence: does johnsons steakhouse require formal attire", "prompt_labels": "does(O) johnsons(B-Restaurant Name) steakhouse(I-Restaurant Name) require(O) formal(B-Amenity) attire(I-Amenity)"}}
{"id": "457", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Cuisine", "Location", "Dish", "Rating", "Hours", "Amenity", "Price"], "instance": {"id": "457", "words": ["find", "us", "a", "sushi", "bar", "near", "brooklyn", "heights"], "labels": ["O", "O", "O", "B-Cuisine", "I-Cuisine", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Cuisine, Location, Dish, Rating, Hours, Amenity, Price and O.\nSentence: find us a sushi bar near brooklyn heights", "prompt_labels": "find(O) us(O) a(O) sushi(B-Cuisine) bar(I-Cuisine) near(B-Location) brooklyn(I-Location) heights(I-Location)"}}
{"id": "1371", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Rating", "Dish", "Restaurant Name", "Amenity", "Location", "Price", "Hours"], "instance": {"id": "1371", "words": ["where", "in", "the", "theater", "district", "is", "there", "a", "restaurant", "with", "portions", "that", "are", "a", "bit", "small"], "labels": ["O", "O", "O", "B-Location", "I-Location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Dish, Restaurant Name, Amenity, Location, Price, Hours and O.\nSentence: where in the theater district is there a restaurant with portions that are a bit small", "prompt_labels": "where(O) in(O) the(O) theater(B-Location) district(I-Location) is(O) there(O) a(O) restaurant(O) with(O) portions(O) that(O) are(O) a(O) bit(O) small(O)"}}
{"id": "847", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Rating", "Cuisine", "Restaurant Name", "Location", "Dish", "Price", "Hours"], "instance": {"id": "847", "words": ["is", "there", "a", "place", "to", "get", "high", "tea", "nearby"], "labels": ["O", "O", "O", "B-Location", "O", "O", "B-Cuisine", "I-Cuisine", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Cuisine, Restaurant Name, Location, Dish, Price, Hours and O.\nSentence: is there a place to get high tea nearby", "prompt_labels": "is(O) there(O) a(O) place(B-Location) to(O) get(O) high(B-Cuisine) tea(I-Cuisine) nearby(B-Location)"}}
{"id": "1253", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Hours", "Rating", "Cuisine", "Dish", "Price", "Amenity", "Location"], "instance": {"id": "1253", "words": ["where", "are", "some", "mexican", "restaurants"], "labels": ["O", "O", "O", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Rating, Cuisine, Dish, Price, Amenity, Location and O.\nSentence: where are some mexican restaurants", "prompt_labels": "where(O) are(O) some(O) mexican(B-Cuisine) restaurants(O)"}}
{"id": "897", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Hours", "Price", "Amenity", "Rating", "Restaurant Name", "Dish", "Location"], "instance": {"id": "897", "words": ["is", "there", "an", "open", "taco", "bell", "nearby"], "labels": ["O", "O", "O", "B-Hours", "B-Restaurant Name", "I-Restaurant Name", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Hours, Price, Amenity, Rating, Restaurant Name, Dish, Location and O.\nSentence: is there an open taco bell nearby", "prompt_labels": "is(O) there(O) an(O) open(B-Hours) taco(B-Restaurant Name) bell(I-Restaurant Name) nearby(B-Location)"}}
{"id": "978", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Cuisine", "Hours", "Dish", "Location", "Price", "Rating", "Restaurant Name"], "instance": {"id": "978", "words": ["looking", "for", "hotel", "dining", "on", "central", "square", "with", "great", "pricing"], "labels": ["O", "O", "B-Cuisine", "I-Cuisine", "O", "B-Location", "I-Location", "O", "B-Price", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Cuisine, Hours, Dish, Location, Price, Rating, Restaurant Name and O.\nSentence: looking for hotel dining on central square with great pricing", "prompt_labels": "looking(O) for(O) hotel(B-Cuisine) dining(I-Cuisine) on(O) central(B-Location) square(I-Location) with(O) great(B-Price) pricing(O)"}}
{"id": "682", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Dish", "Cuisine", "Rating", "Location", "Hours", "Price", "Amenity"], "instance": {"id": "682", "words": ["i", "want", "to", "try", "something", "new", "for", "dinner", "tonight"], "labels": ["O", "O", "O", "O", "O", "B-Cuisine", "O", "B-Cuisine", "B-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Dish, Cuisine, Rating, Location, Hours, Price, Amenity and O.\nSentence: i want to try something new for dinner tonight", "prompt_labels": "i(O) want(O) to(O) try(O) something(O) new(B-Cuisine) for(O) dinner(B-Cuisine) tonight(B-Hours)"}}
{"id": "805", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Location", "Price", "Cuisine", "Amenity", "Hours", "Dish", "Restaurant Name"], "instance": {"id": "805", "words": ["is", "there", "a", "cheap", "vegetarian", "restaurant", "nearby"], "labels": ["O", "O", "O", "B-Price", "B-Cuisine", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Location, Price, Cuisine, Amenity, Hours, Dish, Restaurant Name and O.\nSentence: is there a cheap vegetarian restaurant nearby", "prompt_labels": "is(O) there(O) a(O) cheap(B-Price) vegetarian(B-Cuisine) restaurant(O) nearby(B-Location)"}}
{"id": "193", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Dish", "Location", "Cuisine", "Amenity", "Hours", "Price", "Restaurant Name"], "instance": {"id": "193", "words": ["can", "you", "locate", "the", "business", "hours", "for", "a", "restaurant", "that", "serves", "brunch"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Dish, Location, Cuisine, Amenity, Hours, Price, Restaurant Name and O.\nSentence: can you locate the business hours for a restaurant that serves brunch", "prompt_labels": "can(O) you(O) locate(O) the(O) business(O) hours(O) for(O) a(O) restaurant(O) that(O) serves(O) brunch(B-Hours)"}}
{"id": "296", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Dish", "Restaurant Name", "Amenity", "Cuisine", "Location", "Rating", "Price"], "instance": {"id": "296", "words": ["does", "osaka", "sushi", "express", "have", "great", "portions"], "labels": ["O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Dish, Restaurant Name, Amenity, Cuisine, Location, Rating, Price and O.\nSentence: does osaka sushi express have great portions", "prompt_labels": "does(O) osaka(B-Restaurant Name) sushi(I-Restaurant Name) express(I-Restaurant Name) have(O) great(B-Amenity) portions(I-Amenity)"}}
{"id": "580", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Hours", "Dish", "Rating", "Cuisine", "Location", "Amenity", "Price"], "instance": {"id": "580", "words": ["i", "need", "a", "middle", "eastern", "restaurant", "with", "friendly", "service"], "labels": ["O", "O", "O", "B-Cuisine", "I-Cuisine", "O", "O", "B-Rating", "I-Rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Dish, Rating, Cuisine, Location, Amenity, Price and O.\nSentence: i need a middle eastern restaurant with friendly service", "prompt_labels": "i(O) need(O) a(O) middle(B-Cuisine) eastern(I-Cuisine) restaurant(O) with(O) friendly(B-Rating) service(I-Rating)"}}
{"id": "47", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Cuisine", "Dish", "Price", "Location", "Restaurant Name", "Rating", "Amenity"], "instance": {"id": "47", "words": ["are", "there", "any", "hamburger", "restaurants", "close", "by"], "labels": ["O", "O", "O", "B-Cuisine", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Dish, Price, Location, Restaurant Name, Rating, Amenity and O.\nSentence: are there any hamburger restaurants close by", "prompt_labels": "are(O) there(O) any(O) hamburger(B-Cuisine) restaurants(O) close(B-Location) by(I-Location)"}}
{"id": "594", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Cuisine", "Price", "Amenity", "Dish", "Hours", "Rating", "Restaurant Name"], "instance": {"id": "594", "words": ["i", "need", "directions", "from", "my", "location", "to", "luigis", "pizza"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Cuisine, Price, Amenity, Dish, Hours, Rating, Restaurant Name and O.\nSentence: i need directions from my location to luigis pizza", "prompt_labels": "i(O) need(O) directions(O) from(O) my(O) location(O) to(O) luigis(B-Restaurant Name) pizza(I-Restaurant Name)"}}
{"id": "170", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Restaurant Name", "Rating", "Cuisine", "Dish", "Price", "Amenity", "Hours"], "instance": {"id": "170", "words": ["can", "you", "find", "me", "some", "take", "out", "ribs"], "labels": ["O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Restaurant Name, Rating, Cuisine, Dish, Price, Amenity, Hours and O.\nSentence: can you find me some take out ribs", "prompt_labels": "can(O) you(O) find(O) me(O) some(O) take(B-Amenity) out(I-Amenity) ribs(B-Dish)"}}
{"id": "578", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Restaurant Name", "Amenity", "Dish", "Hours", "Price", "Rating", "Location"], "instance": {"id": "578", "words": ["i", "need", "a", "late", "night", "spot", "with", "good", "service"], "labels": ["O", "O", "O", "B-Hours", "I-Hours", "O", "O", "B-Rating", "I-Rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Restaurant Name, Amenity, Dish, Hours, Price, Rating, Location and O.\nSentence: i need a late night spot with good service", "prompt_labels": "i(O) need(O) a(O) late(B-Hours) night(I-Hours) spot(O) with(O) good(B-Rating) service(I-Rating)"}}
{"id": "76", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Cuisine", "Hours", "Rating", "Restaurant Name", "Amenity", "Price", "Location"], "instance": {"id": "76", "words": ["are", "there", "any", "restaurants", "on", "kilmarnock", "street", "that", "feature", "large", "portions", "and", "a", "brewpub"], "labels": ["O", "O", "O", "O", "O", "B-Location", "I-Location", "O", "O", "B-Amenity", "I-Amenity", "O", "O", "B-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Cuisine, Hours, Rating, Restaurant Name, Amenity, Price, Location and O.\nSentence: are there any restaurants on kilmarnock street that feature large portions and a brewpub", "prompt_labels": "are(O) there(O) any(O) restaurants(O) on(O) kilmarnock(B-Location) street(I-Location) that(O) feature(O) large(B-Amenity) portions(I-Amenity) and(O) a(O) brewpub(B-Amenity)"}}
{"id": "14", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Amenity", "Dish", "Rating", "Restaurant Name", "Price", "Location", "Hours"], "instance": {"id": "14", "words": ["any", "stores", "around", "where", "i", "could", "buy", "a", "pasta", "dish", "where", "the", "prices", "are", "not", "too", "high"], "labels": ["O", "O", "B-Location", "O", "O", "O", "O", "O", "B-Dish", "O", "O", "O", "B-Price", "I-Price", "I-Price", "I-Price", "I-Price"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Amenity, Dish, Rating, Restaurant Name, Price, Location, Hours and O.\nSentence: any stores around where i could buy a pasta dish where the prices are not too high", "prompt_labels": "any(O) stores(O) around(B-Location) where(O) i(O) could(O) buy(O) a(O) pasta(B-Dish) dish(O) where(O) the(O) prices(B-Price) are(I-Price) not(I-Price) too(I-Price) high(I-Price)"}}
{"id": "1519", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Price", "Hours", "Rating", "Dish", "Location", "Cuisine", "Restaurant Name"], "instance": {"id": "1519", "words": ["yes", "we", "need", "some", "chicken", "for", "our", "new", "diet", "so", "chik", "fa", "lay", "it", "is"], "labels": ["O", "O", "O", "O", "B-Dish", "O", "O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Price, Hours, Rating, Dish, Location, Cuisine, Restaurant Name and O.\nSentence: yes we need some chicken for our new diet so chik fa lay it is", "prompt_labels": "yes(O) we(O) need(O) some(O) chicken(B-Dish) for(O) our(O) new(O) diet(O) so(O) chik(B-Restaurant Name) fa(I-Restaurant Name) lay(I-Restaurant Name) it(O) is(O)"}}
{"id": "1162", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Hours", "Price", "Location", "Cuisine", "Dish", "Rating", "Restaurant Name"], "instance": {"id": "1162", "words": ["what", "local", "restaurants", "have", "an", "outdoor", "play", "area", "for", "young", "children"], "labels": ["O", "B-Location", "O", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Price, Location, Cuisine, Dish, Rating, Restaurant Name and O.\nSentence: what local restaurants have an outdoor play area for young children", "prompt_labels": "what(O) local(B-Location) restaurants(O) have(O) an(O) outdoor(B-Amenity) play(I-Amenity) area(I-Amenity) for(O) young(O) children(O)"}}
{"id": "754", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Price", "Dish", "Cuisine", "Amenity", "Location", "Restaurant Name", "Hours"], "instance": {"id": "754", "words": ["im", "starving", "so", "fast", "food", "will", "do"], "labels": ["O", "O", "O", "B-Cuisine", "I-Cuisine", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Price, Dish, Cuisine, Amenity, Location, Restaurant Name, Hours and O.\nSentence: im starving so fast food will do", "prompt_labels": "im(O) starving(O) so(O) fast(B-Cuisine) food(I-Cuisine) will(O) do(O)"}}
{"id": "42", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Rating", "Price", "Cuisine", "Hours", "Amenity", "Restaurant Name", "Location"], "instance": {"id": "42", "words": ["are", "there", "any", "fun", "restaurants", "serving", "brisket", "in", "town"], "labels": ["O", "O", "O", "B-Amenity", "O", "O", "B-Dish", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Rating, Price, Cuisine, Hours, Amenity, Restaurant Name, Location and O.\nSentence: are there any fun restaurants serving brisket in town", "prompt_labels": "are(O) there(O) any(O) fun(B-Amenity) restaurants(O) serving(O) brisket(B-Dish) in(B-Location) town(I-Location)"}}
{"id": "1010", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Cuisine", "Hours", "Amenity", "Rating", "Restaurant Name", "Location", "Dish"], "instance": {"id": "1010", "words": ["nearest", "fast", "food", "restaurant"], "labels": ["B-Location", "B-Cuisine", "I-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Cuisine, Hours, Amenity, Rating, Restaurant Name, Location, Dish and O.\nSentence: nearest fast food restaurant", "prompt_labels": "nearest(B-Location) fast(B-Cuisine) food(I-Cuisine) restaurant(O)"}}
{"id": "592", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Rating", "Price", "Dish", "Cuisine", "Restaurant Name", "Hours", "Amenity"], "instance": {"id": "592", "words": ["i", "need", "an", "italian", "restaurant", "with", "a", "kids", "menu"], "labels": ["O", "O", "O", "B-Cuisine", "O", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Rating, Price, Dish, Cuisine, Restaurant Name, Hours, Amenity and O.\nSentence: i need an italian restaurant with a kids menu", "prompt_labels": "i(O) need(O) an(O) italian(B-Cuisine) restaurant(O) with(O) a(O) kids(B-Amenity) menu(I-Amenity)"}}
{"id": "1511", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Hours", "Restaurant Name", "Cuisine", "Rating", "Dish", "Location", "Amenity"], "instance": {"id": "1511", "words": ["who", "makes", "the", "best", "chicken", "wings", "in", "town"], "labels": ["O", "O", "O", "B-Rating", "B-Dish", "I-Dish", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Hours, Restaurant Name, Cuisine, Rating, Dish, Location, Amenity and O.\nSentence: who makes the best chicken wings in town", "prompt_labels": "who(O) makes(O) the(O) best(B-Rating) chicken(B-Dish) wings(I-Dish) in(B-Location) town(I-Location)"}}
{"id": "455", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Rating", "Cuisine", "Hours", "Dish", "Restaurant Name", "Location", "Amenity"], "instance": {"id": "455", "words": ["find", "the", "number", "of", "the", "best", "barbecue", "joint", "in", "town"], "labels": ["O", "O", "O", "O", "O", "B-Rating", "B-Cuisine", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Rating, Cuisine, Hours, Dish, Restaurant Name, Location, Amenity and O.\nSentence: find the number of the best barbecue joint in town", "prompt_labels": "find(O) the(O) number(O) of(O) the(O) best(B-Rating) barbecue(B-Cuisine) joint(O) in(B-Location) town(I-Location)"}}
{"id": "402", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Dish", "Amenity", "Hours", "Location", "Cuisine", "Rating", "Price"], "instance": {"id": "402", "words": ["find", "me", "a", "romantic", "restaurant", "in", "7", "hills"], "labels": ["O", "O", "O", "B-Amenity", "B-Location", "I-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Dish, Amenity, Hours, Location, Cuisine, Rating, Price and O.\nSentence: find me a romantic restaurant in 7 hills", "prompt_labels": "find(O) me(O) a(O) romantic(B-Amenity) restaurant(B-Location) in(I-Location) 7(I-Location) hills(I-Location)"}}
{"id": "1035", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Hours", "Rating", "Dish", "Restaurant Name", "Price", "Cuisine", "Amenity"], "instance": {"id": "1035", "words": ["please", "make", "me", "a", "reservation", "for", "tonight", "at", "jax", "cafe", "at", "7", "pm", "for", "six"], "labels": ["O", "O", "O", "O", "B-Amenity", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "O", "B-Hours", "I-Hours", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Hours, Rating, Dish, Restaurant Name, Price, Cuisine, Amenity and O.\nSentence: please make me a reservation for tonight at jax cafe at 7 pm for six", "prompt_labels": "please(O) make(O) me(O) a(O) reservation(B-Amenity) for(O) tonight(O) at(O) jax(B-Restaurant Name) cafe(I-Restaurant Name) at(O) 7(B-Hours) pm(I-Hours) for(O) six(O)"}}
{"id": "1340", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Location", "Cuisine", "Hours", "Restaurant Name", "Amenity", "Rating", "Dish"], "instance": {"id": "1340", "words": ["where", "can", "i", "get", "some", "slushy"], "labels": ["O", "O", "O", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Location, Cuisine, Hours, Restaurant Name, Amenity, Rating, Dish and O.\nSentence: where can i get some slushy", "prompt_labels": "where(O) can(O) i(O) get(O) some(O) slushy(B-Dish)"}}
{"id": "855", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Location", "Rating", "Hours", "Dish", "Price", "Amenity", "Restaurant Name"], "instance": {"id": "855", "words": ["is", "there", "a", "reasonably", "priced", "restaurant", "that", "has", "perfect", "portion", "sizes"], "labels": ["O", "O", "O", "B-Price", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Location, Rating, Hours, Dish, Price, Amenity, Restaurant Name and O.\nSentence: is there a reasonably priced restaurant that has perfect portion sizes", "prompt_labels": "is(O) there(O) a(O) reasonably(B-Price) priced(O) restaurant(O) that(O) has(O) perfect(B-Amenity) portion(I-Amenity) sizes(I-Amenity)"}}
{"id": "48", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Dish", "Hours", "Amenity", "Price", "Restaurant Name", "Cuisine", "Location"], "instance": {"id": "48", "words": ["are", "there", "any", "hotels", "nearby", "that", "have", "private", "rooms", "available", "at", "1", "am"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-Hours", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Dish, Hours, Amenity, Price, Restaurant Name, Cuisine, Location and O.\nSentence: are there any hotels nearby that have private rooms available at 1 am", "prompt_labels": "are(O) there(O) any(O) hotels(O) nearby(O) that(O) have(O) private(O) rooms(O) available(O) at(O) 1(B-Hours) am(O)"}}
{"id": "114", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Cuisine", "Price", "Dish", "Rating", "Restaurant Name", "Hours", "Amenity"], "instance": {"id": "114", "words": ["call", "the", "closest", "korean", "restaurant"], "labels": ["O", "O", "B-Location", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Cuisine, Price, Dish, Rating, Restaurant Name, Hours, Amenity and O.\nSentence: call the closest korean restaurant", "prompt_labels": "call(O) the(O) closest(B-Location) korean(B-Cuisine) restaurant(O)"}}
{"id": "450", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Location", "Amenity", "Rating", "Dish", "Hours", "Cuisine", "Restaurant Name"], "instance": {"id": "450", "words": ["find", "the", "closest", "dunkin", "donuts"], "labels": ["O", "O", "B-Location", "B-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Location, Amenity, Rating, Dish, Hours, Cuisine, Restaurant Name and O.\nSentence: find the closest dunkin donuts", "prompt_labels": "find(O) the(O) closest(B-Location) dunkin(B-Restaurant Name) donuts(I-Restaurant Name)"}}
{"id": "242", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Hours", "Amenity", "Dish", "Cuisine", "Restaurant Name", "Location", "Price"], "instance": {"id": "242", "words": ["do", "any", "restaurants", "serve", "loaves", "of", "patitza", "around", "christmas", "time"], "labels": ["O", "O", "O", "O", "B-Dish", "I-Dish", "I-Dish", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Hours, Amenity, Dish, Cuisine, Restaurant Name, Location, Price and O.\nSentence: do any restaurants serve loaves of patitza around christmas time", "prompt_labels": "do(O) any(O) restaurants(O) serve(O) loaves(B-Dish) of(I-Dish) patitza(I-Dish) around(O) christmas(O) time(O)"}}
{"id": "598", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Dish", "Price", "Location", "Amenity", "Cuisine", "Hours", "Restaurant Name"], "instance": {"id": "598", "words": ["i", "need", "gluten", "free", "options", "on", "the", "restauarant", "menu"], "labels": ["O", "O", "B-Amenity", "I-Amenity", "I-Amenity", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Dish, Price, Location, Amenity, Cuisine, Hours, Restaurant Name and O.\nSentence: i need gluten free options on the restauarant menu", "prompt_labels": "i(O) need(O) gluten(B-Amenity) free(I-Amenity) options(I-Amenity) on(O) the(O) restauarant(O) menu(O)"}}
{"id": "1189", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Dish", "Amenity", "Restaurant Name", "Location", "Cuisine", "Hours", "Rating"], "instance": {"id": "1189", "words": ["what", "restaurants", "are", "open", "in", "the", "downtown", "area"], "labels": ["O", "O", "O", "B-Hours", "B-Location", "I-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Dish, Amenity, Restaurant Name, Location, Cuisine, Hours, Rating and O.\nSentence: what restaurants are open in the downtown area", "prompt_labels": "what(O) restaurants(O) are(O) open(B-Hours) in(B-Location) the(I-Location) downtown(I-Location) area(I-Location)"}}
{"id": "393", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Price", "Hours", "Restaurant Name", "Amenity", "Location", "Rating", "Cuisine"], "instance": {"id": "393", "words": ["find", "me", "a", "restaurant", "near", "the", "stadium"], "labels": ["O", "O", "O", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Price, Hours, Restaurant Name, Amenity, Location, Rating, Cuisine and O.\nSentence: find me a restaurant near the stadium", "prompt_labels": "find(O) me(O) a(O) restaurant(O) near(B-Location) the(I-Location) stadium(I-Location)"}}
{"id": "94", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Price", "Rating", "Dish", "Location", "Hours", "Restaurant Name", "Amenity"], "instance": {"id": "94", "words": ["are", "there", "any", "turkish", "restaurants", "in", "florida"], "labels": ["O", "O", "O", "B-Cuisine", "O", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Rating, Dish, Location, Hours, Restaurant Name, Amenity and O.\nSentence: are there any turkish restaurants in florida", "prompt_labels": "are(O) there(O) any(O) turkish(B-Cuisine) restaurants(O) in(O) florida(B-Location)"}}
{"id": "1497", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Restaurant Name", "Location", "Price", "Amenity", "Cuisine", "Dish", "Hours"], "instance": {"id": "1497", "words": ["which", "restaurants", "around", "my", "city", "have", "the", "best", "deserts"], "labels": ["O", "O", "B-Location", "I-Location", "I-Location", "O", "O", "B-Rating", "B-Cuisine"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Location, Price, Amenity, Cuisine, Dish, Hours and O.\nSentence: which restaurants around my city have the best deserts", "prompt_labels": "which(O) restaurants(O) around(B-Location) my(I-Location) city(I-Location) have(O) the(O) best(B-Rating) deserts(B-Cuisine)"}}
{"id": "228", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Location", "Cuisine", "Restaurant Name", "Price", "Amenity", "Hours", "Rating"], "instance": {"id": "228", "words": ["dining", "along", "the", "waterfront", "restaurant", "is", "located", "on", "sutton", "street"], "labels": ["B-Cuisine", "B-Amenity", "I-Amenity", "B-Restaurant Name", "O", "O", "O", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Cuisine, Restaurant Name, Price, Amenity, Hours, Rating and O.\nSentence: dining along the waterfront restaurant is located on sutton street", "prompt_labels": "dining(B-Cuisine) along(B-Amenity) the(I-Amenity) waterfront(B-Restaurant Name) restaurant(O) is(O) located(O) on(O) sutton(B-Location) street(I-Location)"}}
{"id": "1398", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Rating", "Restaurant Name", "Amenity", "Price", "Dish", "Cuisine", "Hours"], "instance": {"id": "1398", "words": ["where", "is", "station", "donuts"], "labels": ["O", "O", "B-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Rating, Restaurant Name, Amenity, Price, Dish, Cuisine, Hours and O.\nSentence: where is station donuts", "prompt_labels": "where(O) is(O) station(B-Restaurant Name) donuts(I-Restaurant Name)"}}
{"id": "443", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Price", "Hours", "Dish", "Rating", "Restaurant Name", "Location", "Amenity"], "instance": {"id": "443", "words": ["find", "pizza", "places"], "labels": ["O", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Hours, Dish, Rating, Restaurant Name, Location, Amenity and O.\nSentence: find pizza places", "prompt_labels": "find(O) pizza(B-Cuisine) places(O)"}}
{"id": "148", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Location", "Hours", "Restaurant Name", "Price", "Cuisine", "Amenity", "Rating"], "instance": {"id": "148", "words": ["can", "you", "find", "an", "italian", "restaurant", "that", "serves", "brunch", "on", "sunday"], "labels": ["O", "O", "O", "O", "B-Cuisine", "O", "O", "O", "B-Hours", "O", "B-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Hours, Restaurant Name, Price, Cuisine, Amenity, Rating and O.\nSentence: can you find an italian restaurant that serves brunch on sunday", "prompt_labels": "can(O) you(O) find(O) an(O) italian(B-Cuisine) restaurant(O) that(O) serves(O) brunch(B-Hours) on(O) sunday(B-Hours)"}}
{"id": "314", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Rating", "Price", "Amenity", "Hours", "Dish", "Restaurant Name", "Location"], "instance": {"id": "314", "words": ["does", "the", "italian", "restaurant", "in", "the", "town", "center", "offer", "carry", "out"], "labels": ["O", "O", "B-Cuisine", "O", "B-Location", "I-Location", "I-Location", "I-Location", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Price, Amenity, Hours, Dish, Restaurant Name, Location and O.\nSentence: does the italian restaurant in the town center offer carry out", "prompt_labels": "does(O) the(O) italian(B-Cuisine) restaurant(O) in(B-Location) the(I-Location) town(I-Location) center(I-Location) offer(O) carry(B-Amenity) out(I-Amenity)"}}
{"id": "1326", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Rating", "Location", "Dish", "Hours", "Restaurant Name", "Price", "Amenity"], "instance": {"id": "1326", "words": ["where", "can", "i", "get", "doughnuts", "right", "now"], "labels": ["O", "O", "O", "O", "B-Dish", "O", "B-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Location, Dish, Hours, Restaurant Name, Price, Amenity and O.\nSentence: where can i get doughnuts right now", "prompt_labels": "where(O) can(O) i(O) get(O) doughnuts(B-Dish) right(O) now(B-Hours)"}}
{"id": "1346", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Amenity", "Location", "Cuisine", "Hours", "Price", "Restaurant Name", "Dish"], "instance": {"id": "1346", "words": ["where", "can", "i", "get", "starbucks", "around", "me"], "labels": ["O", "O", "O", "O", "B-Restaurant Name", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Location, Cuisine, Hours, Price, Restaurant Name, Dish and O.\nSentence: where can i get starbucks around me", "prompt_labels": "where(O) can(O) i(O) get(O) starbucks(B-Restaurant Name) around(B-Location) me(I-Location)"}}
{"id": "959", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Location", "Rating", "Dish", "Hours", "Amenity", "Price", "Restaurant Name"], "instance": {"id": "959", "words": ["looking", "for", "a", "5", "star", "restaurant", "that", "serves", "a", "great", "steack"], "labels": ["O", "O", "O", "B-Rating", "I-Rating", "I-Rating", "O", "O", "O", "B-Rating", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Location, Rating, Dish, Hours, Amenity, Price, Restaurant Name and O.\nSentence: looking for a 5 star restaurant that serves a great steack", "prompt_labels": "looking(O) for(O) a(O) 5(B-Rating) star(I-Rating) restaurant(I-Rating) that(O) serves(O) a(O) great(B-Rating) steack(B-Dish)"}}
{"id": "106", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Amenity", "Hours", "Location", "Cuisine", "Dish", "Rating", "Price"], "instance": {"id": "106", "words": ["borscht"], "labels": ["B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Hours, Location, Cuisine, Dish, Rating, Price and O.\nSentence: borscht", "prompt_labels": "borscht(B-Dish)"}}
{"id": "1283", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Location", "Rating", "Hours", "Price", "Amenity", "Cuisine", "Restaurant Name"], "instance": {"id": "1283", "words": ["where", "can", "i", "find", "a", "place", "within", "5", "minutes", "with", "great", "prices"], "labels": ["O", "O", "O", "O", "O", "O", "B-Location", "I-Location", "I-Location", "O", "B-Price", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Rating, Hours, Price, Amenity, Cuisine, Restaurant Name and O.\nSentence: where can i find a place within 5 minutes with great prices", "prompt_labels": "where(O) can(O) i(O) find(O) a(O) place(O) within(B-Location) 5(I-Location) minutes(I-Location) with(O) great(B-Price) prices(O)"}}
{"id": "1313", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Amenity", "Location", "Cuisine", "Rating", "Dish", "Hours", "Restaurant Name"], "instance": {"id": "1313", "words": ["where", "can", "i", "get", "a", "hamburger"], "labels": ["O", "O", "O", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Location, Cuisine, Rating, Dish, Hours, Restaurant Name and O.\nSentence: where can i get a hamburger", "prompt_labels": "where(O) can(O) i(O) get(O) a(O) hamburger(B-Dish)"}}
{"id": "772", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Price", "Location", "Dish", "Restaurant Name", "Rating", "Hours", "Cuisine"], "instance": {"id": "772", "words": ["is", "outback", "having", "any", "specials", "today"], "labels": ["O", "B-Restaurant Name", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Price, Location, Dish, Restaurant Name, Rating, Hours, Cuisine and O.\nSentence: is outback having any specials today", "prompt_labels": "is(O) outback(B-Restaurant Name) having(O) any(O) specials(O) today(O)"}}
{"id": "1000", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Price", "Restaurant Name", "Hours", "Location", "Amenity", "Cuisine", "Dish"], "instance": {"id": "1000", "words": ["mexican", "food", "to", "go"], "labels": ["B-Cuisine", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Price, Restaurant Name, Hours, Location, Amenity, Cuisine, Dish and O.\nSentence: mexican food to go", "prompt_labels": "mexican(B-Cuisine) food(O) to(B-Amenity) go(I-Amenity)"}}
{"id": "489", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Rating", "Dish", "Location", "Amenity", "Restaurant Name", "Cuisine", "Price"], "instance": {"id": "489", "words": ["hi", "hershel", "is", "there", "any", "place", "around", "here", "with", "something", "good", "to", "eat"], "labels": ["O", "O", "O", "O", "O", "O", "B-Location", "I-Location", "O", "O", "B-Rating", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Rating, Dish, Location, Amenity, Restaurant Name, Cuisine, Price and O.\nSentence: hi hershel is there any place around here with something good to eat", "prompt_labels": "hi(O) hershel(O) is(O) there(O) any(O) place(O) around(B-Location) here(I-Location) with(O) something(O) good(B-Rating) to(O) eat(O)"}}
{"id": "1255", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Dish", "Price", "Hours", "Amenity", "Cuisine", "Location", "Rating"], "instance": {"id": "1255", "words": ["where", "can", "a", "group", "be", "served", "omelets"], "labels": ["O", "O", "O", "O", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Dish, Price, Hours, Amenity, Cuisine, Location, Rating and O.\nSentence: where can a group be served omelets", "prompt_labels": "where(O) can(O) a(O) group(O) be(O) served(O) omelets(B-Dish)"}}
{"id": "625", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Dish", "Hours", "Price", "Restaurant Name", "Rating", "Location", "Cuisine"], "instance": {"id": "625", "words": ["i", "want", "a", "place", "that", "allows", "smoking", "and", "serves", "health", "food"], "labels": ["O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "O", "B-Amenity", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Hours, Price, Restaurant Name, Rating, Location, Cuisine and O.\nSentence: i want a place that allows smoking and serves health food", "prompt_labels": "i(O) want(O) a(O) place(O) that(O) allows(B-Amenity) smoking(I-Amenity) and(O) serves(B-Amenity) health(B-Cuisine) food(O)"}}
{"id": "1188", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Dish", "Cuisine", "Location", "Rating", "Price", "Hours", "Restaurant Name"], "instance": {"id": "1188", "words": ["what", "restaurants", "are", "nearby"], "labels": ["O", "O", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Cuisine, Location, Rating, Price, Hours, Restaurant Name and O.\nSentence: what restaurants are nearby", "prompt_labels": "what(O) restaurants(O) are(O) nearby(B-Location)"}}
{"id": "119", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Dish", "Rating", "Location", "Cuisine", "Restaurant Name", "Price", "Hours"], "instance": {"id": "119", "words": ["can", "i", "find", "a", "good", "chinese", "buffet", "within", "3", "miles", "from", "me"], "labels": ["O", "O", "O", "O", "B-Rating", "B-Cuisine", "B-Amenity", "B-Location", "I-Location", "I-Location", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Rating, Location, Cuisine, Restaurant Name, Price, Hours and O.\nSentence: can i find a good chinese buffet within 3 miles from me", "prompt_labels": "can(O) i(O) find(O) a(O) good(B-Rating) chinese(B-Cuisine) buffet(B-Amenity) within(B-Location) 3(I-Location) miles(I-Location) from(O) me(O)"}}
{"id": "755", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Cuisine", "Dish", "Amenity", "Price", "Hours", "Location", "Restaurant Name"], "instance": {"id": "755", "words": ["im", "starving", "tell", "me", "where", "the", "closest", "mcdonalds", "is"], "labels": ["O", "O", "O", "O", "O", "O", "B-Location", "B-Restaurant Name", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Cuisine, Dish, Amenity, Price, Hours, Location, Restaurant Name and O.\nSentence: im starving tell me where the closest mcdonalds is", "prompt_labels": "im(O) starving(O) tell(O) me(O) where(O) the(O) closest(B-Location) mcdonalds(B-Restaurant Name) is(O)"}}
{"id": "213", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Hours", "Restaurant Name", "Price", "Amenity", "Cuisine", "Dish", "Rating"], "instance": {"id": "213", "words": ["cheap", "place", "close", "to", "home"], "labels": ["B-Price", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Hours, Restaurant Name, Price, Amenity, Cuisine, Dish, Rating and O.\nSentence: cheap place close to home", "prompt_labels": "cheap(B-Price) place(O) close(B-Location) to(I-Location) home(I-Location)"}}
{"id": "912", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Rating", "Cuisine", "Restaurant Name", "Location", "Amenity", "Price", "Dish"], "instance": {"id": "912", "words": ["is", "there", "any", "restaurant", "close", "by", "has", "live", "music"], "labels": ["O", "O", "O", "O", "B-Location", "I-Location", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Rating, Cuisine, Restaurant Name, Location, Amenity, Price, Dish and O.\nSentence: is there any restaurant close by has live music", "prompt_labels": "is(O) there(O) any(O) restaurant(O) close(B-Location) by(I-Location) has(O) live(B-Amenity) music(I-Amenity)"}}
{"id": "224", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Dish", "Price", "Cuisine", "Hours", "Rating", "Location", "Restaurant Name"], "instance": {"id": "224", "words": ["create", "directions", "to", "closest", "chinese", "restaurtant"], "labels": ["O", "O", "O", "B-Location", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Price, Cuisine, Hours, Rating, Location, Restaurant Name and O.\nSentence: create directions to closest chinese restaurtant", "prompt_labels": "create(O) directions(O) to(O) closest(B-Location) chinese(B-Cuisine) restaurtant(O)"}}
{"id": "144", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Price", "Amenity", "Restaurant Name", "Cuisine", "Rating", "Dish", "Location"], "instance": {"id": "144", "words": ["can", "you", "find", "a", "site", "where", "i", "can", "see", "reviews", "on", "restaurant", "downtown"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Price, Amenity, Restaurant Name, Cuisine, Rating, Dish, Location and O.\nSentence: can you find a site where i can see reviews on restaurant downtown", "prompt_labels": "can(O) you(O) find(O) a(O) site(O) where(O) i(O) can(O) see(O) reviews(O) on(O) restaurant(O) downtown(B-Location)"}}
{"id": "928", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Cuisine", "Location", "Restaurant Name", "Amenity", "Hours", "Rating", "Price"], "instance": {"id": "928", "words": ["is", "this", "restaurant", "a", "hidden", "find"], "labels": ["O", "O", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Cuisine, Location, Restaurant Name, Amenity, Hours, Rating, Price and O.\nSentence: is this restaurant a hidden find", "prompt_labels": "is(O) this(O) restaurant(O) a(O) hidden(B-Amenity) find(I-Amenity)"}}
{"id": "814", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Price", "Dish", "Amenity", "Cuisine", "Hours", "Rating", "Location"], "instance": {"id": "814", "words": ["is", "there", "a", "donut", "and", "donuts", "restaurant", "within", "5", "miles", "with", "a", "beer", "list"], "labels": ["O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "B-Location", "I-Location", "I-Location", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Price, Dish, Amenity, Cuisine, Hours, Rating, Location and O.\nSentence: is there a donut and donuts restaurant within 5 miles with a beer list", "prompt_labels": "is(O) there(O) a(O) donut(B-Restaurant Name) and(I-Restaurant Name) donuts(I-Restaurant Name) restaurant(I-Restaurant Name) within(B-Location) 5(I-Location) miles(I-Location) with(O) a(O) beer(B-Amenity) list(I-Amenity)"}}
{"id": "917", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Cuisine", "Amenity", "Price", "Dish", "Rating", "Location", "Restaurant Name"], "instance": {"id": "917", "words": ["is", "there", "anywhere", "in", "kendall", "square", "that", "has", "tea", "and", "delivers"], "labels": ["O", "O", "O", "O", "B-Location", "I-Location", "O", "O", "B-Dish", "O", "B-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Amenity, Price, Dish, Rating, Location, Restaurant Name and O.\nSentence: is there anywhere in kendall square that has tea and delivers", "prompt_labels": "is(O) there(O) anywhere(O) in(O) kendall(B-Location) square(I-Location) that(O) has(O) tea(B-Dish) and(O) delivers(B-Amenity)"}}
