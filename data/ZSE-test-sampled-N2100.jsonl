{"id":"9","dataset":"mit-movie","split":"test","instance":{"id":"9","prompt_labels":"what(O) is(O) the(O) most(O) current(B-year) movie(O) featuring(O) mat(B-actor) damon(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, actor, rating, plot, song, year, trailer, average ratings, genre, character, title, director and O.\nSentence: what is the most current movie featuring mat damon","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","most","current","movie","featuring","mat","damon"],"labels":["O","O","O","O","B-year","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["review","actor","rating","plot","song","year","trailer","average_ratings","genre","character","title","director"]}
{"id":"10","dataset":"mit-movie","split":"test","instance":{"id":"10","prompt_labels":"show(O) me(O) films(O) where(O) jim(B-actor) carrey(I-actor) is(O) a(O) detective(B-character)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, character, trailer, year, review, song, rating, title, director, actor, plot, genre and O.\nSentence: show me films where jim carrey is a detective","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","films","where","jim","carrey","is","a","detective"],"labels":["O","O","O","O","B-actor","I-actor","O","O","B-character"],"target_index":null,"target_label":null},"label_list":["average_ratings","character","trailer","year","review","song","rating","title","director","actor","plot","genre"]}
{"id":"27","dataset":"mit-movie","split":"test","instance":{"id":"27","prompt_labels":"list(O) the(O) action(B-genre) films(I-genre) starring(O) hugh(B-actor) jackman(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, trailer, character, review, rating, title, actor, average ratings, year, plot, song, genre and O.\nSentence: list the action films starring hugh jackman","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","the","action","films","starring","hugh","jackman"],"labels":["O","O","B-genre","I-genre","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["director","trailer","character","review","rating","title","actor","average_ratings","year","plot","song","genre"]}
{"id":"36","dataset":"mit-movie","split":"test","instance":{"id":"36","prompt_labels":"what(O) is(O) brad(B-actor) pitts(I-actor) first(O) movie(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, year, plot, character, director, genre, rating, average ratings, song, title, review, trailer and O.\nSentence: what is brad pitts first movie","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","brad","pitts","first","movie"],"labels":["O","O","B-actor","I-actor","O","O"],"target_index":null,"target_label":null},"label_list":["actor","year","plot","character","director","genre","rating","average_ratings","song","title","review","trailer"]}
{"id":"54","dataset":"mit-movie","split":"test","instance":{"id":"54","prompt_labels":"are(O) there(O) any(O) meg(B-actor) ryan(I-actor) romantic(B-genre) comedy(I-genre) movies(O) that(O) are(O) considered(O) must(B-review) see(I-review)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, title, character, actor, director, review, average ratings, year, genre, trailer, plot, rating and O.\nSentence: are there any meg ryan romantic comedy movies that are considered must see","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","meg","ryan","romantic","comedy","movies","that","are","considered","must","see"],"labels":["O","O","O","B-actor","I-actor","B-genre","I-genre","O","O","O","O","B-review","I-review"],"target_index":null,"target_label":null},"label_list":["song","title","character","actor","director","review","average_ratings","year","genre","trailer","plot","rating"]}
{"id":"59","dataset":"mit-movie","split":"test","instance":{"id":"59","prompt_labels":"what(O) was(O) the(O) most(B-review) popular(I-review) movie(I-review) from(O) 2004(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, genre, average ratings, character, review, song, director, actor, rating, plot, title, trailer and O.\nSentence: what was the most popular movie from 2004","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","most","popular","movie","from","2004"],"labels":["O","O","O","B-review","I-review","I-review","O","B-year"],"target_index":null,"target_label":null},"label_list":["year","genre","average_ratings","character","review","song","director","actor","rating","plot","title","trailer"]}
{"id":"62","dataset":"mit-movie","split":"test","instance":{"id":"62","prompt_labels":"how(O) many(O) movies(O) have(O) tim(B-actor) burton(I-actor) and(O) johnny(B-actor) depp(I-actor) done(O) together(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, actor, director, character, trailer, plot, average ratings, rating, year, genre, title, song and O.\nSentence: how many movies have tim burton and johnny depp done together","prediction_output":null,"prediction_outputs":null,"group":null,"words":["how","many","movies","have","tim","burton","and","johnny","depp","done","together"],"labels":["O","O","O","O","B-actor","I-actor","O","B-actor","I-actor","O","O"],"target_index":null,"target_label":null},"label_list":["review","actor","director","character","trailer","plot","average_ratings","rating","year","genre","title","song"]}
{"id":"64","dataset":"mit-movie","split":"test","instance":{"id":"64","prompt_labels":"whats(O) the(O) latetest(B-year) foreign(B-genre) romantic(I-genre) movie(O) with(O) lots(O) of(O) sex(B-plot) and(O) sadness(B-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, director, trailer, year, rating, average ratings, actor, review, plot, genre, title, song and O.\nSentence: whats the latetest foreign romantic movie with lots of sex and sadness","prediction_output":null,"prediction_outputs":null,"group":null,"words":["whats","the","latetest","foreign","romantic","movie","with","lots","of","sex","and","sadness"],"labels":["O","O","B-year","B-genre","I-genre","O","O","O","O","B-plot","O","B-plot"],"target_index":null,"target_label":null},"label_list":["character","director","trailer","year","rating","average_ratings","actor","review","plot","genre","title","song"]}
{"id":"75","dataset":"mit-movie","split":"test","instance":{"id":"75","prompt_labels":"which(O) movies(O) are(O) made(O) with(O) video(B-plot) game(I-plot) plots(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, character, rating, actor, review, year, trailer, song, director, plot, genre, average ratings and O.\nSentence: which movies are made with video game plots","prediction_output":null,"prediction_outputs":null,"group":null,"words":["which","movies","are","made","with","video","game","plots"],"labels":["O","O","O","O","O","B-plot","I-plot","O"],"target_index":null,"target_label":null},"label_list":["title","character","rating","actor","review","year","trailer","song","director","plot","genre","average_ratings"]}
{"id":"78","dataset":"mit-movie","split":"test","instance":{"id":"78","prompt_labels":"what(O) movie(O) has(O) the(O) most(B-review) remakes(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, title, year, song, director, character, review, trailer, average ratings, rating, genre, actor and O.\nSentence: what movie has the most remakes","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","movie","has","the","most","remakes"],"labels":["O","O","O","O","B-review","O"],"target_index":null,"target_label":null},"label_list":["plot","title","year","song","director","character","review","trailer","average_ratings","rating","genre","actor"]}
{"id":"81","dataset":"mit-movie","split":"test","instance":{"id":"81","prompt_labels":"i(O) would(O) like(O) a(O) list(O) of(O) movies(O) about(O) dancing(B-plot) from(O) the(O) past(B-year) 10(I-year) years(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, plot, character, actor, rating, average ratings, title, trailer, song, director, genre, year and O.\nSentence: i would like a list of movies about dancing from the past 10 years","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","would","like","a","list","of","movies","about","dancing","from","the","past","10","years"],"labels":["O","O","O","O","O","O","O","O","B-plot","O","O","B-year","I-year","I-year"],"target_index":null,"target_label":null},"label_list":["review","plot","character","actor","rating","average_ratings","title","trailer","song","director","genre","year"]}
{"id":"92","dataset":"mit-movie","split":"test","instance":{"id":"92","prompt_labels":"what(O) science(B-genre) fiction(I-genre) movies(O) were(O) directed(O) by(O) george(B-director) lucas(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, actor, review, year, title, rating, character, average ratings, genre, director, trailer, plot and O.\nSentence: what science fiction movies were directed by george lucas","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","science","fiction","movies","were","directed","by","george","lucas"],"labels":["O","B-genre","I-genre","O","O","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["song","actor","review","year","title","rating","character","average_ratings","genre","director","trailer","plot"]}
{"id":"95","dataset":"mit-movie","split":"test","instance":{"id":"95","prompt_labels":"name(O) a(O) western(B-genre) comedy(I-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, rating, trailer, genre, song, average ratings, director, plot, actor, year, title, character and O.\nSentence: name a western comedy","prediction_output":null,"prediction_outputs":null,"group":null,"words":["name","a","western","comedy"],"labels":["O","O","B-genre","I-genre"],"target_index":null,"target_label":null},"label_list":["review","rating","trailer","genre","song","average_ratings","director","plot","actor","year","title","character"]}
{"id":"112","dataset":"mit-movie","split":"test","instance":{"id":"112","prompt_labels":"did(O) simon(B-actor) pegg(I-actor) write(O) any(O) movies(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, average ratings, song, director, rating, title, genre, actor, plot, trailer, year, character and O.\nSentence: did simon pegg write any movies","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","simon","pegg","write","any","movies"],"labels":["O","B-actor","I-actor","O","O","O"],"target_index":null,"target_label":null},"label_list":["review","average_ratings","song","director","rating","title","genre","actor","plot","trailer","year","character"]}
{"id":"114","dataset":"mit-movie","split":"test","instance":{"id":"114","prompt_labels":"what(O) movie(O) won(O) the(O) most(B-average ratings) awards(I-average ratings) in(O) 2005(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, average ratings, director, song, plot, character, title, review, actor, rating, trailer, year and O.\nSentence: what movie won the most awards in 2005","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","movie","won","the","most","awards","in","2005"],"labels":["O","O","O","O","B-average ratings","I-average ratings","O","B-year"],"target_index":null,"target_label":null},"label_list":["genre","average_ratings","director","song","plot","character","title","review","actor","rating","trailer","year"]}
{"id":"126","dataset":"mit-movie","split":"test","instance":{"id":"126","prompt_labels":"show(O) me(O) an(O) alfred(B-director) hitchcock(I-director) movie(O) about(O) trains(B-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, average ratings, character, trailer, year, actor, rating, director, genre, review, song, title and O.\nSentence: show me an alfred hitchcock movie about trains","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","an","alfred","hitchcock","movie","about","trains"],"labels":["O","O","O","B-director","I-director","O","O","B-plot"],"target_index":null,"target_label":null},"label_list":["plot","average_ratings","character","trailer","year","actor","rating","director","genre","review","song","title"]}
{"id":"144","dataset":"mit-movie","split":"test","instance":{"id":"144","prompt_labels":"where(O) can(O) i(O) watch(O) a(O) preview(B-trailer) of(O) moneyball(B-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, title, actor, song, trailer, rating, director, character, year, genre, plot, review and O.\nSentence: where can i watch a preview of moneyball","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","watch","a","preview","of","moneyball"],"labels":["O","O","O","O","O","B-trailer","O","B-title"],"target_index":null,"target_label":null},"label_list":["average_ratings","title","actor","song","trailer","rating","director","character","year","genre","plot","review"]}
{"id":"156","dataset":"mit-movie","split":"test","instance":{"id":"156","prompt_labels":"who(O) directed(B-director) 310(B-title) to(I-title) yuma(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, average ratings, genre, trailer, year, review, title, plot, director, character, rating, actor and O.\nSentence: who directed 310 to yuma","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","directed","310","to","yuma"],"labels":["O","B-director","B-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["song","average_ratings","genre","trailer","year","review","title","plot","director","character","rating","actor"]}
{"id":"159","dataset":"mit-movie","split":"test","instance":{"id":"159","prompt_labels":"did(O) the(O) mpaa(O) give(O) doubt(B-title) a(O) pg(B-rating) 13(I-rating) rating(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, character, year, song, average ratings, rating, genre, plot, director, review, actor, title and O.\nSentence: did the mpaa give doubt a pg 13 rating","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","the","mpaa","give","doubt","a","pg","13","rating"],"labels":["O","O","O","O","B-title","O","B-rating","I-rating","O"],"target_index":null,"target_label":null},"label_list":["trailer","character","year","song","average_ratings","rating","genre","plot","director","review","actor","title"]}
{"id":"160","dataset":"mit-movie","split":"test","instance":{"id":"160","prompt_labels":"did(O) vin(B-actor) diesel(I-actor) star(O) in(O) any(O) comdedies(B-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, director, review, year, genre, trailer, song, rating, average ratings, actor, character, title and O.\nSentence: did vin diesel star in any comdedies","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","vin","diesel","star","in","any","comdedies"],"labels":["O","B-actor","I-actor","O","O","O","B-genre"],"target_index":null,"target_label":null},"label_list":["plot","director","review","year","genre","trailer","song","rating","average_ratings","actor","character","title"]}
{"id":"164","dataset":"mit-movie","split":"test","instance":{"id":"164","prompt_labels":"find(O) me(O) a(O) movie(O) with(O) the(O) song(O) lets(B-song) hear(I-song) it(I-song) for(I-song) the(I-song) boys(I-song)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, actor, trailer, year, plot, rating, genre, director, title, character, average ratings, song and O.\nSentence: find me a movie with the song lets hear it for the boys","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","movie","with","the","song","lets","hear","it","for","the","boys"],"labels":["O","O","O","O","O","O","O","B-song","I-song","I-song","I-song","I-song","I-song"],"target_index":null,"target_label":null},"label_list":["review","actor","trailer","year","plot","rating","genre","director","title","character","average_ratings","song"]}
{"id":"172","dataset":"mit-movie","split":"test","instance":{"id":"172","prompt_labels":"kung(B-title) fu(I-title) panda(I-title) 2(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, director, actor, song, genre, trailer, year, plot, character, title, average ratings, rating and O.\nSentence: kung fu panda 2","prediction_output":null,"prediction_outputs":null,"group":null,"words":["kung","fu","panda","2"],"labels":["B-title","I-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["review","director","actor","song","genre","trailer","year","plot","character","title","average_ratings","rating"]}
{"id":"174","dataset":"mit-movie","split":"test","instance":{"id":"174","prompt_labels":"is(O) there(O) a(O) comedy(B-genre) crime(I-genre) drama(I-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, trailer, year, average ratings, song, director, review, actor, genre, title, plot, character and O.\nSentence: is there a comedy crime drama","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","comedy","crime","drama"],"labels":["O","O","O","B-genre","I-genre","I-genre"],"target_index":null,"target_label":null},"label_list":["rating","trailer","year","average_ratings","song","director","review","actor","genre","title","plot","character"]}
{"id":"183","dataset":"mit-movie","split":"test","instance":{"id":"183","prompt_labels":"what(O) was(O) a(O) love(B-plot) story(I-plot) about(I-plot) a(I-plot) woman(I-plot) who(I-plot) had(I-plot) alzheimers(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, director, genre, review, song, trailer, rating, character, average ratings, title, plot, actor and O.\nSentence: what was a love story about a woman who had alzheimers","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","a","love","story","about","a","woman","who","had","alzheimers"],"labels":["O","O","O","B-plot","I-plot","I-plot","I-plot","I-plot","I-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["year","director","genre","review","song","trailer","rating","character","average_ratings","title","plot","actor"]}
{"id":"188","dataset":"mit-movie","split":"test","instance":{"id":"188","prompt_labels":"show(O) me(O) dramas(B-genre) about(O) the(O) british(B-plot) royal(I-plot) family(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, director, genre, character, song, average ratings, actor, year, rating, review, trailer, title and O.\nSentence: show me dramas about the british royal family","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","dramas","about","the","british","royal","family"],"labels":["O","O","B-genre","O","O","B-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["plot","director","genre","character","song","average_ratings","actor","year","rating","review","trailer","title"]}
{"id":"192","dataset":"mit-movie","split":"test","instance":{"id":"192","prompt_labels":"what(O) was(O) the(O) title(O) of(O) the(O) bio(O) pic(O) about(O) robert(B-actor) e(I-actor) howard(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, plot, review, year, character, actor, trailer, genre, director, average ratings, song, title and O.\nSentence: what was the title of the bio pic about robert e howard","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","title","of","the","bio","pic","about","robert","e","howard"],"labels":["O","O","O","O","O","O","O","O","O","B-actor","I-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["rating","plot","review","year","character","actor","trailer","genre","director","average_ratings","song","title"]}
{"id":"204","dataset":"mit-movie","split":"test","instance":{"id":"204","prompt_labels":"did(O) people(O) like(B-review) or(O) hate(B-review) the(O) last(O) twilight(B-title) movie(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, character, year, genre, average ratings, rating, plot, title, actor, director, trailer and O.\nSentence: did people like or hate the last twilight movie","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","people","like","or","hate","the","last","twilight","movie"],"labels":["O","O","B-review","O","B-review","O","O","B-title","O"],"target_index":null,"target_label":null},"label_list":["review","song","character","year","genre","average_ratings","rating","plot","title","actor","director","trailer"]}
{"id":"211","dataset":"mit-movie","split":"test","instance":{"id":"211","prompt_labels":"show(O) me(O) the(O) latest(O) trailer(B-trailer) for(O) the(B-title) avengers(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, rating, director, title, genre, plot, actor, song, review, character, trailer, year and O.\nSentence: show me the latest trailer for the avengers","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","the","latest","trailer","for","the","avengers"],"labels":["O","O","O","O","B-trailer","O","B-title","I-title"],"target_index":null,"target_label":null},"label_list":["average_ratings","rating","director","title","genre","plot","actor","song","review","character","trailer","year"]}
{"id":"212","dataset":"mit-movie","split":"test","instance":{"id":"212","prompt_labels":"who(O) directed(O) beaty(B-title) and(I-title) the(I-title) beast(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, character, plot, actor, rating, trailer, director, average ratings, genre, song, review, title and O.\nSentence: who directed beaty and the beast","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","directed","beaty","and","the","beast"],"labels":["O","O","B-title","I-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["year","character","plot","actor","rating","trailer","director","average_ratings","genre","song","review","title"]}
{"id":"214","dataset":"mit-movie","split":"test","instance":{"id":"214","prompt_labels":"how(O) many(O) movies(O) are(O) in(O) the(O) sex(B-title) and(I-title) the(I-title) city(I-title) series(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, trailer, title, genre, review, director, year, actor, character, average ratings, rating, song and O.\nSentence: how many movies are in the sex and the city series","prediction_output":null,"prediction_outputs":null,"group":null,"words":["how","many","movies","are","in","the","sex","and","the","city","series"],"labels":["O","O","O","O","O","O","B-title","I-title","I-title","I-title","O"],"target_index":null,"target_label":null},"label_list":["plot","trailer","title","genre","review","director","year","actor","character","average_ratings","rating","song"]}
{"id":"227","dataset":"mit-movie","split":"test","instance":{"id":"227","prompt_labels":"find(O) a(O) film(O) with(O) animated(B-plot) skeletons(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, actor, song, review, average ratings, title, trailer, year, character, rating, genre, plot and O.\nSentence: find a film with animated skeletons","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","a","film","with","animated","skeletons"],"labels":["O","O","O","O","B-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["director","actor","song","review","average_ratings","title","trailer","year","character","rating","genre","plot"]}
{"id":"228","dataset":"mit-movie","split":"test","instance":{"id":"228","prompt_labels":"what(O) animated(B-genre) voices(O) has(O) eddie(B-actor) murphy(I-actor) done(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, genre, character, year, director, title, review, song, average ratings, rating, trailer, actor and O.\nSentence: what animated voices has eddie murphy done","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","animated","voices","has","eddie","murphy","done"],"labels":["O","B-genre","O","O","B-actor","I-actor","O"],"target_index":null,"target_label":null},"label_list":["plot","genre","character","year","director","title","review","song","average_ratings","rating","trailer","actor"]}
{"id":"230","dataset":"mit-movie","split":"test","instance":{"id":"230","prompt_labels":"show(O) me(O) the(O) trailer(B-trailer) for(O) rage(B-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, song, title, trailer, actor, average ratings, character, rating, year, plot, director, review and O.\nSentence: show me the trailer for rage","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","the","trailer","for","rage"],"labels":["O","O","O","B-trailer","O","B-title"],"target_index":null,"target_label":null},"label_list":["genre","song","title","trailer","actor","average_ratings","character","rating","year","plot","director","review"]}
{"id":"233","dataset":"mit-movie","split":"test","instance":{"id":"233","prompt_labels":"who(O) directed(O) terminator(B-title) 2(I-title) judgement(I-title) day(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, genre, trailer, character, actor, review, year, rating, plot, director, title, average ratings and O.\nSentence: who directed terminator 2 judgement day","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","directed","terminator","2","judgement","day"],"labels":["O","O","B-title","I-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["song","genre","trailer","character","actor","review","year","rating","plot","director","title","average_ratings"]}
{"id":"251","dataset":"mit-movie","split":"test","instance":{"id":"251","prompt_labels":"show(O) me(O) a(O) wes(B-actor) craven(I-actor) movie(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, song, review, actor, director, trailer, average ratings, character, genre, rating, title, plot and O.\nSentence: show me a wes craven movie","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","a","wes","craven","movie"],"labels":["O","O","O","B-actor","I-actor","O"],"target_index":null,"target_label":null},"label_list":["year","song","review","actor","director","trailer","average_ratings","character","genre","rating","title","plot"]}
{"id":"257","dataset":"mit-movie","split":"test","instance":{"id":"257","prompt_labels":"what(O) films(O) did(O) pearce(B-actor) brosnan(I-actor) star(O) in(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, song, rating, actor, director, average ratings, plot, genre, trailer, year, review, character and O.\nSentence: what films did pearce brosnan star in","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","films","did","pearce","brosnan","star","in"],"labels":["O","O","O","B-actor","I-actor","O","O"],"target_index":null,"target_label":null},"label_list":["title","song","rating","actor","director","average_ratings","plot","genre","trailer","year","review","character"]}
{"id":"260","dataset":"mit-movie","split":"test","instance":{"id":"260","prompt_labels":"who(O) starred(O) in(O) good(B-title) burger(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, director, character, genre, rating, title, average ratings, year, trailer, actor, plot and O.\nSentence: who starred in good burger","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","starred","in","good","burger"],"labels":["O","O","O","B-title","I-title"],"target_index":null,"target_label":null},"label_list":["review","song","director","character","genre","rating","title","average_ratings","year","trailer","actor","plot"]}
{"id":"269","dataset":"mit-movie","split":"test","instance":{"id":"269","prompt_labels":"find(O) me(O) the(O) number(O) of(O) samurai(B-plot) films(O) made(O) in(O) the(O) 1960s(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, year, actor, trailer, director, song, genre, average ratings, character, title, review, plot and O.\nSentence: find me the number of samurai films made in the 1960s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","the","number","of","samurai","films","made","in","the","1960s"],"labels":["O","O","O","O","O","B-plot","O","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["rating","year","actor","trailer","director","song","genre","average_ratings","character","title","review","plot"]}
{"id":"291","dataset":"mit-movie","split":"test","instance":{"id":"291","prompt_labels":"find(O) a(O) trailer(B-trailer) for(O) space(B-title) 2010(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, rating, song, director, title, genre, trailer, actor, plot, review, character, year and O.\nSentence: find a trailer for space 2010","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","a","trailer","for","space","2010"],"labels":["O","O","B-trailer","O","B-title","I-title"],"target_index":null,"target_label":null},"label_list":["average_ratings","rating","song","director","title","genre","trailer","actor","plot","review","character","year"]}
{"id":"314","dataset":"mit-movie","split":"test","instance":{"id":"314","prompt_labels":"did(O) gregg(B-actor) kinnear(I-actor) star(O) in(O) any(O) movies(O) directed(O) my(O) michael(B-director) mann(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, year, trailer, title, average ratings, plot, genre, rating, review, actor, character, director and O.\nSentence: did gregg kinnear star in any movies directed my michael mann","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","gregg","kinnear","star","in","any","movies","directed","my","michael","mann"],"labels":["O","B-actor","I-actor","O","O","O","O","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["song","year","trailer","title","average_ratings","plot","genre","rating","review","actor","character","director"]}
{"id":"319","dataset":"mit-movie","split":"test","instance":{"id":"319","prompt_labels":"was(O) the(O) rock(B-actor) in(O) drive(B-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, genre, song, year, review, director, title, actor, average ratings, plot, rating, character and O.\nSentence: was the rock in drive","prediction_output":null,"prediction_outputs":null,"group":null,"words":["was","the","rock","in","drive"],"labels":["O","O","B-actor","O","B-title"],"target_index":null,"target_label":null},"label_list":["trailer","genre","song","year","review","director","title","actor","average_ratings","plot","rating","character"]}
{"id":"325","dataset":"mit-movie","split":"test","instance":{"id":"325","prompt_labels":"what(O) is(O) a(O) horror(B-genre) film(O) released(O) in(O) 1981(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, actor, character, genre, song, director, rating, trailer, plot, review, average ratings, title and O.\nSentence: what is a horror film released in 1981","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","a","horror","film","released","in","1981"],"labels":["O","O","O","B-genre","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["year","actor","character","genre","song","director","rating","trailer","plot","review","average_ratings","title"]}
{"id":"326","dataset":"mit-movie","split":"test","instance":{"id":"326","prompt_labels":"what(O) did(O) michael(B-director) bay(I-director) direct(O) besides(O) transformers(B-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, title, director, review, year, trailer, rating, average ratings, plot, character, genre, song and O.\nSentence: what did michael bay direct besides transformers","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","did","michael","bay","direct","besides","transformers"],"labels":["O","O","B-director","I-director","O","O","B-title"],"target_index":null,"target_label":null},"label_list":["actor","title","director","review","year","trailer","rating","average_ratings","plot","character","genre","song"]}
{"id":"328","dataset":"mit-movie","split":"test","instance":{"id":"328","prompt_labels":"how(O) many(O) saw(B-title) films(O) are(O) there(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, trailer, year, character, title, rating, actor, song, average ratings, genre, director, review and O.\nSentence: how many saw films are there","prediction_output":null,"prediction_outputs":null,"group":null,"words":["how","many","saw","films","are","there"],"labels":["O","O","B-title","O","O","O"],"target_index":null,"target_label":null},"label_list":["plot","trailer","year","character","title","rating","actor","song","average_ratings","genre","director","review"]}
{"id":"339","dataset":"mit-movie","split":"test","instance":{"id":"339","prompt_labels":"whats(O) a(O) 1940s(B-year) bernard(O) herman(O) soundtrack(O) film(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, average ratings, trailer, title, actor, director, review, song, rating, year, genre, plot and O.\nSentence: whats a 1940s bernard herman soundtrack film","prediction_output":null,"prediction_outputs":null,"group":null,"words":["whats","a","1940s","bernard","herman","soundtrack","film"],"labels":["O","O","B-year","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["character","average_ratings","trailer","title","actor","director","review","song","rating","year","genre","plot"]}
{"id":"340","dataset":"mit-movie","split":"test","instance":{"id":"340","prompt_labels":"show(O) me(O) which(O) movie(O) won(O) the(O) academy(B-review) award(I-review) for(I-review) best(I-review) picture(I-review) in(O) 2009(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, plot, character, song, director, review, title, average ratings, trailer, genre, rating, year and O.\nSentence: show me which movie won the academy award for best picture in 2009","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","which","movie","won","the","academy","award","for","best","picture","in","2009"],"labels":["O","O","O","O","O","O","B-review","I-review","I-review","I-review","I-review","O","B-year"],"target_index":null,"target_label":null},"label_list":["actor","plot","character","song","director","review","title","average_ratings","trailer","genre","rating","year"]}
{"id":"376","dataset":"mit-movie","split":"test","instance":{"id":"376","prompt_labels":"list(O) uma(B-actor) thurman(I-actor) movies(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, average ratings, year, plot, song, actor, trailer, title, character, director, rating, review and O.\nSentence: list uma thurman movies","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","uma","thurman","movies"],"labels":["O","B-actor","I-actor","O"],"target_index":null,"target_label":null},"label_list":["genre","average_ratings","year","plot","song","actor","trailer","title","character","director","rating","review"]}
{"id":"384","dataset":"mit-movie","split":"test","instance":{"id":"384","prompt_labels":"who(O) was(O) the(O) cast(B-actor) of(O) terminator(B-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, average ratings, character, title, song, genre, rating, review, trailer, actor, year, plot and O.\nSentence: who was the cast of terminator","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","was","the","cast","of","terminator"],"labels":["O","O","O","B-actor","O","B-title"],"target_index":null,"target_label":null},"label_list":["director","average_ratings","character","title","song","genre","rating","review","trailer","actor","year","plot"]}
{"id":"386","dataset":"mit-movie","split":"test","instance":{"id":"386","prompt_labels":"can(O) you(O) find(O) me(O) a(O) list(O) of(O) romantic(B-genre) comedies(I-genre) from(O) 2008(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, actor, rating, trailer, year, genre, review, director, song, plot, average ratings, title and O.\nSentence: can you find me a list of romantic comedies from 2008","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","me","a","list","of","romantic","comedies","from","2008"],"labels":["O","O","O","O","O","O","O","B-genre","I-genre","O","B-year"],"target_index":null,"target_label":null},"label_list":["character","actor","rating","trailer","year","genre","review","director","song","plot","average_ratings","title"]}
{"id":"391","dataset":"mit-movie","split":"test","instance":{"id":"391","prompt_labels":"what(O) was(O) thirteen(B-title) candles(I-title) rated(B-rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, title, actor, plot, genre, character, year, song, average ratings, review, rating, trailer and O.\nSentence: what was thirteen candles rated","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","thirteen","candles","rated"],"labels":["O","O","B-title","I-title","B-rating"],"target_index":null,"target_label":null},"label_list":["director","title","actor","plot","genre","character","year","song","average_ratings","review","rating","trailer"]}
{"id":"393","dataset":"mit-movie","split":"test","instance":{"id":"393","prompt_labels":"who(O) stared(O) in(O) the(O) movie(O) the(B-title) bank(I-title) job(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, character, genre, rating, song, title, trailer, year, director, average ratings, actor, review and O.\nSentence: who stared in the movie the bank job","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","stared","in","the","movie","the","bank","job"],"labels":["O","O","O","O","O","B-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["plot","character","genre","rating","song","title","trailer","year","director","average_ratings","actor","review"]}
{"id":"395","dataset":"mit-movie","split":"test","instance":{"id":"395","prompt_labels":"who(O) directed(O) star(B-title) wars(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, genre, year, plot, review, song, director, character, average ratings, actor, trailer, rating and O.\nSentence: who directed star wars","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","directed","star","wars"],"labels":["O","O","B-title","I-title"],"target_index":null,"target_label":null},"label_list":["title","genre","year","plot","review","song","director","character","average_ratings","actor","trailer","rating"]}
{"id":"410","dataset":"mit-movie","split":"test","instance":{"id":"410","prompt_labels":"whats(O) a(O) film(O) based(O) on(O) a(O) best(B-plot) selling(I-plot) contemporary(I-plot) novel(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, character, song, rating, review, actor, director, average ratings, plot, year, title, genre and O.\nSentence: whats a film based on a best selling contemporary novel","prediction_output":null,"prediction_outputs":null,"group":null,"words":["whats","a","film","based","on","a","best","selling","contemporary","novel"],"labels":["O","O","O","O","O","O","B-plot","I-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["trailer","character","song","rating","review","actor","director","average_ratings","plot","year","title","genre"]}
{"id":"414","dataset":"mit-movie","split":"test","instance":{"id":"414","prompt_labels":"what(O) movie(O) did(O) alen(B-song) menken(I-song) score(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, genre, trailer, director, actor, average ratings, year, character, plot, title, rating and O.\nSentence: what movie did alen menken score","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","movie","did","alen","menken","score"],"labels":["O","O","O","B-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["review","song","genre","trailer","director","actor","average_ratings","year","character","plot","title","rating"]}
{"id":"435","dataset":"mit-movie","split":"test","instance":{"id":"435","prompt_labels":"show(O) me(O) action(O) movies(O) with(O) vin(B-actor) diesel(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, review, plot, character, rating, year, genre, average ratings, title, actor, trailer, song and O.\nSentence: show me action movies with vin diesel","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","action","movies","with","vin","diesel"],"labels":["O","O","O","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["director","review","plot","character","rating","year","genre","average_ratings","title","actor","trailer","song"]}
{"id":"442","dataset":"mit-movie","split":"test","instance":{"id":"442","prompt_labels":"find(O) an(O) action(O) flick(O) with(O) martin(B-actor) lawrence(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, character, review, average ratings, plot, title, year, song, trailer, actor, director, rating and O.\nSentence: find an action flick with martin lawrence","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","an","action","flick","with","martin","lawrence"],"labels":["O","O","O","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["genre","character","review","average_ratings","plot","title","year","song","trailer","actor","director","rating"]}
{"id":"445","dataset":"mit-movie","split":"test","instance":{"id":"445","prompt_labels":"show(O) movies(O) from(O) the(O) 1960s(B-year) directed(O) by(O) stanley(B-director) kubrick(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, year, plot, actor, song, character, director, genre, trailer, title, average ratings, rating and O.\nSentence: show movies from the 1960s directed by stanley kubrick","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","movies","from","the","1960s","directed","by","stanley","kubrick"],"labels":["O","O","O","O","B-year","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["review","year","plot","actor","song","character","director","genre","trailer","title","average_ratings","rating"]}
{"id":"452","dataset":"mit-movie","split":"test","instance":{"id":"452","prompt_labels":"are(O) there(O) any(O) horror(B-genre) movies(O) with(O) a(O) g(B-rating) rating(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, trailer, review, director, song, character, average ratings, actor, year, title, rating, genre and O.\nSentence: are there any horror movies with a g rating","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","horror","movies","with","a","g","rating"],"labels":["O","O","O","B-genre","O","O","O","B-rating","O"],"target_index":null,"target_label":null},"label_list":["plot","trailer","review","director","song","character","average_ratings","actor","year","title","rating","genre"]}
{"id":"457","dataset":"mit-movie","split":"test","instance":{"id":"457","prompt_labels":"show(O) me(O) all(O) of(O) the(O) films(O) directed(O) by(O) clint(B-director) eastwood(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, plot, review, character, title, song, genre, director, actor, rating, trailer, average ratings and O.\nSentence: show me all of the films directed by clint eastwood","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","all","of","the","films","directed","by","clint","eastwood"],"labels":["O","O","O","O","O","O","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["year","plot","review","character","title","song","genre","director","actor","rating","trailer","average_ratings"]}
{"id":"461","dataset":"mit-movie","split":"test","instance":{"id":"461","prompt_labels":"what(O) was(O) the(O) best(O) rated(O) comedy(O) of(O) 2000(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, character, average ratings, director, song, actor, title, trailer, rating, review, year, genre and O.\nSentence: what was the best rated comedy of 2000","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","best","rated","comedy","of","2000"],"labels":["O","O","O","O","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["plot","character","average_ratings","director","song","actor","title","trailer","rating","review","year","genre"]}
{"id":"471","dataset":"mit-movie","split":"test","instance":{"id":"471","prompt_labels":"can(O) you(O) find(O) the(O) soundtrack(B-song) of(O) the(O) austin(B-title) powers(I-title) movies(O) from(O) the(O) 1990s(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, trailer, average ratings, title, year, song, genre, director, rating, review, plot, character and O.\nSentence: can you find the soundtrack of the austin powers movies from the 1990s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","the","soundtrack","of","the","austin","powers","movies","from","the","1990s"],"labels":["O","O","O","O","B-song","O","O","B-title","I-title","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["actor","trailer","average_ratings","title","year","song","genre","director","rating","review","plot","character"]}
{"id":"479","dataset":"mit-movie","split":"test","instance":{"id":"479","prompt_labels":"list(O) all(O) films(O) based(O) on(O) the(O) wizard(B-title) of(I-title) oz(I-title) books(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, director, actor, plot, average ratings, character, genre, trailer, song, rating, title, year and O.\nSentence: list all films based on the wizard of oz books","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","all","films","based","on","the","wizard","of","oz","books"],"labels":["O","O","O","O","O","O","B-title","I-title","I-title","O"],"target_index":null,"target_label":null},"label_list":["review","director","actor","plot","average_ratings","character","genre","trailer","song","rating","title","year"]}
{"id":"485","dataset":"mit-movie","split":"test","instance":{"id":"485","prompt_labels":"what(O) was(O) the(O) title(O) song(O) for(O) kissing(B-title) cousins(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, director, title, genre, actor, trailer, average ratings, review, song, character, rating, plot and O.\nSentence: what was the title song for kissing cousins","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","title","song","for","kissing","cousins"],"labels":["O","O","O","O","O","O","B-title","I-title"],"target_index":null,"target_label":null},"label_list":["year","director","title","genre","actor","trailer","average_ratings","review","song","character","rating","plot"]}
{"id":"490","dataset":"mit-movie","split":"test","instance":{"id":"490","prompt_labels":"find(O) me(O) all(O) the(O) movies(O) with(O) jason(B-character) bourne(I-character)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, actor, song, review, plot, trailer, year, character, title, genre, director, average ratings and O.\nSentence: find me all the movies with jason bourne","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","all","the","movies","with","jason","bourne"],"labels":["O","O","O","O","O","O","B-character","I-character"],"target_index":null,"target_label":null},"label_list":["rating","actor","song","review","plot","trailer","year","character","title","genre","director","average_ratings"]}
{"id":"500","dataset":"mit-movie","split":"test","instance":{"id":"500","prompt_labels":"play(O) a(O) trailer(B-trailer) for(O) teen(B-title) wolf(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, trailer, character, director, review, plot, song, title, actor, genre, year, rating and O.\nSentence: play a trailer for teen wolf","prediction_output":null,"prediction_outputs":null,"group":null,"words":["play","a","trailer","for","teen","wolf"],"labels":["O","O","B-trailer","O","B-title","I-title"],"target_index":null,"target_label":null},"label_list":["average_ratings","trailer","character","director","review","plot","song","title","actor","genre","year","rating"]}
{"id":"504","dataset":"mit-movie","split":"test","instance":{"id":"504","prompt_labels":"what(O) pg(B-rating) 13(I-rating) movies(O) feature(O) goldie(B-actor) hawn(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, genre, actor, plot, song, review, trailer, rating, director, title, character, average ratings and O.\nSentence: what pg 13 movies feature goldie hawn","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","pg","13","movies","feature","goldie","hawn"],"labels":["O","B-rating","I-rating","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["year","genre","actor","plot","song","review","trailer","rating","director","title","character","average_ratings"]}
{"id":"505","dataset":"mit-movie","split":"test","instance":{"id":"505","prompt_labels":"are(O) there(O) any(O) movies(O) from(O) 1996(B-year) that(O) involve(O) aliens(B-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, title, character, song, year, actor, genre, trailer, director, review, rating, plot and O.\nSentence: are there any movies from 1996 that involve aliens","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","movies","from","1996","that","involve","aliens"],"labels":["O","O","O","O","O","B-year","O","O","B-plot"],"target_index":null,"target_label":null},"label_list":["average_ratings","title","character","song","year","actor","genre","trailer","director","review","rating","plot"]}
{"id":"519","dataset":"mit-movie","split":"test","instance":{"id":"519","prompt_labels":"what(O) was(O) the(O) last(O) terminator(B-title) film(O) to(O) be(O) released(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, review, character, average ratings, title, year, rating, director, plot, genre, actor, song and O.\nSentence: what was the last terminator film to be released","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","last","terminator","film","to","be","released"],"labels":["O","O","O","O","B-title","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["trailer","review","character","average_ratings","title","year","rating","director","plot","genre","actor","song"]}
{"id":"520","dataset":"mit-movie","split":"test","instance":{"id":"520","prompt_labels":"what(O) are(O) some(O) funny(O) arnold(B-actor) schwarzenegger(I-actor) comedies(B-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, title, song, actor, review, genre, year, average ratings, plot, rating, trailer, director and O.\nSentence: what are some funny arnold schwarzenegger comedies","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","some","funny","arnold","schwarzenegger","comedies"],"labels":["O","O","O","O","B-actor","I-actor","B-genre"],"target_index":null,"target_label":null},"label_list":["character","title","song","actor","review","genre","year","average_ratings","plot","rating","trailer","director"]}
{"id":"525","dataset":"mit-movie","split":"test","instance":{"id":"525","prompt_labels":"find(O) a(O) horror(B-genre) movie(O) with(O) creepy(B-plot) little(I-plot) girls(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, title, character, average ratings, genre, plot, actor, year, review, director, rating, song and O.\nSentence: find a horror movie with creepy little girls","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","a","horror","movie","with","creepy","little","girls"],"labels":["O","O","B-genre","O","O","B-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["trailer","title","character","average_ratings","genre","plot","actor","year","review","director","rating","song"]}
{"id":"534","dataset":"mit-movie","split":"test","instance":{"id":"534","prompt_labels":"was(O) there(O) a(O) supernatural(B-genre) film(I-genre) noir(I-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, average ratings, review, rating, trailer, plot, year, character, title, director, actor, song and O.\nSentence: was there a supernatural film noir","prediction_output":null,"prediction_outputs":null,"group":null,"words":["was","there","a","supernatural","film","noir"],"labels":["O","O","O","B-genre","I-genre","I-genre"],"target_index":null,"target_label":null},"label_list":["genre","average_ratings","review","rating","trailer","plot","year","character","title","director","actor","song"]}
{"id":"537","dataset":"mit-movie","split":"test","instance":{"id":"537","prompt_labels":"who(O) directed(O) saw(B-title) 1(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, trailer, song, director, review, title, genre, plot, year, actor, average ratings, character and O.\nSentence: who directed saw 1","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","directed","saw","1"],"labels":["O","O","B-title","I-title"],"target_index":null,"target_label":null},"label_list":["rating","trailer","song","director","review","title","genre","plot","year","actor","average_ratings","character"]}
{"id":"546","dataset":"mit-movie","split":"test","instance":{"id":"546","prompt_labels":"show(O) movies(O) about(O) food(B-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, year, actor, review, title, rating, average ratings, song, plot, trailer, director, genre and O.\nSentence: show movies about food","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","movies","about","food"],"labels":["O","O","O","B-plot"],"target_index":null,"target_label":null},"label_list":["character","year","actor","review","title","rating","average_ratings","song","plot","trailer","director","genre"]}
{"id":"556","dataset":"mit-movie","split":"test","instance":{"id":"556","prompt_labels":"was(O) there(O) a(O) documentary(B-genre) about(O) a(O) russian(B-plot) electronics(I-plot) professor(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, director, rating, average ratings, character, plot, year, genre, song, review, trailer, actor and O.\nSentence: was there a documentary about a russian electronics professor","prediction_output":null,"prediction_outputs":null,"group":null,"words":["was","there","a","documentary","about","a","russian","electronics","professor"],"labels":["O","O","O","B-genre","O","O","B-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["title","director","rating","average_ratings","character","plot","year","genre","song","review","trailer","actor"]}
{"id":"564","dataset":"mit-movie","split":"test","instance":{"id":"564","prompt_labels":"what(O) is(O) the(O) movie(O) step(B-title) up(I-title) rated(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, review, average ratings, song, trailer, actor, rating, plot, character, director, genre, year and O.\nSentence: what is the movie step up rated","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","movie","step","up","rated"],"labels":["O","O","O","O","B-title","I-title","O"],"target_index":null,"target_label":null},"label_list":["title","review","average_ratings","song","trailer","actor","rating","plot","character","director","genre","year"]}
{"id":"577","dataset":"mit-movie","split":"test","instance":{"id":"577","prompt_labels":"who(O) directed(O) twister(B-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, character, title, rating, year, actor, review, average ratings, genre, plot, trailer, director and O.\nSentence: who directed twister","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","directed","twister"],"labels":["O","O","B-title"],"target_index":null,"target_label":null},"label_list":["song","character","title","rating","year","actor","review","average_ratings","genre","plot","trailer","director"]}
{"id":"579","dataset":"mit-movie","split":"test","instance":{"id":"579","prompt_labels":"what(O) is(O) the(O) name(O) of(O) cameron(O) diazs(O) character(O) in(O) bad(B-title) teacher(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, genre, director, rating, review, year, plot, actor, average ratings, trailer, character, song and O.\nSentence: what is the name of cameron diazs character in bad teacher","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","name","of","cameron","diazs","character","in","bad","teacher"],"labels":["O","O","O","O","O","O","O","O","O","B-title","I-title"],"target_index":null,"target_label":null},"label_list":["title","genre","director","rating","review","year","plot","actor","average_ratings","trailer","character","song"]}
{"id":"589","dataset":"mit-movie","split":"test","instance":{"id":"589","prompt_labels":"which(O) films(O) have(O) sherlock(B-character) holmes(I-character) as(O) a(O) character(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, plot, actor, character, title, genre, rating, director, song, average ratings, year, review and O.\nSentence: which films have sherlock holmes as a character","prediction_output":null,"prediction_outputs":null,"group":null,"words":["which","films","have","sherlock","holmes","as","a","character"],"labels":["O","O","O","B-character","I-character","O","O","O"],"target_index":null,"target_label":null},"label_list":["trailer","plot","actor","character","title","genre","rating","director","song","average_ratings","year","review"]}
{"id":"600","dataset":"mit-movie","split":"test","instance":{"id":"600","prompt_labels":"name(O) a(O) film(O) with(O) brian(B-actor) entwhistle(I-actor) in(O) it(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, director, trailer, title, character, plot, song, rating, actor, average ratings, year, review and O.\nSentence: name a film with brian entwhistle in it","prediction_output":null,"prediction_outputs":null,"group":null,"words":["name","a","film","with","brian","entwhistle","in","it"],"labels":["O","O","O","O","B-actor","I-actor","O","O"],"target_index":null,"target_label":null},"label_list":["genre","director","trailer","title","character","plot","song","rating","actor","average_ratings","year","review"]}
{"id":"605","dataset":"mit-movie","split":"test","instance":{"id":"605","prompt_labels":"find(O) all(O) movies(O) directed(O) by(O) steven(B-director) speilburg(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, plot, song, trailer, rating, review, title, average ratings, year, actor, character, director and O.\nSentence: find all movies directed by steven speilburg","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","all","movies","directed","by","steven","speilburg"],"labels":["O","O","O","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["genre","plot","song","trailer","rating","review","title","average_ratings","year","actor","character","director"]}
{"id":"614","dataset":"mit-movie","split":"test","instance":{"id":"614","prompt_labels":"show(O) me(O) a(O) movie(O) starring(O) tim(B-actor) allen(I-actor) about(O) christmas(B-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, song, average ratings, plot, trailer, genre, rating, review, actor, year, title, character and O.\nSentence: show me a movie starring tim allen about christmas","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","a","movie","starring","tim","allen","about","christmas"],"labels":["O","O","O","O","O","B-actor","I-actor","O","B-plot"],"target_index":null,"target_label":null},"label_list":["director","song","average_ratings","plot","trailer","genre","rating","review","actor","year","title","character"]}
{"id":"642","dataset":"mit-movie","split":"test","instance":{"id":"642","prompt_labels":"find(O) me(O) a(O) movie(O) about(O) serial(B-plot) killers(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, average ratings, plot, genre, trailer, year, rating, director, actor, title, character, song and O.\nSentence: find me a movie about serial killers","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","movie","about","serial","killers"],"labels":["O","O","O","O","O","B-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["review","average_ratings","plot","genre","trailer","year","rating","director","actor","title","character","song"]}
{"id":"646","dataset":"mit-movie","split":"test","instance":{"id":"646","prompt_labels":"find(O) a(O) movie(O) with(O) corey(B-actor) haim(I-actor) and(O) corey(B-actor) feldman(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, character, trailer, year, plot, rating, title, average ratings, song, actor, review, director and O.\nSentence: find a movie with corey haim and corey feldman","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","a","movie","with","corey","haim","and","corey","feldman"],"labels":["O","O","O","O","B-actor","I-actor","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["genre","character","trailer","year","plot","rating","title","average_ratings","song","actor","review","director"]}
{"id":"670","dataset":"mit-movie","split":"test","instance":{"id":"670","prompt_labels":"find(O) an(O) action(B-genre) movie(O) from(O) the(O) 1990s(B-year) with(O) nicolas(B-actor) cage(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, plot, song, character, trailer, year, title, director, average ratings, actor, genre, rating and O.\nSentence: find an action movie from the 1990s with nicolas cage","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","an","action","movie","from","the","1990s","with","nicolas","cage"],"labels":["O","O","B-genre","O","O","O","B-year","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["review","plot","song","character","trailer","year","title","director","average_ratings","actor","genre","rating"]}
{"id":"679","dataset":"mit-movie","split":"test","instance":{"id":"679","prompt_labels":"what(O) are(O) the(O) top(B-review) 10(I-review) science(B-genre) fiction(I-genre) movies(O) of(B-review) all(I-review) time(I-review)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, trailer, year, rating, song, genre, director, review, character, average ratings, actor, title and O.\nSentence: what are the top 10 science fiction movies of all time","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","the","top","10","science","fiction","movies","of","all","time"],"labels":["O","O","O","B-review","I-review","B-genre","I-genre","O","B-review","I-review","I-review"],"target_index":null,"target_label":null},"label_list":["plot","trailer","year","rating","song","genre","director","review","character","average_ratings","actor","title"]}
{"id":"680","dataset":"mit-movie","split":"test","instance":{"id":"680","prompt_labels":"find(O) me(O) biopics(B-genre) starring(O) country(B-actor) music(I-actor) stars(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, genre, review, song, trailer, plot, average ratings, title, character, year, rating, director and O.\nSentence: find me biopics starring country music stars","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","biopics","starring","country","music","stars"],"labels":["O","O","B-genre","O","B-actor","I-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["actor","genre","review","song","trailer","plot","average_ratings","title","character","year","rating","director"]}
{"id":"696","dataset":"mit-movie","split":"test","instance":{"id":"696","prompt_labels":"show(O) me(O) all(O) the(O) police(B-title) academy(I-title) movies(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, character, year, director, title, average ratings, song, plot, review, actor, genre, trailer and O.\nSentence: show me all the police academy movies","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","all","the","police","academy","movies"],"labels":["O","O","O","O","B-title","I-title","O"],"target_index":null,"target_label":null},"label_list":["rating","character","year","director","title","average_ratings","song","plot","review","actor","genre","trailer"]}
{"id":"697","dataset":"mit-movie","split":"test","instance":{"id":"697","prompt_labels":"show(O) me(O) films(O) with(O) wyatt(B-character) earp(I-character) from(O) the(O) 1980s(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, average ratings, review, rating, plot, genre, character, director, title, song, actor, trailer and O.\nSentence: show me films with wyatt earp from the 1980s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","films","with","wyatt","earp","from","the","1980s"],"labels":["O","O","O","O","B-character","I-character","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["year","average_ratings","review","rating","plot","genre","character","director","title","song","actor","trailer"]}
{"id":"725","dataset":"mit-movie","split":"test","instance":{"id":"725","prompt_labels":"what(O) r(B-rating) rated(I-rating) movies(O) from(O) 2011(B-year) do(O) not(O) have(O) full(B-plot) frontal(I-plot) nudity(I-plot) in(O) them(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, average ratings, song, year, genre, actor, trailer, review, rating, title, character, director and O.\nSentence: what r rated movies from 2011 do not have full frontal nudity in them","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","r","rated","movies","from","2011","do","not","have","full","frontal","nudity","in","them"],"labels":["O","B-rating","I-rating","O","O","B-year","O","O","O","B-plot","I-plot","I-plot","O","O"],"target_index":null,"target_label":null},"label_list":["plot","average_ratings","song","year","genre","actor","trailer","review","rating","title","character","director"]}
{"id":"732","dataset":"mit-movie","split":"test","instance":{"id":"732","prompt_labels":"are(O) there(O) any(O) musicals(O) from(O) the(O) 1990s(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, year, average ratings, trailer, song, character, title, rating, actor, plot, review, director and O.\nSentence: are there any musicals from the 1990s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","musicals","from","the","1990s"],"labels":["O","O","O","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["genre","year","average_ratings","trailer","song","character","title","rating","actor","plot","review","director"]}
{"id":"734","dataset":"mit-movie","split":"test","instance":{"id":"734","prompt_labels":"show(O) me(O) a(O) movie(O) starring(O) meryl(B-actor) streep(I-actor) and(O) robert(B-actor) deniro(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, character, actor, review, year, director, genre, title, song, average ratings, plot, trailer and O.\nSentence: show me a movie starring meryl streep and robert deniro","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","a","movie","starring","meryl","streep","and","robert","deniro"],"labels":["O","O","O","O","O","B-actor","I-actor","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["rating","character","actor","review","year","director","genre","title","song","average_ratings","plot","trailer"]}
{"id":"738","dataset":"mit-movie","split":"test","instance":{"id":"738","prompt_labels":"who(O) created(O) lord(B-title) of(I-title) the(I-title) rings(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, average ratings, trailer, director, review, title, rating, actor, character, plot, song, genre and O.\nSentence: who created lord of the rings","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","created","lord","of","the","rings"],"labels":["O","O","B-title","I-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["year","average_ratings","trailer","director","review","title","rating","actor","character","plot","song","genre"]}
{"id":"743","dataset":"mit-movie","split":"test","instance":{"id":"743","prompt_labels":"find(O) movies(O) about(O) football(B-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, rating, genre, director, song, title, plot, review, year, trailer, character, actor and O.\nSentence: find movies about football","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","movies","about","football"],"labels":["O","O","O","B-plot"],"target_index":null,"target_label":null},"label_list":["average_ratings","rating","genre","director","song","title","plot","review","year","trailer","character","actor"]}
{"id":"744","dataset":"mit-movie","split":"test","instance":{"id":"744","prompt_labels":"what(O) is(O) mpaa(O) rating(O) for(O) the(B-title) rhino(I-title) brothers(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, year, plot, director, average ratings, rating, title, actor, genre, song, character, review and O.\nSentence: what is mpaa rating for the rhino brothers","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","mpaa","rating","for","the","rhino","brothers"],"labels":["O","O","O","O","O","B-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["trailer","year","plot","director","average_ratings","rating","title","actor","genre","song","character","review"]}
{"id":"759","dataset":"mit-movie","split":"test","instance":{"id":"759","prompt_labels":"show(O) me(O) a(O) movie(O) with(O) lots(O) of(O) sky(B-plot) diving(I-plot) in(O) it(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, genre, average ratings, rating, character, title, review, trailer, actor, director, year, song and O.\nSentence: show me a movie with lots of sky diving in it","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","a","movie","with","lots","of","sky","diving","in","it"],"labels":["O","O","O","O","O","O","O","B-plot","I-plot","O","O"],"target_index":null,"target_label":null},"label_list":["plot","genre","average_ratings","rating","character","title","review","trailer","actor","director","year","song"]}
{"id":"761","dataset":"mit-movie","split":"test","instance":{"id":"761","prompt_labels":"what(O) animated(B-genre) movies(O) were(O) nominated(B-average ratings) for(I-average ratings) oscars(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, year, average ratings, plot, rating, song, genre, director, title, review, character, trailer and O.\nSentence: what animated movies were nominated for oscars","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","animated","movies","were","nominated","for","oscars"],"labels":["O","B-genre","O","O","B-average ratings","I-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["actor","year","average_ratings","plot","rating","song","genre","director","title","review","character","trailer"]}
{"id":"764","dataset":"mit-movie","split":"test","instance":{"id":"764","prompt_labels":"show(O) me(O) the(O) french(B-genre) film(I-genre) about(B-plot) a(I-plot) postman(I-plot) who(I-plot) is(I-plot) obsessed(I-plot) with(I-plot) an(I-plot) opera(I-plot) star(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, average ratings, character, actor, song, genre, trailer, title, director, year, review, rating and O.\nSentence: show me the french film about a postman who is obsessed with an opera star","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","the","french","film","about","a","postman","who","is","obsessed","with","an","opera","star"],"labels":["O","O","O","B-genre","I-genre","B-plot","I-plot","I-plot","I-plot","I-plot","I-plot","I-plot","I-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["plot","average_ratings","character","actor","song","genre","trailer","title","director","year","review","rating"]}
{"id":"780","dataset":"mit-movie","split":"test","instance":{"id":"780","prompt_labels":"list(O) the(O) romance(B-genre) films(O) directed(O) by(O) james(B-actor) cameron(I-actor) rated(O) must(B-average ratings) see(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, review, genre, song, rating, character, title, director, year, actor, average ratings, plot and O.\nSentence: list the romance films directed by james cameron rated must see","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","the","romance","films","directed","by","james","cameron","rated","must","see"],"labels":["O","O","B-genre","O","O","O","B-actor","I-actor","O","B-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["trailer","review","genre","song","rating","character","title","director","year","actor","average_ratings","plot"]}
{"id":"783","dataset":"mit-movie","split":"test","instance":{"id":"783","prompt_labels":"whats(O) the(O) movie(O) with(O) the(O) line(O) doomed(O) is(O) your(O) soul(O) and(O) damned(O) is(O) your(O) life(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, trailer, genre, review, actor, year, average ratings, title, plot, character, rating, director and O.\nSentence: whats the movie with the line doomed is your soul and damned is your life","prediction_output":null,"prediction_outputs":null,"group":null,"words":["whats","the","movie","with","the","line","doomed","is","your","soul","and","damned","is","your","life"],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["song","trailer","genre","review","actor","year","average_ratings","title","plot","character","rating","director"]}
{"id":"784","dataset":"mit-movie","split":"test","instance":{"id":"784","prompt_labels":"who(O) directed(O) who(B-director) framed(I-director) roger(I-director) rabbit(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, actor, song, title, director, review, genre, plot, character, average ratings, year, trailer and O.\nSentence: who directed who framed roger rabbit","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","directed","who","framed","roger","rabbit"],"labels":["O","O","B-director","I-director","I-director","I-director"],"target_index":null,"target_label":null},"label_list":["rating","actor","song","title","director","review","genre","plot","character","average_ratings","year","trailer"]}
{"id":"794","dataset":"mit-movie","split":"test","instance":{"id":"794","prompt_labels":"which(O) actor(O) potrayed(O) jackie(B-character) robinson(I-character)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, actor, average ratings, plot, year, song, review, title, rating, genre, trailer, director and O.\nSentence: which actor potrayed jackie robinson","prediction_output":null,"prediction_outputs":null,"group":null,"words":["which","actor","potrayed","jackie","robinson"],"labels":["O","O","O","B-character","I-character"],"target_index":null,"target_label":null},"label_list":["character","actor","average_ratings","plot","year","song","review","title","rating","genre","trailer","director"]}
{"id":"802","dataset":"mit-movie","split":"test","instance":{"id":"802","prompt_labels":"what(O) movie(O) has(O) the(O) highest(B-average ratings) viewers(I-average ratings) rating(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, director, trailer, actor, average ratings, year, title, genre, song, rating, plot, character and O.\nSentence: what movie has the highest viewers rating","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","movie","has","the","highest","viewers","rating"],"labels":["O","O","O","O","B-average ratings","I-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["review","director","trailer","actor","average_ratings","year","title","genre","song","rating","plot","character"]}
{"id":"807","dataset":"mit-movie","split":"test","instance":{"id":"807","prompt_labels":"show(O) me(O) a(O) linda(B-actor) hunt(I-actor) movie(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, character, plot, title, genre, year, average ratings, actor, trailer, review, song, rating and O.\nSentence: show me a linda hunt movie","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","a","linda","hunt","movie"],"labels":["O","O","O","B-actor","I-actor","O"],"target_index":null,"target_label":null},"label_list":["director","character","plot","title","genre","year","average_ratings","actor","trailer","review","song","rating"]}
{"id":"808","dataset":"mit-movie","split":"test","instance":{"id":"808","prompt_labels":"are(O) there(O) any(O) r(B-rating) movies(O) with(O) johnny(B-actor) depp(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, rating, character, trailer, plot, title, song, review, average ratings, actor, director, genre and O.\nSentence: are there any r movies with johnny depp","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","r","movies","with","johnny","depp"],"labels":["O","O","O","B-rating","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["year","rating","character","trailer","plot","title","song","review","average_ratings","actor","director","genre"]}
{"id":"822","dataset":"mit-movie","split":"test","instance":{"id":"822","prompt_labels":"tell(O) me(O) the(O) title(O) of(O) the(O) movie(O) about(O) ghosts(B-plot) starring(O) nicole(B-actor) kidman(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, director, song, character, average ratings, review, year, plot, actor, title, rating, trailer and O.\nSentence: tell me the title of the movie about ghosts starring nicole kidman","prediction_output":null,"prediction_outputs":null,"group":null,"words":["tell","me","the","title","of","the","movie","about","ghosts","starring","nicole","kidman"],"labels":["O","O","O","O","O","O","O","O","B-plot","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["genre","director","song","character","average_ratings","review","year","plot","actor","title","rating","trailer"]}
{"id":"825","dataset":"mit-movie","split":"test","instance":{"id":"825","prompt_labels":"what(O) was(O) the(O) last(O) movie(O) released(O) in(O) 2011(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, review, genre, director, title, trailer, character, rating, song, plot, actor, average ratings and O.\nSentence: what was the last movie released in 2011","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","last","movie","released","in","2011"],"labels":["O","O","O","O","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["year","review","genre","director","title","trailer","character","rating","song","plot","actor","average_ratings"]}
{"id":"847","dataset":"mit-movie","split":"test","instance":{"id":"847","prompt_labels":"what(O) romance(B-genre) movies(O) came(O) out(O) in(O) the(O) 90s(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, title, director, plot, average ratings, character, year, trailer, genre, actor, rating and O.\nSentence: what romance movies came out in the 90s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","romance","movies","came","out","in","the","90s"],"labels":["O","B-genre","O","O","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["review","song","title","director","plot","average_ratings","character","year","trailer","genre","actor","rating"]}
{"id":"876","dataset":"mit-movie","split":"test","instance":{"id":"876","prompt_labels":"show(O) me(O) a(O) list(O) of(O) r(B-rating) rated(I-rating) movies(O) about(O) aliens(B-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, title, song, actor, year, character, trailer, average ratings, review, genre, rating, director and O.\nSentence: show me a list of r rated movies about aliens","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","a","list","of","r","rated","movies","about","aliens"],"labels":["O","O","O","O","O","B-rating","I-rating","O","O","B-plot"],"target_index":null,"target_label":null},"label_list":["plot","title","song","actor","year","character","trailer","average_ratings","review","genre","rating","director"]}
{"id":"889","dataset":"mit-movie","split":"test","instance":{"id":"889","prompt_labels":"find(O) movies(O) from(O) the(O) 1990s(B-year) with(O) strong(B-plot) female(I-plot) leads(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, actor, trailer, plot, director, title, song, review, genre, character, average ratings, year and O.\nSentence: find movies from the 1990s with strong female leads","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","movies","from","the","1990s","with","strong","female","leads"],"labels":["O","O","O","O","B-year","O","B-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["rating","actor","trailer","plot","director","title","song","review","genre","character","average_ratings","year"]}
{"id":"893","dataset":"mit-movie","split":"test","instance":{"id":"893","prompt_labels":"what(O) police(B-genre) film(O) had(O) a(O) main(O) character(O) called(O) popeye(B-character) doyle(I-character)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, trailer, year, actor, plot, review, character, director, average ratings, song, genre, title and O.\nSentence: what police film had a main character called popeye doyle","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","police","film","had","a","main","character","called","popeye","doyle"],"labels":["O","B-genre","O","O","O","O","O","O","B-character","I-character"],"target_index":null,"target_label":null},"label_list":["rating","trailer","year","actor","plot","review","character","director","average_ratings","song","genre","title"]}
{"id":"909","dataset":"mit-movie","split":"test","instance":{"id":"909","prompt_labels":"find(O) me(O) all(O) movies(O) directed(O) by(O) cameron(B-director) crowe(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, genre, year, character, title, song, plot, average ratings, rating, actor, trailer, review and O.\nSentence: find me all movies directed by cameron crowe","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","all","movies","directed","by","cameron","crowe"],"labels":["O","O","O","O","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["director","genre","year","character","title","song","plot","average_ratings","rating","actor","trailer","review"]}
{"id":"925","dataset":"mit-movie","split":"test","instance":{"id":"925","prompt_labels":"what(O) are(O) some(O) pg(B-rating) 13(I-rating) romantic(B-genre) comedies(I-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, title, review, character, average ratings, genre, director, trailer, actor, year, plot, rating and O.\nSentence: what are some pg 13 romantic comedies","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","some","pg","13","romantic","comedies"],"labels":["O","O","O","B-rating","I-rating","B-genre","I-genre"],"target_index":null,"target_label":null},"label_list":["song","title","review","character","average_ratings","genre","director","trailer","actor","year","plot","rating"]}
{"id":"930","dataset":"mit-movie","split":"test","instance":{"id":"930","prompt_labels":"find(O) a(O) review(B-average ratings) for(O) bend(B-title) it(I-title) like(I-title) beckham(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, plot, genre, title, average ratings, character, trailer, year, rating, song, director, actor and O.\nSentence: find a review for bend it like beckham","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","a","review","for","bend","it","like","beckham"],"labels":["O","O","B-average ratings","O","B-title","I-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["review","plot","genre","title","average_ratings","character","trailer","year","rating","song","director","actor"]}
{"id":"932","dataset":"mit-movie","split":"test","instance":{"id":"932","prompt_labels":"are(O) there(O) any(O) movies(O) directed(O) by(O) peter(B-director) jackson(I-director) except(O) lord(O) of(O) the(O) rings(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, title, genre, year, actor, character, trailer, plot, review, song, rating, average ratings and O.\nSentence: are there any movies directed by peter jackson except lord of the rings","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","movies","directed","by","peter","jackson","except","lord","of","the","rings"],"labels":["O","O","O","O","O","O","B-director","I-director","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["director","title","genre","year","actor","character","trailer","plot","review","song","rating","average_ratings"]}
{"id":"939","dataset":"mit-movie","split":"test","instance":{"id":"939","prompt_labels":"what(O) comedys(O) had(O) eddie(B-actor) murphy(I-actor) play(O) a(O) big(O) role(O) in(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, genre, song, review, rating, average ratings, plot, trailer, year, title, actor, character and O.\nSentence: what comedys had eddie murphy play a big role in","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","comedys","had","eddie","murphy","play","a","big","role","in"],"labels":["O","O","O","B-actor","I-actor","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["director","genre","song","review","rating","average_ratings","plot","trailer","year","title","actor","character"]}
{"id":"942","dataset":"mit-movie","split":"test","instance":{"id":"942","prompt_labels":"name(O) the(O) the(O) 2010(B-year) movie(O) directed(O) by(O) ben(B-director) affleck(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, actor, genre, director, plot, character, average ratings, review, title, song, rating, trailer and O.\nSentence: name the the 2010 movie directed by ben affleck","prediction_output":null,"prediction_outputs":null,"group":null,"words":["name","the","the","2010","movie","directed","by","ben","affleck"],"labels":["O","O","O","B-year","O","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["year","actor","genre","director","plot","character","average_ratings","review","title","song","rating","trailer"]}
{"id":"951","dataset":"mit-movie","split":"test","instance":{"id":"951","prompt_labels":"what(O) david(B-director) fincher(I-director) movie(O) had(O) a(O) soundtrack(O) by(O) the(B-song) dust(I-song) brothers(I-song)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, character, plot, song, trailer, year, title, actor, review, genre, rating, average ratings and O.\nSentence: what david fincher movie had a soundtrack by the dust brothers","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","david","fincher","movie","had","a","soundtrack","by","the","dust","brothers"],"labels":["O","B-director","I-director","O","O","O","O","O","B-song","I-song","I-song"],"target_index":null,"target_label":null},"label_list":["director","character","plot","song","trailer","year","title","actor","review","genre","rating","average_ratings"]}
{"id":"952","dataset":"mit-movie","split":"test","instance":{"id":"952","prompt_labels":"who(O) plays(O) cindy(B-character) in(O) the(O) scary(B-title) movie(I-title) films(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, song, average ratings, year, review, genre, title, plot, character, director, rating, actor and O.\nSentence: who plays cindy in the scary movie films","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","plays","cindy","in","the","scary","movie","films"],"labels":["O","O","B-character","O","O","B-title","I-title","O"],"target_index":null,"target_label":null},"label_list":["trailer","song","average_ratings","year","review","genre","title","plot","character","director","rating","actor"]}
{"id":"959","dataset":"mit-movie","split":"test","instance":{"id":"959","prompt_labels":"what(O) was(O) charlie(B-actor) sheens(I-actor) first(O) movie(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, character, song, actor, review, average ratings, genre, trailer, rating, plot, director, year and O.\nSentence: what was charlie sheens first movie","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","charlie","sheens","first","movie"],"labels":["O","O","B-actor","I-actor","O","O"],"target_index":null,"target_label":null},"label_list":["title","character","song","actor","review","average_ratings","genre","trailer","rating","plot","director","year"]}
{"id":"990","dataset":"mit-movie","split":"test","instance":{"id":"990","prompt_labels":"who(O) directed(O) the(B-title) bedford(I-title) incident(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, rating, director, average ratings, year, genre, trailer, review, title, plot, actor, character and O.\nSentence: who directed the bedford incident","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","directed","the","bedford","incident"],"labels":["O","O","B-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["song","rating","director","average_ratings","year","genre","trailer","review","title","plot","actor","character"]}
{"id":"999","dataset":"mit-movie","split":"test","instance":{"id":"999","prompt_labels":"which(O) nc(O) 17(O) film(O) had(O) the(O) highest(O) box(B-average ratings) office(I-average ratings) gross(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, genre, actor, rating, trailer, plot, director, song, year, review, character, title and O.\nSentence: which nc 17 film had the highest box office gross","prediction_output":null,"prediction_outputs":null,"group":null,"words":["which","nc","17","film","had","the","highest","box","office","gross"],"labels":["O","O","O","O","O","O","O","B-average ratings","I-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["average_ratings","genre","actor","rating","trailer","plot","director","song","year","review","character","title"]}
{"id":"1029","dataset":"mit-movie","split":"test","instance":{"id":"1029","prompt_labels":"find(O) the(O) melissa(B-actor) leo(I-actor) movie(I-actor) about(B-plot) smuggling(I-plot) illegals(I-plot) across(I-plot) the(I-plot) border(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, actor, rating, average ratings, genre, review, title, year, trailer, song, character, plot and O.\nSentence: find the melissa leo movie about smuggling illegals across the border","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","the","melissa","leo","movie","about","smuggling","illegals","across","the","border"],"labels":["O","O","B-actor","I-actor","I-actor","B-plot","I-plot","I-plot","I-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["director","actor","rating","average_ratings","genre","review","title","year","trailer","song","character","plot"]}
{"id":"1053","dataset":"mit-movie","split":"test","instance":{"id":"1053","prompt_labels":"what(O) was(O) the(O) name(O) of(O) the(O) stepmother(O) played(O) by(O) allison(B-actor) janney(I-actor) in(O) juno(B-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, actor, trailer, director, rating, year, review, title, genre, average ratings, plot, song and O.\nSentence: what was the name of the stepmother played by allison janney in juno","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","name","of","the","stepmother","played","by","allison","janney","in","juno"],"labels":["O","O","O","O","O","O","O","O","O","B-actor","I-actor","O","B-title"],"target_index":null,"target_label":null},"label_list":["character","actor","trailer","director","rating","year","review","title","genre","average_ratings","plot","song"]}
{"id":"1063","dataset":"mit-movie","split":"test","instance":{"id":"1063","prompt_labels":"a(O) movie(O) with(O) actress(O) judy(B-actor) davis(I-actor) was(O) made(O) prior(O) to(O) the(O) past(B-year) six(I-year) decades(I-year) would(O) be(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, genre, review, average ratings, character, year, plot, song, rating, title, director, trailer and O.\nSentence: a movie with actress judy davis was made prior to the past six decades would be","prediction_output":null,"prediction_outputs":null,"group":null,"words":["a","movie","with","actress","judy","davis","was","made","prior","to","the","past","six","decades","would","be"],"labels":["O","O","O","O","B-actor","I-actor","O","O","O","O","O","B-year","I-year","I-year","O","O"],"target_index":null,"target_label":null},"label_list":["actor","genre","review","average_ratings","character","year","plot","song","rating","title","director","trailer"]}
{"id":"1066","dataset":"mit-movie","split":"test","instance":{"id":"1066","prompt_labels":"amanda(B-director) fire(I-director) directed(O) this(O) family(B-genre) film(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, rating, review, director, plot, year, song, character, title, trailer, average ratings, genre and O.\nSentence: amanda fire directed this family film","prediction_output":null,"prediction_outputs":null,"group":null,"words":["amanda","fire","directed","this","family","film"],"labels":["B-director","I-director","O","O","B-genre","O"],"target_index":null,"target_label":null},"label_list":["actor","rating","review","director","plot","year","song","character","title","trailer","average_ratings","genre"]}
{"id":"1081","dataset":"mit-movie","split":"test","instance":{"id":"1081","prompt_labels":"are(O) there(O) any(O) scarlett(B-actor) johansson(I-actor) drama(B-genre) movies(O) from(O) the(O) 1990(B-year) s(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, character, rating, genre, year, trailer, director, average ratings, plot, song, review, actor and O.\nSentence: are there any scarlett johansson drama movies from the 1990 s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","scarlett","johansson","drama","movies","from","the","1990","s"],"labels":["O","O","O","B-actor","I-actor","B-genre","O","O","O","B-year","I-year"],"target_index":null,"target_label":null},"label_list":["title","character","rating","genre","year","trailer","director","average_ratings","plot","song","review","actor"]}
{"id":"1085","dataset":"mit-movie","split":"test","instance":{"id":"1085","prompt_labels":"are(O) there(O) any(O) biography(B-genre) films(O) about(O) the(O) 1930(B-plot) s(I-plot) directed(O) gregg(B-director) sacon(I-director) that(O) are(O) unrated(B-rating) and(O) produced(O) during(O) the(O) last(B-year) six(I-year) years(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, genre, rating, trailer, character, actor, director, year, song, plot, title, review and O.\nSentence: are there any biography films about the 1930 s directed gregg sacon that are unrated and produced during the last six years","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","biography","films","about","the","1930","s","directed","gregg","sacon","that","are","unrated","and","produced","during","the","last","six","years"],"labels":["O","O","O","B-genre","O","O","O","B-plot","I-plot","O","B-director","I-director","O","O","B-rating","O","O","O","O","B-year","I-year","I-year"],"target_index":null,"target_label":null},"label_list":["average_ratings","genre","rating","trailer","character","actor","director","year","song","plot","title","review"]}
{"id":"1088","dataset":"mit-movie","split":"test","instance":{"id":"1088","prompt_labels":"are(O) there(O) any(O) family(B-genre) movies(O) tarring(O) hettie(B-director) macdonald(I-director) in(O) the(O) last(B-year) five(I-year) decades(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, character, average ratings, actor, plot, title, genre, director, review, song, rating, trailer and O.\nSentence: are there any family movies tarring hettie macdonald in the last five decades","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","family","movies","tarring","hettie","macdonald","in","the","last","five","decades"],"labels":["O","O","O","B-genre","O","O","B-director","I-director","O","O","B-year","I-year","I-year"],"target_index":null,"target_label":null},"label_list":["year","character","average_ratings","actor","plot","title","genre","director","review","song","rating","trailer"]}
{"id":"1109","dataset":"mit-movie","split":"test","instance":{"id":"1109","prompt_labels":"can(O) you(O) name(O) a(O) western(B-genre) movie(O) rated(O) r(B-rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, director, genre, year, actor, song, average ratings, plot, review, character, trailer, title and O.\nSentence: can you name a western movie rated r","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","name","a","western","movie","rated","r"],"labels":["O","O","O","O","B-genre","O","O","B-rating"],"target_index":null,"target_label":null},"label_list":["rating","director","genre","year","actor","song","average_ratings","plot","review","character","trailer","title"]}
{"id":"1117","dataset":"mit-movie","split":"test","instance":{"id":"1117","prompt_labels":"can(O) you(O) show(O) me(O) where(O) i(O) can(O) find(O) a(O) movie(O) about(O) violence(B-genre) from(O) 1970(B-year) that(O) was(O) directed(O) by(O) grace(B-director) lee(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, plot, director, actor, year, review, genre, trailer, rating, title, average ratings, song and O.\nSentence: can you show me where i can find a movie about violence from 1970 that was directed by grace lee","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","show","me","where","i","can","find","a","movie","about","violence","from","1970","that","was","directed","by","grace","lee"],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-genre","O","B-year","O","O","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["character","plot","director","actor","year","review","genre","trailer","rating","title","average_ratings","song"]}
{"id":"1126","dataset":"mit-movie","split":"test","instance":{"id":"1126","prompt_labels":"did(O) alfred(B-director) hitchcock(I-director) ever(O) direct(O) a(O) documentary(B-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, title, genre, rating, song, year, director, review, plot, average ratings, trailer, actor and O.\nSentence: did alfred hitchcock ever direct a documentary","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","alfred","hitchcock","ever","direct","a","documentary"],"labels":["O","B-director","I-director","O","O","O","B-genre"],"target_index":null,"target_label":null},"label_list":["character","title","genre","rating","song","year","director","review","plot","average_ratings","trailer","actor"]}
{"id":"1134","dataset":"mit-movie","split":"test","instance":{"id":"1134","prompt_labels":"did(O) david(B-actor) hyde(I-actor) pierce(I-actor) ever(O) star(O) in(O) a(O) movie(O) involving(O) abandonment(B-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, rating, song, actor, trailer, character, director, plot, review, title, year, genre and O.\nSentence: did david hyde pierce ever star in a movie involving abandonment","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","david","hyde","pierce","ever","star","in","a","movie","involving","abandonment"],"labels":["O","B-actor","I-actor","I-actor","O","O","O","O","O","O","B-plot"],"target_index":null,"target_label":null},"label_list":["average_ratings","rating","song","actor","trailer","character","director","plot","review","title","year","genre"]}
{"id":"1135","dataset":"mit-movie","split":"test","instance":{"id":"1135","prompt_labels":"did(O) david(B-director) lean(I-director) direct(O) any(O) fantasy(B-genre) movies(O) in(O) the(O) 2000(O) s(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, review, rating, actor, plot, genre, song, title, trailer, director, character, year and O.\nSentence: did david lean direct any fantasy movies in the 2000 s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","david","lean","direct","any","fantasy","movies","in","the","2000","s"],"labels":["O","B-director","I-director","O","O","B-genre","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["average_ratings","review","rating","actor","plot","genre","song","title","trailer","director","character","year"]}
{"id":"1136","dataset":"mit-movie","split":"test","instance":{"id":"1136","prompt_labels":"did(O) david(B-director) lean(I-director) ever(O) direct(O) a(O) drama(B-genre) in(O) the(O) 1980(B-year) s(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, review, character, song, genre, year, rating, average ratings, director, trailer, title, actor and O.\nSentence: did david lean ever direct a drama in the 1980 s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","david","lean","ever","direct","a","drama","in","the","1980","s"],"labels":["O","B-director","I-director","O","O","O","B-genre","O","O","B-year","I-year"],"target_index":null,"target_label":null},"label_list":["plot","review","character","song","genre","year","rating","average_ratings","director","trailer","title","actor"]}
{"id":"1140","dataset":"mit-movie","split":"test","instance":{"id":"1140","prompt_labels":"did(O) frank(B-director) capra(I-director) do(O) and(O) ok(B-average ratings) animated(B-genre) movie(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, director, rating, genre, title, plot, actor, year, character, average ratings, review, song and O.\nSentence: did frank capra do and ok animated movie","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","frank","capra","do","and","ok","animated","movie"],"labels":["O","B-director","I-director","O","O","B-average ratings","B-genre","O"],"target_index":null,"target_label":null},"label_list":["trailer","director","rating","genre","title","plot","actor","year","character","average_ratings","review","song"]}
{"id":"1152","dataset":"mit-movie","split":"test","instance":{"id":"1152","prompt_labels":"did(O) lucille(B-actor) ball(I-actor) appear(O) in(O) any(O) pg(B-rating) 13(I-rating) movies(O) in(O) the(O) 1990(B-year) s(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, song, review, title, average ratings, year, trailer, director, character, genre, rating, actor and O.\nSentence: did lucille ball appear in any pg 13 movies in the 1990 s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","lucille","ball","appear","in","any","pg","13","movies","in","the","1990","s"],"labels":["O","B-actor","I-actor","O","O","O","B-rating","I-rating","O","O","O","B-year","I-year"],"target_index":null,"target_label":null},"label_list":["plot","song","review","title","average_ratings","year","trailer","director","character","genre","rating","actor"]}
{"id":"1153","dataset":"mit-movie","split":"test","instance":{"id":"1153","prompt_labels":"did(O) neil(B-director) h(I-director) weiss(I-director) direct(O) a(O) murder(B-plot) film(O) in(O) the(O) past(B-year) ten(I-year) years(I-year) that(O) got(O) eight(B-average ratings) stars(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, average ratings, plot, director, year, title, actor, trailer, rating, genre, character, review and O.\nSentence: did neil h weiss direct a murder film in the past ten years that got eight stars","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","neil","h","weiss","direct","a","murder","film","in","the","past","ten","years","that","got","eight","stars"],"labels":["O","B-director","I-director","I-director","O","O","B-plot","O","O","O","B-year","I-year","I-year","O","O","B-average ratings","O"],"target_index":null,"target_label":null},"label_list":["song","average_ratings","plot","director","year","title","actor","trailer","rating","genre","character","review"]}
{"id":"1170","dataset":"mit-movie","split":"test","instance":{"id":"1170","prompt_labels":"did(O) steve(B-director) barron(I-director) direct(O) any(O) comedy(B-genre) movies(O) that(O) people(O) thought(O) was(O) all(B-average ratings) right(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, trailer, director, title, year, genre, plot, actor, character, average ratings, review, rating and O.\nSentence: did steve barron direct any comedy movies that people thought was all right","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","steve","barron","direct","any","comedy","movies","that","people","thought","was","all","right"],"labels":["O","B-director","I-director","O","O","B-genre","O","O","O","O","O","B-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["song","trailer","director","title","year","genre","plot","actor","character","average_ratings","review","rating"]}
{"id":"1171","dataset":"mit-movie","split":"test","instance":{"id":"1171","prompt_labels":"did(O) steven(B-director) shainberg(I-director) star(O) in(O) a(O) very(B-average ratings) popular(I-average ratings) mystery(B-genre) movie(O) that(O) was(O) rated(O) r(B-rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, title, review, director, song, plot, year, actor, genre, rating, average ratings, trailer and O.\nSentence: did steven shainberg star in a very popular mystery movie that was rated r","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","steven","shainberg","star","in","a","very","popular","mystery","movie","that","was","rated","r"],"labels":["O","B-director","I-director","O","O","O","B-average ratings","I-average ratings","B-genre","O","O","O","O","B-rating"],"target_index":null,"target_label":null},"label_list":["character","title","review","director","song","plot","year","actor","genre","rating","average_ratings","trailer"]}
{"id":"1174","dataset":"mit-movie","split":"test","instance":{"id":"1174","prompt_labels":"did(O) steven(B-director) spielberg(I-director) direct(O) wall(B-title) e(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, year, song, rating, title, genre, trailer, actor, plot, character, director, average ratings and O.\nSentence: did steven spielberg direct wall e","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","steven","spielberg","direct","wall","e"],"labels":["O","B-director","I-director","O","B-title","I-title"],"target_index":null,"target_label":null},"label_list":["review","year","song","rating","title","genre","trailer","actor","plot","character","director","average_ratings"]}
{"id":"1189","dataset":"mit-movie","split":"test","instance":{"id":"1189","prompt_labels":"do(O) you(O) have(O) colossus(B-title) the(I-title) forbin(I-title) project(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, song, average ratings, plot, trailer, genre, year, actor, director, character, review, title and O.\nSentence: do you have colossus the forbin project","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","you","have","colossus","the","forbin","project"],"labels":["O","O","O","B-title","I-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["rating","song","average_ratings","plot","trailer","genre","year","actor","director","character","review","title"]}
{"id":"1192","dataset":"mit-movie","split":"test","instance":{"id":"1192","prompt_labels":"do(O) you(O) have(O) a(O) comedy(B-genre) from(O) the(O) 1940(B-year) s(I-year) directed(O) by(O) joey(B-director) stewart(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, director, plot, average ratings, title, genre, review, year, actor, rating, song, trailer and O.\nSentence: do you have a comedy from the 1940 s directed by joey stewart","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","you","have","a","comedy","from","the","1940","s","directed","by","joey","stewart"],"labels":["O","O","O","O","B-genre","O","O","B-year","I-year","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["character","director","plot","average_ratings","title","genre","review","year","actor","rating","song","trailer"]}
{"id":"1193","dataset":"mit-movie","split":"test","instance":{"id":"1193","prompt_labels":"do(O) you(O) have(O) a(O) decent(B-average ratings) film(O) from(O) the(O) 2010(B-year) s(I-year) that(O) stars(O) randolph(B-actor) mantooth(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, director, actor, average ratings, song, trailer, title, review, genre, year, rating, plot and O.\nSentence: do you have a decent film from the 2010 s that stars randolph mantooth","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","you","have","a","decent","film","from","the","2010","s","that","stars","randolph","mantooth"],"labels":["O","O","O","O","B-average ratings","O","O","O","B-year","I-year","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["character","director","actor","average_ratings","song","trailer","title","review","genre","year","rating","plot"]}
{"id":"1199","dataset":"mit-movie","split":"test","instance":{"id":"1199","prompt_labels":"do(O) you(O) have(O) any(O) pg(B-rating) 13(I-rating) family(B-genre) films(O) from(O) the(O) last(B-year) two(I-year) years(I-year) that(O) was(O) liked(B-average ratings) by(I-average ratings) many(I-average ratings) and(O) starred(O) chris(B-actor) noth(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, average ratings, plot, character, genre, actor, rating, director, song, year, title, trailer and O.\nSentence: do you have any pg 13 family films from the last two years that was liked by many and starred chris noth","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","you","have","any","pg","13","family","films","from","the","last","two","years","that","was","liked","by","many","and","starred","chris","noth"],"labels":["O","O","O","O","B-rating","I-rating","B-genre","O","O","O","B-year","I-year","I-year","O","O","B-average ratings","I-average ratings","I-average ratings","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["review","average_ratings","plot","character","genre","actor","rating","director","song","year","title","trailer"]}
{"id":"1202","dataset":"mit-movie","split":"test","instance":{"id":"1202","prompt_labels":"do(O) you(O) have(O) that(O) really(B-average ratings) popular(I-average ratings) romance(B-genre) flick(O) from(O) the(O) 2010(B-year) s(I-year) rated(O) pg(B-rating) 13(I-rating) directed(O) by(O) robin(B-director) blazak(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, year, song, plot, review, genre, rating, character, actor, director, title, average ratings and O.\nSentence: do you have that really popular romance flick from the 2010 s rated pg 13 directed by robin blazak","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","you","have","that","really","popular","romance","flick","from","the","2010","s","rated","pg","13","directed","by","robin","blazak"],"labels":["O","O","O","O","B-average ratings","I-average ratings","B-genre","O","O","O","B-year","I-year","O","B-rating","I-rating","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["trailer","year","song","plot","review","genre","rating","character","actor","director","title","average_ratings"]}
{"id":"1207","dataset":"mit-movie","split":"test","instance":{"id":"1207","prompt_labels":"do(O) you(O) have(O) the(O) film(O) the(B-title) loss(I-title) of(I-title) sexual(I-title) innocence(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, average ratings, director, genre, year, review, plot, character, title, rating, trailer, song and O.\nSentence: do you have the film the loss of sexual innocence","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","you","have","the","film","the","loss","of","sexual","innocence"],"labels":["O","O","O","O","O","B-title","I-title","I-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["actor","average_ratings","director","genre","year","review","plot","character","title","rating","trailer","song"]}
{"id":"1208","dataset":"mit-movie","split":"test","instance":{"id":"1208","prompt_labels":"do(O) you(O) have(O) the(O) gangster(B-genre) film(O) from(O) the(O) past(B-year) four(I-year) years(I-year) directed(O) by(O) rob(B-director) hardy(I-director) about(O) a(O) missing(B-plot) prisoner(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, plot, review, actor, year, title, character, trailer, genre, song, average ratings, director and O.\nSentence: do you have the gangster film from the past four years directed by rob hardy about a missing prisoner","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","you","have","the","gangster","film","from","the","past","four","years","directed","by","rob","hardy","about","a","missing","prisoner"],"labels":["O","O","O","O","B-genre","O","O","O","B-year","I-year","I-year","O","O","B-director","I-director","O","O","B-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["rating","plot","review","actor","year","title","character","trailer","genre","song","average_ratings","director"]}
{"id":"1218","dataset":"mit-movie","split":"test","instance":{"id":"1218","prompt_labels":"do(O) you(O) know(O) where(O) i(O) can(O) find(O) a(O) pg(B-rating) sports(B-genre) movie(O) from(O) the(O) 1940(B-year) s(I-year) that(O) was(O) directed(O) by(O) wych(B-director) kaosayananda(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, rating, actor, trailer, director, average ratings, year, character, title, genre, review, song and O.\nSentence: do you know where i can find a pg sports movie from the 1940 s that was directed by wych kaosayananda","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","you","know","where","i","can","find","a","pg","sports","movie","from","the","1940","s","that","was","directed","by","wych","kaosayananda"],"labels":["O","O","O","O","O","O","O","O","B-rating","B-genre","O","O","O","B-year","I-year","O","O","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["plot","rating","actor","trailer","director","average_ratings","year","character","title","genre","review","song"]}
{"id":"1226","dataset":"mit-movie","split":"test","instance":{"id":"1226","prompt_labels":"does(O) a(B-actor) j(I-actor) langer(I-actor) have(O) a(O) g(B-rating) movie(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, plot, trailer, director, average ratings, genre, review, song, actor, title, rating, character and O.\nSentence: does a j langer have a g movie","prediction_output":null,"prediction_outputs":null,"group":null,"words":["does","a","j","langer","have","a","g","movie"],"labels":["O","B-actor","I-actor","I-actor","O","O","B-rating","O"],"target_index":null,"target_label":null},"label_list":["year","plot","trailer","director","average_ratings","genre","review","song","actor","title","rating","character"]}
{"id":"1228","dataset":"mit-movie","split":"test","instance":{"id":"1228","prompt_labels":"does(O) julia(B-actor) roberts(I-actor) have(O) a(O) voice(O) in(O) wall(B-title) e(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, actor, year, rating, song, director, plot, review, genre, title, trailer, character and O.\nSentence: does julia roberts have a voice in wall e","prediction_output":null,"prediction_outputs":null,"group":null,"words":["does","julia","roberts","have","a","voice","in","wall","e"],"labels":["O","B-actor","I-actor","O","O","O","O","B-title","I-title"],"target_index":null,"target_label":null},"label_list":["average_ratings","actor","year","rating","song","director","plot","review","genre","title","trailer","character"]}
{"id":"1233","dataset":"mit-movie","split":"test","instance":{"id":"1233","prompt_labels":"find(O) a(O) 1990(B-year) s(I-year) film(O) about(O) a(O) curse(B-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, genre, average ratings, review, trailer, rating, song, year, director, title, character, actor and O.\nSentence: find a 1990 s film about a curse","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","a","1990","s","film","about","a","curse"],"labels":["O","O","B-year","I-year","O","O","O","B-plot"],"target_index":null,"target_label":null},"label_list":["plot","genre","average_ratings","review","trailer","rating","song","year","director","title","character","actor"]}
{"id":"1236","dataset":"mit-movie","split":"test","instance":{"id":"1236","prompt_labels":"find(O) a(O) critically(B-average ratings) acclaimed(I-average ratings) 1980(B-year) sci(B-genre) fi(I-genre) film(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, director, title, trailer, character, average ratings, year, rating, plot, actor, genre, song and O.\nSentence: find a critically acclaimed 1980 sci fi film","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","a","critically","acclaimed","1980","sci","fi","film"],"labels":["O","O","B-average ratings","I-average ratings","B-year","B-genre","I-genre","O"],"target_index":null,"target_label":null},"label_list":["review","director","title","trailer","character","average_ratings","year","rating","plot","actor","genre","song"]}
{"id":"1243","dataset":"mit-movie","split":"test","instance":{"id":"1243","prompt_labels":"find(O) a(O) movie(O) called(O) crossover(B-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, song, director, review, year, average ratings, plot, title, genre, actor, character, rating and O.\nSentence: find a movie called crossover","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","a","movie","called","crossover"],"labels":["O","O","O","O","B-title"],"target_index":null,"target_label":null},"label_list":["trailer","song","director","review","year","average_ratings","plot","title","genre","actor","character","rating"]}
{"id":"1249","dataset":"mit-movie","split":"test","instance":{"id":"1249","prompt_labels":"find(O) me(O) a(O) fantasy(B-genre) movie(O) made(O) withing(O) the(O) past(B-year) eight(I-year) decades(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, title, review, rating, average ratings, director, genre, character, plot, song, actor, trailer and O.\nSentence: find me a fantasy movie made withing the past eight decades","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","fantasy","movie","made","withing","the","past","eight","decades"],"labels":["O","O","O","B-genre","O","O","O","O","B-year","I-year","I-year"],"target_index":null,"target_label":null},"label_list":["year","title","review","rating","average_ratings","director","genre","character","plot","song","actor","trailer"]}
{"id":"1270","dataset":"mit-movie","split":"test","instance":{"id":"1270","prompt_labels":"has(O) james(B-actor) dean(I-actor) acted(O) in(O) any(O) drama(B-genre) films(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, review, genre, trailer, plot, year, title, average ratings, actor, rating, director, song and O.\nSentence: has james dean acted in any drama films","prediction_output":null,"prediction_outputs":null,"group":null,"words":["has","james","dean","acted","in","any","drama","films"],"labels":["O","B-actor","I-actor","O","O","O","B-genre","O"],"target_index":null,"target_label":null},"label_list":["character","review","genre","trailer","plot","year","title","average_ratings","actor","rating","director","song"]}
{"id":"1278","dataset":"mit-movie","split":"test","instance":{"id":"1278","prompt_labels":"has(O) michael(B-director) keusch(I-director) directed(O) any(O) rated(O) r(B-rating) musicals(B-genre) in(O) the(O) past(B-year) ten(I-year) decades(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, trailer, rating, year, actor, character, director, title, song, plot, average ratings, genre and O.\nSentence: has michael keusch directed any rated r musicals in the past ten decades","prediction_output":null,"prediction_outputs":null,"group":null,"words":["has","michael","keusch","directed","any","rated","r","musicals","in","the","past","ten","decades"],"labels":["O","B-director","I-director","O","O","O","B-rating","B-genre","O","O","B-year","I-year","I-year"],"target_index":null,"target_label":null},"label_list":["review","trailer","rating","year","actor","character","director","title","song","plot","average_ratings","genre"]}
{"id":"1280","dataset":"mit-movie","split":"test","instance":{"id":"1280","prompt_labels":"has(O) michelangelo(B-director) antonioni(I-director) ever(O) directed(O) a(O) western(B-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, title, review, song, actor, genre, year, average ratings, trailer, character, director, plot and O.\nSentence: has michelangelo antonioni ever directed a western","prediction_output":null,"prediction_outputs":null,"group":null,"words":["has","michelangelo","antonioni","ever","directed","a","western"],"labels":["O","B-director","I-director","O","O","O","B-genre"],"target_index":null,"target_label":null},"label_list":["rating","title","review","song","actor","genre","year","average_ratings","trailer","character","director","plot"]}
{"id":"1297","dataset":"mit-movie","split":"test","instance":{"id":"1297","prompt_labels":"have(O) you(O) seen(O) the(O) movie(O) inception(B-title) with(O) jodie(B-actor) foster(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, song, year, character, plot, title, rating, director, actor, trailer, average ratings, review and O.\nSentence: have you seen the movie inception with jodie foster","prediction_output":null,"prediction_outputs":null,"group":null,"words":["have","you","seen","the","movie","inception","with","jodie","foster"],"labels":["O","O","O","O","O","B-title","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["genre","song","year","character","plot","title","rating","director","actor","trailer","average_ratings","review"]}
{"id":"1304","dataset":"mit-movie","split":"test","instance":{"id":"1304","prompt_labels":"how(O) many(O) r(B-rating) rated(O) emotional(B-genre) movies(O) have(O) been(O) made(O) in(O) the(O) past(B-year) five(I-year) decades(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, trailer, plot, director, actor, genre, average ratings, character, song, rating, year, review and O.\nSentence: how many r rated emotional movies have been made in the past five decades","prediction_output":null,"prediction_outputs":null,"group":null,"words":["how","many","r","rated","emotional","movies","have","been","made","in","the","past","five","decades"],"labels":["O","O","B-rating","O","B-genre","O","O","O","O","O","O","B-year","I-year","I-year"],"target_index":null,"target_label":null},"label_list":["title","trailer","plot","director","actor","genre","average_ratings","character","song","rating","year","review"]}
{"id":"1305","dataset":"mit-movie","split":"test","instance":{"id":"1305","prompt_labels":"how(O) many(O) comedy(B-genre) movies(O) has(O) meryl(B-actor) streep(I-actor) has(O) been(O) in(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, year, director, trailer, review, genre, plot, actor, character, rating, average ratings, title and O.\nSentence: how many comedy movies has meryl streep has been in","prediction_output":null,"prediction_outputs":null,"group":null,"words":["how","many","comedy","movies","has","meryl","streep","has","been","in"],"labels":["O","O","B-genre","O","O","B-actor","I-actor","O","O","O"],"target_index":null,"target_label":null},"label_list":["song","year","director","trailer","review","genre","plot","actor","character","rating","average_ratings","title"]}
{"id":"1307","dataset":"mit-movie","split":"test","instance":{"id":"1307","prompt_labels":"how(O) many(O) levels(O) of(O) dreams(O) are(O) in(O) the(O) movie(O) inception(B-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, actor, genre, average ratings, title, character, song, review, plot, director, trailer, rating and O.\nSentence: how many levels of dreams are in the movie inception","prediction_output":null,"prediction_outputs":null,"group":null,"words":["how","many","levels","of","dreams","are","in","the","movie","inception"],"labels":["O","O","O","O","O","O","O","O","O","B-title"],"target_index":null,"target_label":null},"label_list":["year","actor","genre","average_ratings","title","character","song","review","plot","director","trailer","rating"]}
{"id":"1312","dataset":"mit-movie","split":"test","instance":{"id":"1312","prompt_labels":"i(O) am(O) looking(O) for(O) a(O) sci(B-genre) fi(I-genre) from(O) 1970(B-year) with(O) excellent(B-average ratings) ratings(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, trailer, genre, rating, review, title, average ratings, director, plot, actor, song, year and O.\nSentence: i am looking for a sci fi from 1970 with excellent ratings","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","am","looking","for","a","sci","fi","from","1970","with","excellent","ratings"],"labels":["O","O","O","O","O","B-genre","I-genre","O","B-year","O","B-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["character","trailer","genre","rating","review","title","average_ratings","director","plot","actor","song","year"]}
{"id":"1318","dataset":"mit-movie","split":"test","instance":{"id":"1318","prompt_labels":"i(O) am(O) trying(O) to(O) find(O) a(O) police(B-genre) movie(O) rated(O) pg(B-rating) 13(I-rating) that(O) had(O) a(O) rating(O) of(O) six(B-average ratings) and(O) starred(O) jane(B-director) campion(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, year, plot, rating, actor, song, average ratings, genre, trailer, character, title, review and O.\nSentence: i am trying to find a police movie rated pg 13 that had a rating of six and starred jane campion","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","am","trying","to","find","a","police","movie","rated","pg","13","that","had","a","rating","of","six","and","starred","jane","campion"],"labels":["O","O","O","O","O","O","B-genre","O","O","B-rating","I-rating","O","O","O","O","O","B-average ratings","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["director","year","plot","rating","actor","song","average_ratings","genre","trailer","character","title","review"]}
{"id":"1320","dataset":"mit-movie","split":"test","instance":{"id":"1320","prompt_labels":"i(O) want(O) a(O) pg(B-rating) 2010(B-year) documentary(B-genre) that(O) was(O) directed(O) by(O) hiromichi(B-director) matano(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, actor, title, song, review, year, character, director, rating, plot, average ratings, genre and O.\nSentence: i want a pg 2010 documentary that was directed by hiromichi matano","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","a","pg","2010","documentary","that","was","directed","by","hiromichi","matano"],"labels":["O","O","O","B-rating","B-year","B-genre","O","O","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["trailer","actor","title","song","review","year","character","director","rating","plot","average_ratings","genre"]}
{"id":"1330","dataset":"mit-movie","split":"test","instance":{"id":"1330","prompt_labels":"i(O) would(O) a(O) movie(O) about(O) gangs(B-plot) with(O) actor(O) barry(B-actor) pepper(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, director, trailer, review, song, genre, rating, character, title, average ratings, year, plot and O.\nSentence: i would a movie about gangs with actor barry pepper","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","would","a","movie","about","gangs","with","actor","barry","pepper"],"labels":["O","O","O","O","O","B-plot","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["actor","director","trailer","review","song","genre","rating","character","title","average_ratings","year","plot"]}
{"id":"1331","dataset":"mit-movie","split":"test","instance":{"id":"1331","prompt_labels":"i(O) would(O) like(O) a(O) biography(B-genre) directed(O) by(O) ian(B-director) mackenzie(I-director) that(O) had(O) an(O) average(O) rating(O) of(O) six(B-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, director, title, year, character, average ratings, review, plot, genre, rating, actor, song and O.\nSentence: i would like a biography directed by ian mackenzie that had an average rating of six","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","would","like","a","biography","directed","by","ian","mackenzie","that","had","an","average","rating","of","six"],"labels":["O","O","O","O","B-genre","O","O","B-director","I-director","O","O","O","O","O","O","B-average ratings"],"target_index":null,"target_label":null},"label_list":["trailer","director","title","year","character","average_ratings","review","plot","genre","rating","actor","song"]}
{"id":"1346","dataset":"mit-movie","split":"test","instance":{"id":"1346","prompt_labels":"id(O) like(O) that(O) watchable(B-average ratings) and(O) unrated(B-rating) film(B-genre) noir(I-genre) movie(O) from(O) 1950(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, year, character, review, director, trailer, song, average ratings, rating, plot, title, genre and O.\nSentence: id like that watchable and unrated film noir movie from 1950","prediction_output":null,"prediction_outputs":null,"group":null,"words":["id","like","that","watchable","and","unrated","film","noir","movie","from","1950"],"labels":["O","O","O","B-average ratings","O","B-rating","B-genre","I-genre","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["actor","year","character","review","director","trailer","song","average_ratings","rating","plot","title","genre"]}
{"id":"1355","dataset":"mit-movie","split":"test","instance":{"id":"1355","prompt_labels":"im(O) looking(O) for(O) a(O) pg(B-rating) 13(I-rating) documentary(B-genre) directed(O) by(O) shuhei(B-director) morita(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, title, director, song, actor, review, trailer, year, rating, plot, genre, average ratings and O.\nSentence: im looking for a pg 13 documentary directed by shuhei morita","prediction_output":null,"prediction_outputs":null,"group":null,"words":["im","looking","for","a","pg","13","documentary","directed","by","shuhei","morita"],"labels":["O","O","O","O","B-rating","I-rating","B-genre","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["character","title","director","song","actor","review","trailer","year","rating","plot","genre","average_ratings"]}
{"id":"1361","dataset":"mit-movie","split":"test","instance":{"id":"1361","prompt_labels":"im(O) looking(O) for(O) a(O) fantasy(B-genre) movie(O) directed(O) by(O) samuel(B-actor) l(I-actor) jackson(I-actor) thats(O) rated(O) pg(B-rating) 13(I-rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, year, character, song, genre, actor, trailer, plot, review, director, rating, average ratings and O.\nSentence: im looking for a fantasy movie directed by samuel l jackson thats rated pg 13","prediction_output":null,"prediction_outputs":null,"group":null,"words":["im","looking","for","a","fantasy","movie","directed","by","samuel","l","jackson","thats","rated","pg","13"],"labels":["O","O","O","O","B-genre","O","O","O","B-actor","I-actor","I-actor","O","O","B-rating","I-rating"],"target_index":null,"target_label":null},"label_list":["title","year","character","song","genre","actor","trailer","plot","review","director","rating","average_ratings"]}
{"id":"1366","dataset":"mit-movie","split":"test","instance":{"id":"1366","prompt_labels":"im(O) looking(O) for(O) a(O) science(B-genre) fiction(I-genre) movie(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, genre, rating, year, song, plot, trailer, average ratings, director, actor, review, title and O.\nSentence: im looking for a science fiction movie","prediction_output":null,"prediction_outputs":null,"group":null,"words":["im","looking","for","a","science","fiction","movie"],"labels":["O","O","O","O","B-genre","I-genre","O"],"target_index":null,"target_label":null},"label_list":["character","genre","rating","year","song","plot","trailer","average_ratings","director","actor","review","title"]}
{"id":"1370","dataset":"mit-movie","split":"test","instance":{"id":"1370","prompt_labels":"im(O) looking(O) for(O) a(O) war(B-genre) film(O) from(O) the(O) past(B-year) ten(I-year) years(I-year) rated(O) nc(B-rating) 17(I-rating) and(O) directed(O) by(O) putipong(B-director) saisikaew(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, genre, year, actor, trailer, title, average ratings, director, plot, rating, song, character and O.\nSentence: im looking for a war film from the past ten years rated nc 17 and directed by putipong saisikaew","prediction_output":null,"prediction_outputs":null,"group":null,"words":["im","looking","for","a","war","film","from","the","past","ten","years","rated","nc","17","and","directed","by","putipong","saisikaew"],"labels":["O","O","O","O","B-genre","O","O","O","B-year","I-year","I-year","O","B-rating","I-rating","O","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["review","genre","year","actor","trailer","title","average_ratings","director","plot","rating","song","character"]}
{"id":"1376","dataset":"mit-movie","split":"test","instance":{"id":"1376","prompt_labels":"im(O) looking(O) for(O) an(O) emotional(B-genre) movie(O) rated(O) pg(B-rating) 13(I-rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, year, average ratings, genre, trailer, rating, plot, director, song, actor, character, review and O.\nSentence: im looking for an emotional movie rated pg 13","prediction_output":null,"prediction_outputs":null,"group":null,"words":["im","looking","for","an","emotional","movie","rated","pg","13"],"labels":["O","O","O","O","B-genre","O","O","B-rating","I-rating"],"target_index":null,"target_label":null},"label_list":["title","year","average_ratings","genre","trailer","rating","plot","director","song","actor","character","review"]}
{"id":"1381","dataset":"mit-movie","split":"test","instance":{"id":"1381","prompt_labels":"im(O) looking(O) for(O) that(O) childrens(B-genre) movie(O) starring(O) sylvester(B-director) stallone(I-director) from(O) the(O) 1980(B-year) s(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, genre, song, plot, rating, actor, director, review, trailer, character, title, year and O.\nSentence: im looking for that childrens movie starring sylvester stallone from the 1980 s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["im","looking","for","that","childrens","movie","starring","sylvester","stallone","from","the","1980","s"],"labels":["O","O","O","O","B-genre","O","O","B-director","I-director","O","O","B-year","I-year"],"target_index":null,"target_label":null},"label_list":["average_ratings","genre","song","plot","rating","actor","director","review","trailer","character","title","year"]}
{"id":"1391","dataset":"mit-movie","split":"test","instance":{"id":"1391","prompt_labels":"im(O) want(O) to(O) watch(O) an(O) all(B-average ratings) right(I-average ratings) western(B-genre) from(O) 1980(B-year) where(O) they(O) search(B-plot) for(I-plot) gold(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, song, genre, actor, review, year, character, average ratings, title, director, trailer, rating and O.\nSentence: im want to watch an all right western from 1980 where they search for gold","prediction_output":null,"prediction_outputs":null,"group":null,"words":["im","want","to","watch","an","all","right","western","from","1980","where","they","search","for","gold"],"labels":["O","O","O","O","O","B-average ratings","I-average ratings","B-genre","O","B-year","O","O","B-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["plot","song","genre","actor","review","year","character","average_ratings","title","director","trailer","rating"]}
{"id":"1411","dataset":"mit-movie","split":"test","instance":{"id":"1411","prompt_labels":"in(O) the(O) last(B-year) decade(I-year) what(O) history(B-genre) films(O) by(O) adam(B-director) wingard(I-director) were(O) really(B-average ratings) good(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, title, genre, director, plot, trailer, rating, year, song, character, average ratings, actor and O.\nSentence: in the last decade what history films by adam wingard were really good","prediction_output":null,"prediction_outputs":null,"group":null,"words":["in","the","last","decade","what","history","films","by","adam","wingard","were","really","good"],"labels":["O","O","B-year","I-year","O","B-genre","O","O","B-director","I-director","O","B-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["review","title","genre","director","plot","trailer","rating","year","song","character","average_ratings","actor"]}
{"id":"1413","dataset":"mit-movie","split":"test","instance":{"id":"1413","prompt_labels":"in(O) the(O) last(B-year) five(I-year) years(I-year) was(O) benicio(B-actor) del(I-actor) torro(I-actor) in(O) a(O) pg(B-rating) 13(I-rating) movie(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, rating, average ratings, actor, director, plot, year, title, song, trailer, genre, character and O.\nSentence: in the last five years was benicio del torro in a pg 13 movie","prediction_output":null,"prediction_outputs":null,"group":null,"words":["in","the","last","five","years","was","benicio","del","torro","in","a","pg","13","movie"],"labels":["O","O","B-year","I-year","I-year","O","B-actor","I-actor","I-actor","O","O","B-rating","I-rating","O"],"target_index":null,"target_label":null},"label_list":["review","rating","average_ratings","actor","director","plot","year","title","song","trailer","genre","character"]}
{"id":"1416","dataset":"mit-movie","split":"test","instance":{"id":"1416","prompt_labels":"in(O) the(O) last(B-year) six(I-year) decades(I-year) what(O) movies(O) has(O) adam(B-actor) storke(I-actor) been(O) in(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, plot, song, average ratings, director, year, rating, character, genre, title, actor, trailer and O.\nSentence: in the last six decades what movies has adam storke been in","prediction_output":null,"prediction_outputs":null,"group":null,"words":["in","the","last","six","decades","what","movies","has","adam","storke","been","in"],"labels":["O","O","B-year","I-year","I-year","O","O","O","B-actor","I-actor","O","O"],"target_index":null,"target_label":null},"label_list":["review","plot","song","average_ratings","director","year","rating","character","genre","title","actor","trailer"]}
{"id":"1425","dataset":"mit-movie","split":"test","instance":{"id":"1425","prompt_labels":"in(O) the(O) past(B-year) seven(I-year) decades(I-year) was(O) mimi(B-actor) rogers(I-actor) an(O) many(O) adventure(B-genre) films(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, plot, review, year, actor, average ratings, genre, song, title, trailer, director, rating and O.\nSentence: in the past seven decades was mimi rogers an many adventure films","prediction_output":null,"prediction_outputs":null,"group":null,"words":["in","the","past","seven","decades","was","mimi","rogers","an","many","adventure","films"],"labels":["O","O","B-year","I-year","I-year","O","B-actor","I-actor","O","O","B-genre","O"],"target_index":null,"target_label":null},"label_list":["character","plot","review","year","actor","average_ratings","genre","song","title","trailer","director","rating"]}
{"id":"1426","dataset":"mit-movie","split":"test","instance":{"id":"1426","prompt_labels":"in(O) the(O) past(B-year) seven(I-year) years(I-year) has(O) there(O) been(O) a(O) horror(B-genre) film(O) starring(O) brenden(B-actor) sexton(I-actor) jr(I-actor) with(O) good(B-average ratings) ratings(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, genre, rating, character, plot, review, song, average ratings, title, trailer, director, year and O.\nSentence: in the past seven years has there been a horror film starring brenden sexton jr with good ratings","prediction_output":null,"prediction_outputs":null,"group":null,"words":["in","the","past","seven","years","has","there","been","a","horror","film","starring","brenden","sexton","jr","with","good","ratings"],"labels":["O","O","B-year","I-year","I-year","O","O","O","O","B-genre","O","O","B-actor","I-actor","I-actor","O","B-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["actor","genre","rating","character","plot","review","song","average_ratings","title","trailer","director","year"]}
{"id":"1447","dataset":"mit-movie","split":"test","instance":{"id":"1447","prompt_labels":"is(O) it(O) true(O) that(O) kate(B-actor) mulgrew(I-actor) was(O) born(O) in(O) 1940(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, actor, average ratings, year, genre, review, song, plot, director, trailer, character, rating and O.\nSentence: is it true that kate mulgrew was born in 1940","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","it","true","that","kate","mulgrew","was","born","in","1940"],"labels":["O","O","O","O","B-actor","I-actor","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["title","actor","average_ratings","year","genre","review","song","plot","director","trailer","character","rating"]}
{"id":"1464","dataset":"mit-movie","split":"test","instance":{"id":"1464","prompt_labels":"is(O) there(O) a(O) pg(B-rating) 13(I-rating) film(O) out(O) there(O) from(O) the(O) 2000(B-year) s(I-year) that(O) focuses(O) on(O) an(O) evil(B-plot) character(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, rating, title, director, average ratings, song, trailer, review, character, year, actor, plot and O.\nSentence: is there a pg 13 film out there from the 2000 s that focuses on an evil character","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","pg","13","film","out","there","from","the","2000","s","that","focuses","on","an","evil","character"],"labels":["O","O","O","B-rating","I-rating","O","O","O","O","O","B-year","I-year","O","O","O","O","B-plot","O"],"target_index":null,"target_label":null},"label_list":["genre","rating","title","director","average_ratings","song","trailer","review","character","year","actor","plot"]}
{"id":"1465","dataset":"mit-movie","split":"test","instance":{"id":"1465","prompt_labels":"is(O) there(O) a(O) pg(B-rating) 13(I-rating) mockumentary(B-genre) that(O) is(O) liked(B-average ratings) by(I-average ratings) many(I-average ratings) and(O) starring(O) nicolas(B-actor) cage(I-actor) from(O) the(O) last(B-year) nine(I-year) years(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, title, song, average ratings, character, trailer, director, review, year, actor, rating, genre and O.\nSentence: is there a pg 13 mockumentary that is liked by many and starring nicolas cage from the last nine years","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","pg","13","mockumentary","that","is","liked","by","many","and","starring","nicolas","cage","from","the","last","nine","years"],"labels":["O","O","O","B-rating","I-rating","B-genre","O","O","B-average ratings","I-average ratings","I-average ratings","O","O","B-actor","I-actor","O","O","B-year","I-year","I-year"],"target_index":null,"target_label":null},"label_list":["plot","title","song","average_ratings","character","trailer","director","review","year","actor","rating","genre"]}
{"id":"1466","dataset":"mit-movie","split":"test","instance":{"id":"1466","prompt_labels":"is(O) there(O) a(O) paul(B-director) hunter(I-director) movie(O) that(O) was(O) very(B-average ratings) popular(I-average ratings) and(O) released(O) in(O) 1970(B-year) with(O) a(O) pg(B-rating) rating(O) that(O) was(O) about(O) global(B-plot) power(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, actor, trailer, director, rating, average ratings, song, plot, review, year, genre, character and O.\nSentence: is there a paul hunter movie that was very popular and released in 1970 with a pg rating that was about global power","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","paul","hunter","movie","that","was","very","popular","and","released","in","1970","with","a","pg","rating","that","was","about","global","power"],"labels":["O","O","O","B-director","I-director","O","O","O","B-average ratings","I-average ratings","O","O","O","B-year","O","O","B-rating","O","O","O","O","B-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["title","actor","trailer","director","rating","average_ratings","song","plot","review","year","genre","character"]}
{"id":"1470","dataset":"mit-movie","split":"test","instance":{"id":"1470","prompt_labels":"is(O) there(O) a(O) biographical(B-genre) movie(O) by(O) the(O) director(O) andrei(B-director) tarkovsky(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, trailer, director, actor, title, rating, year, average ratings, plot, genre, character and O.\nSentence: is there a biographical movie by the director andrei tarkovsky","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","biographical","movie","by","the","director","andrei","tarkovsky"],"labels":["O","O","O","B-genre","O","O","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["review","song","trailer","director","actor","title","rating","year","average_ratings","plot","genre","character"]}
{"id":"1471","dataset":"mit-movie","split":"test","instance":{"id":"1471","prompt_labels":"is(O) there(O) a(O) comedy(B-genre) with(O) a(O) rating(O) of(O) r(B-rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, song, plot, director, year, trailer, genre, rating, actor, review, title, character and O.\nSentence: is there a comedy with a rating of r","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","comedy","with","a","rating","of","r"],"labels":["O","O","O","B-genre","O","O","O","O","B-rating"],"target_index":null,"target_label":null},"label_list":["average_ratings","song","plot","director","year","trailer","genre","rating","actor","review","title","character"]}
{"id":"1475","dataset":"mit-movie","split":"test","instance":{"id":"1475","prompt_labels":"is(O) there(O) a(O) evil(B-plot) horror(B-genre) movie(O) with(O) at(O) least(O) four(B-average ratings) stars(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, trailer, character, title, actor, review, genre, average ratings, song, rating, plot, director and O.\nSentence: is there a evil horror movie with at least four stars","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","evil","horror","movie","with","at","least","four","stars"],"labels":["O","O","O","B-plot","B-genre","O","O","O","O","B-average ratings","O"],"target_index":null,"target_label":null},"label_list":["year","trailer","character","title","actor","review","genre","average_ratings","song","rating","plot","director"]}
{"id":"1477","dataset":"mit-movie","split":"test","instance":{"id":"1477","prompt_labels":"is(O) there(O) a(O) film(O) with(O) the(O) title(O) f(B-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, rating, title, review, trailer, year, average ratings, director, song, plot, character, genre and O.\nSentence: is there a film with the title f","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","film","with","the","title","f"],"labels":["O","O","O","O","O","O","O","B-title"],"target_index":null,"target_label":null},"label_list":["actor","rating","title","review","trailer","year","average_ratings","director","song","plot","character","genre"]}
{"id":"1479","dataset":"mit-movie","split":"test","instance":{"id":"1479","prompt_labels":"is(O) there(O) a(O) five(B-average ratings) star(O) jenna(B-actor) elfman(I-actor) romantic(B-genre) comedy(I-genre) type(O) movie(O) about(O) an(O) unhappy(B-plot) marriage(I-plot) set(O) in(O) the(O) 1960(B-year) s(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, plot, average ratings, character, trailer, year, genre, song, title, rating, actor, review and O.\nSentence: is there a five star jenna elfman romantic comedy type movie about an unhappy marriage set in the 1960 s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","five","star","jenna","elfman","romantic","comedy","type","movie","about","an","unhappy","marriage","set","in","the","1960","s"],"labels":["O","O","O","B-average ratings","O","B-actor","I-actor","B-genre","I-genre","O","O","O","O","B-plot","I-plot","O","O","O","B-year","I-year"],"target_index":null,"target_label":null},"label_list":["director","plot","average_ratings","character","trailer","year","genre","song","title","rating","actor","review"]}
{"id":"1483","dataset":"mit-movie","split":"test","instance":{"id":"1483","prompt_labels":"is(O) there(O) a(O) good(O) robin(B-actor) williams(I-actor) movie(O) that(O) is(O) an(O) adventure(B-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, title, character, actor, plot, rating, genre, trailer, review, average ratings, director, song and O.\nSentence: is there a good robin williams movie that is an adventure","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","good","robin","williams","movie","that","is","an","adventure"],"labels":["O","O","O","O","B-actor","I-actor","O","O","O","O","B-genre"],"target_index":null,"target_label":null},"label_list":["year","title","character","actor","plot","rating","genre","trailer","review","average_ratings","director","song"]}
{"id":"1492","dataset":"mit-movie","split":"test","instance":{"id":"1492","prompt_labels":"is(O) there(O) a(O) good(O) rated(O) r(B-rating) military(B-genre) movie(O) about(O) camp(B-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, genre, average ratings, actor, plot, director, title, review, year, character, rating, song and O.\nSentence: is there a good rated r military movie about camp","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","good","rated","r","military","movie","about","camp"],"labels":["O","O","O","O","O","B-rating","B-genre","O","O","B-plot"],"target_index":null,"target_label":null},"label_list":["trailer","genre","average_ratings","actor","plot","director","title","review","year","character","rating","song"]}
{"id":"1496","dataset":"mit-movie","split":"test","instance":{"id":"1496","prompt_labels":"is(O) there(O) a(O) good(O) sci(B-genre) fi(I-genre) movie(O) that(O) could(O) be(O) recommended(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, title, review, plot, actor, director, year, rating, trailer, average ratings, character, genre and O.\nSentence: is there a good sci fi movie that could be recommended","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","good","sci","fi","movie","that","could","be","recommended"],"labels":["O","O","O","O","B-genre","I-genre","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["song","title","review","plot","actor","director","year","rating","trailer","average_ratings","character","genre"]}
{"id":"1499","dataset":"mit-movie","split":"test","instance":{"id":"1499","prompt_labels":"is(O) there(O) a(O) highly(B-average ratings) rated(I-average ratings) drama(B-genre) film(O) that(O) stars(O) julie(B-actor) andrews(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, rating, trailer, actor, song, genre, title, average ratings, plot, review, director, character and O.\nSentence: is there a highly rated drama film that stars julie andrews","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","highly","rated","drama","film","that","stars","julie","andrews"],"labels":["O","O","O","B-average ratings","I-average ratings","B-genre","O","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["year","rating","trailer","actor","song","genre","title","average_ratings","plot","review","director","character"]}
{"id":"1515","dataset":"mit-movie","split":"test","instance":{"id":"1515","prompt_labels":"is(O) there(O) a(O) movie(O) called(O) wall(B-title) e(I-title) that(O) was(O) directed(O) by(O) roman(B-director) polanski(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, director, song, title, average ratings, character, plot, year, genre, actor, trailer, review and O.\nSentence: is there a movie called wall e that was directed by roman polanski","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","movie","called","wall","e","that","was","directed","by","roman","polanski"],"labels":["O","O","O","O","O","B-title","I-title","O","O","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["rating","director","song","title","average_ratings","character","plot","year","genre","actor","trailer","review"]}
{"id":"1519","dataset":"mit-movie","split":"test","instance":{"id":"1519","prompt_labels":"is(O) there(O) a(O) movie(O) from(O) the(O) 1990(B-year) s(I-year) about(O) romance(B-genre) and(O) rated(O) r(B-rating) starring(O) julie(B-actor) andrews(I-actor) and(O) has(O) excellent(B-average ratings) ratings(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, title, review, actor, average ratings, character, trailer, director, genre, year, plot, song and O.\nSentence: is there a movie from the 1990 s about romance and rated r starring julie andrews and has excellent ratings","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","movie","from","the","1990","s","about","romance","and","rated","r","starring","julie","andrews","and","has","excellent","ratings"],"labels":["O","O","O","O","O","O","B-year","I-year","O","B-genre","O","O","B-rating","O","B-actor","I-actor","O","O","B-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["rating","title","review","actor","average_ratings","character","trailer","director","genre","year","plot","song"]}
{"id":"1521","dataset":"mit-movie","split":"test","instance":{"id":"1521","prompt_labels":"is(O) there(O) a(O) movie(O) starring(O) cary(B-actor) grant(I-actor) witch(O) director(O) was(O) hayao(B-director) miyazaki(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, trailer, song, character, title, plot, review, director, average ratings, rating, year, genre and O.\nSentence: is there a movie starring cary grant witch director was hayao miyazaki","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","movie","starring","cary","grant","witch","director","was","hayao","miyazaki"],"labels":["O","O","O","O","O","B-actor","I-actor","O","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["actor","trailer","song","character","title","plot","review","director","average_ratings","rating","year","genre"]}
{"id":"1530","dataset":"mit-movie","split":"test","instance":{"id":"1530","prompt_labels":"is(O) there(O) a(O) rated(O) g(B-rating) western(B-genre) movie(O) with(O) director(O) faith(B-director) granger(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, plot, rating, genre, actor, song, character, year, average ratings, director, title, review and O.\nSentence: is there a rated g western movie with director faith granger","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","rated","g","western","movie","with","director","faith","granger"],"labels":["O","O","O","O","B-rating","B-genre","O","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["trailer","plot","rating","genre","actor","song","character","year","average_ratings","director","title","review"]}
{"id":"1539","dataset":"mit-movie","split":"test","instance":{"id":"1539","prompt_labels":"is(O) there(O) a(O) spaghetti(B-genre) western(I-genre) about(O) a(O) sheriff(B-plot) with(O) an(O) average(B-average ratings) rating(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, title, review, actor, character, plot, year, director, genre, trailer, average ratings, song and O.\nSentence: is there a spaghetti western about a sheriff with an average rating","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","spaghetti","western","about","a","sheriff","with","an","average","rating"],"labels":["O","O","O","B-genre","I-genre","O","O","B-plot","O","O","B-average ratings","O"],"target_index":null,"target_label":null},"label_list":["rating","title","review","actor","character","plot","year","director","genre","trailer","average_ratings","song"]}
{"id":"1542","dataset":"mit-movie","split":"test","instance":{"id":"1542","prompt_labels":"is(O) there(O) a(O) war(B-genre) movie(O) which(O) director(O) was(O) frank(B-director) capra(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, actor, song, year, genre, rating, trailer, title, character, director, plot, review and O.\nSentence: is there a war movie which director was frank capra","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","war","movie","which","director","was","frank","capra"],"labels":["O","O","O","B-genre","O","O","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["average_ratings","actor","song","year","genre","rating","trailer","title","character","director","plot","review"]}
{"id":"1548","dataset":"mit-movie","split":"test","instance":{"id":"1548","prompt_labels":"is(O) there(O) an(O) action(B-genre) movie(O) starring(O) sean(B-actor) connery(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, rating, actor, average ratings, review, title, genre, year, plot, character, song, director and O.\nSentence: is there an action movie starring sean connery","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","an","action","movie","starring","sean","connery"],"labels":["O","O","O","B-genre","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["trailer","rating","actor","average_ratings","review","title","genre","year","plot","character","song","director"]}
{"id":"1552","dataset":"mit-movie","split":"test","instance":{"id":"1552","prompt_labels":"is(O) there(O) an(O) animated(B-genre) movie(O) that(O) is(O) directed(O) by(O) christopher(B-director) nolan(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, director, actor, song, plot, genre, trailer, rating, character, average ratings, title, year and O.\nSentence: is there an animated movie that is directed by christopher nolan","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","an","animated","movie","that","is","directed","by","christopher","nolan"],"labels":["O","O","O","B-genre","O","O","O","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["review","director","actor","song","plot","genre","trailer","rating","character","average_ratings","title","year"]}
{"id":"1569","dataset":"mit-movie","split":"test","instance":{"id":"1569","prompt_labels":"is(O) there(O) any(O) scary(B-genre) sequels(O) coming(O) out(O) soon(O) like(O) another(O) saw(O) movie(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, song, character, actor, review, genre, average ratings, plot, director, rating, trailer, title and O.\nSentence: is there any scary sequels coming out soon like another saw movie","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","any","scary","sequels","coming","out","soon","like","another","saw","movie"],"labels":["O","O","O","B-genre","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["year","song","character","actor","review","genre","average_ratings","plot","director","rating","trailer","title"]}
{"id":"1576","dataset":"mit-movie","split":"test","instance":{"id":"1576","prompt_labels":"june(B-actor) lockheart(I-actor) was(O) featured(O) in(O) this(O) unrated(B-rating) animation(B-genre) film(O) of(O) the(O) last(B-year) decade(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, song, genre, plot, director, title, trailer, actor, character, year, review, average ratings and O.\nSentence: june lockheart was featured in this unrated animation film of the last decade","prediction_output":null,"prediction_outputs":null,"group":null,"words":["june","lockheart","was","featured","in","this","unrated","animation","film","of","the","last","decade"],"labels":["B-actor","I-actor","O","O","O","O","B-rating","B-genre","O","O","O","B-year","I-year"],"target_index":null,"target_label":null},"label_list":["rating","song","genre","plot","director","title","trailer","actor","character","year","review","average_ratings"]}
{"id":"1583","dataset":"mit-movie","split":"test","instance":{"id":"1583","prompt_labels":"list(O) r(B-rating) rated(O) movies(O) starring(O) clint(B-actor) eastwood(I-actor) in(O) the(O) last(B-year) ten(I-year) decades(I-year) with(O) a(O) fugitive(B-plot) plot(O) style(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, review, actor, plot, rating, song, average ratings, character, director, title, trailer, genre and O.\nSentence: list r rated movies starring clint eastwood in the last ten decades with a fugitive plot style","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","r","rated","movies","starring","clint","eastwood","in","the","last","ten","decades","with","a","fugitive","plot","style"],"labels":["O","B-rating","O","O","O","B-actor","I-actor","O","O","B-year","I-year","I-year","O","O","B-plot","O","O"],"target_index":null,"target_label":null},"label_list":["year","review","actor","plot","rating","song","average_ratings","character","director","title","trailer","genre"]}
{"id":"1584","dataset":"mit-movie","split":"test","instance":{"id":"1584","prompt_labels":"list(O) samuel(B-actor) l(I-actor) jackson(I-actor) movies(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, title, song, plot, rating, trailer, director, character, average ratings, genre, actor, year and O.\nSentence: list samuel l jackson movies","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","samuel","l","jackson","movies"],"labels":["O","B-actor","I-actor","I-actor","O"],"target_index":null,"target_label":null},"label_list":["review","title","song","plot","rating","trailer","director","character","average_ratings","genre","actor","year"]}
{"id":"1585","dataset":"mit-movie","split":"test","instance":{"id":"1585","prompt_labels":"list(O) the(B-title) shining(I-title) film(O) starring(O) clark(B-actor) gable(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, review, director, average ratings, rating, song, genre, title, trailer, actor, year, plot and O.\nSentence: list the shining film starring clark gable","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","the","shining","film","starring","clark","gable"],"labels":["O","B-title","I-title","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["character","review","director","average_ratings","rating","song","genre","title","trailer","actor","year","plot"]}
{"id":"1587","dataset":"mit-movie","split":"test","instance":{"id":"1587","prompt_labels":"list(O) a(O) 1980(B-year) biography(B-genre) that(O) was(O) liked(B-average ratings) by(I-average ratings) many(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, song, genre, rating, character, year, trailer, review, plot, average ratings, title, director and O.\nSentence: list a 1980 biography that was liked by many","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","a","1980","biography","that","was","liked","by","many"],"labels":["O","O","B-year","B-genre","O","O","B-average ratings","I-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["actor","song","genre","rating","character","year","trailer","review","plot","average_ratings","title","director"]}
{"id":"1592","dataset":"mit-movie","split":"test","instance":{"id":"1592","prompt_labels":"list(O) a(O) humphrey(B-actor) bogart(I-actor) thriller(B-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, genre, song, character, year, review, average ratings, director, plot, trailer, rating, title and O.\nSentence: list a humphrey bogart thriller","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","a","humphrey","bogart","thriller"],"labels":["O","O","B-actor","I-actor","B-genre"],"target_index":null,"target_label":null},"label_list":["actor","genre","song","character","year","review","average_ratings","director","plot","trailer","rating","title"]}
{"id":"1603","dataset":"mit-movie","split":"test","instance":{"id":"1603","prompt_labels":"list(O) a(O) wall(B-title) e(I-title) movie(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, actor, title, character, average ratings, song, year, rating, review, genre, trailer, director and O.\nSentence: list a wall e movie","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","a","wall","e","movie"],"labels":["O","O","B-title","I-title","O"],"target_index":null,"target_label":null},"label_list":["plot","actor","title","character","average_ratings","song","year","rating","review","genre","trailer","director"]}
{"id":"1619","dataset":"mit-movie","split":"test","instance":{"id":"1619","prompt_labels":"list(O) a(O) good(B-average ratings) thriller(B-genre) movie(O) made(O) in(O) 1960(B-year) s(I-year) starring(O) cate(B-actor) blanchett(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, trailer, plot, review, year, average ratings, character, title, rating, genre, director, song and O.\nSentence: list a good thriller movie made in 1960 s starring cate blanchett","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","a","good","thriller","movie","made","in","1960","s","starring","cate","blanchett"],"labels":["O","O","B-average ratings","B-genre","O","O","O","B-year","I-year","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["actor","trailer","plot","review","year","average_ratings","character","title","rating","genre","director","song"]}
{"id":"1631","dataset":"mit-movie","split":"test","instance":{"id":"1631","prompt_labels":"list(O) a(O) must(B-average ratings) see(I-average ratings) thriller(B-genre) made(O) last(B-year) year(I-year) but(O) is(O) pg(B-rating) 13(I-rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, genre, actor, trailer, rating, review, song, character, title, director, plot, average ratings and O.\nSentence: list a must see thriller made last year but is pg 13","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","a","must","see","thriller","made","last","year","but","is","pg","13"],"labels":["O","O","B-average ratings","I-average ratings","B-genre","O","B-year","I-year","O","O","B-rating","I-rating"],"target_index":null,"target_label":null},"label_list":["year","genre","actor","trailer","rating","review","song","character","title","director","plot","average_ratings"]}
{"id":"1639","dataset":"mit-movie","split":"test","instance":{"id":"1639","prompt_labels":"list(O) a(O) sci(B-genre) fi(I-genre) movie(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, genre, character, plot, average ratings, title, director, actor, review, trailer, song, year and O.\nSentence: list a sci fi movie","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","a","sci","fi","movie"],"labels":["O","O","B-genre","I-genre","O"],"target_index":null,"target_label":null},"label_list":["rating","genre","character","plot","average_ratings","title","director","actor","review","trailer","song","year"]}
{"id":"1646","dataset":"mit-movie","split":"test","instance":{"id":"1646","prompt_labels":"list(O) a(O) teen(B-genre) movie(O) thats(O) rated(O) pg(B-rating) 13(I-rating) that(O) received(O) a(O) five(B-average ratings) stars(I-average ratings) and(I-average ratings) above(I-average ratings) rating(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, review, title, trailer, song, average ratings, rating, director, actor, character, plot, year and O.\nSentence: list a teen movie thats rated pg 13 that received a five stars and above rating","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","a","teen","movie","thats","rated","pg","13","that","received","a","five","stars","and","above","rating"],"labels":["O","O","B-genre","O","O","O","B-rating","I-rating","O","O","O","B-average ratings","I-average ratings","I-average ratings","I-average ratings","O"],"target_index":null,"target_label":null},"label_list":["genre","review","title","trailer","song","average_ratings","rating","director","actor","character","plot","year"]}
{"id":"1657","dataset":"mit-movie","split":"test","instance":{"id":"1657","prompt_labels":"list(O) all(O) adventure(B-genre) movies(O) that(O) came(O) out(O) in(O) 2000(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, actor, song, review, title, plot, rating, trailer, character, average ratings, genre, year and O.\nSentence: list all adventure movies that came out in 2000","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","all","adventure","movies","that","came","out","in","2000"],"labels":["O","O","B-genre","O","O","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["director","actor","song","review","title","plot","rating","trailer","character","average_ratings","genre","year"]}
{"id":"1662","dataset":"mit-movie","split":"test","instance":{"id":"1662","prompt_labels":"list(O) an(O) inception(B-title) movie(O) which(O) director(O) was(O) frank(B-director) capra(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, trailer, title, character, song, plot, genre, director, review, year, rating, average ratings and O.\nSentence: list an inception movie which director was frank capra","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","an","inception","movie","which","director","was","frank","capra"],"labels":["O","O","B-title","O","O","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["actor","trailer","title","character","song","plot","genre","director","review","year","rating","average_ratings"]}
{"id":"1679","dataset":"mit-movie","split":"test","instance":{"id":"1679","prompt_labels":"list(O) childrens(B-genre) movies(O) that(O) included(O) john(B-actor) wayne(I-actor) in(O) the(O) cast(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, plot, genre, rating, song, year, actor, review, director, character, average ratings, trailer and O.\nSentence: list childrens movies that included john wayne in the cast","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","childrens","movies","that","included","john","wayne","in","the","cast"],"labels":["O","B-genre","O","O","O","B-actor","I-actor","O","O","O"],"target_index":null,"target_label":null},"label_list":["title","plot","genre","rating","song","year","actor","review","director","character","average_ratings","trailer"]}
{"id":"1680","dataset":"mit-movie","split":"test","instance":{"id":"1680","prompt_labels":"list(O) crime(B-genre) movies(O) from(O) the(O) 2000(B-year) s(I-year) that(O) were(O) rated(O) nc(B-rating) 17(I-rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, song, year, genre, plot, director, title, review, trailer, rating, average ratings, actor and O.\nSentence: list crime movies from the 2000 s that were rated nc 17","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","crime","movies","from","the","2000","s","that","were","rated","nc","17"],"labels":["O","B-genre","O","O","O","B-year","I-year","O","O","O","B-rating","I-rating"],"target_index":null,"target_label":null},"label_list":["character","song","year","genre","plot","director","title","review","trailer","rating","average_ratings","actor"]}
{"id":"1695","dataset":"mit-movie","split":"test","instance":{"id":"1695","prompt_labels":"list(O) some(O) g(B-rating) rated(O) family(B-genre) movies(O) from(O) jerry(B-director) london(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, title, plot, director, song, rating, average ratings, actor, trailer, review, genre, character and O.\nSentence: list some g rated family movies from jerry london","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","some","g","rated","family","movies","from","jerry","london"],"labels":["O","O","B-rating","O","B-genre","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["year","title","plot","director","song","rating","average_ratings","actor","trailer","review","genre","character"]}
{"id":"1710","dataset":"mit-movie","split":"test","instance":{"id":"1710","prompt_labels":"looking(O) for(O) an(O) ok(B-average ratings) terrorist(B-plot) film(O) to(O) watch(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, rating, character, song, title, review, average ratings, plot, year, actor, director, trailer and O.\nSentence: looking for an ok terrorist film to watch","prediction_output":null,"prediction_outputs":null,"group":null,"words":["looking","for","an","ok","terrorist","film","to","watch"],"labels":["O","O","O","B-average ratings","B-plot","O","O","O"],"target_index":null,"target_label":null},"label_list":["genre","rating","character","song","title","review","average_ratings","plot","year","actor","director","trailer"]}
{"id":"1721","dataset":"mit-movie","split":"test","instance":{"id":"1721","prompt_labels":"name(O) a(O) rick(B-director) bieber(I-director) directed(O) family(B-genre) film(O) rated(O) pg(B-rating) 13(I-rating) from(O) the(O) past(B-year) nine(I-year) decades(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, genre, director, year, song, average ratings, character, plot, trailer, rating, actor, review and O.\nSentence: name a rick bieber directed family film rated pg 13 from the past nine decades","prediction_output":null,"prediction_outputs":null,"group":null,"words":["name","a","rick","bieber","directed","family","film","rated","pg","13","from","the","past","nine","decades"],"labels":["O","O","B-director","I-director","O","B-genre","O","O","B-rating","I-rating","O","O","B-year","I-year","I-year"],"target_index":null,"target_label":null},"label_list":["title","genre","director","year","song","average_ratings","character","plot","trailer","rating","actor","review"]}
{"id":"1728","dataset":"mit-movie","split":"test","instance":{"id":"1728","prompt_labels":"name(O) a(O) film(O) that(O) stars(O) deidre(B-actor) hall(I-actor) and(O) was(O) released(O) in(O) the(O) past(B-year) six(I-year) decades(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, genre, trailer, review, title, actor, average ratings, rating, director, song, character, year and O.\nSentence: name a film that stars deidre hall and was released in the past six decades","prediction_output":null,"prediction_outputs":null,"group":null,"words":["name","a","film","that","stars","deidre","hall","and","was","released","in","the","past","six","decades"],"labels":["O","O","O","O","O","B-actor","I-actor","O","O","O","O","O","B-year","I-year","I-year"],"target_index":null,"target_label":null},"label_list":["plot","genre","trailer","review","title","actor","average_ratings","rating","director","song","character","year"]}
{"id":"1751","dataset":"mit-movie","split":"test","instance":{"id":"1751","prompt_labels":"please(O) list(O) some(O) mediocre(B-average ratings) film(B-genre) noir(I-genre) movies(O) released(O) in(O) the(O) past(B-year) two(I-year) decades(I-year) directed(O) by(O) robert(B-director) hiltzik(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, actor, trailer, plot, review, title, character, year, rating, genre, song, director and O.\nSentence: please list some mediocre film noir movies released in the past two decades directed by robert hiltzik","prediction_output":null,"prediction_outputs":null,"group":null,"words":["please","list","some","mediocre","film","noir","movies","released","in","the","past","two","decades","directed","by","robert","hiltzik"],"labels":["O","O","O","B-average ratings","B-genre","I-genre","O","O","O","O","B-year","I-year","I-year","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["average_ratings","actor","trailer","plot","review","title","character","year","rating","genre","song","director"]}
{"id":"1758","dataset":"mit-movie","split":"test","instance":{"id":"1758","prompt_labels":"rose(B-actor) jackson(I-actor) starred(O) in(O) this(O) g(B-rating) rated(O) film(O) of(O) the(O) 2010(B-year) s(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, review, average ratings, plot, genre, song, trailer, character, director, rating, actor, title and O.\nSentence: rose jackson starred in this g rated film of the 2010 s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["rose","jackson","starred","in","this","g","rated","film","of","the","2010","s"],"labels":["B-actor","I-actor","O","O","O","B-rating","O","O","O","O","B-year","I-year"],"target_index":null,"target_label":null},"label_list":["year","review","average_ratings","plot","genre","song","trailer","character","director","rating","actor","title"]}
{"id":"1759","dataset":"mit-movie","split":"test","instance":{"id":"1759","prompt_labels":"show(O) a(O) family(B-genre) movie(O) with(O) a(O) ratings(O) average(O) of(O) five(B-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, average ratings, review, genre, rating, director, character, song, year, plot, title, trailer and O.\nSentence: show a family movie with a ratings average of five","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","a","family","movie","with","a","ratings","average","of","five"],"labels":["O","O","B-genre","O","O","O","O","O","O","B-average ratings"],"target_index":null,"target_label":null},"label_list":["actor","average_ratings","review","genre","rating","director","character","song","year","plot","title","trailer"]}
{"id":"1760","dataset":"mit-movie","split":"test","instance":{"id":"1760","prompt_labels":"show(O) me(O) all(O) action(B-genre) films(O) that(O) were(O) rated(O) very(B-average ratings) good(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, trailer, character, director, genre, actor, title, average ratings, plot, year, review, rating and O.\nSentence: show me all action films that were rated very good","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","all","action","films","that","were","rated","very","good"],"labels":["O","O","O","B-genre","O","O","O","O","B-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["song","trailer","character","director","genre","actor","title","average_ratings","plot","year","review","rating"]}
{"id":"1761","dataset":"mit-movie","split":"test","instance":{"id":"1761","prompt_labels":"show(O) me(O) all(O) unrated(B-rating) scary(B-genre) moves(O) about(O) gore(B-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, character, rating, plot, year, review, genre, trailer, average ratings, director, actor, title and O.\nSentence: show me all unrated scary moves about gore","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","all","unrated","scary","moves","about","gore"],"labels":["O","O","O","B-rating","B-genre","O","O","B-plot"],"target_index":null,"target_label":null},"label_list":["song","character","rating","plot","year","review","genre","trailer","average_ratings","director","actor","title"]}
{"id":"1765","dataset":"mit-movie","split":"test","instance":{"id":"1765","prompt_labels":"tell(O) me(O) about(O) green(B-title) lantern(I-title) emerald(I-title) knights(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, director, average ratings, genre, actor, title, rating, song, trailer, review, character, plot and O.\nSentence: tell me about green lantern emerald knights","prediction_output":null,"prediction_outputs":null,"group":null,"words":["tell","me","about","green","lantern","emerald","knights"],"labels":["O","O","O","B-title","I-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["year","director","average_ratings","genre","actor","title","rating","song","trailer","review","character","plot"]}
{"id":"1775","dataset":"mit-movie","split":"test","instance":{"id":"1775","prompt_labels":"the(O) actor(O) brad(B-actor) renfro(I-actor) starred(O) in(O) the(O) highly(B-average ratings) recommended(I-average ratings) film(B-genre) noir(I-genre) in(O) the(O) 1960(B-year) s(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, director, review, trailer, genre, character, song, plot, average ratings, year, title, actor and O.\nSentence: the actor brad renfro starred in the highly recommended film noir in the 1960 s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["the","actor","brad","renfro","starred","in","the","highly","recommended","film","noir","in","the","1960","s"],"labels":["O","O","B-actor","I-actor","O","O","O","B-average ratings","I-average ratings","B-genre","I-genre","O","O","B-year","I-year"],"target_index":null,"target_label":null},"label_list":["rating","director","review","trailer","genre","character","song","plot","average_ratings","year","title","actor"]}
{"id":"1785","dataset":"mit-movie","split":"test","instance":{"id":"1785","prompt_labels":"wanted(O) to(O) know(O) if(O) in(O) the(O) past(B-year) two(I-year) years(I-year) has(O) there(O) been(O) an(O) all(B-average ratings) right(I-average ratings) action(B-genre) movie(O) that(O) came(O) to(O) have(O) a(O) standoff(B-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, song, plot, title, trailer, average ratings, actor, character, review, rating, genre, director and O.\nSentence: wanted to know if in the past two years has there been an all right action movie that came to have a standoff","prediction_output":null,"prediction_outputs":null,"group":null,"words":["wanted","to","know","if","in","the","past","two","years","has","there","been","an","all","right","action","movie","that","came","to","have","a","standoff"],"labels":["O","O","O","O","O","O","B-year","I-year","I-year","O","O","O","O","B-average ratings","I-average ratings","B-genre","O","O","O","O","O","O","B-plot"],"target_index":null,"target_label":null},"label_list":["year","song","plot","title","trailer","average_ratings","actor","character","review","rating","genre","director"]}
{"id":"1789","dataset":"mit-movie","split":"test","instance":{"id":"1789","prompt_labels":"was(O) cate(B-actor) blanchett(I-actor) in(O) any(O) independent(B-genre) films(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, review, genre, title, average ratings, trailer, actor, director, character, rating, plot, song and O.\nSentence: was cate blanchett in any independent films","prediction_output":null,"prediction_outputs":null,"group":null,"words":["was","cate","blanchett","in","any","independent","films"],"labels":["O","B-actor","I-actor","O","O","B-genre","O"],"target_index":null,"target_label":null},"label_list":["year","review","genre","title","average_ratings","trailer","actor","director","character","rating","plot","song"]}
{"id":"1806","dataset":"mit-movie","split":"test","instance":{"id":"1806","prompt_labels":"was(O) vivien(B-actor) leigh(I-actor) in(O) a(O) musical(B-genre) that(O) was(O) rated(O) r(B-rating) and(O) received(O) nine(B-average ratings) stars(I-average ratings) and(I-average ratings) above(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, actor, genre, song, rating, character, review, average ratings, title, director, plot, trailer and O.\nSentence: was vivien leigh in a musical that was rated r and received nine stars and above","prediction_output":null,"prediction_outputs":null,"group":null,"words":["was","vivien","leigh","in","a","musical","that","was","rated","r","and","received","nine","stars","and","above"],"labels":["O","B-actor","I-actor","O","O","B-genre","O","O","O","B-rating","O","O","B-average ratings","I-average ratings","I-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["year","actor","genre","song","rating","character","review","average_ratings","title","director","plot","trailer"]}
{"id":"1827","dataset":"mit-movie","split":"test","instance":{"id":"1827","prompt_labels":"what(O) 1980(B-year) s(I-year) horror(B-genre) movie(O) starred(O) james(B-actor) cagney(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, trailer, song, director, title, rating, character, average ratings, plot, actor, review, year and O.\nSentence: what 1980 s horror movie starred james cagney","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","1980","s","horror","movie","starred","james","cagney"],"labels":["O","B-year","I-year","B-genre","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["genre","trailer","song","director","title","rating","character","average_ratings","plot","actor","review","year"]}
{"id":"1830","dataset":"mit-movie","split":"test","instance":{"id":"1830","prompt_labels":"what(O) 2000(B-year) highly(B-average ratings) recommended(I-average ratings) kristen(B-actor) scott(I-actor) thomas(I-actor) movie(O) about(O) a(O) runaway(B-plot) was(O) rated(O) nc(B-rating) 17(I-rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, plot, actor, year, character, review, director, average ratings, rating, genre, trailer, title and O.\nSentence: what 2000 highly recommended kristen scott thomas movie about a runaway was rated nc 17","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","2000","highly","recommended","kristen","scott","thomas","movie","about","a","runaway","was","rated","nc","17"],"labels":["O","B-year","B-average ratings","I-average ratings","B-actor","I-actor","I-actor","O","O","O","B-plot","O","O","B-rating","I-rating"],"target_index":null,"target_label":null},"label_list":["song","plot","actor","year","character","review","director","average_ratings","rating","genre","trailer","title"]}
{"id":"1832","dataset":"mit-movie","split":"test","instance":{"id":"1832","prompt_labels":"what(O) 2010(B-year) movies(O) did(O) robert(B-actor) evan(I-actor) star(O) in(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, actor, trailer, song, title, rating, review, year, director, genre, plot, character and O.\nSentence: what 2010 movies did robert evan star in","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","2010","movies","did","robert","evan","star","in"],"labels":["O","B-year","O","O","B-actor","I-actor","O","O"],"target_index":null,"target_label":null},"label_list":["average_ratings","actor","trailer","song","title","rating","review","year","director","genre","plot","character"]}
{"id":"1835","dataset":"mit-movie","split":"test","instance":{"id":"1835","prompt_labels":"what(O) bentley(B-director) dean(I-director) kid(B-genre) movie(O) in(O) the(O) last(B-year) seven(I-year) years(I-year) received(O) excellent(B-average ratings) ratings(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, director, review, title, trailer, actor, song, year, character, rating, genre, average ratings and O.\nSentence: what bentley dean kid movie in the last seven years received excellent ratings","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","bentley","dean","kid","movie","in","the","last","seven","years","received","excellent","ratings"],"labels":["O","B-director","I-director","B-genre","O","O","O","B-year","I-year","I-year","O","B-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["plot","director","review","title","trailer","actor","song","year","character","rating","genre","average_ratings"]}
{"id":"1868","dataset":"mit-movie","split":"test","instance":{"id":"1868","prompt_labels":"what(O) r(B-rating) rated(O) western(B-genre) with(O) a(O) rating(O) of(O) eight(B-average ratings) that(O) was(O) directed(O) by(O) ken(B-director) wheat(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, plot, trailer, rating, average ratings, actor, song, review, character, year, title, genre and O.\nSentence: what r rated western with a rating of eight that was directed by ken wheat","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","r","rated","western","with","a","rating","of","eight","that","was","directed","by","ken","wheat"],"labels":["O","B-rating","O","B-genre","O","O","O","O","B-average ratings","O","O","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["director","plot","trailer","rating","average_ratings","actor","song","review","character","year","title","genre"]}
{"id":"1876","dataset":"mit-movie","split":"test","instance":{"id":"1876","prompt_labels":"what(O) western(B-genre) movies(O) are(O) there(O) from(O) the(O) 2010(B-year) s(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, review, title, plot, song, character, rating, average ratings, year, genre, actor, director and O.\nSentence: what western movies are there from the 2010 s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","western","movies","are","there","from","the","2010","s"],"labels":["O","B-genre","O","O","O","O","O","B-year","I-year"],"target_index":null,"target_label":null},"label_list":["trailer","review","title","plot","song","character","rating","average_ratings","year","genre","actor","director"]}
{"id":"1890","dataset":"mit-movie","split":"test","instance":{"id":"1890","prompt_labels":"what(O) are(O) some(O) pg(B-rating) 13(I-rating) thrillers(B-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, song, character, average ratings, actor, rating, review, title, year, genre, plot, trailer and O.\nSentence: what are some pg 13 thrillers","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","some","pg","13","thrillers"],"labels":["O","O","O","B-rating","I-rating","B-genre"],"target_index":null,"target_label":null},"label_list":["director","song","character","average_ratings","actor","rating","review","title","year","genre","plot","trailer"]}
{"id":"1893","dataset":"mit-movie","split":"test","instance":{"id":"1893","prompt_labels":"what(O) are(O) some(O) r(B-rating) rated(O) gangster(B-genre) movies(O) from(O) the(O) 1990(B-year) s(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, trailer, genre, director, character, rating, average ratings, title, actor, plot, year and O.\nSentence: what are some r rated gangster movies from the 1990 s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","some","r","rated","gangster","movies","from","the","1990","s"],"labels":["O","O","O","B-rating","O","B-genre","O","O","O","B-year","I-year"],"target_index":null,"target_label":null},"label_list":["review","song","trailer","genre","director","character","rating","average_ratings","title","actor","plot","year"]}
{"id":"1897","dataset":"mit-movie","split":"test","instance":{"id":"1897","prompt_labels":"what(O) are(O) some(O) action(B-genre) films(O) that(O) are(O) pg(B-rating) 13(I-rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, title, rating, trailer, year, genre, plot, song, director, character, average ratings, actor and O.\nSentence: what are some action films that are pg 13","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","some","action","films","that","are","pg","13"],"labels":["O","O","O","B-genre","O","O","O","B-rating","I-rating"],"target_index":null,"target_label":null},"label_list":["review","title","rating","trailer","year","genre","plot","song","director","character","average_ratings","actor"]}
{"id":"1901","dataset":"mit-movie","split":"test","instance":{"id":"1901","prompt_labels":"what(O) are(O) some(O) classic(O) military(B-genre) movies(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, actor, review, genre, song, year, character, plot, rating, title, average ratings, director and O.\nSentence: what are some classic military movies","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","some","classic","military","movies"],"labels":["O","O","O","O","B-genre","O"],"target_index":null,"target_label":null},"label_list":["trailer","actor","review","genre","song","year","character","plot","rating","title","average_ratings","director"]}
{"id":"1905","dataset":"mit-movie","split":"test","instance":{"id":"1905","prompt_labels":"what(O) are(O) some(O) fantasy(B-genre) movies(O) in(O) the(O) past(B-year) six(I-year) years(I-year) directed(O) by(O) corey(B-director) yuen(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, song, rating, title, trailer, director, year, character, review, genre, actor, average ratings and O.\nSentence: what are some fantasy movies in the past six years directed by corey yuen","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","some","fantasy","movies","in","the","past","six","years","directed","by","corey","yuen"],"labels":["O","O","O","B-genre","O","O","O","B-year","I-year","I-year","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["plot","song","rating","title","trailer","director","year","character","review","genre","actor","average_ratings"]}
{"id":"1911","dataset":"mit-movie","split":"test","instance":{"id":"1911","prompt_labels":"what(O) are(O) some(O) must(B-average ratings) see(I-average ratings) westerns(B-genre) from(O) the(O) 1950(B-year) s(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, director, character, trailer, song, year, genre, rating, actor, title, review, plot and O.\nSentence: what are some must see westerns from the 1950 s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","some","must","see","westerns","from","the","1950","s"],"labels":["O","O","O","B-average ratings","I-average ratings","B-genre","O","O","B-year","I-year"],"target_index":null,"target_label":null},"label_list":["average_ratings","director","character","trailer","song","year","genre","rating","actor","title","review","plot"]}
{"id":"1917","dataset":"mit-movie","split":"test","instance":{"id":"1917","prompt_labels":"what(O) are(O) some(O) titles(O) of(O) animation(B-genre) films(O) directed(O) by(O) tomm(B-director) coker(I-director) that(O) received(O) average(B-average ratings) ratings(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, review, actor, plot, trailer, year, genre, song, average ratings, title, rating, director and O.\nSentence: what are some titles of animation films directed by tomm coker that received average ratings","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","some","titles","of","animation","films","directed","by","tomm","coker","that","received","average","ratings"],"labels":["O","O","O","O","O","B-genre","O","O","O","B-director","I-director","O","O","B-average ratings","O"],"target_index":null,"target_label":null},"label_list":["character","review","actor","plot","trailer","year","genre","song","average_ratings","title","rating","director"]}
{"id":"1933","dataset":"mit-movie","split":"test","instance":{"id":"1933","prompt_labels":"what(O) are(O) the(O) worst(O) sci(B-genre) fi(I-genre) movies(O) rated(O) pg(B-rating) 13(I-rating) in(O) the(O) last(B-year) seven(I-year) decades(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, song, rating, average ratings, title, director, review, actor, genre, plot, trailer, year and O.\nSentence: what are the worst sci fi movies rated pg 13 in the last seven decades","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","the","worst","sci","fi","movies","rated","pg","13","in","the","last","seven","decades"],"labels":["O","O","O","O","B-genre","I-genre","O","O","B-rating","I-rating","O","O","B-year","I-year","I-year"],"target_index":null,"target_label":null},"label_list":["character","song","rating","average_ratings","title","director","review","actor","genre","plot","trailer","year"]}
{"id":"1935","dataset":"mit-movie","split":"test","instance":{"id":"1935","prompt_labels":"what(O) avante(B-genre) garde(I-genre) film(O) is(O) the(O) director(O) orson(B-director) welles(I-director) best(O) known(O) for(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, director, average ratings, rating, actor, genre, title, trailer, plot, character, year, song and O.\nSentence: what avante garde film is the director orson welles best known for","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","avante","garde","film","is","the","director","orson","welles","best","known","for"],"labels":["O","B-genre","I-genre","O","O","O","O","B-director","I-director","O","O","O"],"target_index":null,"target_label":null},"label_list":["review","director","average_ratings","rating","actor","genre","title","trailer","plot","character","year","song"]}
{"id":"1953","dataset":"mit-movie","split":"test","instance":{"id":"1953","prompt_labels":"what(O) fantasy(B-genre) movies(O) were(O) released(O) in(O) 2011(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, character, actor, director, title, genre, review, trailer, average ratings, plot, rating, song and O.\nSentence: what fantasy movies were released in 2011","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","fantasy","movies","were","released","in","2011"],"labels":["O","B-genre","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["year","character","actor","director","title","genre","review","trailer","average_ratings","plot","rating","song"]}
{"id":"1965","dataset":"mit-movie","split":"test","instance":{"id":"1965","prompt_labels":"what(O) good(O) romantic(B-genre) comedy(I-genre) can(O) i(O) watch(O) that(O) is(O) about(O) star(B-plot) crossed(I-plot) lovers(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, director, title, song, genre, actor, review, rating, year, trailer, average ratings, character and O.\nSentence: what good romantic comedy can i watch that is about star crossed lovers","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","good","romantic","comedy","can","i","watch","that","is","about","star","crossed","lovers"],"labels":["O","O","B-genre","I-genre","O","O","O","O","O","O","B-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["plot","director","title","song","genre","actor","review","rating","year","trailer","average_ratings","character"]}
{"id":"1970","dataset":"mit-movie","split":"test","instance":{"id":"1970","prompt_labels":"what(O) history(B-genre) movies(O) in(O) the(O) past(B-year) eight(I-year) decades(I-year) starred(O) george(B-actor) carlin(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, song, trailer, character, title, rating, director, plot, review, actor, year, genre and O.\nSentence: what history movies in the past eight decades starred george carlin","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","history","movies","in","the","past","eight","decades","starred","george","carlin"],"labels":["O","B-genre","O","O","O","B-year","I-year","I-year","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["average_ratings","song","trailer","character","title","rating","director","plot","review","actor","year","genre"]}
{"id":"1976","dataset":"mit-movie","split":"test","instance":{"id":"1976","prompt_labels":"what(O) is(O) eugene(B-title) about(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, character, plot, trailer, title, genre, actor, year, director, average ratings, song, review and O.\nSentence: what is eugene about","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","eugene","about"],"labels":["O","O","B-title","O"],"target_index":null,"target_label":null},"label_list":["rating","character","plot","trailer","title","genre","actor","year","director","average_ratings","song","review"]}
{"id":"1993","dataset":"mit-movie","split":"test","instance":{"id":"1993","prompt_labels":"what(O) is(O) a(O) 1990(B-year) rated(O) r(B-rating) film(O) staring(O) randolph(B-actor) mantooth(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, genre, trailer, song, rating, plot, title, actor, character, year, director, review and O.\nSentence: what is a 1990 rated r film staring randolph mantooth","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","a","1990","rated","r","film","staring","randolph","mantooth"],"labels":["O","O","O","B-year","O","B-rating","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["average_ratings","genre","trailer","song","rating","plot","title","actor","character","year","director","review"]}
{"id":"1994","dataset":"mit-movie","split":"test","instance":{"id":"1994","prompt_labels":"what(O) is(O) a(O) disney(B-genre) movie(O) about(O) a(O) runaway(B-plot) that(O) was(O) directed(O) by(O) brian(B-director) burns(I-director) in(O) 1980(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, genre, plot, year, rating, title, song, character, trailer, actor, average ratings, review and O.\nSentence: what is a disney movie about a runaway that was directed by brian burns in 1980","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","a","disney","movie","about","a","runaway","that","was","directed","by","brian","burns","in","1980"],"labels":["O","O","O","B-genre","O","O","O","B-plot","O","O","O","O","B-director","I-director","O","B-year"],"target_index":null,"target_label":null},"label_list":["director","genre","plot","year","rating","title","song","character","trailer","actor","average_ratings","review"]}
{"id":"1996","dataset":"mit-movie","split":"test","instance":{"id":"1996","prompt_labels":"what(O) is(O) a(O) pg(B-rating) thriller(B-genre) rated(O) seven(B-average ratings) stars(I-average ratings) directed(O) by(O) hironobu(B-director) sakaguchi(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, rating, title, plot, actor, year, song, average ratings, genre, review, character, trailer and O.\nSentence: what is a pg thriller rated seven stars directed by hironobu sakaguchi","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","a","pg","thriller","rated","seven","stars","directed","by","hironobu","sakaguchi"],"labels":["O","O","O","B-rating","B-genre","O","B-average ratings","I-average ratings","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["director","rating","title","plot","actor","year","song","average_ratings","genre","review","character","trailer"]}
{"id":"1997","dataset":"mit-movie","split":"test","instance":{"id":"1997","prompt_labels":"what(O) is(O) a(O) pg(B-rating) 13(I-rating) quenton(B-actor) tarantino(I-actor) movie(O) made(O) in(O) 1970(B-year) featuring(O) rejection(B-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, title, review, year, plot, character, song, trailer, director, actor, rating, average ratings and O.\nSentence: what is a pg 13 quenton tarantino movie made in 1970 featuring rejection","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","a","pg","13","quenton","tarantino","movie","made","in","1970","featuring","rejection"],"labels":["O","O","O","B-rating","I-rating","B-actor","I-actor","O","O","O","B-year","O","B-plot"],"target_index":null,"target_label":null},"label_list":["genre","title","review","year","plot","character","song","trailer","director","actor","rating","average_ratings"]}
{"id":"1998","dataset":"mit-movie","split":"test","instance":{"id":"1998","prompt_labels":"what(O) is(O) a(O) pg(B-rating) 13(I-rating) tracey(B-actor) nelson(I-actor) history(B-genre) movie(O) with(O) good(B-average ratings) rating(I-average ratings) from(O) the(O) 1970(B-year) s(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, song, plot, title, genre, character, actor, review, rating, average ratings, trailer, director and O.\nSentence: what is a pg 13 tracey nelson history movie with good rating from the 1970 s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","a","pg","13","tracey","nelson","history","movie","with","good","rating","from","the","1970","s"],"labels":["O","O","O","B-rating","I-rating","B-actor","I-actor","B-genre","O","O","B-average ratings","I-average ratings","O","O","B-year","I-year"],"target_index":null,"target_label":null},"label_list":["year","song","plot","title","genre","character","actor","review","rating","average_ratings","trailer","director"]}
{"id":"2002","dataset":"mit-movie","split":"test","instance":{"id":"2002","prompt_labels":"what(O) is(O) a(O) decent(B-average ratings) movie(O) that(O) stars(O) don(B-actor) johnson(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, trailer, average ratings, director, character, review, rating, title, actor, plot, song, genre and O.\nSentence: what is a decent movie that stars don johnson","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","a","decent","movie","that","stars","don","johnson"],"labels":["O","O","O","B-average ratings","O","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["year","trailer","average_ratings","director","character","review","rating","title","actor","plot","song","genre"]}
{"id":"2011","dataset":"mit-movie","split":"test","instance":{"id":"2011","prompt_labels":"what(O) is(O) a(O) good(O) 1990(B-year) s(I-year) romance(B-genre) movie(O) starring(O) kelsy(B-actor) grammer(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, average ratings, title, song, review, year, actor, genre, trailer, plot, character, rating and O.\nSentence: what is a good 1990 s romance movie starring kelsy grammer","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","a","good","1990","s","romance","movie","starring","kelsy","grammer"],"labels":["O","O","O","O","B-year","I-year","B-genre","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["director","average_ratings","title","song","review","year","actor","genre","trailer","plot","character","rating"]}
{"id":"2024","dataset":"mit-movie","split":"test","instance":{"id":"2024","prompt_labels":"what(O) is(O) a(O) history(B-genre) movie(O) starring(O) tommy(B-actor) lee(I-actor) jones(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, song, average ratings, review, actor, year, title, trailer, plot, rating, character, genre and O.\nSentence: what is a history movie starring tommy lee jones","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","a","history","movie","starring","tommy","lee","jones"],"labels":["O","O","O","B-genre","O","O","B-actor","I-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["director","song","average_ratings","review","actor","year","title","trailer","plot","rating","character","genre"]}
{"id":"2038","dataset":"mit-movie","split":"test","instance":{"id":"2038","prompt_labels":"what(O) is(O) a(O) summary(O) of(O) the(O) plot(O) of(O) the(B-title) shining(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, average ratings, review, director, trailer, plot, genre, character, title, actor, song, year and O.\nSentence: what is a summary of the plot of the shining","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","a","summary","of","the","plot","of","the","shining"],"labels":["O","O","O","O","O","O","O","O","B-title","I-title"],"target_index":null,"target_label":null},"label_list":["rating","average_ratings","review","director","trailer","plot","genre","character","title","actor","song","year"]}
{"id":"2051","dataset":"mit-movie","split":"test","instance":{"id":"2051","prompt_labels":"what(O) is(O) an(O) animated(B-genre) movie(O) with(O) a(O) five(B-average ratings) star(I-average ratings) rating(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, plot, rating, average ratings, song, director, genre, year, character, actor, trailer, review and O.\nSentence: what is an animated movie with a five star rating","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","an","animated","movie","with","a","five","star","rating"],"labels":["O","O","O","B-genre","O","O","O","B-average ratings","I-average ratings","O"],"target_index":null,"target_label":null},"label_list":["title","plot","rating","average_ratings","song","director","genre","year","character","actor","trailer","review"]}
{"id":"2064","dataset":"mit-movie","split":"test","instance":{"id":"2064","prompt_labels":"what(O) is(O) the(O) best(O) movie(O) starring(O) daniela(B-actor) pestova(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, year, trailer, song, genre, rating, average ratings, actor, character, title, director, review and O.\nSentence: what is the best movie starring daniela pestova","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","best","movie","starring","daniela","pestova"],"labels":["O","O","O","O","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["plot","year","trailer","song","genre","rating","average_ratings","actor","character","title","director","review"]}
{"id":"2070","dataset":"mit-movie","split":"test","instance":{"id":"2070","prompt_labels":"what(O) is(O) the(O) last(O) science(B-genre) fiction(I-genre) film(O) that(O) was(O) rated(O) r(B-rating) and(O) directed(O) by(O) hayao(B-director) miyazaki(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, genre, director, song, year, actor, plot, trailer, average ratings, title, review, character and O.\nSentence: what is the last science fiction film that was rated r and directed by hayao miyazaki","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","last","science","fiction","film","that","was","rated","r","and","directed","by","hayao","miyazaki"],"labels":["O","O","O","O","B-genre","I-genre","O","O","O","O","B-rating","O","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["rating","genre","director","song","year","actor","plot","trailer","average_ratings","title","review","character"]}
{"id":"2082","dataset":"mit-movie","split":"test","instance":{"id":"2082","prompt_labels":"what(O) is(O) the(O) movie(O) easy(B-title) a(I-title) about(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, genre, song, trailer, review, rating, character, title, director, plot, year, actor and O.\nSentence: what is the movie easy a about","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","movie","easy","a","about"],"labels":["O","O","O","O","B-title","I-title","O"],"target_index":null,"target_label":null},"label_list":["average_ratings","genre","song","trailer","review","rating","character","title","director","plot","year","actor"]}
{"id":"2092","dataset":"mit-movie","split":"test","instance":{"id":"2092","prompt_labels":"what(O) is(O) the(O) name(O) of(O) a(O) movie(O) for(O) children(B-genre) directed(O) by(O) robert(B-director) zemeckis(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, review, actor, song, year, director, trailer, title, character, plot, average ratings, genre and O.\nSentence: what is the name of a movie for children directed by robert zemeckis","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","name","of","a","movie","for","children","directed","by","robert","zemeckis"],"labels":["O","O","O","O","O","O","O","O","B-genre","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["rating","review","actor","song","year","director","trailer","title","character","plot","average_ratings","genre"]}
{"id":"2102","dataset":"mit-movie","split":"test","instance":{"id":"2102","prompt_labels":"what(O) is(O) the(O) name(O) of(O) the(O) psychological(B-genre) movie(O) about(O) a(O) lawyer(B-plot) starring(O) david(B-actor) mccallum(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, director, review, song, actor, rating, average ratings, plot, character, title, year, trailer and O.\nSentence: what is the name of the psychological movie about a lawyer starring david mccallum","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","name","of","the","psychological","movie","about","a","lawyer","starring","david","mccallum"],"labels":["O","O","O","O","O","O","B-genre","O","O","O","B-plot","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["genre","director","review","song","actor","rating","average_ratings","plot","character","title","year","trailer"]}
{"id":"2104","dataset":"mit-movie","split":"test","instance":{"id":"2104","prompt_labels":"what(O) is(O) the(O) name(O) of(O) the(O) unrated(B-rating) military(B-genre) movie(O) about(O) d(B-plot) day(I-plot) released(O) in(O) 2010(B-year) directed(O) by(O) peter(B-director) kuran(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, actor, director, review, average ratings, title, plot, year, trailer, genre, character, rating and O.\nSentence: what is the name of the unrated military movie about d day released in 2010 directed by peter kuran","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","name","of","the","unrated","military","movie","about","d","day","released","in","2010","directed","by","peter","kuran"],"labels":["O","O","O","O","O","O","B-rating","B-genre","O","O","B-plot","I-plot","O","O","B-year","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["song","actor","director","review","average_ratings","title","plot","year","trailer","genre","character","rating"]}
{"id":"2112","dataset":"mit-movie","split":"test","instance":{"id":"2112","prompt_labels":"what(O) is(O) the(O) rated(O) r(B-rating) movie(O) that(O) marleen(B-director) gorris(I-director) directed(O) with(O) the(O) emotional(B-genre) decision(B-plot) aspect(O) to(O) it(O) receiving(O) an(O) average(B-average ratings) rating(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, year, review, title, average ratings, song, genre, trailer, plot, actor, character, director and O.\nSentence: what is the rated r movie that marleen gorris directed with the emotional decision aspect to it receiving an average rating","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","rated","r","movie","that","marleen","gorris","directed","with","the","emotional","decision","aspect","to","it","receiving","an","average","rating"],"labels":["O","O","O","O","B-rating","O","O","B-director","I-director","O","O","O","B-genre","B-plot","O","O","O","O","O","B-average ratings","O"],"target_index":null,"target_label":null},"label_list":["rating","year","review","title","average_ratings","song","genre","trailer","plot","actor","character","director"]}
{"id":"2133","dataset":"mit-movie","split":"test","instance":{"id":"2133","prompt_labels":"what(O) movie(O) came(O) out(O) in(O) the(O) last(B-year) four(I-year) years(I-year) and(O) is(O) about(O) new(B-plot) york(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, review, character, average ratings, trailer, director, song, plot, genre, rating, year, actor and O.\nSentence: what movie came out in the last four years and is about new york","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","movie","came","out","in","the","last","four","years","and","is","about","new","york"],"labels":["O","O","O","O","O","O","B-year","I-year","I-year","O","O","O","B-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["title","review","character","average_ratings","trailer","director","song","plot","genre","rating","year","actor"]}
{"id":"2135","dataset":"mit-movie","split":"test","instance":{"id":"2135","prompt_labels":"what(O) movie(O) did(O) george(B-director) lucas(I-director) direct(O) in(O) the(O) 1980(B-year) s(I-year) that(O) is(O) a(O) comedy(B-genre) and(O) rated(O) pg(B-rating) 13(I-rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, average ratings, actor, genre, director, title, year, song, review, character, plot, trailer and O.\nSentence: what movie did george lucas direct in the 1980 s that is a comedy and rated pg 13","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","movie","did","george","lucas","direct","in","the","1980","s","that","is","a","comedy","and","rated","pg","13"],"labels":["O","O","O","B-director","I-director","O","O","O","B-year","I-year","O","O","O","B-genre","O","O","B-rating","I-rating"],"target_index":null,"target_label":null},"label_list":["rating","average_ratings","actor","genre","director","title","year","song","review","character","plot","trailer"]}
{"id":"2149","dataset":"mit-movie","split":"test","instance":{"id":"2149","prompt_labels":"what(O) movies(O) has(O) fritz(B-director) lang(I-director) directed(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, song, director, trailer, review, plot, average ratings, actor, rating, character, title, genre and O.\nSentence: what movies has fritz lang directed","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","movies","has","fritz","lang","directed"],"labels":["O","O","O","B-director","I-director","O"],"target_index":null,"target_label":null},"label_list":["year","song","director","trailer","review","plot","average_ratings","actor","rating","character","title","genre"]}
{"id":"2167","dataset":"mit-movie","split":"test","instance":{"id":"2167","prompt_labels":"what(O) prison(B-plot) movies(O) have(O) at(O) least(O) a(O) seven(B-average ratings) star(I-average ratings) rating(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, genre, year, character, rating, average ratings, review, director, title, actor, song, plot and O.\nSentence: what prison movies have at least a seven star rating","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","prison","movies","have","at","least","a","seven","star","rating"],"labels":["O","B-plot","O","O","O","O","O","B-average ratings","I-average ratings","O"],"target_index":null,"target_label":null},"label_list":["trailer","genre","year","character","rating","average_ratings","review","director","title","actor","song","plot"]}
{"id":"2176","dataset":"mit-movie","split":"test","instance":{"id":"2176","prompt_labels":"what(O) rating(O) is(O) the(O) movie(O) busting(B-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, review, character, trailer, plot, rating, year, genre, average ratings, song, director, actor and O.\nSentence: what rating is the movie busting","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","rating","is","the","movie","busting"],"labels":["O","O","O","O","O","B-title"],"target_index":null,"target_label":null},"label_list":["title","review","character","trailer","plot","rating","year","genre","average_ratings","song","director","actor"]}
{"id":"2194","dataset":"mit-movie","split":"test","instance":{"id":"2194","prompt_labels":"what(O) thriller(B-genre) film(O) did(O) best(O) at(O) the(O) box(O) office(O) in(O) 2011(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, title, review, director, character, trailer, year, average ratings, actor, plot, genre, song and O.\nSentence: what thriller film did best at the box office in 2011","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","thriller","film","did","best","at","the","box","office","in","2011"],"labels":["O","B-genre","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["rating","title","review","director","character","trailer","year","average_ratings","actor","plot","genre","song"]}
{"id":"2197","dataset":"mit-movie","split":"test","instance":{"id":"2197","prompt_labels":"what(O) unrated(B-rating) movies(O) has(O) lena(B-actor) horne(I-actor) acted(O) in(O) within(O) the(O) last(B-year) six(I-year) decades(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, rating, actor, review, director, character, plot, song, title, genre, trailer, year and O.\nSentence: what unrated movies has lena horne acted in within the last six decades","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","unrated","movies","has","lena","horne","acted","in","within","the","last","six","decades"],"labels":["O","B-rating","O","O","B-actor","I-actor","O","O","O","O","B-year","I-year","I-year"],"target_index":null,"target_label":null},"label_list":["average_ratings","rating","actor","review","director","character","plot","song","title","genre","trailer","year"]}
{"id":"2198","dataset":"mit-movie","split":"test","instance":{"id":"2198","prompt_labels":"what(O) urban(B-plot) legend(I-plot) movies(O) did(O) martin(B-director) barnewitz(I-director) direct(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, year, review, song, genre, actor, average ratings, title, director, character, trailer, rating and O.\nSentence: what urban legend movies did martin barnewitz direct","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","urban","legend","movies","did","martin","barnewitz","direct"],"labels":["O","B-plot","I-plot","O","O","B-director","I-director","O"],"target_index":null,"target_label":null},"label_list":["plot","year","review","song","genre","actor","average_ratings","title","director","character","trailer","rating"]}
{"id":"2202","dataset":"mit-movie","split":"test","instance":{"id":"2202","prompt_labels":"what(O) was(O) planet(B-title) of(I-title) the(I-title) vampires(I-title) about(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, actor, song, title, review, plot, year, average ratings, genre, director, character, trailer and O.\nSentence: what was planet of the vampires about","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","planet","of","the","vampires","about"],"labels":["O","O","B-title","I-title","I-title","I-title","O"],"target_index":null,"target_label":null},"label_list":["rating","actor","song","title","review","plot","year","average_ratings","genre","director","character","trailer"]}
{"id":"2203","dataset":"mit-movie","split":"test","instance":{"id":"2203","prompt_labels":"what(O) was(O) the(B-title) woods(I-title) have(I-title) eyes(I-title) rated(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, rating, year, song, genre, plot, review, trailer, title, average ratings, character, director and O.\nSentence: what was the woods have eyes rated","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","woods","have","eyes","rated"],"labels":["O","O","B-title","I-title","I-title","I-title","O"],"target_index":null,"target_label":null},"label_list":["actor","rating","year","song","genre","plot","review","trailer","title","average_ratings","character","director"]}
{"id":"2214","dataset":"mit-movie","split":"test","instance":{"id":"2214","prompt_labels":"what(O) was(O) the(O) nc(B-rating) 17(I-rating) crime(B-genre) movie(O) that(O) antony(B-director) szeto(I-director) directed(O) that(O) got(O) good(B-average ratings) ratings(I-average ratings) and(O) was(O) about(O) missing(B-plot) people(I-plot) that(O) came(O) out(O) in(O) the(O) 1960(B-year) s(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, plot, director, trailer, title, genre, average ratings, year, song, review, rating, character and O.\nSentence: what was the nc 17 crime movie that antony szeto directed that got good ratings and was about missing people that came out in the 1960 s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","nc","17","crime","movie","that","antony","szeto","directed","that","got","good","ratings","and","was","about","missing","people","that","came","out","in","the","1960","s"],"labels":["O","O","O","B-rating","I-rating","B-genre","O","O","B-director","I-director","O","O","O","B-average ratings","I-average ratings","O","O","O","B-plot","I-plot","O","O","O","O","O","B-year","I-year"],"target_index":null,"target_label":null},"label_list":["actor","plot","director","trailer","title","genre","average_ratings","year","song","review","rating","character"]}
{"id":"2217","dataset":"mit-movie","split":"test","instance":{"id":"2217","prompt_labels":"what(O) was(O) the(O) best(O) mystery(B-genre) film(O) released(O) in(O) the(O) last(B-year) seven(I-year) years(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, trailer, genre, actor, year, plot, director, rating, title, average ratings, song, review and O.\nSentence: what was the best mystery film released in the last seven years","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","best","mystery","film","released","in","the","last","seven","years"],"labels":["O","O","O","O","B-genre","O","O","O","O","B-year","I-year","I-year"],"target_index":null,"target_label":null},"label_list":["character","trailer","genre","actor","year","plot","director","rating","title","average_ratings","song","review"]}
{"id":"2223","dataset":"mit-movie","split":"test","instance":{"id":"2223","prompt_labels":"what(O) was(O) the(O) last(O) science(B-genre) fiction(I-genre) movie(O) that(O) liam(B-actor) neeson(I-actor) was(O) in(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, actor, plot, character, genre, average ratings, director, review, rating, trailer, title, song and O.\nSentence: what was the last science fiction movie that liam neeson was in","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","last","science","fiction","movie","that","liam","neeson","was","in"],"labels":["O","O","O","O","B-genre","I-genre","O","O","B-actor","I-actor","O","O"],"target_index":null,"target_label":null},"label_list":["year","actor","plot","character","genre","average_ratings","director","review","rating","trailer","title","song"]}
{"id":"2232","dataset":"mit-movie","split":"test","instance":{"id":"2232","prompt_labels":"what(O) was(O) the(O) title(O) of(O) that(O) 1980(B-year) s(I-year) r(B-rating) rated(O) crime(B-genre) drama(O) with(O) james(B-actor) marsden(I-actor) which(O) some(O) critics(O) called(O) average(B-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, plot, year, trailer, average ratings, character, genre, review, actor, director, song, title and O.\nSentence: what was the title of that 1980 s r rated crime drama with james marsden which some critics called average","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","title","of","that","1980","s","r","rated","crime","drama","with","james","marsden","which","some","critics","called","average"],"labels":["O","O","O","O","O","O","B-year","I-year","B-rating","O","B-genre","O","O","B-actor","I-actor","O","O","O","O","B-average ratings"],"target_index":null,"target_label":null},"label_list":["rating","plot","year","trailer","average_ratings","character","genre","review","actor","director","song","title"]}
{"id":"2242","dataset":"mit-movie","split":"test","instance":{"id":"2242","prompt_labels":"what(O) year(O) did(O) young(B-title) sherlock(I-title) holmes(I-title) come(O) out(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, director, review, actor, character, plot, song, trailer, year, title, average ratings, rating and O.\nSentence: what year did young sherlock holmes come out","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","year","did","young","sherlock","holmes","come","out"],"labels":["O","O","O","B-title","I-title","I-title","O","O"],"target_index":null,"target_label":null},"label_list":["genre","director","review","actor","character","plot","song","trailer","year","title","average_ratings","rating"]}
{"id":"2250","dataset":"mit-movie","split":"test","instance":{"id":"2250","prompt_labels":"whats(O) a(O) nine(B-average ratings) star(I-average ratings) reviewed(O) movie(O) from(O) last(B-year) year(I-year) that(O) features(O) chris(B-actor) tucker(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, actor, title, review, plot, genre, director, year, rating, song, average ratings, character and O.\nSentence: whats a nine star reviewed movie from last year that features chris tucker","prediction_output":null,"prediction_outputs":null,"group":null,"words":["whats","a","nine","star","reviewed","movie","from","last","year","that","features","chris","tucker"],"labels":["O","O","B-average ratings","I-average ratings","O","O","O","B-year","I-year","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["trailer","actor","title","review","plot","genre","director","year","rating","song","average_ratings","character"]}
{"id":"2269","dataset":"mit-movie","split":"test","instance":{"id":"2269","prompt_labels":"when(O) was(O) the(O) movie(O) sculpture(B-title) released(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, actor, rating, year, director, review, plot, genre, character, title, song, trailer and O.\nSentence: when was the movie sculpture released","prediction_output":null,"prediction_outputs":null,"group":null,"words":["when","was","the","movie","sculpture","released"],"labels":["O","O","O","O","B-title","O"],"target_index":null,"target_label":null},"label_list":["average_ratings","actor","rating","year","director","review","plot","genre","character","title","song","trailer"]}
{"id":"2284","dataset":"mit-movie","split":"test","instance":{"id":"2284","prompt_labels":"where(O) is(O) the(O) movie(O) hachiko(B-title) a(I-title) dogs(I-title) story(I-title) set(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, character, song, genre, director, title, rating, actor, review, plot, average ratings, trailer and O.\nSentence: where is the movie hachiko a dogs story set","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","is","the","movie","hachiko","a","dogs","story","set"],"labels":["O","O","O","O","B-title","I-title","I-title","I-title","O"],"target_index":null,"target_label":null},"label_list":["year","character","song","genre","director","title","rating","actor","review","plot","average_ratings","trailer"]}
{"id":"2292","dataset":"mit-movie","split":"test","instance":{"id":"2292","prompt_labels":"which(O) documentary(B-genre) received(O) a(O) rating(O) of(O) six(B-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, rating, actor, title, trailer, character, average ratings, plot, review, genre, year, song and O.\nSentence: which documentary received a rating of six","prediction_output":null,"prediction_outputs":null,"group":null,"words":["which","documentary","received","a","rating","of","six"],"labels":["O","B-genre","O","O","O","O","B-average ratings"],"target_index":null,"target_label":null},"label_list":["director","rating","actor","title","trailer","character","average_ratings","plot","review","genre","year","song"]}
{"id":"2300","dataset":"mit-movie","split":"test","instance":{"id":"2300","prompt_labels":"who(O) directed(O) all(B-title) star(I-title) superman(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, song, title, genre, actor, review, average ratings, trailer, director, year, plot, rating and O.\nSentence: who directed all star superman","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","directed","all","star","superman"],"labels":["O","O","B-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["character","song","title","genre","actor","review","average_ratings","trailer","director","year","plot","rating"]}
{"id":"2315","dataset":"mit-movie","split":"test","instance":{"id":"2315","prompt_labels":"who(O) is(O) the(O) main(O) character(O) in(O) the(O) movie(O) tenshi(B-title) no(I-title) tamago(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, director, genre, actor, rating, year, title, song, plot, review, trailer, average ratings and O.\nSentence: who is the main character in the movie tenshi no tamago","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","is","the","main","character","in","the","movie","tenshi","no","tamago"],"labels":["O","O","O","O","O","O","O","O","B-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["character","director","genre","actor","rating","year","title","song","plot","review","trailer","average_ratings"]}
{"id":"2329","dataset":"mit-movie","split":"test","instance":{"id":"2329","prompt_labels":"who(O) stars(O) in(O) beatdown(B-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, average ratings, review, plot, song, director, rating, character, year, genre, title, actor and O.\nSentence: who stars in beatdown","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","stars","in","beatdown"],"labels":["O","O","O","B-title"],"target_index":null,"target_label":null},"label_list":["trailer","average_ratings","review","plot","song","director","rating","character","year","genre","title","actor"]}
{"id":"2331","dataset":"mit-movie","split":"test","instance":{"id":"2331","prompt_labels":"who(O) stars(O) in(O) boogeyman(B-title) 2(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, rating, title, trailer, director, year, song, review, character, plot, actor, genre and O.\nSentence: who stars in boogeyman 2","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","stars","in","boogeyman","2"],"labels":["O","O","O","B-title","I-title"],"target_index":null,"target_label":null},"label_list":["average_ratings","rating","title","trailer","director","year","song","review","character","plot","actor","genre"]}
{"id":"2354","dataset":"mit-movie","split":"test","instance":{"id":"2354","prompt_labels":"is(O) clark(B-actor) gable(I-actor) in(O) a(O) romantic(B-genre) comedy(I-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, review, trailer, actor, average ratings, year, rating, plot, genre, director, character, title and O.\nSentence: is clark gable in a romantic comedy","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","clark","gable","in","a","romantic","comedy"],"labels":["O","B-actor","I-actor","O","O","B-genre","I-genre"],"target_index":null,"target_label":null},"label_list":["song","review","trailer","actor","average_ratings","year","rating","plot","genre","director","character","title"]}
{"id":"2355","dataset":"mit-movie","split":"test","instance":{"id":"2355","prompt_labels":"is(O) francis(B-director) ford(I-director) coppola(I-director) the(O) director(O) for(O) the(B-title) shining(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, genre, year, rating, character, average ratings, review, trailer, title, actor, director, plot and O.\nSentence: is francis ford coppola the director for the shining","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","francis","ford","coppola","the","director","for","the","shining"],"labels":["O","B-director","I-director","I-director","O","O","O","B-title","I-title"],"target_index":null,"target_label":null},"label_list":["song","genre","year","rating","character","average_ratings","review","trailer","title","actor","director","plot"]}
{"id":"2378","dataset":"mit-movie","split":"test","instance":{"id":"2378","prompt_labels":"name(O) some(O) john(B-director) huston(I-director) films(O) about(O) children(B-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, rating, average ratings, title, year, song, director, trailer, character, plot, actor, genre and O.\nSentence: name some john huston films about children","prediction_output":null,"prediction_outputs":null,"group":null,"words":["name","some","john","huston","films","about","children"],"labels":["O","O","B-director","I-director","O","O","B-genre"],"target_index":null,"target_label":null},"label_list":["review","rating","average_ratings","title","year","song","director","trailer","character","plot","actor","genre"]}
{"id":"2382","dataset":"mit-movie","split":"test","instance":{"id":"2382","prompt_labels":"what(O) is(O) a(O) fantasy(B-genre) rated(O) pg(B-rating) starring(O) christian(B-actor) bale(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, review, character, plot, rating, actor, title, year, director, song, average ratings, trailer and O.\nSentence: what is a fantasy rated pg starring christian bale","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","a","fantasy","rated","pg","starring","christian","bale"],"labels":["O","O","O","B-genre","O","B-rating","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["genre","review","character","plot","rating","actor","title","year","director","song","average_ratings","trailer"]}
{"id":"2397","dataset":"mit-movie","split":"test","instance":{"id":"2397","prompt_labels":"can(O) i(O) see(O) the(O) trailer(B-trailer) for(O) kiss(B-title) me(I-title) kate(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, trailer, character, genre, review, average ratings, director, plot, song, year, rating, actor and O.\nSentence: can i see the trailer for kiss me kate","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","i","see","the","trailer","for","kiss","me","kate"],"labels":["O","O","O","O","B-trailer","O","B-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["title","trailer","character","genre","review","average_ratings","director","plot","song","year","rating","actor"]}
{"id":"2399","dataset":"mit-movie","split":"test","instance":{"id":"2399","prompt_labels":"where(O) can(O) i(O) see(O) some(O) short(O) clips(B-trailer) of(O) a(B-title) hard(I-title) day(I-title) s(I-title) night(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, director, character, trailer, genre, plot, review, song, actor, average ratings, title, rating and O.\nSentence: where can i see some short clips of a hard day s night","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","see","some","short","clips","of","a","hard","day","s","night"],"labels":["O","O","O","O","O","O","B-trailer","O","B-title","I-title","I-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["year","director","character","trailer","genre","plot","review","song","actor","average_ratings","title","rating"]}
{"id":"2401","dataset":"mit-movie","split":"test","instance":{"id":"2401","prompt_labels":"which(O) is(O) the(O) film(O) in(O) which(O) a(O) major(O) role(O) was(O) done(O) by(O) dr(B-character) evil(I-character) released(O) in(O) 1997(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, genre, year, rating, character, song, average ratings, title, actor, trailer, review, director and O.\nSentence: which is the film in which a major role was done by dr evil released in 1997","prediction_output":null,"prediction_outputs":null,"group":null,"words":["which","is","the","film","in","which","a","major","role","was","done","by","dr","evil","released","in","1997"],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-character","I-character","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["plot","genre","year","rating","character","song","average_ratings","title","actor","trailer","review","director"]}
{"id":"2411","dataset":"mit-movie","split":"test","instance":{"id":"2411","prompt_labels":"million(B-title) dollar(I-title) baby(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, review, rating, director, average ratings, trailer, plot, actor, year, song, title, genre and O.\nSentence: million dollar baby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["million","dollar","baby"],"labels":["B-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["character","review","rating","director","average_ratings","trailer","plot","actor","year","song","title","genre"]}
{"id":"2425","dataset":"mit-movie","split":"test","instance":{"id":"2425","prompt_labels":"lord(B-title) of(I-title) the(I-title) rings(I-title) the(I-title) return(I-title) of(I-title) the(I-title) king(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, trailer, character, review, year, title, rating, song, plot, director, genre, actor and O.\nSentence: lord of the rings the return of the king","prediction_output":null,"prediction_outputs":null,"group":null,"words":["lord","of","the","rings","the","return","of","the","king"],"labels":["B-title","I-title","I-title","I-title","I-title","I-title","I-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["average_ratings","trailer","character","review","year","title","rating","song","plot","director","genre","actor"]}
{"id":"2430","dataset":"mit-movie","split":"test","instance":{"id":"2430","prompt_labels":"city(B-title) of(I-title) god(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, year, actor, genre, director, character, plot, title, song, rating, trailer, average ratings and O.\nSentence: city of god","prediction_output":null,"prediction_outputs":null,"group":null,"words":["city","of","god"],"labels":["B-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["review","year","actor","genre","director","character","plot","title","song","rating","trailer","average_ratings"]}
{"id":"2436","dataset":"mit-movie","split":"test","instance":{"id":"2436","prompt_labels":"play(O) the(O) trailer(B-trailer) of(O) dark(B-title) of(I-title) the(I-title) moon(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, actor, director, review, genre, plot, trailer, character, rating, average ratings, song, title and O.\nSentence: play the trailer of dark of the moon","prediction_output":null,"prediction_outputs":null,"group":null,"words":["play","the","trailer","of","dark","of","the","moon"],"labels":["O","O","B-trailer","O","B-title","I-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["year","actor","director","review","genre","plot","trailer","character","rating","average_ratings","song","title"]}
{"id":"2","dataset":"mit-restaurant","split":"test","instance":{"id":"2","prompt_labels":"any(O) bbq(B-Cuisine) places(O) open(B-Hours) before(I-Hours) 5(I-Hours) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Price, Location, Hours, Amenity, Cuisine, Restaurant Name, Rating and O.\nSentence: any bbq places open before 5 nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["any","bbq","places","open","before","5","nearby"],"labels":["O","B-Cuisine","O","B-Hours","I-Hours","I-Hours","B-Location"],"target_index":null,"target_label":null},"label_list":["dish","price","location","hours","amenity","cuisine","restaurant_name","rating"]}
{"id":"3","dataset":"mit-restaurant","split":"test","instance":{"id":"3","prompt_labels":"any(O) dancing(B-Location) establishments(I-Location) with(O) reasonable(B-Price) pricing(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Rating, Restaurant Name, Price, Hours, Cuisine, Amenity and O.\nSentence: any dancing establishments with reasonable pricing","prediction_output":null,"prediction_outputs":null,"group":null,"words":["any","dancing","establishments","with","reasonable","pricing"],"labels":["O","B-Location","I-Location","O","B-Price","O"],"target_index":null,"target_label":null},"label_list":["dish","location","rating","restaurant_name","price","hours","cuisine","amenity"]}
{"id":"8","dataset":"mit-restaurant","split":"test","instance":{"id":"8","prompt_labels":"any(O) mexican(B-Cuisine) places(O) have(O) a(O) tameles(B-Dish) special(B-Amenity) today(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Price, Dish, Location, Restaurant Name, Rating, Hours, Cuisine and O.\nSentence: any mexican places have a tameles special today","prediction_output":null,"prediction_outputs":null,"group":null,"words":["any","mexican","places","have","a","tameles","special","today"],"labels":["O","B-Cuisine","O","O","O","B-Dish","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["amenity","price","dish","location","restaurant_name","rating","hours","cuisine"]}
{"id":"10","dataset":"mit-restaurant","split":"test","instance":{"id":"10","prompt_labels":"any(O) places(O) around(B-Location) here(I-Location) that(O) has(O) a(O) nice(B-Amenity) view(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Price, Location, Dish, Cuisine, Restaurant Name, Rating, Hours and O.\nSentence: any places around here that has a nice view","prediction_output":null,"prediction_outputs":null,"group":null,"words":["any","places","around","here","that","has","a","nice","view"],"labels":["O","O","B-Location","I-Location","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["amenity","price","location","dish","cuisine","restaurant_name","rating","hours"]}
{"id":"11","dataset":"mit-restaurant","split":"test","instance":{"id":"11","prompt_labels":"any(O) reasonably(B-Price) priced(O) indian(B-Cuisine) restaurants(O) in(B-Location) the(I-Location) theater(I-Location) district(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Price, Cuisine, Location, Rating, Dish, Hours, Restaurant Name and O.\nSentence: any reasonably priced indian restaurants in the theater district","prediction_output":null,"prediction_outputs":null,"group":null,"words":["any","reasonably","priced","indian","restaurants","in","the","theater","district"],"labels":["O","B-Price","O","B-Cuisine","O","B-Location","I-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["amenity","price","cuisine","location","rating","dish","hours","restaurant_name"]}
{"id":"18","dataset":"mit-restaurant","split":"test","instance":{"id":"18","prompt_labels":"are(O) reservations(O) available(O) for(O) four(O) people(O) for(O) 8(O) pm(O) tonight(O) at(O) 112(B-Restaurant Name) eatery(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Dish, Location, Hours, Cuisine, Rating, Restaurant Name and O.\nSentence: are reservations available for four people for 8 pm tonight at 112 eatery","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","reservations","available","for","four","people","for","8","pm","tonight","at","112","eatery"],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["price","amenity","dish","location","hours","cuisine","rating","restaurant_name"]}
{"id":"25","dataset":"mit-restaurant","split":"test","instance":{"id":"25","prompt_labels":"are(O) there(O) any(O) cafeterias(B-Cuisine) near(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Amenity, Hours, Price, Cuisine, Rating, Restaurant Name and O.\nSentence: are there any cafeterias near","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","cafeterias","near"],"labels":["O","O","O","B-Cuisine","B-Location"],"target_index":null,"target_label":null},"label_list":["dish","location","amenity","hours","price","cuisine","rating","restaurant_name"]}
{"id":"34","dataset":"mit-restaurant","split":"test","instance":{"id":"34","prompt_labels":"are(O) there(O) any(O) eatery(O) at(O) the(O) hotel(B-Location) downtown(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Rating, Location, Dish, Price, Cuisine, Restaurant Name and O.\nSentence: are there any eatery at the hotel downtown","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","eatery","at","the","hotel","downtown"],"labels":["O","O","O","O","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["amenity","hours","rating","location","dish","price","cuisine","restaurant_name"]}
{"id":"37","dataset":"mit-restaurant","split":"test","instance":{"id":"37","prompt_labels":"are(O) there(O) any(O) fast(B-Cuisine) food(I-Cuisine) joints(O) east(B-Location) of(I-Location) here(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Dish, Restaurant Name, Amenity, Hours, Rating, Price, Location and O.\nSentence: are there any fast food joints east of here","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","fast","food","joints","east","of","here"],"labels":["O","O","O","B-Cuisine","I-Cuisine","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["cuisine","dish","restaurant_name","amenity","hours","rating","price","location"]}
{"id":"41","dataset":"mit-restaurant","split":"test","instance":{"id":"41","prompt_labels":"are(O) there(O) any(O) four(B-Rating) star(I-Rating) restaurants(O) in(O) this(O) town(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Location, Amenity, Cuisine, Rating, Dish, Price, Restaurant Name and O.\nSentence: are there any four star restaurants in this town","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","four","star","restaurants","in","this","town"],"labels":["O","O","O","B-Rating","I-Rating","O","O","O","B-Location"],"target_index":null,"target_label":null},"label_list":["hours","location","amenity","cuisine","rating","dish","price","restaurant_name"]}
{"id":"43","dataset":"mit-restaurant","split":"test","instance":{"id":"43","prompt_labels":"are(O) there(O) any(O) good(O) family(B-Amenity) style(I-Amenity) restaurants(O) around(B-Location) boston(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Price, Restaurant Name, Dish, Cuisine, Hours, Location and O.\nSentence: are there any good family style restaurants around boston","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","good","family","style","restaurants","around","boston"],"labels":["O","O","O","O","B-Amenity","I-Amenity","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["amenity","rating","price","restaurant_name","dish","cuisine","hours","location"]}
{"id":"45","dataset":"mit-restaurant","split":"test","instance":{"id":"45","prompt_labels":"are(O) there(O) any(O) greek(B-Cuisine) restaurants(O) in(B-Location) the(I-Location) area(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Hours, Rating, Price, Location, Cuisine, Dish and O.\nSentence: are there any greek restaurants in the area","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","greek","restaurants","in","the","area"],"labels":["O","O","O","B-Cuisine","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["restaurant_name","amenity","hours","rating","price","location","cuisine","dish"]}
{"id":"51","dataset":"mit-restaurant","split":"test","instance":{"id":"51","prompt_labels":"are(O) there(O) any(O) italian(B-Cuisine) eateries(O) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Price, Rating, Location, Restaurant Name, Dish, Cuisine, Amenity and O.\nSentence: are there any italian eateries nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","italian","eateries","nearby"],"labels":["O","O","O","B-Cuisine","O","B-Location"],"target_index":null,"target_label":null},"label_list":["hours","price","rating","location","restaurant_name","dish","cuisine","amenity"]}
{"id":"53","dataset":"mit-restaurant","split":"test","instance":{"id":"53","prompt_labels":"are(O) there(O) any(O) jazz(B-Amenity) clubs(I-Amenity) that(O) serve(O) food(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Dish, Location, Restaurant Name, Amenity, Price, Cuisine, Hours and O.\nSentence: are there any jazz clubs that serve food","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","jazz","clubs","that","serve","food"],"labels":["O","O","O","B-Amenity","I-Amenity","O","O","O"],"target_index":null,"target_label":null},"label_list":["rating","dish","location","restaurant_name","amenity","price","cuisine","hours"]}
{"id":"54","dataset":"mit-restaurant","split":"test","instance":{"id":"54","prompt_labels":"are(O) there(O) any(O) kid(B-Amenity) friendly(I-Amenity) restaurants(O) close(B-Location) by(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Amenity, Location, Rating, Price, Dish, Restaurant Name, Hours and O.\nSentence: are there any kid friendly restaurants close by","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","kid","friendly","restaurants","close","by"],"labels":["O","O","O","B-Amenity","I-Amenity","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["cuisine","amenity","location","rating","price","dish","restaurant_name","hours"]}
{"id":"59","dataset":"mit-restaurant","split":"test","instance":{"id":"59","prompt_labels":"are(O) there(O) any(O) mid(B-Price) priced(O) restaurants(O) within(B-Location) 5(I-Location) miles(I-Location) that(O) offer(B-Amenity) curb(I-Amenity) side(I-Amenity) pickup(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Restaurant Name, Rating, Price, Hours, Cuisine, Location, Dish and O.\nSentence: are there any mid priced restaurants within 5 miles that offer curb side pickup","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","mid","priced","restaurants","within","5","miles","that","offer","curb","side","pickup"],"labels":["O","O","O","B-Price","O","O","B-Location","I-Location","I-Location","O","B-Amenity","I-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["amenity","restaurant_name","rating","price","hours","cuisine","location","dish"]}
{"id":"68","dataset":"mit-restaurant","split":"test","instance":{"id":"68","prompt_labels":"are(O) there(O) any(O) places(O) to(O) eat(O) in(B-Location) the(I-Location) area(I-Location) that(O) offer(O) a(O) two(B-Amenity) for(I-Amenity) one(I-Amenity) special(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Price, Amenity, Cuisine, Location, Rating, Dish and O.\nSentence: are there any places to eat in the area that offer a two for one special","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","places","to","eat","in","the","area","that","offer","a","two","for","one","special"],"labels":["O","O","O","O","O","O","B-Location","I-Location","I-Location","O","O","O","B-Amenity","I-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["restaurant_name","hours","price","amenity","cuisine","location","rating","dish"]}
{"id":"71","dataset":"mit-restaurant","split":"test","instance":{"id":"71","prompt_labels":"are(O) there(O) any(O) restaurant(O) nearby(B-Location) that(O) serve(O) thai(B-Cuisine) food(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Price, Dish, Cuisine, Rating, Location, Amenity and O.\nSentence: are there any restaurant nearby that serve thai food","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","restaurant","nearby","that","serve","thai","food"],"labels":["O","O","O","O","B-Location","O","O","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["restaurant_name","hours","price","dish","cuisine","rating","location","amenity"]}
{"id":"72","dataset":"mit-restaurant","split":"test","instance":{"id":"72","prompt_labels":"are(O) there(O) any(O) restaurants(O) around(O) with(O) a(O) smoking(B-Amenity) area(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Location, Dish, Amenity, Hours, Price, Restaurant Name and O.\nSentence: are there any restaurants around with a smoking area","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","restaurants","around","with","a","smoking","area"],"labels":["O","O","O","O","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["cuisine","rating","location","dish","amenity","hours","price","restaurant_name"]}
{"id":"74","dataset":"mit-restaurant","split":"test","instance":{"id":"74","prompt_labels":"are(O) there(O) any(O) restaurants(O) nearby(B-Location) that(O) have(O) great(B-Rating) reviews(I-Rating) and(O) plenty(B-Amenity) of(I-Amenity) parking(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Cuisine, Amenity, Hours, Restaurant Name, Price, Rating, Location and O.\nSentence: are there any restaurants nearby that have great reviews and plenty of parking","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","restaurants","nearby","that","have","great","reviews","and","plenty","of","parking"],"labels":["O","O","O","O","B-Location","O","O","B-Rating","I-Rating","O","B-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["dish","cuisine","amenity","hours","restaurant_name","price","rating","location"]}
{"id":"75","dataset":"mit-restaurant","split":"test","instance":{"id":"75","prompt_labels":"are(O) there(O) any(O) restaurants(O) nearby(B-Location) that(O) have(O) outdoor(B-Amenity) dining(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Price, Location, Dish, Restaurant Name, Cuisine, Amenity, Rating and O.\nSentence: are there any restaurants nearby that have outdoor dining","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","restaurants","nearby","that","have","outdoor","dining"],"labels":["O","O","O","O","B-Location","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["hours","price","location","dish","restaurant_name","cuisine","amenity","rating"]}
{"id":"78","dataset":"mit-restaurant","split":"test","instance":{"id":"78","prompt_labels":"are(O) there(O) any(O) restaurants(O) on(O) the(O) way(B-Location) to(I-Location) my(I-Location) destination(I-Location) that(O) have(O) a(O) fireplace(B-Amenity) inside(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Rating, Hours, Restaurant Name, Location, Cuisine, Dish, Amenity and O.\nSentence: are there any restaurants on the way to my destination that have a fireplace inside","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","restaurants","on","the","way","to","my","destination","that","have","a","fireplace","inside"],"labels":["O","O","O","O","O","O","B-Location","I-Location","I-Location","I-Location","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["price","rating","hours","restaurant_name","location","cuisine","dish","amenity"]}
{"id":"80","dataset":"mit-restaurant","split":"test","instance":{"id":"80","prompt_labels":"are(O) there(O) any(O) restaurants(O) open(B-Hours) after(I-Hours) 2(I-Hours) am(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Hours, Restaurant Name, Location, Dish, Cuisine, Rating and O.\nSentence: are there any restaurants open after 2 am","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","restaurants","open","after","2","am"],"labels":["O","O","O","O","B-Hours","I-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["price","amenity","hours","restaurant_name","location","dish","cuisine","rating"]}
{"id":"81","dataset":"mit-restaurant","split":"test","instance":{"id":"81","prompt_labels":"are(O) there(O) any(O) restaurants(O) that(O) are(O) open(O) 24(B-Hours) hours(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Price, Amenity, Location, Cuisine, Restaurant Name, Dish, Hours and O.\nSentence: are there any restaurants that are open 24 hours","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","restaurants","that","are","open","24","hours"],"labels":["O","O","O","O","O","O","O","B-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["rating","price","amenity","location","cuisine","restaurant_name","dish","hours"]}
{"id":"99","dataset":"mit-restaurant","split":"test","instance":{"id":"99","prompt_labels":"are(O) there(O) are(O) any(O) cracker(B-Restaurant Name) barrells(I-Restaurant Name) on(O) long(B-Location) island(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Price, Restaurant Name, Amenity, Location, Hours, Rating, Cuisine and O.\nSentence: are there are any cracker barrells on long island","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","are","any","cracker","barrells","on","long","island"],"labels":["O","O","O","O","B-Restaurant Name","I-Restaurant Name","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["dish","price","restaurant_name","amenity","location","hours","rating","cuisine"]}
{"id":"100","dataset":"mit-restaurant","split":"test","instance":{"id":"100","prompt_labels":"are(O) there(O) reservations(O) still(O) available(O) for(O) bar(B-Restaurant Name) la(I-Restaurant Name) grassa(I-Restaurant Name) for(O) 2(O) tomorrow(B-Hours) at(I-Hours) 7(I-Hours) pm(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Location, Rating, Amenity, Price, Dish, Restaurant Name, Hours and O.\nSentence: are there reservations still available for bar la grassa for 2 tomorrow at 7 pm","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","reservations","still","available","for","bar","la","grassa","for","2","tomorrow","at","7","pm"],"labels":["O","O","O","O","O","O","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name","O","O","B-Hours","I-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["cuisine","location","rating","amenity","price","dish","restaurant_name","hours"]}
{"id":"105","dataset":"mit-restaurant","split":"test","instance":{"id":"105","prompt_labels":"best(B-Rating) chinese(B-Cuisine) food(O) in(B-Location) the(I-Location) area(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Cuisine, Hours, Amenity, Location, Rating, Dish, Price and O.\nSentence: best chinese food in the area","prediction_output":null,"prediction_outputs":null,"group":null,"words":["best","chinese","food","in","the","area"],"labels":["B-Rating","B-Cuisine","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["restaurant_name","cuisine","hours","amenity","location","rating","dish","price"]}
{"id":"112","dataset":"mit-restaurant","split":"test","instance":{"id":"112","prompt_labels":"call(O) chinese(B-Cuisine)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Restaurant Name, Cuisine, Amenity, Hours, Rating, Price and O.\nSentence: call chinese","prediction_output":null,"prediction_outputs":null,"group":null,"words":["call","chinese"],"labels":["O","B-Cuisine"],"target_index":null,"target_label":null},"label_list":["dish","location","restaurant_name","cuisine","amenity","hours","rating","price"]}
{"id":"132","dataset":"mit-restaurant","split":"test","instance":{"id":"132","prompt_labels":"can(O) you(O) find(O) a(O) bar(B-Amenity) that(O) serves(O) tapas(B-Dish) and(O) takes(O) reservations(B-Amenity) for(O) happy(B-Amenity) hour(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Rating, Hours, Dish, Price, Location, Cuisine and O.\nSentence: can you find a bar that serves tapas and takes reservations for happy hour","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","a","bar","that","serves","tapas","and","takes","reservations","for","happy","hour"],"labels":["O","O","O","O","B-Amenity","O","O","B-Dish","O","O","B-Amenity","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["restaurant_name","amenity","rating","hours","dish","price","location","cuisine"]}
{"id":"143","dataset":"mit-restaurant","split":"test","instance":{"id":"143","prompt_labels":"can(O) you(O) find(O) a(O) seafood(B-Cuisine) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Hours, Dish, Cuisine, Restaurant Name, Location, Price and O.\nSentence: can you find a seafood restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","a","seafood","restaurant"],"labels":["O","O","O","O","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["amenity","rating","hours","dish","cuisine","restaurant_name","location","price"]}
{"id":"162","dataset":"mit-restaurant","split":"test","instance":{"id":"162","prompt_labels":"can(O) you(O) find(O) me(O) a(O) place(O) nearby(B-Location) thats(O) open(O) after(B-Hours) 12(I-Hours) pm(I-Hours) with(O) bean(B-Dish) dishes(I-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Restaurant Name, Price, Hours, Location, Rating, Dish, Amenity and O.\nSentence: can you find me a place nearby thats open after 12 pm with bean dishes","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","me","a","place","nearby","thats","open","after","12","pm","with","bean","dishes"],"labels":["O","O","O","O","O","O","B-Location","O","O","B-Hours","I-Hours","I-Hours","O","B-Dish","I-Dish"],"target_index":null,"target_label":null},"label_list":["cuisine","restaurant_name","price","hours","location","rating","dish","amenity"]}
{"id":"165","dataset":"mit-restaurant","split":"test","instance":{"id":"165","prompt_labels":"can(O) you(O) find(O) me(O) a(O) restaurant(O) that(O) has(O) entrees(O) priced(O) between(B-Price) 15(I-Price) and(I-Price) 20(I-Price) dollars(I-Price)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Price, Cuisine, Location, Rating, Amenity, Dish and O.\nSentence: can you find me a restaurant that has entrees priced between 15 and 20 dollars","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","me","a","restaurant","that","has","entrees","priced","between","15","and","20","dollars"],"labels":["O","O","O","O","O","O","O","O","O","O","B-Price","I-Price","I-Price","I-Price","I-Price"],"target_index":null,"target_label":null},"label_list":["restaurant_name","hours","price","cuisine","location","rating","amenity","dish"]}
{"id":"173","dataset":"mit-restaurant","split":"test","instance":{"id":"173","prompt_labels":"can(O) you(O) find(O) the(O) closest(B-Location) ihop(B-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Rating, Price, Amenity, Cuisine, Location, Dish and O.\nSentence: can you find the closest ihop","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","find","the","closest","ihop"],"labels":["O","O","O","O","B-Location","B-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["hours","restaurant_name","rating","price","amenity","cuisine","location","dish"]}
{"id":"179","dataset":"mit-restaurant","split":"test","instance":{"id":"179","prompt_labels":"can(O) you(O) give(O) me(O) the(O) name(B-Restaurant Name) of(O) the(O) restaurant(O) on(O) green(B-Location) st(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Dish, Restaurant Name, Amenity, Location, Rating, Cuisine, Hours and O.\nSentence: can you give me the name of the restaurant on green st","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","give","me","the","name","of","the","restaurant","on","green","st"],"labels":["O","O","O","O","O","B-Restaurant Name","O","O","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["price","dish","restaurant_name","amenity","location","rating","cuisine","hours"]}
{"id":"181","dataset":"mit-restaurant","split":"test","instance":{"id":"181","prompt_labels":"can(O) you(O) help(O) me(O) find(O) a(O) fancy(B-Amenity) restaurant(O) with(O) 5(B-Rating) star(I-Rating) ratings(I-Rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Location, Dish, Price, Rating, Amenity, Restaurant Name, Cuisine and O.\nSentence: can you help me find a fancy restaurant with 5 star ratings","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","help","me","find","a","fancy","restaurant","with","5","star","ratings"],"labels":["O","O","O","O","O","O","B-Amenity","O","O","B-Rating","I-Rating","I-Rating"],"target_index":null,"target_label":null},"label_list":["hours","location","dish","price","rating","amenity","restaurant_name","cuisine"]}
{"id":"183","dataset":"mit-restaurant","split":"test","instance":{"id":"183","prompt_labels":"can(O) you(O) help(O) me(O) find(O) a(O) korean(B-Cuisine) restaurant(O) that(O) is(O) close(B-Location) by(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Price, Restaurant Name, Cuisine, Dish, Amenity, Rating, Hours and O.\nSentence: can you help me find a korean restaurant that is close by","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","help","me","find","a","korean","restaurant","that","is","close","by"],"labels":["O","O","O","O","O","O","B-Cuisine","O","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["location","price","restaurant_name","cuisine","dish","amenity","rating","hours"]}
{"id":"186","dataset":"mit-restaurant","split":"test","instance":{"id":"186","prompt_labels":"can(O) you(O) help(O) me(O) find(O) a(O) tong(B-Restaurant Name) villa(I-Restaurant Name) that(O) serves(O) small(B-Amenity) portions(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Location, Cuisine, Restaurant Name, Dish, Amenity, Hours, Rating and O.\nSentence: can you help me find a tong villa that serves small portions","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","help","me","find","a","tong","villa","that","serves","small","portions"],"labels":["O","O","O","O","O","O","B-Restaurant Name","I-Restaurant Name","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["price","location","cuisine","restaurant_name","dish","amenity","hours","rating"]}
{"id":"189","dataset":"mit-restaurant","split":"test","instance":{"id":"189","prompt_labels":"can(O) you(O) locate(O) a(O) 4(B-Rating) star(I-Rating) or(I-Rating) higher(I-Rating) restaurant(O) that(O) serves(O) italian(B-Cuisine) food(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Hours, Cuisine, Restaurant Name, Amenity, Dish, Price, Location and O.\nSentence: can you locate a 4 star or higher restaurant that serves italian food","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","locate","a","4","star","or","higher","restaurant","that","serves","italian","food"],"labels":["O","O","O","O","B-Rating","I-Rating","I-Rating","I-Rating","O","O","O","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["rating","hours","cuisine","restaurant_name","amenity","dish","price","location"]}
{"id":"197","dataset":"mit-restaurant","split":"test","instance":{"id":"197","prompt_labels":"can(O) you(O) make(O) me(O) a(O) reservation(O) for(O) 4(O) at(O) the(O) nearest(B-Location) upscale(B-Price) steakhouse(B-Cuisine)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Location, Dish, Amenity, Cuisine, Restaurant Name, Price, Hours and O.\nSentence: can you make me a reservation for 4 at the nearest upscale steakhouse","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","make","me","a","reservation","for","4","at","the","nearest","upscale","steakhouse"],"labels":["O","O","O","O","O","O","O","O","O","O","B-Location","B-Price","B-Cuisine"],"target_index":null,"target_label":null},"label_list":["rating","location","dish","amenity","cuisine","restaurant_name","price","hours"]}
{"id":"199","dataset":"mit-restaurant","split":"test","instance":{"id":"199","prompt_labels":"can(O) you(O) make(O) reservations(O) for(O) two(O) at(O) heartland(B-Restaurant Name) restaurant(I-Restaurant Name) for(O) tonight(B-Hours) at(I-Hours) 7(I-Hours) 30(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Restaurant Name, Amenity, Price, Cuisine, Rating, Hours and O.\nSentence: can you make reservations for two at heartland restaurant for tonight at 7 30","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","make","reservations","for","two","at","heartland","restaurant","for","tonight","at","7","30"],"labels":["O","O","O","O","O","O","O","B-Restaurant Name","I-Restaurant Name","O","B-Hours","I-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["dish","location","restaurant_name","amenity","price","cuisine","rating","hours"]}
{"id":"200","dataset":"mit-restaurant","split":"test","instance":{"id":"200","prompt_labels":"can(O) you(O) order(O) me(O) a(O) pizza(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Cuisine, Restaurant Name, Location, Hours, Rating, Price, Dish and O.\nSentence: can you order me a pizza","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","order","me","a","pizza"],"labels":["O","O","O","O","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["amenity","cuisine","restaurant_name","location","hours","rating","price","dish"]}
{"id":"217","dataset":"mit-restaurant","split":"test","instance":{"id":"217","prompt_labels":"could(O) you(O) find(O) a(O) restaurant(O) which(O) plays(B-Amenity) live(I-Amenity) music(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Price, Hours, Cuisine, Restaurant Name, Dish, Location, Amenity and O.\nSentence: could you find a restaurant which plays live music","prediction_output":null,"prediction_outputs":null,"group":null,"words":["could","you","find","a","restaurant","which","plays","live","music"],"labels":["O","O","O","O","O","O","B-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["rating","price","hours","cuisine","restaurant_name","dish","location","amenity"]}
{"id":"223","dataset":"mit-restaurant","split":"test","instance":{"id":"223","prompt_labels":"could(O) you(O) make(O) a(O) reservation(O) for(O) me(O) at(O) a(O) nice(B-Rating) italian(B-Cuisine) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Restaurant Name, Hours, Dish, Cuisine, Location, Price and O.\nSentence: could you make a reservation for me at a nice italian restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["could","you","make","a","reservation","for","me","at","a","nice","italian","restaurant"],"labels":["O","O","O","O","O","O","O","O","O","B-Rating","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["rating","amenity","restaurant_name","hours","dish","cuisine","location","price"]}
{"id":"239","dataset":"mit-restaurant","split":"test","instance":{"id":"239","prompt_labels":"do(O) any(O) of(O) the(O) middle(B-Cuisine) eastern(I-Cuisine) restaurants(O) close(B-Hours) by(I-Hours) 7(I-Hours) pm(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Dish, Rating, Amenity, Cuisine, Location, Restaurant Name, Price and O.\nSentence: do any of the middle eastern restaurants close by 7 pm","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","any","of","the","middle","eastern","restaurants","close","by","7","pm"],"labels":["O","O","O","O","B-Cuisine","I-Cuisine","O","B-Hours","I-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["hours","dish","rating","amenity","cuisine","location","restaurant_name","price"]}
{"id":"240","dataset":"mit-restaurant","split":"test","instance":{"id":"240","prompt_labels":"do(O) any(O) of(O) the(O) restaurants(O) in(O) this(O) town(O) also(O) have(O) a(O) dance(B-Amenity) floor(I-Amenity) and(O) live(B-Amenity) music(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Hours, Restaurant Name, Rating, Cuisine, Price, Amenity, Dish and O.\nSentence: do any of the restaurants in this town also have a dance floor and live music","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","any","of","the","restaurants","in","this","town","also","have","a","dance","floor","and","live","music"],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-Amenity","I-Amenity","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["location","hours","restaurant_name","rating","cuisine","price","amenity","dish"]}
{"id":"245","dataset":"mit-restaurant","split":"test","instance":{"id":"245","prompt_labels":"do(O) know(O) any(O) place(B-Location) around(I-Location) here(I-Location) to(O) take(B-Amenity) clients(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Price, Amenity, Dish, Hours, Location, Cuisine, Rating and O.\nSentence: do know any place around here to take clients","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","know","any","place","around","here","to","take","clients"],"labels":["O","O","O","B-Location","I-Location","I-Location","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["restaurant_name","price","amenity","dish","hours","location","cuisine","rating"]}
{"id":"246","dataset":"mit-restaurant","split":"test","instance":{"id":"246","prompt_labels":"do(O) they(O) have(O) a(O) smoking(B-Amenity) area(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Rating, Cuisine, Price, Hours, Location, Dish and O.\nSentence: do they have a smoking area","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","they","have","a","smoking","area"],"labels":["O","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["restaurant_name","amenity","rating","cuisine","price","hours","location","dish"]}
{"id":"255","dataset":"mit-restaurant","split":"test","instance":{"id":"255","prompt_labels":"do(O) you(O) know(O) if(O) reggianos(B-Restaurant Name) serve(O) breakfast(B-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Cuisine, Location, Rating, Hours, Amenity, Dish, Restaurant Name and O.\nSentence: do you know if reggianos serve breakfast","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","you","know","if","reggianos","serve","breakfast"],"labels":["O","O","O","O","B-Restaurant Name","O","B-Hours"],"target_index":null,"target_label":null},"label_list":["price","cuisine","location","rating","hours","amenity","dish","restaurant_name"]}
{"id":"257","dataset":"mit-restaurant","split":"test","instance":{"id":"257","prompt_labels":"do(O) you(O) know(O) if(O) the(O) purple(B-Restaurant Name) cactus(I-Restaurant Name) burrito(I-Restaurant Name) and(I-Restaurant Name) wrap(I-Restaurant Name) bar(I-Restaurant Name) ongreentree(B-Location) lane(I-Location) have(O) byob(B-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Price, Cuisine, Amenity, Hours, Location, Dish and O.\nSentence: do you know if the purple cactus burrito and wrap bar ongreentree lane have byob","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","you","know","if","the","purple","cactus","burrito","and","wrap","bar","ongreentree","lane","have","byob"],"labels":["O","O","O","O","O","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name","I-Restaurant Name","I-Restaurant Name","I-Restaurant Name","B-Location","I-Location","O","B-Amenity"],"target_index":null,"target_label":null},"label_list":["rating","restaurant_name","price","cuisine","amenity","hours","location","dish"]}
{"id":"262","dataset":"mit-restaurant","split":"test","instance":{"id":"262","prompt_labels":"do(O) you(O) know(O) if(O) there(O) are(O) any(O) resataurants(O) in(B-Location) the(I-Location) mall(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Amenity, Rating, Dish, Restaurant Name, Location, Cuisine, Price and O.\nSentence: do you know if there are any resataurants in the mall","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","you","know","if","there","are","any","resataurants","in","the","mall"],"labels":["O","O","O","O","O","O","O","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["hours","amenity","rating","dish","restaurant_name","location","cuisine","price"]}
{"id":"263","dataset":"mit-restaurant","split":"test","instance":{"id":"263","prompt_labels":"do(O) you(O) know(O) if(O) there(O) are(O) any(O) reviews(O) on(O) monacos(B-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Rating, Hours, Amenity, Location, Cuisine, Price, Dish and O.\nSentence: do you know if there are any reviews on monacos","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","you","know","if","there","are","any","reviews","on","monacos"],"labels":["O","O","O","O","O","O","O","O","O","B-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["restaurant_name","rating","hours","amenity","location","cuisine","price","dish"]}
{"id":"273","dataset":"mit-restaurant","split":"test","instance":{"id":"273","prompt_labels":"does(O) angels(B-Restaurant Name) have(O) a(O) dress(B-Amenity) code(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Rating, Amenity, Hours, Restaurant Name, Cuisine, Location, Dish and O.\nSentence: does angels have a dress code","prediction_output":null,"prediction_outputs":null,"group":null,"words":["does","angels","have","a","dress","code"],"labels":["O","B-Restaurant Name","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["price","rating","amenity","hours","restaurant_name","cuisine","location","dish"]}
{"id":"275","dataset":"mit-restaurant","split":"test","instance":{"id":"275","prompt_labels":"does(O) anyone(O) in(B-Location) town(I-Location) deliver(B-Amenity) tasty(B-Rating) vegan(B-Dish) pizza(I-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Rating, Cuisine, Location, Hours, Price, Dish and O.\nSentence: does anyone in town deliver tasty vegan pizza","prediction_output":null,"prediction_outputs":null,"group":null,"words":["does","anyone","in","town","deliver","tasty","vegan","pizza"],"labels":["O","O","B-Location","I-Location","B-Amenity","B-Rating","B-Dish","I-Dish"],"target_index":null,"target_label":null},"label_list":["restaurant_name","amenity","rating","cuisine","location","hours","price","dish"]}
{"id":"276","dataset":"mit-restaurant","split":"test","instance":{"id":"276","prompt_labels":"does(O) bellinis(B-Restaurant Name) have(O) any(O) outdoor(B-Amenity) parking(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Location, Price, Hours, Cuisine, Amenity, Rating, Dish and O.\nSentence: does bellinis have any outdoor parking","prediction_output":null,"prediction_outputs":null,"group":null,"words":["does","bellinis","have","any","outdoor","parking"],"labels":["O","B-Restaurant Name","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["restaurant_name","location","price","hours","cuisine","amenity","rating","dish"]}
{"id":"285","dataset":"mit-restaurant","split":"test","instance":{"id":"285","prompt_labels":"does(O) gustovs(B-Restaurant Name) have(O) a(O) bar(B-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Rating, Restaurant Name, Amenity, Dish, Cuisine, Hours, Location and O.\nSentence: does gustovs have a bar","prediction_output":null,"prediction_outputs":null,"group":null,"words":["does","gustovs","have","a","bar"],"labels":["O","B-Restaurant Name","O","O","B-Amenity"],"target_index":null,"target_label":null},"label_list":["price","rating","restaurant_name","amenity","dish","cuisine","hours","location"]}
{"id":"286","dataset":"mit-restaurant","split":"test","instance":{"id":"286","prompt_labels":"does(O) jaimes(B-Restaurant Name) bakery(I-Restaurant Name) have(O) a(O) great(B-Amenity) decor(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Dish, Price, Location, Amenity, Hours, Cuisine and O.\nSentence: does jaimes bakery have a great decor","prediction_output":null,"prediction_outputs":null,"group":null,"words":["does","jaimes","bakery","have","a","great","decor"],"labels":["O","B-Restaurant Name","I-Restaurant Name","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["rating","restaurant_name","dish","price","location","amenity","hours","cuisine"]}
{"id":"287","dataset":"mit-restaurant","split":"test","instance":{"id":"287","prompt_labels":"does(O) johnsons(B-Restaurant Name) steakhouse(I-Restaurant Name) require(O) formal(B-Amenity) attire(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Location, Restaurant Name, Price, Hours, Cuisine, Dish and O.\nSentence: does johnsons steakhouse require formal attire","prediction_output":null,"prediction_outputs":null,"group":null,"words":["does","johnsons","steakhouse","require","formal","attire"],"labels":["O","B-Restaurant Name","I-Restaurant Name","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["amenity","rating","location","restaurant_name","price","hours","cuisine","dish"]}
{"id":"288","dataset":"mit-restaurant","split":"test","instance":{"id":"288","prompt_labels":"does(O) logans(B-Restaurant Name) serve(O) hamburgers(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Rating, Hours, Amenity, Cuisine, Price, Restaurant Name and O.\nSentence: does logans serve hamburgers","prediction_output":null,"prediction_outputs":null,"group":null,"words":["does","logans","serve","hamburgers"],"labels":["O","B-Restaurant Name","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["dish","location","rating","hours","amenity","cuisine","price","restaurant_name"]}
{"id":"290","dataset":"mit-restaurant","split":"test","instance":{"id":"290","prompt_labels":"does(O) midys(B-Restaurant Name) have(O) takeout(B-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Hours, Rating, Cuisine, Restaurant Name, Dish, Location, Amenity and O.\nSentence: does midys have takeout","prediction_output":null,"prediction_outputs":null,"group":null,"words":["does","midys","have","takeout"],"labels":["O","B-Restaurant Name","O","B-Amenity"],"target_index":null,"target_label":null},"label_list":["price","hours","rating","cuisine","restaurant_name","dish","location","amenity"]}
{"id":"299","dataset":"mit-restaurant","split":"test","instance":{"id":"299","prompt_labels":"does(O) pizza(B-Restaurant Name) hut(I-Restaurant Name) accept(O) credit(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Price, Restaurant Name, Hours, Dish, Location, Amenity, Cuisine and O.\nSentence: does pizza hut accept credit","prediction_output":null,"prediction_outputs":null,"group":null,"words":["does","pizza","hut","accept","credit"],"labels":["O","B-Restaurant Name","I-Restaurant Name","O","O"],"target_index":null,"target_label":null},"label_list":["rating","price","restaurant_name","hours","dish","location","amenity","cuisine"]}
{"id":"302","dataset":"mit-restaurant","split":"test","instance":{"id":"302","prompt_labels":"does(O) ruby(B-Restaurant Name) tuesday(I-Restaurant Name) on(O) drake(B-Location) avenue(I-Location) have(O) a(O) salad(B-Amenity) bar(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Rating, Dish, Cuisine, Amenity, Price, Location and O.\nSentence: does ruby tuesday on drake avenue have a salad bar","prediction_output":null,"prediction_outputs":null,"group":null,"words":["does","ruby","tuesday","on","drake","avenue","have","a","salad","bar"],"labels":["O","B-Restaurant Name","I-Restaurant Name","O","B-Location","I-Location","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["restaurant_name","hours","rating","dish","cuisine","amenity","price","location"]}
{"id":"316","dataset":"mit-restaurant","split":"test","instance":{"id":"316","prompt_labels":"does(O) the(O) italos(B-Restaurant Name) bakery(I-Restaurant Name) along(B-Location) the(I-Location) road(I-Location) have(O) waterfront(B-Amenity) dining(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Location, Restaurant Name, Dish, Price, Rating, Amenity, Cuisine and O.\nSentence: does the italos bakery along the road have waterfront dining","prediction_output":null,"prediction_outputs":null,"group":null,"words":["does","the","italos","bakery","along","the","road","have","waterfront","dining"],"labels":["O","O","B-Restaurant Name","I-Restaurant Name","B-Location","I-Location","I-Location","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["hours","location","restaurant_name","dish","price","rating","amenity","cuisine"]}
{"id":"337","dataset":"mit-restaurant","split":"test","instance":{"id":"337","prompt_labels":"find(O) a(O) chinese(B-Cuisine) restaurant(O) with(O) a(O) large(B-Amenity) buffet(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Hours, Restaurant Name, Location, Price, Dish, Amenity and O.\nSentence: find a chinese restaurant with a large buffet","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","a","chinese","restaurant","with","a","large","buffet"],"labels":["O","O","B-Cuisine","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["cuisine","rating","hours","restaurant_name","location","price","dish","amenity"]}
{"id":"345","dataset":"mit-restaurant","split":"test","instance":{"id":"345","prompt_labels":"find(O) a(O) mike(B-Restaurant Name) donuts(I-Restaurant Name) for(O) non(B-Amenity) smokers(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Location, Amenity, Cuisine, Hours, Price, Restaurant Name, Dish and O.\nSentence: find a mike donuts for non smokers","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","a","mike","donuts","for","non","smokers"],"labels":["O","O","B-Restaurant Name","I-Restaurant Name","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["rating","location","amenity","cuisine","hours","price","restaurant_name","dish"]}
{"id":"352","dataset":"mit-restaurant","split":"test","instance":{"id":"352","prompt_labels":"find(O) an(O) inexpensive(B-Price) mexican(B-Cuisine) restaurant(O) in(B-Location) the(I-Location) area(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Rating, Restaurant Name, Price, Cuisine, Amenity, Location, Dish and O.\nSentence: find an inexpensive mexican restaurant in the area","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","an","inexpensive","mexican","restaurant","in","the","area"],"labels":["O","O","B-Price","B-Cuisine","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["hours","rating","restaurant_name","price","cuisine","amenity","location","dish"]}
{"id":"353","dataset":"mit-restaurant","split":"test","instance":{"id":"353","prompt_labels":"find(O) an(O) italian(B-Cuisine) resturant(O) that(O) serves(O) family(B-Amenity) style(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Location, Dish, Rating, Cuisine, Price, Hours and O.\nSentence: find an italian resturant that serves family style","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","an","italian","resturant","that","serves","family","style"],"labels":["O","O","B-Cuisine","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["restaurant_name","amenity","location","dish","rating","cuisine","price","hours"]}
{"id":"354","dataset":"mit-restaurant","split":"test","instance":{"id":"354","prompt_labels":"find(O) italian(B-Cuisine) restaurants(O) in(O) atlanta(B-Location) ga(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Cuisine, Dish, Location, Price, Restaurant Name, Rating, Hours and O.\nSentence: find italian restaurants in atlanta ga","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","italian","restaurants","in","atlanta","ga"],"labels":["O","B-Cuisine","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["amenity","cuisine","dish","location","price","restaurant_name","rating","hours"]}
{"id":"357","dataset":"mit-restaurant","split":"test","instance":{"id":"357","prompt_labels":"find(O) me(O) a(O) causal(B-Amenity) bar(B-Cuisine) and(I-Cuisine) grill(I-Cuisine)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Cuisine, Restaurant Name, Rating, Price, Amenity, Dish, Hours and O.\nSentence: find me a causal bar and grill","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","causal","bar","and","grill"],"labels":["O","O","O","B-Amenity","B-Cuisine","I-Cuisine","I-Cuisine"],"target_index":null,"target_label":null},"label_list":["location","cuisine","restaurant_name","rating","price","amenity","dish","hours"]}
{"id":"360","dataset":"mit-restaurant","split":"test","instance":{"id":"360","prompt_labels":"find(O) me(O) a(O) cheaply(B-Price) priced(O) thai(B-Cuisine) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Location, Cuisine, Amenity, Hours, Restaurant Name, Dish, Price and O.\nSentence: find me a cheaply priced thai restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","cheaply","priced","thai","restaurant"],"labels":["O","O","O","B-Price","O","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["rating","location","cuisine","amenity","hours","restaurant_name","dish","price"]}
{"id":"361","dataset":"mit-restaurant","split":"test","instance":{"id":"361","prompt_labels":"find(O) me(O) a(O) chinese(B-Cuisine) restaurant(O) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Cuisine, Location, Rating, Amenity, Price, Dish and O.\nSentence: find me a chinese restaurant nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","chinese","restaurant","nearby"],"labels":["O","O","O","B-Cuisine","O","B-Location"],"target_index":null,"target_label":null},"label_list":["restaurant_name","hours","cuisine","location","rating","amenity","price","dish"]}
{"id":"362","dataset":"mit-restaurant","split":"test","instance":{"id":"362","prompt_labels":"find(O) me(O) a(O) chinese(B-Cuisine) take(B-Amenity) out(I-Amenity) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Dish, Cuisine, Restaurant Name, Location, Price, Rating, Amenity and O.\nSentence: find me a chinese take out restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","chinese","take","out","restaurant"],"labels":["O","O","O","B-Cuisine","B-Amenity","I-Amenity","O"],"target_index":null,"target_label":null},"label_list":["hours","dish","cuisine","restaurant_name","location","price","rating","amenity"]}
{"id":"363","dataset":"mit-restaurant","split":"test","instance":{"id":"363","prompt_labels":"find(O) me(O) a(O) close(B-Location) by(I-Location) meat(B-Cuisine) market(O) which(O) sells(O) produce(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Cuisine, Hours, Rating, Dish, Location, Amenity, Price and O.\nSentence: find me a close by meat market which sells produce","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","close","by","meat","market","which","sells","produce"],"labels":["O","O","O","B-Location","I-Location","B-Cuisine","O","O","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["restaurant_name","cuisine","hours","rating","dish","location","amenity","price"]}
{"id":"366","dataset":"mit-restaurant","split":"test","instance":{"id":"366","prompt_labels":"find(O) me(O) a(O) deli(B-Cuisine) which(O) has(O) an(O) eat(B-Amenity) in(I-Amenity) area(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Restaurant Name, Amenity, Hours, Rating, Cuisine, Location, Dish and O.\nSentence: find me a deli which has an eat in area","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","deli","which","has","an","eat","in","area"],"labels":["O","O","O","B-Cuisine","O","O","O","B-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["price","restaurant_name","amenity","hours","rating","cuisine","location","dish"]}
{"id":"370","dataset":"mit-restaurant","split":"test","instance":{"id":"370","prompt_labels":"find(O) me(O) a(O) german(B-Cuisine) restaurant(O) within(B-Location) 10(I-Location) miles(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Price, Location, Dish, Hours, Restaurant Name, Amenity and O.\nSentence: find me a german restaurant within 10 miles","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","german","restaurant","within","10","miles"],"labels":["O","O","O","B-Cuisine","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["cuisine","rating","price","location","dish","hours","restaurant_name","amenity"]}
{"id":"373","dataset":"mit-restaurant","split":"test","instance":{"id":"373","prompt_labels":"find(O) me(O) a(O) good(B-Rating) deli(B-Cuisine) in(O) manhattan(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Cuisine, Dish, Location, Amenity, Rating, Price and O.\nSentence: find me a good deli in manhattan","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","good","deli","in","manhattan"],"labels":["O","O","O","B-Rating","B-Cuisine","O","B-Location"],"target_index":null,"target_label":null},"label_list":["restaurant_name","hours","cuisine","dish","location","amenity","rating","price"]}
{"id":"377","dataset":"mit-restaurant","split":"test","instance":{"id":"377","prompt_labels":"find(O) me(O) a(O) jack(B-Restaurant Name) in(I-Restaurant Name) the(I-Restaurant Name) box(I-Restaurant Name) thats(O) open(B-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Price, Amenity, Dish, Restaurant Name, Rating, Location and O.\nSentence: find me a jack in the box thats open","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","jack","in","the","box","thats","open"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name","I-Restaurant Name","O","B-Hours"],"target_index":null,"target_label":null},"label_list":["hours","cuisine","price","amenity","dish","restaurant_name","rating","location"]}
{"id":"385","dataset":"mit-restaurant","split":"test","instance":{"id":"385","prompt_labels":"find(O) me(O) a(O) place(O) that(O) serves(O) chinese(B-Cuisine) takeout(B-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Dish, Location, Hours, Amenity, Cuisine, Restaurant Name, Rating and O.\nSentence: find me a place that serves chinese takeout","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","place","that","serves","chinese","takeout"],"labels":["O","O","O","O","O","O","B-Cuisine","B-Amenity"],"target_index":null,"target_label":null},"label_list":["price","dish","location","hours","amenity","cuisine","restaurant_name","rating"]}
{"id":"387","dataset":"mit-restaurant","split":"test","instance":{"id":"387","prompt_labels":"find(O) me(O) a(O) place(O) to(O) eat(O) that(O) has(O) excellent(B-Rating) sushi(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Price, Amenity, Dish, Location, Rating, Restaurant Name and O.\nSentence: find me a place to eat that has excellent sushi","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","place","to","eat","that","has","excellent","sushi"],"labels":["O","O","O","O","O","O","O","O","B-Rating","B-Dish"],"target_index":null,"target_label":null},"label_list":["hours","cuisine","price","amenity","dish","location","rating","restaurant_name"]}
{"id":"388","dataset":"mit-restaurant","split":"test","instance":{"id":"388","prompt_labels":"find(O) me(O) a(O) place(O) to(O) get(O) pizza(B-Dish) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Rating, Location, Cuisine, Restaurant Name, Price, Amenity, Hours and O.\nSentence: find me a place to get pizza nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","place","to","get","pizza","nearby"],"labels":["O","O","O","O","O","O","B-Dish","B-Location"],"target_index":null,"target_label":null},"label_list":["dish","rating","location","cuisine","restaurant_name","price","amenity","hours"]}
{"id":"406","dataset":"mit-restaurant","split":"test","instance":{"id":"406","prompt_labels":"find(O) me(O) a(O) spanish(B-Cuisine) restaurant(O) where(O) i(O) can(B-Amenity) smoke(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Dish, Location, Rating, Cuisine, Price, Amenity, Restaurant Name and O.\nSentence: find me a spanish restaurant where i can smoke","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","spanish","restaurant","where","i","can","smoke"],"labels":["O","O","O","B-Cuisine","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["hours","dish","location","rating","cuisine","price","amenity","restaurant_name"]}
{"id":"412","dataset":"mit-restaurant","split":"test","instance":{"id":"412","prompt_labels":"find(O) me(O) a(O) tgi(B-Restaurant Name) fridays(I-Restaurant Name) near(B-Location) me(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Restaurant Name, Price, Cuisine, Amenity, Rating, Dish, Hours and O.\nSentence: find me a tgi fridays near me","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","tgi","fridays","near","me"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["location","restaurant_name","price","cuisine","amenity","rating","dish","hours"]}
{"id":"415","dataset":"mit-restaurant","split":"test","instance":{"id":"415","prompt_labels":"find(O) me(O) a(O) very(O) well(B-Price) priced(O) cuban(B-Cuisine) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Location, Price, Dish, Hours, Amenity, Restaurant Name, Rating and O.\nSentence: find me a very well priced cuban restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","very","well","priced","cuban","restaurant"],"labels":["O","O","O","O","B-Price","O","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["cuisine","location","price","dish","hours","amenity","restaurant_name","rating"]}
{"id":"418","dataset":"mit-restaurant","split":"test","instance":{"id":"418","prompt_labels":"find(O) me(O) an(O) ethiopian(B-Cuisine) restaurant(O) within(B-Location) 5(I-Location) miles(I-Location) of(O) here(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Location, Dish, Hours, Cuisine, Rating, Restaurant Name, Price and O.\nSentence: find me an ethiopian restaurant within 5 miles of here","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","an","ethiopian","restaurant","within","5","miles","of","here"],"labels":["O","O","O","B-Cuisine","O","B-Location","I-Location","I-Location","O","O"],"target_index":null,"target_label":null},"label_list":["amenity","location","dish","hours","cuisine","rating","restaurant_name","price"]}
{"id":"419","dataset":"mit-restaurant","split":"test","instance":{"id":"419","prompt_labels":"find(O) me(O) an(O) expensive(B-Price) american(B-Cuisine) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Location, Restaurant Name, Dish, Rating, Price, Amenity, Hours and O.\nSentence: find me an expensive american restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","an","expensive","american","restaurant"],"labels":["O","O","O","B-Price","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["cuisine","location","restaurant_name","dish","rating","price","amenity","hours"]}
{"id":"423","dataset":"mit-restaurant","split":"test","instance":{"id":"423","prompt_labels":"find(O) me(O) chicken(B-Dish) places(O) that(O) accept(B-Amenity) discover(I-Amenity) card(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Amenity, Restaurant Name, Hours, Dish, Price, Rating, Cuisine and O.\nSentence: find me chicken places that accept discover card","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","chicken","places","that","accept","discover","card"],"labels":["O","O","B-Dish","O","O","B-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["location","amenity","restaurant_name","hours","dish","price","rating","cuisine"]}
{"id":"431","dataset":"mit-restaurant","split":"test","instance":{"id":"431","prompt_labels":"find(O) me(O) the(O) closest(B-Location) bakers(B-Cuisine)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Restaurant Name, Cuisine, Price, Location, Dish, Rating and O.\nSentence: find me the closest bakers","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","the","closest","bakers"],"labels":["O","O","O","B-Location","B-Cuisine"],"target_index":null,"target_label":null},"label_list":["amenity","hours","restaurant_name","cuisine","price","location","dish","rating"]}
{"id":"437","dataset":"mit-restaurant","split":"test","instance":{"id":"437","prompt_labels":"find(O) me(O) the(O) nearest(B-Location) placed(O) to(O) eat(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Amenity, Restaurant Name, Dish, Hours, Price, Location, Rating and O.\nSentence: find me the nearest placed to eat","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","the","nearest","placed","to","eat"],"labels":["O","O","O","B-Location","O","O","O"],"target_index":null,"target_label":null},"label_list":["cuisine","amenity","restaurant_name","dish","hours","price","location","rating"]}
{"id":"452","dataset":"mit-restaurant","split":"test","instance":{"id":"452","prompt_labels":"find(O) the(O) closest(B-Location) sea(B-Cuisine) food(I-Cuisine) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Price, Hours, Restaurant Name, Cuisine, Rating, Dish, Location and O.\nSentence: find the closest sea food restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","the","closest","sea","food","restaurant"],"labels":["O","O","B-Location","B-Cuisine","I-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["amenity","price","hours","restaurant_name","cuisine","rating","dish","location"]}
{"id":"460","dataset":"mit-restaurant","split":"test","instance":{"id":"460","prompt_labels":"get(O) me(O) to(O) a(O) good(B-Rating) pho(B-Cuisine) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Rating, Dish, Price, Location, Amenity, Restaurant Name, Cuisine and O.\nSentence: get me to a good pho restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["get","me","to","a","good","pho","restaurant"],"labels":["O","O","O","O","B-Rating","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["hours","rating","dish","price","location","amenity","restaurant_name","cuisine"]}
{"id":"463","dataset":"mit-restaurant","split":"test","instance":{"id":"463","prompt_labels":"give(O) me(O) a(O) list(O) of(O) restaurants(O) that(O) have(O) seafood(B-Cuisine) on(O) the(O) menu(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Price, Cuisine, Hours, Restaurant Name, Dish, Location and O.\nSentence: give me a list of restaurants that have seafood on the menu","prediction_output":null,"prediction_outputs":null,"group":null,"words":["give","me","a","list","of","restaurants","that","have","seafood","on","the","menu"],"labels":["O","O","O","O","O","O","O","O","B-Cuisine","O","O","O"],"target_index":null,"target_label":null},"label_list":["rating","amenity","price","cuisine","hours","restaurant_name","dish","location"]}
{"id":"465","dataset":"mit-restaurant","split":"test","instance":{"id":"465","prompt_labels":"give(O) me(O) directions(O) to(O) a(O) mcdonalds(B-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Amenity, Rating, Price, Cuisine, Location, Restaurant Name, Hours and O.\nSentence: give me directions to a mcdonalds","prediction_output":null,"prediction_outputs":null,"group":null,"words":["give","me","directions","to","a","mcdonalds"],"labels":["O","O","O","O","O","B-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["dish","amenity","rating","price","cuisine","location","restaurant_name","hours"]}
{"id":"473","dataset":"mit-restaurant","split":"test","instance":{"id":"473","prompt_labels":"gryos(B-Dish) nears(B-Location) restaurant(O) serving(O) them(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Amenity, Restaurant Name, Dish, Price, Cuisine, Location, Rating and O.\nSentence: gryos nears restaurant serving them","prediction_output":null,"prediction_outputs":null,"group":null,"words":["gryos","nears","restaurant","serving","them"],"labels":["B-Dish","B-Location","O","O","O"],"target_index":null,"target_label":null},"label_list":["hours","amenity","restaurant_name","dish","price","cuisine","location","rating"]}
{"id":"477","dataset":"mit-restaurant","split":"test","instance":{"id":"477","prompt_labels":"help(O) me(O) find(O) a(O) burger(B-Cuisine) joint(I-Cuisine)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Dish, Cuisine, Price, Location, Rating, Amenity and O.\nSentence: help me find a burger joint","prediction_output":null,"prediction_outputs":null,"group":null,"words":["help","me","find","a","burger","joint"],"labels":["O","O","O","O","B-Cuisine","I-Cuisine"],"target_index":null,"target_label":null},"label_list":["hours","restaurant_name","dish","cuisine","price","location","rating","amenity"]}
{"id":"479","dataset":"mit-restaurant","split":"test","instance":{"id":"479","prompt_labels":"help(O) me(O) find(O) a(O) good(B-Rating) place(O) to(O) eat(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Restaurant Name, Dish, Hours, Location, Rating, Amenity, Price and O.\nSentence: help me find a good place to eat","prediction_output":null,"prediction_outputs":null,"group":null,"words":["help","me","find","a","good","place","to","eat"],"labels":["O","O","O","O","B-Rating","O","O","O"],"target_index":null,"target_label":null},"label_list":["cuisine","restaurant_name","dish","hours","location","rating","amenity","price"]}
{"id":"480","dataset":"mit-restaurant","split":"test","instance":{"id":"480","prompt_labels":"help(O) me(O) find(O) a(O) high(O) end(O) restaurant(O) that(O) is(O) open(B-Hours) until(I-Hours) 11(I-Hours) pm(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Rating, Cuisine, Price, Hours, Location, Restaurant Name and O.\nSentence: help me find a high end restaurant that is open until 11 pm","prediction_output":null,"prediction_outputs":null,"group":null,"words":["help","me","find","a","high","end","restaurant","that","is","open","until","11","pm"],"labels":["O","O","O","O","O","O","O","O","O","B-Hours","I-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["amenity","dish","rating","cuisine","price","hours","location","restaurant_name"]}
{"id":"488","dataset":"mit-restaurant","split":"test","instance":{"id":"488","prompt_labels":"hey(O) tell(O) me(O) where(O) theres(O) a(O) taco(B-Restaurant Name) bell(I-Restaurant Name) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Cuisine, Dish, Price, Restaurant Name, Rating, Hours, Amenity and O.\nSentence: hey tell me where theres a taco bell nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["hey","tell","me","where","theres","a","taco","bell","nearby"],"labels":["O","O","O","O","O","O","B-Restaurant Name","I-Restaurant Name","B-Location"],"target_index":null,"target_label":null},"label_list":["location","cuisine","dish","price","restaurant_name","rating","hours","amenity"]}
{"id":"491","dataset":"mit-restaurant","split":"test","instance":{"id":"491","prompt_labels":"hi(O) please(O) find(O) me(O) a(O) sushi(B-Cuisine) restaurant(O) that(O) has(O) good(B-Rating) reviews(I-Rating) and(O) that(O) isnt(B-Price) too(I-Price) expensive(I-Price)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Dish, Cuisine, Price, Amenity, Location, Rating, Restaurant Name and O.\nSentence: hi please find me a sushi restaurant that has good reviews and that isnt too expensive","prediction_output":null,"prediction_outputs":null,"group":null,"words":["hi","please","find","me","a","sushi","restaurant","that","has","good","reviews","and","that","isnt","too","expensive"],"labels":["O","O","O","O","O","B-Cuisine","O","O","O","B-Rating","I-Rating","O","O","B-Price","I-Price","I-Price"],"target_index":null,"target_label":null},"label_list":["hours","dish","cuisine","price","amenity","location","rating","restaurant_name"]}
{"id":"510","dataset":"mit-restaurant","split":"test","instance":{"id":"510","prompt_labels":"how(O) far(O) away(O) is(O) the(O) closest(B-Location) burger(B-Restaurant Name) king(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Amenity, Location, Dish, Cuisine, Rating, Restaurant Name, Price and O.\nSentence: how far away is the closest burger king","prediction_output":null,"prediction_outputs":null,"group":null,"words":["how","far","away","is","the","closest","burger","king"],"labels":["O","O","O","O","O","B-Location","B-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["hours","amenity","location","dish","cuisine","rating","restaurant_name","price"]}
{"id":"512","dataset":"mit-restaurant","split":"test","instance":{"id":"512","prompt_labels":"how(O) far(O) away(O) is(O) the(O) nearest(B-Location) steak(B-Cuisine) house(I-Cuisine)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Rating, Dish, Restaurant Name, Price, Location, Amenity and O.\nSentence: how far away is the nearest steak house","prediction_output":null,"prediction_outputs":null,"group":null,"words":["how","far","away","is","the","nearest","steak","house"],"labels":["O","O","O","O","O","B-Location","B-Cuisine","I-Cuisine"],"target_index":null,"target_label":null},"label_list":["hours","cuisine","rating","dish","restaurant_name","price","location","amenity"]}
{"id":"513","dataset":"mit-restaurant","split":"test","instance":{"id":"513","prompt_labels":"how(O) far(O) for(O) a(O) burger(B-Cuisine) place(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Hours, Price, Restaurant Name, Amenity, Location, Cuisine, Rating and O.\nSentence: how far for a burger place","prediction_output":null,"prediction_outputs":null,"group":null,"words":["how","far","for","a","burger","place"],"labels":["O","O","O","O","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["dish","hours","price","restaurant_name","amenity","location","cuisine","rating"]}
{"id":"519","dataset":"mit-restaurant","split":"test","instance":{"id":"519","prompt_labels":"how(O) far(O) is(O) the(O) taco(B-Restaurant Name) bell(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Location, Price, Dish, Rating, Hours, Cuisine, Amenity and O.\nSentence: how far is the taco bell","prediction_output":null,"prediction_outputs":null,"group":null,"words":["how","far","is","the","taco","bell"],"labels":["O","O","O","O","B-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["restaurant_name","location","price","dish","rating","hours","cuisine","amenity"]}
{"id":"521","dataset":"mit-restaurant","split":"test","instance":{"id":"521","prompt_labels":"how(O) far(O) to(O) the(O) next(B-Location) subway(B-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Price, Rating, Amenity, Cuisine, Hours, Location, Dish and O.\nSentence: how far to the next subway","prediction_output":null,"prediction_outputs":null,"group":null,"words":["how","far","to","the","next","subway"],"labels":["O","O","O","O","B-Location","B-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["restaurant_name","price","rating","amenity","cuisine","hours","location","dish"]}
{"id":"536","dataset":"mit-restaurant","split":"test","instance":{"id":"536","prompt_labels":"how(O) much(O) is(O) small(O) box(O) of(O) fries(B-Dish) at(O) jack(B-Restaurant Name) in(I-Restaurant Name) the(I-Restaurant Name) box(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Location, Amenity, Restaurant Name, Hours, Dish, Rating and O.\nSentence: how much is small box of fries at jack in the box","prediction_output":null,"prediction_outputs":null,"group":null,"words":["how","much","is","small","box","of","fries","at","jack","in","the","box"],"labels":["O","O","O","O","O","O","B-Dish","O","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["cuisine","price","location","amenity","restaurant_name","hours","dish","rating"]}
{"id":"538","dataset":"mit-restaurant","split":"test","instance":{"id":"538","prompt_labels":"how(O) should(O) i(O) dress(O) when(O) going(O) to(O) francoiss(B-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Dish, Hours, Rating, Cuisine, Location, Amenity, Restaurant Name and O.\nSentence: how should i dress when going to francoiss","prediction_output":null,"prediction_outputs":null,"group":null,"words":["how","should","i","dress","when","going","to","francoiss"],"labels":["O","O","O","O","O","O","O","B-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["price","dish","hours","rating","cuisine","location","amenity","restaurant_name"]}
{"id":"544","dataset":"mit-restaurant","split":"test","instance":{"id":"544","prompt_labels":"i(O) am(O) in(O) the(O) mood(O) for(O) some(O) chinese(B-Cuisine) food(O) can(O) you(O) find(O) me(O) a(O) place(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Price, Dish, Restaurant Name, Cuisine, Hours, Location, Rating and O.\nSentence: i am in the mood for some chinese food can you find me a place","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","am","in","the","mood","for","some","chinese","food","can","you","find","me","a","place"],"labels":["O","O","O","O","O","O","O","B-Cuisine","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["amenity","price","dish","restaurant_name","cuisine","hours","location","rating"]}
{"id":"551","dataset":"mit-restaurant","split":"test","instance":{"id":"551","prompt_labels":"i(O) am(O) looking(O) for(O) a(O) restaurant(O) that(O) allows(B-Amenity) smoking(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Location, Cuisine, Hours, Restaurant Name, Price, Dish, Rating and O.\nSentence: i am looking for a restaurant that allows smoking","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","am","looking","for","a","restaurant","that","allows","smoking"],"labels":["O","O","O","O","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["amenity","location","cuisine","hours","restaurant_name","price","dish","rating"]}
{"id":"552","dataset":"mit-restaurant","split":"test","instance":{"id":"552","prompt_labels":"i(O) am(O) looking(O) for(O) a(O) restaurant(O) that(O) serves(O) english(B-Cuisine) food(O) i(O) want(O) the(O) price(O) to(O) be(O) cheap(B-Price) to(I-Price) moderate(I-Price)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Restaurant Name, Price, Hours, Amenity, Dish, Location and O.\nSentence: i am looking for a restaurant that serves english food i want the price to be cheap to moderate","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","am","looking","for","a","restaurant","that","serves","english","food","i","want","the","price","to","be","cheap","to","moderate"],"labels":["O","O","O","O","O","O","O","O","B-Cuisine","O","O","O","O","O","O","O","B-Price","I-Price","I-Price"],"target_index":null,"target_label":null},"label_list":["cuisine","rating","restaurant_name","price","hours","amenity","dish","location"]}
{"id":"554","dataset":"mit-restaurant","split":"test","instance":{"id":"554","prompt_labels":"i(O) am(O) looking(O) for(O) lunch(B-Hours) buffets(B-Amenity) within(B-Location) 15(I-Location) minutes(I-Location) driving(I-Location) distance(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Dish, Hours, Rating, Cuisine, Price, Amenity, Location and O.\nSentence: i am looking for lunch buffets within 15 minutes driving distance","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","am","looking","for","lunch","buffets","within","15","minutes","driving","distance"],"labels":["O","O","O","O","B-Hours","B-Amenity","B-Location","I-Location","I-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["restaurant_name","dish","hours","rating","cuisine","price","amenity","location"]}
{"id":"556","dataset":"mit-restaurant","split":"test","instance":{"id":"556","prompt_labels":"i(O) am(O) looking(O) for(O) some(O) chinese(B-Cuisine) food(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Dish, Cuisine, Restaurant Name, Amenity, Hours, Location, Rating and O.\nSentence: i am looking for some chinese food","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","am","looking","for","some","chinese","food"],"labels":["O","O","O","O","O","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["price","dish","cuisine","restaurant_name","amenity","hours","location","rating"]}
{"id":"558","dataset":"mit-restaurant","split":"test","instance":{"id":"558","prompt_labels":"i(O) feel(O) in(O) the(O) mood(O) for(O) spicy(B-Cuisine) food(O) what(O) can(O) you(O) do(O) for(O) me(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Rating, Cuisine, Location, Price, Hours, Dish, Amenity and O.\nSentence: i feel in the mood for spicy food what can you do for me","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","feel","in","the","mood","for","spicy","food","what","can","you","do","for","me"],"labels":["O","O","O","O","O","O","B-Cuisine","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["restaurant_name","rating","cuisine","location","price","hours","dish","amenity"]}
{"id":"559","dataset":"mit-restaurant","split":"test","instance":{"id":"559","prompt_labels":"i(O) feel(O) the(O) need(O) for(O) some(O) killer(B-Rating) barbeque(B-Dish) help(O) me(O) please(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Location, Cuisine, Amenity, Hours, Dish, Price, Restaurant Name and O.\nSentence: i feel the need for some killer barbeque help me please","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","feel","the","need","for","some","killer","barbeque","help","me","please"],"labels":["O","O","O","O","O","O","B-Rating","B-Dish","O","O","O"],"target_index":null,"target_label":null},"label_list":["rating","location","cuisine","amenity","hours","dish","price","restaurant_name"]}
{"id":"560","dataset":"mit-restaurant","split":"test","instance":{"id":"560","prompt_labels":"i(O) hate(B-Amenity) smoke(I-Amenity) so(O) list(O) off(O) some(O) eateries(O) i(O) can(O) go(O) relax(B-Amenity) at(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Dish, Price, Amenity, Location, Rating, Cuisine and O.\nSentence: i hate smoke so list off some eateries i can go relax at","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","hate","smoke","so","list","off","some","eateries","i","can","go","relax","at"],"labels":["O","B-Amenity","I-Amenity","O","O","O","O","O","O","O","O","B-Amenity","O"],"target_index":null,"target_label":null},"label_list":["hours","restaurant_name","dish","price","amenity","location","rating","cuisine"]}
{"id":"561","dataset":"mit-restaurant","split":"test","instance":{"id":"561","prompt_labels":"i(O) have(O) a(O) coupon(O) for(O) sweet(B-Restaurant Name) tomatoes(I-Restaurant Name) where(O) is(O) the(O) nearest(B-Location) one(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Cuisine, Restaurant Name, Rating, Amenity, Hours, Dish, Price and O.\nSentence: i have a coupon for sweet tomatoes where is the nearest one","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","have","a","coupon","for","sweet","tomatoes","where","is","the","nearest","one"],"labels":["O","O","O","O","O","B-Restaurant Name","I-Restaurant Name","O","O","O","B-Location","O"],"target_index":null,"target_label":null},"label_list":["location","cuisine","restaurant_name","rating","amenity","hours","dish","price"]}
{"id":"564","dataset":"mit-restaurant","split":"test","instance":{"id":"564","prompt_labels":"i(O) have(O) alcohol(O) where(O) can(O) i(O) find(O) a(O) good(B-Rating) appetizer(B-Cuisine)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Rating, Price, Location, Dish, Cuisine, Amenity, Restaurant Name and O.\nSentence: i have alcohol where can i find a good appetizer","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","have","alcohol","where","can","i","find","a","good","appetizer"],"labels":["O","O","O","O","O","O","O","O","B-Rating","B-Cuisine"],"target_index":null,"target_label":null},"label_list":["hours","rating","price","location","dish","cuisine","amenity","restaurant_name"]}
{"id":"576","dataset":"mit-restaurant","split":"test","instance":{"id":"576","prompt_labels":"i(O) need(O) a(O) kid(B-Amenity) friendly(I-Amenity) place(O) to(O) get(O) sushi(B-Cuisine)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Cuisine, Amenity, Location, Restaurant Name, Dish, Price, Hours and O.\nSentence: i need a kid friendly place to get sushi","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","need","a","kid","friendly","place","to","get","sushi"],"labels":["O","O","O","B-Amenity","I-Amenity","O","O","O","B-Cuisine"],"target_index":null,"target_label":null},"label_list":["rating","cuisine","amenity","location","restaurant_name","dish","price","hours"]}
{"id":"605","dataset":"mit-restaurant","split":"test","instance":{"id":"605","prompt_labels":"i(O) need(O) somewhere(O) decently(B-Price) priced(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Amenity, Cuisine, Dish, Rating, Restaurant Name, Price, Location and O.\nSentence: i need somewhere decently priced","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","need","somewhere","decently","priced"],"labels":["O","O","O","B-Price","O"],"target_index":null,"target_label":null},"label_list":["hours","amenity","cuisine","dish","rating","restaurant_name","price","location"]}
{"id":"606","dataset":"mit-restaurant","split":"test","instance":{"id":"606","prompt_labels":"i(O) need(O) the(O) closest(B-Location) chic(B-Restaurant Name) fil(I-Restaurant Name) a(I-Restaurant Name) that(O) is(O) still(O) serving(O) the(O) peach(B-Dish) shake(I-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Hours, Price, Rating, Amenity, Dish, Restaurant Name, Cuisine and O.\nSentence: i need the closest chic fil a that is still serving the peach shake","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","need","the","closest","chic","fil","a","that","is","still","serving","the","peach","shake"],"labels":["O","O","O","B-Location","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name","O","O","O","O","O","B-Dish","I-Dish"],"target_index":null,"target_label":null},"label_list":["location","hours","price","rating","amenity","dish","restaurant_name","cuisine"]}
{"id":"608","dataset":"mit-restaurant","split":"test","instance":{"id":"608","prompt_labels":"i(O) need(O) to(O) book(O) a(O) business(B-Amenity) breakfast(O) at(O) the(O) right(B-Price) price(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Cuisine, Location, Dish, Amenity, Price, Rating and O.\nSentence: i need to book a business breakfast at the right price","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","need","to","book","a","business","breakfast","at","the","right","price"],"labels":["O","O","O","O","O","B-Amenity","O","O","O","B-Price","O"],"target_index":null,"target_label":null},"label_list":["hours","restaurant_name","cuisine","location","dish","amenity","price","rating"]}
{"id":"610","dataset":"mit-restaurant","split":"test","instance":{"id":"610","prompt_labels":"i(O) need(O) to(O) find(O) a(O) place(O) that(O) is(O) open(B-Hours) every(I-Hours) day(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Dish, Restaurant Name, Amenity, Rating, Price, Cuisine, Hours and O.\nSentence: i need to find a place that is open every day","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","need","to","find","a","place","that","is","open","every","day"],"labels":["O","O","O","O","O","O","O","O","B-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["location","dish","restaurant_name","amenity","rating","price","cuisine","hours"]}
{"id":"611","dataset":"mit-restaurant","split":"test","instance":{"id":"611","prompt_labels":"i(O) need(O) to(O) find(O) a(O) restaurant(O) int(O) he(O) government(B-Location) center(I-Location) with(O) good(B-Rating) service(I-Rating) an(O) a(O) decent(B-Price) price(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Dish, Hours, Cuisine, Amenity, Rating, Location, Price and O.\nSentence: i need to find a restaurant int he government center with good service an a decent price","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","need","to","find","a","restaurant","int","he","government","center","with","good","service","an","a","decent","price"],"labels":["O","O","O","O","O","O","O","O","B-Location","I-Location","O","B-Rating","I-Rating","O","O","B-Price","O"],"target_index":null,"target_label":null},"label_list":["restaurant_name","dish","hours","cuisine","amenity","rating","location","price"]}
{"id":"612","dataset":"mit-restaurant","split":"test","instance":{"id":"612","prompt_labels":"i(O) need(O) to(O) know(O) of(O) a(O) place(O) that(O) serves(O) breakfast(B-Cuisine) beginning(O) as(O) early(O) as(O) 5(B-Hours) 30(I-Hours) am(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Restaurant Name, Location, Rating, Amenity, Hours, Cuisine, Dish and O.\nSentence: i need to know of a place that serves breakfast beginning as early as 5 30 am","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","need","to","know","of","a","place","that","serves","breakfast","beginning","as","early","as","5","30","am"],"labels":["O","O","O","O","O","O","O","O","O","B-Cuisine","O","O","O","O","B-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["price","restaurant_name","location","rating","amenity","hours","cuisine","dish"]}
{"id":"614","dataset":"mit-restaurant","split":"test","instance":{"id":"614","prompt_labels":"i(O) smell(O) bread(B-Dish) take(O) me(O) there(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Rating, Cuisine, Amenity, Dish, Price, Location and O.\nSentence: i smell bread take me there","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","smell","bread","take","me","there"],"labels":["O","O","B-Dish","O","O","O"],"target_index":null,"target_label":null},"label_list":["restaurant_name","hours","rating","cuisine","amenity","dish","price","location"]}
{"id":"617","dataset":"mit-restaurant","split":"test","instance":{"id":"617","prompt_labels":"i(O) want(O) a(O) 5(B-Rating) star(I-Rating) restaurant(O) that(O) does(O) carry(B-Amenity) out(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Cuisine, Price, Restaurant Name, Location, Rating, Hours and O.\nSentence: i want a 5 star restaurant that does carry out","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","a","5","star","restaurant","that","does","carry","out"],"labels":["O","O","O","B-Rating","I-Rating","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["amenity","dish","cuisine","price","restaurant_name","location","rating","hours"]}
{"id":"627","dataset":"mit-restaurant","split":"test","instance":{"id":"627","prompt_labels":"i(O) want(O) a(O) restaurant(O) on(O) smith(B-Location) st(I-Location) that(O) serves(O) toast(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Cuisine, Dish, Price, Location, Restaurant Name, Hours, Amenity and O.\nSentence: i want a restaurant on smith st that serves toast","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","a","restaurant","on","smith","st","that","serves","toast"],"labels":["O","O","O","O","O","B-Location","I-Location","O","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["rating","cuisine","dish","price","location","restaurant_name","hours","amenity"]}
{"id":"629","dataset":"mit-restaurant","split":"test","instance":{"id":"629","prompt_labels":"i(O) want(O) a(O) restaurant(O) where(O) i(O) can(O) order(O) some(O) carry(B-Amenity) out(I-Amenity) potato(B-Dish) soup(I-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Location, Dish, Price, Amenity, Cuisine, Restaurant Name, Hours and O.\nSentence: i want a restaurant where i can order some carry out potato soup","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","a","restaurant","where","i","can","order","some","carry","out","potato","soup"],"labels":["O","O","O","O","O","O","O","O","O","B-Amenity","I-Amenity","B-Dish","I-Dish"],"target_index":null,"target_label":null},"label_list":["rating","location","dish","price","amenity","cuisine","restaurant_name","hours"]}
{"id":"651","dataset":"mit-restaurant","split":"test","instance":{"id":"651","prompt_labels":"i(O) want(O) to(O) eat(O) sushi(B-Dish) please(O) find(O) me(O) a(O) place(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Hours, Dish, Rating, Amenity, Restaurant Name, Price, Cuisine and O.\nSentence: i want to eat sushi please find me a place","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","to","eat","sushi","please","find","me","a","place"],"labels":["O","O","O","O","B-Dish","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","hours","dish","rating","amenity","restaurant_name","price","cuisine"]}
{"id":"652","dataset":"mit-restaurant","split":"test","instance":{"id":"652","prompt_labels":"i(O) want(O) to(O) find(O) a(O) burger(B-Dish) that(O) isnt(B-Rating) fast(I-Rating) food(I-Rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Location, Dish, Price, Amenity, Hours, Restaurant Name, Cuisine and O.\nSentence: i want to find a burger that isnt fast food","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","to","find","a","burger","that","isnt","fast","food"],"labels":["O","O","O","O","O","B-Dish","O","B-Rating","I-Rating","I-Rating"],"target_index":null,"target_label":null},"label_list":["rating","location","dish","price","amenity","hours","restaurant_name","cuisine"]}
{"id":"660","dataset":"mit-restaurant","split":"test","instance":{"id":"660","prompt_labels":"i(O) want(O) to(O) find(O) a(O) place(O) with(O) spaghetti(B-Dish) and(I-Dish) meatballs(I-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Rating, Cuisine, Amenity, Restaurant Name, Price, Dish, Hours and O.\nSentence: i want to find a place with spaghetti and meatballs","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","to","find","a","place","with","spaghetti","and","meatballs"],"labels":["O","O","O","O","O","O","O","B-Dish","I-Dish","I-Dish"],"target_index":null,"target_label":null},"label_list":["location","rating","cuisine","amenity","restaurant_name","price","dish","hours"]}
{"id":"663","dataset":"mit-restaurant","split":"test","instance":{"id":"663","prompt_labels":"i(O) want(O) to(O) find(O) an(O) italian(B-Cuisine) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Amenity, Price, Cuisine, Dish, Hours, Restaurant Name, Rating and O.\nSentence: i want to find an italian restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","to","find","an","italian","restaurant"],"labels":["O","O","O","O","O","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["location","amenity","price","cuisine","dish","hours","restaurant_name","rating"]}
{"id":"667","dataset":"mit-restaurant","split":"test","instance":{"id":"667","prompt_labels":"i(O) want(O) to(O) get(O) some(O) chinese(B-Cuisine) food(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Price, Location, Dish, Restaurant Name, Amenity, Hours, Cuisine and O.\nSentence: i want to get some chinese food","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","to","get","some","chinese","food"],"labels":["O","O","O","O","O","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["rating","price","location","dish","restaurant_name","amenity","hours","cuisine"]}
{"id":"669","dataset":"mit-restaurant","split":"test","instance":{"id":"669","prompt_labels":"i(O) want(O) to(O) get(O) to(O) a(O) restauarant(O) as(B-Location) fast(I-Location) as(I-Location) possible(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Location, Hours, Cuisine, Price, Dish, Amenity and O.\nSentence: i want to get to a restauarant as fast as possible","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","to","get","to","a","restauarant","as","fast","as","possible"],"labels":["O","O","O","O","O","O","O","B-Location","I-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["rating","restaurant_name","location","hours","cuisine","price","dish","amenity"]}
{"id":"670","dataset":"mit-restaurant","split":"test","instance":{"id":"670","prompt_labels":"i(O) want(O) to(O) go(O) dancing(B-Amenity) at(O) a(O) nearby(B-Location) place(O) and(O) i(O) want(O) scallops(B-Dish) while(O) im(O) at(O) it(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Amenity, Restaurant Name, Hours, Price, Rating, Cuisine, Dish and O.\nSentence: i want to go dancing at a nearby place and i want scallops while im at it","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","to","go","dancing","at","a","nearby","place","and","i","want","scallops","while","im","at","it"],"labels":["O","O","O","O","B-Amenity","O","O","B-Location","O","O","O","O","B-Dish","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","amenity","restaurant_name","hours","price","rating","cuisine","dish"]}
{"id":"673","dataset":"mit-restaurant","split":"test","instance":{"id":"673","prompt_labels":"i(O) want(O) to(O) go(O) to(O) an(O) indian(B-Cuisine) restaurant(O) downtown(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Restaurant Name, Amenity, Cuisine, Rating, Dish, Hours, Price and O.\nSentence: i want to go to an indian restaurant downtown","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","to","go","to","an","indian","restaurant","downtown"],"labels":["O","O","O","O","O","O","B-Cuisine","O","B-Location"],"target_index":null,"target_label":null},"label_list":["location","restaurant_name","amenity","cuisine","rating","dish","hours","price"]}
{"id":"675","dataset":"mit-restaurant","split":"test","instance":{"id":"675","prompt_labels":"i(O) want(O) to(O) have(O) lunch(B-Hours) in(O) downtown(B-Location) la(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Dish, Price, Amenity, Location, Restaurant Name, Rating, Hours and O.\nSentence: i want to have lunch in downtown la","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","to","have","lunch","in","downtown","la"],"labels":["O","O","O","O","B-Hours","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["cuisine","dish","price","amenity","location","restaurant_name","rating","hours"]}
{"id":"690","dataset":"mit-restaurant","split":"test","instance":{"id":"690","prompt_labels":"i(O) would(O) like(O) to(O) eat(O) pizza(B-Dish) at(O) a(O) place(O) with(O) outdoor(B-Amenity) seating(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Cuisine, Hours, Rating, Restaurant Name, Location, Dish, Amenity and O.\nSentence: i would like to eat pizza at a place with outdoor seating","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","would","like","to","eat","pizza","at","a","place","with","outdoor","seating"],"labels":["O","O","O","O","O","B-Dish","O","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["price","cuisine","hours","rating","restaurant_name","location","dish","amenity"]}
{"id":"691","dataset":"mit-restaurant","split":"test","instance":{"id":"691","prompt_labels":"i(O) would(O) like(O) to(O) eat(O) tofu(B-Dish) at(B-Hours) 12(I-Hours) pm(I-Hours) for(O) a(O) reasonable(B-Price) price(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Hours, Price, Restaurant Name, Rating, Dish, Amenity, Cuisine and O.\nSentence: i would like to eat tofu at 12 pm for a reasonable price","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","would","like","to","eat","tofu","at","12","pm","for","a","reasonable","price"],"labels":["O","O","O","O","O","B-Dish","B-Hours","I-Hours","I-Hours","O","O","B-Price","O"],"target_index":null,"target_label":null},"label_list":["location","hours","price","restaurant_name","rating","dish","amenity","cuisine"]}
{"id":"696","dataset":"mit-restaurant","split":"test","instance":{"id":"696","prompt_labels":"i(O) would(O) like(O) to(O) find(O) a(O) vegeterian(B-Cuisine) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Cuisine, Rating, Price, Location, Hours, Amenity, Dish and O.\nSentence: i would like to find a vegeterian restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","would","like","to","find","a","vegeterian","restaurant"],"labels":["O","O","O","O","O","O","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["restaurant_name","cuisine","rating","price","location","hours","amenity","dish"]}
{"id":"704","dataset":"mit-restaurant","split":"test","instance":{"id":"704","prompt_labels":"id(O) like(O) some(O) comfort(B-Cuisine) food(I-Cuisine) farm(B-Dish) vegetables(I-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Rating, Price, Location, Restaurant Name, Amenity, Hours, Cuisine and O.\nSentence: id like some comfort food farm vegetables","prediction_output":null,"prediction_outputs":null,"group":null,"words":["id","like","some","comfort","food","farm","vegetables"],"labels":["O","O","O","B-Cuisine","I-Cuisine","B-Dish","I-Dish"],"target_index":null,"target_label":null},"label_list":["dish","rating","price","location","restaurant_name","amenity","hours","cuisine"]}
{"id":"705","dataset":"mit-restaurant","split":"test","instance":{"id":"705","prompt_labels":"id(O) like(O) to(O) eat(O) at(O) a(O) good(O) lunch(B-Cuisine) spot(O) for(B-Price) the(I-Price) right(I-Price) price(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Dish, Amenity, Restaurant Name, Hours, Location, Rating and O.\nSentence: id like to eat at a good lunch spot for the right price","prediction_output":null,"prediction_outputs":null,"group":null,"words":["id","like","to","eat","at","a","good","lunch","spot","for","the","right","price"],"labels":["O","O","O","O","O","O","O","B-Cuisine","O","B-Price","I-Price","I-Price","O"],"target_index":null,"target_label":null},"label_list":["cuisine","price","dish","amenity","restaurant_name","hours","location","rating"]}
{"id":"710","dataset":"mit-restaurant","split":"test","instance":{"id":"710","prompt_labels":"id(O) like(O) to(O) find(O) a(O) chinese(B-Cuisine) restaurant(O) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Restaurant Name, Price, Rating, Amenity, Cuisine, Hours, Dish and O.\nSentence: id like to find a chinese restaurant nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["id","like","to","find","a","chinese","restaurant","nearby"],"labels":["O","O","O","O","O","B-Cuisine","O","B-Location"],"target_index":null,"target_label":null},"label_list":["location","restaurant_name","price","rating","amenity","cuisine","hours","dish"]}
{"id":"713","dataset":"mit-restaurant","split":"test","instance":{"id":"713","prompt_labels":"id(O) like(O) to(O) know(O) the(O) closest(B-Location) starbucks(B-Restaurant Name) open(O) past(B-Hours) 9(I-Hours) pm(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Hours, Restaurant Name, Amenity, Cuisine, Rating, Dish, Location and O.\nSentence: id like to know the closest starbucks open past 9 pm","prediction_output":null,"prediction_outputs":null,"group":null,"words":["id","like","to","know","the","closest","starbucks","open","past","9","pm"],"labels":["O","O","O","O","O","B-Location","B-Restaurant Name","O","B-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["price","hours","restaurant_name","amenity","cuisine","rating","dish","location"]}
{"id":"717","dataset":"mit-restaurant","split":"test","instance":{"id":"717","prompt_labels":"im(O) craving(O) twice(B-Dish) baked(I-Dish) potatoes(I-Dish) where(O) do(O) they(O) serve(O) them(O) in(O) a(O) ten(B-Location) mile(I-Location) radius(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Dish, Cuisine, Amenity, Rating, Restaurant Name, Price, Location and O.\nSentence: im craving twice baked potatoes where do they serve them in a ten mile radius","prediction_output":null,"prediction_outputs":null,"group":null,"words":["im","craving","twice","baked","potatoes","where","do","they","serve","them","in","a","ten","mile","radius"],"labels":["O","O","B-Dish","I-Dish","I-Dish","O","O","O","O","O","O","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["hours","dish","cuisine","amenity","rating","restaurant_name","price","location"]}
{"id":"720","dataset":"mit-restaurant","split":"test","instance":{"id":"720","prompt_labels":"im(O) hungry(O) and(O) want(O) a(O) tasty(B-Rating) burger(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Location, Dish, Cuisine, Restaurant Name, Rating, Hours and O.\nSentence: im hungry and want a tasty burger","prediction_output":null,"prediction_outputs":null,"group":null,"words":["im","hungry","and","want","a","tasty","burger"],"labels":["O","O","O","O","O","B-Rating","B-Dish"],"target_index":null,"target_label":null},"label_list":["price","amenity","location","dish","cuisine","restaurant_name","rating","hours"]}
{"id":"724","dataset":"mit-restaurant","split":"test","instance":{"id":"724","prompt_labels":"im(O) hungry(O) lets(O) get(O) some(O) tacos(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Price, Amenity, Dish, Rating, Restaurant Name, Location and O.\nSentence: im hungry lets get some tacos","prediction_output":null,"prediction_outputs":null,"group":null,"words":["im","hungry","lets","get","some","tacos"],"labels":["O","O","O","O","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["hours","cuisine","price","amenity","dish","rating","restaurant_name","location"]}
{"id":"735","dataset":"mit-restaurant","split":"test","instance":{"id":"735","prompt_labels":"im(O) looking(O) for(O) a(O) family(B-Amenity) style(I-Amenity) restaurant(O) so(O) i(O) can(O) eat(B-Amenity) at(I-Amenity) the(I-Amenity) bar(I-Amenity) anything(O) within(B-Location) two(I-Location) miles(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Location, Cuisine, Dish, Rating, Restaurant Name, Amenity, Hours and O.\nSentence: im looking for a family style restaurant so i can eat at the bar anything within two miles","prediction_output":null,"prediction_outputs":null,"group":null,"words":["im","looking","for","a","family","style","restaurant","so","i","can","eat","at","the","bar","anything","within","two","miles"],"labels":["O","O","O","O","B-Amenity","I-Amenity","O","O","O","O","B-Amenity","I-Amenity","I-Amenity","I-Amenity","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["price","location","cuisine","dish","rating","restaurant_name","amenity","hours"]}
{"id":"745","dataset":"mit-restaurant","split":"test","instance":{"id":"745","prompt_labels":"im(O) looking(O) for(O) somewhere(O) i(O) can(O) get(O) a(O) lot(B-Amenity) of(I-Amenity) food(I-Amenity) for(O) not(B-Price) too(I-Price) much(I-Price) money(I-Price)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Amenity, Rating, Dish, Location, Restaurant Name, Hours, Price and O.\nSentence: im looking for somewhere i can get a lot of food for not too much money","prediction_output":null,"prediction_outputs":null,"group":null,"words":["im","looking","for","somewhere","i","can","get","a","lot","of","food","for","not","too","much","money"],"labels":["O","O","O","O","O","O","O","O","B-Amenity","I-Amenity","I-Amenity","O","B-Price","I-Price","I-Price","I-Price"],"target_index":null,"target_label":null},"label_list":["cuisine","amenity","rating","dish","location","restaurant_name","hours","price"]}
{"id":"764","dataset":"mit-restaurant","split":"test","instance":{"id":"764","prompt_labels":"is(O) chepes(B-Restaurant Name) restaurant(I-Restaurant Name) on(O) the(O) way(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Rating, Dish, Price, Cuisine, Location, Amenity, Hours and O.\nSentence: is chepes restaurant on the way","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","chepes","restaurant","on","the","way"],"labels":["O","B-Restaurant Name","I-Restaurant Name","O","O","B-Location"],"target_index":null,"target_label":null},"label_list":["restaurant_name","rating","dish","price","cuisine","location","amenity","hours"]}
{"id":"770","dataset":"mit-restaurant","split":"test","instance":{"id":"770","prompt_labels":"is(O) izzys(B-Restaurant Name) ice(I-Restaurant Name) cream(I-Restaurant Name) shop(I-Restaurant Name) open(B-Hours) right(I-Hours) now(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Rating, Hours, Price, Cuisine, Location, Amenity, Restaurant Name and O.\nSentence: is izzys ice cream shop open right now","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","izzys","ice","cream","shop","open","right","now"],"labels":["O","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name","I-Restaurant Name","B-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["dish","rating","hours","price","cuisine","location","amenity","restaurant_name"]}
{"id":"771","dataset":"mit-restaurant","split":"test","instance":{"id":"771","prompt_labels":"is(O) jade(B-Restaurant Name) garden(I-Restaurant Name) within(B-Location) a(I-Location) mile(I-Location) of(O) my(O) current(O) location(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Price, Rating, Location, Dish, Cuisine, Amenity, Hours and O.\nSentence: is jade garden within a mile of my current location","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","jade","garden","within","a","mile","of","my","current","location"],"labels":["O","B-Restaurant Name","I-Restaurant Name","B-Location","I-Location","I-Location","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["restaurant_name","price","rating","location","dish","cuisine","amenity","hours"]}
{"id":"779","dataset":"mit-restaurant","split":"test","instance":{"id":"779","prompt_labels":"is(O) sidney(B-Restaurant Name) and(I-Restaurant Name) hampton(I-Restaurant Name) an(O) expensive(B-Price) hotel(O) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Cuisine, Amenity, Location, Hours, Price, Rating, Dish and O.\nSentence: is sidney and hampton an expensive hotel restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","sidney","and","hampton","an","expensive","hotel","restaurant"],"labels":["O","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name","O","B-Price","O","O"],"target_index":null,"target_label":null},"label_list":["restaurant_name","cuisine","amenity","location","hours","price","rating","dish"]}
{"id":"790","dataset":"mit-restaurant","split":"test","instance":{"id":"790","prompt_labels":"is(O) the(O) rubios(B-Restaurant Name) fresh(I-Restaurant Name) mex(I-Restaurant Name) grill(I-Restaurant Name) within(B-Location) 10(I-Location) miles(I-Location) from(O) my(B-Location) house(I-Location) a(O) hidden(B-Cuisine) find(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Hours, Amenity, Rating, Dish, Cuisine, Location, Restaurant Name and O.\nSentence: is the rubios fresh mex grill within 10 miles from my house a hidden find","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","the","rubios","fresh","mex","grill","within","10","miles","from","my","house","a","hidden","find"],"labels":["O","O","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name","I-Restaurant Name","B-Location","I-Location","I-Location","O","B-Location","I-Location","O","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["price","hours","amenity","rating","dish","cuisine","location","restaurant_name"]}
{"id":"795","dataset":"mit-restaurant","split":"test","instance":{"id":"795","prompt_labels":"is(O) the(O) tylers(B-Restaurant Name) restaurant(I-Restaurant Name) in(O) baltimore(B-Location) a(O) little(B-Price) pricey(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Restaurant Name, Hours, Price, Dish, Rating, Cuisine, Location and O.\nSentence: is the tylers restaurant in baltimore a little pricey","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","the","tylers","restaurant","in","baltimore","a","little","pricey"],"labels":["O","O","B-Restaurant Name","I-Restaurant Name","O","B-Location","O","B-Price","O"],"target_index":null,"target_label":null},"label_list":["amenity","restaurant_name","hours","price","dish","rating","cuisine","location"]}
{"id":"797","dataset":"mit-restaurant","split":"test","instance":{"id":"797","prompt_labels":"is(O) there(O) a(O) bakery(B-Cuisine) near(B-Location) here(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Rating, Location, Dish, Restaurant Name, Amenity, Price and O.\nSentence: is there a bakery near here","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","bakery","near","here"],"labels":["O","O","O","B-Cuisine","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["hours","cuisine","rating","location","dish","restaurant_name","amenity","price"]}
{"id":"801","dataset":"mit-restaurant","split":"test","instance":{"id":"801","prompt_labels":"is(O) there(O) a(O) breakfast(B-Hours) place(O) that(O) has(O) valet(B-Amenity) parking(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Price, Location, Amenity, Rating, Cuisine, Restaurant Name, Dish and O.\nSentence: is there a breakfast place that has valet parking","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","breakfast","place","that","has","valet","parking"],"labels":["O","O","O","B-Hours","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["hours","price","location","amenity","rating","cuisine","restaurant_name","dish"]}
{"id":"804","dataset":"mit-restaurant","split":"test","instance":{"id":"804","prompt_labels":"is(O) there(O) a(O) cheap(B-Price) fast(B-Cuisine) food(I-Cuisine) restaurant(O) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Dish, Amenity, Rating, Location, Hours, Restaurant Name, Cuisine and O.\nSentence: is there a cheap fast food restaurant nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","cheap","fast","food","restaurant","nearby"],"labels":["O","O","O","B-Price","B-Cuisine","I-Cuisine","O","B-Location"],"target_index":null,"target_label":null},"label_list":["price","dish","amenity","rating","location","hours","restaurant_name","cuisine"]}
{"id":"807","dataset":"mit-restaurant","split":"test","instance":{"id":"807","prompt_labels":"is(O) there(O) a(O) chinese(B-Cuisine) buffet(B-Amenity) that(O) serves(O) family(B-Amenity) style(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Location, Price, Dish, Cuisine, Rating, Hours and O.\nSentence: is there a chinese buffet that serves family style","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","chinese","buffet","that","serves","family","style"],"labels":["O","O","O","B-Cuisine","B-Amenity","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["restaurant_name","amenity","location","price","dish","cuisine","rating","hours"]}
{"id":"808","dataset":"mit-restaurant","split":"test","instance":{"id":"808","prompt_labels":"is(O) there(O) a(O) chinese(B-Cuisine) restaurant(O) in(O) the(O) midvale(B-Location) mall(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Amenity, Location, Cuisine, Rating, Hours, Restaurant Name, Price and O.\nSentence: is there a chinese restaurant in the midvale mall","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","chinese","restaurant","in","the","midvale","mall"],"labels":["O","O","O","B-Cuisine","O","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["dish","amenity","location","cuisine","rating","hours","restaurant_name","price"]}
{"id":"810","dataset":"mit-restaurant","split":"test","instance":{"id":"810","prompt_labels":"is(O) there(O) a(O) ciao(B-Restaurant Name) bella(I-Restaurant Name) on(O) holyoke(B-Location) st(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Restaurant Name, Location, Hours, Dish, Rating, Amenity, Cuisine and O.\nSentence: is there a ciao bella on holyoke st","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","ciao","bella","on","holyoke","st"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["price","restaurant_name","location","hours","dish","rating","amenity","cuisine"]}
{"id":"811","dataset":"mit-restaurant","split":"test","instance":{"id":"811","prompt_labels":"is(O) there(O) a(O) club(B-Amenity) diner(B-Cuisine) in(O) watertown(B-Location) with(O) a(O) bar(B-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Dish, Rating, Location, Amenity, Restaurant Name, Cuisine, Hours and O.\nSentence: is there a club diner in watertown with a bar","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","club","diner","in","watertown","with","a","bar"],"labels":["O","O","O","B-Amenity","B-Cuisine","O","B-Location","O","O","B-Amenity"],"target_index":null,"target_label":null},"label_list":["price","dish","rating","location","amenity","restaurant_name","cuisine","hours"]}
{"id":"812","dataset":"mit-restaurant","split":"test","instance":{"id":"812","prompt_labels":"is(O) there(O) a(O) deli(B-Cuisine) nearby(B-Location) that(O) takes(B-Amenity) credit(I-Amenity) cards(I-Amenity) and(O) is(O) open(B-Hours) right(I-Hours) now(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Hours, Dish, Amenity, Rating, Restaurant Name, Location and O.\nSentence: is there a deli nearby that takes credit cards and is open right now","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","deli","nearby","that","takes","credit","cards","and","is","open","right","now"],"labels":["O","O","O","B-Cuisine","B-Location","O","B-Amenity","I-Amenity","I-Amenity","O","O","B-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["cuisine","price","hours","dish","amenity","rating","restaurant_name","location"]}
{"id":"824","dataset":"mit-restaurant","split":"test","instance":{"id":"824","prompt_labels":"is(O) there(O) a(O) japanese(B-Cuisine) restraunt(O) near(B-Location) by(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Dish, Restaurant Name, Hours, Price, Location, Cuisine and O.\nSentence: is there a japanese restraunt near by","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","japanese","restraunt","near","by"],"labels":["O","O","O","B-Cuisine","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["rating","amenity","dish","restaurant_name","hours","price","location","cuisine"]}
{"id":"827","dataset":"mit-restaurant","split":"test","instance":{"id":"827","prompt_labels":"is(O) there(O) a(O) late(B-Hours) night(I-Hours) place(O) in(O) west(B-Location) newbury(I-Location) where(O) i(O) can(O) smoke(B-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Restaurant Name, Rating, Hours, Amenity, Cuisine, Dish, Price and O.\nSentence: is there a late night place in west newbury where i can smoke","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","late","night","place","in","west","newbury","where","i","can","smoke"],"labels":["O","O","O","B-Hours","I-Hours","O","O","B-Location","I-Location","O","O","O","B-Amenity"],"target_index":null,"target_label":null},"label_list":["location","restaurant_name","rating","hours","amenity","cuisine","dish","price"]}
{"id":"829","dataset":"mit-restaurant","split":"test","instance":{"id":"829","prompt_labels":"is(O) there(O) a(O) malaysian(B-Cuisine) restaurant(O) near(O) roxbury(B-Location) crossing(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Restaurant Name, Cuisine, Price, Amenity, Rating, Hours, Location and O.\nSentence: is there a malaysian restaurant near roxbury crossing","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","malaysian","restaurant","near","roxbury","crossing"],"labels":["O","O","O","B-Cuisine","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["dish","restaurant_name","cuisine","price","amenity","rating","hours","location"]}
{"id":"833","dataset":"mit-restaurant","split":"test","instance":{"id":"833","prompt_labels":"is(O) there(O) a(O) mcdonalds(B-Restaurant Name) within(B-Location) two(I-Location) miles(I-Location) of(O) here(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Hours, Amenity, Dish, Rating, Cuisine, Restaurant Name, Price and O.\nSentence: is there a mcdonalds within two miles of here","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","mcdonalds","within","two","miles","of","here"],"labels":["O","O","O","B-Restaurant Name","B-Location","I-Location","I-Location","O","O"],"target_index":null,"target_label":null},"label_list":["location","hours","amenity","dish","rating","cuisine","restaurant_name","price"]}
{"id":"835","dataset":"mit-restaurant","split":"test","instance":{"id":"835","prompt_labels":"is(O) there(O) a(O) moderately(B-Price) priced(O) french(B-Cuisine) restaurant(O) in(B-Location) this(I-Location) area(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Rating, Amenity, Price, Restaurant Name, Hours, Cuisine, Location and O.\nSentence: is there a moderately priced french restaurant in this area","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","moderately","priced","french","restaurant","in","this","area"],"labels":["O","O","O","B-Price","O","B-Cuisine","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["dish","rating","amenity","price","restaurant_name","hours","cuisine","location"]}
{"id":"839","dataset":"mit-restaurant","split":"test","instance":{"id":"839","prompt_labels":"is(O) there(O) a(O) pancake(B-Cuisine) house(I-Cuisine) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Amenity, Price, Cuisine, Location, Rating, Hours, Restaurant Name and O.\nSentence: is there a pancake house nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","pancake","house","nearby"],"labels":["O","O","O","B-Cuisine","I-Cuisine","B-Location"],"target_index":null,"target_label":null},"label_list":["dish","amenity","price","cuisine","location","rating","hours","restaurant_name"]}
{"id":"840","dataset":"mit-restaurant","split":"test","instance":{"id":"840","prompt_labels":"is(O) there(O) a(O) patio(B-Amenity) at(O) w(B-Restaurant Name) a(I-Restaurant Name) frost(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Rating, Dish, Cuisine, Restaurant Name, Location, Price, Amenity and O.\nSentence: is there a patio at w a frost","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","patio","at","w","a","frost"],"labels":["O","O","O","B-Amenity","O","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["hours","rating","dish","cuisine","restaurant_name","location","price","amenity"]}
{"id":"841","dataset":"mit-restaurant","split":"test","instance":{"id":"841","prompt_labels":"is(O) there(O) a(O) place(O) near(B-Location) by(I-Location) that(O) serves(O) tapas(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Restaurant Name, Dish, Price, Location, Amenity, Rating, Hours and O.\nSentence: is there a place near by that serves tapas","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","place","near","by","that","serves","tapas"],"labels":["O","O","O","O","B-Location","I-Location","O","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["cuisine","restaurant_name","dish","price","location","amenity","rating","hours"]}
{"id":"849","dataset":"mit-restaurant","split":"test","instance":{"id":"849","prompt_labels":"is(O) there(O) a(O) place(O) within(B-Location) 10(I-Location) minutes(I-Location) that(O) has(O) great(B-Amenity) atmosphere(I-Amenity) for(O) a(O) special(O) meal(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Restaurant Name, Price, Rating, Amenity, Hours, Location, Dish and O.\nSentence: is there a place within 10 minutes that has great atmosphere for a special meal","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","place","within","10","minutes","that","has","great","atmosphere","for","a","special","meal"],"labels":["O","O","O","O","B-Location","I-Location","I-Location","O","O","B-Amenity","I-Amenity","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["cuisine","restaurant_name","price","rating","amenity","hours","location","dish"]}
{"id":"852","dataset":"mit-restaurant","split":"test","instance":{"id":"852","prompt_labels":"is(O) there(O) a(O) portuguese(B-Cuisine) restaurant(O) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Amenity, Rating, Price, Hours, Dish, Restaurant Name, Location and O.\nSentence: is there a portuguese restaurant nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","portuguese","restaurant","nearby"],"labels":["O","O","O","B-Cuisine","O","B-Location"],"target_index":null,"target_label":null},"label_list":["cuisine","amenity","rating","price","hours","dish","restaurant_name","location"]}
{"id":"853","dataset":"mit-restaurant","split":"test","instance":{"id":"853","prompt_labels":"is(O) there(O) a(O) rancho(B-Restaurant Name) veo(I-Restaurant Name) restaurant(O) in(O) north(B-Location) memphis(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Dish, Restaurant Name, Location, Amenity, Price, Hours, Rating and O.\nSentence: is there a rancho veo restaurant in north memphis","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","rancho","veo","restaurant","in","north","memphis"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["cuisine","dish","restaurant_name","location","amenity","price","hours","rating"]}
{"id":"857","dataset":"mit-restaurant","split":"test","instance":{"id":"857","prompt_labels":"is(O) there(O) a(O) restaurant(O) around(B-Location) here(I-Location) that(O) serves(O) chicken(B-Dish) wings(I-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Cuisine, Price, Location, Restaurant Name, Rating, Dish, Hours and O.\nSentence: is there a restaurant around here that serves chicken wings","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","restaurant","around","here","that","serves","chicken","wings"],"labels":["O","O","O","O","B-Location","I-Location","O","O","B-Dish","I-Dish"],"target_index":null,"target_label":null},"label_list":["amenity","cuisine","price","location","restaurant_name","rating","dish","hours"]}
{"id":"863","dataset":"mit-restaurant","split":"test","instance":{"id":"863","prompt_labels":"is(O) there(O) a(O) restaurant(O) on(O) the(O) way(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Amenity, Price, Hours, Cuisine, Restaurant Name, Dish, Rating and O.\nSentence: is there a restaurant on the way","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","restaurant","on","the","way"],"labels":["O","O","O","O","O","O","B-Location"],"target_index":null,"target_label":null},"label_list":["location","amenity","price","hours","cuisine","restaurant_name","dish","rating"]}
{"id":"871","dataset":"mit-restaurant","split":"test","instance":{"id":"871","prompt_labels":"is(O) there(O) a(O) small(B-Amenity) place(O) in(B-Location) franklin(I-Location) that(O) has(O) beans(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Location, Rating, Hours, Price, Dish, Restaurant Name, Cuisine and O.\nSentence: is there a small place in franklin that has beans","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","small","place","in","franklin","that","has","beans"],"labels":["O","O","O","B-Amenity","O","B-Location","I-Location","O","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["amenity","location","rating","hours","price","dish","restaurant_name","cuisine"]}
{"id":"876","dataset":"mit-restaurant","split":"test","instance":{"id":"876","prompt_labels":"is(O) there(O) a(O) swensens(B-Restaurant Name) restaurant(I-Restaurant Name) in(B-Location) the(I-Location) north(I-Location) end(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Restaurant Name, Amenity, Cuisine, Location, Rating, Dish, Hours and O.\nSentence: is there a swensens restaurant in the north end","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","swensens","restaurant","in","the","north","end"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name","B-Location","I-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["price","restaurant_name","amenity","cuisine","location","rating","dish","hours"]}
{"id":"877","dataset":"mit-restaurant","split":"test","instance":{"id":"877","prompt_labels":"is(O) there(O) a(O) taco(B-Cuisine) joint(I-Cuisine) near(B-Location) the(I-Location) college(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Location, Rating, Restaurant Name, Cuisine, Price, Hours and O.\nSentence: is there a taco joint near the college","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","taco","joint","near","the","college"],"labels":["O","O","O","B-Cuisine","I-Cuisine","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["amenity","dish","location","rating","restaurant_name","cuisine","price","hours"]}
{"id":"881","dataset":"mit-restaurant","split":"test","instance":{"id":"881","prompt_labels":"is(O) there(O) a(O) very(B-Price) high(I-Price) end(I-Price) pastry(B-Cuisine) place(O) close(B-Location) to(I-Location) me(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Cuisine, Restaurant Name, Price, Location, Hours, Dish and O.\nSentence: is there a very high end pastry place close to me","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","very","high","end","pastry","place","close","to","me"],"labels":["O","O","O","B-Price","I-Price","I-Price","B-Cuisine","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["rating","amenity","cuisine","restaurant_name","price","location","hours","dish"]}
{"id":"887","dataset":"mit-restaurant","split":"test","instance":{"id":"887","prompt_labels":"is(O) there(O) an(O) applebees(B-Restaurant Name) in(O) hicksville(B-Location) ny(I-Location) with(O) a(O) special(B-Amenity) 10(I-Amenity) menu(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Hours, Location, Price, Amenity, Dish, Restaurant Name, Cuisine and O.\nSentence: is there an applebees in hicksville ny with a special 10 menu","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","an","applebees","in","hicksville","ny","with","a","special","10","menu"],"labels":["O","O","O","B-Restaurant Name","O","B-Location","I-Location","O","O","B-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["rating","hours","location","price","amenity","dish","restaurant_name","cuisine"]}
{"id":"890","dataset":"mit-restaurant","split":"test","instance":{"id":"890","prompt_labels":"is(O) there(O) an(O) expensive(B-Price) hotel(O) named(O) sidney(B-Restaurant Name) and(I-Restaurant Name) hampton(I-Restaurant Name) with(O) dining(B-Amenity) services(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Cuisine, Price, Restaurant Name, Hours, Rating, Dish, Location and O.\nSentence: is there an expensive hotel named sidney and hampton with dining services","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","an","expensive","hotel","named","sidney","and","hampton","with","dining","services"],"labels":["O","O","O","B-Price","O","O","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["amenity","cuisine","price","restaurant_name","hours","rating","dish","location"]}
{"id":"895","dataset":"mit-restaurant","split":"test","instance":{"id":"895","prompt_labels":"is(O) there(O) an(O) italian(B-Cuisine) place(O) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Dish, Restaurant Name, Cuisine, Location, Price, Hours and O.\nSentence: is there an italian place nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","an","italian","place","nearby"],"labels":["O","O","O","B-Cuisine","O","B-Location"],"target_index":null,"target_label":null},"label_list":["rating","amenity","dish","restaurant_name","cuisine","location","price","hours"]}
{"id":"896","dataset":"mit-restaurant","split":"test","instance":{"id":"896","prompt_labels":"is(O) there(O) an(O) olive(B-Restaurant Name) garden(I-Restaurant Name) in(O) kendall(B-Location) square(I-Location) with(O) private(B-Amenity) dining(I-Amenity) rooms(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Restaurant Name, Price, Rating, Dish, Cuisine, Hours, Location and O.\nSentence: is there an olive garden in kendall square with private dining rooms","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","an","olive","garden","in","kendall","square","with","private","dining","rooms"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name","O","B-Location","I-Location","O","B-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["amenity","restaurant_name","price","rating","dish","cuisine","hours","location"]}
{"id":"898","dataset":"mit-restaurant","split":"test","instance":{"id":"898","prompt_labels":"is(O) there(O) an(O) restaurant(O) in(O) this(B-Location) part(I-Location) of(I-Location) town(I-Location) that(O) serves(O) thai(B-Cuisine) food(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Price, Amenity, Dish, Rating, Restaurant Name, Hours, Cuisine and O.\nSentence: is there an restaurant in this part of town that serves thai food","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","an","restaurant","in","this","part","of","town","that","serves","thai","food"],"labels":["O","O","O","O","O","B-Location","I-Location","I-Location","I-Location","O","O","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["location","price","amenity","dish","rating","restaurant_name","hours","cuisine"]}
{"id":"927","dataset":"mit-restaurant","split":"test","instance":{"id":"927","prompt_labels":"is(O) there(O) somewhere(O) to(O) eat(O) that(O) is(O) kid(B-Amenity) friendly(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Price, Cuisine, Hours, Dish, Rating, Location, Amenity and O.\nSentence: is there somewhere to eat that is kid friendly","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","somewhere","to","eat","that","is","kid","friendly"],"labels":["O","O","O","O","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["restaurant_name","price","cuisine","hours","dish","rating","location","amenity"]}
{"id":"928","dataset":"mit-restaurant","split":"test","instance":{"id":"928","prompt_labels":"is(O) this(O) restaurant(O) a(O) hidden(B-Amenity) find(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Dish, Rating, Cuisine, Price, Hours, Restaurant Name, Amenity and O.\nSentence: is this restaurant a hidden find","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","this","restaurant","a","hidden","find"],"labels":["O","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["location","dish","rating","cuisine","price","hours","restaurant_name","amenity"]}
{"id":"937","dataset":"mit-restaurant","split":"test","instance":{"id":"937","prompt_labels":"lets(O) find(O) the(O) fanciest(B-Amenity) french(B-Cuisine) cuisine(O) in(B-Location) the(I-Location) city(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Dish, Amenity, Location, Rating, Restaurant Name, Hours and O.\nSentence: lets find the fanciest french cuisine in the city","prediction_output":null,"prediction_outputs":null,"group":null,"words":["lets","find","the","fanciest","french","cuisine","in","the","city"],"labels":["O","O","O","B-Amenity","B-Cuisine","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["cuisine","price","dish","amenity","location","rating","restaurant_name","hours"]}
{"id":"940","dataset":"mit-restaurant","split":"test","instance":{"id":"940","prompt_labels":"list(O) all(O) restaurants(O) that(O) cater(B-Amenity) to(I-Amenity) families(I-Amenity) within(B-Location) a(I-Location) ten(I-Location) mile(I-Location) location(B-Amenity) of(O) my(O) current(B-Location) location(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Dish, Amenity, Location, Price, Rating, Cuisine and O.\nSentence: list all restaurants that cater to families within a ten mile location of my current location","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","all","restaurants","that","cater","to","families","within","a","ten","mile","location","of","my","current","location"],"labels":["O","O","O","O","B-Amenity","I-Amenity","I-Amenity","B-Location","I-Location","I-Location","I-Location","B-Amenity","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["hours","restaurant_name","dish","amenity","location","price","rating","cuisine"]}
{"id":"953","dataset":"mit-restaurant","split":"test","instance":{"id":"953","prompt_labels":"locate(O) an(O) all(B-Amenity) you(I-Amenity) can(I-Amenity) eat(I-Amenity) buffet(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Cuisine, Amenity, Dish, Price, Hours, Location and O.\nSentence: locate an all you can eat buffet","prediction_output":null,"prediction_outputs":null,"group":null,"words":["locate","an","all","you","can","eat","buffet"],"labels":["O","O","B-Amenity","I-Amenity","I-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["rating","restaurant_name","cuisine","amenity","dish","price","hours","location"]}
{"id":"957","dataset":"mit-restaurant","split":"test","instance":{"id":"957","prompt_labels":"look(O) for(O) inexpensive(B-Price) authentic(B-Cuisine) mexican(I-Cuisine) restaurants(O) in(O) chattanooga(B-Location) tennessee(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Location, Price, Dish, Cuisine, Restaurant Name, Hours and O.\nSentence: look for inexpensive authentic mexican restaurants in chattanooga tennessee","prediction_output":null,"prediction_outputs":null,"group":null,"words":["look","for","inexpensive","authentic","mexican","restaurants","in","chattanooga","tennessee"],"labels":["O","O","B-Price","B-Cuisine","I-Cuisine","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["amenity","rating","location","price","dish","cuisine","restaurant_name","hours"]}
{"id":"958","dataset":"mit-restaurant","split":"test","instance":{"id":"958","prompt_labels":"look(O) up(O) the(O) reviews(O) for(O) this(O) new(O) asain(B-Cuisine) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Price, Restaurant Name, Hours, Rating, Cuisine, Amenity, Dish and O.\nSentence: look up the reviews for this new asain restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["look","up","the","reviews","for","this","new","asain","restaurant"],"labels":["O","O","O","O","O","O","O","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["location","price","restaurant_name","hours","rating","cuisine","amenity","dish"]}
{"id":"961","dataset":"mit-restaurant","split":"test","instance":{"id":"961","prompt_labels":"looking(O) for(O) a(O) drive(B-Location) through(O) restaurant(O) close(B-Location) by(I-Location) opened(B-Hours) 24(I-Hours) 7(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Rating, Price, Hours, Location, Cuisine, Dish and O.\nSentence: looking for a drive through restaurant close by opened 24 7","prediction_output":null,"prediction_outputs":null,"group":null,"words":["looking","for","a","drive","through","restaurant","close","by","opened","24","7"],"labels":["O","O","O","B-Location","O","O","B-Location","I-Location","B-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["restaurant_name","amenity","rating","price","hours","location","cuisine","dish"]}
{"id":"967","dataset":"mit-restaurant","split":"test","instance":{"id":"967","prompt_labels":"looking(O) for(O) a(O) strong(O) fair(B-Price) priced(O) restaurant(O) that(O) is(O) near(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Restaurant Name, Amenity, Dish, Rating, Price, Hours, Cuisine and O.\nSentence: looking for a strong fair priced restaurant that is near","prediction_output":null,"prediction_outputs":null,"group":null,"words":["looking","for","a","strong","fair","priced","restaurant","that","is","near"],"labels":["O","O","O","O","B-Price","O","O","O","O","B-Location"],"target_index":null,"target_label":null},"label_list":["location","restaurant_name","amenity","dish","rating","price","hours","cuisine"]}
{"id":"981","dataset":"mit-restaurant","split":"test","instance":{"id":"981","prompt_labels":"looking(O) for(O) reasonably(B-Price) priced(O) diners(B-Cuisine)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Restaurant Name, Cuisine, Price, Location, Rating, Dish and O.\nSentence: looking for reasonably priced diners","prediction_output":null,"prediction_outputs":null,"group":null,"words":["looking","for","reasonably","priced","diners"],"labels":["O","O","B-Price","O","B-Cuisine"],"target_index":null,"target_label":null},"label_list":["amenity","hours","restaurant_name","cuisine","price","location","rating","dish"]}
{"id":"983","dataset":"mit-restaurant","split":"test","instance":{"id":"983","prompt_labels":"looking(O) for(O) star(B-Amenity) hangout(I-Amenity) close(B-Location) by(I-Location) called(O) marios(B-Restaurant Name) italian(I-Restaurant Name) restaurant(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Restaurant Name, Cuisine, Dish, Hours, Location, Price and O.\nSentence: looking for star hangout close by called marios italian restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["looking","for","star","hangout","close","by","called","marios","italian","restaurant"],"labels":["O","O","B-Amenity","I-Amenity","B-Location","I-Location","O","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["rating","amenity","restaurant_name","cuisine","dish","hours","location","price"]}
{"id":"989","dataset":"mit-restaurant","split":"test","instance":{"id":"989","prompt_labels":"looking(O) for(O) urban(B-Cuisine) gourmet(I-Cuisine) on(O) bay(B-Location) road(I-Location) with(O) great(B-Rating) wine(O) lists(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Amenity, Restaurant Name, Dish, Location, Hours, Rating and O.\nSentence: looking for urban gourmet on bay road with great wine lists","prediction_output":null,"prediction_outputs":null,"group":null,"words":["looking","for","urban","gourmet","on","bay","road","with","great","wine","lists"],"labels":["O","O","B-Cuisine","I-Cuisine","O","B-Location","I-Location","O","B-Rating","O","O"],"target_index":null,"target_label":null},"label_list":["cuisine","price","amenity","restaurant_name","dish","location","hours","rating"]}
{"id":"990","dataset":"mit-restaurant","split":"test","instance":{"id":"990","prompt_labels":"looking(O) for(O) very(B-Price) cheap(I-Price) sake(B-Cuisine) restaurant(O) that(O) allows(B-Amenity) smoking(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Rating, Cuisine, Amenity, Hours, Dish, Price, Restaurant Name and O.\nSentence: looking for very cheap sake restaurant that allows smoking","prediction_output":null,"prediction_outputs":null,"group":null,"words":["looking","for","very","cheap","sake","restaurant","that","allows","smoking"],"labels":["O","O","B-Price","I-Price","B-Cuisine","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["location","rating","cuisine","amenity","hours","dish","price","restaurant_name"]}
{"id":"991","dataset":"mit-restaurant","split":"test","instance":{"id":"991","prompt_labels":"make(O) a(O) 5(O) 00(O) p(O) m(O) reservation(O) for(O) black(B-Restaurant Name) angus(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Amenity, Cuisine, Hours, Dish, Price, Restaurant Name, Rating and O.\nSentence: make a 5 00 p m reservation for black angus","prediction_output":null,"prediction_outputs":null,"group":null,"words":["make","a","5","00","p","m","reservation","for","black","angus"],"labels":["O","O","O","O","O","O","O","O","B-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["location","amenity","cuisine","hours","dish","price","restaurant_name","rating"]}
{"id":"992","dataset":"mit-restaurant","split":"test","instance":{"id":"992","prompt_labels":"make(O) a(O) reservation(B-Amenity) for(I-Amenity) 3(I-Amenity) people(I-Amenity) at(O) 8(B-Hours) pm(I-Hours) tonight(I-Hours) at(O) the(O) melting(B-Restaurant Name) pot(I-Restaurant Name) in(O) towson(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Amenity, Restaurant Name, Price, Dish, Location, Rating, Cuisine and O.\nSentence: make a reservation for 3 people at 8 pm tonight at the melting pot in towson","prediction_output":null,"prediction_outputs":null,"group":null,"words":["make","a","reservation","for","3","people","at","8","pm","tonight","at","the","melting","pot","in","towson"],"labels":["O","O","B-Amenity","I-Amenity","I-Amenity","I-Amenity","O","B-Hours","I-Hours","I-Hours","O","O","B-Restaurant Name","I-Restaurant Name","O","B-Location"],"target_index":null,"target_label":null},"label_list":["hours","amenity","restaurant_name","price","dish","location","rating","cuisine"]}
{"id":"997","dataset":"mit-restaurant","split":"test","instance":{"id":"997","prompt_labels":"make(O) me(O) reservations(O) for(O) three(O) people(O) at(O) devitos(B-Restaurant Name) italian(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Dish, Restaurant Name, Location, Amenity, Hours, Price and O.\nSentence: make me reservations for three people at devitos italian","prediction_output":null,"prediction_outputs":null,"group":null,"words":["make","me","reservations","for","three","people","at","devitos","italian"],"labels":["O","O","O","O","O","O","O","B-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["cuisine","rating","dish","restaurant_name","location","amenity","hours","price"]}
{"id":"999","dataset":"mit-restaurant","split":"test","instance":{"id":"999","prompt_labels":"mexican(B-Cuisine) food(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Dish, Hours, Restaurant Name, Location, Price, Amenity and O.\nSentence: mexican food","prediction_output":null,"prediction_outputs":null,"group":null,"words":["mexican","food"],"labels":["B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["cuisine","rating","dish","hours","restaurant_name","location","price","amenity"]}
{"id":"1000","dataset":"mit-restaurant","split":"test","instance":{"id":"1000","prompt_labels":"mexican(B-Cuisine) food(O) to(B-Amenity) go(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Hours, Price, Rating, Restaurant Name, Cuisine, Amenity, Location and O.\nSentence: mexican food to go","prediction_output":null,"prediction_outputs":null,"group":null,"words":["mexican","food","to","go"],"labels":["B-Cuisine","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["dish","hours","price","rating","restaurant_name","cuisine","amenity","location"]}
{"id":"1003","dataset":"mit-restaurant","split":"test","instance":{"id":"1003","prompt_labels":"my(O) date(O) and(O) i(O) would(O) like(O) some(O) espresso(B-Dish) thats(O) within(B-Location) 10(I-Location) min(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Location, Cuisine, Rating, Price, Hours, Restaurant Name and O.\nSentence: my date and i would like some espresso thats within 10 min","prediction_output":null,"prediction_outputs":null,"group":null,"words":["my","date","and","i","would","like","some","espresso","thats","within","10","min"],"labels":["O","O","O","O","O","O","O","B-Dish","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["amenity","dish","location","cuisine","rating","price","hours","restaurant_name"]}
{"id":"1008","dataset":"mit-restaurant","split":"test","instance":{"id":"1008","prompt_labels":"name(O) the(O) local(B-Location) buffets(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Location, Price, Hours, Rating, Restaurant Name, Dish, Amenity and O.\nSentence: name the local buffets","prediction_output":null,"prediction_outputs":null,"group":null,"words":["name","the","local","buffets"],"labels":["O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["cuisine","location","price","hours","rating","restaurant_name","dish","amenity"]}
{"id":"1009","dataset":"mit-restaurant","split":"test","instance":{"id":"1009","prompt_labels":"navigate(O) me(O) to(O) a(O) thai(B-Cuisine) restaurant(O) thats(O) 4(B-Rating) stars(I-Rating) or(I-Rating) higher(I-Rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Cuisine, Rating, Restaurant Name, Dish, Hours, Amenity, Location and O.\nSentence: navigate me to a thai restaurant thats 4 stars or higher","prediction_output":null,"prediction_outputs":null,"group":null,"words":["navigate","me","to","a","thai","restaurant","thats","4","stars","or","higher"],"labels":["O","O","O","O","B-Cuisine","O","O","B-Rating","I-Rating","I-Rating","I-Rating"],"target_index":null,"target_label":null},"label_list":["price","cuisine","rating","restaurant_name","dish","hours","amenity","location"]}
{"id":"1010","dataset":"mit-restaurant","split":"test","instance":{"id":"1010","prompt_labels":"nearest(B-Location) fast(B-Cuisine) food(I-Cuisine) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Restaurant Name, Rating, Amenity, Price, Hours, Dish, Cuisine and O.\nSentence: nearest fast food restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["nearest","fast","food","restaurant"],"labels":["B-Location","B-Cuisine","I-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["location","restaurant_name","rating","amenity","price","hours","dish","cuisine"]}
{"id":"1011","dataset":"mit-restaurant","split":"test","instance":{"id":"1011","prompt_labels":"need(O) a(O) four(B-Rating) star(I-Rating) restaurant(O) in(B-Location) a(I-Location) hotel(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Dish, Restaurant Name, Hours, Location, Price, Amenity, Rating and O.\nSentence: need a four star restaurant in a hotel","prediction_output":null,"prediction_outputs":null,"group":null,"words":["need","a","four","star","restaurant","in","a","hotel"],"labels":["O","O","B-Rating","I-Rating","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["cuisine","dish","restaurant_name","hours","location","price","amenity","rating"]}
{"id":"1013","dataset":"mit-restaurant","split":"test","instance":{"id":"1013","prompt_labels":"newbury(B-Restaurant Name) street(I-Restaurant Name) bakery(I-Restaurant Name) and(I-Restaurant Name) deli(I-Restaurant Name) directions(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Rating, Amenity, Location, Hours, Restaurant Name, Price, Cuisine and O.\nSentence: newbury street bakery and deli directions","prediction_output":null,"prediction_outputs":null,"group":null,"words":["newbury","street","bakery","and","deli","directions"],"labels":["B-Restaurant Name","I-Restaurant Name","I-Restaurant Name","I-Restaurant Name","I-Restaurant Name","O"],"target_index":null,"target_label":null},"label_list":["dish","rating","amenity","location","hours","restaurant_name","price","cuisine"]}
{"id":"1014","dataset":"mit-restaurant","split":"test","instance":{"id":"1014","prompt_labels":"nyc(B-Location) 5(B-Rating) star(I-Rating) pizza(B-Dish) parlors(B-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Location, Rating, Hours, Restaurant Name, Dish, Cuisine, Amenity and O.\nSentence: nyc 5 star pizza parlors","prediction_output":null,"prediction_outputs":null,"group":null,"words":["nyc","5","star","pizza","parlors"],"labels":["B-Location","B-Rating","I-Rating","B-Dish","B-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["price","location","rating","hours","restaurant_name","dish","cuisine","amenity"]}
{"id":"1015","dataset":"mit-restaurant","split":"test","instance":{"id":"1015","prompt_labels":"of(O) the(O) restaurants(O) that(O) require(B-Amenity) suit(I-Amenity) jackets(I-Amenity) for(I-Amenity) men(I-Amenity) which(O) have(O) the(O) best(B-Rating) service(I-Rating) and(I-Rating) food(I-Rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Location, Hours, Price, Cuisine, Dish, Restaurant Name and O.\nSentence: of the restaurants that require suit jackets for men which have the best service and food","prediction_output":null,"prediction_outputs":null,"group":null,"words":["of","the","restaurants","that","require","suit","jackets","for","men","which","have","the","best","service","and","food"],"labels":["O","O","O","O","B-Amenity","I-Amenity","I-Amenity","I-Amenity","I-Amenity","O","O","O","B-Rating","I-Rating","I-Rating","I-Rating"],"target_index":null,"target_label":null},"label_list":["amenity","rating","location","hours","price","cuisine","dish","restaurant_name"]}
{"id":"1018","dataset":"mit-restaurant","split":"test","instance":{"id":"1018","prompt_labels":"please(O) find(O) a(O) mexican(B-Cuisine) restaurant(O) with(O) a(O) smoking(B-Amenity) section(I-Amenity) and(O) more(B-Rating) then(I-Rating) 1(I-Rating) star(I-Rating) review(I-Rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Hours, Restaurant Name, Amenity, Location, Dish, Price, Rating and O.\nSentence: please find a mexican restaurant with a smoking section and more then 1 star review","prediction_output":null,"prediction_outputs":null,"group":null,"words":["please","find","a","mexican","restaurant","with","a","smoking","section","and","more","then","1","star","review"],"labels":["O","O","O","B-Cuisine","O","O","O","B-Amenity","I-Amenity","O","B-Rating","I-Rating","I-Rating","I-Rating","I-Rating"],"target_index":null,"target_label":null},"label_list":["cuisine","hours","restaurant_name","amenity","location","dish","price","rating"]}
{"id":"1019","dataset":"mit-restaurant","split":"test","instance":{"id":"1019","prompt_labels":"please(O) find(O) a(O) restaurant(O) where(O) i(O) can(O) order(O) cocktails(B-Amenity) with(O) lunch(B-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Dish, Hours, Price, Restaurant Name, Rating, Cuisine, Amenity and O.\nSentence: please find a restaurant where i can order cocktails with lunch","prediction_output":null,"prediction_outputs":null,"group":null,"words":["please","find","a","restaurant","where","i","can","order","cocktails","with","lunch"],"labels":["O","O","O","O","O","O","O","O","B-Amenity","O","B-Amenity"],"target_index":null,"target_label":null},"label_list":["location","dish","hours","price","restaurant_name","rating","cuisine","amenity"]}
{"id":"1028","dataset":"mit-restaurant","split":"test","instance":{"id":"1028","prompt_labels":"please(O) get(O) me(O) the(O) street(B-Location) address(I-Location) of(O) the(O) dennys(B-Restaurant Name) in(O) this(O) area(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Cuisine, Hours, Amenity, Price, Location, Dish, Rating and O.\nSentence: please get me the street address of the dennys in this area","prediction_output":null,"prediction_outputs":null,"group":null,"words":["please","get","me","the","street","address","of","the","dennys","in","this","area"],"labels":["O","O","O","O","B-Location","I-Location","O","O","B-Restaurant Name","O","O","B-Location"],"target_index":null,"target_label":null},"label_list":["restaurant_name","cuisine","hours","amenity","price","location","dish","rating"]}
{"id":"1033","dataset":"mit-restaurant","split":"test","instance":{"id":"1033","prompt_labels":"please(O) locate(O) a(O) bar(B-Cuisine) off(B-Location) of(I-Location) main(I-Location) street(I-Location) down(I-Location) town(I-Location) that(O) has(O) valet(B-Amenity) parking(I-Amenity) is(O) open(B-Hours) late(I-Hours) and(O) has(O) good(B-Rating) reviews(I-Rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Restaurant Name, Cuisine, Price, Hours, Amenity, Rating and O.\nSentence: please locate a bar off of main street down town that has valet parking is open late and has good reviews","prediction_output":null,"prediction_outputs":null,"group":null,"words":["please","locate","a","bar","off","of","main","street","down","town","that","has","valet","parking","is","open","late","and","has","good","reviews"],"labels":["O","O","O","B-Cuisine","B-Location","I-Location","I-Location","I-Location","I-Location","I-Location","O","O","B-Amenity","I-Amenity","O","B-Hours","I-Hours","O","O","B-Rating","I-Rating"],"target_index":null,"target_label":null},"label_list":["dish","location","restaurant_name","cuisine","price","hours","amenity","rating"]}
{"id":"1040","dataset":"mit-restaurant","split":"test","instance":{"id":"1040","prompt_labels":"pub(B-Cuisine)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Price, Restaurant Name, Amenity, Dish, Hours, Cuisine, Rating and O.\nSentence: pub","prediction_output":null,"prediction_outputs":null,"group":null,"words":["pub"],"labels":["B-Cuisine"],"target_index":null,"target_label":null},"label_list":["location","price","restaurant_name","amenity","dish","hours","cuisine","rating"]}
{"id":"1049","dataset":"mit-restaurant","split":"test","instance":{"id":"1049","prompt_labels":"search(O) for(O) a(O) buffet(B-Cuisine) that(O) is(O) a(O) smoke(B-Amenity) free(I-Amenity) environment(I-Amenity) within(B-Location) 5(I-Location) miles(I-Location) of(I-Location) my(I-Location) location(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Hours, Restaurant Name, Location, Dish, Amenity, Cuisine, Price and O.\nSentence: search for a buffet that is a smoke free environment within 5 miles of my location","prediction_output":null,"prediction_outputs":null,"group":null,"words":["search","for","a","buffet","that","is","a","smoke","free","environment","within","5","miles","of","my","location"],"labels":["O","O","O","B-Cuisine","O","O","O","B-Amenity","I-Amenity","I-Amenity","B-Location","I-Location","I-Location","I-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["rating","hours","restaurant_name","location","dish","amenity","cuisine","price"]}
{"id":"1058","dataset":"mit-restaurant","split":"test","instance":{"id":"1058","prompt_labels":"show(O) me(O) some(O) cheap(B-Price) eats(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Restaurant Name, Price, Rating, Location, Hours, Amenity, Dish and O.\nSentence: show me some cheap eats","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","some","cheap","eats"],"labels":["O","O","O","B-Price","O"],"target_index":null,"target_label":null},"label_list":["cuisine","restaurant_name","price","rating","location","hours","amenity","dish"]}
{"id":"1062","dataset":"mit-restaurant","split":"test","instance":{"id":"1062","prompt_labels":"show(O) me(O) the(O) three(O) closest(B-Location) wendys(B-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Dish, Location, Restaurant Name, Price, Rating, Cuisine, Amenity and O.\nSentence: show me the three closest wendys","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","the","three","closest","wendys"],"labels":["O","O","O","O","B-Location","B-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["hours","dish","location","restaurant_name","price","rating","cuisine","amenity"]}
{"id":"1065","dataset":"mit-restaurant","split":"test","instance":{"id":"1065","prompt_labels":"smoke(B-Amenity) friendly(I-Amenity) restaurants(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Restaurant Name, Rating, Cuisine, Amenity, Location, Price, Hours and O.\nSentence: smoke friendly restaurants","prediction_output":null,"prediction_outputs":null,"group":null,"words":["smoke","friendly","restaurants"],"labels":["B-Amenity","I-Amenity","O"],"target_index":null,"target_label":null},"label_list":["dish","restaurant_name","rating","cuisine","amenity","location","price","hours"]}
{"id":"1073","dataset":"mit-restaurant","split":"test","instance":{"id":"1073","prompt_labels":"take(O) me(O) to(O) a(O) local(B-Location) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Location, Dish, Rating, Price, Amenity, Hours, Cuisine and O.\nSentence: take me to a local restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["take","me","to","a","local","restaurant"],"labels":["O","O","O","O","B-Location","O"],"target_index":null,"target_label":null},"label_list":["restaurant_name","location","dish","rating","price","amenity","hours","cuisine"]}
{"id":"1091","dataset":"mit-restaurant","split":"test","instance":{"id":"1091","prompt_labels":"what(O) are(O) some(O) of(O) the(O) best(B-Rating) restaurants(O) in(B-Location) this(I-Location) city(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Location, Rating, Cuisine, Hours, Price, Dish and O.\nSentence: what are some of the best restaurants in this city","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","some","of","the","best","restaurants","in","this","city"],"labels":["O","O","O","O","O","B-Rating","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["restaurant_name","amenity","location","rating","cuisine","hours","price","dish"]}
{"id":"1094","dataset":"mit-restaurant","split":"test","instance":{"id":"1094","prompt_labels":"what(O) are(O) the(O) average(O) prices(O) for(O) lunch(B-Hours) at(O) mikanos(B-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Restaurant Name, Location, Price, Rating, Hours, Dish, Amenity and O.\nSentence: what are the average prices for lunch at mikanos","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","the","average","prices","for","lunch","at","mikanos"],"labels":["O","O","O","O","O","O","B-Hours","O","B-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["cuisine","restaurant_name","location","price","rating","hours","dish","amenity"]}
{"id":"1095","dataset":"mit-restaurant","split":"test","instance":{"id":"1095","prompt_labels":"what(O) are(O) the(O) directions(O) to(O) siu(B-Restaurant Name) kuen(I-Restaurant Name) chinese(I-Restaurant Name) restaurant(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Price, Location, Rating, Dish, Amenity, Restaurant Name, Cuisine and O.\nSentence: what are the directions to siu kuen chinese restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","the","directions","to","siu","kuen","chinese","restaurant"],"labels":["O","O","O","O","O","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["hours","price","location","rating","dish","amenity","restaurant_name","cuisine"]}
{"id":"1125","dataset":"mit-restaurant","split":"test","instance":{"id":"1125","prompt_labels":"what(O) is(O) the(O) closest(B-Location) chic(B-Restaurant Name) fil(I-Restaurant Name) a(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Cuisine, Location, Rating, Hours, Dish, Amenity, Restaurant Name and O.\nSentence: what is the closest chic fil a","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","closest","chic","fil","a"],"labels":["O","O","O","B-Location","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["price","cuisine","location","rating","hours","dish","amenity","restaurant_name"]}
{"id":"1126","dataset":"mit-restaurant","split":"test","instance":{"id":"1126","prompt_labels":"what(O) is(O) the(O) closest(B-Location) fast(B-Cuisine) food(I-Cuisine) to(O) me(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Amenity, Price, Dish, Hours, Location, Restaurant Name and O.\nSentence: what is the closest fast food to me","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","closest","fast","food","to","me"],"labels":["O","O","O","B-Location","B-Cuisine","I-Cuisine","O","O"],"target_index":null,"target_label":null},"label_list":["cuisine","rating","amenity","price","dish","hours","location","restaurant_name"]}
{"id":"1129","dataset":"mit-restaurant","split":"test","instance":{"id":"1129","prompt_labels":"what(O) is(O) the(O) dress(O) code(O) at(O) the(B-Restaurant Name) precinct(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Price, Rating, Cuisine, Dish, Location, Amenity and O.\nSentence: what is the dress code at the precinct","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","dress","code","at","the","precinct"],"labels":["O","O","O","O","O","O","B-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["hours","restaurant_name","price","rating","cuisine","dish","location","amenity"]}
{"id":"1132","dataset":"mit-restaurant","split":"test","instance":{"id":"1132","prompt_labels":"what(O) is(O) the(O) entree(B-Price) price(O) at(O) kiosque(B-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Amenity, Restaurant Name, Rating, Dish, Price, Hours, Cuisine and O.\nSentence: what is the entree price at kiosque","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","entree","price","at","kiosque"],"labels":["O","O","O","B-Price","O","O","B-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["location","amenity","restaurant_name","rating","dish","price","hours","cuisine"]}
{"id":"1135","dataset":"mit-restaurant","split":"test","instance":{"id":"1135","prompt_labels":"what(O) is(O) the(O) highest(B-Rating) rated(I-Rating) thai(B-Cuisine) restaurant(O) in(O) austin(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Restaurant Name, Dish, Hours, Rating, Cuisine, Amenity, Price and O.\nSentence: what is the highest rated thai restaurant in austin","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","highest","rated","thai","restaurant","in","austin"],"labels":["O","O","O","B-Rating","I-Rating","B-Cuisine","O","O","B-Location"],"target_index":null,"target_label":null},"label_list":["location","restaurant_name","dish","hours","rating","cuisine","amenity","price"]}
{"id":"1151","dataset":"mit-restaurant","split":"test","instance":{"id":"1151","prompt_labels":"what(O) is(O) the(O) rating(O) on(O) arbys(B-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Restaurant Name, Cuisine, Price, Dish, Hours, Amenity, Rating and O.\nSentence: what is the rating on arbys","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","rating","on","arbys"],"labels":["O","O","O","O","O","B-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["location","restaurant_name","cuisine","price","dish","hours","amenity","rating"]}
{"id":"1152","dataset":"mit-restaurant","split":"test","instance":{"id":"1152","prompt_labels":"what(O) is(O) the(O) rating(O) on(O) burger(B-Restaurant Name) island(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Location, Rating, Dish, Hours, Price, Restaurant Name, Cuisine and O.\nSentence: what is the rating on burger island","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","rating","on","burger","island"],"labels":["O","O","O","O","O","B-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["amenity","location","rating","dish","hours","price","restaurant_name","cuisine"]}
{"id":"1163","dataset":"mit-restaurant","split":"test","instance":{"id":"1163","prompt_labels":"what(O) nearby(B-Location) is(O) open(B-Hours) past(I-Hours) midnight(I-Hours) and(O) has(O) fabulous(B-Rating) service(I-Rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Rating, Amenity, Location, Dish, Restaurant Name, Cuisine, Price and O.\nSentence: what nearby is open past midnight and has fabulous service","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","nearby","is","open","past","midnight","and","has","fabulous","service"],"labels":["O","B-Location","O","B-Hours","I-Hours","I-Hours","O","O","B-Rating","I-Rating"],"target_index":null,"target_label":null},"label_list":["hours","rating","amenity","location","dish","restaurant_name","cuisine","price"]}
{"id":"1166","dataset":"mit-restaurant","split":"test","instance":{"id":"1166","prompt_labels":"what(O) nearby(B-Location) restaurants(O) serve(O) grilled(B-Dish) tilapia(I-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Amenity, Price, Restaurant Name, Rating, Location, Hours, Cuisine and O.\nSentence: what nearby restaurants serve grilled tilapia","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","nearby","restaurants","serve","grilled","tilapia"],"labels":["O","B-Location","O","O","B-Dish","I-Dish"],"target_index":null,"target_label":null},"label_list":["dish","amenity","price","restaurant_name","rating","location","hours","cuisine"]}
{"id":"1173","dataset":"mit-restaurant","split":"test","instance":{"id":"1173","prompt_labels":"what(O) places(O) in(O) roxbury(B-Location) are(O) open(B-Hours) before(I-Hours) 8(I-Hours) am(I-Hours) and(O) have(O) excellent(B-Rating) service(I-Rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Price, Dish, Hours, Cuisine, Location, Restaurant Name and O.\nSentence: what places in roxbury are open before 8 am and have excellent service","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","places","in","roxbury","are","open","before","8","am","and","have","excellent","service"],"labels":["O","O","O","B-Location","O","B-Hours","I-Hours","I-Hours","I-Hours","O","O","B-Rating","I-Rating"],"target_index":null,"target_label":null},"label_list":["rating","amenity","price","dish","hours","cuisine","location","restaurant_name"]}
{"id":"1175","dataset":"mit-restaurant","split":"test","instance":{"id":"1175","prompt_labels":"what(O) portugeese(B-Cuisine) restaurant(O) is(O) on(O) colonel(B-Location) bell(I-Location) drive(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Location, Price, Dish, Cuisine, Restaurant Name, Hours and O.\nSentence: what portugeese restaurant is on colonel bell drive","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","portugeese","restaurant","is","on","colonel","bell","drive"],"labels":["O","B-Cuisine","O","O","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["rating","amenity","location","price","dish","cuisine","restaurant_name","hours"]}
{"id":"1176","dataset":"mit-restaurant","split":"test","instance":{"id":"1176","prompt_labels":"what(O) restaurant(O) are(O) cheap(B-Price) and(O) only(O) vegan(B-Cuisine)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Cuisine, Restaurant Name, Rating, Amenity, Location, Hours, Price and O.\nSentence: what restaurant are cheap and only vegan","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","restaurant","are","cheap","and","only","vegan"],"labels":["O","O","O","B-Price","O","O","B-Cuisine"],"target_index":null,"target_label":null},"label_list":["dish","cuisine","restaurant_name","rating","amenity","location","hours","price"]}
{"id":"1180","dataset":"mit-restaurant","split":"test","instance":{"id":"1180","prompt_labels":"what(O) restaurant(O) is(O) open(B-Hours) till(I-Hours) 9(I-Hours) pm(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Hours, Amenity, Cuisine, Restaurant Name, Price, Rating, Dish and O.\nSentence: what restaurant is open till 9 pm","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","restaurant","is","open","till","9","pm"],"labels":["O","O","O","B-Hours","I-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["location","hours","amenity","cuisine","restaurant_name","price","rating","dish"]}
{"id":"1181","dataset":"mit-restaurant","split":"test","instance":{"id":"1181","prompt_labels":"what(O) restaurant(O) nearby(B-Location) are(O) affordable(B-Price)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Restaurant Name, Hours, Cuisine, Rating, Price, Amenity and O.\nSentence: what restaurant nearby are affordable","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","restaurant","nearby","are","affordable"],"labels":["O","O","B-Location","O","B-Price"],"target_index":null,"target_label":null},"label_list":["dish","location","restaurant_name","hours","cuisine","rating","price","amenity"]}
{"id":"1184","dataset":"mit-restaurant","split":"test","instance":{"id":"1184","prompt_labels":"what(O) restaurant(O) serves(O) burritos(B-Dish) 7(B-Hours) days(I-Hours) per(I-Hours) week(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Hours, Location, Cuisine, Price, Restaurant Name, Amenity, Dish and O.\nSentence: what restaurant serves burritos 7 days per week","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","restaurant","serves","burritos","7","days","per","week"],"labels":["O","O","O","B-Dish","B-Hours","I-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["rating","hours","location","cuisine","price","restaurant_name","amenity","dish"]}
{"id":"1190","dataset":"mit-restaurant","split":"test","instance":{"id":"1190","prompt_labels":"what(O) restaurants(O) are(O) open(B-Hours) past(I-Hours) midnight(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Cuisine, Dish, Restaurant Name, Location, Hours, Price and O.\nSentence: what restaurants are open past midnight","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","restaurants","are","open","past","midnight"],"labels":["O","O","O","B-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["rating","amenity","cuisine","dish","restaurant_name","location","hours","price"]}
{"id":"1203","dataset":"mit-restaurant","split":"test","instance":{"id":"1203","prompt_labels":"what(O) time(O) does(O) kareems(B-Restaurant Name) restaurant(O) open(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Hours, Restaurant Name, Location, Price, Dish, Cuisine, Amenity and O.\nSentence: what time does kareems restaurant open","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","time","does","kareems","restaurant","open"],"labels":["O","O","O","B-Restaurant Name","O","O"],"target_index":null,"target_label":null},"label_list":["rating","hours","restaurant_name","location","price","dish","cuisine","amenity"]}
{"id":"1205","dataset":"mit-restaurant","split":"test","instance":{"id":"1205","prompt_labels":"what(O) time(O) does(O) little(B-Restaurant Name) johns(I-Restaurant Name) open(O) on(B-Hours) sunday(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Location, Price, Dish, Amenity, Rating, Hours, Restaurant Name and O.\nSentence: what time does little johns open on sunday","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","time","does","little","johns","open","on","sunday"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name","O","B-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["cuisine","location","price","dish","amenity","rating","hours","restaurant_name"]}
{"id":"1206","dataset":"mit-restaurant","split":"test","instance":{"id":"1206","prompt_labels":"what(O) time(O) does(O) mcdonalds(B-Restaurant Name) in(O) main(B-Location) street(I-Location) open(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Rating, Amenity, Dish, Price, Location, Restaurant Name and O.\nSentence: what time does mcdonalds in main street open","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","time","does","mcdonalds","in","main","street","open"],"labels":["O","O","O","B-Restaurant Name","O","B-Location","I-Location","O"],"target_index":null,"target_label":null},"label_list":["hours","cuisine","rating","amenity","dish","price","location","restaurant_name"]}
{"id":"1208","dataset":"mit-restaurant","split":"test","instance":{"id":"1208","prompt_labels":"what(O) time(O) does(O) papa(B-Restaurant Name) johns(I-Restaurant Name) close(B-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Price, Hours, Cuisine, Location, Dish, Restaurant Name, Rating and O.\nSentence: what time does papa johns close","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","time","does","papa","johns","close"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name","B-Hours"],"target_index":null,"target_label":null},"label_list":["amenity","price","hours","cuisine","location","dish","restaurant_name","rating"]}
{"id":"1209","dataset":"mit-restaurant","split":"test","instance":{"id":"1209","prompt_labels":"what(O) time(O) does(O) red(B-Restaurant Name) robin(I-Restaurant Name) close(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Cuisine, Rating, Location, Restaurant Name, Dish, Price and O.\nSentence: what time does red robin close","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","time","does","red","robin","close"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name","O"],"target_index":null,"target_label":null},"label_list":["amenity","hours","cuisine","rating","location","restaurant_name","dish","price"]}
{"id":"1211","dataset":"mit-restaurant","split":"test","instance":{"id":"1211","prompt_labels":"what(O) time(O) does(O) sonic(B-Restaurant Name) open(B-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Hours, Restaurant Name, Rating, Cuisine, Dish, Price, Amenity and O.\nSentence: what time does sonic open","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","time","does","sonic","open"],"labels":["O","O","O","B-Restaurant Name","B-Hours"],"target_index":null,"target_label":null},"label_list":["location","hours","restaurant_name","rating","cuisine","dish","price","amenity"]}
{"id":"1212","dataset":"mit-restaurant","split":"test","instance":{"id":"1212","prompt_labels":"what(O) time(O) does(B-Hours) subway(B-Restaurant Name) open(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Hours, Restaurant Name, Location, Rating, Cuisine, Dish, Amenity and O.\nSentence: what time does subway open","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","time","does","subway","open"],"labels":["O","O","B-Hours","B-Restaurant Name","O"],"target_index":null,"target_label":null},"label_list":["price","hours","restaurant_name","location","rating","cuisine","dish","amenity"]}
{"id":"1229","dataset":"mit-restaurant","split":"test","instance":{"id":"1229","prompt_labels":"whats(O) the(O) best(B-Rating) barbeque(B-Cuisine) restaurant(O) in(O) memphis(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Dish, Restaurant Name, Location, Amenity, Rating, Hours and O.\nSentence: whats the best barbeque restaurant in memphis","prediction_output":null,"prediction_outputs":null,"group":null,"words":["whats","the","best","barbeque","restaurant","in","memphis"],"labels":["O","O","B-Rating","B-Cuisine","O","O","B-Location"],"target_index":null,"target_label":null},"label_list":["cuisine","price","dish","restaurant_name","location","amenity","rating","hours"]}
{"id":"1234","dataset":"mit-restaurant","split":"test","instance":{"id":"1234","prompt_labels":"whats(O) the(O) cheapest(B-Price) fast(B-Cuisine) food(I-Cuisine) restaurant(O) that(O) has(O) a(O) play(B-Amenity) area(I-Amenity) for(O) children(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Amenity, Restaurant Name, Rating, Hours, Dish, Price, Cuisine and O.\nSentence: whats the cheapest fast food restaurant that has a play area for children","prediction_output":null,"prediction_outputs":null,"group":null,"words":["whats","the","cheapest","fast","food","restaurant","that","has","a","play","area","for","children"],"labels":["O","O","B-Price","B-Cuisine","I-Cuisine","O","O","O","O","B-Amenity","I-Amenity","O","O"],"target_index":null,"target_label":null},"label_list":["location","amenity","restaurant_name","rating","hours","dish","price","cuisine"]}
{"id":"1235","dataset":"mit-restaurant","split":"test","instance":{"id":"1235","prompt_labels":"whats(O) the(O) closest(B-Location) and(O) cheapest(B-Price) bistro(B-Cuisine) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Location, Restaurant Name, Price, Hours, Amenity, Rating, Dish and O.\nSentence: whats the closest and cheapest bistro nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["whats","the","closest","and","cheapest","bistro","nearby"],"labels":["O","O","B-Location","O","B-Price","B-Cuisine","B-Location"],"target_index":null,"target_label":null},"label_list":["cuisine","location","restaurant_name","price","hours","amenity","rating","dish"]}
{"id":"1245","dataset":"mit-restaurant","split":"test","instance":{"id":"1245","prompt_labels":"whats(O) the(O) shortest(O) route(O) to(O) mcdonalds(B-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Amenity, Cuisine, Price, Hours, Dish, Location and O.\nSentence: whats the shortest route to mcdonalds","prediction_output":null,"prediction_outputs":null,"group":null,"words":["whats","the","shortest","route","to","mcdonalds"],"labels":["O","O","O","O","O","B-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["rating","restaurant_name","amenity","cuisine","price","hours","dish","location"]}
{"id":"1251","dataset":"mit-restaurant","split":"test","instance":{"id":"1251","prompt_labels":"when(O) does(O) white(B-Restaurant Name) castle(I-Restaurant Name) close(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Hours, Price, Location, Dish, Cuisine, Restaurant Name, Amenity and O.\nSentence: when does white castle close","prediction_output":null,"prediction_outputs":null,"group":null,"words":["when","does","white","castle","close"],"labels":["O","O","B-Restaurant Name","I-Restaurant Name","B-Location"],"target_index":null,"target_label":null},"label_list":["rating","hours","price","location","dish","cuisine","restaurant_name","amenity"]}
{"id":"1252","dataset":"mit-restaurant","split":"test","instance":{"id":"1252","prompt_labels":"where(O) are(O) some(O) fast(B-Cuisine) food(I-Cuisine) joints(I-Cuisine) in(O) driving(B-Location) distance(I-Location) to(O) here(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Amenity, Dish, Price, Cuisine, Restaurant Name, Rating, Location and O.\nSentence: where are some fast food joints in driving distance to here","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","are","some","fast","food","joints","in","driving","distance","to","here"],"labels":["O","O","O","B-Cuisine","I-Cuisine","I-Cuisine","O","B-Location","I-Location","O","O"],"target_index":null,"target_label":null},"label_list":["hours","amenity","dish","price","cuisine","restaurant_name","rating","location"]}
{"id":"1253","dataset":"mit-restaurant","split":"test","instance":{"id":"1253","prompt_labels":"where(O) are(O) some(O) mexican(B-Cuisine) restaurants(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Price, Hours, Restaurant Name, Dish, Rating, Amenity, Cuisine and O.\nSentence: where are some mexican restaurants","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","are","some","mexican","restaurants"],"labels":["O","O","O","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["location","price","hours","restaurant_name","dish","rating","amenity","cuisine"]}
{"id":"1255","dataset":"mit-restaurant","split":"test","instance":{"id":"1255","prompt_labels":"where(O) can(O) a(O) group(O) be(O) served(O) omelets(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Price, Cuisine, Rating, Amenity, Location, Dish, Hours and O.\nSentence: where can a group be served omelets","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","a","group","be","served","omelets"],"labels":["O","O","O","O","O","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["restaurant_name","price","cuisine","rating","amenity","location","dish","hours"]}
{"id":"1260","dataset":"mit-restaurant","split":"test","instance":{"id":"1260","prompt_labels":"where(O) can(O) i(O) eat(O) around(B-Location) here(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Cuisine, Hours, Restaurant Name, Location, Dish, Rating, Amenity and O.\nSentence: where can i eat around here","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","eat","around","here"],"labels":["O","O","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["price","cuisine","hours","restaurant_name","location","dish","rating","amenity"]}
{"id":"1271","dataset":"mit-restaurant","split":"test","instance":{"id":"1271","prompt_labels":"where(O) can(O) i(O) find(O) a(O) cracker(B-Restaurant Name) barrel(I-Restaurant Name) near(O) miami(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Restaurant Name, Hours, Cuisine, Location, Price, Rating, Dish and O.\nSentence: where can i find a cracker barrel near miami","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","find","a","cracker","barrel","near","miami"],"labels":["O","O","O","O","O","B-Restaurant Name","I-Restaurant Name","O","B-Location"],"target_index":null,"target_label":null},"label_list":["amenity","restaurant_name","hours","cuisine","location","price","rating","dish"]}
{"id":"1274","dataset":"mit-restaurant","split":"test","instance":{"id":"1274","prompt_labels":"where(O) can(O) i(O) find(O) a(O) good(B-Rating) pork(B-Dish) chop(I-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Location, Dish, Amenity, Restaurant Name, Hours, Rating and O.\nSentence: where can i find a good pork chop","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","find","a","good","pork","chop"],"labels":["O","O","O","O","O","B-Rating","B-Dish","I-Dish"],"target_index":null,"target_label":null},"label_list":["cuisine","price","location","dish","amenity","restaurant_name","hours","rating"]}
{"id":"1280","dataset":"mit-restaurant","split":"test","instance":{"id":"1280","prompt_labels":"where(O) can(O) i(O) find(O) a(O) place(O) that(O) serves(O) gelato(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Cuisine, Rating, Restaurant Name, Price, Hours, Amenity, Dish and O.\nSentence: where can i find a place that serves gelato","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","find","a","place","that","serves","gelato"],"labels":["O","O","O","O","O","O","O","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["location","cuisine","rating","restaurant_name","price","hours","amenity","dish"]}
{"id":"1285","dataset":"mit-restaurant","split":"test","instance":{"id":"1285","prompt_labels":"where(O) can(O) i(O) find(O) a(O) restaurant(O) near(B-Location) me(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Location, Rating, Price, Dish, Hours, Restaurant Name, Amenity and O.\nSentence: where can i find a restaurant near me","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","find","a","restaurant","near","me"],"labels":["O","O","O","O","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["cuisine","location","rating","price","dish","hours","restaurant_name","amenity"]}
{"id":"1290","dataset":"mit-restaurant","split":"test","instance":{"id":"1290","prompt_labels":"where(O) can(O) i(O) find(O) a(O) thai(B-Cuisine) restaurant(O) that(O) is(O) non(B-Amenity) smoking(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Location, Dish, Restaurant Name, Price, Rating, Amenity, Hours and O.\nSentence: where can i find a thai restaurant that is non smoking","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","find","a","thai","restaurant","that","is","non","smoking"],"labels":["O","O","O","O","O","B-Cuisine","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["cuisine","location","dish","restaurant_name","price","rating","amenity","hours"]}
{"id":"1293","dataset":"mit-restaurant","split":"test","instance":{"id":"1293","prompt_labels":"where(O) can(O) i(O) find(O) cheap(B-Price) vegan(B-Cuisine) food(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Location, Hours, Dish, Restaurant Name, Price, Amenity and O.\nSentence: where can i find cheap vegan food","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","find","cheap","vegan","food"],"labels":["O","O","O","O","B-Price","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["cuisine","rating","location","hours","dish","restaurant_name","price","amenity"]}
{"id":"1300","dataset":"mit-restaurant","split":"test","instance":{"id":"1300","prompt_labels":"where(O) can(O) i(O) find(O) the(O) cheapest(B-Price) pizza(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Location, Dish, Cuisine, Restaurant Name, Amenity, Rating, Price and O.\nSentence: where can i find the cheapest pizza","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","find","the","cheapest","pizza"],"labels":["O","O","O","O","O","B-Price","B-Dish"],"target_index":null,"target_label":null},"label_list":["hours","location","dish","cuisine","restaurant_name","amenity","rating","price"]}
{"id":"1308","dataset":"mit-restaurant","split":"test","instance":{"id":"1308","prompt_labels":"where(O) can(O) i(O) get(O) a(O) cheap(B-Price) slice(B-Dish) of(I-Dish) pizza(I-Dish) in(O) huntington(B-Location) ny(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Dish, Location, Amenity, Cuisine, Rating, Hours, Price and O.\nSentence: where can i get a cheap slice of pizza in huntington ny","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","get","a","cheap","slice","of","pizza","in","huntington","ny"],"labels":["O","O","O","O","O","B-Price","B-Dish","I-Dish","I-Dish","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["restaurant_name","dish","location","amenity","cuisine","rating","hours","price"]}
{"id":"1316","dataset":"mit-restaurant","split":"test","instance":{"id":"1316","prompt_labels":"where(O) can(O) i(O) get(O) a(O) milkshake(B-Dish) within(B-Location) walking(I-Location) distance(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Cuisine, Amenity, Dish, Hours, Restaurant Name, Price, Location and O.\nSentence: where can i get a milkshake within walking distance","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","get","a","milkshake","within","walking","distance"],"labels":["O","O","O","O","O","B-Dish","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["rating","cuisine","amenity","dish","hours","restaurant_name","price","location"]}
{"id":"1317","dataset":"mit-restaurant","split":"test","instance":{"id":"1317","prompt_labels":"where(O) can(O) i(O) get(O) a(O) pho(B-Dish) king(I-Dish) sized(I-Dish) bowl(I-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Dish, Location, Cuisine, Price, Rating, Amenity and O.\nSentence: where can i get a pho king sized bowl","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","get","a","pho","king","sized","bowl"],"labels":["O","O","O","O","O","B-Dish","I-Dish","I-Dish","I-Dish"],"target_index":null,"target_label":null},"label_list":["hours","restaurant_name","dish","location","cuisine","price","rating","amenity"]}
{"id":"1321","dataset":"mit-restaurant","split":"test","instance":{"id":"1321","prompt_labels":"where(O) can(O) i(O) get(O) bagels(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Hours, Restaurant Name, Cuisine, Location, Dish, Rating and O.\nSentence: where can i get bagels","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","get","bagels"],"labels":["O","O","O","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["price","amenity","hours","restaurant_name","cuisine","location","dish","rating"]}
{"id":"1327","dataset":"mit-restaurant","split":"test","instance":{"id":"1327","prompt_labels":"where(O) can(O) i(O) get(O) eggrolls(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Hours, Dish, Amenity, Restaurant Name, Location, Rating, Price and O.\nSentence: where can i get eggrolls","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","get","eggrolls"],"labels":["O","O","O","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["cuisine","hours","dish","amenity","restaurant_name","location","rating","price"]}
{"id":"1330","dataset":"mit-restaurant","split":"test","instance":{"id":"1330","prompt_labels":"where(O) can(O) i(O) get(O) nachos(B-Dish) within(B-Location) 1(I-Location) mile(I-Location) of(O) me(O) that(O) is(O) open(B-Hours) late(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Rating, Dish, Price, Hours, Location, Cuisine, Amenity and O.\nSentence: where can i get nachos within 1 mile of me that is open late","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","get","nachos","within","1","mile","of","me","that","is","open","late"],"labels":["O","O","O","O","B-Dish","B-Location","I-Location","I-Location","O","O","O","O","B-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["restaurant_name","rating","dish","price","hours","location","cuisine","amenity"]}
{"id":"1332","dataset":"mit-restaurant","split":"test","instance":{"id":"1332","prompt_labels":"where(O) can(O) i(O) get(O) some(O) chicken(B-Dish) nuggets(I-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Cuisine, Price, Rating, Amenity, Location, Hours, Dish and O.\nSentence: where can i get some chicken nuggets","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","get","some","chicken","nuggets"],"labels":["O","O","O","O","O","B-Dish","I-Dish"],"target_index":null,"target_label":null},"label_list":["restaurant_name","cuisine","price","rating","amenity","location","hours","dish"]}
{"id":"1334","dataset":"mit-restaurant","split":"test","instance":{"id":"1334","prompt_labels":"where(O) can(O) i(O) get(O) some(O) crab(B-Dish) nearby(B-Location) that(O) is(O) open(B-Hours) until(I-Hours) 2(I-Hours) am(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Price, Dish, Cuisine, Restaurant Name, Hours, Location, Amenity and O.\nSentence: where can i get some crab nearby that is open until 2 am","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","get","some","crab","nearby","that","is","open","until","2","am"],"labels":["O","O","O","O","O","B-Dish","B-Location","O","O","B-Hours","I-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["rating","price","dish","cuisine","restaurant_name","hours","location","amenity"]}
{"id":"1335","dataset":"mit-restaurant","split":"test","instance":{"id":"1335","prompt_labels":"where(O) can(O) i(O) get(O) some(O) fast(B-Cuisine) food(I-Cuisine)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Amenity, Rating, Location, Cuisine, Price, Hours, Restaurant Name and O.\nSentence: where can i get some fast food","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","get","some","fast","food"],"labels":["O","O","O","O","O","B-Cuisine","I-Cuisine"],"target_index":null,"target_label":null},"label_list":["dish","amenity","rating","location","cuisine","price","hours","restaurant_name"]}
{"id":"1337","dataset":"mit-restaurant","split":"test","instance":{"id":"1337","prompt_labels":"where(O) can(O) i(O) get(O) some(O) ice(B-Dish) cream(I-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Amenity, Price, Hours, Restaurant Name, Rating, Cuisine, Location and O.\nSentence: where can i get some ice cream","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","get","some","ice","cream"],"labels":["O","O","O","O","O","B-Dish","I-Dish"],"target_index":null,"target_label":null},"label_list":["dish","amenity","price","hours","restaurant_name","rating","cuisine","location"]}
{"id":"1339","dataset":"mit-restaurant","split":"test","instance":{"id":"1339","prompt_labels":"where(O) can(O) i(O) get(O) some(O) shrimp(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Dish, Location, Restaurant Name, Amenity, Price, Rating, Cuisine and O.\nSentence: where can i get some shrimp","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","get","some","shrimp"],"labels":["O","O","O","O","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["hours","dish","location","restaurant_name","amenity","price","rating","cuisine"]}
{"id":"1350","dataset":"mit-restaurant","split":"test","instance":{"id":"1350","prompt_labels":"where(O) can(O) i(O) go(O) for(O) expensive(B-Price) gelato(B-Dish) for(O) carry(B-Amenity) out(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Restaurant Name, Hours, Rating, Cuisine, Dish, Location, Price and O.\nSentence: where can i go for expensive gelato for carry out","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","go","for","expensive","gelato","for","carry","out"],"labels":["O","O","O","O","O","B-Price","B-Dish","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["amenity","restaurant_name","hours","rating","cuisine","dish","location","price"]}
{"id":"1354","dataset":"mit-restaurant","split":"test","instance":{"id":"1354","prompt_labels":"where(O) can(O) i(O) order(O) fried(B-Dish) chicken(I-Dish) to(O) eat(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Hours, Price, Restaurant Name, Cuisine, Location, Amenity, Rating and O.\nSentence: where can i order fried chicken to eat","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","order","fried","chicken","to","eat"],"labels":["O","O","O","O","B-Dish","I-Dish","O","O"],"target_index":null,"target_label":null},"label_list":["dish","hours","price","restaurant_name","cuisine","location","amenity","rating"]}
{"id":"1358","dataset":"mit-restaurant","split":"test","instance":{"id":"1358","prompt_labels":"where(O) can(O) i(O) take(O) my(O) date(B-Amenity) for(O) a(O) cheap(B-Price) meal(O) in(O) charlestown(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Amenity, Price, Hours, Cuisine, Dish, Location and O.\nSentence: where can i take my date for a cheap meal in charlestown","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","take","my","date","for","a","cheap","meal","in","charlestown"],"labels":["O","O","O","O","O","B-Amenity","O","O","B-Price","O","O","B-Location"],"target_index":null,"target_label":null},"label_list":["rating","restaurant_name","amenity","price","hours","cuisine","dish","location"]}
{"id":"1359","dataset":"mit-restaurant","split":"test","instance":{"id":"1359","prompt_labels":"where(O) can(O) i(O) take(O) my(O) kids(B-Amenity) for(O) good(B-Rating) tacos(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Dish, Cuisine, Location, Hours, Restaurant Name, Price, Amenity and O.\nSentence: where can i take my kids for good tacos","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","take","my","kids","for","good","tacos"],"labels":["O","O","O","O","O","B-Amenity","O","B-Rating","B-Dish"],"target_index":null,"target_label":null},"label_list":["rating","dish","cuisine","location","hours","restaurant_name","price","amenity"]}
{"id":"1364","dataset":"mit-restaurant","split":"test","instance":{"id":"1364","prompt_labels":"where(O) could(O) i(O) find(O) some(O) good(B-Rating) tomato(B-Dish) sauce(I-Dish) thats(O) 10(B-Location) minutes(I-Location) away(I-Location) with(O) excellent(B-Price) pricing(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Hours, Cuisine, Restaurant Name, Price, Rating, Location and O.\nSentence: where could i find some good tomato sauce thats 10 minutes away with excellent pricing","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","could","i","find","some","good","tomato","sauce","thats","10","minutes","away","with","excellent","pricing"],"labels":["O","O","O","O","O","B-Rating","B-Dish","I-Dish","O","B-Location","I-Location","I-Location","O","B-Price","O"],"target_index":null,"target_label":null},"label_list":["amenity","dish","hours","cuisine","restaurant_name","price","rating","location"]}
{"id":"1366","dataset":"mit-restaurant","split":"test","instance":{"id":"1366","prompt_labels":"where(O) could(O) i(O) go(O) for(O) a(O) burger(B-Dish) and(O) a(O) movie(B-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Cuisine, Amenity, Restaurant Name, Price, Hours, Location, Rating and O.\nSentence: where could i go for a burger and a movie","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","could","i","go","for","a","burger","and","a","movie"],"labels":["O","O","O","O","O","O","B-Dish","O","O","B-Amenity"],"target_index":null,"target_label":null},"label_list":["dish","cuisine","amenity","restaurant_name","price","hours","location","rating"]}
{"id":"1371","dataset":"mit-restaurant","split":"test","instance":{"id":"1371","prompt_labels":"where(O) in(O) the(O) theater(B-Location) district(I-Location) is(O) there(O) a(O) restaurant(O) with(O) portions(O) that(O) are(O) a(O) bit(O) small(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Rating, Cuisine, Dish, Price, Location, Hours, Amenity and O.\nSentence: where in the theater district is there a restaurant with portions that are a bit small","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","in","the","theater","district","is","there","a","restaurant","with","portions","that","are","a","bit","small"],"labels":["O","O","O","B-Location","I-Location","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["restaurant_name","rating","cuisine","dish","price","location","hours","amenity"]}
{"id":"1376","dataset":"mit-restaurant","split":"test","instance":{"id":"1376","prompt_labels":"where(O) is(O) a(O) good(B-Rating) restaurant(O) on(O) the(O) water(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Location, Amenity, Hours, Price, Rating, Dish, Cuisine and O.\nSentence: where is a good restaurant on the water","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","is","a","good","restaurant","on","the","water"],"labels":["O","O","O","B-Rating","O","O","O","B-Location"],"target_index":null,"target_label":null},"label_list":["restaurant_name","location","amenity","hours","price","rating","dish","cuisine"]}
{"id":"1382","dataset":"mit-restaurant","split":"test","instance":{"id":"1382","prompt_labels":"where(O) is(O) a(O) restaurant(O) that(O) opens(B-Hours) 24(I-Hours) hours(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Cuisine, Dish, Location, Amenity, Price, Hours, Rating and O.\nSentence: where is a restaurant that opens 24 hours","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","is","a","restaurant","that","opens","24","hours"],"labels":["O","O","O","O","O","B-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["restaurant_name","cuisine","dish","location","amenity","price","hours","rating"]}
{"id":"1390","dataset":"mit-restaurant","split":"test","instance":{"id":"1390","prompt_labels":"where(O) is(O) ii(B-Restaurant Name) moro(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Location, Hours, Rating, Amenity, Restaurant Name, Dish, Cuisine and O.\nSentence: where is ii moro","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","is","ii","moro"],"labels":["O","O","B-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["price","location","hours","rating","amenity","restaurant_name","dish","cuisine"]}
{"id":"1406","dataset":"mit-restaurant","split":"test","instance":{"id":"1406","prompt_labels":"where(O) is(O) the(O) best(B-Rating) reviewed(I-Rating) place(O) to(O) get(O) a(O) burger(B-Dish) and(I-Dish) fries(I-Dish) near(B-Location) beacon(I-Location) hill(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Location, Dish, Amenity, Rating, Restaurant Name, Hours, Cuisine and O.\nSentence: where is the best reviewed place to get a burger and fries near beacon hill","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","is","the","best","reviewed","place","to","get","a","burger","and","fries","near","beacon","hill"],"labels":["O","O","O","B-Rating","I-Rating","O","O","O","O","B-Dish","I-Dish","I-Dish","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["price","location","dish","amenity","rating","restaurant_name","hours","cuisine"]}
{"id":"1407","dataset":"mit-restaurant","split":"test","instance":{"id":"1407","prompt_labels":"where(O) is(O) the(O) cheapest(B-Price) place(O) to(O) eat(O) nearest(B-Location) to(O) me(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Location, Price, Cuisine, Rating, Restaurant Name, Dish and O.\nSentence: where is the cheapest place to eat nearest to me","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","is","the","cheapest","place","to","eat","nearest","to","me"],"labels":["O","O","O","B-Price","O","O","O","B-Location","O","O"],"target_index":null,"target_label":null},"label_list":["amenity","hours","location","price","cuisine","rating","restaurant_name","dish"]}
{"id":"1409","dataset":"mit-restaurant","split":"test","instance":{"id":"1409","prompt_labels":"where(O) is(O) the(O) closest(B-Location) apple(B-Restaurant Name) bees(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Dish, Location, Amenity, Rating, Hours, Restaurant Name and O.\nSentence: where is the closest apple bees","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","is","the","closest","apple","bees"],"labels":["O","O","O","B-Location","B-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["cuisine","price","dish","location","amenity","rating","hours","restaurant_name"]}
{"id":"1415","dataset":"mit-restaurant","split":"test","instance":{"id":"1415","prompt_labels":"where(O) is(O) the(O) closest(B-Location) non(B-Amenity) smoking(I-Amenity) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Restaurant Name, Amenity, Location, Hours, Price, Cuisine, Rating and O.\nSentence: where is the closest non smoking restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","is","the","closest","non","smoking","restaurant"],"labels":["O","O","O","B-Location","B-Amenity","I-Amenity","O"],"target_index":null,"target_label":null},"label_list":["dish","restaurant_name","amenity","location","hours","price","cuisine","rating"]}
{"id":"1416","dataset":"mit-restaurant","split":"test","instance":{"id":"1416","prompt_labels":"where(O) is(O) the(O) closest(B-Location) pizza(B-Restaurant Name) hut(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Restaurant Name, Hours, Location, Rating, Amenity, Cuisine, Dish and O.\nSentence: where is the closest pizza hut","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","is","the","closest","pizza","hut"],"labels":["O","O","O","B-Location","B-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["price","restaurant_name","hours","location","rating","amenity","cuisine","dish"]}
{"id":"1434","dataset":"mit-restaurant","split":"test","instance":{"id":"1434","prompt_labels":"where(O) is(O) the(O) nearest(B-Location) ice(B-Cuisine) cream(I-Cuisine) shop(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Price, Restaurant Name, Amenity, Cuisine, Location, Rating, Dish and O.\nSentence: where is the nearest ice cream shop","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","is","the","nearest","ice","cream","shop"],"labels":["O","O","O","B-Location","B-Cuisine","I-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["hours","price","restaurant_name","amenity","cuisine","location","rating","dish"]}
{"id":"1436","dataset":"mit-restaurant","split":"test","instance":{"id":"1436","prompt_labels":"where(O) is(O) the(O) nearest(B-Location) mexican(B-Cuisine) restaurant(O) open(B-Hours) late(I-Hours) for(O) dinner(B-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Price, Amenity, Dish, Cuisine, Location, Rating, Hours and O.\nSentence: where is the nearest mexican restaurant open late for dinner","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","is","the","nearest","mexican","restaurant","open","late","for","dinner"],"labels":["O","O","O","B-Location","B-Cuisine","O","B-Hours","I-Hours","O","B-Hours"],"target_index":null,"target_label":null},"label_list":["restaurant_name","price","amenity","dish","cuisine","location","rating","hours"]}
{"id":"1444","dataset":"mit-restaurant","split":"test","instance":{"id":"1444","prompt_labels":"where(O) is(O) the(O) nearest(B-Location) restaurant(O) that(O) serves(O) steak(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Hours, Price, Location, Amenity, Restaurant Name, Dish, Cuisine and O.\nSentence: where is the nearest restaurant that serves steak","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","is","the","nearest","restaurant","that","serves","steak"],"labels":["O","O","O","B-Location","O","O","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["rating","hours","price","location","amenity","restaurant_name","dish","cuisine"]}
{"id":"1471","dataset":"mit-restaurant","split":"test","instance":{"id":"1471","prompt_labels":"wheres(O) the(O) closest(B-Location) pizza(B-Dish) place(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Cuisine, Hours, Location, Amenity, Price, Dish, Rating and O.\nSentence: wheres the closest pizza place","prediction_output":null,"prediction_outputs":null,"group":null,"words":["wheres","the","closest","pizza","place"],"labels":["O","O","B-Location","B-Dish","O"],"target_index":null,"target_label":null},"label_list":["restaurant_name","cuisine","hours","location","amenity","price","dish","rating"]}
{"id":"1475","dataset":"mit-restaurant","split":"test","instance":{"id":"1475","prompt_labels":"wheres(O) the(O) nearest(B-Location) french(B-Cuisine) place(O) to(O) eat(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Cuisine, Restaurant Name, Location, Dish, Amenity, Hours, Rating and O.\nSentence: wheres the nearest french place to eat","prediction_output":null,"prediction_outputs":null,"group":null,"words":["wheres","the","nearest","french","place","to","eat"],"labels":["O","O","B-Location","B-Cuisine","O","O","O"],"target_index":null,"target_label":null},"label_list":["price","cuisine","restaurant_name","location","dish","amenity","hours","rating"]}
{"id":"1483","dataset":"mit-restaurant","split":"test","instance":{"id":"1483","prompt_labels":"which(O) five(B-Rating) star(I-Rating) italian(B-Cuisine) restaurants(O) in(O) manattan(B-Location) have(O) the(O) best(B-Rating) reviews(I-Rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Cuisine, Dish, Location, Restaurant Name, Hours, Rating and O.\nSentence: which five star italian restaurants in manattan have the best reviews","prediction_output":null,"prediction_outputs":null,"group":null,"words":["which","five","star","italian","restaurants","in","manattan","have","the","best","reviews"],"labels":["O","B-Rating","I-Rating","B-Cuisine","O","O","B-Location","O","O","B-Rating","I-Rating"],"target_index":null,"target_label":null},"label_list":["price","amenity","cuisine","dish","location","restaurant_name","hours","rating"]}
{"id":"1492","dataset":"mit-restaurant","split":"test","instance":{"id":"1492","prompt_labels":"which(O) restaurant(O) has(O) fried(B-Dish) pickles(I-Dish) for(O) appetizers(B-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Dish, Amenity, Hours, Location, Cuisine, Restaurant Name, Price and O.\nSentence: which restaurant has fried pickles for appetizers","prediction_output":null,"prediction_outputs":null,"group":null,"words":["which","restaurant","has","fried","pickles","for","appetizers"],"labels":["O","O","O","B-Dish","I-Dish","O","B-Amenity"],"target_index":null,"target_label":null},"label_list":["rating","dish","amenity","hours","location","cuisine","restaurant_name","price"]}
{"id":"1493","dataset":"mit-restaurant","split":"test","instance":{"id":"1493","prompt_labels":"which(O) restaurant(O) in(O) my(B-Location) city(I-Location) has(O) the(O) highest(B-Rating) reviews(I-Rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Hours, Location, Rating, Price, Restaurant Name, Cuisine and O.\nSentence: which restaurant in my city has the highest reviews","prediction_output":null,"prediction_outputs":null,"group":null,"words":["which","restaurant","in","my","city","has","the","highest","reviews"],"labels":["O","O","O","B-Location","I-Location","O","O","B-Rating","I-Rating"],"target_index":null,"target_label":null},"label_list":["amenity","dish","hours","location","rating","price","restaurant_name","cuisine"]}
{"id":"1503","dataset":"mit-restaurant","split":"test","instance":{"id":"1503","prompt_labels":"whixh(O) restaurant(O) has(O) delivery(B-Amenity) in(O) washington(B-Location) court(I-Location) for(O) a(O) reasonable(B-Price) price(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Amenity, Restaurant Name, Rating, Price, Location, Cuisine, Hours and O.\nSentence: whixh restaurant has delivery in washington court for a reasonable price","prediction_output":null,"prediction_outputs":null,"group":null,"words":["whixh","restaurant","has","delivery","in","washington","court","for","a","reasonable","price"],"labels":["O","O","O","B-Amenity","O","B-Location","I-Location","O","O","B-Price","O"],"target_index":null,"target_label":null},"label_list":["dish","amenity","restaurant_name","rating","price","location","cuisine","hours"]}
{"id":"1505","dataset":"mit-restaurant","split":"test","instance":{"id":"1505","prompt_labels":"who(O) has(O) happy(B-Amenity) hour(I-Amenity) right(B-Hours) now(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Location, Restaurant Name, Dish, Rating, Hours, Amenity and O.\nSentence: who has happy hour right now","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","has","happy","hour","right","now"],"labels":["O","O","B-Amenity","I-Amenity","B-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["cuisine","price","location","restaurant_name","dish","rating","hours","amenity"]}
{"id":"1506","dataset":"mit-restaurant","split":"test","instance":{"id":"1506","prompt_labels":"who(O) has(O) the(O) best(B-Rating) lobster(B-Dish) in(B-Location) town(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Amenity, Price, Rating, Dish, Cuisine, Location and O.\nSentence: who has the best lobster in town","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","has","the","best","lobster","in","town"],"labels":["O","O","O","B-Rating","B-Dish","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["restaurant_name","hours","amenity","price","rating","dish","cuisine","location"]}
{"id":"1510","dataset":"mit-restaurant","split":"test","instance":{"id":"1510","prompt_labels":"who(O) makes(O) the(O) best(B-Rating) burgers(B-Dish) in(B-Location) town(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Amenity, Hours, Restaurant Name, Rating, Location, Price, Cuisine and O.\nSentence: who makes the best burgers in town","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","makes","the","best","burgers","in","town"],"labels":["O","O","O","B-Rating","B-Dish","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["dish","amenity","hours","restaurant_name","rating","location","price","cuisine"]}
{"id":"1515","dataset":"mit-restaurant","split":"test","instance":{"id":"1515","prompt_labels":"will(O) i(O) be(O) able(O) to(O) find(O) a(O) romantic(B-Amenity) restaurant(O) for(O) my(O) date(O) tonight(B-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Location, Cuisine, Price, Dish, Amenity, Hours and O.\nSentence: will i be able to find a romantic restaurant for my date tonight","prediction_output":null,"prediction_outputs":null,"group":null,"words":["will","i","be","able","to","find","a","romantic","restaurant","for","my","date","tonight"],"labels":["O","O","O","O","O","O","O","B-Amenity","O","O","O","O","B-Hours"],"target_index":null,"target_label":null},"label_list":["rating","restaurant_name","location","cuisine","price","dish","amenity","hours"]}
{"id":"1517","dataset":"mit-restaurant","split":"test","instance":{"id":"1517","prompt_labels":"yes(O) please(O) get(O) me(O) mcdonalds(B-Restaurant Name) phone(O) number(O) in(O) patchogue(B-Location) new(I-Location) york(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Restaurant Name, Price, Cuisine, Amenity, Dish, Rating, Hours and O.\nSentence: yes please get me mcdonalds phone number in patchogue new york","prediction_output":null,"prediction_outputs":null,"group":null,"words":["yes","please","get","me","mcdonalds","phone","number","in","patchogue","new","york"],"labels":["O","O","O","O","B-Restaurant Name","O","O","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["location","restaurant_name","price","cuisine","amenity","dish","rating","hours"]}
{"id":"0","dataset":"crossner_ai","split":"test","instance":{"id":"0","prompt_labels":"Typical(O) generative(O) model(O) approaches(O) include(O) naive(B-algorithm) Bayes(I-algorithm) classifier(I-algorithm) s(O) ,(O) Gaussian(B-algorithm) mixture(I-algorithm) model(I-algorithm) s(O) ,(O) variational(B-algorithm) autoencoders(I-algorithm) and(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, product, researcher, task, programming language, metric, field, algorithm, location, country, person, conference, university and O.\nSentence: Typical generative model approaches include naive Bayes classifier s , Gaussian mixture model s , variational autoencoders and others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Typical","generative","model","approaches","include","naive","Bayes","classifier","s",",","Gaussian","mixture","model","s",",","variational","autoencoders","and","others","."],"labels":["O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","B-algorithm","I-algorithm","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","product","researcher","task","programming_language","metric","field","algorithm","location","country","person","conference","university"]}
{"id":"2","dataset":"crossner_ai","split":"test","instance":{"id":"2","prompt_labels":"The(O) task(O) is(O) usually(O) to(O) derive(O) the(O) maximum(B-algorithm) likelihood(I-algorithm) estimate(I-algorithm) of(O) the(O) parameters(O) of(O) the(O) HMM(B-algorithm) given(O) the(O) of(O) output(O) sequences(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, university, researcher, country, metric, product, person, programming language, organization, conference, task, location, algorithm and O.\nSentence: The task is usually to derive the maximum likelihood estimate of the parameters of the HMM given the of output sequences .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","task","is","usually","to","derive","the","maximum","likelihood","estimate","of","the","parameters","of","the","HMM","given","the","of","output","sequences","."],"labels":["O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","B-algorithm","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","university","researcher","country","metric","product","person","programming_language","organization","conference","task","location","algorithm"]}
{"id":"3","dataset":"crossner_ai","split":"test","instance":{"id":"3","prompt_labels":"Unlike(O) neural(B-algorithm) network(I-algorithm) s(O) and(O) Support(B-algorithm) vector(I-algorithm) machine(I-algorithm) ,(O) the(O) AdaBoost(B-algorithm) training(O) process(O) selects(O) only(O) those(O) features(O) known(O) to(O) improve(O) the(O) predictive(O) power(O) of(O) the(O) model(O) ,(O) reducing(O) dimensionality(O) and(O) potentially(O) improving(O) execution(O) time(O) as(O) irrelevant(O) features(O) need(O) not(O) be(O) computed(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, person, organization, programming language, algorithm, product, conference, field, country, researcher, task, university, location and O.\nSentence: Unlike neural network s and Support vector machine , the AdaBoost training process selects only those features known to improve the predictive power of the model , reducing dimensionality and potentially improving execution time as irrelevant features need not be computed .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Unlike","neural","network","s","and","Support","vector","machine",",","the","AdaBoost","training","process","selects","only","those","features","known","to","improve","the","predictive","power","of","the","model",",","reducing","dimensionality","and","potentially","improving","execution","time","as","irrelevant","features","need","not","be","computed","."],"labels":["O","B-algorithm","I-algorithm","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","B-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","person","organization","programming_language","algorithm","product","conference","field","country","researcher","task","university","location"]}
{"id":"5","dataset":"crossner_ai","split":"test","instance":{"id":"5","prompt_labels":"A(O) frame(O) language(O) is(O) a(O) technology(O) used(O) for(O) knowledge(B-task) representation(I-task) in(O) artificial(B-field) intelligence(I-field) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, metric, researcher, country, location, field, organization, product, person, conference, algorithm, task, programming language and O.\nSentence: A frame language is a technology used for knowledge representation in artificial intelligence .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","frame","language","is","a","technology","used","for","knowledge","representation","in","artificial","intelligence","."],"labels":["O","O","O","O","O","O","O","O","B-task","I-task","O","B-field","I-field","O"],"target_index":null,"target_label":null},"label_list":["university","metric","researcher","country","location","field","organization","product","person","conference","algorithm","task","programming_language"]}
{"id":"6","dataset":"crossner_ai","split":"test","instance":{"id":"6","prompt_labels":"NIST(B-metric) also(O) differs(O) from(O) Bilingual(B-metric) evaluation(I-metric) understudy(I-metric) in(O) its(O) calculation(O) of(O) the(O) brevity(O) penalty(O) insofar(O) as(O) small(O) variations(O) in(O) translation(O) length(O) do(O) not(O) impact(O) the(O) overall(O) score(O) as(O) much(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, algorithm, product, organization, programming language, field, location, conference, country, metric, university, person, researcher and O.\nSentence: NIST also differs from Bilingual evaluation understudy in its calculation of the brevity penalty insofar as small variations in translation length do not impact the overall score as much .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["NIST","also","differs","from","Bilingual","evaluation","understudy","in","its","calculation","of","the","brevity","penalty","insofar","as","small","variations","in","translation","length","do","not","impact","the","overall","score","as","much","."],"labels":["B-metric","O","O","O","B-metric","I-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["task","algorithm","product","organization","programming_language","field","location","conference","country","metric","university","person","researcher"]}
{"id":"8","dataset":"crossner_ai","split":"test","instance":{"id":"8","prompt_labels":"FrameNet(B-product) has(O) been(O) used(O) in(O) applications(O) like(O) question(B-task) answering(I-task) ,(O) paraphrasing(B-task) ,(O) recognizing(B-task) textual(I-task) entailment(I-task) ,(O) and(O) information(B-task) extraction(I-task) ,(O) either(O) directly(O) or(O) by(O) means(O) of(O) Semantic(B-task) Role(I-task) Labeling(I-task) tools(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, university, field, country, algorithm, researcher, product, location, programming language, metric, task, person, organization and O.\nSentence: FrameNet has been used in applications like question answering , paraphrasing , recognizing textual entailment , and information extraction , either directly or by means of Semantic Role Labeling tools .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["FrameNet","has","been","used","in","applications","like","question","answering",",","paraphrasing",",","recognizing","textual","entailment",",","and","information","extraction",",","either","directly","or","by","means","of","Semantic","Role","Labeling","tools","."],"labels":["B-product","O","O","O","O","O","O","B-task","I-task","O","B-task","O","B-task","I-task","I-task","O","O","B-task","I-task","O","O","O","O","O","O","O","B-task","I-task","I-task","O","O"],"target_index":null,"target_label":null},"label_list":["conference","university","field","country","algorithm","researcher","product","location","programming_language","metric","task","person","organization"]}
{"id":"9","dataset":"crossner_ai","split":"test","instance":{"id":"9","prompt_labels":"This(O) would(O) include(O) programs(O) such(O) as(O) data(B-field) analysis(I-field) and(O) extraction(O) tools(O) ,(O) spreadsheets(O) ((O) e.g.(O) Excel(B-product) )(O) ,(O) databases(O) ((O) e.g.(O) Access(B-product) )(O) ,(O) statistical(B-field) analysis(I-field) ((O) e.g.(O) SAS(B-product) )(O) ,(O) generalized(O) audit(O) software(O) ((O) e.g.(O) ACL(B-product) ,(O) Arbutus(B-product) ,(O) EAS(B-product) )(O) ,(O) business(O) intelligence(O) ((O) e.g.(O) Crystal(B-product) Reports(I-product) and(O) Business(B-product) Objects(I-product) )(O) ,(O) etc(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, field, university, researcher, task, person, location, country, organization, algorithm, product, conference, metric and O.\nSentence: This would include programs such as data analysis and extraction tools , spreadsheets ( e.g. Excel ) , databases ( e.g. Access ) , statistical analysis ( e.g. SAS ) , generalized audit software ( e.g. ACL , Arbutus , EAS ) , business intelligence ( e.g. Crystal Reports and Business Objects ) , etc .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","would","include","programs","such","as","data","analysis","and","extraction","tools",",","spreadsheets","(","e.g.","Excel",")",",","databases","(","e.g.","Access",")",",","statistical","analysis","(","e.g.","SAS",")",",","generalized","audit","software","(","e.g.","ACL",",","Arbutus",",","EAS",")",",","business","intelligence","(","e.g.","Crystal","Reports","and","Business","Objects",")",",","etc","."],"labels":["O","O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","B-product","O","O","O","O","O","B-product","O","O","B-field","I-field","O","O","B-product","O","O","O","O","O","O","O","B-product","O","B-product","O","B-product","O","O","O","O","O","O","B-product","I-product","O","B-product","I-product","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","field","university","researcher","task","person","location","country","organization","algorithm","product","conference","metric"]}
{"id":"11","dataset":"crossner_ai","split":"test","instance":{"id":"11","prompt_labels":"Typical(O) text(B-field) mining(I-field) tasks(O) include(O) text(B-task) categorization(I-task) ,(O) text(B-task) clustering(I-task) ,(O) concept(B-task) /(I-task) entity(I-task) extraction(I-task) ,(O) production(B-task) of(I-task) granular(I-task) taxonomies(I-task) ,(O) sentiment(B-task) analysis(I-task) ,(O) document(B-task) summarization(I-task) ,(O) and(O) entity(B-task) relation(I-task) modeling(I-task) ((O) i.e.(O) ,(O) learning(O) relations(O) between(O) named(B-task) entity(I-task) recognition(I-task) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, organization, researcher, algorithm, country, product, person, location, university, programming language, task, field, metric and O.\nSentence: Typical text mining tasks include text categorization , text clustering , concept / entity extraction , production of granular taxonomies , sentiment analysis , document summarization , and entity relation modeling ( i.e. , learning relations between named entity recognition ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Typical","text","mining","tasks","include","text","categorization",",","text","clustering",",","concept","/","entity","extraction",",","production","of","granular","taxonomies",",","sentiment","analysis",",","document","summarization",",","and","entity","relation","modeling","(","i.e.",",","learning","relations","between","named","entity","recognition",")","."],"labels":["O","B-field","I-field","O","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","I-task","I-task","O","B-task","I-task","I-task","I-task","O","B-task","I-task","O","B-task","I-task","O","O","B-task","I-task","I-task","O","O","O","O","O","O","B-task","I-task","I-task","O","O"],"target_index":null,"target_label":null},"label_list":["conference","organization","researcher","algorithm","country","product","person","location","university","programming_language","task","field","metric"]}
{"id":"12","dataset":"crossner_ai","split":"test","instance":{"id":"12","prompt_labels":"Nonetheless(O) ,(O) stemming(O) reduces(O) precision(B-metric) ,(O) or(O) TRUE(B-metric) negative(I-metric) rate(I-metric) ,(O) for(O) such(O) systems(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, organization, metric, programming language, university, field, location, person, algorithm, product, task, researcher, country and O.\nSentence: Nonetheless , stemming reduces precision , or TRUE negative rate , for such systems .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Nonetheless",",","stemming","reduces","precision",",","or","TRUE","negative","rate",",","for","such","systems","."],"labels":["O","O","O","O","B-metric","O","O","B-metric","I-metric","I-metric","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["conference","organization","metric","programming_language","university","field","location","person","algorithm","product","task","researcher","country"]}
{"id":"13","dataset":"crossner_ai","split":"test","instance":{"id":"13","prompt_labels":"A(O) special(O) case(O) of(O) keyword(B-task) spotting(I-task) is(O) wake(O) word(O) ((O) also(O) called(O) hot(O) word(O) )(O) detection(O) used(O) by(O) personal(O) digital(O) assistants(O) such(O) as(O) Alexa(B-product) or(O) Siri(B-product) to(O) wake(O) up(O) when(O) their(O) name(O) is(O) spoken(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, location, programming language, country, task, person, conference, field, university, organization, researcher, metric, algorithm and O.\nSentence: A special case of keyword spotting is wake word ( also called hot word ) detection used by personal digital assistants such as Alexa or Siri to wake up when their name is spoken .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","special","case","of","keyword","spotting","is","wake","word","(","also","called","hot","word",")","detection","used","by","personal","digital","assistants","such","as","Alexa","or","Siri","to","wake","up","when","their","name","is","spoken","."],"labels":["O","O","O","O","B-task","I-task","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","O","B-product","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["product","location","programming_language","country","task","person","conference","field","university","organization","researcher","metric","algorithm"]}
{"id":"14","dataset":"crossner_ai","split":"test","instance":{"id":"14","prompt_labels":"Prova(B-programming language) is(O) an(O) open(O) source(O) programming(O) language(O) that(O) combines(O) Prolog(B-programming language) with(O) Java(B-programming language) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, researcher, location, university, algorithm, product, task, programming language, organization, conference, field, metric, country and O.\nSentence: Prova is an open source programming language that combines Prolog with Java .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Prova","is","an","open","source","programming","language","that","combines","Prolog","with","Java","."],"labels":["B-programming language","O","O","O","O","O","O","O","O","B-programming language","O","B-programming language","O"],"target_index":null,"target_label":null},"label_list":["person","researcher","location","university","algorithm","product","task","programming_language","organization","conference","field","metric","country"]}
{"id":"16","dataset":"crossner_ai","split":"test","instance":{"id":"16","prompt_labels":"Engelberger(B-researcher) 's(O) most(O) famous(O) co-invention(O) ,(O) the(O) Unimate(B-product) industrial(I-product) robotic(I-product) arm(I-product) ,(O) was(O) among(O) the(O) first(O) inductees(O) into(O) the(O) Robot(B-location) Hall(I-location) of(I-location) Fame(I-location) in(O) 2003(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, university, organization, programming language, researcher, task, conference, location, metric, algorithm, product, person, field and O.\nSentence: Engelberger 's most famous co-invention , the Unimate industrial robotic arm , was among the first inductees into the Robot Hall of Fame in 2003 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Engelberger","'s","most","famous","co-invention",",","the","Unimate","industrial","robotic","arm",",","was","among","the","first","inductees","into","the","Robot","Hall","of","Fame","in","2003","."],"labels":["B-researcher","O","O","O","O","O","O","B-product","I-product","I-product","I-product","O","O","O","O","O","O","O","O","B-location","I-location","I-location","I-location","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","university","organization","programming_language","researcher","task","conference","location","metric","algorithm","product","person","field"]}
{"id":"17","dataset":"crossner_ai","split":"test","instance":{"id":"17","prompt_labels":"Originally(O) controlled(O) via(O) static(O) html(O) web(O) pages(O) using(O) CGI(O) ,(O) work(O) by(O) Dalton(B-person) saw(O) the(O) introduction(O) of(O) an(O) augmented(B-field) reality(I-field) Java(B-programming language) -based(O) interface(O) that(O) met(O) with(O) limited(O) success(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, university, field, programming language, conference, country, task, researcher, organization, person, product, location, metric and O.\nSentence: Originally controlled via static html web pages using CGI , work by Dalton saw the introduction of an augmented reality Java -based interface that met with limited success .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Originally","controlled","via","static","html","web","pages","using","CGI",",","work","by","Dalton","saw","the","introduction","of","an","augmented","reality","Java","-based","interface","that","met","with","limited","success","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-person","O","O","O","O","O","B-field","I-field","B-programming language","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["algorithm","university","field","programming_language","conference","country","task","researcher","organization","person","product","location","metric"]}
{"id":"19","dataset":"crossner_ai","split":"test","instance":{"id":"19","prompt_labels":"A(O) confusion(B-metric) matrix(I-metric) or(O) matching(O) matrix(O) is(O) often(O) used(O) as(O) a(O) tool(O) to(O) validate(O) the(O) accuracy(B-metric) of(O) k(B-algorithm) -NN(I-algorithm) classification(I-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, country, algorithm, conference, university, person, programming language, field, metric, location, organization, researcher, task and O.\nSentence: A confusion matrix or matching matrix is often used as a tool to validate the accuracy of k -NN classification .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","confusion","matrix","or","matching","matrix","is","often","used","as","a","tool","to","validate","the","accuracy","of","k","-NN","classification","."],"labels":["O","B-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","O","B-algorithm","I-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["product","country","algorithm","conference","university","person","programming_language","field","metric","location","organization","researcher","task"]}
{"id":"21","dataset":"crossner_ai","split":"test","instance":{"id":"21","prompt_labels":"At(O) runtime(O) ,(O) the(O) target(O) prosody(O) of(O) a(O) sentence(O) is(O) superimposed(O) on(O) these(O) minimal(O) units(O) by(O) means(O) of(O) signal(B-field) processing(I-field) techniques(O) such(O) as(O) linear(B-algorithm) predictive(I-algorithm) coding(I-algorithm) ,(O) PSOLA(B-algorithm)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, task, person, conference, algorithm, organization, field, metric, researcher, product, programming language, university and O.\nSentence: At runtime , the target prosody of a sentence is superimposed on these minimal units by means of signal processing techniques such as linear predictive coding , PSOLA","prediction_output":null,"prediction_outputs":null,"group":null,"words":["At","runtime",",","the","target","prosody","of","a","sentence","is","superimposed","on","these","minimal","units","by","means","of","signal","processing","techniques","such","as","linear","predictive","coding",",","PSOLA"],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm"],"target_index":null,"target_label":null},"label_list":["location","country","task","person","conference","algorithm","organization","field","metric","researcher","product","programming_language","university"]}
{"id":"22","dataset":"crossner_ai","split":"test","instance":{"id":"22","prompt_labels":"This(O) approach(O) utilized(O) artificial(B-field) intelligence(I-field) and(O) machine(B-field) learning(I-field) to(O) allow(O) researchers(O) to(O) visibly(O) compare(O) conventional(O) and(O) thermal(O) facial(B-task) imagery(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, programming language, task, person, algorithm, country, field, product, metric, researcher, location, conference, university and O.\nSentence: This approach utilized artificial intelligence and machine learning to allow researchers to visibly compare conventional and thermal facial imagery .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","approach","utilized","artificial","intelligence","and","machine","learning","to","allow","researchers","to","visibly","compare","conventional","and","thermal","facial","imagery","."],"labels":["O","O","O","B-field","I-field","O","B-field","I-field","O","O","O","O","O","O","O","O","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["organization","programming_language","task","person","algorithm","country","field","product","metric","researcher","location","conference","university"]}
{"id":"26","dataset":"crossner_ai","split":"test","instance":{"id":"26","prompt_labels":"It(O) was(O) during(O) this(O) time(O) that(O) a(O) total(O) of(O) 43(O) publications(O) were(O) recognized(O) by(O) the(O) CVPR(B-conference) and(O) the(O) International(B-conference) Conference(I-conference) on(I-conference) Computer(I-conference) Vision(I-conference) ((O) ICCV(B-conference) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, field, conference, task, product, person, metric, programming language, location, university, researcher, algorithm, country and O.\nSentence: It was during this time that a total of 43 publications were recognized by the CVPR and the International Conference on Computer Vision ( ICCV ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","during","this","time","that","a","total","of","43","publications","were","recognized","by","the","CVPR","and","the","International","Conference","on","Computer","Vision","(","ICCV",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-conference","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","O","B-conference","O","O"],"target_index":null,"target_label":null},"label_list":["organization","field","conference","task","product","person","metric","programming_language","location","university","researcher","algorithm","country"]}
{"id":"27","dataset":"crossner_ai","split":"test","instance":{"id":"27","prompt_labels":"The(O) AIBO(B-product) has(O) seen(O) much(O) use(O) as(O) an(O) inexpensive(O) platform(O) for(O) artificial(B-field) intelligence(I-field) education(O) and(O) research(O) ,(O) because(O) integrates(O) a(O) computer(O) ,(O) Computer(B-field) vision(I-field) ,(O) and(O) articulators(O) in(O) a(O) package(O) vastly(O) cheaper(O) than(O) conventional(O) research(O) robots(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, country, programming language, metric, algorithm, university, location, organization, person, researcher, field, conference, task and O.\nSentence: The AIBO has seen much use as an inexpensive platform for artificial intelligence education and research , because integrates a computer , Computer vision , and articulators in a package vastly cheaper than conventional research robots .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","AIBO","has","seen","much","use","as","an","inexpensive","platform","for","artificial","intelligence","education","and","research",",","because","integrates","a","computer",",","Computer","vision",",","and","articulators","in","a","package","vastly","cheaper","than","conventional","research","robots","."],"labels":["O","B-product","O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["product","country","programming_language","metric","algorithm","university","location","organization","person","researcher","field","conference","task"]}
{"id":"28","dataset":"crossner_ai","split":"test","instance":{"id":"28","prompt_labels":"She(O) served(O) as(O) Program(O) Chair(O) of(O) International(B-conference) Conference(I-conference) on(I-conference) Computer(I-conference) Vision(I-conference) 2021(I-conference) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, conference, product, field, location, researcher, organization, metric, person, university, task, country, algorithm and O.\nSentence: She served as Program Chair of International Conference on Computer Vision 2021 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","served","as","Program","Chair","of","International","Conference","on","Computer","Vision","2021","."],"labels":["O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O"],"target_index":null,"target_label":null},"label_list":["programming_language","conference","product","field","location","researcher","organization","metric","person","university","task","country","algorithm"]}
{"id":"29","dataset":"crossner_ai","split":"test","instance":{"id":"29","prompt_labels":"Scheinman(B-researcher) ,(O) after(O) receiving(O) a(O) fellowship(O) from(O) Unimation(B-organization) to(O) develop(O) his(O) designs(O) ,(O) sold(O) those(O) designs(O) to(O) Unimation(B-organization) who(O) further(O) developed(O) them(O) with(O) support(O) from(O) General(B-organization) Motors(I-organization) and(O) later(O) marketed(O) it(O) as(O) the(O) Programmable(B-product) Universal(I-product) Machine(I-product) for(I-product) Assembly(I-product) ((O) PUMA(B-product) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, person, country, location, researcher, field, product, task, programming language, university, conference, organization, algorithm and O.\nSentence: Scheinman , after receiving a fellowship from Unimation to develop his designs , sold those designs to Unimation who further developed them with support from General Motors and later marketed it as the Programmable Universal Machine for Assembly ( PUMA ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Scheinman",",","after","receiving","a","fellowship","from","Unimation","to","develop","his","designs",",","sold","those","designs","to","Unimation","who","further","developed","them","with","support","from","General","Motors","and","later","marketed","it","as","the","Programmable","Universal","Machine","for","Assembly","(","PUMA",")","."],"labels":["B-researcher","O","O","O","O","O","O","B-organization","O","O","O","O","O","O","O","O","O","B-organization","O","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","B-product","I-product","I-product","I-product","I-product","O","B-product","O","O"],"target_index":null,"target_label":null},"label_list":["metric","person","country","location","researcher","field","product","task","programming_language","university","conference","organization","algorithm"]}
{"id":"30","dataset":"crossner_ai","split":"test","instance":{"id":"30","prompt_labels":"An(O) overview(O) of(O) calibration(O) methods(O) for(O) binary(B-task) classification(I-task) and(O) multiclass(B-task) classification(I-task) classification(I-task) tasks(I-task) is(O) given(O) by(O) Gebel(B-researcher) ((O) 2009(O) )(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, metric, location, conference, task, programming language, person, university, researcher, algorithm, organization, country, product and O.\nSentence: An overview of calibration methods for binary classification and multiclass classification classification tasks is given by Gebel ( 2009 )","prediction_output":null,"prediction_outputs":null,"group":null,"words":["An","overview","of","calibration","methods","for","binary","classification","and","multiclass","classification","classification","tasks","is","given","by","Gebel","(","2009",")"],"labels":["O","O","O","O","O","O","B-task","I-task","O","B-task","I-task","I-task","I-task","O","O","O","B-researcher","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","metric","location","conference","task","programming_language","person","university","researcher","algorithm","organization","country","product"]}
{"id":"33","dataset":"crossner_ai","split":"test","instance":{"id":"33","prompt_labels":"Johnson-Laird(B-researcher) is(O) a(O) Fellow(O) of(O) the(O) American(B-organization) Philosophical(I-organization) Society(I-organization) ,(O) a(O) Fellow(O) of(O) the(O) Royal(B-organization) Society(I-organization) ,(O) a(O) Fellow(O) of(O) the(O) British(B-organization) Academy(I-organization) ,(O) a(O) William(B-researcher) James(I-researcher) Fellow(O) of(O) the(O) Association(B-organization) for(I-organization) Psychological(I-organization) Science(I-organization) ,(O) and(O) a(O) Fellow(O) of(O) the(O) Cognitive(B-organization) Science(I-organization) Society(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, programming language, person, country, algorithm, location, field, researcher, organization, university, conference, metric, task and O.\nSentence: Johnson-Laird is a Fellow of the American Philosophical Society , a Fellow of the Royal Society , a Fellow of the British Academy , a William James Fellow of the Association for Psychological Science , and a Fellow of the Cognitive Science Society .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Johnson-Laird","is","a","Fellow","of","the","American","Philosophical","Society",",","a","Fellow","of","the","Royal","Society",",","a","Fellow","of","the","British","Academy",",","a","William","James","Fellow","of","the","Association","for","Psychological","Science",",","and","a","Fellow","of","the","Cognitive","Science","Society","."],"labels":["B-researcher","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","B-organization","I-organization","O","O","B-researcher","I-researcher","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["product","programming_language","person","country","algorithm","location","field","researcher","organization","university","conference","metric","task"]}
{"id":"35","dataset":"crossner_ai","split":"test","instance":{"id":"35","prompt_labels":"BLEU(B-metric) uses(O) a(O) modified(O) form(O) of(O) precision(B-metric) to(O) compare(O) a(O) candidate(O) translation(O) against(O) multiple(O) reference(O) translations(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, country, algorithm, metric, location, person, programming language, organization, conference, product, university, task, field and O.\nSentence: BLEU uses a modified form of precision to compare a candidate translation against multiple reference translations .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["BLEU","uses","a","modified","form","of","precision","to","compare","a","candidate","translation","against","multiple","reference","translations","."],"labels":["B-metric","O","O","O","O","O","B-metric","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","country","algorithm","metric","location","person","programming_language","organization","conference","product","university","task","field"]}
{"id":"36","dataset":"crossner_ai","split":"test","instance":{"id":"36","prompt_labels":"For(O) the(O) case(O) of(O) a(O) general(O) base(O) space(O) math(O) ((O) Y(O) ,(O) \\(O) mathcal(O) {(O) B(O) }(O) ,(O) \\(O) nu(O) )(O) /(O) math(O) ((O) i.e.(O) a(O) base(O) space(O) which(O) is(O) not(O) countable(O) )(O) ,(O) one(O) typically(O) considers(O) the(O) relative(B-metric) entropy(I-metric) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, programming language, university, organization, algorithm, person, location, field, product, metric, country, researcher, conference and O.\nSentence: For the case of a general base space math ( Y , \\ mathcal { B } , \\ nu ) / math ( i.e. a base space which is not countable ) , one typically considers the relative entropy .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","the","case","of","a","general","base","space","math","(","Y",",","\\","mathcal","{","B","}",",","\\","nu",")","/","math","(","i.e.","a","base","space","which","is","not","countable",")",",","one","typically","considers","the","relative","entropy","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","O"],"target_index":null,"target_label":null},"label_list":["task","programming_language","university","organization","algorithm","person","location","field","product","metric","country","researcher","conference"]}
{"id":"37","dataset":"crossner_ai","split":"test","instance":{"id":"37","prompt_labels":"As(O) of(O) October(O) 2011(O) ,(O) the(O) already-existing(O) partnerships(O) with(O) the(O) United(B-country) States(I-country) '(O) National(B-organization) Park(I-organization) Service(I-organization) ((O) NPS(B-organization) )(O) ,(O) the(O) United(B-country) Kingdom(I-country) 's(O) Historic(B-organization) Scotland(I-organization) ((O) HS(B-organization) )(O) ,(O) World(B-organization) Monuments(I-organization) Fund(I-organization) ,(O) and(O) Mexico(B-country) 's(O) Instituto(B-organization) Nacional(I-organization) de(I-organization) Antropologa(I-organization) y(I-organization) Historia(I-organization) ((O) INAH(B-organization) )(O) had(O) been(O) greatly(O) expanded(O) ,(O) ,(O) CyArk(O) website(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, researcher, person, country, task, organization, algorithm, location, field, programming language, conference, product, university and O.\nSentence: As of October 2011 , the already-existing partnerships with the United States ' National Park Service ( NPS ) , the United Kingdom 's Historic Scotland ( HS ) , World Monuments Fund , and Mexico 's Instituto Nacional de Antropologa y Historia ( INAH ) had been greatly expanded , , CyArk website","prediction_output":null,"prediction_outputs":null,"group":null,"words":["As","of","October","2011",",","the","already-existing","partnerships","with","the","United","States","'","National","Park","Service","(","NPS",")",",","the","United","Kingdom","'s","Historic","Scotland","(","HS",")",",","World","Monuments","Fund",",","and","Mexico","'s","Instituto","Nacional","de","Antropologa","y","Historia","(","INAH",")","had","been","greatly","expanded",",",",","CyArk","website"],"labels":["O","O","O","O","O","O","O","O","O","O","B-country","I-country","O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","B-country","I-country","O","B-organization","I-organization","O","B-organization","O","O","B-organization","I-organization","I-organization","O","O","B-country","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","researcher","person","country","task","organization","algorithm","location","field","programming_language","conference","product","university"]}
{"id":"38","dataset":"crossner_ai","split":"test","instance":{"id":"38","prompt_labels":"Kernel(B-algorithm) SVMs(I-algorithm) are(O) available(O) in(O) many(O) machine-learning(B-field) toolkits(O) ,(O) including(O) LIBSVM(B-product) ,(O) MATLAB(B-product) ,(O) and(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, person, conference, algorithm, programming language, product, location, organization, university, field, metric, task, country and O.\nSentence: Kernel SVMs are available in many machine-learning toolkits , including LIBSVM , MATLAB , and others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Kernel","SVMs","are","available","in","many","machine-learning","toolkits",",","including","LIBSVM",",","MATLAB",",","and","others","."],"labels":["B-algorithm","I-algorithm","O","O","O","O","B-field","O","O","O","B-product","O","B-product","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","person","conference","algorithm","programming_language","product","location","organization","university","field","metric","task","country"]}
{"id":"39","dataset":"crossner_ai","split":"test","instance":{"id":"39","prompt_labels":"The(O) 2009(O) Loebner(O) Prize(O) Competition(O) was(O) held(O) September(O) 6(O) ,(O) 2009(O) at(O) the(O) Brighton(B-location) Centre(I-location) ,(O) Brighton(B-location) UK(B-country) in(O) conjunction(O) with(O) the(O) Interspeech(B-conference) 2009(I-conference) conference(I-conference) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, organization, country, conference, metric, product, researcher, university, location, person, task, field, algorithm and O.\nSentence: The 2009 Loebner Prize Competition was held September 6 , 2009 at the Brighton Centre , Brighton UK in conjunction with the Interspeech 2009 conference .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","2009","Loebner","Prize","Competition","was","held","September","6",",","2009","at","the","Brighton","Centre",",","Brighton","UK","in","conjunction","with","the","Interspeech","2009","conference","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","B-country","O","O","O","O","B-conference","I-conference","I-conference","O"],"target_index":null,"target_label":null},"label_list":["programming_language","organization","country","conference","metric","product","researcher","university","location","person","task","field","algorithm"]}
{"id":"40","dataset":"crossner_ai","split":"test","instance":{"id":"40","prompt_labels":"The(O) humanoid(O) QRIO(B-product) robot(I-product) was(O) designed(O) as(O) the(O) successor(O) to(O) AIBO(B-product) ,(O) and(O) runs(O) the(O) same(O) base(O) R-CODE(B-product) Aperios(B-product) operating(I-product) system(I-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, algorithm, university, field, task, conference, organization, location, person, product, metric, researcher, programming language and O.\nSentence: The humanoid QRIO robot was designed as the successor to AIBO , and runs the same base R-CODE Aperios operating system .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","humanoid","QRIO","robot","was","designed","as","the","successor","to","AIBO",",","and","runs","the","same","base","R-CODE","Aperios","operating","system","."],"labels":["O","O","B-product","I-product","O","O","O","O","O","O","B-product","O","O","O","O","O","O","B-product","B-product","I-product","I-product","O"],"target_index":null,"target_label":null},"label_list":["country","algorithm","university","field","task","conference","organization","location","person","product","metric","researcher","programming_language"]}
{"id":"41","dataset":"crossner_ai","split":"test","instance":{"id":"41","prompt_labels":"Speech(O) waveforms(O) are(O) generated(O) from(O) HMMs(B-algorithm) themselves(O) based(O) on(O) the(O) maximum(B-algorithm) likelihood(I-algorithm) criterion(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, university, researcher, organization, conference, algorithm, location, person, metric, country, programming language, field, product and O.\nSentence: Speech waveforms are generated from HMMs themselves based on the maximum likelihood criterion .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Speech","waveforms","are","generated","from","HMMs","themselves","based","on","the","maximum","likelihood","criterion","."],"labels":["O","O","O","O","O","B-algorithm","O","O","O","O","B-algorithm","I-algorithm","O","O"],"target_index":null,"target_label":null},"label_list":["task","university","researcher","organization","conference","algorithm","location","person","metric","country","programming_language","field","product"]}
{"id":"43","dataset":"crossner_ai","split":"test","instance":{"id":"43","prompt_labels":"Skeletons(O) are(O) widely(O) used(O) in(O) computer(B-field) vision(I-field) ,(O) image(B-field) analysis(I-field) ,(O) pattern(B-field) recognition(I-field) and(O) digital(B-field) image(I-field) processing(I-field) for(O) purposes(O) such(O) as(O) optical(B-task) character(I-task) recognition(I-task) ,(O) fingerprint(B-task) recognition(I-task) ,(O) visual(B-task) inspection(I-task) or(I-task) compression(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, metric, algorithm, product, task, organization, programming language, country, university, location, person, conference, researcher and O.\nSentence: Skeletons are widely used in computer vision , image analysis , pattern recognition and digital image processing for purposes such as optical character recognition , fingerprint recognition , visual inspection or compression .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Skeletons","are","widely","used","in","computer","vision",",","image","analysis",",","pattern","recognition","and","digital","image","processing","for","purposes","such","as","optical","character","recognition",",","fingerprint","recognition",",","visual","inspection","or","compression","."],"labels":["O","O","O","O","O","B-field","I-field","O","B-field","I-field","O","B-field","I-field","O","B-field","I-field","I-field","O","O","O","O","B-task","I-task","I-task","O","B-task","I-task","O","B-task","I-task","I-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["field","metric","algorithm","product","task","organization","programming_language","country","university","location","person","conference","researcher"]}
{"id":"44","dataset":"crossner_ai","split":"test","instance":{"id":"44","prompt_labels":"The(O) ImageNet(B-conference) Large(I-conference) Scale(I-conference) Visual(I-conference) Recognition(I-conference) Challenge(I-conference) is(O) a(O) benchmark(O) in(O) object(B-task) classification(I-task) and(I-task) detection(I-task) ,(O) with(O) millions(O) of(O) images(O) and(O) hundreds(O) of(O) object(O) classes(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, metric, product, field, organization, programming language, researcher, country, conference, person, algorithm, location, task and O.\nSentence: The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection , with millions of images and hundreds of object classes .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","ImageNet","Large","Scale","Visual","Recognition","Challenge","is","a","benchmark","in","object","classification","and","detection",",","with","millions","of","images","and","hundreds","of","object","classes","."],"labels":["O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","O","O","O","B-task","I-task","I-task","I-task","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","metric","product","field","organization","programming_language","researcher","country","conference","person","algorithm","location","task"]}
{"id":"47","dataset":"crossner_ai","split":"test","instance":{"id":"47","prompt_labels":"NSA(B-organization) Bethesda(I-organization) is(O) responsible(O) for(O) base(O) operational(O) support(O) for(O) its(O) major(O) tenant(O) ,(O) the(O) Walter(B-organization) Reed(I-organization) National(I-organization) Military(I-organization) Medical(I-organization) Center(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, conference, country, university, metric, product, programming language, person, researcher, location, field, task, algorithm and O.\nSentence: NSA Bethesda is responsible for base operational support for its major tenant , the Walter Reed National Military Medical Center .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["NSA","Bethesda","is","responsible","for","base","operational","support","for","its","major","tenant",",","the","Walter","Reed","National","Military","Medical","Center","."],"labels":["B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["organization","conference","country","university","metric","product","programming_language","person","researcher","location","field","task","algorithm"]}
{"id":"48","dataset":"crossner_ai","split":"test","instance":{"id":"48","prompt_labels":"The(O) three(O) major(O) learning(O) paradigms(O) are(O) supervised(B-field) learning(I-field) ,(O) unsupervised(B-field) learning(I-field) and(O) reinforcement(B-field) learning(I-field) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, conference, task, metric, field, location, programming language, country, researcher, algorithm, organization, product, person and O.\nSentence: The three major learning paradigms are supervised learning , unsupervised learning and reinforcement learning .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","three","major","learning","paradigms","are","supervised","learning",",","unsupervised","learning","and","reinforcement","learning","."],"labels":["O","O","O","O","O","O","B-field","I-field","O","B-field","I-field","O","B-field","I-field","O"],"target_index":null,"target_label":null},"label_list":["university","conference","task","metric","field","location","programming_language","country","researcher","algorithm","organization","product","person"]}
{"id":"50","dataset":"crossner_ai","split":"test","instance":{"id":"50","prompt_labels":"In(O) 1991(O) he(O) was(O) elected(O) as(O) a(O) fellow(O) of(O) the(O) Association(B-conference) for(I-conference) the(I-conference) Advancement(I-conference) of(I-conference) Artificial(I-conference) Intelligence(I-conference) ((O) 1990(O) ,(O) founding(O) fellow(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, conference, programming language, organization, task, country, university, product, metric, person, algorithm, researcher, field and O.\nSentence: In 1991 he was elected as a fellow of the Association for the Advancement of Artificial Intelligence ( 1990 , founding fellow ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1991","he","was","elected","as","a","fellow","of","the","Association","for","the","Advancement","of","Artificial","Intelligence","(","1990",",","founding","fellow",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","conference","programming_language","organization","task","country","university","product","metric","person","algorithm","researcher","field"]}
{"id":"51","dataset":"crossner_ai","split":"test","instance":{"id":"51","prompt_labels":"However(O) ,(O) by(O) formulating(O) the(O) problem(O) as(O) the(O) solution(O) of(O) a(O) Toeplitz(O) matrix(O) and(O) using(O) Levinson(B-algorithm) recursion(I-algorithm) ,(O) we(O) can(O) relatively(O) quickly(O) estimate(O) a(O) filter(O) with(O) the(O) smallest(O) mean(B-metric) squared(I-metric) error(I-metric) possible(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, task, country, product, location, university, person, field, organization, algorithm, programming language, metric, researcher and O.\nSentence: However , by formulating the problem as the solution of a Toeplitz matrix and using Levinson recursion , we can relatively quickly estimate a filter with the smallest mean squared error possible .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["However",",","by","formulating","the","problem","as","the","solution","of","a","Toeplitz","matrix","and","using","Levinson","recursion",",","we","can","relatively","quickly","estimate","a","filter","with","the","smallest","mean","squared","error","possible","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O"],"target_index":null,"target_label":null},"label_list":["conference","task","country","product","location","university","person","field","organization","algorithm","programming_language","metric","researcher"]}
{"id":"52","dataset":"crossner_ai","split":"test","instance":{"id":"52","prompt_labels":"In(O) July(O) 2011(O) the(O) 15th(B-conference) edition(I-conference) of(I-conference) Campus(I-conference) Party(I-conference) Spain(I-conference) will(O) be(O) held(O) at(O) the(O) City(B-location) of(I-location) Arts(I-location) and(I-location) Sciences(I-location) in(O) Valencia(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, location, product, metric, country, conference, field, person, organization, researcher, task, algorithm, university and O.\nSentence: In July 2011 the 15th edition of Campus Party Spain will be held at the City of Arts and Sciences in Valencia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","July","2011","the","15th","edition","of","Campus","Party","Spain","will","be","held","at","the","City","of","Arts","and","Sciences","in","Valencia","."],"labels":["O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","O","O","O","O","B-location","I-location","I-location","I-location","I-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["programming_language","location","product","metric","country","conference","field","person","organization","researcher","task","algorithm","university"]}
{"id":"53","dataset":"crossner_ai","split":"test","instance":{"id":"53","prompt_labels":"Often(O) this(O) is(O) generally(O) only(O) possible(O) at(O) the(O) very(O) end(O) of(O) complicated(O) games(O) such(O) as(O) chess(B-product) or(O) go(B-product) ,(O) since(O) it(O) is(O) not(O) computationally(O) feasible(O) to(O) look(O) ahead(O) as(O) far(O) as(O) the(O) completion(O) of(O) the(O) game(O) ,(O) except(O) towards(O) the(O) end(O) ,(O) and(O) instead(O) ,(O) positions(O) are(O) given(O) finite(O) values(O) as(O) estimates(O) of(O) the(O) degree(O) of(O) belief(O) that(O) they(O) will(O) lead(O) to(O) a(O) win(O) for(O) one(O) player(O) or(O) another(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, task, location, conference, university, organization, person, metric, country, field, programming language, product, researcher and O.\nSentence: Often this is generally only possible at the very end of complicated games such as chess or go , since it is not computationally feasible to look ahead as far as the completion of the game , except towards the end , and instead , positions are given finite values as estimates of the degree of belief that they will lead to a win for one player or another .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Often","this","is","generally","only","possible","at","the","very","end","of","complicated","games","such","as","chess","or","go",",","since","it","is","not","computationally","feasible","to","look","ahead","as","far","as","the","completion","of","the","game",",","except","towards","the","end",",","and","instead",",","positions","are","given","finite","values","as","estimates","of","the","degree","of","belief","that","they","will","lead","to","a","win","for","one","player","or","another","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","O","B-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["algorithm","task","location","conference","university","organization","person","metric","country","field","programming_language","product","researcher"]}
{"id":"55","dataset":"crossner_ai","split":"test","instance":{"id":"55","prompt_labels":"Association(B-conference) for(I-conference) Computational(I-conference) Linguistics(I-conference) ,(O) published(O) by(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, product, university, metric, researcher, location, conference, field, person, organization, task, programming language, country and O.\nSentence: Association for Computational Linguistics , published by","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Association","for","Computational","Linguistics",",","published","by"],"labels":["B-conference","I-conference","I-conference","I-conference","O","O","O"],"target_index":null,"target_label":null},"label_list":["algorithm","product","university","metric","researcher","location","conference","field","person","organization","task","programming_language","country"]}
{"id":"57","dataset":"crossner_ai","split":"test","instance":{"id":"57","prompt_labels":"In(O) 2002(O) ,(O) his(O) son(O) ,(O) Daniel(B-person) Pearl(I-person) ,(O) a(O) journalist(O) working(O) for(O) the(O) Wall(B-organization) Street(I-organization) Journal(I-organization) was(O) kidnapped(O) and(O) murdered(O) in(O) Pakistan(B-country) ,(O) leading(O) Judea(B-person) and(O) the(O) other(O) members(O) of(O) the(O) family(O) and(O) friends(O) to(O) create(O) the(O) Daniel(B-organization) Pearl(I-organization) Foundation(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, programming language, person, field, conference, researcher, location, metric, product, algorithm, university, task, organization and O.\nSentence: In 2002 , his son , Daniel Pearl , a journalist working for the Wall Street Journal was kidnapped and murdered in Pakistan , leading Judea and the other members of the family and friends to create the Daniel Pearl Foundation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2002",",","his","son",",","Daniel","Pearl",",","a","journalist","working","for","the","Wall","Street","Journal","was","kidnapped","and","murdered","in","Pakistan",",","leading","Judea","and","the","other","members","of","the","family","and","friends","to","create","the","Daniel","Pearl","Foundation","."],"labels":["O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","B-country","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["country","programming_language","person","field","conference","researcher","location","metric","product","algorithm","university","task","organization"]}
{"id":"58","dataset":"crossner_ai","split":"test","instance":{"id":"58","prompt_labels":"As(O) of(O) late(O) 2006(O) ,(O) Red(B-organization) Envelope(I-organization) Entertainment(I-organization) also(O) expanded(O) into(O) producing(O) original(O) content(O) with(O) filmmakers(O) such(O) as(O) John(B-person) Waters(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, person, organization, country, task, programming language, researcher, location, metric, field, university, algorithm, product and O.\nSentence: As of late 2006 , Red Envelope Entertainment also expanded into producing original content with filmmakers such as John Waters .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["As","of","late","2006",",","Red","Envelope","Entertainment","also","expanded","into","producing","original","content","with","filmmakers","such","as","John","Waters","."],"labels":["O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["conference","person","organization","country","task","programming_language","researcher","location","metric","field","university","algorithm","product"]}
{"id":"59","dataset":"crossner_ai","split":"test","instance":{"id":"59","prompt_labels":"The(O) building(O) is(O) now(O) part(O) of(O) the(O) Beth(B-organization) Israel(I-organization) Deaconess(I-organization) Medical(I-organization) Center(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, algorithm, conference, person, location, field, product, country, organization, researcher, programming language, university, metric and O.\nSentence: The building is now part of the Beth Israel Deaconess Medical Center .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","building","is","now","part","of","the","Beth","Israel","Deaconess","Medical","Center","."],"labels":["O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["task","algorithm","conference","person","location","field","product","country","organization","researcher","programming_language","university","metric"]}
{"id":"60","dataset":"crossner_ai","split":"test","instance":{"id":"60","prompt_labels":"A(O) common(O) theme(O) of(O) this(O) work(O) is(O) the(O) adoption(O) of(O) a(O) sign-theoretic(O) perspective(O) on(O) issues(O) of(O) artificial(B-field) intelligence(I-field) and(O) knowledge(B-task) representation(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, algorithm, location, conference, university, person, programming language, country, researcher, metric, field, task, product and O.\nSentence: A common theme of this work is the adoption of a sign-theoretic perspective on issues of artificial intelligence and knowledge representation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","common","theme","of","this","work","is","the","adoption","of","a","sign-theoretic","perspective","on","issues","of","artificial","intelligence","and","knowledge","representation","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["organization","algorithm","location","conference","university","person","programming_language","country","researcher","metric","field","task","product"]}
{"id":"63","dataset":"crossner_ai","split":"test","instance":{"id":"63","prompt_labels":"Notable(O) former(O) PhD(O) students(O) and(O) postdoctoral(O) researchers(O) from(O) his(O) group(O) include(O) Richard(B-researcher) Zemel(I-researcher) ,(O) and(O) Zoubin(B-researcher) Ghahramani(I-researcher) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, product, field, conference, country, metric, location, university, task, algorithm, organization, programming language, person and O.\nSentence: Notable former PhD students and postdoctoral researchers from his group include Richard Zemel , and Zoubin Ghahramani .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Notable","former","PhD","students","and","postdoctoral","researchers","from","his","group","include","Richard","Zemel",",","and","Zoubin","Ghahramani","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-researcher","I-researcher","O","O","B-researcher","I-researcher","O"],"target_index":null,"target_label":null},"label_list":["researcher","product","field","conference","country","metric","location","university","task","algorithm","organization","programming_language","person"]}
{"id":"65","dataset":"crossner_ai","split":"test","instance":{"id":"65","prompt_labels":"In(O) 1997(O) Thrun(B-researcher) and(O) his(O) colleagues(O) Wolfram(B-researcher) Burgard(I-researcher) and(O) Dieter(B-researcher) Fox(I-researcher) developed(O) the(O) world(O) 's(O) first(O) robotic(B-product) tour(I-product) guide(I-product) in(O) the(O) Deutsches(B-location) Museum(I-location) Bonn(I-location) ((O) 1997(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, field, researcher, location, country, person, organization, university, metric, task, conference, programming language, product and O.\nSentence: In 1997 Thrun and his colleagues Wolfram Burgard and Dieter Fox developed the world 's first robotic tour guide in the Deutsches Museum Bonn ( 1997 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1997","Thrun","and","his","colleagues","Wolfram","Burgard","and","Dieter","Fox","developed","the","world","'s","first","robotic","tour","guide","in","the","Deutsches","Museum","Bonn","(","1997",")","."],"labels":["O","O","B-researcher","O","O","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","O","O","O","O","B-product","I-product","I-product","O","O","B-location","I-location","I-location","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["algorithm","field","researcher","location","country","person","organization","university","metric","task","conference","programming_language","product"]}
{"id":"67","dataset":"crossner_ai","split":"test","instance":{"id":"67","prompt_labels":"Conferences(O) in(O) the(O) field(O) of(O) natural(B-field) language(I-field) processing(I-field) ,(O) such(O) as(O) Association(B-conference) for(I-conference) Computational(I-conference) Linguistics(I-conference) ,(O) North(B-conference) American(I-conference) Chapter(I-conference) of(I-conference) the(I-conference) Association(I-conference) for(I-conference) Computational(I-conference) Linguistics(I-conference) ,(O) EMNLP(B-conference) ,(O) and(O) HLT(B-conference) ,(O) are(O) beginning(O) to(O) include(O) papers(O) on(O) speech(B-field) processing(I-field) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, person, metric, country, programming language, algorithm, researcher, university, location, field, conference, organization, product and O.\nSentence: Conferences in the field of natural language processing , such as Association for Computational Linguistics , North American Chapter of the Association for Computational Linguistics , EMNLP , and HLT , are beginning to include papers on speech processing .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Conferences","in","the","field","of","natural","language","processing",",","such","as","Association","for","Computational","Linguistics",",","North","American","Chapter","of","the","Association","for","Computational","Linguistics",",","EMNLP",",","and","HLT",",","are","beginning","to","include","papers","on","speech","processing","."],"labels":["O","O","O","O","O","B-field","I-field","I-field","O","O","O","B-conference","I-conference","I-conference","I-conference","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","B-conference","O","O","B-conference","O","O","O","O","O","O","O","B-field","I-field","O"],"target_index":null,"target_label":null},"label_list":["task","person","metric","country","programming_language","algorithm","researcher","university","location","field","conference","organization","product"]}
{"id":"68","dataset":"crossner_ai","split":"test","instance":{"id":"68","prompt_labels":"A(O) set(O) of(O) Java(B-programming language) programs(O) use(O) the(O) lexicon(O) to(O) work(O) through(O) the(O) variations(O) in(O) biomedical(O) texts(O) by(O) relating(O) words(O) by(O) their(O) parts(O) of(O) speech(O) ,(O) which(O) can(O) be(O) helpful(O) in(O) web(O) searches(O) or(O) searches(O) through(O) an(O) electronic(O) medical(O) record(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, programming language, researcher, task, field, conference, algorithm, country, product, university, organization, metric and O.\nSentence: A set of Java programs use the lexicon to work through the variations in biomedical texts by relating words by their parts of speech , which can be helpful in web searches or searches through an electronic medical record .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","set","of","Java","programs","use","the","lexicon","to","work","through","the","variations","in","biomedical","texts","by","relating","words","by","their","parts","of","speech",",","which","can","be","helpful","in","web","searches","or","searches","through","an","electronic","medical","record","."],"labels":["O","O","O","B-programming language","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","location","programming_language","researcher","task","field","conference","algorithm","country","product","university","organization","metric"]}
{"id":"69","dataset":"crossner_ai","split":"test","instance":{"id":"69","prompt_labels":"There(O) are(O) many(O) more(O) recent(O) algorithms(O) such(O) as(O) LPBoost(B-algorithm) ,(O) TotalBoost(B-algorithm) ,(O) BrownBoost(B-algorithm) ,(O) xgboost(B-algorithm) ,(O) MadaBoost(B-algorithm) ,(O) ,(O) and(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, university, location, metric, person, conference, product, country, field, organization, researcher, programming language, algorithm and O.\nSentence: There are many more recent algorithms such as LPBoost , TotalBoost , BrownBoost , xgboost , MadaBoost , , and others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["There","are","many","more","recent","algorithms","such","as","LPBoost",",","TotalBoost",",","BrownBoost",",","xgboost",",","MadaBoost",",",",","and","others","."],"labels":["O","O","O","O","O","O","O","O","B-algorithm","O","B-algorithm","O","B-algorithm","O","B-algorithm","O","B-algorithm","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["task","university","location","metric","person","conference","product","country","field","organization","researcher","programming_language","algorithm"]}
{"id":"70","dataset":"crossner_ai","split":"test","instance":{"id":"70","prompt_labels":"This(O) is(O) an(O) example(O) implementation(O) in(O) Python(B-programming language) :(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, location, conference, programming language, product, person, university, country, algorithm, researcher, task, organization, metric and O.\nSentence: This is an example implementation in Python :","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","is","an","example","implementation","in","Python",":"],"labels":["O","O","O","O","O","O","B-programming language","O"],"target_index":null,"target_label":null},"label_list":["field","location","conference","programming_language","product","person","university","country","algorithm","researcher","task","organization","metric"]}
{"id":"71","dataset":"crossner_ai","split":"test","instance":{"id":"71","prompt_labels":"The(O) Mattel(B-product) Intellivision(B-product) game(O) console(O) offered(O) the(O) Intellivoice(B-task) Voice(I-task) Synthesis(I-task) module(O) in(O) 1982(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, product, researcher, task, location, organization, programming language, metric, field, university, country, conference, algorithm and O.\nSentence: The Mattel Intellivision game console offered the Intellivoice Voice Synthesis module in 1982 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Mattel","Intellivision","game","console","offered","the","Intellivoice","Voice","Synthesis","module","in","1982","."],"labels":["O","B-product","B-product","O","O","O","O","B-task","I-task","I-task","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","product","researcher","task","location","organization","programming_language","metric","field","university","country","conference","algorithm"]}
{"id":"73","dataset":"crossner_ai","split":"test","instance":{"id":"73","prompt_labels":"Wolfram(B-organization) Mathematica(I-organization) ((O) usually(O) termed(O) Mathematica(B-organization) )(O) is(O) a(O) modern(O) technical(O) computing(O) system(O) spanning(O) most(O) areas(O) of(O) technical(O) -(O) including(O) neural(B-algorithm) networks(I-algorithm) ,(O) machine(B-field) learning(I-field) ,(O) image(B-field) processing(I-field) ,(O) geometry(B-field) ,(O) data(B-field) science(I-field) ,(O) visualizations(B-field) ,(O) and(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, algorithm, field, programming language, conference, product, university, researcher, task, person, location, metric and O.\nSentence: Wolfram Mathematica ( usually termed Mathematica ) is a modern technical computing system spanning most areas of technical - including neural networks , machine learning , image processing , geometry , data science , visualizations , and others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Wolfram","Mathematica","(","usually","termed","Mathematica",")","is","a","modern","technical","computing","system","spanning","most","areas","of","technical","-","including","neural","networks",",","machine","learning",",","image","processing",",","geometry",",","data","science",",","visualizations",",","and","others","."],"labels":["B-organization","I-organization","O","O","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","B-field","I-field","O","B-field","I-field","O","B-field","O","B-field","I-field","O","B-field","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","country","algorithm","field","programming_language","conference","product","university","researcher","task","person","location","metric"]}
{"id":"75","dataset":"crossner_ai","split":"test","instance":{"id":"75","prompt_labels":"Like(O) DBNs(B-algorithm) ,(O) DBMs(B-algorithm) can(O) learn(O) complex(O) and(O) abstract(O) internal(O) representations(O) of(O) the(O) input(O) in(O) tasks(O) such(O) as(O) Object(B-task) recognition(I-task) or(O) speech(B-task) recognition(I-task) ,(O) using(O) limited(O) ,(O) labeled(O) data(O) to(O) fine-tune(O) the(O) representations(O) built(O) using(O) a(O) large(O) set(O) of(O) unlabeled(O) sensory(O) input(O) data(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, university, organization, location, algorithm, metric, conference, product, task, field, country, programming language, person and O.\nSentence: Like DBNs , DBMs can learn complex and abstract internal representations of the input in tasks such as Object recognition or speech recognition , using limited , labeled data to fine-tune the representations built using a large set of unlabeled sensory input data .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Like","DBNs",",","DBMs","can","learn","complex","and","abstract","internal","representations","of","the","input","in","tasks","such","as","Object","recognition","or","speech","recognition",",","using","limited",",","labeled","data","to","fine-tune","the","representations","built","using","a","large","set","of","unlabeled","sensory","input","data","."],"labels":["O","B-algorithm","O","B-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-task","I-task","O","B-task","I-task","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","university","organization","location","algorithm","metric","conference","product","task","field","country","programming_language","person"]}
{"id":"78","dataset":"crossner_ai","split":"test","instance":{"id":"78","prompt_labels":"Similarly(O) ,(O) investigators(O) sometimes(O) report(O) the(O) FALSE(B-metric) Positive(I-metric) Rate(I-metric) ((O) FPR(B-metric) )(O) as(O) well(O) as(O) the(O) FALSE(B-metric) Negative(I-metric) Rate(I-metric) ((O) FNR(B-metric) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, person, programming language, researcher, country, field, algorithm, task, conference, organization, metric, location, university and O.\nSentence: Similarly , investigators sometimes report the FALSE Positive Rate ( FPR ) as well as the FALSE Negative Rate ( FNR ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Similarly",",","investigators","sometimes","report","the","FALSE","Positive","Rate","(","FPR",")","as","well","as","the","FALSE","Negative","Rate","(","FNR",")","."],"labels":["O","O","O","O","O","O","B-metric","I-metric","I-metric","O","B-metric","O","O","O","O","O","B-metric","I-metric","I-metric","O","B-metric","O","O"],"target_index":null,"target_label":null},"label_list":["product","person","programming_language","researcher","country","field","algorithm","task","conference","organization","metric","location","university"]}
{"id":"81","dataset":"crossner_ai","split":"test","instance":{"id":"81","prompt_labels":"In(O) 1913(O) ,(O) Walter(B-person) R.(I-person) Booth(I-person) directed(O) 10(O) films(O) for(O) the(O) U.K.(B-organization) Kinoplastikon(I-organization) ,(O) presumably(O) in(O) collaboration(O) with(O) Cecil(B-person) Hepworth(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, conference, metric, programming language, task, field, country, product, algorithm, location, researcher, organization, university and O.\nSentence: In 1913 , Walter R. Booth directed 10 films for the U.K. Kinoplastikon , presumably in collaboration with Cecil Hepworth .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1913",",","Walter","R.","Booth","directed","10","films","for","the","U.K.","Kinoplastikon",",","presumably","in","collaboration","with","Cecil","Hepworth","."],"labels":["O","O","O","B-person","I-person","I-person","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["person","conference","metric","programming_language","task","field","country","product","algorithm","location","researcher","organization","university"]}
{"id":"82","dataset":"crossner_ai","split":"test","instance":{"id":"82","prompt_labels":"They(O) introduced(O) their(O) new(O) robot(O) in(O) 1961(O) at(O) a(O) trade(O) show(O) at(O) Chicago(B-location) 's(O) Cow(B-location) Palace(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, researcher, task, conference, university, product, organization, programming language, person, algorithm, country, location, metric and O.\nSentence: They introduced their new robot in 1961 at a trade show at Chicago 's Cow Palace .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["They","introduced","their","new","robot","in","1961","at","a","trade","show","at","Chicago","'s","Cow","Palace","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["field","researcher","task","conference","university","product","organization","programming_language","person","algorithm","country","location","metric"]}
{"id":"83","dataset":"crossner_ai","split":"test","instance":{"id":"83","prompt_labels":"While(O) some(O) chatbot(B-product) applications(O) use(O) extensive(O) word-classification(B-task) processes(O) ,(O) natural(B-field) language(I-field) processing(I-field) processors(O) ,(O) and(O) sophisticated(O) Artificial(B-field) intelligence(I-field) ,(O) others(O) simply(O) scan(O) for(O) general(O) keywords(O) and(O) generate(O) responses(O) using(O) common(O) phrases(O) obtained(O) from(O) an(O) associated(O) library(O) or(O) database(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, person, university, conference, programming language, metric, organization, task, algorithm, country, location, field, product and O.\nSentence: While some chatbot applications use extensive word-classification processes , natural language processing processors , and sophisticated Artificial intelligence , others simply scan for general keywords and generate responses using common phrases obtained from an associated library or database .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["While","some","chatbot","applications","use","extensive","word-classification","processes",",","natural","language","processing","processors",",","and","sophisticated","Artificial","intelligence",",","others","simply","scan","for","general","keywords","and","generate","responses","using","common","phrases","obtained","from","an","associated","library","or","database","."],"labels":["O","O","B-product","O","O","O","B-task","O","O","B-field","I-field","I-field","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","person","university","conference","programming_language","metric","organization","task","algorithm","country","location","field","product"]}
{"id":"84","dataset":"crossner_ai","split":"test","instance":{"id":"84","prompt_labels":"The(O) WaveNet(B-product) model(O) proposed(O) in(O) 2016(O) achieves(O) great(O) performance(O) on(O) speech(O) quality(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, product, conference, researcher, country, university, task, person, algorithm, field, location, metric, organization and O.\nSentence: The WaveNet model proposed in 2016 achieves great performance on speech quality .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","WaveNet","model","proposed","in","2016","achieves","great","performance","on","speech","quality","."],"labels":["O","B-product","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","product","conference","researcher","country","university","task","person","algorithm","field","location","metric","organization"]}
{"id":"85","dataset":"crossner_ai","split":"test","instance":{"id":"85","prompt_labels":"Organizations(O) known(O) to(O) use(O) ALE(B-product) for(O) Emergency(O) management(O) ,(O) disaster(O) relief(O) ,(O) ordinary(O) communication(O) or(O) extraordinary(O) situation(O) response(O) :(O) American(B-organization) Red(I-organization) Cross(I-organization) ,(O) FEMA(B-organization) ,(O) Disaster(B-organization) Medical(I-organization) Assistance(I-organization) Team(I-organization) s(O) ,(O) NATO(B-organization) ,(O) Federal(B-organization) Bureau(I-organization) of(I-organization) Investigation(I-organization) ,(O) United(B-organization) Nations(I-organization) ,(O) AT(B-organization) &(I-organization) T(I-organization) ,(O) Civil(B-organization) Air(I-organization) Patrol(I-organization) ,(O) ((O) ARES(B-organization) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, metric, algorithm, university, programming language, conference, product, person, field, task, location, country, researcher and O.\nSentence: Organizations known to use ALE for Emergency management , disaster relief , ordinary communication or extraordinary situation response : American Red Cross , FEMA , Disaster Medical Assistance Team s , NATO , Federal Bureau of Investigation , United Nations , AT & T , Civil Air Patrol , ( ARES ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Organizations","known","to","use","ALE","for","Emergency","management",",","disaster","relief",",","ordinary","communication","or","extraordinary","situation","response",":","American","Red","Cross",",","FEMA",",","Disaster","Medical","Assistance","Team","s",",","NATO",",","Federal","Bureau","of","Investigation",",","United","Nations",",","AT","&","T",",","Civil","Air","Patrol",",","(","ARES",")","."],"labels":["O","O","O","O","B-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","O","B-organization","O","O"],"target_index":null,"target_label":null},"label_list":["organization","metric","algorithm","university","programming_language","conference","product","person","field","task","location","country","researcher"]}
{"id":"86","dataset":"crossner_ai","split":"test","instance":{"id":"86","prompt_labels":"Here(O) ,(O) the(O) Kronecker(B-algorithm) delta(I-algorithm) is(O) used(O) for(O) simplicity(O) ((O) cf.(O) the(O) derivative(O) of(O) a(O) sigmoid(B-algorithm) function(I-algorithm) ,(O) being(O) expressed(O) via(O) the(O) function(O) itself(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, product, conference, task, person, country, university, field, organization, researcher, algorithm, programming language, location and O.\nSentence: Here , the Kronecker delta is used for simplicity ( cf. the derivative of a sigmoid function , being expressed via the function itself ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Here",",","the","Kronecker","delta","is","used","for","simplicity","(","cf.","the","derivative","of","a","sigmoid","function",",","being","expressed","via","the","function","itself",")","."],"labels":["O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","product","conference","task","person","country","university","field","organization","researcher","algorithm","programming_language","location"]}
{"id":"88","dataset":"crossner_ai","split":"test","instance":{"id":"88","prompt_labels":"WordNet(B-product) ,(O) a(O) freely(O) available(O) database(O) originally(O) designed(O) as(O) a(O) semantic(O) network(O) based(O) on(O) psycholinguistic(O) principles(O) ,(O) was(O) expanded(O) by(O) addition(O) of(O) definitions(O) and(O) is(O) now(O) also(O) viewed(O) as(O) a(O) dictionary(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, organization, university, country, product, person, field, programming language, metric, algorithm, task, conference, location and O.\nSentence: WordNet , a freely available database originally designed as a semantic network based on psycholinguistic principles , was expanded by addition of definitions and is now also viewed as a dictionary .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["WordNet",",","a","freely","available","database","originally","designed","as","a","semantic","network","based","on","psycholinguistic","principles",",","was","expanded","by","addition","of","definitions","and","is","now","also","viewed","as","a","dictionary","."],"labels":["B-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","organization","university","country","product","person","field","programming_language","metric","algorithm","task","conference","location"]}
{"id":"89","dataset":"crossner_ai","split":"test","instance":{"id":"89","prompt_labels":"Advances(O) in(O) the(O) field(O) of(O) computational(B-field) imaging(I-field) research(O) is(O) presented(O) in(O) several(O) venues(O) including(O) publications(O) of(O) SIGGRAPH(B-conference) and(O) the(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, task, algorithm, field, product, organization, programming language, conference, metric, country, university, person, location and O.\nSentence: Advances in the field of computational imaging research is presented in several venues including publications of SIGGRAPH and the .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Advances","in","the","field","of","computational","imaging","research","is","presented","in","several","venues","including","publications","of","SIGGRAPH","and","the","."],"labels":["O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","B-conference","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","task","algorithm","field","product","organization","programming_language","conference","metric","country","university","person","location"]}
{"id":"90","dataset":"crossner_ai","split":"test","instance":{"id":"90","prompt_labels":"Classification(B-task) can(O) be(O) thought(O) of(O) as(O) two(O) separate(O) problems(O) -(O) binary(B-task) classification(I-task) and(O) multiclass(B-task) classification(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, location, person, algorithm, organization, product, programming language, country, conference, task, researcher, metric, university and O.\nSentence: Classification can be thought of as two separate problems - binary classification and multiclass classification .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Classification","can","be","thought","of","as","two","separate","problems","-","binary","classification","and","multiclass","classification","."],"labels":["B-task","O","O","O","O","O","O","O","O","O","B-task","I-task","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["field","location","person","algorithm","organization","product","programming_language","country","conference","task","researcher","metric","university"]}
{"id":"92","dataset":"crossner_ai","split":"test","instance":{"id":"92","prompt_labels":"Neuroevolution(O) ,(O) or(O) neuro-evolution(O) ,(O) is(O) a(O) form(O) of(O) artificial(B-field) intelligence(I-field) that(O) uses(O) evolutionary(B-algorithm) algorithm(I-algorithm) s(O) to(O) generate(O) artificial(B-algorithm) neural(I-algorithm) network(I-algorithm) s(O) ((O) ANN(B-algorithm) )(O) ,(O) parameters(O) ,(O) topology(O) and(O) rules.(O) and(O) evolutionary(B-algorithm) robotics(I-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, product, algorithm, location, metric, person, programming language, researcher, university, organization, field, conference, country and O.\nSentence: Neuroevolution , or neuro-evolution , is a form of artificial intelligence that uses evolutionary algorithm s to generate artificial neural network s ( ANN ) , parameters , topology and rules. and evolutionary robotics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Neuroevolution",",","or","neuro-evolution",",","is","a","form","of","artificial","intelligence","that","uses","evolutionary","algorithm","s","to","generate","artificial","neural","network","s","(","ANN",")",",","parameters",",","topology","and","rules.","and","evolutionary","robotics","."],"labels":["O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","B-algorithm","I-algorithm","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","B-algorithm","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["task","product","algorithm","location","metric","person","programming_language","researcher","university","organization","field","conference","country"]}
{"id":"95","dataset":"crossner_ai","split":"test","instance":{"id":"95","prompt_labels":"After(O) boosting(O) ,(O) a(O) classifier(O) constructed(O) from(O) 200(O) features(O) could(O) yield(O) a(O) 95(O) %(O) detection(O) rate(O) under(O) a(O) ^(O) {(O) -5(O) }(O) /(O) math(O) FALSE(B-metric) positive(I-metric) rate(I-metric) .P.(B-researcher) Viola(I-researcher) ,(O) M.(B-researcher) Jones(I-researcher) ,(O) Robust(B-task) Real-time(I-task) Object(I-task) Detection(I-task) ,(O) 2001(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, task, university, person, location, organization, metric, conference, country, programming language, algorithm, product, researcher and O.\nSentence: After boosting , a classifier constructed from 200 features could yield a 95 % detection rate under a ^ { -5 } / math FALSE positive rate .P. Viola , M. Jones , Robust Real-time Object Detection , 2001 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","boosting",",","a","classifier","constructed","from","200","features","could","yield","a","95","%","detection","rate","under","a","^","{","-5","}","/","math","FALSE","positive","rate",".P.","Viola",",","M.","Jones",",","Robust","Real-time","Object","Detection",",","2001","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","B-researcher","I-researcher","O","B-researcher","I-researcher","O","B-task","I-task","I-task","I-task","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","task","university","person","location","organization","metric","conference","country","programming_language","algorithm","product","researcher"]}
{"id":"96","dataset":"crossner_ai","split":"test","instance":{"id":"96","prompt_labels":"The(O) website(O) was(O) originally(O) Perl(B-programming language) -based(O) ,(O) but(O) IMDb(B-organization) no(O) longer(O) discloses(O) what(O) software(O) it(O) uses(O) for(O) reasons(O) of(O) security(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, location, country, person, researcher, task, metric, field, organization, programming language, conference, algorithm, university and O.\nSentence: The website was originally Perl -based , but IMDb no longer discloses what software it uses for reasons of security .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","website","was","originally","Perl","-based",",","but","IMDb","no","longer","discloses","what","software","it","uses","for","reasons","of","security","."],"labels":["O","O","O","O","B-programming language","O","O","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["product","location","country","person","researcher","task","metric","field","organization","programming_language","conference","algorithm","university"]}
{"id":"97","dataset":"crossner_ai","split":"test","instance":{"id":"97","prompt_labels":"The(O) start-up(O) was(O) founded(O) by(O) Demis(B-researcher) Hassabis(I-researcher) ,(O) Shane(B-researcher) Legg(I-researcher) and(O) Mustafa(B-person) Suleyman(I-person) in(O) 2010(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, university, conference, organization, programming language, person, algorithm, field, product, researcher, task, location, metric and O.\nSentence: The start-up was founded by Demis Hassabis , Shane Legg and Mustafa Suleyman in 2010 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","start-up","was","founded","by","Demis","Hassabis",",","Shane","Legg","and","Mustafa","Suleyman","in","2010","."],"labels":["O","O","O","O","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","B-person","I-person","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","university","conference","organization","programming_language","person","algorithm","field","product","researcher","task","location","metric"]}
{"id":"98","dataset":"crossner_ai","split":"test","instance":{"id":"98","prompt_labels":"Two(O) very(O) commonly(O) used(O) loss(O) functions(O) are(O) the(O) mean(B-metric) squared(I-metric) error(I-metric) ,(O) mathL(O) ((O) a(O) )(O) =(O) a(O) ^(O) 2(O) /(O) math(O) ,(O) and(O) the(O) absolute(B-metric) loss(I-metric) ,(O) mathL(O) ((O) a(O) )(O) =(O) |(O) a(O) |(O) /(O) math(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, metric, conference, task, researcher, location, field, programming language, organization, country, product, person, university and O.\nSentence: Two very commonly used loss functions are the mean squared error , mathL ( a ) = a ^ 2 / math , and the absolute loss , mathL ( a ) = | a | / math .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Two","very","commonly","used","loss","functions","are","the","mean","squared","error",",","mathL","(","a",")","=","a","^","2","/","math",",","and","the","absolute","loss",",","mathL","(","a",")","=","|","a","|","/","math","."],"labels":["O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["algorithm","metric","conference","task","researcher","location","field","programming_language","organization","country","product","person","university"]}
{"id":"99","dataset":"crossner_ai","split":"test","instance":{"id":"99","prompt_labels":"The(O) soft-margin(O) support(B-algorithm) vector(I-algorithm) machine(I-algorithm) described(O) above(O) is(O) an(O) example(O) of(O) an(O) empirical(B-algorithm) risk(I-algorithm) minimization(I-algorithm) ((O) ERM(B-algorithm) )(O) for(O) the(O) hinge(B-metric) loss(I-metric) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, person, location, country, programming language, field, metric, university, conference, researcher, algorithm, organization, product and O.\nSentence: The soft-margin support vector machine described above is an example of an empirical risk minimization ( ERM ) for the hinge loss .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","soft-margin","support","vector","machine","described","above","is","an","example","of","an","empirical","risk","minimization","(","ERM",")","for","the","hinge","loss","."],"labels":["O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","O","O","O","B-metric","I-metric","O"],"target_index":null,"target_label":null},"label_list":["task","person","location","country","programming_language","field","metric","university","conference","researcher","algorithm","organization","product"]}
{"id":"103","dataset":"crossner_ai","split":"test","instance":{"id":"103","prompt_labels":"trained(O) by(O) maximum(B-algorithm) likelihood(I-algorithm) estimation(I-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, programming language, conference, organization, task, researcher, location, product, metric, algorithm, field, university, country and O.\nSentence: trained by maximum likelihood estimation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["trained","by","maximum","likelihood","estimation","."],"labels":["O","O","B-algorithm","I-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["person","programming_language","conference","organization","task","researcher","location","product","metric","algorithm","field","university","country"]}
{"id":"104","dataset":"crossner_ai","split":"test","instance":{"id":"104","prompt_labels":",(O) Ltd.(O) in(O) Thailand(B-country) ;(O) Komatsu(B-organization) ((I-organization) Shanghai(I-organization) )(I-organization) Ltd.(I-organization) in(O) 1996(O) in(O) Shanghai(B-location) ,(O) China(B-country) ;(O) Industrial(B-organization) Power(I-organization) Alliance(I-organization) Ltd.(I-organization) in(O) Japan(B-country) ,(O) a(O) joint(O) venture(O) with(O) Cummins(B-organization) ,(O) in(O) 1998(O) ;(O) L(B-organization) &(I-organization) T-Komatsu(I-organization) Limited(I-organization) in(O) India(B-country) in(O) 1998(O) ((O) shares(O) sold(O) in(O) 2013(O) )(O) ;(O) and(O) Komatsu(B-organization) Brasil(I-organization) International(I-organization) Ltda.(I-organization) in(O) Brazil(B-country) in(O) 1998(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, university, country, organization, person, product, task, field, researcher, location, conference, metric, programming language and O.\nSentence: , Ltd. in Thailand ; Komatsu ( Shanghai ) Ltd. in 1996 in Shanghai , China ; Industrial Power Alliance Ltd. in Japan , a joint venture with Cummins , in 1998 ; L & T-Komatsu Limited in India in 1998 ( shares sold in 2013 ) ; and Komatsu Brasil International Ltda. in Brazil in 1998 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":[",","Ltd.","in","Thailand",";","Komatsu","(","Shanghai",")","Ltd.","in","1996","in","Shanghai",",","China",";","Industrial","Power","Alliance","Ltd.","in","Japan",",","a","joint","venture","with","Cummins",",","in","1998",";","L","&","T-Komatsu","Limited","in","India","in","1998","(","shares","sold","in","2013",")",";","and","Komatsu","Brasil","International","Ltda.","in","Brazil","in","1998","."],"labels":["O","O","O","B-country","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-location","O","B-country","O","B-organization","I-organization","I-organization","I-organization","O","B-country","O","O","O","O","O","B-organization","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-country","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-country","O","O","O"],"target_index":null,"target_label":null},"label_list":["algorithm","university","country","organization","person","product","task","field","researcher","location","conference","metric","programming_language"]}
{"id":"105","dataset":"crossner_ai","split":"test","instance":{"id":"105","prompt_labels":"dgp(B-organization) also(O) occasionally(O) hosts(O) artists(O) in(O) residence(O) ((O) e.g.(O) ,(O) Oscar(O) -winner(O) Chris(B-person) Landreth(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, field, task, product, university, researcher, programming language, location, conference, country, organization, person, metric and O.\nSentence: dgp also occasionally hosts artists in residence ( e.g. , Oscar -winner Chris Landreth .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["dgp","also","occasionally","hosts","artists","in","residence","(","e.g.",",","Oscar","-winner","Chris","Landreth","."],"labels":["B-organization","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["algorithm","field","task","product","university","researcher","programming_language","location","conference","country","organization","person","metric"]}
{"id":"107","dataset":"crossner_ai","split":"test","instance":{"id":"107","prompt_labels":"By(O) the(O) early(O) 2000s(O) ,(O) the(O) dominant(O) speech(B-field) processing(I-field) strategy(O) started(O) to(O) shift(O) away(O) from(O) Hidden(B-algorithm) Markov(I-algorithm) model(I-algorithm) towards(O) more(O) modern(O) neural(B-algorithm) networks(I-algorithm) and(O) deep(B-field) learning(I-field) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, person, researcher, algorithm, conference, product, programming language, field, location, task, metric, university and O.\nSentence: By the early 2000s , the dominant speech processing strategy started to shift away from Hidden Markov model towards more modern neural networks and deep learning .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["By","the","early","2000s",",","the","dominant","speech","processing","strategy","started","to","shift","away","from","Hidden","Markov","model","towards","more","modern","neural","networks","and","deep","learning","."],"labels":["O","O","O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","B-algorithm","I-algorithm","O","B-field","I-field","O"],"target_index":null,"target_label":null},"label_list":["country","organization","person","researcher","algorithm","conference","product","programming_language","field","location","task","metric","university"]}
{"id":"108","dataset":"crossner_ai","split":"test","instance":{"id":"108","prompt_labels":"Another(O) equivalent(O) expression(O) ,(O) in(O) the(O) case(O) of(O) a(O) binary(B-metric) target(I-metric) rate(I-metric) ,(O) is(O) that(O) the(O) TRUE(B-metric) positive(I-metric) rate(I-metric) and(O) the(O) FALSE(B-metric) positive(I-metric) rate(I-metric) are(O) equal(O) ((O) and(O) therefore(O) the(O) FALSE(B-metric) negative(I-metric) rate(I-metric) and(O) the(O) TRUE(B-metric) negative(I-metric) rate(I-metric) are(O) equal(O) )(O) for(O) every(O) value(O) of(O) the(O) sensitive(O) characteristics(O) :(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, product, university, algorithm, programming language, metric, field, conference, organization, task, researcher, country and O.\nSentence: Another equivalent expression , in the case of a binary target rate , is that the TRUE positive rate and the FALSE positive rate are equal ( and therefore the FALSE negative rate and the TRUE negative rate are equal ) for every value of the sensitive characteristics :","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Another","equivalent","expression",",","in","the","case","of","a","binary","target","rate",",","is","that","the","TRUE","positive","rate","and","the","FALSE","positive","rate","are","equal","(","and","therefore","the","FALSE","negative","rate","and","the","TRUE","negative","rate","are","equal",")","for","every","value","of","the","sensitive","characteristics",":"],"labels":["O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O","O","O","B-metric","I-metric","I-metric","O","O","B-metric","I-metric","I-metric","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O","B-metric","I-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","location","product","university","algorithm","programming_language","metric","field","conference","organization","task","researcher","country"]}
{"id":"109","dataset":"crossner_ai","split":"test","instance":{"id":"109","prompt_labels":"The(O) MATLAB(B-product) function(O) ,(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, field, country, person, university, task, metric, algorithm, programming language, conference, organization, researcher, location and O.\nSentence: The MATLAB function ,","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","MATLAB","function",","],"labels":["O","B-product","O","O"],"target_index":null,"target_label":null},"label_list":["product","field","country","person","university","task","metric","algorithm","programming_language","conference","organization","researcher","location"]}
{"id":"110","dataset":"crossner_ai","split":"test","instance":{"id":"110","prompt_labels":"An(O) articulated(B-product) robot(I-product) is(O) a(O) robot(O) with(O) rotary(O) joint(O) s(O) ((O) e.g.(O) a(O) legged(O) robot(O) or(O) an(O) industrial(B-product) robot(I-product) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, field, university, researcher, product, metric, organization, conference, task, person, algorithm, programming language and O.\nSentence: An articulated robot is a robot with rotary joint s ( e.g. a legged robot or an industrial robot ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["An","articulated","robot","is","a","robot","with","rotary","joint","s","(","e.g.","a","legged","robot","or","an","industrial","robot",")","."],"labels":["O","B-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","I-product","O","O"],"target_index":null,"target_label":null},"label_list":["country","location","field","university","researcher","product","metric","organization","conference","task","person","algorithm","programming_language"]}
{"id":"113","dataset":"crossner_ai","split":"test","instance":{"id":"113","prompt_labels":"James(B-researcher) S.(I-researcher) Albus(I-researcher) of(O) the(O) National(B-organization) Institute(I-organization) of(I-organization) Standards(I-organization) and(I-organization) Technology(I-organization) ((O) NIST(B-organization) )(O) developed(O) the(O) Robocrane(B-product) ,(O) where(O) the(O) platform(O) hangs(O) from(O) six(O) cables(O) instead(O) of(O) being(O) supported(O) by(O) six(O) jacks(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, metric, university, task, country, field, product, location, programming language, person, researcher, algorithm, conference and O.\nSentence: James S. Albus of the National Institute of Standards and Technology ( NIST ) developed the Robocrane , where the platform hangs from six cables instead of being supported by six jacks .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["James","S.","Albus","of","the","National","Institute","of","Standards","and","Technology","(","NIST",")","developed","the","Robocrane",",","where","the","platform","hangs","from","six","cables","instead","of","being","supported","by","six","jacks","."],"labels":["B-researcher","I-researcher","I-researcher","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","B-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","metric","university","task","country","field","product","location","programming_language","person","researcher","algorithm","conference"]}
{"id":"114","dataset":"crossner_ai","split":"test","instance":{"id":"114","prompt_labels":"Another(O) class(O) of(O) direct(O) search(O) algorithms(O) are(O) the(O) various(O) evolutionary(B-algorithm) algorithm(I-algorithm) s(O) ,(O) e.g.(O) genetic(B-algorithm) algorithm(I-algorithm) s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, conference, person, algorithm, field, researcher, university, product, location, programming language, organization, metric, country and O.\nSentence: Another class of direct search algorithms are the various evolutionary algorithm s , e.g. genetic algorithm s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Another","class","of","direct","search","algorithms","are","the","various","evolutionary","algorithm","s",",","e.g.","genetic","algorithm","s","."],"labels":["O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","B-algorithm","I-algorithm","O","O"],"target_index":null,"target_label":null},"label_list":["task","conference","person","algorithm","field","researcher","university","product","location","programming_language","organization","metric","country"]}
{"id":"115","dataset":"crossner_ai","split":"test","instance":{"id":"115","prompt_labels":"KUKA(B-organization) is(O) a(O) German(O) manufacturer(O) of(O) industrial(B-product) robot(I-product) s(O) and(O) solution(O) s(O) for(O) factory(O) automation(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, person, field, conference, algorithm, location, researcher, organization, task, programming language, university, product, country and O.\nSentence: KUKA is a German manufacturer of industrial robot s and solution s for factory automation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["KUKA","is","a","German","manufacturer","of","industrial","robot","s","and","solution","s","for","factory","automation","."],"labels":["B-organization","O","O","O","O","O","B-product","I-product","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","person","field","conference","algorithm","location","researcher","organization","task","programming_language","university","product","country"]}
{"id":"117","dataset":"crossner_ai","split":"test","instance":{"id":"117","prompt_labels":"The(O) trial(O) of(O) MICR(O) E13B(O) font(O) was(O) shown(O) to(O) the(O) American(B-organization) Bankers(I-organization) Association(I-organization) ((O) ABA(B-organization) )(O) in(O) July(O) 1956(O) ,(O) which(O) adopted(O) it(O) in(O) 1958(O) as(O) the(O) MICR(O) standard(O) for(O) negotiable(O) document(O) s(O) in(O) the(O) United(B-country) States(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, conference, organization, algorithm, location, country, researcher, university, task, person, field, metric, programming language and O.\nSentence: The trial of MICR E13B font was shown to the American Bankers Association ( ABA ) in July 1956 , which adopted it in 1958 as the MICR standard for negotiable document s in the United States .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","trial","of","MICR","E13B","font","was","shown","to","the","American","Bankers","Association","(","ABA",")","in","July","1956",",","which","adopted","it","in","1958","as","the","MICR","standard","for","negotiable","document","s","in","the","United","States","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["product","conference","organization","algorithm","location","country","researcher","university","task","person","field","metric","programming_language"]}
{"id":"118","dataset":"crossner_ai","split":"test","instance":{"id":"118","prompt_labels":"Local(O) search(O) algorithms(O) are(O) widely(O) applied(O) to(O) numerous(O) hard(O) computational(O) problems(O) ,(O) including(O) problems(O) from(O) computer(B-field) science(I-field) ((O) particularly(O) artificial(B-field) intelligence(I-field) )(O) ,(O) mathematics(B-field) ,(O) operations(B-field) research(I-field) ,(O) engineering(B-field) ,(O) and(O) bioinformatics(B-field) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, conference, country, product, task, algorithm, programming language, location, person, field, metric, researcher, organization and O.\nSentence: Local search algorithms are widely applied to numerous hard computational problems , including problems from computer science ( particularly artificial intelligence ) , mathematics , operations research , engineering , and bioinformatics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Local","search","algorithms","are","widely","applied","to","numerous","hard","computational","problems",",","including","problems","from","computer","science","(","particularly","artificial","intelligence",")",",","mathematics",",","operations","research",",","engineering",",","and","bioinformatics","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","B-field","I-field","O","O","B-field","O","B-field","I-field","O","B-field","O","O","B-field","O"],"target_index":null,"target_label":null},"label_list":["university","conference","country","product","task","algorithm","programming_language","location","person","field","metric","researcher","organization"]}
{"id":"119","dataset":"crossner_ai","split":"test","instance":{"id":"119","prompt_labels":"Gerd(B-researcher) Gigerenzer(I-researcher) ((O) born(O) September(O) 3(O) ,(O) 1947(O) ,(O) Wallersdorf(B-location) ,(O) Germany(B-country) )(O) is(O) a(O) Germany(B-country) psychologist(O) who(O) has(O) studied(O) the(O) use(O) of(O) bounded(B-algorithm) rationality(I-algorithm) and(O) heuristic(B-algorithm) s(O) in(O) decision(B-task) making(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, researcher, field, metric, product, country, programming language, task, location, algorithm, organization, conference, university and O.\nSentence: Gerd Gigerenzer ( born September 3 , 1947 , Wallersdorf , Germany ) is a Germany psychologist who has studied the use of bounded rationality and heuristic s in decision making .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Gerd","Gigerenzer","(","born","September","3",",","1947",",","Wallersdorf",",","Germany",")","is","a","Germany","psychologist","who","has","studied","the","use","of","bounded","rationality","and","heuristic","s","in","decision","making","."],"labels":["B-researcher","I-researcher","O","O","O","O","O","O","O","B-location","O","B-country","O","O","O","B-country","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","B-algorithm","O","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["person","researcher","field","metric","product","country","programming_language","task","location","algorithm","organization","conference","university"]}
{"id":"121","dataset":"crossner_ai","split":"test","instance":{"id":"121","prompt_labels":"But(O) even(O) an(O) official(O) language(O) with(O) a(O) regulating(O) academy(O) ,(O) such(O) as(O) Standard(O) French(O) with(O) the(O) Acadmie(B-organization) franaise(I-organization) ,(O) is(O) classified(O) as(O) a(O) natural(O) language(O) ((O) for(O) example(O) ,(O) in(O) the(O) field(O) of(O) natural(B-field) language(I-field) processing(I-field) )(O) ,(O) as(O) its(O) prescriptive(O) points(O) do(O) not(O) make(O) it(O) either(O) constructed(O) enough(O) to(O) be(O) classified(O) as(O) a(O) constructed(O) language(O) or(O) controlled(O) enough(O) to(O) be(O) classified(O) as(O) a(O) controlled(O) natural(O) language(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, person, product, university, metric, country, field, conference, task, location, researcher, programming language, organization and O.\nSentence: But even an official language with a regulating academy , such as Standard French with the Acadmie franaise , is classified as a natural language ( for example , in the field of natural language processing ) , as its prescriptive points do not make it either constructed enough to be classified as a constructed language or controlled enough to be classified as a controlled natural language .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["But","even","an","official","language","with","a","regulating","academy",",","such","as","Standard","French","with","the","Acadmie","franaise",",","is","classified","as","a","natural","language","(","for","example",",","in","the","field","of","natural","language","processing",")",",","as","its","prescriptive","points","do","not","make","it","either","constructed","enough","to","be","classified","as","a","constructed","language","or","controlled","enough","to","be","classified","as","a","controlled","natural","language","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["algorithm","person","product","university","metric","country","field","conference","task","location","researcher","programming_language","organization"]}
{"id":"122","dataset":"crossner_ai","split":"test","instance":{"id":"122","prompt_labels":"There(O) are(O) a(O) number(O) of(O) other(O) metrics(O) ,(O) most(O) simply(O) the(O) accuracy(B-metric) or(O) Fraction(B-metric) Correct(I-metric) ((O) FC(B-metric) )(O) ,(O) which(O) measures(O) the(O) fraction(O) of(O) all(O) instances(O) that(O) are(O) correctly(O) categorized(O) ;(O) the(O) complement(O) is(O) the(O) Fraction(B-metric) Incorrect(I-metric) ((O) FiC(B-metric) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, product, location, task, programming language, conference, researcher, organization, field, algorithm, person, university, metric and O.\nSentence: There are a number of other metrics , most simply the accuracy or Fraction Correct ( FC ) , which measures the fraction of all instances that are correctly categorized ; the complement is the Fraction Incorrect ( FiC ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["There","are","a","number","of","other","metrics",",","most","simply","the","accuracy","or","Fraction","Correct","(","FC",")",",","which","measures","the","fraction","of","all","instances","that","are","correctly","categorized",";","the","complement","is","the","Fraction","Incorrect","(","FiC",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-metric","O","B-metric","I-metric","O","B-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","O","B-metric","O","O"],"target_index":null,"target_label":null},"label_list":["country","product","location","task","programming_language","conference","researcher","organization","field","algorithm","person","university","metric"]}
{"id":"123","dataset":"crossner_ai","split":"test","instance":{"id":"123","prompt_labels":"Cardie(B-researcher) became(O) a(O) Fellow(O) of(O) the(O) Association(B-conference) for(I-conference) Computational(I-conference) Linguistics(I-conference) in(O) 2016(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, product, programming language, country, organization, metric, algorithm, location, conference, university, person, task, researcher and O.\nSentence: Cardie became a Fellow of the Association for Computational Linguistics in 2016 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Cardie","became","a","Fellow","of","the","Association","for","Computational","Linguistics","in","2016","."],"labels":["B-researcher","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","product","programming_language","country","organization","metric","algorithm","location","conference","university","person","task","researcher"]}
{"id":"124","dataset":"crossner_ai","split":"test","instance":{"id":"124","prompt_labels":"Learning(O) the(O) parameters(O) math(O) \\(O) theta(O) /(O) math(O) is(O) usually(O) done(O) by(O) maximum(B-algorithm) likelihood(I-algorithm) learning(I-algorithm) for(O) mathp(O) ((O) Y(O) _(O) i(O) |(O) X(O) _(O) i(O) ;(O) \\(O) theta(O) )(O) /(O) math(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, product, researcher, programming language, location, organization, algorithm, conference, field, person, metric, task, country and O.\nSentence: Learning the parameters math \\ theta / math is usually done by maximum likelihood learning for mathp ( Y _ i | X _ i ; \\ theta ) / math .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Learning","the","parameters","math","\\","theta","/","math","is","usually","done","by","maximum","likelihood","learning","for","mathp","(","Y","_","i","|","X","_","i",";","\\","theta",")","/","math","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","product","researcher","programming_language","location","organization","algorithm","conference","field","person","metric","task","country"]}
{"id":"125","dataset":"crossner_ai","split":"test","instance":{"id":"125","prompt_labels":"Cluster(B-task) analysis(I-task) ,(O) and(O) Non-negative(B-algorithm) matrix(I-algorithm) factorization(I-algorithm) for(O) descriptive(B-task) mining(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, conference, university, task, algorithm, person, programming language, metric, location, researcher, product, country, organization and O.\nSentence: Cluster analysis , and Non-negative matrix factorization for descriptive mining .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Cluster","analysis",",","and","Non-negative","matrix","factorization","for","descriptive","mining","."],"labels":["B-task","I-task","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["field","conference","university","task","algorithm","person","programming_language","metric","location","researcher","product","country","organization"]}
{"id":"126","dataset":"crossner_ai","split":"test","instance":{"id":"126","prompt_labels":"In(O) computer(B-field) science(I-field) and(O) the(O) information(B-field) technology(I-field) that(O) it(O) enables(O) ,(O) it(O) has(O) been(O) a(O) long-term(O) challenge(O) to(O) the(O) ability(O) in(O) computers(O) to(O) do(O) natural(B-field) language(I-field) processing(I-field) and(O) machine(B-field) learning(I-field) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, person, field, country, organization, researcher, algorithm, university, conference, product, task, programming language, location and O.\nSentence: In computer science and the information technology that it enables , it has been a long-term challenge to the ability in computers to do natural language processing and machine learning .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","computer","science","and","the","information","technology","that","it","enables",",","it","has","been","a","long-term","challenge","to","the","ability","in","computers","to","do","natural","language","processing","and","machine","learning","."],"labels":["O","B-field","I-field","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","I-field","O","B-field","I-field","O"],"target_index":null,"target_label":null},"label_list":["metric","person","field","country","organization","researcher","algorithm","university","conference","product","task","programming_language","location"]}
{"id":"127","dataset":"crossner_ai","split":"test","instance":{"id":"127","prompt_labels":"((O) Code(O) for(O) Gabor(B-algorithm) feature(I-algorithm) extraction(I-algorithm) from(O) images(O) in(O) MATLAB(B-product) can(O) be(O) found(O) at(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, university, country, algorithm, metric, field, task, conference, product, organization, location, person, researcher and O.\nSentence: ( Code for Gabor feature extraction from images in MATLAB can be found at","prediction_output":null,"prediction_outputs":null,"group":null,"words":["(","Code","for","Gabor","feature","extraction","from","images","in","MATLAB","can","be","found","at"],"labels":["O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","B-product","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","university","country","algorithm","metric","field","task","conference","product","organization","location","person","researcher"]}
{"id":"130","dataset":"crossner_ai","split":"test","instance":{"id":"130","prompt_labels":"The(O) construction(O) of(O) a(O) rich(O) lexicon(O) with(O) a(O) suitable(O) ontology(O) requires(O) significant(O) effort(O) ,(O) e.g.(O) ,(O) Wordnet(B-product) lexicon(O) required(O) many(O) person-years(O) of(O) effort.(O) G.(B-researcher) A.(I-researcher) Miller(I-researcher) ,(O) R.(B-researcher) Beckwith(I-researcher) ,(O) C.(B-researcher) D.(I-researcher) Fellbaum(I-researcher) ,(O) D.(B-researcher) Gross(I-researcher) ,(O) K.(B-researcher) Miller(I-researcher) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, country, algorithm, product, person, organization, location, task, programming language, conference, researcher, university, field and O.\nSentence: The construction of a rich lexicon with a suitable ontology requires significant effort , e.g. , Wordnet lexicon required many person-years of effort. G. A. Miller , R. Beckwith , C. D. Fellbaum , D. Gross , K. Miller .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","construction","of","a","rich","lexicon","with","a","suitable","ontology","requires","significant","effort",",","e.g.",",","Wordnet","lexicon","required","many","person-years","of","effort.","G.","A.","Miller",",","R.","Beckwith",",","C.","D.","Fellbaum",",","D.","Gross",",","K.","Miller","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","O","O","O","O","O","O","B-researcher","I-researcher","I-researcher","O","B-researcher","I-researcher","O","B-researcher","I-researcher","I-researcher","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O"],"target_index":null,"target_label":null},"label_list":["metric","country","algorithm","product","person","organization","location","task","programming_language","conference","researcher","university","field"]}
{"id":"131","dataset":"crossner_ai","split":"test","instance":{"id":"131","prompt_labels":"Kawasaki(B-organization) 's(O) portfolio(O) also(O) includes(O) retractable(O) roofs(O) ,(O) floors(O) and(O) other(O) giant(O) structures(O) ,(O) the(O) Sapporo(B-location) Dome(I-location) '(O) retractable(O) surface(O) is(O) one(O) example(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, organization, product, algorithm, field, person, programming language, metric, university, location, task, country, researcher and O.\nSentence: Kawasaki 's portfolio also includes retractable roofs , floors and other giant structures , the Sapporo Dome ' retractable surface is one example .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Kawasaki","'s","portfolio","also","includes","retractable","roofs",",","floors","and","other","giant","structures",",","the","Sapporo","Dome","'","retractable","surface","is","one","example","."],"labels":["B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["conference","organization","product","algorithm","field","person","programming_language","metric","university","location","task","country","researcher"]}
{"id":"134","dataset":"crossner_ai","split":"test","instance":{"id":"134","prompt_labels":"2004(O) -(O) The(O) first(O) Cobot(B-product) KUKA(I-product) LBR(I-product) 3(I-product) is(O) released(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, metric, organization, field, algorithm, task, person, product, university, country, programming language, location, researcher and O.\nSentence: 2004 - The first Cobot KUKA LBR 3 is released .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["2004","-","The","first","Cobot","KUKA","LBR","3","is","released","."],"labels":["O","O","O","O","B-product","I-product","I-product","I-product","O","O","O"],"target_index":null,"target_label":null},"label_list":["conference","metric","organization","field","algorithm","task","person","product","university","country","programming_language","location","researcher"]}
{"id":"137","dataset":"crossner_ai","split":"test","instance":{"id":"137","prompt_labels":"For(O) example(O) ,(O) speech(B-task) synthesis(I-task) ,(O) combined(O) with(O) speech(B-task) recognition(I-task) ,(O) allows(O) for(O) interaction(O) with(O) mobile(O) devices(O) via(O) language(B-field) processing(I-field) interfaces(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, programming language, location, task, university, person, algorithm, metric, country, conference, researcher, field, product and O.\nSentence: For example , speech synthesis , combined with speech recognition , allows for interaction with mobile devices via language processing interfaces .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","example",",","speech","synthesis",",","combined","with","speech","recognition",",","allows","for","interaction","with","mobile","devices","via","language","processing","interfaces","."],"labels":["O","O","O","B-task","I-task","O","O","O","B-task","I-task","O","O","O","O","O","O","O","O","B-field","I-field","O","O"],"target_index":null,"target_label":null},"label_list":["organization","programming_language","location","task","university","person","algorithm","metric","country","conference","researcher","field","product"]}
{"id":"138","dataset":"crossner_ai","split":"test","instance":{"id":"138","prompt_labels":"Phidgets(B-product) can(O) be(O) programmed(O) using(O) a(O) variety(O) of(O) software(O) and(O) programming(O) languages(O) ,(O) ranging(O) from(O) Java(B-programming language) to(O) Microsoft(B-product) Excel(I-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, product, university, location, country, task, researcher, field, person, programming language, conference, metric, organization and O.\nSentence: Phidgets can be programmed using a variety of software and programming languages , ranging from Java to Microsoft Excel .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Phidgets","can","be","programmed","using","a","variety","of","software","and","programming","languages",",","ranging","from","Java","to","Microsoft","Excel","."],"labels":["B-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-programming language","O","B-product","I-product","O"],"target_index":null,"target_label":null},"label_list":["algorithm","product","university","location","country","task","researcher","field","person","programming_language","conference","metric","organization"]}
{"id":"140","dataset":"crossner_ai","split":"test","instance":{"id":"140","prompt_labels":"The(O) Israeli(O) poet(O) David(B-person) Avidan(I-person) ,(O) who(O) was(O) fascinated(O) with(O) future(O) technologies(O) and(O) their(O) relation(O) to(O) art(O) ,(O) desired(O) to(O) explore(O) the(O) use(O) of(O) computers(O) for(O) writing(O) literature(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, university, conference, field, task, country, location, metric, organization, programming language, person, algorithm, researcher and O.\nSentence: The Israeli poet David Avidan , who was fascinated with future technologies and their relation to art , desired to explore the use of computers for writing literature .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Israeli","poet","David","Avidan",",","who","was","fascinated","with","future","technologies","and","their","relation","to","art",",","desired","to","explore","the","use","of","computers","for","writing","literature","."],"labels":["O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["product","university","conference","field","task","country","location","metric","organization","programming_language","person","algorithm","researcher"]}
{"id":"141","dataset":"crossner_ai","split":"test","instance":{"id":"141","prompt_labels":"As(O) part(O) of(O) the(O) GATEway(O) Project(O) in(O) 2017(O) ,(O) Oxbotica(B-organization) trialled(O) seven(O) autonomous(O) shuttle(O) buses(O) in(O) Greenwich(B-location) ,(O) navigating(O) a(O) two-mile(O) riverside(O) path(O) near(O) London(B-location) 's(O) The(B-location) O2(I-location) Arena(I-location) on(O) a(O) route(O) also(O) used(O) by(O) pedestrians(O) and(O) cyclists(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, organization, field, university, task, algorithm, programming language, location, country, metric, product, conference, person and O.\nSentence: As part of the GATEway Project in 2017 , Oxbotica trialled seven autonomous shuttle buses in Greenwich , navigating a two-mile riverside path near London 's The O2 Arena on a route also used by pedestrians and cyclists .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["As","part","of","the","GATEway","Project","in","2017",",","Oxbotica","trialled","seven","autonomous","shuttle","buses","in","Greenwich",",","navigating","a","two-mile","riverside","path","near","London","'s","The","O2","Arena","on","a","route","also","used","by","pedestrians","and","cyclists","."],"labels":["O","O","O","O","O","O","O","O","O","B-organization","O","O","O","O","O","O","B-location","O","O","O","O","O","O","O","B-location","O","B-location","I-location","I-location","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","organization","field","university","task","algorithm","programming_language","location","country","metric","product","conference","person"]}
{"id":"142","dataset":"crossner_ai","split":"test","instance":{"id":"142","prompt_labels":"An(O) unrelated(O) but(O) commonly(O) used(O) combination(O) of(O) basic(O) statistics(O) from(O) information(B-task) retrieval(I-task) is(O) the(O) F-score(B-metric) ,(O) being(O) a(O) ((O) possibly(O) weighted(O) )(O) harmonic(O) mean(O) of(O) recall(B-metric) and(O) precision(B-metric) where(O) recall(B-metric) =(O) sensitivity(B-metric) =(O) TRUE(B-metric) positive(I-metric) rate(I-metric) ,(O) but(O) specificity(B-metric) and(O) precision(B-metric) are(O) totally(O) different(O) measures(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, person, country, task, conference, organization, university, field, metric, programming language, location, product, algorithm and O.\nSentence: An unrelated but commonly used combination of basic statistics from information retrieval is the F-score , being a ( possibly weighted ) harmonic mean of recall and precision where recall = sensitivity = TRUE positive rate , but specificity and precision are totally different measures .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["An","unrelated","but","commonly","used","combination","of","basic","statistics","from","information","retrieval","is","the","F-score",",","being","a","(","possibly","weighted",")","harmonic","mean","of","recall","and","precision","where","recall","=","sensitivity","=","TRUE","positive","rate",",","but","specificity","and","precision","are","totally","different","measures","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-task","I-task","O","O","B-metric","O","O","O","O","O","O","O","O","O","O","B-metric","O","B-metric","O","B-metric","O","B-metric","O","B-metric","I-metric","I-metric","O","O","B-metric","O","B-metric","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","person","country","task","conference","organization","university","field","metric","programming_language","location","product","algorithm"]}
{"id":"143","dataset":"crossner_ai","split":"test","instance":{"id":"143","prompt_labels":"Neuromorphic(B-field) engineering(I-field) is(O) an(O) interdisciplinary(O) subject(O) that(O) takes(O) inspiration(O) from(O) biology(B-field) ,(O) physics(B-field) ,(O) mathematics(B-field) ,(O) computer(B-field) science(I-field) ,(O) and(O) electronic(B-field) engineering(I-field) to(O) design(O) artificial(O) neural(O) systems(O) ,(O) such(O) as(O) vision(B-product) systems(I-product) ,(O) head-eye(B-product) systems(I-product) ,(O) auditory(B-product) processors(I-product) ,(O) and(O) autonomous(B-product) robots(I-product) ,(O) whose(O) physical(O) architecture(O) and(O) design(O) principles(O) are(O) based(O) on(O) those(O) of(O) biological(B-product) nervous(I-product) systems(I-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, university, algorithm, conference, field, programming language, person, location, task, researcher, country, organization, product and O.\nSentence: Neuromorphic engineering is an interdisciplinary subject that takes inspiration from biology , physics , mathematics , computer science , and electronic engineering to design artificial neural systems , such as vision systems , head-eye systems , auditory processors , and autonomous robots , whose physical architecture and design principles are based on those of biological nervous systems .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Neuromorphic","engineering","is","an","interdisciplinary","subject","that","takes","inspiration","from","biology",",","physics",",","mathematics",",","computer","science",",","and","electronic","engineering","to","design","artificial","neural","systems",",","such","as","vision","systems",",","head-eye","systems",",","auditory","processors",",","and","autonomous","robots",",","whose","physical","architecture","and","design","principles","are","based","on","those","of","biological","nervous","systems","."],"labels":["B-field","I-field","O","O","O","O","O","O","O","O","B-field","O","B-field","O","B-field","O","B-field","I-field","O","O","B-field","I-field","O","O","O","O","O","O","O","O","B-product","I-product","O","B-product","I-product","O","B-product","I-product","O","O","B-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","B-product","I-product","I-product","O"],"target_index":null,"target_label":null},"label_list":["metric","university","algorithm","conference","field","programming_language","person","location","task","researcher","country","organization","product"]}
{"id":"146","dataset":"crossner_ai","split":"test","instance":{"id":"146","prompt_labels":"The(O) MCC(B-metric) can(O) be(O) calculated(O) directly(O) from(O) the(O) confusion(B-metric) matrix(I-metric) using(O) the(O) formula(O) :(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, person, researcher, algorithm, task, university, metric, conference, field, programming language, product, country and O.\nSentence: The MCC can be calculated directly from the confusion matrix using the formula :","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","MCC","can","be","calculated","directly","from","the","confusion","matrix","using","the","formula",":"],"labels":["O","B-metric","O","O","O","O","O","O","B-metric","I-metric","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","location","person","researcher","algorithm","task","university","metric","conference","field","programming_language","product","country"]}
{"id":"147","dataset":"crossner_ai","split":"test","instance":{"id":"147","prompt_labels":"It(O) was(O) developed(O) by(O) a(O) team(O) at(O) the(O) MIT-IBM(B-organization) Watson(I-organization) AI(I-organization) Lab(I-organization) and(O) first(O) presented(O) at(O) the(O) 2018(B-conference) International(I-conference) Conference(I-conference) on(I-conference) Learning(I-conference) Representations(I-conference) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, algorithm, product, conference, country, person, field, researcher, organization, metric, location, task, university and O.\nSentence: It was developed by a team at the MIT-IBM Watson AI Lab and first presented at the 2018 International Conference on Learning Representations .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","developed","by","a","team","at","the","MIT-IBM","Watson","AI","Lab","and","first","presented","at","the","2018","International","Conference","on","Learning","Representations","."],"labels":["O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O"],"target_index":null,"target_label":null},"label_list":["programming_language","algorithm","product","conference","country","person","field","researcher","organization","metric","location","task","university"]}
{"id":"149","dataset":"crossner_ai","split":"test","instance":{"id":"149","prompt_labels":"The(O) Building(B-conference) Educational(I-conference) Applications(I-conference) workshop(I-conference) ((O) BEA(B-conference) )(O) at(O) NAACL(B-conference) 2013(O) hosted(O) the(O) inaugural(O) NLI(B-task) shared(I-task) task.(I-task) Tetreault(B-researcher) et(O) al(O) ,(O) 2013(O) The(O) competition(O) resulted(O) in(O) 29(O) entries(O) from(O) teams(O) across(O) the(O) globe(O) ,(O) 24(O) of(O) which(O) also(O) published(O) a(O) paper(O) describing(O) their(O) systems(O) and(O) approaches(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, field, person, location, country, task, researcher, programming language, product, algorithm, conference, organization, university and O.\nSentence: The Building Educational Applications workshop ( BEA ) at NAACL 2013 hosted the inaugural NLI shared task. Tetreault et al , 2013 The competition resulted in 29 entries from teams across the globe , 24 of which also published a paper describing their systems and approaches .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Building","Educational","Applications","workshop","(","BEA",")","at","NAACL","2013","hosted","the","inaugural","NLI","shared","task.","Tetreault","et","al",",","2013","The","competition","resulted","in","29","entries","from","teams","across","the","globe",",","24","of","which","also","published","a","paper","describing","their","systems","and","approaches","."],"labels":["O","B-conference","I-conference","I-conference","I-conference","O","B-conference","O","O","B-conference","O","O","O","O","B-task","I-task","I-task","B-researcher","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","field","person","location","country","task","researcher","programming_language","product","algorithm","conference","organization","university"]}
{"id":"150","dataset":"crossner_ai","split":"test","instance":{"id":"150","prompt_labels":"The(O) Viterbi(B-algorithm) algorithm(I-algorithm) is(O) a(O) dynamic(B-algorithm) programming(I-algorithm) algorithm(I-algorithm) for(O) finding(O) the(O) most(O) likely(O) sequence(O) of(O) hidden(O) states(O) called(O) the(O) Viterbi(O) path(O) that(O) results(O) in(O) a(O) sequence(O) of(O) observed(O) events(O) ,(O) especially(O) in(O) the(O) context(O) of(O) Markov(O) information(O) source(O) s(O) and(O) hidden(B-algorithm) Markov(I-algorithm) model(I-algorithm) s(O) ((O) HMM(B-algorithm) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, algorithm, metric, university, country, person, programming language, task, conference, field, location, organization, researcher and O.\nSentence: The Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states called the Viterbi path that results in a sequence of observed events , especially in the context of Markov information source s and hidden Markov model s ( HMM ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Viterbi","algorithm","is","a","dynamic","programming","algorithm","for","finding","the","most","likely","sequence","of","hidden","states","called","the","Viterbi","path","that","results","in","a","sequence","of","observed","events",",","especially","in","the","context","of","Markov","information","source","s","and","hidden","Markov","model","s","(","HMM",")","."],"labels":["O","B-algorithm","I-algorithm","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","B-algorithm","O","O"],"target_index":null,"target_label":null},"label_list":["product","algorithm","metric","university","country","person","programming_language","task","conference","field","location","organization","researcher"]}
{"id":"151","dataset":"crossner_ai","split":"test","instance":{"id":"151","prompt_labels":"In(O) statistics(B-field) ,(O) multinomial(B-algorithm) logistic(I-algorithm) regression(I-algorithm) is(O) a(O) classification(O) method(O) that(O) generalizes(O) logistic(B-algorithm) regression(I-algorithm) to(O) multiclass(B-task) classification(I-task) ,(O) i.e.(O) with(O) more(O) than(O) two(O) possible(O) discrete(O) outcomes(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, product, algorithm, task, organization, country, metric, conference, university, location, field, researcher, programming language and O.\nSentence: In statistics , multinomial logistic regression is a classification method that generalizes logistic regression to multiclass classification , i.e. with more than two possible discrete outcomes .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","statistics",",","multinomial","logistic","regression","is","a","classification","method","that","generalizes","logistic","regression","to","multiclass","classification",",","i.e.","with","more","than","two","possible","discrete","outcomes","."],"labels":["O","B-field","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","B-algorithm","I-algorithm","O","B-task","I-task","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","product","algorithm","task","organization","country","metric","conference","university","location","field","researcher","programming_language"]}
{"id":"152","dataset":"crossner_ai","split":"test","instance":{"id":"152","prompt_labels":"Hidden(B-algorithm) Markov(I-algorithm) models(I-algorithm) are(O) known(O) for(O) their(O) applications(O) to(O) reinforcement(B-field) learning(I-field) and(O) temporal(B-field) pattern(I-field) recognition(I-field) such(O) as(O) speech(B-task) ,(O) handwriting(B-task) recognition(I-task) ,(O) gesture(B-task) recognition(I-task) ,(O) Thad(B-researcher) Starner(I-researcher) ,(O) Alex(B-researcher) Pentland(I-researcher) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, task, field, algorithm, product, metric, organization, university, country, conference, programming language, person, location and O.\nSentence: Hidden Markov models are known for their applications to reinforcement learning and temporal pattern recognition such as speech , handwriting recognition , gesture recognition , Thad Starner , Alex Pentland .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Hidden","Markov","models","are","known","for","their","applications","to","reinforcement","learning","and","temporal","pattern","recognition","such","as","speech",",","handwriting","recognition",",","gesture","recognition",",","Thad","Starner",",","Alex","Pentland","."],"labels":["B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","B-field","I-field","O","B-field","I-field","I-field","O","O","B-task","O","B-task","I-task","O","B-task","I-task","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O"],"target_index":null,"target_label":null},"label_list":["researcher","task","field","algorithm","product","metric","organization","university","country","conference","programming_language","person","location"]}
{"id":"153","dataset":"crossner_ai","split":"test","instance":{"id":"153","prompt_labels":"Essentially(O) ,(O) this(O) means(O) that(O) if(O) the(O) n-gram(O) has(O) been(O) seen(O) more(O) than(O) k(O) times(O) in(O) training(O) ,(O) the(O) conditional(O) probability(O) of(O) a(O) word(O) given(O) its(O) history(O) is(O) proportional(O) to(O) the(O) maximum(B-metric) likelihood(I-metric) estimate(I-metric) of(O) that(O) n(O) -gram(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, programming language, person, university, organization, task, metric, algorithm, researcher, product, country, location, conference and O.\nSentence: Essentially , this means that if the n-gram has been seen more than k times in training , the conditional probability of a word given its history is proportional to the maximum likelihood estimate of that n -gram .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Essentially",",","this","means","that","if","the","n-gram","has","been","seen","more","than","k","times","in","training",",","the","conditional","probability","of","a","word","given","its","history","is","proportional","to","the","maximum","likelihood","estimate","of","that","n","-gram","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","programming_language","person","university","organization","task","metric","algorithm","researcher","product","country","location","conference"]}
{"id":"154","dataset":"crossner_ai","split":"test","instance":{"id":"154","prompt_labels":"He(O) is(O) interested(O) in(O) knowledge(B-task) representation(I-task) ,(O) commonsense(B-task) reasoning(I-task) ,(O) and(O) natural(B-task) language(I-task) understanding(I-task) ,(O) believing(O) that(O) deep(B-task) language(I-task) understanding(I-task) can(O) only(O) currently(O) be(O) achieved(O) by(O) significant(O) hand-engineering(O) of(O) semantically-rich(O) formalisms(O) coupled(O) with(O) statistical(O) preferences(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, location, university, metric, person, conference, task, field, country, organization, algorithm, product, researcher and O.\nSentence: He is interested in knowledge representation , commonsense reasoning , and natural language understanding , believing that deep language understanding can only currently be achieved by significant hand-engineering of semantically-rich formalisms coupled with statistical preferences .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","is","interested","in","knowledge","representation",",","commonsense","reasoning",",","and","natural","language","understanding",",","believing","that","deep","language","understanding","can","only","currently","be","achieved","by","significant","hand-engineering","of","semantically-rich","formalisms","coupled","with","statistical","preferences","."],"labels":["O","O","O","O","B-task","I-task","O","B-task","I-task","O","O","B-task","I-task","I-task","O","O","O","B-task","I-task","I-task","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","location","university","metric","person","conference","task","field","country","organization","algorithm","product","researcher"]}
{"id":"155","dataset":"crossner_ai","split":"test","instance":{"id":"155","prompt_labels":"In(O) JavaScript(B-programming language) ,(O) Python(B-programming language) or(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, conference, product, field, country, organization, university, algorithm, programming language, person, task, location, researcher and O.\nSentence: In JavaScript , Python or","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","JavaScript",",","Python","or"],"labels":["O","B-programming language","O","B-programming language","O"],"target_index":null,"target_label":null},"label_list":["metric","conference","product","field","country","organization","university","algorithm","programming_language","person","task","location","researcher"]}
{"id":"156","dataset":"crossner_ai","split":"test","instance":{"id":"156","prompt_labels":"The(O) Newcomb(O) Awards(O) are(O) announced(O) in(O) the(O) AI(O) Magazine(O) published(O) by(O) AAAI(B-conference) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, university, algorithm, organization, product, country, field, location, metric, task, programming language, person, researcher and O.\nSentence: The Newcomb Awards are announced in the AI Magazine published by AAAI .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Newcomb","Awards","are","announced","in","the","AI","Magazine","published","by","AAAI","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-conference","O"],"target_index":null,"target_label":null},"label_list":["conference","university","algorithm","organization","product","country","field","location","metric","task","programming_language","person","researcher"]}
{"id":"159","dataset":"crossner_ai","split":"test","instance":{"id":"159","prompt_labels":"Chatbots(B-product) are(O) typically(O) used(O) in(O) dialog(B-product) systems(I-product) for(O) various(O) purposes(O) including(O) customer(O) service(O) ,(O) request(O) routing(O) ,(O) or(O) for(O) information(O) gathering(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, programming language, field, researcher, organization, metric, country, algorithm, location, person, product, task, university and O.\nSentence: Chatbots are typically used in dialog systems for various purposes including customer service , request routing , or for information gathering .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Chatbots","are","typically","used","in","dialog","systems","for","various","purposes","including","customer","service",",","request","routing",",","or","for","information","gathering","."],"labels":["B-product","O","O","O","O","B-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["conference","programming_language","field","researcher","organization","metric","country","algorithm","location","person","product","task","university"]}
{"id":"162","dataset":"crossner_ai","split":"test","instance":{"id":"162","prompt_labels":"While(O) there(O) is(O) no(O) perfect(O) way(O) of(O) describing(O) the(O) confusion(B-metric) matrix(I-metric) of(O) TRUE(O) and(O) FALSE(O) positives(O) and(O) negatives(O) by(O) a(O) single(O) number(O) ,(O) the(O) Matthews(B-metric) correlation(I-metric) coefficient(I-metric) is(O) generally(O) regarded(O) as(O) being(O) one(O) of(O) the(O) best(O) such(O) measures(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, field, person, programming language, researcher, country, product, location, metric, university, algorithm, organization, conference and O.\nSentence: While there is no perfect way of describing the confusion matrix of TRUE and FALSE positives and negatives by a single number , the Matthews correlation coefficient is generally regarded as being one of the best such measures .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["While","there","is","no","perfect","way","of","describing","the","confusion","matrix","of","TRUE","and","FALSE","positives","and","negatives","by","a","single","number",",","the","Matthews","correlation","coefficient","is","generally","regarded","as","being","one","of","the","best","such","measures","."],"labels":["O","O","O","O","O","O","O","O","O","B-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["task","field","person","programming_language","researcher","country","product","location","metric","university","algorithm","organization","conference"]}
{"id":"163","dataset":"crossner_ai","split":"test","instance":{"id":"163","prompt_labels":"As(O) data(O) set(O) s(O) have(O) grown(O) in(O) size(O) and(O) complexity(O) ,(O) direct(O) hands-on(O) data(B-field) analysis(I-field) has(O) been(O) augmented(O) with(O) indirect(O) ,(O) automated(O) data(O) processing(O) ,(O) aided(O) by(O) other(O) discoveries(O) in(O) computer(B-field) science(I-field) ,(O) specially(O) in(O) the(O) field(O) of(O) machine(B-field) learning(I-field) ,(O) such(O) as(O) neural(B-algorithm) networks(I-algorithm) ,(O) cluster(B-task) analysis(I-task) ,(O) genetic(B-algorithm) algorithms(I-algorithm) ((O) 1950s(O) )(O) ,(O) decision(B-algorithm) tree(I-algorithm) learning(I-algorithm) and(O) decision(B-algorithm) rules(I-algorithm) ((O) 1960s(O) )(O) ,(O) and(O) support(B-algorithm) vector(I-algorithm) machines(I-algorithm) ((O) 1990s(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, algorithm, organization, university, product, researcher, programming language, location, metric, conference, person, task, country and O.\nSentence: As data set s have grown in size and complexity , direct hands-on data analysis has been augmented with indirect , automated data processing , aided by other discoveries in computer science , specially in the field of machine learning , such as neural networks , cluster analysis , genetic algorithms ( 1950s ) , decision tree learning and decision rules ( 1960s ) , and support vector machines ( 1990s ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["As","data","set","s","have","grown","in","size","and","complexity",",","direct","hands-on","data","analysis","has","been","augmented","with","indirect",",","automated","data","processing",",","aided","by","other","discoveries","in","computer","science",",","specially","in","the","field","of","machine","learning",",","such","as","neural","networks",",","cluster","analysis",",","genetic","algorithms","(","1950s",")",",","decision","tree","learning","and","decision","rules","(","1960s",")",",","and","support","vector","machines","(","1990s",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","B-field","I-field","O","O","O","B-algorithm","I-algorithm","O","B-task","I-task","O","B-algorithm","I-algorithm","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","I-algorithm","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","algorithm","organization","university","product","researcher","programming_language","location","metric","conference","person","task","country"]}
{"id":"164","dataset":"crossner_ai","split":"test","instance":{"id":"164","prompt_labels":"In(O) the(O) fall(O) of(O) 2005(O) ,(O) Thrun(B-researcher) published(O) a(O) textbook(O) entitled(O) Probabilistic(O) Robotics(O) together(O) with(O) his(O) long-term(O) co-workers(O) Dieter(B-researcher) Fox(I-researcher) and(O) Wolfram(B-researcher) Burgard(I-researcher) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, person, algorithm, researcher, product, university, field, metric, task, programming language, conference, location and O.\nSentence: In the fall of 2005 , Thrun published a textbook entitled Probabilistic Robotics together with his long-term co-workers Dieter Fox and Wolfram Burgard .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","fall","of","2005",",","Thrun","published","a","textbook","entitled","Probabilistic","Robotics","together","with","his","long-term","co-workers","Dieter","Fox","and","Wolfram","Burgard","."],"labels":["O","O","O","O","O","O","B-researcher","O","O","O","O","O","O","O","O","O","O","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O"],"target_index":null,"target_label":null},"label_list":["organization","country","person","algorithm","researcher","product","university","field","metric","task","programming_language","conference","location"]}
{"id":"165","dataset":"crossner_ai","split":"test","instance":{"id":"165","prompt_labels":"John(B-researcher) D.(I-researcher) Lafferty(I-researcher) ,(O) Andrew(B-researcher) McCallum(I-researcher) and(O) Pereiramath(B-researcher) as(O) follows(O) :(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, task, researcher, conference, field, organization, location, product, country, person, programming language, university, metric and O.\nSentence: John D. Lafferty , Andrew McCallum and Pereiramath as follows :","prediction_output":null,"prediction_outputs":null,"group":null,"words":["John","D.","Lafferty",",","Andrew","McCallum","and","Pereiramath","as","follows",":"],"labels":["B-researcher","I-researcher","I-researcher","O","B-researcher","I-researcher","O","B-researcher","O","O","O"],"target_index":null,"target_label":null},"label_list":["algorithm","task","researcher","conference","field","organization","location","product","country","person","programming_language","university","metric"]}
{"id":"167","dataset":"crossner_ai","split":"test","instance":{"id":"167","prompt_labels":"However(O) ,(O) in(O) the(O) version(O) of(O) the(O) metric(O) used(O) by(O) NIST(B-metric) evaluations(O) prior(O) to(O) 2009(O) ,(O) the(O) shortest(O) reference(O) sentence(O) had(O) been(O) used(O) instead(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, conference, programming language, field, product, location, organization, task, algorithm, person, metric, country, university and O.\nSentence: However , in the version of the metric used by NIST evaluations prior to 2009 , the shortest reference sentence had been used instead .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["However",",","in","the","version","of","the","metric","used","by","NIST","evaluations","prior","to","2009",",","the","shortest","reference","sentence","had","been","used","instead","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","conference","programming_language","field","product","location","organization","task","algorithm","person","metric","country","university"]}
{"id":"168","dataset":"crossner_ai","split":"test","instance":{"id":"168","prompt_labels":"On(O) August(O) 27(O) ,(O) 2018(O) ,(O) Toyota(B-person) announced(O) an(O) investment(O) of(O) $(O) 500(O) Million(O) in(O) Uber(B-organization) '(O) s(O) autonomous(B-product) car(I-product) s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, location, product, field, conference, person, task, country, algorithm, organization, programming language, university, researcher and O.\nSentence: On August 27 , 2018 , Toyota announced an investment of $ 500 Million in Uber ' s autonomous car s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","August","27",",","2018",",","Toyota","announced","an","investment","of","$","500","Million","in","Uber","'","s","autonomous","car","s","."],"labels":["O","O","O","O","O","O","B-person","O","O","O","O","O","O","O","O","B-organization","O","O","B-product","I-product","O","O"],"target_index":null,"target_label":null},"label_list":["metric","location","product","field","conference","person","task","country","algorithm","organization","programming_language","university","researcher"]}
{"id":"169","dataset":"crossner_ai","split":"test","instance":{"id":"169","prompt_labels":"The(O) sample(O) maximum(O) is(O) the(O) maximum(B-metric) likelihood(I-metric) estimator(I-metric) for(O) the(O) population(O) maximum(O) ,(O) but(O) ,(O) as(O) discussed(O) above(O) ,(O) it(O) is(O) biased(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, researcher, task, metric, programming language, conference, field, university, organization, product, location, algorithm, country and O.\nSentence: The sample maximum is the maximum likelihood estimator for the population maximum , but , as discussed above , it is biased .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","sample","maximum","is","the","maximum","likelihood","estimator","for","the","population","maximum",",","but",",","as","discussed","above",",","it","is","biased","."],"labels":["O","O","O","O","O","B-metric","I-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","researcher","task","metric","programming_language","conference","field","university","organization","product","location","algorithm","country"]}
{"id":"170","dataset":"crossner_ai","split":"test","instance":{"id":"170","prompt_labels":"LSI(B-task) helps(O) overcome(O) synonymy(O) by(O) increasing(O) recall(B-metric) ,(O) one(O) of(O) the(O) most(O) problematic(O) constraints(O) of(O) Boolean(B-algorithm) keyword(I-algorithm) queries(I-algorithm) and(O) vector(B-algorithm) space(I-algorithm) models(I-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, university, programming language, conference, organization, field, product, researcher, metric, country, algorithm, person, location and O.\nSentence: LSI helps overcome synonymy by increasing recall , one of the most problematic constraints of Boolean keyword queries and vector space models .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["LSI","helps","overcome","synonymy","by","increasing","recall",",","one","of","the","most","problematic","constraints","of","Boolean","keyword","queries","and","vector","space","models","."],"labels":["B-task","O","O","O","O","O","B-metric","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","I-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["task","university","programming_language","conference","organization","field","product","researcher","metric","country","algorithm","person","location"]}
{"id":"171","dataset":"crossner_ai","split":"test","instance":{"id":"171","prompt_labels":"Data(B-task) acquisition(I-task) applications(O) are(O) usually(O) controlled(O) by(O) software(O) programs(O) developed(O) using(O) various(O) general(O) purpose(O) programming(O) languages(O) such(O) as(O) Assembly(B-programming language) ,(O) BASIC(B-programming language) ,(O) C(B-programming language) ,(O) C(B-programming language) +(I-programming language) +(I-programming language) ,(O) C(B-programming language) #(I-programming language) ,(O) Fortran(B-programming language) ,(O) Java(B-programming language) ,(O) LabVIEW(B-programming language) ,(O) Lisp(B-programming language) ,(O) Pascal(B-programming language) ,(O) etc(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, country, programming language, organization, field, person, algorithm, researcher, university, product, metric, location, task and O.\nSentence: Data acquisition applications are usually controlled by software programs developed using various general purpose programming languages such as Assembly , BASIC , C , C + + , C # , Fortran , Java , LabVIEW , Lisp , Pascal , etc .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Data","acquisition","applications","are","usually","controlled","by","software","programs","developed","using","various","general","purpose","programming","languages","such","as","Assembly",",","BASIC",",","C",",","C","+","+",",","C","#",",","Fortran",",","Java",",","LabVIEW",",","Lisp",",","Pascal",",","etc","."],"labels":["B-task","I-task","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-programming language","O","B-programming language","O","B-programming language","O","B-programming language","I-programming language","I-programming language","O","B-programming language","I-programming language","O","B-programming language","O","B-programming language","O","B-programming language","O","B-programming language","O","B-programming language","O","O","O"],"target_index":null,"target_label":null},"label_list":["conference","country","programming_language","organization","field","person","algorithm","researcher","university","product","metric","location","task"]}
{"id":"172","dataset":"crossner_ai","split":"test","instance":{"id":"172","prompt_labels":"In(O) 2003(O) ,(O) Honda(B-organization) released(O) its(O) Cog(B-product) advertisement(O) in(O) the(O) UK(B-country) and(O) on(O) the(O) Internet(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, field, researcher, country, university, person, algorithm, organization, metric, location, conference, programming language, product and O.\nSentence: In 2003 , Honda released its Cog advertisement in the UK and on the Internet .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2003",",","Honda","released","its","Cog","advertisement","in","the","UK","and","on","the","Internet","."],"labels":["O","O","O","B-organization","O","O","B-product","O","O","O","B-country","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["task","field","researcher","country","university","person","algorithm","organization","metric","location","conference","programming_language","product"]}
{"id":"173","dataset":"crossner_ai","split":"test","instance":{"id":"173","prompt_labels":"The(O) Association(B-conference) for(I-conference) Computational(I-conference) Linguistics(I-conference) defines(O) computational(B-field) linguistics(I-field) as(O) :(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, product, researcher, organization, field, algorithm, university, country, metric, person, conference, task, programming language and O.\nSentence: The Association for Computational Linguistics defines computational linguistics as :","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Association","for","Computational","Linguistics","defines","computational","linguistics","as",":"],"labels":["O","B-conference","I-conference","I-conference","I-conference","O","B-field","I-field","O","O"],"target_index":null,"target_label":null},"label_list":["location","product","researcher","organization","field","algorithm","university","country","metric","person","conference","task","programming_language"]}
{"id":"174","dataset":"crossner_ai","split":"test","instance":{"id":"174","prompt_labels":"Expectation-maximization(B-algorithm) algorithm(I-algorithm) s(O) may(O) be(O) employed(O) to(O) calculate(O) approximate(O) maximum(B-algorithm) likelihood(I-algorithm) estimates(I-algorithm) of(O) unknown(O) state-space(O) parameters(O) within(O) minimum-variance(O) filters(O) and(O) smoothers(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, university, researcher, algorithm, task, conference, product, metric, country, organization, programming language, person, location and O.\nSentence: Expectation-maximization algorithm s may be employed to calculate approximate maximum likelihood estimates of unknown state-space parameters within minimum-variance filters and smoothers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Expectation-maximization","algorithm","s","may","be","employed","to","calculate","approximate","maximum","likelihood","estimates","of","unknown","state-space","parameters","within","minimum-variance","filters","and","smoothers","."],"labels":["B-algorithm","I-algorithm","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","university","researcher","algorithm","task","conference","product","metric","country","organization","programming_language","person","location"]}
{"id":"175","dataset":"crossner_ai","split":"test","instance":{"id":"175","prompt_labels":"Correspondents(O) included(O) former(O) Baywatch(O) actresses(O) Donna(B-person) D(I-person) 'Errico(I-person) ,(O) Carmen(B-person) Electra(I-person) ,(O) and(O) Traci(B-person) Bingham(I-person) ,(O) former(O) Playboy(O) Playmate(O) Heidi(B-person) Mark(I-person) ,(O) comedian(O) Arj(B-person) Barker(I-person) and(O) identical(O) twins(O) Randy(B-person) and(O) Jason(B-person) Sklar(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, task, programming language, field, organization, algorithm, country, product, conference, metric, person, researcher, location and O.\nSentence: Correspondents included former Baywatch actresses Donna D 'Errico , Carmen Electra , and Traci Bingham , former Playboy Playmate Heidi Mark , comedian Arj Barker and identical twins Randy and Jason Sklar .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Correspondents","included","former","Baywatch","actresses","Donna","D","'Errico",",","Carmen","Electra",",","and","Traci","Bingham",",","former","Playboy","Playmate","Heidi","Mark",",","comedian","Arj","Barker","and","identical","twins","Randy","and","Jason","Sklar","."],"labels":["O","O","O","O","O","B-person","I-person","I-person","O","B-person","I-person","O","O","B-person","I-person","O","O","O","O","B-person","I-person","O","O","B-person","I-person","O","O","O","B-person","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["university","task","programming_language","field","organization","algorithm","country","product","conference","metric","person","researcher","location"]}
{"id":"176","dataset":"crossner_ai","split":"test","instance":{"id":"176","prompt_labels":"It(O) is(O) commonly(O) used(O) to(O) generate(O) representations(O) for(O) speech(B-task) recognition(I-task) ((O) ASR(B-task) )(O) ,(O) e.g.(O) the(O) CMU(B-product) Sphinx(I-product) system(I-product) ,(O) and(O) speech(B-task) synthesis(I-task) ((O) TTS(B-task) )(O) ,(O) e.g.(O) the(O) Festival(B-product) system(I-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, product, country, task, conference, metric, algorithm, university, location, programming language, organization, researcher, field and O.\nSentence: It is commonly used to generate representations for speech recognition ( ASR ) , e.g. the CMU Sphinx system , and speech synthesis ( TTS ) , e.g. the Festival system .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","is","commonly","used","to","generate","representations","for","speech","recognition","(","ASR",")",",","e.g.","the","CMU","Sphinx","system",",","and","speech","synthesis","(","TTS",")",",","e.g.","the","Festival","system","."],"labels":["O","O","O","O","O","O","O","O","B-task","I-task","O","B-task","O","O","O","O","B-product","I-product","I-product","O","O","B-task","I-task","O","B-task","O","O","O","O","B-product","I-product","O"],"target_index":null,"target_label":null},"label_list":["person","product","country","task","conference","metric","algorithm","university","location","programming_language","organization","researcher","field"]}
{"id":"177","dataset":"crossner_ai","split":"test","instance":{"id":"177","prompt_labels":"Sensitivity(B-metric) or(O) TRUE(B-metric) Positive(I-metric) Rate(I-metric) ((O) TPR(B-metric) )(O) ,(O) also(O) known(O) as(O) recall(B-metric) ,(O) is(O) the(O) proportion(O) of(O) people(O) that(O) tested(O) positive(O) and(O) are(O) positive(O) ((O) TRUE(B-metric) Positive(I-metric) ,(O) TP(B-metric) )(O) of(O) all(O) the(O) people(O) that(O) actually(O) are(O) positive(O) ((O) Condition(B-metric) Positive(I-metric) ,(O) CP(B-metric) =(O) TP(B-metric) +(I-metric) FN(I-metric) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, researcher, country, algorithm, programming language, task, metric, organization, location, person, conference, university, field and O.\nSentence: Sensitivity or TRUE Positive Rate ( TPR ) , also known as recall , is the proportion of people that tested positive and are positive ( TRUE Positive , TP ) of all the people that actually are positive ( Condition Positive , CP = TP + FN ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Sensitivity","or","TRUE","Positive","Rate","(","TPR",")",",","also","known","as","recall",",","is","the","proportion","of","people","that","tested","positive","and","are","positive","(","TRUE","Positive",",","TP",")","of","all","the","people","that","actually","are","positive","(","Condition","Positive",",","CP","=","TP","+","FN",")","."],"labels":["B-metric","O","B-metric","I-metric","I-metric","O","B-metric","O","O","O","O","O","B-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","O","B-metric","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","O","B-metric","O","B-metric","I-metric","I-metric","O","O"],"target_index":null,"target_label":null},"label_list":["product","researcher","country","algorithm","programming_language","task","metric","organization","location","person","conference","university","field"]}
{"id":"178","dataset":"crossner_ai","split":"test","instance":{"id":"178","prompt_labels":"Popular(O) speech(B-task) recognition(I-task) conferences(O) held(O) each(O) year(O) or(O) two(O) include(O) SpeechTEK(B-conference) and(O) SpeechTEK(B-conference) Europe(I-conference) ,(O) ICASSP(B-conference) ,(O) Interspeech(B-conference) /(O) Eurospeech(B-conference) ,(O) and(O) the(O) IEEE(B-conference) ASRU(I-conference) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, programming language, field, conference, task, organization, researcher, location, product, university, person, metric, algorithm and O.\nSentence: Popular speech recognition conferences held each year or two include SpeechTEK and SpeechTEK Europe , ICASSP , Interspeech / Eurospeech , and the IEEE ASRU .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Popular","speech","recognition","conferences","held","each","year","or","two","include","SpeechTEK","and","SpeechTEK","Europe",",","ICASSP",",","Interspeech","/","Eurospeech",",","and","the","IEEE","ASRU","."],"labels":["O","B-task","I-task","O","O","O","O","O","O","O","B-conference","O","B-conference","I-conference","O","B-conference","O","B-conference","O","B-conference","O","O","O","B-conference","I-conference","O"],"target_index":null,"target_label":null},"label_list":["country","programming_language","field","conference","task","organization","researcher","location","product","university","person","metric","algorithm"]}
{"id":"179","dataset":"crossner_ai","split":"test","instance":{"id":"179","prompt_labels":"Devol(B-researcher) collaborated(O) with(O) Engelberger(B-researcher) ,(O) who(O) served(O) as(O) president(O) of(O) the(O) company(O) ,(O) to(O) engineer(O) and(O) produce(O) an(O) industrial(B-product) robot(I-product) under(O) the(O) brand(O) name(O) Unimate(B-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, conference, person, field, task, programming language, location, algorithm, country, product, researcher, organization, university and O.\nSentence: Devol collaborated with Engelberger , who served as president of the company , to engineer and produce an industrial robot under the brand name Unimate .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Devol","collaborated","with","Engelberger",",","who","served","as","president","of","the","company",",","to","engineer","and","produce","an","industrial","robot","under","the","brand","name","Unimate","."],"labels":["B-researcher","O","O","B-researcher","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","I-product","O","O","O","O","B-product","O"],"target_index":null,"target_label":null},"label_list":["metric","conference","person","field","task","programming_language","location","algorithm","country","product","researcher","organization","university"]}
{"id":"180","dataset":"crossner_ai","split":"test","instance":{"id":"180","prompt_labels":"A(O) Hidden(B-algorithm) Markov(I-algorithm) model(I-algorithm) ((O) HMM(B-algorithm) )(O) is(O) a(O) statistical(B-algorithm) Markov(I-algorithm) model(I-algorithm) in(O) which(O) the(O) system(O) being(O) modeled(O) is(O) assumed(O) to(O) be(O) a(O) Markov(B-algorithm) process(I-algorithm) with(O) unobserved(O) ((O) hidden(O) )(O) states(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, field, programming language, task, product, location, algorithm, person, university, conference, country, researcher, metric and O.\nSentence: A Hidden Markov model ( HMM ) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved ( hidden ) states .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","Hidden","Markov","model","(","HMM",")","is","a","statistical","Markov","model","in","which","the","system","being","modeled","is","assumed","to","be","a","Markov","process","with","unobserved","(","hidden",")","states","."],"labels":["O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","field","programming_language","task","product","location","algorithm","person","university","conference","country","researcher","metric"]}
{"id":"181","dataset":"crossner_ai","split":"test","instance":{"id":"181","prompt_labels":"This(O) property(O) ,(O) undesirable(O) in(O) many(O) applications(O) ,(O) has(O) led(O) researchers(O) to(O) use(O) alternatives(O) such(O) as(O) the(O) mean(B-metric) absolute(I-metric) error(I-metric) ,(O) or(O) those(O) based(O) on(O) the(O) median(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, field, person, metric, task, organization, university, conference, location, product, country, researcher, algorithm and O.\nSentence: This property , undesirable in many applications , has led researchers to use alternatives such as the mean absolute error , or those based on the median .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","property",",","undesirable","in","many","applications",",","has","led","researchers","to","use","alternatives","such","as","the","mean","absolute","error",",","or","those","based","on","the","median","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","field","person","metric","task","organization","university","conference","location","product","country","researcher","algorithm"]}
{"id":"182","dataset":"crossner_ai","split":"test","instance":{"id":"182","prompt_labels":"Such(O) a(O) sequence(O) ((O) which(O) depends(O) on(O) the(O) outcome(O) of(O) the(O) investigation(O) of(O) previous(O) attributes(O) at(O) each(O) stage(O) )(O) is(O) called(O) a(O) decision(B-algorithm) tree(I-algorithm) and(O) applied(O) in(O) the(O) area(O) of(O) machine(B-field) learning(I-field) known(O) as(O) decision(B-algorithm) tree(I-algorithm) learning(I-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, algorithm, country, location, field, product, programming language, organization, person, university, task, metric, researcher and O.\nSentence: Such a sequence ( which depends on the outcome of the investigation of previous attributes at each stage ) is called a decision tree and applied in the area of machine learning known as decision tree learning .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Such","a","sequence","(","which","depends","on","the","outcome","of","the","investigation","of","previous","attributes","at","each","stage",")","is","called","a","decision","tree","and","applied","in","the","area","of","machine","learning","known","as","decision","tree","learning","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","B-field","I-field","O","O","B-algorithm","I-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["conference","algorithm","country","location","field","product","programming_language","organization","person","university","task","metric","researcher"]}
{"id":"183","dataset":"crossner_ai","split":"test","instance":{"id":"183","prompt_labels":"As(O) in(O) factor(B-task) analysis(I-task) ,(O) the(O) LCA(B-algorithm) can(O) also(O) be(O) used(O) to(O) classify(O) case(O) according(O) to(O) their(O) maximum(B-algorithm) likelihood(I-algorithm) class(O) membership(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, researcher, algorithm, task, person, university, product, conference, country, organization, location, field, metric and O.\nSentence: As in factor analysis , the LCA can also be used to classify case according to their maximum likelihood class membership .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["As","in","factor","analysis",",","the","LCA","can","also","be","used","to","classify","case","according","to","their","maximum","likelihood","class","membership","."],"labels":["O","O","B-task","I-task","O","O","B-algorithm","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","researcher","algorithm","task","person","university","product","conference","country","organization","location","field","metric"]}
{"id":"187","dataset":"crossner_ai","split":"test","instance":{"id":"187","prompt_labels":"Traditional(O) image(O) quality(O) measures(O) ,(O) such(O) as(O) PSNR(B-metric) ,(O) are(O) typically(O) performed(O) on(O) fixed(O) resolution(O) images(O) and(O) do(O) not(O) take(O) into(O) account(O) some(O) aspects(O) of(O) the(O) human(O) visual(O) system(O) ,(O) like(O) the(O) change(O) in(O) spatial(O) resolution(O) across(O) the(O) retina(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, metric, product, programming language, task, university, field, organization, conference, algorithm, person, country, researcher and O.\nSentence: Traditional image quality measures , such as PSNR , are typically performed on fixed resolution images and do not take into account some aspects of the human visual system , like the change in spatial resolution across the retina .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Traditional","image","quality","measures",",","such","as","PSNR",",","are","typically","performed","on","fixed","resolution","images","and","do","not","take","into","account","some","aspects","of","the","human","visual","system",",","like","the","change","in","spatial","resolution","across","the","retina","."],"labels":["O","O","O","O","O","O","O","B-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","metric","product","programming_language","task","university","field","organization","conference","algorithm","person","country","researcher"]}
{"id":"188","dataset":"crossner_ai","split":"test","instance":{"id":"188","prompt_labels":"John(B-person) Ireland(I-person) ,(O) Joanne(B-person) Dru(I-person) and(O) Macdonald(B-person) Carey(I-person) starred(O) in(O) the(O) Jack(B-person) Broder(I-person) color(O) production(O) Hannah(O) Lee(O) ,(O) which(O) premiered(O) June(O) 19(O) ,(O) 1953(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, country, metric, university, algorithm, organization, field, location, programming language, person, task, conference, product and O.\nSentence: John Ireland , Joanne Dru and Macdonald Carey starred in the Jack Broder color production Hannah Lee , which premiered June 19 , 1953 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["John","Ireland",",","Joanne","Dru","and","Macdonald","Carey","starred","in","the","Jack","Broder","color","production","Hannah","Lee",",","which","premiered","June","19",",","1953","."],"labels":["B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","country","metric","university","algorithm","organization","field","location","programming_language","person","task","conference","product"]}
{"id":"189","dataset":"crossner_ai","split":"test","instance":{"id":"189","prompt_labels":"That(O) process(O) is(O) called(O) image(B-task) registration(I-task) ,(O) and(O) uses(O) different(O) methods(O) of(O) computer(B-field) vision(I-field) ,(O) mostly(O) related(O) to(O) tracking(B-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, programming language, conference, researcher, organization, metric, algorithm, field, university, task, location, country, person and O.\nSentence: That process is called image registration , and uses different methods of computer vision , mostly related to tracking .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["That","process","is","called","image","registration",",","and","uses","different","methods","of","computer","vision",",","mostly","related","to","tracking","."],"labels":["O","O","O","O","B-task","I-task","O","O","O","O","O","O","B-field","I-field","O","O","O","O","B-task","O"],"target_index":null,"target_label":null},"label_list":["product","programming_language","conference","researcher","organization","metric","algorithm","field","university","task","location","country","person"]}
{"id":"191","dataset":"crossner_ai","split":"test","instance":{"id":"191","prompt_labels":"The(O) VOICEBOX(B-product) speech(O) processing(O) toolbox(O) for(O) MATLAB(B-product) implements(O) the(O) conversion(O) and(O) its(O) inverse(O) as(O) :(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, product, location, person, task, researcher, country, field, organization, algorithm, conference, programming language, university and O.\nSentence: The VOICEBOX speech processing toolbox for MATLAB implements the conversion and its inverse as :","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","VOICEBOX","speech","processing","toolbox","for","MATLAB","implements","the","conversion","and","its","inverse","as",":"],"labels":["O","B-product","O","O","O","O","B-product","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","product","location","person","task","researcher","country","field","organization","algorithm","conference","programming_language","university"]}
{"id":"192","dataset":"crossner_ai","split":"test","instance":{"id":"192","prompt_labels":"Prolog(B-programming language) is(O) a(O) logic(O) programming(O) language(O) associated(O) with(O) artificial(B-field) intelligence(I-field) and(O) computational(B-field) linguistics(I-field) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, product, task, metric, programming language, location, algorithm, conference, organization, country, person, researcher, field and O.\nSentence: Prolog is a logic programming language associated with artificial intelligence and computational linguistics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Prolog","is","a","logic","programming","language","associated","with","artificial","intelligence","and","computational","linguistics","."],"labels":["B-programming language","O","O","O","O","O","O","O","B-field","I-field","O","B-field","I-field","O"],"target_index":null,"target_label":null},"label_list":["university","product","task","metric","programming_language","location","algorithm","conference","organization","country","person","researcher","field"]}
{"id":"193","dataset":"crossner_ai","split":"test","instance":{"id":"193","prompt_labels":"Milner(B-researcher) has(O) received(O) numerous(O) awards(O) for(O) her(O) contributions(O) to(O) neuroscience(B-field) and(O) psychology(B-field) including(O) memberships(O) in(O) the(O) Royal(B-organization) Society(I-organization) of(I-organization) London(I-organization) ,(O) the(O) Royal(B-organization) Society(I-organization) of(I-organization) Canada(I-organization) and(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, country, task, person, conference, product, algorithm, field, organization, location, metric, university, programming language and O.\nSentence: Milner has received numerous awards for her contributions to neuroscience and psychology including memberships in the Royal Society of London , the Royal Society of Canada and the National Academy of Sciences .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Milner","has","received","numerous","awards","for","her","contributions","to","neuroscience","and","psychology","including","memberships","in","the","Royal","Society","of","London",",","the","Royal","Society","of","Canada","and","the","National","Academy","of","Sciences","."],"labels":["B-researcher","O","O","O","O","O","O","O","O","B-field","O","B-field","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["researcher","country","task","person","conference","product","algorithm","field","organization","location","metric","university","programming_language"]}
{"id":"194","dataset":"crossner_ai","split":"test","instance":{"id":"194","prompt_labels":"By(O) combining(O) these(O) operators(O) one(O) can(O) obtain(O) algorithms(O) for(O) many(O) image(B-field) processing(I-field) tasks(O) ,(O) such(O) as(O) feature(B-task) extraction(I-task) ,(O) image(B-task) segmentation(I-task) ,(O) image(B-task) sharpening(I-task) ,(O) image(B-task) filtering(I-task) ,(O) and(O) classification(B-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, product, programming language, algorithm, location, organization, country, person, metric, university, field, conference, task and O.\nSentence: By combining these operators one can obtain algorithms for many image processing tasks , such as feature extraction , image segmentation , image sharpening , image filtering , and classification .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["By","combining","these","operators","one","can","obtain","algorithms","for","many","image","processing","tasks",",","such","as","feature","extraction",",","image","segmentation",",","image","sharpening",",","image","filtering",",","and","classification","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","O","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O","O","B-task","O"],"target_index":null,"target_label":null},"label_list":["researcher","product","programming_language","algorithm","location","organization","country","person","metric","university","field","conference","task"]}
{"id":"196","dataset":"crossner_ai","split":"test","instance":{"id":"196","prompt_labels":"There(O) are(O) many(O) approaches(O) to(O) learning(O) these(O) embeddings(O) ,(O) notably(O) using(O) Bayesian(B-algorithm) clustering(I-algorithm) frameworks(I-algorithm) or(O) energy-based(B-algorithm) frameworks(I-algorithm) ,(O) and(O) more(O) recently(O) ,(O) TransE(B-conference) ((O) Conference(B-conference) on(I-conference) Neural(I-conference) Information(I-conference) Processing(I-conference) Systems(I-conference) 2013(I-conference) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, product, task, organization, metric, conference, location, researcher, field, programming language, country, algorithm, person and O.\nSentence: There are many approaches to learning these embeddings , notably using Bayesian clustering frameworks or energy-based frameworks , and more recently , TransE ( Conference on Neural Information Processing Systems 2013 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["There","are","many","approaches","to","learning","these","embeddings",",","notably","using","Bayesian","clustering","frameworks","or","energy-based","frameworks",",","and","more","recently",",","TransE","(","Conference","on","Neural","Information","Processing","Systems","2013",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","I-algorithm","O","O","O","O","O","B-conference","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","O"],"target_index":null,"target_label":null},"label_list":["university","product","task","organization","metric","conference","location","researcher","field","programming_language","country","algorithm","person"]}
{"id":"197","dataset":"crossner_ai","split":"test","instance":{"id":"197","prompt_labels":"It(O) is(O) an(O) alternative(O) to(O) the(O) Word(B-metric) error(I-metric) rate(I-metric) ((O) Word(B-metric) Error(I-metric) Rate(I-metric) )(O) used(O) in(O) several(O) countries(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, country, field, product, location, person, university, task, programming language, organization, algorithm, conference, metric and O.\nSentence: It is an alternative to the Word error rate ( Word Error Rate ) used in several countries .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","is","an","alternative","to","the","Word","error","rate","(","Word","Error","Rate",")","used","in","several","countries","."],"labels":["O","O","O","O","O","O","B-metric","I-metric","I-metric","O","B-metric","I-metric","I-metric","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","country","field","product","location","person","university","task","programming_language","organization","algorithm","conference","metric"]}
{"id":"198","dataset":"crossner_ai","split":"test","instance":{"id":"198","prompt_labels":"ANNs(B-algorithm) have(O) been(O) used(O) on(O) a(O) variety(O) of(O) tasks(O) ,(O) including(O) computer(B-field) vision(I-field) ,(O) speech(B-task) recognition(I-task) ,(O) machine(B-task) translation(I-task) ,(O) social(B-task) network(I-task) filtering(I-task) ,(O) playing(B-task) board(I-task) and(I-task) video(I-task) games(I-task) ,(O) medical(B-task) diagnosis(I-task) ,(O) and(O) even(O) in(O) activities(O) that(O) have(O) traditionally(O) been(O) considered(O) as(O) reserved(O) to(O) humans(O) ,(O) like(O) painting(B-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, country, product, organization, programming language, conference, researcher, algorithm, field, university, metric, task and O.\nSentence: ANNs have been used on a variety of tasks , including computer vision , speech recognition , machine translation , social network filtering , playing board and video games , medical diagnosis , and even in activities that have traditionally been considered as reserved to humans , like painting .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["ANNs","have","been","used","on","a","variety","of","tasks",",","including","computer","vision",",","speech","recognition",",","machine","translation",",","social","network","filtering",",","playing","board","and","video","games",",","medical","diagnosis",",","and","even","in","activities","that","have","traditionally","been","considered","as","reserved","to","humans",",","like","painting","."],"labels":["B-algorithm","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","I-task","O","B-task","I-task","I-task","I-task","I-task","O","B-task","I-task","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-task","O"],"target_index":null,"target_label":null},"label_list":["location","person","country","product","organization","programming_language","conference","researcher","algorithm","field","university","metric","task"]}
{"id":"199","dataset":"crossner_ai","split":"test","instance":{"id":"199","prompt_labels":"Modular(B-product) Audio(I-product) Recognition(I-product) Framework(I-product) ((O) MARF(B-product) )(O) is(O) an(O) open-source(O) research(O) platform(O) and(O) a(O) collection(O) of(O) voice(O) ,(O) sound(O) ,(O) speech(O) ,(O) text(O) and(O) natural(B-field) language(I-field) processing(I-field) ((O) NLP(B-field) )(O) algorithm(O) s(O) written(O) in(O) Java(B-programming language) and(O) arranged(O) into(O) a(O) modular(O) and(O) extensible(O) framework(O) that(O) attempts(O) to(O) facilitate(O) addition(O) of(O) new(O) algorithm(O) s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, university, conference, metric, product, researcher, organization, location, person, task, algorithm, field, country and O.\nSentence: Modular Audio Recognition Framework ( MARF ) is an open-source research platform and a collection of voice , sound , speech , text and natural language processing ( NLP ) algorithm s written in Java and arranged into a modular and extensible framework that attempts to facilitate addition of new algorithm s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Modular","Audio","Recognition","Framework","(","MARF",")","is","an","open-source","research","platform","and","a","collection","of","voice",",","sound",",","speech",",","text","and","natural","language","processing","(","NLP",")","algorithm","s","written","in","Java","and","arranged","into","a","modular","and","extensible","framework","that","attempts","to","facilitate","addition","of","new","algorithm","s","."],"labels":["B-product","I-product","I-product","I-product","O","B-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","I-field","O","B-field","O","O","O","O","O","B-programming language","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","university","conference","metric","product","researcher","organization","location","person","task","algorithm","field","country"]}
{"id":"200","dataset":"crossner_ai","split":"test","instance":{"id":"200","prompt_labels":"In(O) 2018(O) ,(O) a(O) report(O) by(O) the(O) civil(O) liberties(O) and(O) rights(O) campaigning(O) organisation(O) Big(B-organization) Brother(I-organization) Watch(I-organization) revealed(O) that(O) two(O) United(B-country) Kingdom(I-country) police(O) forces(O) ,(O) South(B-organization) Wales(I-organization) Police(I-organization) and(O) the(O) Metropolitan(B-organization) Police(I-organization) ,(O) were(O) using(O) live(O) facial(B-task) recognition(I-task) at(O) public(O) events(O) and(O) in(O) public(O) spaces(O) ,(O) in(O) September(O) 2019(O) ,(O) South(B-organization) Wales(I-organization) Police(I-organization) use(O) of(O) facial(B-task) recognition(I-task) was(O) ruled(O) lawful(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, organization, programming language, location, product, conference, person, researcher, field, task, country, algorithm, university and O.\nSentence: In 2018 , a report by the civil liberties and rights campaigning organisation Big Brother Watch revealed that two United Kingdom police forces , South Wales Police and the Metropolitan Police , were using live facial recognition at public events and in public spaces , in September 2019 , South Wales Police use of facial recognition was ruled lawful .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2018",",","a","report","by","the","civil","liberties","and","rights","campaigning","organisation","Big","Brother","Watch","revealed","that","two","United","Kingdom","police","forces",",","South","Wales","Police","and","the","Metropolitan","Police",",","were","using","live","facial","recognition","at","public","events","and","in","public","spaces",",","in","September","2019",",","South","Wales","Police","use","of","facial","recognition","was","ruled","lawful","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","B-country","I-country","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","O","O","O","O","B-task","I-task","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-task","I-task","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","organization","programming_language","location","product","conference","person","researcher","field","task","country","algorithm","university"]}
{"id":"203","dataset":"crossner_ai","split":"test","instance":{"id":"203","prompt_labels":"In(O) July(O) 2016(O) ,(O) Nvidia(B-organization) demonstrated(O) during(O) SIGGRAPH(B-conference) a(O) new(O) method(O) of(O) foveated(O) rendering(O) claimed(O) to(O) be(O) invisible(O) to(O) users(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, algorithm, person, organization, metric, conference, researcher, task, university, product, country, programming language, field and O.\nSentence: In July 2016 , Nvidia demonstrated during SIGGRAPH a new method of foveated rendering claimed to be invisible to users .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","July","2016",",","Nvidia","demonstrated","during","SIGGRAPH","a","new","method","of","foveated","rendering","claimed","to","be","invisible","to","users","."],"labels":["O","O","O","O","B-organization","O","O","B-conference","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","algorithm","person","organization","metric","conference","researcher","task","university","product","country","programming_language","field"]}
{"id":"204","dataset":"crossner_ai","split":"test","instance":{"id":"204","prompt_labels":"Both(O) rely(O) on(O) speech(O) act(O) theory(O) developed(O) by(O) John(B-researcher) Searle(I-researcher) in(O) the(O) 1960s(O) and(O) enhanced(O) by(O) Terry(B-researcher) Winograd(I-researcher) and(O) Flores(B-researcher) in(O) the(O) 1970s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, product, algorithm, conference, task, field, programming language, researcher, metric, country, person, location, organization and O.\nSentence: Both rely on speech act theory developed by John Searle in the 1960s and enhanced by Terry Winograd and Flores in the 1970s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Both","rely","on","speech","act","theory","developed","by","John","Searle","in","the","1960s","and","enhanced","by","Terry","Winograd","and","Flores","in","the","1970s","."],"labels":["O","O","O","O","O","O","O","O","B-researcher","I-researcher","O","O","O","O","O","O","B-researcher","I-researcher","O","B-researcher","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","product","algorithm","conference","task","field","programming_language","researcher","metric","country","person","location","organization"]}
{"id":"205","dataset":"crossner_ai","split":"test","instance":{"id":"205","prompt_labels":"Neural(B-algorithm) network(I-algorithm) models(I-algorithm) of(O) concept(O) formation(O) and(O) the(O) structure(O) of(O) knowledge(O) have(O) opened(O) powerful(O) hierarchical(O) models(O) of(O) knowledge(O) organization(O) such(O) as(O) George(B-researcher) Miller(I-researcher) '(O) s(O) Wordnet(B-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, field, algorithm, conference, programming language, country, task, researcher, university, organization, metric, person, location and O.\nSentence: Neural network models of concept formation and the structure of knowledge have opened powerful hierarchical models of knowledge organization such as George Miller ' s Wordnet .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Neural","network","models","of","concept","formation","and","the","structure","of","knowledge","have","opened","powerful","hierarchical","models","of","knowledge","organization","such","as","George","Miller","'","s","Wordnet","."],"labels":["B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-researcher","I-researcher","O","O","B-product","O"],"target_index":null,"target_label":null},"label_list":["product","field","algorithm","conference","programming_language","country","task","researcher","university","organization","metric","person","location"]}
{"id":"206","dataset":"crossner_ai","split":"test","instance":{"id":"206","prompt_labels":"Template(B-algorithm) matching(I-algorithm) has(O) various(O) applications(O) and(O) is(O) used(O) in(O) such(O) fields(O) as(O) face(B-task) recognition(I-task) ((O) see(O) facial(B-product) recognition(I-product) system(I-product) )(O) and(O) medical(B-task) image(I-task) processing(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, organization, researcher, programming language, task, conference, location, university, algorithm, person, country, product, metric and O.\nSentence: Template matching has various applications and is used in such fields as face recognition ( see facial recognition system ) and medical image processing .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Template","matching","has","various","applications","and","is","used","in","such","fields","as","face","recognition","(","see","facial","recognition","system",")","and","medical","image","processing","."],"labels":["B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","B-task","I-task","O","O","B-product","I-product","I-product","O","O","B-task","I-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["field","organization","researcher","programming_language","task","conference","location","university","algorithm","person","country","product","metric"]}
{"id":"207","dataset":"crossner_ai","split":"test","instance":{"id":"207","prompt_labels":"However(O) ,(O) usage(O) only(O) became(O) widespread(O) in(O) 2005(O) when(O) Navneet(B-researcher) Dalal(I-researcher) and(O) Bill(B-researcher) Triggs(I-researcher) ,(O) researchers(O) for(O) the(O) French(B-organization) National(I-organization) Institute(I-organization) for(I-organization) Research(I-organization) in(I-organization) Computer(I-organization) Science(I-organization) and(I-organization) Automation(I-organization) ((O) INRIA(B-organization) )(O) ,(O) presented(O) their(O) supplementary(O) work(O) on(O) HOG(B-algorithm) descriptors(I-algorithm) at(O) the(O) Conference(B-conference) on(I-conference) Computer(I-conference) Vision(I-conference) and(I-conference) Pattern(I-conference) Recognition(I-conference) ((O) CVPR(B-conference) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, location, conference, country, researcher, metric, field, product, algorithm, person, programming language, task, organization and O.\nSentence: However , usage only became widespread in 2005 when Navneet Dalal and Bill Triggs , researchers for the French National Institute for Research in Computer Science and Automation ( INRIA ) , presented their supplementary work on HOG descriptors at the Conference on Computer Vision and Pattern Recognition ( CVPR ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["However",",","usage","only","became","widespread","in","2005","when","Navneet","Dalal","and","Bill","Triggs",",","researchers","for","the","French","National","Institute","for","Research","in","Computer","Science","and","Automation","(","INRIA",")",",","presented","their","supplementary","work","on","HOG","descriptors","at","the","Conference","on","Computer","Vision","and","Pattern","Recognition","(","CVPR",")","."],"labels":["O","O","O","O","O","O","O","O","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","B-conference","O","O"],"target_index":null,"target_label":null},"label_list":["university","location","conference","country","researcher","metric","field","product","algorithm","person","programming_language","task","organization"]}
{"id":"208","dataset":"crossner_ai","split":"test","instance":{"id":"208","prompt_labels":"Prior(O) to(O) joining(O) the(O) Penn(B-university) faculty(O) in(O) 2002(O) ,(O) he(O) spent(O) a(O) decade(O) ((O) 1991-2001(O) )(O) in(O) AT(B-organization) &(I-organization) T(I-organization) Labs(I-organization) and(O) Bell(B-organization) Labs(I-organization) ,(O) including(O) as(O) head(O) of(O) the(O) AI(B-field) department(O) with(O) colleagues(O) including(O) Michael(B-researcher) L.(I-researcher) Littman(I-researcher) ,(O) David(B-researcher) A.(I-researcher) McAllester(I-researcher) ,(O) and(O) Richard(B-researcher) S.(I-researcher) Sutton(I-researcher) ;(O) Secure(B-organization) Systems(I-organization) Research(I-organization) department(I-organization) ;(O) and(O) Machine(B-field) Learning(I-field) department(O) with(O) members(O) such(O) as(O) Michael(B-researcher) Collins(I-researcher) and(O) the(O) leader(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, country, conference, person, university, location, researcher, programming language, algorithm, product, metric, task, organization and O.\nSentence: Prior to joining the Penn faculty in 2002 , he spent a decade ( 1991-2001 ) in AT & T Labs and Bell Labs , including as head of the AI department with colleagues including Michael L. Littman , David A. McAllester , and Richard S. Sutton ; Secure Systems Research department ; and Machine Learning department with members such as Michael Collins and the leader ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Prior","to","joining","the","Penn","faculty","in","2002",",","he","spent","a","decade","(","1991-2001",")","in","AT","&","T","Labs","and","Bell","Labs",",","including","as","head","of","the","AI","department","with","colleagues","including","Michael","L.","Littman",",","David","A.","McAllester",",","and","Richard","S.","Sutton",";","Secure","Systems","Research","department",";","and","Machine","Learning","department","with","members","such","as","Michael","Collins","and","the","leader",")","."],"labels":["O","O","O","O","B-university","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","O","O","O","O","O","O","B-field","O","O","O","O","B-researcher","I-researcher","I-researcher","O","B-researcher","I-researcher","I-researcher","O","O","B-researcher","I-researcher","I-researcher","O","B-organization","I-organization","I-organization","I-organization","O","O","B-field","I-field","O","O","O","O","O","B-researcher","I-researcher","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","country","conference","person","university","location","researcher","programming_language","algorithm","product","metric","task","organization"]}
{"id":"210","dataset":"crossner_ai","split":"test","instance":{"id":"210","prompt_labels":"This(O) field(O) of(O) computer(B-field) science(I-field) developed(O) in(O) the(O) 1950s(O) at(O) academic(O) institutions(O) such(O) as(O) the(O) MIT(B-organization) A.I.(I-organization) Lab(I-organization) ,(O) originally(O) as(O) a(O) branch(O) of(O) artificial(B-field) intelligence(I-field) and(O) robotics(B-field) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, task, university, organization, conference, researcher, programming language, country, algorithm, location, product, field, metric and O.\nSentence: This field of computer science developed in the 1950s at academic institutions such as the MIT A.I. Lab , originally as a branch of artificial intelligence and robotics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","field","of","computer","science","developed","in","the","1950s","at","academic","institutions","such","as","the","MIT","A.I.","Lab",",","originally","as","a","branch","of","artificial","intelligence","and","robotics","."],"labels":["O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","B-field","I-field","O","B-field","O"],"target_index":null,"target_label":null},"label_list":["person","task","university","organization","conference","researcher","programming_language","country","algorithm","location","product","field","metric"]}
{"id":"211","dataset":"crossner_ai","split":"test","instance":{"id":"211","prompt_labels":"It(O) could(O) also(O) be(O) replaced(O) by(O) the(O) Log(B-metric) loss(I-metric) equation(O) below(O) :(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, person, country, product, conference, metric, organization, location, researcher, field, university, task, programming language and O.\nSentence: It could also be replaced by the Log loss equation below :","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","could","also","be","replaced","by","the","Log","loss","equation","below",":"],"labels":["O","O","O","O","O","O","O","B-metric","I-metric","O","O","O"],"target_index":null,"target_label":null},"label_list":["algorithm","person","country","product","conference","metric","organization","location","researcher","field","university","task","programming_language"]}
{"id":"213","dataset":"crossner_ai","split":"test","instance":{"id":"213","prompt_labels":"Given(O) a(O) set(O) of(O) predicted(O) values(O) and(O) a(O) corresponding(O) set(O) of(O) actual(O) values(O) for(O) X(O) for(O) various(O) time(O) periods(O) ,(O) a(O) common(O) evaluation(O) technique(O) is(O) to(O) use(O) the(O) mean(B-metric) squared(I-metric) prediction(I-metric) error(I-metric) ;(O) other(O) measures(O) are(O) also(O) available(O) ((O) see(O) forecasting(O) #(O) forecasting(B-metric) accuracy(I-metric) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, field, algorithm, location, organization, conference, university, researcher, product, person, task, programming language, country and O.\nSentence: Given a set of predicted values and a corresponding set of actual values for X for various time periods , a common evaluation technique is to use the mean squared prediction error ; other measures are also available ( see forecasting # forecasting accuracy ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Given","a","set","of","predicted","values","and","a","corresponding","set","of","actual","values","for","X","for","various","time","periods",",","a","common","evaluation","technique","is","to","use","the","mean","squared","prediction","error",";","other","measures","are","also","available","(","see","forecasting","#","forecasting","accuracy",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","I-metric","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","O","O"],"target_index":null,"target_label":null},"label_list":["metric","field","algorithm","location","organization","conference","university","researcher","product","person","task","programming_language","country"]}
{"id":"214","dataset":"crossner_ai","split":"test","instance":{"id":"214","prompt_labels":"Other(O) measures(O) ,(O) such(O) as(O) the(O) proportion(O) of(O) correct(O) predictions(O) ((O) also(O) termed(O) accuracy(B-metric) )(O) ,(O) are(O) not(O) useful(O) when(O) the(O) two(O) classes(O) are(O) of(O) very(O) different(O) sizes(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, person, task, metric, programming language, algorithm, product, researcher, field, country, conference, university and O.\nSentence: Other measures , such as the proportion of correct predictions ( also termed accuracy ) , are not useful when the two classes are of very different sizes .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","measures",",","such","as","the","proportion","of","correct","predictions","(","also","termed","accuracy",")",",","are","not","useful","when","the","two","classes","are","of","very","different","sizes","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","location","person","task","metric","programming_language","algorithm","product","researcher","field","country","conference","university"]}
{"id":"216","dataset":"crossner_ai","split":"test","instance":{"id":"216","prompt_labels":"Results(O) have(O) been(O) presented(O) which(O) give(O) correlation(O) of(O) up(O) to(O) 0.964(O) with(O) human(O) judgement(O) at(O) the(O) corpus(O) level(O) ,(O) compared(O) to(O) BLEU(B-metric) '(O) s(O) achievement(O) of(O) 0.817(O) on(O) the(O) same(O) data(O) set(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, university, location, organization, person, product, field, country, programming language, researcher, algorithm, metric, task and O.\nSentence: Results have been presented which give correlation of up to 0.964 with human judgement at the corpus level , compared to BLEU ' s achievement of 0.817 on the same data set .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Results","have","been","presented","which","give","correlation","of","up","to","0.964","with","human","judgement","at","the","corpus","level",",","compared","to","BLEU","'","s","achievement","of","0.817","on","the","same","data","set","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["conference","university","location","organization","person","product","field","country","programming_language","researcher","algorithm","metric","task"]}
{"id":"218","dataset":"crossner_ai","split":"test","instance":{"id":"218","prompt_labels":"For(O) example(O) ,(O) the(O) ambiguity(O) of(O) '(O) mouse(O) '(O) ((O) animal(O) or(O) device(O) )(O) is(O) not(O) relevant(O) in(O) machine(B-task) translation(I-task) ,(O) but(O) is(O) relevant(O) in(O) information(B-task) retrieval(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, university, person, metric, location, algorithm, programming language, product, field, researcher, organization, task, country and O.\nSentence: For example , the ambiguity of ' mouse ' ( animal or device ) is not relevant in machine translation , but is relevant in information retrieval .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","example",",","the","ambiguity","of","'","mouse","'","(","animal","or","device",")","is","not","relevant","in","machine","translation",",","but","is","relevant","in","information","retrieval","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-task","I-task","O","O","O","O","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["conference","university","person","metric","location","algorithm","programming_language","product","field","researcher","organization","task","country"]}
{"id":"219","dataset":"crossner_ai","split":"test","instance":{"id":"219","prompt_labels":"Geometric(B-algorithm) hashing(I-algorithm) was(O) originally(O) suggested(O) in(O) computer(B-field) vision(I-field) for(O) object(B-task) recognition(I-task) in(O) 2D(O) and(O) 3D(O) ,(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, programming language, product, location, conference, task, person, algorithm, researcher, university, field, metric, country and O.\nSentence: Geometric hashing was originally suggested in computer vision for object recognition in 2D and 3D ,","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Geometric","hashing","was","originally","suggested","in","computer","vision","for","object","recognition","in","2D","and","3D",","],"labels":["B-algorithm","I-algorithm","O","O","O","O","B-field","I-field","O","B-task","I-task","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","programming_language","product","location","conference","task","person","algorithm","researcher","university","field","metric","country"]}
{"id":"220","dataset":"crossner_ai","split":"test","instance":{"id":"220","prompt_labels":"It(O) forms(O) one(O) of(O) the(O) three(O) main(O) categories(O) of(O) machine(B-field) learning(I-field) ,(O) along(O) with(O) supervised(B-field) learning(I-field) and(O) reinforcement(B-field) learning(I-field) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, person, metric, country, task, conference, product, location, researcher, programming language, organization, university, field and O.\nSentence: It forms one of the three main categories of machine learning , along with supervised learning and reinforcement learning .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","forms","one","of","the","three","main","categories","of","machine","learning",",","along","with","supervised","learning","and","reinforcement","learning","."],"labels":["O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","O","B-field","I-field","O","B-field","I-field","O"],"target_index":null,"target_label":null},"label_list":["algorithm","person","metric","country","task","conference","product","location","researcher","programming_language","organization","university","field"]}
{"id":"221","dataset":"crossner_ai","split":"test","instance":{"id":"221","prompt_labels":"Reinforcement(B-field) learning(I-field) ,(O) due(O) to(O) its(O) generality(O) ,(O) is(O) studied(O) in(O) many(O) other(O) disciplines(O) ,(O) such(O) as(O) game(B-field) ,(O) control(B-field) theory(I-field) ,(O) operations(B-field) research(I-field) ,(O) information(B-field) theory(I-field) ,(O) simulation-based(B-field) optimization(I-field) ,(O) multi-agent(B-field) systems(I-field) ,(O) swarm(B-field) intelligence(I-field) ,(O) statistics(B-field) and(O) genetic(B-algorithm) algorithm(I-algorithm) s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, product, person, algorithm, country, researcher, organization, task, location, metric, field, programming language, university and O.\nSentence: Reinforcement learning , due to its generality , is studied in many other disciplines , such as game , control theory , operations research , information theory , simulation-based optimization , multi-agent systems , swarm intelligence , statistics and genetic algorithm s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Reinforcement","learning",",","due","to","its","generality",",","is","studied","in","many","other","disciplines",",","such","as","game",",","control","theory",",","operations","research",",","information","theory",",","simulation-based","optimization",",","multi-agent","systems",",","swarm","intelligence",",","statistics","and","genetic","algorithm","s","."],"labels":["B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","O","B-field","I-field","O","B-field","I-field","O","B-field","I-field","O","B-field","I-field","O","B-field","I-field","O","B-field","I-field","O","B-field","O","B-algorithm","I-algorithm","O","O"],"target_index":null,"target_label":null},"label_list":["conference","product","person","algorithm","country","researcher","organization","task","location","metric","field","programming_language","university"]}
{"id":"222","dataset":"crossner_ai","split":"test","instance":{"id":"222","prompt_labels":"Pattern(B-field) recognition(I-field) is(O) closely(O) related(O) to(O) artificial(B-field) intelligence(I-field) and(O) machine(B-field) learning(I-field) ,(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, university, organization, task, person, country, location, conference, researcher, product, algorithm, metric, field and O.\nSentence: Pattern recognition is closely related to artificial intelligence and machine learning ,","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Pattern","recognition","is","closely","related","to","artificial","intelligence","and","machine","learning",","],"labels":["B-field","I-field","O","O","O","O","B-field","I-field","O","B-field","I-field","O"],"target_index":null,"target_label":null},"label_list":["programming_language","university","organization","task","person","country","location","conference","researcher","product","algorithm","metric","field"]}
{"id":"224","dataset":"crossner_ai","split":"test","instance":{"id":"224","prompt_labels":"In(O) 2016(O) ,(O) he(O) was(O) elected(O) Fellow(O) of(O) Association(B-conference) for(I-conference) the(I-conference) Advancement(I-conference) of(I-conference) Artificial(I-conference) Intelligence(I-conference) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, field, conference, country, person, university, metric, algorithm, product, organization, location, task, programming language and O.\nSentence: In 2016 , he was elected Fellow of Association for the Advancement of Artificial Intelligence .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2016",",","he","was","elected","Fellow","of","Association","for","the","Advancement","of","Artificial","Intelligence","."],"labels":["O","O","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O"],"target_index":null,"target_label":null},"label_list":["researcher","field","conference","country","person","university","metric","algorithm","product","organization","location","task","programming_language"]}
{"id":"226","dataset":"crossner_ai","split":"test","instance":{"id":"226","prompt_labels":"During(O) the(O) 1973(O) Yom(O) Kippur(O) War(O) ,(O) Soviet-supplied(O) surface-to-air(B-product) missile(I-product) batteries(O) in(O) Egypt(B-country) and(O) Syria(B-country) caused(O) heavy(O) damage(O) Israeli(O) fighter(O) jet(O) s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, product, conference, programming language, organization, country, researcher, university, field, algorithm, metric, task and O.\nSentence: During the 1973 Yom Kippur War , Soviet-supplied surface-to-air missile batteries in Egypt and Syria caused heavy damage Israeli fighter jet s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","the","1973","Yom","Kippur","War",",","Soviet-supplied","surface-to-air","missile","batteries","in","Egypt","and","Syria","caused","heavy","damage","Israeli","fighter","jet","s","."],"labels":["O","O","O","O","O","O","O","O","B-product","I-product","O","O","B-country","O","B-country","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","location","product","conference","programming_language","organization","country","researcher","university","field","algorithm","metric","task"]}
{"id":"227","dataset":"crossner_ai","split":"test","instance":{"id":"227","prompt_labels":"Another(O) resource(O) ((O) free(O) but(O) copyrighted(O) )(O) is(O) the(O) HTK(B-product) book(I-product) ((O) and(O) the(O) accompanying(O) HTK(B-product) toolkit(I-product) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, person, researcher, location, country, metric, programming language, algorithm, task, product, conference, organization, field and O.\nSentence: Another resource ( free but copyrighted ) is the HTK book ( and the accompanying HTK toolkit ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Another","resource","(","free","but","copyrighted",")","is","the","HTK","book","(","and","the","accompanying","HTK","toolkit",")","."],"labels":["O","O","O","O","O","O","O","O","O","B-product","I-product","O","O","O","O","B-product","I-product","O","O"],"target_index":null,"target_label":null},"label_list":["university","person","researcher","location","country","metric","programming_language","algorithm","task","product","conference","organization","field"]}
{"id":"229","dataset":"crossner_ai","split":"test","instance":{"id":"229","prompt_labels":"A(O) single(O) grid(O) can(O) be(O) analysed(O) for(O) both(O) content(O) ((O) eyeball(B-task) inspection(I-task) )(O) and(O) structure(O) ((O) cluster(B-task) analysis(I-task) ,(O) principal(B-task) component(I-task) analysis(I-task) ,(O) and(O) a(O) variety(O) of(O) structural(O) indices(O) relating(O) to(O) the(O) complexity(O) and(O) range(O) of(O) the(O) ratings(O) being(O) the(O) chief(O) techniques(O) used(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, algorithm, product, researcher, country, person, metric, university, conference, field, programming language, organization, location and O.\nSentence: A single grid can be analysed for both content ( eyeball inspection ) and structure ( cluster analysis , principal component analysis , and a variety of structural indices relating to the complexity and range of the ratings being the chief techniques used ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","single","grid","can","be","analysed","for","both","content","(","eyeball","inspection",")","and","structure","(","cluster","analysis",",","principal","component","analysis",",","and","a","variety","of","structural","indices","relating","to","the","complexity","and","range","of","the","ratings","being","the","chief","techniques","used",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-task","I-task","O","O","O","O","B-task","I-task","O","B-task","I-task","I-task","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["task","algorithm","product","researcher","country","person","metric","university","conference","field","programming_language","organization","location"]}
{"id":"231","dataset":"crossner_ai","split":"test","instance":{"id":"231","prompt_labels":"Such(O) targets(O) include(O) natural(O) objects(O) such(O) as(O) ground(O) ,(O) sea(O) ,(O) precipitation(O) ((O) such(O) as(O) rain(O) ,(O) snow(O) or(O) hail(O) )(O) ,(O) sand(O) storm(O) s(O) ,(O) animals(O) ((O) especially(O) birds(O) )(O) ,(O) atmospheric(O) turbulence(O) ,(O) and(O) other(O) atmospheric(O) effects(O) ,(O) such(O) as(O) ionosphere(O) reflections(O) ,(O) meteor(O) trails(O) ,(O) and(O) three(O) body(O) scatter(O) spike(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, university, country, person, field, researcher, product, location, conference, organization, task, programming language, algorithm and O.\nSentence: Such targets include natural objects such as ground , sea , precipitation ( such as rain , snow or hail ) , sand storm s , animals ( especially birds ) , atmospheric turbulence , and other atmospheric effects , such as ionosphere reflections , meteor trails , and three body scatter spike .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Such","targets","include","natural","objects","such","as","ground",",","sea",",","precipitation","(","such","as","rain",",","snow","or","hail",")",",","sand","storm","s",",","animals","(","especially","birds",")",",","atmospheric","turbulence",",","and","other","atmospheric","effects",",","such","as","ionosphere","reflections",",","meteor","trails",",","and","three","body","scatter","spike","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","university","country","person","field","researcher","product","location","conference","organization","task","programming_language","algorithm"]}
{"id":"232","dataset":"crossner_ai","split":"test","instance":{"id":"232","prompt_labels":"In(O) planning(O) and(O) control(O) ,(O) the(O) essential(O) difference(O) between(O) humanoids(O) and(O) other(O) kinds(O) of(O) robots(O) ((O) like(O) industrial(B-product) ones(O) )(O) is(O) that(O) the(O) movement(O) of(O) the(O) robot(O) must(O) be(O) human-like(O) ,(O) using(O) legged(O) locomotion(O) ,(O) especially(O) biped(O) gait(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, task, researcher, organization, programming language, person, location, university, algorithm, country, conference, product, field and O.\nSentence: In planning and control , the essential difference between humanoids and other kinds of robots ( like industrial ones ) is that the movement of the robot must be human-like , using legged locomotion , especially biped gait .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","planning","and","control",",","the","essential","difference","between","humanoids","and","other","kinds","of","robots","(","like","industrial","ones",")","is","that","the","movement","of","the","robot","must","be","human-like",",","using","legged","locomotion",",","especially","biped","gait","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","task","researcher","organization","programming_language","person","location","university","algorithm","country","conference","product","field"]}
{"id":"233","dataset":"crossner_ai","split":"test","instance":{"id":"233","prompt_labels":"The(O) gradient(B-algorithm) descent(I-algorithm) can(O) take(O) many(O) iterations(O) to(O) compute(O) a(O) local(O) minimum(O) with(O) a(O) required(O) accuracy(B-metric) ,(O) if(O) the(O) curvature(O) in(O) different(O) directions(O) is(O) very(O) different(O) for(O) the(O) given(O) function(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, conference, algorithm, organization, person, university, researcher, programming language, field, product, location, country, metric and O.\nSentence: The gradient descent can take many iterations to compute a local minimum with a required accuracy , if the curvature in different directions is very different for the given function .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","gradient","descent","can","take","many","iterations","to","compute","a","local","minimum","with","a","required","accuracy",",","if","the","curvature","in","different","directions","is","very","different","for","the","given","function","."],"labels":["O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["task","conference","algorithm","organization","person","university","researcher","programming_language","field","product","location","country","metric"]}
{"id":"234","dataset":"crossner_ai","split":"test","instance":{"id":"234","prompt_labels":"The(O) 1997(O) RoboCup(O) 2D(O) Soccer(O) Simulation(O) League(O) was(O) the(O) first(O) RoboCup(O) competition(O) promoted(O) in(O) conjunction(O) with(O) International(B-conference) Joint(I-conference) Conference(I-conference) on(I-conference) Artificial(I-conference) Intelligence(I-conference) held(O) in(O) Nagoya(B-location) ,(O) Japan(B-country) ,(O) from(O) 23(O) to(O) 29(O) August(O) 1997(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, university, researcher, programming language, task, product, organization, field, country, conference, metric, person, location and O.\nSentence: The 1997 RoboCup 2D Soccer Simulation League was the first RoboCup competition promoted in conjunction with International Joint Conference on Artificial Intelligence held in Nagoya , Japan , from 23 to 29 August 1997 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","1997","RoboCup","2D","Soccer","Simulation","League","was","the","first","RoboCup","competition","promoted","in","conjunction","with","International","Joint","Conference","on","Artificial","Intelligence","held","in","Nagoya",",","Japan",",","from","23","to","29","August","1997","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","O","B-location","O","B-country","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["algorithm","university","researcher","programming_language","task","product","organization","field","country","conference","metric","person","location"]}
{"id":"235","dataset":"crossner_ai","split":"test","instance":{"id":"235","prompt_labels":"Other(O) programming(O) options(O) include(O) an(O) embedded(O) Python(B-programming language) environment(O) ,(O) and(O) an(O) R(B-programming language) Console(O) plus(O) support(O) for(O) Rserve(B-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, task, algorithm, country, location, organization, person, metric, programming language, field, product, researcher, conference and O.\nSentence: Other programming options include an embedded Python environment , and an R Console plus support for Rserve .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","programming","options","include","an","embedded","Python","environment",",","and","an","R","Console","plus","support","for","Rserve","."],"labels":["O","O","O","O","O","O","B-programming language","O","O","O","O","B-programming language","O","O","O","O","B-product","O"],"target_index":null,"target_label":null},"label_list":["university","task","algorithm","country","location","organization","person","metric","programming_language","field","product","researcher","conference"]}
{"id":"236","dataset":"crossner_ai","split":"test","instance":{"id":"236","prompt_labels":"From(O) Bonn(B-researcher) he(O) has(O) contributed(O) fundamentally(O) to(O) artificial(B-field) intelligence(I-field) and(O) robotics(B-field) ((O) with(O) Wolfram(B-researcher) Burgard(I-researcher) ,(O) Dieter(B-researcher) Fox(I-researcher) ,(O) Sebastian(B-researcher) Thrun(I-researcher) among(O) his(O) students(O) )(O) ,(O) and(O) to(O) the(O) development(O) of(O) software(B-field) engineering(I-field) ,(O) particularly(O) in(O) civil(B-field) engineering(I-field) ,(O) and(O) information(B-field) systems(I-field) ,(O) particularly(O) in(O) the(O) geosciences.(B-field) won(O) the(O) AAAI(O) Classic(O) Paper(O) award(O) of(O) 2016.2014(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, task, product, location, university, programming language, conference, organization, country, researcher, person, algorithm, metric and O.\nSentence: From Bonn he has contributed fundamentally to artificial intelligence and robotics ( with Wolfram Burgard , Dieter Fox , Sebastian Thrun among his students ) , and to the development of software engineering , particularly in civil engineering , and information systems , particularly in the geosciences. won the AAAI Classic Paper award of 2016.2014 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["From","Bonn","he","has","contributed","fundamentally","to","artificial","intelligence","and","robotics","(","with","Wolfram","Burgard",",","Dieter","Fox",",","Sebastian","Thrun","among","his","students",")",",","and","to","the","development","of","software","engineering",",","particularly","in","civil","engineering",",","and","information","systems",",","particularly","in","the","geosciences.","won","the","AAAI","Classic","Paper","award","of","2016.2014","."],"labels":["O","B-researcher","O","O","O","O","O","B-field","I-field","O","B-field","O","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","O","B-field","I-field","O","O","B-field","I-field","O","O","O","O","B-field","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","task","product","location","university","programming_language","conference","organization","country","researcher","person","algorithm","metric"]}
{"id":"237","dataset":"crossner_ai","split":"test","instance":{"id":"237","prompt_labels":"The(O) first(O) USA(B-conference) edition(I-conference) of(I-conference) Campus(I-conference) Party(I-conference) will(O) take(O) place(O) from(O) 20(O) to(O) 22(O) of(O) August(O) at(O) TCF(B-location) Center(I-location) in(O) Detroit(B-location) ,(O) Michigan(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, location, metric, field, researcher, programming language, country, organization, person, conference, university, product, algorithm and O.\nSentence: The first USA edition of Campus Party will take place from 20 to 22 of August at TCF Center in Detroit , Michigan .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","first","USA","edition","of","Campus","Party","will","take","place","from","20","to","22","of","August","at","TCF","Center","in","Detroit",",","Michigan","."],"labels":["O","O","B-conference","I-conference","I-conference","I-conference","I-conference","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["task","location","metric","field","researcher","programming_language","country","organization","person","conference","university","product","algorithm"]}
{"id":"239","dataset":"crossner_ai","split":"test","instance":{"id":"239","prompt_labels":"Euler(B-product) Math(I-product) Toolbox(I-product) uses(O) a(O) matrix(O) language(O) similar(O) to(O) MATLAB(B-product) ,(O) a(O) system(O) that(O) had(O) been(O) under(O) development(O) since(O) the(O) 1970s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, metric, product, task, location, programming language, algorithm, university, conference, field, country, organization, researcher and O.\nSentence: Euler Math Toolbox uses a matrix language similar to MATLAB , a system that had been under development since the 1970s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Euler","Math","Toolbox","uses","a","matrix","language","similar","to","MATLAB",",","a","system","that","had","been","under","development","since","the","1970s","."],"labels":["B-product","I-product","I-product","O","O","O","O","O","O","B-product","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","metric","product","task","location","programming_language","algorithm","university","conference","field","country","organization","researcher"]}
{"id":"240","dataset":"crossner_ai","split":"test","instance":{"id":"240","prompt_labels":"Some(O) languages(O) make(O) it(O) possible(O) portably(O) ((O) e.g.(O) Scheme(B-programming language) ,(O) Common(B-programming language) Lisp(I-programming language) ,(O) Perl(B-programming language) or(O) D(B-programming language) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, task, field, algorithm, researcher, programming language, metric, organization, conference, country, product, location, person and O.\nSentence: Some languages make it possible portably ( e.g. Scheme , Common Lisp , Perl or D ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","languages","make","it","possible","portably","(","e.g.","Scheme",",","Common","Lisp",",","Perl","or","D",")","."],"labels":["O","O","O","O","O","O","O","O","B-programming language","O","B-programming language","I-programming language","O","B-programming language","O","B-programming language","O","O"],"target_index":null,"target_label":null},"label_list":["university","task","field","algorithm","researcher","programming_language","metric","organization","conference","country","product","location","person"]}
{"id":"241","dataset":"crossner_ai","split":"test","instance":{"id":"241","prompt_labels":"In(O) 1969(O) a(O) famous(O) book(O) entitled(O) Perceptrons(O) by(O) Marvin(B-researcher) Minsky(I-researcher) and(O) Seymour(B-researcher) Papert(I-researcher) showed(O) that(O) it(O) was(O) impossible(O) for(O) these(O) classes(O) of(O) network(O) to(O) learn(O) an(O) XOR(O) function(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, algorithm, location, programming language, metric, country, field, university, task, conference, researcher, product and O.\nSentence: In 1969 a famous book entitled Perceptrons by Marvin Minsky and Seymour Papert showed that it was impossible for these classes of network to learn an XOR function .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1969","a","famous","book","entitled","Perceptrons","by","Marvin","Minsky","and","Seymour","Papert","showed","that","it","was","impossible","for","these","classes","of","network","to","learn","an","XOR","function","."],"labels":["O","O","O","O","O","O","O","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","organization","algorithm","location","programming_language","metric","country","field","university","task","conference","researcher","product"]}
{"id":"245","dataset":"crossner_ai","split":"test","instance":{"id":"245","prompt_labels":"The(O) Cleveland(B-location) Clinic(I-location) has(O) used(O) Cyc(B-product) to(O) develop(O) a(O) natural(B-product) language(I-product) query(I-product) interface(I-product) of(I-product) biomedical(I-product) information(I-product) ,(O) spanning(O) decades(O) of(O) information(O) on(O) cardiothoracic(O) surgeries(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, programming language, researcher, country, task, location, algorithm, conference, field, product, metric, university, organization and O.\nSentence: The Cleveland Clinic has used Cyc to develop a natural language query interface of biomedical information , spanning decades of information on cardiothoracic surgeries .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Cleveland","Clinic","has","used","Cyc","to","develop","a","natural","language","query","interface","of","biomedical","information",",","spanning","decades","of","information","on","cardiothoracic","surgeries","."],"labels":["O","B-location","I-location","O","O","B-product","O","O","O","B-product","I-product","I-product","I-product","I-product","I-product","I-product","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","programming_language","researcher","country","task","location","algorithm","conference","field","product","metric","university","organization"]}
{"id":"247","dataset":"crossner_ai","split":"test","instance":{"id":"247","prompt_labels":"If(O) the(O) modeling(O) is(O) done(O) by(O) an(O) artificial(B-algorithm) neural(I-algorithm) network(I-algorithm) or(O) other(O) machine(B-field) learning(I-field) ,(O) the(O) optimization(O) of(O) parameters(O) is(O) called(O) training(O) ,(O) while(O) the(O) optimization(O) of(O) model(O) hyperparameters(O) is(O) called(O) tuning(O) and(O) often(O) uses(O) cross-validation(B-algorithm) ..(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, task, person, field, programming language, conference, country, researcher, organization, metric, algorithm, product, university and O.\nSentence: If the modeling is done by an artificial neural network or other machine learning , the optimization of parameters is called training , while the optimization of model hyperparameters is called tuning and often uses cross-validation ..","prediction_output":null,"prediction_outputs":null,"group":null,"words":["If","the","modeling","is","done","by","an","artificial","neural","network","or","other","machine","learning",",","the","optimization","of","parameters","is","called","training",",","while","the","optimization","of","model","hyperparameters","is","called","tuning","and","often","uses","cross-validation",".."],"labels":["O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","O"],"target_index":null,"target_label":null},"label_list":["location","task","person","field","programming_language","conference","country","researcher","organization","metric","algorithm","product","university"]}
{"id":"252","dataset":"crossner_ai","split":"test","instance":{"id":"252","prompt_labels":"He(O) received(O) a(O) B.E.(O) in(O) electronics(B-field) engineering(I-field) from(O) B.M.S.(B-university) College(I-university) of(I-university) Engineering(I-university) in(O) Bangalore(B-location) ,(O) India(B-country) in(O) 1982(O) ,(O) when(O) it(O) was(O) affiliated(O) with(O) Bangalore(B-university) University(I-university) ,(O) an(O) M.S.(O) in(O) electrical(B-field) and(I-field) computer(I-field) engineering(I-field) in(O) 1984(O) from(O) Drexel(B-university) University(I-university) ,(O) and(O) an(O) M.S.(O) in(O) computer(B-field) science(I-field) in(O) 1989(O) ,(O) and(O) a(O) Ph.D.(O) in(O) 1990(O) ,(O) respectively(O) ,(O) from(O) the(O) University(B-university) of(I-university) Wisconsin-Madison(I-university) ,(O) where(O) he(O) studied(O) Artificial(B-field) Intelligence(I-field) and(O) worked(O) with(O) Leonard(B-researcher) Uhr(I-researcher) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, metric, task, algorithm, organization, university, person, programming language, field, country, conference, location, researcher and O.\nSentence: He received a B.E. in electronics engineering from B.M.S. College of Engineering in Bangalore , India in 1982 , when it was affiliated with Bangalore University , an M.S. in electrical and computer engineering in 1984 from Drexel University , and an M.S. in computer science in 1989 , and a Ph.D. in 1990 , respectively , from the University of Wisconsin-Madison , where he studied Artificial Intelligence and worked with Leonard Uhr .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","received","a","B.E.","in","electronics","engineering","from","B.M.S.","College","of","Engineering","in","Bangalore",",","India","in","1982",",","when","it","was","affiliated","with","Bangalore","University",",","an","M.S.","in","electrical","and","computer","engineering","in","1984","from","Drexel","University",",","and","an","M.S.","in","computer","science","in","1989",",","and","a","Ph.D.","in","1990",",","respectively",",","from","the","University","of","Wisconsin-Madison",",","where","he","studied","Artificial","Intelligence","and","worked","with","Leonard","Uhr","."],"labels":["O","O","O","O","O","B-field","I-field","O","B-university","I-university","I-university","I-university","O","B-location","O","B-country","O","O","O","O","O","O","O","O","B-university","I-university","O","O","O","O","B-field","I-field","I-field","I-field","O","O","O","B-university","I-university","O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","B-university","I-university","I-university","O","O","O","O","B-field","I-field","O","O","O","B-researcher","I-researcher","O"],"target_index":null,"target_label":null},"label_list":["product","metric","task","algorithm","organization","university","person","programming_language","field","country","conference","location","researcher"]}
{"id":"254","dataset":"crossner_ai","split":"test","instance":{"id":"254","prompt_labels":"In(O) 1971(O) Terry(B-researcher) Winograd(I-researcher) developed(O) an(O) early(O) natural(B-field) language(I-field) processing(I-field) engine(O) capable(O) of(O) interpreting(O) naturally(O) written(O) commands(O) within(O) a(O) simple(O) rule-governed(O) environment(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, person, task, organization, university, metric, researcher, programming language, product, conference, algorithm, field and O.\nSentence: In 1971 Terry Winograd developed an early natural language processing engine capable of interpreting naturally written commands within a simple rule-governed environment .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1971","Terry","Winograd","developed","an","early","natural","language","processing","engine","capable","of","interpreting","naturally","written","commands","within","a","simple","rule-governed","environment","."],"labels":["O","O","B-researcher","I-researcher","O","O","O","B-field","I-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","country","person","task","organization","university","metric","researcher","programming_language","product","conference","algorithm","field"]}
{"id":"255","dataset":"crossner_ai","split":"test","instance":{"id":"255","prompt_labels":"In(O) artificial(B-field) intelligence(I-field) ,(O) Marvin(B-researcher) Minsky(I-researcher) ,(O) Herbert(B-researcher) A.(I-researcher) Simon(I-researcher) ,(O) and(O) Allen(B-researcher) Newell(I-researcher) are(O) prominent(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, product, programming language, field, person, algorithm, university, organization, researcher, country, task, location, conference and O.\nSentence: In artificial intelligence , Marvin Minsky , Herbert A. Simon , and Allen Newell are prominent .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","artificial","intelligence",",","Marvin","Minsky",",","Herbert","A.","Simon",",","and","Allen","Newell","are","prominent","."],"labels":["O","B-field","I-field","O","B-researcher","I-researcher","O","B-researcher","I-researcher","I-researcher","O","O","B-researcher","I-researcher","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","product","programming_language","field","person","algorithm","university","organization","researcher","country","task","location","conference"]}
{"id":"256","dataset":"crossner_ai","split":"test","instance":{"id":"256","prompt_labels":"In(O) the(O) latter(O) half(O) of(O) the(O) 20th(O) century(O) ,(O) electrical(B-field) engineering(I-field) itself(O) separated(O) into(O) several(O) disciplines(O) ,(O) specialising(O) in(O) the(O) design(O) and(O) analysis(O) of(O) systems(O) that(O) manipulate(O) physical(O) signals(O) ;(O) electronic(B-field) engineering(I-field) and(O) computer(B-field) engineering(I-field) as(O) examples(O) ;(O) while(O) design(B-field) engineering(I-field) developed(O) to(O) deal(O) with(O) functional(O) design(O) of(O) user-machine(O) interfaces(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, university, country, product, researcher, location, person, task, organization, metric, algorithm, conference, field and O.\nSentence: In the latter half of the 20th century , electrical engineering itself separated into several disciplines , specialising in the design and analysis of systems that manipulate physical signals ; electronic engineering and computer engineering as examples ; while design engineering developed to deal with functional design of user-machine interfaces .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","latter","half","of","the","20th","century",",","electrical","engineering","itself","separated","into","several","disciplines",",","specialising","in","the","design","and","analysis","of","systems","that","manipulate","physical","signals",";","electronic","engineering","and","computer","engineering","as","examples",";","while","design","engineering","developed","to","deal","with","functional","design","of","user-machine","interfaces","."],"labels":["O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","B-field","I-field","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","university","country","product","researcher","location","person","task","organization","metric","algorithm","conference","field"]}
{"id":"257","dataset":"crossner_ai","split":"test","instance":{"id":"257","prompt_labels":"Perhaps(O) the(O) simplest(O) statistic(O) is(O) accuracy(B-metric) or(O) Fraction(B-metric) Correct(I-metric) ((O) FC(B-metric) )(O) ,(O) which(O) measures(O) the(O) fraction(O) of(O) all(O) instances(O) that(O) are(O) correctly(O) categorized(O) ;(O) it(O) is(O) the(O) ratio(O) of(O) the(O) number(O) of(O) correct(O) classifications(O) to(O) the(O) total(O) number(O) of(O) correct(O) or(O) incorrect(O) classifications(O) :(O) ((O) TP(B-metric) +(I-metric) TN(I-metric) )(O) /(O) Total(O) Population(O) =(O) ((O) TP(B-metric) +(I-metric) TN(I-metric) )(O) /(O) ((O) TP(B-metric) +(I-metric) TN(I-metric) +(I-metric) FP(I-metric) +(I-metric) FN(I-metric) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, researcher, conference, field, product, person, university, algorithm, organization, task, country, programming language, metric and O.\nSentence: Perhaps the simplest statistic is accuracy or Fraction Correct ( FC ) , which measures the fraction of all instances that are correctly categorized ; it is the ratio of the number of correct classifications to the total number of correct or incorrect classifications : ( TP + TN ) / Total Population = ( TP + TN ) / ( TP + TN + FP + FN ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Perhaps","the","simplest","statistic","is","accuracy","or","Fraction","Correct","(","FC",")",",","which","measures","the","fraction","of","all","instances","that","are","correctly","categorized",";","it","is","the","ratio","of","the","number","of","correct","classifications","to","the","total","number","of","correct","or","incorrect","classifications",":","(","TP","+","TN",")","/","Total","Population","=","(","TP","+","TN",")","/","(","TP","+","TN","+","FP","+","FN",")","."],"labels":["O","O","O","O","O","B-metric","O","B-metric","I-metric","O","B-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O","O","B-metric","I-metric","I-metric","I-metric","I-metric","I-metric","I-metric","O","O"],"target_index":null,"target_label":null},"label_list":["location","researcher","conference","field","product","person","university","algorithm","organization","task","country","programming_language","metric"]}
{"id":"258","dataset":"crossner_ai","split":"test","instance":{"id":"258","prompt_labels":"In(O) the(O) academic(O) community(O) ,(O) the(O) major(O) forums(O) for(O) research(O) started(O) in(O) 1995(O) when(O) the(O) First(B-conference) International(I-conference) Conference(I-conference) Data(I-conference) Mining(I-conference) and(I-conference) Knowledge(I-conference) Discovery(I-conference) ((O) KDD-95(B-conference) )(O) was(O) started(O) in(O) Montreal(B-location) under(O) AAAI(B-conference) sponsorship(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, task, programming language, person, product, algorithm, country, location, university, organization, conference, researcher, field and O.\nSentence: In the academic community , the major forums for research started in 1995 when the First International Conference Data Mining and Knowledge Discovery ( KDD-95 ) was started in Montreal under AAAI sponsorship .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","academic","community",",","the","major","forums","for","research","started","in","1995","when","the","First","International","Conference","Data","Mining","and","Knowledge","Discovery","(","KDD-95",")","was","started","in","Montreal","under","AAAI","sponsorship","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","B-conference","O","O","O","O","B-location","O","B-conference","O","O"],"target_index":null,"target_label":null},"label_list":["metric","task","programming_language","person","product","algorithm","country","location","university","organization","conference","researcher","field"]}
{"id":"260","dataset":"crossner_ai","split":"test","instance":{"id":"260","prompt_labels":"In(O) light(O) of(O) the(O) above(O) discussion(O) ,(O) we(O) see(O) that(O) the(O) SVM(B-algorithm) technique(O) is(O) equivalent(O) to(O) empirical(B-algorithm) risk(I-algorithm) with(O) Tikhonov(B-algorithm) regularization(I-algorithm) ,(O) where(O) in(O) this(O) case(O) the(O) loss(O) function(O) is(O) the(O) hinge(B-metric) loss(I-metric)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, conference, organization, university, location, researcher, algorithm, product, country, person, programming language, metric, field and O.\nSentence: In light of the above discussion , we see that the SVM technique is equivalent to empirical risk with Tikhonov regularization , where in this case the loss function is the hinge loss","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","light","of","the","above","discussion",",","we","see","that","the","SVM","technique","is","equivalent","to","empirical","risk","with","Tikhonov","regularization",",","where","in","this","case","the","loss","function","is","the","hinge","loss"],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-algorithm","O","O","O","O","B-algorithm","I-algorithm","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric"],"target_index":null,"target_label":null},"label_list":["task","conference","organization","university","location","researcher","algorithm","product","country","person","programming_language","metric","field"]}
{"id":"265","dataset":"crossner_ai","split":"test","instance":{"id":"265","prompt_labels":"A(O) widely(O) used(O) type(O) of(O) composition(O) is(O) the(O) nonlinear(B-algorithm) weighted(I-algorithm) sum(I-algorithm) ,(O) where(O) math(O) \\(O) textstyle(O) f(O) ((O) x(O) )(O) =(O) K(O) \\(O) left(O) ((O) \\(O) sum(O) _(O) i(O) w(O) _(O) i(O) g(O) _(O) i(O) ((O) x(O) )(O) \\(O) right(O) )(O) /(O) math(O) ,(O) where(O) math(O) \\(O) textstyle(O) K(O) /(O) math(O) ((O) commonly(O) referred(O) to(O) as(O) the(O) activation(O) function(O) )(O) is(O) some(O) predefined(O) function(O) ,(O) such(O) as(O) the(O) hyperbolic(B-algorithm) tangent(I-algorithm) ,(O) sigmoid(B-algorithm) function(I-algorithm) ,(O) softmax(B-algorithm) function(I-algorithm) ,(O) or(O) rectifier(B-algorithm) function(I-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, programming language, researcher, country, field, university, conference, task, product, organization, metric, location, algorithm and O.\nSentence: A widely used type of composition is the nonlinear weighted sum , where math \\ textstyle f ( x ) = K \\ left ( \\ sum _ i w _ i g _ i ( x ) \\ right ) / math , where math \\ textstyle K / math ( commonly referred to as the activation function ) is some predefined function , such as the hyperbolic tangent , sigmoid function , softmax function , or rectifier function .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","widely","used","type","of","composition","is","the","nonlinear","weighted","sum",",","where","math","\\","textstyle","f","(","x",")","=","K","\\","left","(","\\","sum","_","i","w","_","i","g","_","i","(","x",")","\\","right",")","/","math",",","where","math","\\","textstyle","K","/","math","(","commonly","referred","to","as","the","activation","function",")","is","some","predefined","function",",","such","as","the","hyperbolic","tangent",",","sigmoid","function",",","softmax","function",",","or","rectifier","function","."],"labels":["O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","B-algorithm","I-algorithm","O","B-algorithm","I-algorithm","O","O","B-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["person","programming_language","researcher","country","field","university","conference","task","product","organization","metric","location","algorithm"]}
{"id":"266","dataset":"crossner_ai","split":"test","instance":{"id":"266","prompt_labels":"In(O) the(O) film(O) Westworld(O) ,(O) female(O) robots(O) actually(O) engaged(O) in(O) intercourse(O) with(O) human(O) men(O) as(O) part(O) of(O) the(O) make-believe(O) vacation(O) world(O) human(O) customers(O) paid(O) to(O) attend(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, conference, field, metric, organization, product, location, algorithm, task, university, person, country, researcher and O.\nSentence: In the film Westworld , female robots actually engaged in intercourse with human men as part of the make-believe vacation world human customers paid to attend .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","film","Westworld",",","female","robots","actually","engaged","in","intercourse","with","human","men","as","part","of","the","make-believe","vacation","world","human","customers","paid","to","attend","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","conference","field","metric","organization","product","location","algorithm","task","university","person","country","researcher"]}
{"id":"267","dataset":"crossner_ai","split":"test","instance":{"id":"267","prompt_labels":"Typically(O) ,(O) the(O) process(O) starts(O) by(O) terminology(B-task) extraction(I-task) and(O) concepts(O) or(O) noun(O) phrase(O) s(O) from(O) plain(O) text(O) using(O) linguistic(O) processors(O) such(O) as(O) part-of-speech(B-task) tagging(I-task) and(O) phrase(B-task) chunking(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, product, task, programming language, university, metric, country, researcher, location, person, field, organization, conference and O.\nSentence: Typically , the process starts by terminology extraction and concepts or noun phrase s from plain text using linguistic processors such as part-of-speech tagging and phrase chunking .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Typically",",","the","process","starts","by","terminology","extraction","and","concepts","or","noun","phrase","s","from","plain","text","using","linguistic","processors","such","as","part-of-speech","tagging","and","phrase","chunking","."],"labels":["O","O","O","O","O","O","B-task","I-task","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-task","I-task","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["algorithm","product","task","programming_language","university","metric","country","researcher","location","person","field","organization","conference"]}
{"id":"269","dataset":"crossner_ai","split":"test","instance":{"id":"269","prompt_labels":"While(O) studying(O) at(O) Stanford(B-university) ,(O) Scheinman(B-researcher) was(O) awarded(O) a(O) fellowship(O) sponsored(O) by(O) George(B-researcher) Devol(I-researcher) ,(O) the(O) inventor(O) of(O) the(O) Unimate(B-product) ,(O) the(O) first(O) industrial(B-product) robot(I-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, university, country, researcher, programming language, field, person, location, organization, conference, algorithm, task, metric and O.\nSentence: While studying at Stanford , Scheinman was awarded a fellowship sponsored by George Devol , the inventor of the Unimate , the first industrial robot .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["While","studying","at","Stanford",",","Scheinman","was","awarded","a","fellowship","sponsored","by","George","Devol",",","the","inventor","of","the","Unimate",",","the","first","industrial","robot","."],"labels":["O","O","O","B-university","O","B-researcher","O","O","O","O","O","O","B-researcher","I-researcher","O","O","O","O","O","B-product","O","O","O","B-product","I-product","O"],"target_index":null,"target_label":null},"label_list":["product","university","country","researcher","programming_language","field","person","location","organization","conference","algorithm","task","metric"]}
{"id":"272","dataset":"crossner_ai","split":"test","instance":{"id":"272","prompt_labels":"Much(O) of(O) the(O) confusion(O) between(O) these(O) two(O) research(O) communities(O) ((O) which(O) do(O) often(O) have(O) separate(O) conferences(O) and(O) separate(O) journals(O) ,(O) ECML(B-conference) PKDD(I-conference) being(O) a(O) major(O) exception(O) )(O) comes(O) from(O) the(O) basic(O) assumptions(O) they(O) work(O) with(O) :(O) in(O) machine(B-field) learning(I-field) ,(O) performance(O) is(O) usually(O) evaluated(O) with(O) respect(O) to(O) the(O) ability(O) to(O) reproduce(O) known(O) knowledge(O) ,(O) while(O) in(O) knowledge(B-conference) discovery(I-conference) and(I-conference) data(I-conference) mining(I-conference) ((O) KDD(B-conference) )(O) the(O) key(O) task(O) is(O) the(O) discovery(O) of(O) previously(O) unknown(O) knowledge(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, researcher, programming language, person, location, metric, task, university, country, product, organization, algorithm, field and O.\nSentence: Much of the confusion between these two research communities ( which do often have separate conferences and separate journals , ECML PKDD being a major exception ) comes from the basic assumptions they work with : in machine learning , performance is usually evaluated with respect to the ability to reproduce known knowledge , while in knowledge discovery and data mining ( KDD ) the key task is the discovery of previously unknown knowledge .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Much","of","the","confusion","between","these","two","research","communities","(","which","do","often","have","separate","conferences","and","separate","journals",",","ECML","PKDD","being","a","major","exception",")","comes","from","the","basic","assumptions","they","work","with",":","in","machine","learning",",","performance","is","usually","evaluated","with","respect","to","the","ability","to","reproduce","known","knowledge",",","while","in","knowledge","discovery","and","data","mining","(","KDD",")","the","key","task","is","the","discovery","of","previously","unknown","knowledge","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-conference","I-conference","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","O","B-conference","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["conference","researcher","programming_language","person","location","metric","task","university","country","product","organization","algorithm","field"]}
{"id":"274","dataset":"crossner_ai","split":"test","instance":{"id":"274","prompt_labels":",(O) a(O) company(O) in(O) Bangalore(B-location) ,(O) India(B-country) specializing(O) in(O) online(O) handwriting(B-task) recognition(I-task) software(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, researcher, person, programming language, university, task, field, algorithm, metric, product, country, conference, organization and O.\nSentence: , a company in Bangalore , India specializing in online handwriting recognition software .","prediction_output":null,"prediction_outputs":null,"group":null,"words":[",","a","company","in","Bangalore",",","India","specializing","in","online","handwriting","recognition","software","."],"labels":["O","O","O","O","B-location","O","B-country","O","O","O","B-task","I-task","O","O"],"target_index":null,"target_label":null},"label_list":["location","researcher","person","programming_language","university","task","field","algorithm","metric","product","country","conference","organization"]}
{"id":"276","dataset":"crossner_ai","split":"test","instance":{"id":"276","prompt_labels":"He(O) holds(O) fellowships(O) in(O) the(O) American(B-conference) Association(I-conference) for(I-conference) Artificial(I-conference) Intelligence(I-conference) ,(O) the(O) Center(B-organization) for(I-organization) Advanced(I-organization) Study(I-organization) in(I-organization) the(I-organization) Behavioral(I-organization) Sciences(I-organization) at(O) Stanford(B-university) University(I-university) ,(O) the(O) MIT(B-university) Center(O) for(O) Cognitive(B-field) Science(I-field) ,(O) the(O) Canadian(B-organization) Institute(I-organization) for(I-organization) Advanced(I-organization) Research(I-organization) ,(O) the(O) Canadian(B-organization) Psychological(I-organization) Association(I-organization) ,(O) and(O) was(O) elected(O) Fellow(O) of(O) the(O) Royal(B-organization) Society(I-organization) of(I-organization) Canada(I-organization) in(O) 1998(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, metric, algorithm, programming language, country, field, product, university, conference, researcher, location, task and O.\nSentence: He holds fellowships in the American Association for Artificial Intelligence , the Center for Advanced Study in the Behavioral Sciences at Stanford University , the MIT Center for Cognitive Science , the Canadian Institute for Advanced Research , the Canadian Psychological Association , and was elected Fellow of the Royal Society of Canada in 1998 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","holds","fellowships","in","the","American","Association","for","Artificial","Intelligence",",","the","Center","for","Advanced","Study","in","the","Behavioral","Sciences","at","Stanford","University",",","the","MIT","Center","for","Cognitive","Science",",","the","Canadian","Institute","for","Advanced","Research",",","the","Canadian","Psychological","Association",",","and","was","elected","Fellow","of","the","Royal","Society","of","Canada","in","1998","."],"labels":["O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-university","I-university","O","O","B-university","O","O","B-field","I-field","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","person","metric","algorithm","programming_language","country","field","product","university","conference","researcher","location","task"]}
{"id":"278","dataset":"crossner_ai","split":"test","instance":{"id":"278","prompt_labels":"The(O) lightweight(O) open-source(O) speech(O) project(O) eSpeak(B-product) ,(O) which(O) has(O) its(O) own(O) approach(O) to(O) synthesis(O) ,(O) has(O) experimented(O) with(O) Mandarin(O) and(O) Cantonese.(O) eSpeak(B-product) was(O) used(O) by(O) Google(B-product) Translate(I-product) from(O) May(O) 20102010(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, organization, country, conference, programming language, product, algorithm, metric, person, university, field, location, researcher and O.\nSentence: The lightweight open-source speech project eSpeak , which has its own approach to synthesis , has experimented with Mandarin and Cantonese. eSpeak was used by Google Translate from May 20102010 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","lightweight","open-source","speech","project","eSpeak",",","which","has","its","own","approach","to","synthesis",",","has","experimented","with","Mandarin","and","Cantonese.","eSpeak","was","used","by","Google","Translate","from","May","20102010","."],"labels":["O","O","O","O","O","B-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","O","O","O","B-product","I-product","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["task","organization","country","conference","programming_language","product","algorithm","metric","person","university","field","location","researcher"]}
{"id":"279","dataset":"crossner_ai","split":"test","instance":{"id":"279","prompt_labels":"Also(O) released(O) in(O) 1982(O) ,(O) Software(B-product) Automatic(I-product) Mouth(I-product) was(O) the(O) first(O) commercial(O) all-software(O) voice(O) synthesis(B-task) program(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, person, country, location, organization, metric, product, researcher, programming language, university, conference, field, task and O.\nSentence: Also released in 1982 , Software Automatic Mouth was the first commercial all-software voice synthesis program .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Also","released","in","1982",",","Software","Automatic","Mouth","was","the","first","commercial","all-software","voice","synthesis","program","."],"labels":["O","O","O","O","O","B-product","I-product","I-product","O","O","O","O","O","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["algorithm","person","country","location","organization","metric","product","researcher","programming_language","university","conference","field","task"]}
{"id":"280","dataset":"crossner_ai","split":"test","instance":{"id":"280","prompt_labels":"The(O) column(O) ratios(O) are(O) TRUE(B-metric) Positive(I-metric) Rate(I-metric) ((O) TPR(B-metric) ,(O) aka(O) Sensitivity(B-metric) or(O) recall(B-metric) )(O) ((O) TP(B-metric) /(I-metric) ((I-metric) TP(I-metric) +(I-metric) FN(I-metric) )(I-metric) )(O) ,(O) with(O) complement(O) the(O) FALSE(B-metric) Negative(I-metric) Rate(I-metric) ((O) FNR(B-metric) )(O) ((O) FN(B-metric) /(I-metric) ((I-metric) TP(I-metric) +(I-metric) FN(I-metric) )(I-metric) )(O) ;(O) and(O) TRUE(B-metric) Negative(I-metric) Rate(I-metric) ((O) TNR(B-metric) ,(O) aka(O) Specificity(B-metric) ,(O) SPC(B-metric) )(O) ((O) TN(B-metric) /(I-metric) ((I-metric) TN(I-metric) +(I-metric) FP(I-metric) )(I-metric) )(O) ,(O) with(O) complement(O) FALSE(B-metric) Positive(I-metric) Rate(I-metric) ((O) FPR(B-metric) )(O) ((O) FP(B-metric) /(I-metric) ((I-metric) TN(I-metric) +(I-metric) FP(I-metric) )(I-metric) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, task, location, organization, metric, university, country, product, conference, person, programming language, algorithm, researcher and O.\nSentence: The column ratios are TRUE Positive Rate ( TPR , aka Sensitivity or recall ) ( TP / ( TP + FN ) ) , with complement the FALSE Negative Rate ( FNR ) ( FN / ( TP + FN ) ) ; and TRUE Negative Rate ( TNR , aka Specificity , SPC ) ( TN / ( TN + FP ) ) , with complement FALSE Positive Rate ( FPR ) ( FP / ( TN + FP ) ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","column","ratios","are","TRUE","Positive","Rate","(","TPR",",","aka","Sensitivity","or","recall",")","(","TP","/","(","TP","+","FN",")",")",",","with","complement","the","FALSE","Negative","Rate","(","FNR",")","(","FN","/","(","TP","+","FN",")",")",";","and","TRUE","Negative","Rate","(","TNR",",","aka","Specificity",",","SPC",")","(","TN","/","(","TN","+","FP",")",")",",","with","complement","FALSE","Positive","Rate","(","FPR",")","(","FP","/","(","TN","+","FP",")",")","."],"labels":["O","O","O","O","B-metric","I-metric","I-metric","O","B-metric","O","O","B-metric","O","B-metric","O","O","B-metric","I-metric","I-metric","I-metric","I-metric","I-metric","I-metric","O","O","O","O","O","B-metric","I-metric","I-metric","O","B-metric","O","O","B-metric","I-metric","I-metric","I-metric","I-metric","I-metric","I-metric","O","O","O","B-metric","I-metric","I-metric","O","B-metric","O","O","B-metric","O","B-metric","O","O","B-metric","I-metric","I-metric","I-metric","I-metric","I-metric","I-metric","O","O","O","O","B-metric","I-metric","I-metric","O","B-metric","O","O","B-metric","I-metric","I-metric","I-metric","I-metric","I-metric","I-metric","O","O"],"target_index":null,"target_label":null},"label_list":["field","task","location","organization","metric","university","country","product","conference","person","programming_language","algorithm","researcher"]}
{"id":"281","dataset":"crossner_ai","split":"test","instance":{"id":"281","prompt_labels":"Edsinger(B-person) and(O) Weber(B-organization) collaborated(O) on(O) many(O) other(O) robots(O) as(O) well(O) ,(O) and(O) their(O) experience(O) working(O) with(O) the(O) Kismet(B-product)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, university, algorithm, location, conference, metric, researcher, organization, field, product, country, person, task and O.\nSentence: Edsinger and Weber collaborated on many other robots as well , and their experience working with the Kismet","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Edsinger","and","Weber","collaborated","on","many","other","robots","as","well",",","and","their","experience","working","with","the","Kismet"],"labels":["B-person","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product"],"target_index":null,"target_label":null},"label_list":["programming_language","university","algorithm","location","conference","metric","researcher","organization","field","product","country","person","task"]}
{"id":"282","dataset":"crossner_ai","split":"test","instance":{"id":"282","prompt_labels":"R(B-programming language) functionality(O) is(O) accessible(O) from(O) several(O) scripting(O) languages(O) such(O) as(O) Python(B-programming language) ,(O) are(O) available(O) as(O) well(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, person, algorithm, metric, organization, university, conference, task, programming language, product, country, researcher, location and O.\nSentence: R functionality is accessible from several scripting languages such as Python , are available as well .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["R","functionality","is","accessible","from","several","scripting","languages","such","as","Python",",","are","available","as","well","."],"labels":["B-programming language","O","O","O","O","O","O","O","O","O","B-programming language","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","person","algorithm","metric","organization","university","conference","task","programming_language","product","country","researcher","location"]}
{"id":"284","dataset":"crossner_ai","split":"test","instance":{"id":"284","prompt_labels":"They(O) presented(O) their(O) database(O) for(O) the(O) first(O) time(O) as(O) a(O) poster(O) at(O) the(O) 2009(B-conference) Conference(I-conference) on(I-conference) Computer(I-conference) Vision(I-conference) and(I-conference) Pattern(I-conference) Recognition(I-conference) ((O) CVPR(B-conference) )(O) in(O) Florida(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, conference, metric, location, programming language, product, organization, algorithm, university, field, task, researcher, country and O.\nSentence: They presented their database for the first time as a poster at the 2009 Conference on Computer Vision and Pattern Recognition ( CVPR ) in Florida .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["They","presented","their","database","for","the","first","time","as","a","poster","at","the","2009","Conference","on","Computer","Vision","and","Pattern","Recognition","(","CVPR",")","in","Florida","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","B-conference","O","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["person","conference","metric","location","programming_language","product","organization","algorithm","university","field","task","researcher","country"]}
{"id":"285","dataset":"crossner_ai","split":"test","instance":{"id":"285","prompt_labels":"Categorization(O) tasks(O) in(O) which(O) no(O) labels(O) are(O) supplied(O) are(O) referred(O) to(O) as(O) unsupervised(B-task) classification(I-task) ,(O) unsupervised(B-field) learning(I-field) ,(O) Cluster(B-task) analysis(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, university, researcher, product, conference, programming language, location, country, algorithm, field, metric, task and O.\nSentence: Categorization tasks in which no labels are supplied are referred to as unsupervised classification , unsupervised learning , Cluster analysis .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Categorization","tasks","in","which","no","labels","are","supplied","are","referred","to","as","unsupervised","classification",",","unsupervised","learning",",","Cluster","analysis","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-task","I-task","O","B-field","I-field","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["person","organization","university","researcher","product","conference","programming_language","location","country","algorithm","field","metric","task"]}
{"id":"286","dataset":"crossner_ai","split":"test","instance":{"id":"286","prompt_labels":"It(O) needs(O) to(O) Object(B-task) recognition(I-task) ,(O) recognize(O) and(O) locate(O) humans(O) and(O) further(O) emotion(B-task) recognition(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, algorithm, university, programming language, location, person, country, researcher, conference, field, metric, organization, task and O.\nSentence: It needs to Object recognition , recognize and locate humans and further emotion recognition .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","needs","to","Object","recognition",",","recognize","and","locate","humans","and","further","emotion","recognition","."],"labels":["O","O","O","B-task","I-task","O","O","O","O","O","O","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["product","algorithm","university","programming_language","location","person","country","researcher","conference","field","metric","organization","task"]}
{"id":"287","dataset":"crossner_ai","split":"test","instance":{"id":"287","prompt_labels":"The(O) process(O) is(O) complex(O) and(O) contains(O) encoding(O) and(O) recall(O) or(O) retrieval(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, country, location, researcher, conference, product, algorithm, programming language, organization, metric, person, university, task and O.\nSentence: The process is complex and contains encoding and recall or retrieval .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","process","is","complex","and","contains","encoding","and","recall","or","retrieval","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","country","location","researcher","conference","product","algorithm","programming_language","organization","metric","person","university","task"]}
{"id":"288","dataset":"crossner_ai","split":"test","instance":{"id":"288","prompt_labels":"Also(O) known(O) as(O) parallel(O) robots(O) ,(O) or(O) generalized(O) Stewart(B-product) platforms(I-product) ((O) in(O) the(O) Stewart(B-product) platform(I-product) ,(O) the(O) actuators(O) are(O) paired(O) together(O) on(O) both(O) the(O) basis(O) and(O) the(O) platform(O) )(O) ,(O) these(O) systems(O) are(O) articulated(B-product) robot(I-product) s(O) that(O) use(O) similar(O) mechanisms(O) for(O) the(O) movement(O) of(O) either(O) the(O) robot(O) on(O) its(O) base(O) ,(O) or(O) one(O) or(O) more(O) manipulator(O) arms(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, university, person, country, metric, researcher, organization, location, programming language, conference, field, algorithm, product and O.\nSentence: Also known as parallel robots , or generalized Stewart platforms ( in the Stewart platform , the actuators are paired together on both the basis and the platform ) , these systems are articulated robot s that use similar mechanisms for the movement of either the robot on its base , or one or more manipulator arms .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Also","known","as","parallel","robots",",","or","generalized","Stewart","platforms","(","in","the","Stewart","platform",",","the","actuators","are","paired","together","on","both","the","basis","and","the","platform",")",",","these","systems","are","articulated","robot","s","that","use","similar","mechanisms","for","the","movement","of","either","the","robot","on","its","base",",","or","one","or","more","manipulator","arms","."],"labels":["O","O","O","O","O","O","O","O","B-product","I-product","O","O","O","B-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["task","university","person","country","metric","researcher","organization","location","programming_language","conference","field","algorithm","product"]}
{"id":"289","dataset":"crossner_ai","split":"test","instance":{"id":"289","prompt_labels":"Machine(B-field) vision(I-field) as(O) a(O) systems(B-field) engineering(I-field) discipline(O) can(O) be(O) considered(O) distinct(O) from(O) computer(B-field) vision(I-field) ,(O) a(O) form(O) of(O) computer(B-field) science(I-field) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, university, country, conference, programming language, location, algorithm, researcher, organization, task, product, field, person and O.\nSentence: Machine vision as a systems engineering discipline can be considered distinct from computer vision , a form of computer science .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Machine","vision","as","a","systems","engineering","discipline","can","be","considered","distinct","from","computer","vision",",","a","form","of","computer","science","."],"labels":["B-field","I-field","O","O","B-field","I-field","O","O","O","O","O","O","B-field","I-field","O","O","O","O","B-field","I-field","O"],"target_index":null,"target_label":null},"label_list":["metric","university","country","conference","programming_language","location","algorithm","researcher","organization","task","product","field","person"]}
{"id":"291","dataset":"crossner_ai","split":"test","instance":{"id":"291","prompt_labels":"In(O) other(O) words(O) ,(O) the(O) sample(B-metric) mean(I-metric) is(O) the(O) ((O) necessarily(O) unique(O) )(O) efficient(O) estimator(O) ,(O) and(O) thus(O) also(O) the(O) minimum(B-metric) variance(I-metric) unbiased(I-metric) estimator(I-metric) ((O) MVUE(B-metric) )(O) ,(O) in(O) addition(O) to(O) being(O) the(O) maximum(B-metric) likelihood(I-metric) estimator(I-metric) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, product, programming language, country, field, conference, task, organization, algorithm, metric, person, researcher, university and O.\nSentence: In other words , the sample mean is the ( necessarily unique ) efficient estimator , and thus also the minimum variance unbiased estimator ( MVUE ) , in addition to being the maximum likelihood estimator .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","other","words",",","the","sample","mean","is","the","(","necessarily","unique",")","efficient","estimator",",","and","thus","also","the","minimum","variance","unbiased","estimator","(","MVUE",")",",","in","addition","to","being","the","maximum","likelihood","estimator","."],"labels":["O","O","O","O","O","B-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","I-metric","O","B-metric","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O"],"target_index":null,"target_label":null},"label_list":["location","product","programming_language","country","field","conference","task","organization","algorithm","metric","person","researcher","university"]}
{"id":"292","dataset":"crossner_ai","split":"test","instance":{"id":"292","prompt_labels":"The(O) 2001(O) Scientific(O) American(O) article(O) by(O) Berners-Lee(B-researcher) ,(O) James(B-researcher) Hendler(I-researcher) ,(O) and(O) Ora(B-researcher) Lassila(I-researcher) described(O) an(O) expected(O) evolution(O) of(O) the(O) existing(O) Web(B-product) to(O) a(O) Semantic(B-product) Web(I-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, programming language, university, field, person, country, organization, algorithm, location, researcher, conference, metric, product and O.\nSentence: The 2001 Scientific American article by Berners-Lee , James Hendler , and Ora Lassila described an expected evolution of the existing Web to a Semantic Web .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","2001","Scientific","American","article","by","Berners-Lee",",","James","Hendler",",","and","Ora","Lassila","described","an","expected","evolution","of","the","existing","Web","to","a","Semantic","Web","."],"labels":["O","O","O","O","O","O","B-researcher","O","B-researcher","I-researcher","O","O","B-researcher","I-researcher","O","O","O","O","O","O","O","B-product","O","O","B-product","I-product","O"],"target_index":null,"target_label":null},"label_list":["task","programming_language","university","field","person","country","organization","algorithm","location","researcher","conference","metric","product"]}
{"id":"293","dataset":"crossner_ai","split":"test","instance":{"id":"293","prompt_labels":"Blade(O) Runner(O) used(O) a(O) number(O) of(O) then-lesser-known(O) actors(O) :(O) Sean(B-person) Young(I-person) portrays(O) Rachael(B-person) ,(O) an(O) experimental(O) replicant(O) implanted(O) with(O) the(O) memories(O) of(O) Tyrell(B-person) 's(O) niece(O) ,(O) causing(O) her(O) to(O) believe(O) she(O) is(O) human(O) ;(O) Sammon(B-person) ,(O) pp.(O) 92-93(O) Nina(B-person) Axelrod(I-person) auditioned(O) for(O) the(O) role(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, university, metric, location, researcher, programming language, task, algorithm, field, conference, product, organization and O.\nSentence: Blade Runner used a number of then-lesser-known actors : Sean Young portrays Rachael , an experimental replicant implanted with the memories of Tyrell 's niece , causing her to believe she is human ; Sammon , pp. 92-93 Nina Axelrod auditioned for the role .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Blade","Runner","used","a","number","of","then-lesser-known","actors",":","Sean","Young","portrays","Rachael",",","an","experimental","replicant","implanted","with","the","memories","of","Tyrell","'s","niece",",","causing","her","to","believe","she","is","human",";","Sammon",",","pp.","92-93","Nina","Axelrod","auditioned","for","the","role","."],"labels":["O","O","O","O","O","O","O","O","O","B-person","I-person","O","B-person","O","O","O","O","O","O","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","B-person","O","O","O","B-person","I-person","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","country","university","metric","location","researcher","programming_language","task","algorithm","field","conference","product","organization"]}
{"id":"294","dataset":"crossner_ai","split":"test","instance":{"id":"294","prompt_labels":"Gerry(B-researcher) Sussman(I-researcher) ,(O) Eugene(B-researcher) Charniak(I-researcher) ,(O) Seymour(B-researcher) Papert(I-researcher) and(O) Terry(B-researcher) Winograd(I-researcher) visited(O) the(O) University(B-university) of(I-university) Edinburgh(I-university) in(O) 1971(O) spreading(O) the(O) news(O) about(O) Micro-Planner(B-product) and(O) SHRDLU(B-product) and(O) casting(O) doubt(O) on(O) the(O) resolution(O) uniform(O) proof(O) procedure(O) approach(O) that(O) had(O) been(O) the(O) mainstay(O) of(O) the(O) Edinburgh(B-location) Logicists(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, product, researcher, algorithm, person, task, conference, programming language, organization, country, location, field, metric and O.\nSentence: Gerry Sussman , Eugene Charniak , Seymour Papert and Terry Winograd visited the University of Edinburgh in 1971 spreading the news about Micro-Planner and SHRDLU and casting doubt on the resolution uniform proof procedure approach that had been the mainstay of the Edinburgh Logicists .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Gerry","Sussman",",","Eugene","Charniak",",","Seymour","Papert","and","Terry","Winograd","visited","the","University","of","Edinburgh","in","1971","spreading","the","news","about","Micro-Planner","and","SHRDLU","and","casting","doubt","on","the","resolution","uniform","proof","procedure","approach","that","had","been","the","mainstay","of","the","Edinburgh","Logicists","."],"labels":["B-researcher","I-researcher","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","O","B-university","I-university","I-university","O","O","O","O","O","O","B-product","O","B-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","O"],"target_index":null,"target_label":null},"label_list":["university","product","researcher","algorithm","person","task","conference","programming_language","organization","country","location","field","metric"]}
{"id":"295","dataset":"crossner_ai","split":"test","instance":{"id":"295","prompt_labels":"Walter(B-researcher) 's(O) work(O) inspired(O) subsequent(O) generations(O) of(O) robotics(B-field) researchers(O) such(O) as(O) Rodney(B-researcher) Brooks(I-researcher) ,(O) Hans(B-researcher) Moravec(I-researcher) and(O) Mark(B-researcher) Tilden(I-researcher) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, task, field, algorithm, product, researcher, location, programming language, country, university, person, organization, conference and O.\nSentence: Walter 's work inspired subsequent generations of robotics researchers such as Rodney Brooks , Hans Moravec and Mark Tilden .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Walter","'s","work","inspired","subsequent","generations","of","robotics","researchers","such","as","Rodney","Brooks",",","Hans","Moravec","and","Mark","Tilden","."],"labels":["B-researcher","O","O","O","O","O","O","B-field","O","O","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O"],"target_index":null,"target_label":null},"label_list":["metric","task","field","algorithm","product","researcher","location","programming_language","country","university","person","organization","conference"]}
{"id":"297","dataset":"crossner_ai","split":"test","instance":{"id":"297","prompt_labels":"Commonly(O) used(O) loss(O) functions(O) for(O) probabilistic(O) classification(O) include(O) log(B-metric) loss(I-metric) and(O) the(O) Brier(B-metric) score(I-metric) between(O) the(O) predicted(O) and(O) the(O) TRUE(O) probability(O) distributions(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, researcher, country, organization, product, metric, task, programming language, algorithm, person, field, university, conference and O.\nSentence: Commonly used loss functions for probabilistic classification include log loss and the Brier score between the predicted and the TRUE probability distributions .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Commonly","used","loss","functions","for","probabilistic","classification","include","log","loss","and","the","Brier","score","between","the","predicted","and","the","TRUE","probability","distributions","."],"labels":["O","O","O","O","O","O","O","O","B-metric","I-metric","O","O","B-metric","I-metric","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","researcher","country","organization","product","metric","task","programming_language","algorithm","person","field","university","conference"]}
{"id":"298","dataset":"crossner_ai","split":"test","instance":{"id":"298","prompt_labels":"In(O) May(O) 2016(O) ,(O) NtechLab(B-organization) was(O) admitted(O) to(O) the(O) official(O) testing(O) of(O) biometrics(B-field) technology(O) by(O) NIST(B-organization) among(O) the(O) three(O) Russian(O) companies(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, university, organization, task, person, programming language, metric, algorithm, product, field, country, location, researcher and O.\nSentence: In May 2016 , NtechLab was admitted to the official testing of biometrics technology by NIST among the three Russian companies .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","May","2016",",","NtechLab","was","admitted","to","the","official","testing","of","biometrics","technology","by","NIST","among","the","three","Russian","companies","."],"labels":["O","O","O","O","B-organization","O","O","O","O","O","O","O","B-field","O","O","B-organization","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["conference","university","organization","task","person","programming_language","metric","algorithm","product","field","country","location","researcher"]}
{"id":"300","dataset":"crossner_ai","split":"test","instance":{"id":"300","prompt_labels":"During(O) 2015(O) ,(O) many(O) of(O) SenseTime(B-organization) 's(O) papers(O) were(O) accepted(O) into(O) the(O) Conference(B-conference) on(I-conference) Computer(I-conference) Vision(I-conference) and(I-conference) Pattern(I-conference) Recognition(I-conference) ((O) CVPR(B-conference) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, location, programming language, metric, university, person, researcher, task, product, field, algorithm, conference and O.\nSentence: During 2015 , many of SenseTime 's papers were accepted into the Conference on Computer Vision and Pattern Recognition ( CVPR ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","2015",",","many","of","SenseTime","'s","papers","were","accepted","into","the","Conference","on","Computer","Vision","and","Pattern","Recognition","(","CVPR",")","."],"labels":["O","O","O","O","O","B-organization","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","B-conference","O","O"],"target_index":null,"target_label":null},"label_list":["organization","country","location","programming_language","metric","university","person","researcher","task","product","field","algorithm","conference"]}
{"id":"302","dataset":"crossner_ai","split":"test","instance":{"id":"302","prompt_labels":"Stephen(B-researcher) H.(I-researcher) Muggleton(I-researcher) FBCS(B-organization) ,(O) FIET(B-organization) ,(O) Association(B-conference) for(I-conference) the(I-conference) Advancement(I-conference) of(I-conference) Artificial(I-conference) Intelligence(I-conference) ,(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, conference, person, programming language, algorithm, product, researcher, organization, university, country, field, metric, location and O.\nSentence: Stephen H. Muggleton FBCS , FIET , Association for the Advancement of Artificial Intelligence ,","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Stephen","H.","Muggleton","FBCS",",","FIET",",","Association","for","the","Advancement","of","Artificial","Intelligence",","],"labels":["B-researcher","I-researcher","I-researcher","B-organization","O","B-organization","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O"],"target_index":null,"target_label":null},"label_list":["task","conference","person","programming_language","algorithm","product","researcher","organization","university","country","field","metric","location"]}
{"id":"303","dataset":"crossner_ai","split":"test","instance":{"id":"303","prompt_labels":"Edge(B-task) detection(I-task) is(O) a(O) fundamental(O) tool(O) in(O) image(B-field) processing(I-field) ,(O) machine(B-field) vision(I-field) and(O) computer(B-field) vision(I-field) ,(O) particularly(O) in(O) the(O) areas(O) of(O) feature(B-task) detection(I-task) and(O) feature(B-task) extraction(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, task, conference, university, country, location, field, metric, person, programming language, organization, researcher, product and O.\nSentence: Edge detection is a fundamental tool in image processing , machine vision and computer vision , particularly in the areas of feature detection and feature extraction .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Edge","detection","is","a","fundamental","tool","in","image","processing",",","machine","vision","and","computer","vision",",","particularly","in","the","areas","of","feature","detection","and","feature","extraction","."],"labels":["B-task","I-task","O","O","O","O","O","B-field","I-field","O","B-field","I-field","O","B-field","I-field","O","O","O","O","O","O","B-task","I-task","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["algorithm","task","conference","university","country","location","field","metric","person","programming_language","organization","researcher","product"]}
{"id":"305","dataset":"crossner_ai","split":"test","instance":{"id":"305","prompt_labels":"The(O) returning(O) judges(O) are(O) Fon(B-person) Davis(I-person) ,(O) Jessica(B-person) Chobot(I-person) ,(O) and(O) Leland(B-person) Melvin(I-person) ,(O) as(O) well(O) as(O) celebrity(O) guest(O) judges(O) actor(O) Clark(B-person) Gregg(I-person) ,(O) MythBusters(O) host(O) and(O) former(O) Battlebots(O) builder(O) Adam(B-person) Savage(I-person) ,(O) NFL(B-organization) tightend(O) Vernon(B-person) Davis(I-person) ,(O) and(O) YouTube(B-organization) star(O) Michael(B-person) Stevens(I-person) a.k.a.(O) Vsauce(B-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, field, researcher, programming language, conference, task, organization, university, metric, location, person, algorithm, country and O.\nSentence: The returning judges are Fon Davis , Jessica Chobot , and Leland Melvin , as well as celebrity guest judges actor Clark Gregg , MythBusters host and former Battlebots builder Adam Savage , NFL tightend Vernon Davis , and YouTube star Michael Stevens a.k.a. Vsauce .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","returning","judges","are","Fon","Davis",",","Jessica","Chobot",",","and","Leland","Melvin",",","as","well","as","celebrity","guest","judges","actor","Clark","Gregg",",","MythBusters","host","and","former","Battlebots","builder","Adam","Savage",",","NFL","tightend","Vernon","Davis",",","and","YouTube","star","Michael","Stevens","a.k.a.","Vsauce","."],"labels":["O","O","O","O","B-person","I-person","O","B-person","I-person","O","O","B-person","I-person","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","B-person","I-person","O","B-organization","O","B-person","I-person","O","O","B-organization","O","B-person","I-person","O","B-person","O"],"target_index":null,"target_label":null},"label_list":["product","field","researcher","programming_language","conference","task","organization","university","metric","location","person","algorithm","country"]}
{"id":"306","dataset":"crossner_ai","split":"test","instance":{"id":"306","prompt_labels":"But(O) these(O) methods(O) never(O) won(O) over(O) the(O) non-uniform(O) internal-handcrafting(O) Gaussian(B-algorithm) mixture(I-algorithm) model(I-algorithm) /(O) Hidden(B-algorithm) Markov(I-algorithm) model(I-algorithm) ((O) GMM-HMM(B-algorithm) )(O) technology(O) based(O) on(O) generative(O) models(O) of(O) speech(O) trained(O) discriminatively(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, product, organization, programming language, task, metric, person, researcher, country, field, conference, algorithm, location and O.\nSentence: But these methods never won over the non-uniform internal-handcrafting Gaussian mixture model / Hidden Markov model ( GMM-HMM ) technology based on generative models of speech trained discriminatively .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["But","these","methods","never","won","over","the","non-uniform","internal-handcrafting","Gaussian","mixture","model","/","Hidden","Markov","model","(","GMM-HMM",")","technology","based","on","generative","models","of","speech","trained","discriminatively","."],"labels":["O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","product","organization","programming_language","task","metric","person","researcher","country","field","conference","algorithm","location"]}
{"id":"307","dataset":"crossner_ai","split":"test","instance":{"id":"307","prompt_labels":"Software(O) packages(O) like(O) MATLAB(B-product) ,(O) GNU(B-programming language) Octave(I-programming language) ,(O) Scilab(B-programming language) ,(O) and(O) SciPy(B-product) provide(O) convenient(O) ways(O) to(O) apply(O) these(O) different(O) methods(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, metric, conference, task, organization, programming language, field, algorithm, location, product, country, researcher, person and O.\nSentence: Software packages like MATLAB , GNU Octave , Scilab , and SciPy provide convenient ways to apply these different methods .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Software","packages","like","MATLAB",",","GNU","Octave",",","Scilab",",","and","SciPy","provide","convenient","ways","to","apply","these","different","methods","."],"labels":["O","O","O","B-product","O","B-programming language","I-programming language","O","B-programming language","O","O","B-product","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","metric","conference","task","organization","programming_language","field","algorithm","location","product","country","researcher","person"]}
{"id":"308","dataset":"crossner_ai","split":"test","instance":{"id":"308","prompt_labels":"Linear(B-algorithm) predictive(I-algorithm) coding(I-algorithm) ((O) LPC(B-algorithm) )(O) ,(O) a(O) speech(B-task) processing(I-task) algorithm(O) ,(O) was(O) first(O) proposed(O) by(O) Fumitada(B-researcher) Itakura(I-researcher) of(O) Nagoya(B-university) University(I-university) and(O) Shuzo(B-researcher) Saito(I-researcher) of(O) Nippon(B-organization) Telegraph(I-organization) and(I-organization) Telephone(I-organization) ((O) NTT(B-organization) )(O) in(O) 1966(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, organization, task, conference, person, university, algorithm, product, researcher, country, field, location, programming language and O.\nSentence: Linear predictive coding ( LPC ) , a speech processing algorithm , was first proposed by Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone ( NTT ) in 1966 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Linear","predictive","coding","(","LPC",")",",","a","speech","processing","algorithm",",","was","first","proposed","by","Fumitada","Itakura","of","Nagoya","University","and","Shuzo","Saito","of","Nippon","Telegraph","and","Telephone","(","NTT",")","in","1966","."],"labels":["B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","O","O","O","B-task","I-task","O","O","O","O","O","O","B-researcher","I-researcher","O","B-university","I-university","O","B-researcher","I-researcher","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","organization","task","conference","person","university","algorithm","product","researcher","country","field","location","programming_language"]}
{"id":"309","dataset":"crossner_ai","split":"test","instance":{"id":"309","prompt_labels":"In(O) 2006(O) ,(O) for(O) the(O) 25th(O) anniversary(O) of(O) the(O) algorithm(O) ,(O) a(O) workshop(O) was(O) organized(O) at(O) the(O) International(B-conference) Conference(I-conference) on(I-conference) Computer(I-conference) Vision(I-conference) and(I-conference) Pattern(I-conference) Recognition(I-conference) ((O) CVPR(B-conference) )(O) to(O) summarize(O) the(O) most(O) recent(O) contributions(O) and(O) variations(O) to(O) the(O) original(O) algorithm(O) ,(O) mostly(O) meant(O) to(O) improve(O) the(O) speed(O) of(O) the(O) algorithm(O) ,(O) the(O) robustness(O) and(O) accuracy(O) of(O) the(O) estimated(O) solution(O) and(O) to(O) decrease(O) the(O) dependency(O) from(O) user(O) defined(O) constants(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, organization, person, metric, researcher, product, algorithm, country, university, field, location, programming language, task and O.\nSentence: In 2006 , for the 25th anniversary of the algorithm , a workshop was organized at the International Conference on Computer Vision and Pattern Recognition ( CVPR ) to summarize the most recent contributions and variations to the original algorithm , mostly meant to improve the speed of the algorithm , the robustness and accuracy of the estimated solution and to decrease the dependency from user defined constants .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2006",",","for","the","25th","anniversary","of","the","algorithm",",","a","workshop","was","organized","at","the","International","Conference","on","Computer","Vision","and","Pattern","Recognition","(","CVPR",")","to","summarize","the","most","recent","contributions","and","variations","to","the","original","algorithm",",","mostly","meant","to","improve","the","speed","of","the","algorithm",",","the","robustness","and","accuracy","of","the","estimated","solution","and","to","decrease","the","dependency","from","user","defined","constants","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","B-conference","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["conference","organization","person","metric","researcher","product","algorithm","country","university","field","location","programming_language","task"]}
{"id":"310","dataset":"crossner_ai","split":"test","instance":{"id":"310","prompt_labels":"The(O) members(O) went(O) to(O) the(O) University(B-university) of(I-university) Debrecen(I-university) ,(O) the(O) Hungarian(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) Etvs(B-university) Lornd(I-university) University(I-university) ,(O) etc(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, task, person, researcher, organization, algorithm, conference, product, field, location, university, programming language, country and O.\nSentence: The members went to the University of Debrecen , the Hungarian Academy of Sciences , Etvs Lornd University , etc .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","members","went","to","the","University","of","Debrecen",",","the","Hungarian","Academy","of","Sciences",",","Etvs","Lornd","University",",","etc","."],"labels":["O","O","O","O","O","B-university","I-university","I-university","O","O","B-organization","I-organization","I-organization","I-organization","O","B-university","I-university","I-university","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","task","person","researcher","organization","algorithm","conference","product","field","location","university","programming_language","country"]}
{"id":"311","dataset":"crossner_ai","split":"test","instance":{"id":"311","prompt_labels":"To(O) extend(O) SVM(B-algorithm) to(O) cases(O) in(O) which(O) the(O) data(O) are(O) not(O) linearly(O) separable(O) ,(O) we(O) introduce(O) the(O) loss(O) function(O) ,(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, organization, task, conference, algorithm, field, metric, researcher, university, product, country, location, person and O.\nSentence: To extend SVM to cases in which the data are not linearly separable , we introduce the loss function ,","prediction_output":null,"prediction_outputs":null,"group":null,"words":["To","extend","SVM","to","cases","in","which","the","data","are","not","linearly","separable",",","we","introduce","the","loss","function",","],"labels":["O","O","B-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","organization","task","conference","algorithm","field","metric","researcher","university","product","country","location","person"]}
{"id":"313","dataset":"crossner_ai","split":"test","instance":{"id":"313","prompt_labels":"Eyring(B-organization) Research(I-organization) Institute(I-organization) was(O) instrumental(O) to(O) the(O) U.S.(B-organization) Air(I-organization) Force(I-organization) Missile(I-organization) Directorate(I-organization) at(O) Hill(B-location) Air(I-location) Force(I-location) Base(I-location) near(O) Ogden(B-location) ,(O) Utah(B-location) to(O) produce(O) in(O) top(O) military(O) secrecy(O) ,(O) the(O) Intelligent(B-product) Systems(I-product) Technology(I-product) Software(I-product) that(O) was(O) foundational(O) to(O) the(O) later(O) named(O) Reagan(B-product) Star(I-product) Wars(I-product) program(I-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, conference, location, university, researcher, country, algorithm, task, field, metric, product, organization, programming language and O.\nSentence: Eyring Research Institute was instrumental to the U.S. Air Force Missile Directorate at Hill Air Force Base near Ogden , Utah to produce in top military secrecy , the Intelligent Systems Technology Software that was foundational to the later named Reagan Star Wars program .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Eyring","Research","Institute","was","instrumental","to","the","U.S.","Air","Force","Missile","Directorate","at","Hill","Air","Force","Base","near","Ogden",",","Utah","to","produce","in","top","military","secrecy",",","the","Intelligent","Systems","Technology","Software","that","was","foundational","to","the","later","named","Reagan","Star","Wars","program","."],"labels":["B-organization","I-organization","I-organization","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-location","I-location","I-location","I-location","O","B-location","O","B-location","O","O","O","O","O","O","O","O","B-product","I-product","I-product","I-product","O","O","O","O","O","O","O","B-product","I-product","I-product","I-product","O"],"target_index":null,"target_label":null},"label_list":["person","conference","location","university","researcher","country","algorithm","task","field","metric","product","organization","programming_language"]}
{"id":"315","dataset":"crossner_ai","split":"test","instance":{"id":"315","prompt_labels":"The(O) Sobel(B-algorithm) operator(I-algorithm) ,(O) sometimes(O) called(O) the(O) Sobel-Feldman(B-algorithm) operator(I-algorithm) or(O) Sobel(B-algorithm) filter(I-algorithm) ,(O) is(O) used(O) in(O) image(B-field) processing(I-field) and(O) computer(B-field) vision(I-field) ,(O) particularly(O) within(O) edge(O) detection(O) algorithms(O) where(O) it(O) creates(O) an(O) image(O) emphasising(O) edges(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, metric, organization, location, field, university, product, algorithm, person, programming language, country, researcher, task and O.\nSentence: The Sobel operator , sometimes called the Sobel-Feldman operator or Sobel filter , is used in image processing and computer vision , particularly within edge detection algorithms where it creates an image emphasising edges .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Sobel","operator",",","sometimes","called","the","Sobel-Feldman","operator","or","Sobel","filter",",","is","used","in","image","processing","and","computer","vision",",","particularly","within","edge","detection","algorithms","where","it","creates","an","image","emphasising","edges","."],"labels":["O","B-algorithm","I-algorithm","O","O","O","O","B-algorithm","I-algorithm","O","B-algorithm","I-algorithm","O","O","O","O","B-field","I-field","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["conference","metric","organization","location","field","university","product","algorithm","person","programming_language","country","researcher","task"]}
{"id":"316","dataset":"crossner_ai","split":"test","instance":{"id":"316","prompt_labels":"LDA(B-algorithm) is(O) a(O) supervised(B-field) learning(I-field) algorithm(O) that(O) utilizes(O) the(O) labels(O) of(O) the(O) data(O) ,(O) while(O) PCA(B-algorithm) is(O) an(O) learning(O) algorithm(O) that(O) ignores(O) the(O) labels(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, conference, task, university, person, country, field, programming language, metric, algorithm, organization, location, researcher and O.\nSentence: LDA is a supervised learning algorithm that utilizes the labels of the data , while PCA is an learning algorithm that ignores the labels .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["LDA","is","a","supervised","learning","algorithm","that","utilizes","the","labels","of","the","data",",","while","PCA","is","an","learning","algorithm","that","ignores","the","labels","."],"labels":["B-algorithm","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","B-algorithm","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["product","conference","task","university","person","country","field","programming_language","metric","algorithm","organization","location","researcher"]}
{"id":"317","dataset":"crossner_ai","split":"test","instance":{"id":"317","prompt_labels":"Other(O) linear(O) classification(O) algorithms(O) include(O) Winnow(B-algorithm) ,(O) support(B-algorithm) vector(I-algorithm) machine(I-algorithm) and(O) logistic(B-algorithm) regression(I-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, product, metric, location, task, university, algorithm, conference, programming language, researcher, country, field, person and O.\nSentence: Other linear classification algorithms include Winnow , support vector machine and logistic regression .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","linear","classification","algorithms","include","Winnow",",","support","vector","machine","and","logistic","regression","."],"labels":["O","O","O","O","O","B-algorithm","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["organization","product","metric","location","task","university","algorithm","conference","programming_language","researcher","country","field","person"]}
{"id":"318","dataset":"crossner_ai","split":"test","instance":{"id":"318","prompt_labels":"VTK(B-product) consists(O) of(O) a(O) C(B-programming language) +(I-programming language) +(I-programming language) class(O) library(O) and(O) several(O) interpreted(O) interface(O) layers(O) including(O) Tcl(B-product) /(I-product) Tk(I-product) ,(O) Java(B-programming language) ,(O) and(O) Python(B-programming language) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, field, researcher, product, organization, university, task, algorithm, person, metric, country, location, programming language and O.\nSentence: VTK consists of a C + + class library and several interpreted interface layers including Tcl / Tk , Java , and Python .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["VTK","consists","of","a","C","+","+","class","library","and","several","interpreted","interface","layers","including","Tcl","/","Tk",",","Java",",","and","Python","."],"labels":["B-product","O","O","O","B-programming language","I-programming language","I-programming language","O","O","O","O","O","O","O","O","B-product","I-product","I-product","O","B-programming language","O","O","B-programming language","O"],"target_index":null,"target_label":null},"label_list":["conference","field","researcher","product","organization","university","task","algorithm","person","metric","country","location","programming_language"]}
{"id":"319","dataset":"crossner_ai","split":"test","instance":{"id":"319","prompt_labels":"Also(O) ,(O) text(O) produced(O) by(O) processing(O) spontaneous(O) speech(O) using(O) automatic(B-task) speech(I-task) recognition(I-task) and(O) printed(O) or(O) handwritten(O) text(O) using(O) optical(B-task) character(I-task) recognition(I-task) contains(O) processing(O) noise(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, programming language, algorithm, country, conference, product, university, person, location, researcher, task, metric, field and O.\nSentence: Also , text produced by processing spontaneous speech using automatic speech recognition and printed or handwritten text using optical character recognition contains processing noise .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Also",",","text","produced","by","processing","spontaneous","speech","using","automatic","speech","recognition","and","printed","or","handwritten","text","using","optical","character","recognition","contains","processing","noise","."],"labels":["O","O","O","O","O","O","O","O","O","B-task","I-task","I-task","O","O","O","O","O","O","B-task","I-task","I-task","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","programming_language","algorithm","country","conference","product","university","person","location","researcher","task","metric","field"]}
{"id":"320","dataset":"crossner_ai","split":"test","instance":{"id":"320","prompt_labels":"Miller(B-researcher) wrote(O) several(O) books(O) and(O) directed(O) the(O) development(O) of(O) WordNet(B-product) ,(O) an(O) online(O) word-linkage(O) database(O) usable(O) by(O) computer(O) programs(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, task, metric, organization, location, university, person, algorithm, researcher, field, conference, programming language, product and O.\nSentence: Miller wrote several books and directed the development of WordNet , an online word-linkage database usable by computer programs .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Miller","wrote","several","books","and","directed","the","development","of","WordNet",",","an","online","word-linkage","database","usable","by","computer","programs","."],"labels":["B-researcher","O","O","O","O","O","O","O","O","B-product","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","task","metric","organization","location","university","person","algorithm","researcher","field","conference","programming_language","product"]}
{"id":"321","dataset":"crossner_ai","split":"test","instance":{"id":"321","prompt_labels":"Contemporary(O) automata(B-field) are(O) represented(O) by(O) the(O) works(O) of(O) Cabaret(B-organization) Mechanical(I-organization) Theatre(I-organization) in(O) the(O) United(B-country) Kingdom(I-country) ,(O) Dug(B-person) North(I-person) and(O) Chomick(B-person) +(I-person) Meder(I-person) ,(O) Arthur(B-person) Ganson(I-person) ,(O) Joe(B-person) Jones(I-person) in(O) the(O) United(B-country) States(I-country) ,(O) Le(B-location) Dfenseur(I-location) du(I-location) Temps(I-location) by(O) French(O) artist(O) Jacques(B-person) Monestier(I-person) ,(O) and(O) Franois(B-person) Junod(I-person) in(O) Switzerland(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, task, programming language, field, organization, person, country, product, metric, algorithm, conference, university, location and O.\nSentence: Contemporary automata are represented by the works of Cabaret Mechanical Theatre in the United Kingdom , Dug North and Chomick + Meder , Arthur Ganson , Joe Jones in the United States , Le Dfenseur du Temps by French artist Jacques Monestier , and Franois Junod in Switzerland .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Contemporary","automata","are","represented","by","the","works","of","Cabaret","Mechanical","Theatre","in","the","United","Kingdom",",","Dug","North","and","Chomick","+","Meder",",","Arthur","Ganson",",","Joe","Jones","in","the","United","States",",","Le","Dfenseur","du","Temps","by","French","artist","Jacques","Monestier",",","and","Franois","Junod","in","Switzerland","."],"labels":["O","B-field","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-country","I-country","O","B-person","I-person","O","B-person","I-person","I-person","O","B-person","I-person","O","B-person","I-person","O","O","B-country","I-country","O","B-location","I-location","I-location","I-location","O","O","O","B-person","I-person","O","O","B-person","I-person","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["researcher","task","programming_language","field","organization","person","country","product","metric","algorithm","conference","university","location"]}
{"id":"322","dataset":"crossner_ai","split":"test","instance":{"id":"322","prompt_labels":"MATLAB(B-product) does(O) include(O) standard(O) codefor(O) /(O) code(O) and(O) codewhile(O) /(O) code(O) loops(O) ,(O) but(O) ((O) as(O) in(O) other(O) similar(O) applications(O) such(O) as(O) R(B-programming language) )(O) ,(O) using(O) the(O) vectorized(O) notation(O) is(O) encouraged(O) and(O) is(O) often(O) faster(O) to(O) execute(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, metric, conference, researcher, field, country, location, university, person, programming language, product, organization, algorithm and O.\nSentence: MATLAB does include standard codefor / code and codewhile / code loops , but ( as in other similar applications such as R ) , using the vectorized notation is encouraged and is often faster to execute .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["MATLAB","does","include","standard","codefor","/","code","and","codewhile","/","code","loops",",","but","(","as","in","other","similar","applications","such","as","R",")",",","using","the","vectorized","notation","is","encouraged","and","is","often","faster","to","execute","."],"labels":["B-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-programming language","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["task","metric","conference","researcher","field","country","location","university","person","programming_language","product","organization","algorithm"]}
{"id":"323","dataset":"crossner_ai","split":"test","instance":{"id":"323","prompt_labels":"Pausch(B-researcher) received(O) two(O) awards(O) from(O) Association(B-conference) for(I-conference) Computing(I-conference) Machinery(I-conference) in(O) 2007(O) for(O) his(O) achievements(O) in(O) computing(B-field) education(I-field) :(O) the(O) Karl(O) V.(O) Karlstrom(O) Outstanding(O) Educator(O) Award(O) and(O) the(O) ACM(O) SIGCSE(O) Award(O) for(O) Outstanding(O) Contributions(O) to(O) Computer(O) Science(O) Education(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, location, person, country, task, field, product, conference, algorithm, researcher, programming language, organization, university and O.\nSentence: Pausch received two awards from Association for Computing Machinery in 2007 for his achievements in computing education : the Karl V. Karlstrom Outstanding Educator Award and the ACM SIGCSE Award for Outstanding Contributions to Computer Science Education .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Pausch","received","two","awards","from","Association","for","Computing","Machinery","in","2007","for","his","achievements","in","computing","education",":","the","Karl","V.","Karlstrom","Outstanding","Educator","Award","and","the","ACM","SIGCSE","Award","for","Outstanding","Contributions","to","Computer","Science","Education","."],"labels":["B-researcher","O","O","O","O","B-conference","I-conference","I-conference","I-conference","O","O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","location","person","country","task","field","product","conference","algorithm","researcher","programming_language","organization","university"]}
{"id":"324","dataset":"crossner_ai","split":"test","instance":{"id":"324","prompt_labels":"In(O) 1960(O) ,(O) Devol(B-person) personally(O) sold(O) the(O) first(O) Unimate(B-product) robot(B-product) ,(O) which(O) was(O) shipped(O) in(O) 1961(O) to(O) General(B-organization) Motors(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, product, location, metric, country, programming language, field, person, task, university, organization, conference, researcher and O.\nSentence: In 1960 , Devol personally sold the first Unimate robot , which was shipped in 1961 to General Motors .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1960",",","Devol","personally","sold","the","first","Unimate","robot",",","which","was","shipped","in","1961","to","General","Motors","."],"labels":["O","O","O","B-person","O","O","O","O","B-product","B-product","O","O","O","O","O","O","O","B-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["algorithm","product","location","metric","country","programming_language","field","person","task","university","organization","conference","researcher"]}
{"id":"326","dataset":"crossner_ai","split":"test","instance":{"id":"326","prompt_labels":"Some(O) successful(O) applications(O) of(O) deep(B-field) learning(I-field) are(O) computer(B-field) vision(I-field) and(O) speech(B-task) recognition(I-task) .(O) Honglak(B-researcher) Lee(I-researcher) ,(O) Roger(B-researcher) Grosse(I-researcher) ,(O) Rajesh(B-researcher) Ranganath(I-researcher) ,(O) Andrew(B-researcher) Y.(I-researcher) Ng(I-researcher) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, organization, person, field, product, university, algorithm, researcher, metric, country, programming language, location, conference and O.\nSentence: Some successful applications of deep learning are computer vision and speech recognition . Honglak Lee , Roger Grosse , Rajesh Ranganath , Andrew Y. Ng .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","successful","applications","of","deep","learning","are","computer","vision","and","speech","recognition",".","Honglak","Lee",",","Roger","Grosse",",","Rajesh","Ranganath",",","Andrew","Y.","Ng","."],"labels":["O","O","O","O","B-field","I-field","O","B-field","I-field","O","B-task","I-task","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","B-researcher","I-researcher","I-researcher","O"],"target_index":null,"target_label":null},"label_list":["task","organization","person","field","product","university","algorithm","researcher","metric","country","programming_language","location","conference"]}
{"id":"327","dataset":"crossner_ai","split":"test","instance":{"id":"327","prompt_labels":"In(O) addition(O) to(O) maintaining(O) the(O) Discovery(B-product) One(I-product) spacecraft(I-product) systems(I-product) during(O) the(O) interplanetary(O) mission(O) to(O) Jupiter(O) ((O) or(O) Saturn(O) in(O) the(O) novel(O) )(O) ,(O) HAL(B-product) is(O) capable(O) of(O) speech(B-task) synthesis(I-task) ,(O) speech(B-task) recognition(I-task) ,(O) facial(B-task) recognition(I-task) ,(O) natural(B-field) language(I-field) processing(I-field) ,(O) lip(B-task) reading(I-task) ,(O) art(B-field) appreciation(I-field) ,(O) Affective(B-task) computing(I-task) ,(O) automated(B-task) reasoning(I-task) ,(O) spacecraft(B-task) piloting(I-task) and(O) playing(B-task) chess(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, algorithm, organization, location, person, university, metric, country, researcher, product, field, task, programming language and O.\nSentence: In addition to maintaining the Discovery One spacecraft systems during the interplanetary mission to Jupiter ( or Saturn in the novel ) , HAL is capable of speech synthesis , speech recognition , facial recognition , natural language processing , lip reading , art appreciation , Affective computing , automated reasoning , spacecraft piloting and playing chess .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","addition","to","maintaining","the","Discovery","One","spacecraft","systems","during","the","interplanetary","mission","to","Jupiter","(","or","Saturn","in","the","novel",")",",","HAL","is","capable","of","speech","synthesis",",","speech","recognition",",","facial","recognition",",","natural","language","processing",",","lip","reading",",","art","appreciation",",","Affective","computing",",","automated","reasoning",",","spacecraft","piloting","and","playing","chess","."],"labels":["O","O","O","O","O","B-product","I-product","I-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","O","O","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O","B-field","I-field","I-field","O","B-task","I-task","O","B-field","I-field","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["conference","algorithm","organization","location","person","university","metric","country","researcher","product","field","task","programming_language"]}
{"id":"328","dataset":"crossner_ai","split":"test","instance":{"id":"328","prompt_labels":"Dr.(B-researcher) Julesz(I-researcher) emigrated(O) from(O) Hungary(B-country) to(O) the(B-country) United(I-country) States(I-country) following(O) the(O) 1956(O) Soviet(B-country) invasion(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, university, conference, product, location, person, researcher, programming language, task, country, organization, metric, algorithm and O.\nSentence: Dr. Julesz emigrated from Hungary to the United States following the 1956 Soviet invasion .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Dr.","Julesz","emigrated","from","Hungary","to","the","United","States","following","the","1956","Soviet","invasion","."],"labels":["B-researcher","I-researcher","O","O","B-country","O","B-country","I-country","I-country","O","O","O","B-country","O","O"],"target_index":null,"target_label":null},"label_list":["field","university","conference","product","location","person","researcher","programming_language","task","country","organization","metric","algorithm"]}
{"id":"329","dataset":"crossner_ai","split":"test","instance":{"id":"329","prompt_labels":"Sigmoid(B-algorithm) function(I-algorithm) activation(O) functions(O) use(O) a(O) second(O) non-linearity(O) for(O) large(O) inputs(O) :(O) math(O) \\(O) phi(O) ((O) v(O) _(O) i(O) )(O) =(O) ((O) 1(O) +(O) \\(O) exp(O) ((O) -v(O) _(O) i(O) )(O) )(O) ^(O) {(O) -1(O) }(O) /(O) math(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, product, location, university, researcher, task, country, algorithm, person, field, programming language, metric, conference and O.\nSentence: Sigmoid function activation functions use a second non-linearity for large inputs : math \\ phi ( v _ i ) = ( 1 + \\ exp ( -v _ i ) ) ^ { -1 } / math .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Sigmoid","function","activation","functions","use","a","second","non-linearity","for","large","inputs",":","math","\\","phi","(","v","_","i",")","=","(","1","+","\\","exp","(","-v","_","i",")",")","^","{","-1","}","/","math","."],"labels":["B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","product","location","university","researcher","task","country","algorithm","person","field","programming_language","metric","conference"]}
{"id":"330","dataset":"crossner_ai","split":"test","instance":{"id":"330","prompt_labels":"These(O) probabilities(O) are(O) used(O) to(O) determine(O) what(O) the(O) target(O) is(O) using(O) a(O) maximum(B-algorithm) likelihood(I-algorithm) decision(I-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, algorithm, organization, conference, field, location, country, metric, task, person, product, programming language, researcher and O.\nSentence: These probabilities are used to determine what the target is using a maximum likelihood decision .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["These","probabilities","are","used","to","determine","what","the","target","is","using","a","maximum","likelihood","decision","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["university","algorithm","organization","conference","field","location","country","metric","task","person","product","programming_language","researcher"]}
{"id":"331","dataset":"crossner_ai","split":"test","instance":{"id":"331","prompt_labels":"In(O) 1984(O) he(O) moved(O) to(O) the(O) University(B-university) of(I-university) Konstanz(I-university) and(O) in(O) 1990(O) to(O) the(O) University(B-university) of(I-university) Salzburg(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, algorithm, task, conference, country, person, metric, researcher, programming language, product, organization, university, field and O.\nSentence: In 1984 he moved to the University of Konstanz and in 1990 to the University of Salzburg .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1984","he","moved","to","the","University","of","Konstanz","and","in","1990","to","the","University","of","Salzburg","."],"labels":["O","O","O","O","O","O","B-university","I-university","I-university","O","O","O","O","O","B-university","I-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["location","algorithm","task","conference","country","person","metric","researcher","programming_language","product","organization","university","field"]}
{"id":"332","dataset":"crossner_ai","split":"test","instance":{"id":"332","prompt_labels":"Some(O) popular(O) fitness(O) functions(O) based(O) on(O) the(O) confusion(B-metric) matrix(I-metric) include(O) sensitivity(B-metric) /(I-metric) specificity(I-metric) ,(O) recall(B-metric) /(I-metric) precision(I-metric) ,(O) F-measure(B-metric) ,(O) Jaccard(B-metric) similarity(I-metric) ,(O) Matthews(B-metric) correlation(I-metric) coefficient(I-metric) ,(O) and(O) cost(B-metric) /(I-metric) gain(I-metric) matrix(I-metric) which(O) combines(O) the(O) costs(O) and(O) gains(O) assigned(O) to(O) the(O) 4(O) different(O) types(O) of(O) classifications(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, researcher, university, metric, country, task, field, location, algorithm, conference, programming language, product, organization and O.\nSentence: Some popular fitness functions based on the confusion matrix include sensitivity / specificity , recall / precision , F-measure , Jaccard similarity , Matthews correlation coefficient , and cost / gain matrix which combines the costs and gains assigned to the 4 different types of classifications .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","popular","fitness","functions","based","on","the","confusion","matrix","include","sensitivity","/","specificity",",","recall","/","precision",",","F-measure",",","Jaccard","similarity",",","Matthews","correlation","coefficient",",","and","cost","/","gain","matrix","which","combines","the","costs","and","gains","assigned","to","the","4","different","types","of","classifications","."],"labels":["O","O","O","O","O","O","O","B-metric","I-metric","O","B-metric","I-metric","I-metric","O","B-metric","I-metric","I-metric","O","B-metric","O","B-metric","I-metric","O","B-metric","I-metric","I-metric","O","O","B-metric","I-metric","I-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","researcher","university","metric","country","task","field","location","algorithm","conference","programming_language","product","organization"]}
{"id":"334","dataset":"crossner_ai","split":"test","instance":{"id":"334","prompt_labels":"Industrial(B-product) robots(I-product) have(O) been(O) implemented(O) to(O) collaborate(O) with(O) humans(O) to(O) perform(O) industrial(O) manufacturing(O) tasks(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, country, task, university, programming language, algorithm, organization, person, location, researcher, field, product, metric and O.\nSentence: Industrial robots have been implemented to collaborate with humans to perform industrial manufacturing tasks .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Industrial","robots","have","been","implemented","to","collaborate","with","humans","to","perform","industrial","manufacturing","tasks","."],"labels":["B-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["conference","country","task","university","programming_language","algorithm","organization","person","location","researcher","field","product","metric"]}
{"id":"336","dataset":"crossner_ai","split":"test","instance":{"id":"336","prompt_labels":"NIST(B-metric) also(O) differs(O) from(O) BLEU(B-metric) in(O) its(O) calculation(O) of(O) the(O) brevity(O) penalty(O) ,(O) insofar(O) as(O) small(O) variations(O) in(O) translation(O) length(O) do(O) not(O) impact(O) the(O) overall(O) score(O) as(O) much(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, conference, researcher, person, product, organization, university, programming language, algorithm, field, metric, country, task and O.\nSentence: NIST also differs from BLEU in its calculation of the brevity penalty , insofar as small variations in translation length do not impact the overall score as much .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["NIST","also","differs","from","BLEU","in","its","calculation","of","the","brevity","penalty",",","insofar","as","small","variations","in","translation","length","do","not","impact","the","overall","score","as","much","."],"labels":["B-metric","O","O","O","B-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","conference","researcher","person","product","organization","university","programming_language","algorithm","field","metric","country","task"]}
{"id":"337","dataset":"crossner_ai","split":"test","instance":{"id":"337","prompt_labels":"The(O) IJCAI(O) Award(O) for(O) Research(O) Excellence(O) is(O) a(O) biannual(O) award(O) given(O) at(O) the(O) IJCAI(B-conference) conference(O) to(O) researcher(O) in(O) artificial(B-field) intelligence(I-field) as(O) a(O) recognition(O) of(O) excellence(O) of(O) their(O) career(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, field, algorithm, organization, researcher, person, country, product, location, programming language, metric, university, task and O.\nSentence: The IJCAI Award for Research Excellence is a biannual award given at the IJCAI conference to researcher in artificial intelligence as a recognition of excellence of their career .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","IJCAI","Award","for","Research","Excellence","is","a","biannual","award","given","at","the","IJCAI","conference","to","researcher","in","artificial","intelligence","as","a","recognition","of","excellence","of","their","career","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-conference","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["conference","field","algorithm","organization","researcher","person","country","product","location","programming_language","metric","university","task"]}
{"id":"340","dataset":"crossner_ai","split":"test","instance":{"id":"340","prompt_labels":"An(O) alternative(O) to(O) the(O) use(O) of(O) the(O) definitions(O) is(O) to(O) consider(O) general(O) word-sense(O) relatedness(O) and(O) to(O) compute(O) the(O) similarity(O) of(O) each(O) pair(O) of(O) word(O) senses(O) based(O) on(O) a(O) given(O) lexical(O) knowledge(O) base(O) such(O) as(O) WordNet(B-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, product, university, metric, researcher, field, programming language, algorithm, location, conference, task, person, country and O.\nSentence: An alternative to the use of the definitions is to consider general word-sense relatedness and to compute the similarity of each pair of word senses based on a given lexical knowledge base such as WordNet .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["An","alternative","to","the","use","of","the","definitions","is","to","consider","general","word-sense","relatedness","and","to","compute","the","similarity","of","each","pair","of","word","senses","based","on","a","given","lexical","knowledge","base","such","as","WordNet","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","O"],"target_index":null,"target_label":null},"label_list":["organization","product","university","metric","researcher","field","programming_language","algorithm","location","conference","task","person","country"]}
{"id":"341","dataset":"crossner_ai","split":"test","instance":{"id":"341","prompt_labels":"TD-Lambda(B-algorithm) is(O) a(O) learning(O) algorithm(O) invented(O) by(O) Richard(B-researcher) S.(I-researcher) Sutton(I-researcher) based(O) on(O) earlier(O) work(O) on(O) temporal(O) difference(O) learning(O) by(O) Arthur(B-researcher) Samuel(I-researcher) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, programming language, person, university, metric, task, field, product, location, algorithm, researcher, country, organization and O.\nSentence: TD-Lambda is a learning algorithm invented by Richard S. Sutton based on earlier work on temporal difference learning by Arthur Samuel .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["TD-Lambda","is","a","learning","algorithm","invented","by","Richard","S.","Sutton","based","on","earlier","work","on","temporal","difference","learning","by","Arthur","Samuel","."],"labels":["B-algorithm","O","O","O","O","O","O","B-researcher","I-researcher","I-researcher","O","O","O","O","O","O","O","O","O","B-researcher","I-researcher","O"],"target_index":null,"target_label":null},"label_list":["conference","programming_language","person","university","metric","task","field","product","location","algorithm","researcher","country","organization"]}
{"id":"342","dataset":"crossner_ai","split":"test","instance":{"id":"342","prompt_labels":"In(O) data(B-field) mining(I-field) and(O) statistics(B-field) ,(O) hierarchical(B-task) clustering(I-task) ((O) also(O) called(O) hierarchical(B-task) cluster(I-task) analysis(I-task) or(O) HCA(B-task) )(O) is(O) a(O) method(O) of(O) cluster(B-task) analysis(I-task) which(O) seeks(O) to(O) build(O) a(O) hierarchy(O) of(O) clusters(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, conference, person, country, location, product, metric, organization, field, task, programming language, algorithm, researcher and O.\nSentence: In data mining and statistics , hierarchical clustering ( also called hierarchical cluster analysis or HCA ) is a method of cluster analysis which seeks to build a hierarchy of clusters .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","data","mining","and","statistics",",","hierarchical","clustering","(","also","called","hierarchical","cluster","analysis","or","HCA",")","is","a","method","of","cluster","analysis","which","seeks","to","build","a","hierarchy","of","clusters","."],"labels":["O","B-field","I-field","O","B-field","O","B-task","I-task","O","O","O","B-task","I-task","I-task","O","B-task","O","O","O","O","O","B-task","I-task","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","conference","person","country","location","product","metric","organization","field","task","programming_language","algorithm","researcher"]}
{"id":"343","dataset":"crossner_ai","split":"test","instance":{"id":"343","prompt_labels":"The(O) concept(O) of(O) deconvolution(B-algorithm) is(O) widely(O) used(O) in(O) the(O) techniques(O) of(O) signal(B-field) processing(I-field) and(O) image(B-field) processing(I-field) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, product, location, task, field, university, programming language, conference, organization, person, researcher, metric, algorithm and O.\nSentence: The concept of deconvolution is widely used in the techniques of signal processing and image processing .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","concept","of","deconvolution","is","widely","used","in","the","techniques","of","signal","processing","and","image","processing","."],"labels":["O","O","O","B-algorithm","O","O","O","O","O","O","O","B-field","I-field","O","B-field","I-field","O"],"target_index":null,"target_label":null},"label_list":["country","product","location","task","field","university","programming_language","conference","organization","person","researcher","metric","algorithm"]}
{"id":"344","dataset":"crossner_ai","split":"test","instance":{"id":"344","prompt_labels":"Cognitive(B-algorithm) maps(I-algorithm) serve(O) the(O) construction(O) and(O) accumulation(O) of(O) spatial(O) knowledge(O) ,(O) allowing(O) the(O) mind(O) 's(O) eye(O) to(O) visualize(O) images(O) in(O) order(O) to(O) reduce(O) cognitive(O) load(O) ,(O) enhance(O) recall(B-metric) and(O) learning(O) of(O) information(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, algorithm, conference, programming language, metric, researcher, country, organization, task, field, product, university, person and O.\nSentence: Cognitive maps serve the construction and accumulation of spatial knowledge , allowing the mind 's eye to visualize images in order to reduce cognitive load , enhance recall and learning of information .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Cognitive","maps","serve","the","construction","and","accumulation","of","spatial","knowledge",",","allowing","the","mind","'s","eye","to","visualize","images","in","order","to","reduce","cognitive","load",",","enhance","recall","and","learning","of","information","."],"labels":["B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","algorithm","conference","programming_language","metric","researcher","country","organization","task","field","product","university","person"]}
{"id":"345","dataset":"crossner_ai","split":"test","instance":{"id":"345","prompt_labels":",(O) typically(O) providing(O) bindings(O) to(O) languages(O) such(O) as(O) Python(B-programming language) ,(O) C(B-programming language) +(I-programming language) +(I-programming language) ,(O) Java(B-programming language) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, product, country, conference, metric, organization, person, university, programming language, algorithm, location, task, researcher and O.\nSentence: , typically providing bindings to languages such as Python , C + + , Java ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":[",","typically","providing","bindings","to","languages","such","as","Python",",","C","+","+",",","Java",")","."],"labels":["O","O","O","O","O","O","O","O","B-programming language","O","B-programming language","I-programming language","I-programming language","O","B-programming language","O","O"],"target_index":null,"target_label":null},"label_list":["field","product","country","conference","metric","organization","person","university","programming_language","algorithm","location","task","researcher"]}
{"id":"347","dataset":"crossner_ai","split":"test","instance":{"id":"347","prompt_labels":"Jess(B-programming language) is(O) a(O) rule(O) engine(O) for(O) the(O) Java(B-programming language) platform(O) that(O) was(O) developed(O) by(O) Ernest(B-researcher) Friedman-Hill(I-researcher) of(O) Sandia(B-organization) National(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, algorithm, task, location, person, university, product, country, metric, conference, field, organization, researcher and O.\nSentence: Jess is a rule engine for the Java platform that was developed by Ernest Friedman-Hill of Sandia National .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Jess","is","a","rule","engine","for","the","Java","platform","that","was","developed","by","Ernest","Friedman-Hill","of","Sandia","National","."],"labels":["B-programming language","O","O","O","O","O","O","B-programming language","O","O","O","O","O","B-researcher","I-researcher","O","B-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["programming_language","algorithm","task","location","person","university","product","country","metric","conference","field","organization","researcher"]}
{"id":"351","dataset":"crossner_ai","split":"test","instance":{"id":"351","prompt_labels":"|(O) Apple(B-organization) Apple(B-organization) Inc(I-organization) originally(O) licensed(O) software(O) from(O) Nuance(B-organization) to(O) provide(O) speech(B-task) recognition(I-task) capability(O) to(O) its(O) digital(O) assistant(O) Siri(B-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, conference, task, organization, metric, field, researcher, programming language, person, location, algorithm, product, country and O.\nSentence: | Apple Apple Inc originally licensed software from Nuance to provide speech recognition capability to its digital assistant Siri .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["|","Apple","Apple","Inc","originally","licensed","software","from","Nuance","to","provide","speech","recognition","capability","to","its","digital","assistant","Siri","."],"labels":["O","B-organization","B-organization","I-organization","O","O","O","O","B-organization","O","O","B-task","I-task","O","O","O","O","O","B-product","O"],"target_index":null,"target_label":null},"label_list":["university","conference","task","organization","metric","field","researcher","programming_language","person","location","algorithm","product","country"]}
{"id":"352","dataset":"crossner_ai","split":"test","instance":{"id":"352","prompt_labels":"Columbia(B-organization) released(O) several(O) 3D(O) westerns(O) produced(O) by(O) Sam(B-person) Katzman(I-person) and(O) directed(O) by(O) William(B-person) Castle(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, location, university, country, task, field, researcher, organization, algorithm, product, conference, person, programming language and O.\nSentence: Columbia released several 3D westerns produced by Sam Katzman and directed by William Castle .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Columbia","released","several","3D","westerns","produced","by","Sam","Katzman","and","directed","by","William","Castle","."],"labels":["B-organization","O","O","O","O","O","O","B-person","I-person","O","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["metric","location","university","country","task","field","researcher","organization","algorithm","product","conference","person","programming_language"]}
{"id":"353","dataset":"crossner_ai","split":"test","instance":{"id":"353","prompt_labels":"It(O) incorporates(O) knowledge(O) and(O) research(O) in(O) the(O) computer(B-field) science(I-field) ,(O) linguistics(B-field) and(O) computer(B-field) engineering(I-field) fields(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, product, field, conference, organization, researcher, metric, country, university, algorithm, person, task, location and O.\nSentence: It incorporates knowledge and research in the computer science , linguistics and computer engineering fields .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","incorporates","knowledge","and","research","in","the","computer","science",",","linguistics","and","computer","engineering","fields","."],"labels":["O","O","O","O","O","O","O","B-field","I-field","O","B-field","O","B-field","I-field","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","product","field","conference","organization","researcher","metric","country","university","algorithm","person","task","location"]}
{"id":"354","dataset":"crossner_ai","split":"test","instance":{"id":"354","prompt_labels":"Here(O) is(O) an(O) example(O) of(O) R(B-programming language) code(O) :(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, researcher, country, product, field, university, programming language, location, algorithm, metric, organization, person, task and O.\nSentence: Here is an example of R code :","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Here","is","an","example","of","R","code",":"],"labels":["O","O","O","O","O","B-programming language","O","O"],"target_index":null,"target_label":null},"label_list":["conference","researcher","country","product","field","university","programming_language","location","algorithm","metric","organization","person","task"]}
{"id":"355","dataset":"crossner_ai","split":"test","instance":{"id":"355","prompt_labels":"The(O) ROC(B-metric) curve(I-metric) is(O) created(O) by(O) plotting(O) the(O) TRUE(B-metric) positive(I-metric) rate(I-metric) ((O) TPR(B-metric) )(O) against(O) the(O) FALSE(B-metric) positive(I-metric) rate(I-metric) ((O) FPR(B-metric) )(O) at(O) various(O) threshold(O) settings(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, task, location, person, metric, algorithm, conference, programming language, university, country, organization, researcher, field and O.\nSentence: The ROC curve is created by plotting the TRUE positive rate ( TPR ) against the FALSE positive rate ( FPR ) at various threshold settings .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","ROC","curve","is","created","by","plotting","the","TRUE","positive","rate","(","TPR",")","against","the","FALSE","positive","rate","(","FPR",")","at","various","threshold","settings","."],"labels":["O","B-metric","I-metric","O","O","O","O","O","B-metric","I-metric","I-metric","O","B-metric","O","O","O","B-metric","I-metric","I-metric","O","B-metric","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["product","task","location","person","metric","algorithm","conference","programming_language","university","country","organization","researcher","field"]}
{"id":"358","dataset":"crossner_ai","split":"test","instance":{"id":"358","prompt_labels":"The(O) metric(O) was(O) designed(O) to(O) fix(O) some(O) of(O) the(O) problems(O) found(O) in(O) the(O) more(O) popular(O) BLEU(B-metric) metric(I-metric) ,(O) and(O) also(O) produce(O) good(O) correlation(O) with(O) human(O) judgement(O) at(O) the(O) sentence(O) or(O) segment(O) level(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, country, conference, university, algorithm, metric, product, organization, field, programming language, task, location, person and O.\nSentence: The metric was designed to fix some of the problems found in the more popular BLEU metric , and also produce good correlation with human judgement at the sentence or segment level .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","metric","was","designed","to","fix","some","of","the","problems","found","in","the","more","popular","BLEU","metric",",","and","also","produce","good","correlation","with","human","judgement","at","the","sentence","or","segment","level","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","country","conference","university","algorithm","metric","product","organization","field","programming_language","task","location","person"]}
{"id":"360","dataset":"crossner_ai","split":"test","instance":{"id":"360","prompt_labels":"Mass-produced(O) printed(B-product) circuit(I-product) board(I-product) s(O) ((O) PCBs(B-product) )(O) are(O) almost(O) exclusively(O) manufactured(O) by(O) pick-and-place(B-product) robots(I-product) ,(O) typically(O) with(O) SCARA(B-product) manipulators(O) ,(O) which(O) remove(O) tiny(O) electronic(O) component(O) s(O) from(O) strips(O) or(O) trays(O) ,(O) and(O) place(O) them(O) on(O) to(O) PCBs(B-product) with(O) great(O) accuracy(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, person, location, conference, metric, product, task, organization, field, country, programming language, university, researcher and O.\nSentence: Mass-produced printed circuit board s ( PCBs ) are almost exclusively manufactured by pick-and-place robots , typically with SCARA manipulators , which remove tiny electronic component s from strips or trays , and place them on to PCBs with great accuracy .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Mass-produced","printed","circuit","board","s","(","PCBs",")","are","almost","exclusively","manufactured","by","pick-and-place","robots",",","typically","with","SCARA","manipulators",",","which","remove","tiny","electronic","component","s","from","strips","or","trays",",","and","place","them","on","to","PCBs","with","great","accuracy","."],"labels":["O","B-product","I-product","I-product","O","O","B-product","O","O","O","O","O","O","B-product","I-product","O","O","O","B-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["algorithm","person","location","conference","metric","product","task","organization","field","country","programming_language","university","researcher"]}
{"id":"361","dataset":"crossner_ai","split":"test","instance":{"id":"361","prompt_labels":"In(O) the(O) context(O) of(O) machine(B-field) learning(I-field) ,(O) where(O) it(O) is(O) most(O) widely(O) applied(O) today(O) ,(O) LDA(B-algorithm) was(O) rediscovered(O) independently(O) by(O) David(B-researcher) Blei(I-researcher) ,(O) Andrew(B-researcher) Ng(I-researcher) and(O) Michael(B-researcher) I.(I-researcher) Jordan(I-researcher) in(O) 2003(O) ,(O) and(O) presented(O) as(O) a(O) graphical(B-algorithm) model(I-algorithm) for(O) topic(B-task) discovery(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, conference, field, university, metric, person, organization, programming language, product, country, algorithm, task, location and O.\nSentence: In the context of machine learning , where it is most widely applied today , LDA was rediscovered independently by David Blei , Andrew Ng and Michael I. Jordan in 2003 , and presented as a graphical model for topic discovery .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","context","of","machine","learning",",","where","it","is","most","widely","applied","today",",","LDA","was","rediscovered","independently","by","David","Blei",",","Andrew","Ng","and","Michael","I.","Jordan","in","2003",",","and","presented","as","a","graphical","model","for","topic","discovery","."],"labels":["O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","B-algorithm","O","O","O","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","B-researcher","I-researcher","I-researcher","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["researcher","conference","field","university","metric","person","organization","programming_language","product","country","algorithm","task","location"]}
{"id":"362","dataset":"crossner_ai","split":"test","instance":{"id":"362","prompt_labels":"The(O) measured(O) performance(O) on(O) test(O) data(O) of(O) eight(O) naive(O) WSI(B-task) across(O) various(O) tauopathies(O) resulted(O) in(O) the(O) recall(B-metric) ,(O) precision(B-metric) ,(O) and(O) an(O) F1(B-metric) score(I-metric) of(O) 0.92(O) ,(O) 0.72(O) ,(O) and(O) 0.81(O) ,(O) respectively(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, person, metric, organization, conference, university, researcher, algorithm, task, country, field, product, location and O.\nSentence: The measured performance on test data of eight naive WSI across various tauopathies resulted in the recall , precision , and an F1 score of 0.92 , 0.72 , and 0.81 , respectively .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","measured","performance","on","test","data","of","eight","naive","WSI","across","various","tauopathies","resulted","in","the","recall",",","precision",",","and","an","F1","score","of","0.92",",","0.72",",","and","0.81",",","respectively","."],"labels":["O","O","O","O","O","O","O","O","O","B-task","O","O","O","O","O","O","B-metric","O","B-metric","O","O","O","B-metric","I-metric","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","person","metric","organization","conference","university","researcher","algorithm","task","country","field","product","location"]}
{"id":"363","dataset":"crossner_ai","split":"test","instance":{"id":"363","prompt_labels":"With(O) the(O) help(O) of(O) advanced(O) AR(B-field) technologies(O) ((O) e.g.(O) adding(O) computer(B-field) vision(I-field) ,(O) incorporating(O) AR(B-field) cameras(O) into(O) smartphone(O) and(O) object(B-task) recognition(I-task) )(O) the(O) information(O) about(O) the(O) surrounding(O) real(O) world(O) of(O) the(O) user(O) becomes(O) interactive(O) and(O) digitally(O) manipulated(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, organization, algorithm, university, researcher, task, conference, product, field, programming language, location, person, country and O.\nSentence: With the help of advanced AR technologies ( e.g. adding computer vision , incorporating AR cameras into smartphone and object recognition ) the information about the surrounding real world of the user becomes interactive and digitally manipulated .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["With","the","help","of","advanced","AR","technologies","(","e.g.","adding","computer","vision",",","incorporating","AR","cameras","into","smartphone","and","object","recognition",")","the","information","about","the","surrounding","real","world","of","the","user","becomes","interactive","and","digitally","manipulated","."],"labels":["O","O","O","O","O","B-field","O","O","O","O","B-field","I-field","O","O","B-field","O","O","O","O","B-task","I-task","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","organization","algorithm","university","researcher","task","conference","product","field","programming_language","location","person","country"]}
{"id":"364","dataset":"crossner_ai","split":"test","instance":{"id":"364","prompt_labels":"In(O) 2014(O) ,(O) Schmidhuber(B-researcher) formed(O) a(O) company(O) ,(O) Nnaisense(B-organization) ,(O) to(O) work(O) on(O) commercial(O) applications(O) of(O) artificial(B-field) intelligence(I-field) in(O) fields(O) such(O) as(O) finance(O) ,(O) heavy(O) industry(O) and(O) self-driving(B-product) car(I-product) s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, person, product, algorithm, organization, field, university, task, conference, metric, country, location, researcher and O.\nSentence: In 2014 , Schmidhuber formed a company , Nnaisense , to work on commercial applications of artificial intelligence in fields such as finance , heavy industry and self-driving car s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2014",",","Schmidhuber","formed","a","company",",","Nnaisense",",","to","work","on","commercial","applications","of","artificial","intelligence","in","fields","such","as","finance",",","heavy","industry","and","self-driving","car","s","."],"labels":["O","O","O","B-researcher","O","O","O","O","B-organization","O","O","O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","B-product","I-product","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","person","product","algorithm","organization","field","university","task","conference","metric","country","location","researcher"]}
{"id":"366","dataset":"crossner_ai","split":"test","instance":{"id":"366","prompt_labels":"Bigrams(O) are(O) used(O) in(O) most(O) successful(O) language(B-algorithm) model(I-algorithm) s(O) for(O) speech(B-task) recognition(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, metric, programming language, organization, country, algorithm, product, university, researcher, task, location, field, person and O.\nSentence: Bigrams are used in most successful language model s for speech recognition .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Bigrams","are","used","in","most","successful","language","model","s","for","speech","recognition","."],"labels":["O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["conference","metric","programming_language","organization","country","algorithm","product","university","researcher","task","location","field","person"]}
{"id":"367","dataset":"crossner_ai","split":"test","instance":{"id":"367","prompt_labels":"His(O) research(O) in(O) cognitive(B-field) psychology(I-field) has(O) won(O) the(O) Early(O) Career(O) Award(O) ((O) 1984(O) )(O) and(O) Boyd(O) McCandless(O) Award(O) 1986(O) )(O) from(O) the(O) American(B-organization) Psychological(I-organization) Association(I-organization) ,(O) the(O) Troland(O) Research(O) Award(O) ((O) 1993(O) )(O) from(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) the(O) Henry(O) Dale(O) Prize(O) ((O) 2004(O) )(O) from(O) the(O) Royal(B-organization) Institution(I-organization) of(I-organization) Great(I-organization) Britain(I-organization) ,(O) and(O) the(O) George(O) Miller(O) Prize(O) ((O) 2010(O) )(O) from(O) the(O) Cognitive(B-organization) Neuroscience(I-organization) Society(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, university, programming language, conference, metric, organization, algorithm, task, location, researcher, field, product, country and O.\nSentence: His research in cognitive psychology has won the Early Career Award ( 1984 ) and Boyd McCandless Award 1986 ) from the American Psychological Association , the Troland Research Award ( 1993 ) from the National Academy of Sciences , the Henry Dale Prize ( 2004 ) from the Royal Institution of Great Britain , and the George Miller Prize ( 2010 ) from the Cognitive Neuroscience Society .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","research","in","cognitive","psychology","has","won","the","Early","Career","Award","(","1984",")","and","Boyd","McCandless","Award","1986",")","from","the","American","Psychological","Association",",","the","Troland","Research","Award","(","1993",")","from","the","National","Academy","of","Sciences",",","the","Henry","Dale","Prize","(","2004",")","from","the","Royal","Institution","of","Great","Britain",",","and","the","George","Miller","Prize","(","2010",")","from","the","Cognitive","Neuroscience","Society","."],"labels":["O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["person","university","programming_language","conference","metric","organization","algorithm","task","location","researcher","field","product","country"]}
{"id":"369","dataset":"crossner_ai","split":"test","instance":{"id":"369","prompt_labels":"A(O) lexical(O) dictionary(O) such(O) as(O) WordNet(B-product) can(O) then(O) be(O) used(O) for(O) understanding(O) the(O) context(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, university, researcher, programming language, task, conference, country, metric, location, person, algorithm, product, organization and O.\nSentence: A lexical dictionary such as WordNet can then be used for understanding the context .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","lexical","dictionary","such","as","WordNet","can","then","be","used","for","understanding","the","context","."],"labels":["O","O","O","O","O","B-product","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","university","researcher","programming_language","task","conference","country","metric","location","person","algorithm","product","organization"]}
{"id":"370","dataset":"crossner_ai","split":"test","instance":{"id":"370","prompt_labels":"Hyponymy(O) is(O) the(O) most(O) frequently(O) encoded(O) relation(O) among(O) synsets(O) used(O) in(O) lexical(O) databases(O) such(O) as(O) WordNet(B-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, task, programming language, country, conference, metric, algorithm, field, location, product, researcher, university and O.\nSentence: Hyponymy is the most frequently encoded relation among synsets used in lexical databases such as WordNet .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Hyponymy","is","the","most","frequently","encoded","relation","among","synsets","used","in","lexical","databases","such","as","WordNet","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","O"],"target_index":null,"target_label":null},"label_list":["organization","person","task","programming_language","country","conference","metric","algorithm","field","location","product","researcher","university"]}
{"id":"372","dataset":"crossner_ai","split":"test","instance":{"id":"372","prompt_labels":"In(O) that(O) page(O) ,(O) Samurai(O) Damashii(O) exaggerated(O) the(O) Senkousha(B-product) as(O) the(O) crystallization(O) of(O) China(B-country) 's(O) four(O) thousand(O) years(O) of(O) scientific(O) knowledge(O) ,(O) commented(O) on(O) the(O) crude(O) design(O) ((O) e.g.(O) the(O) Chinese(O) Cannon(O) on(O) its(O) crotch(O) )(O) ,(O) and(O) put(O) its(O) image(O) among(O) images(O) of(O) Honda(B-organization) '(O) s(O) ASIMO(B-product) and(O) Sony(B-organization) '(O) s(O) QRIO(B-product) SDR-3X(I-product) for(O) juxtaposition(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, metric, programming language, algorithm, organization, university, researcher, country, person, conference, task, field, location and O.\nSentence: In that page , Samurai Damashii exaggerated the Senkousha as the crystallization of China 's four thousand years of scientific knowledge , commented on the crude design ( e.g. the Chinese Cannon on its crotch ) , and put its image among images of Honda ' s ASIMO and Sony ' s QRIO SDR-3X for juxtaposition .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","that","page",",","Samurai","Damashii","exaggerated","the","Senkousha","as","the","crystallization","of","China","'s","four","thousand","years","of","scientific","knowledge",",","commented","on","the","crude","design","(","e.g.","the","Chinese","Cannon","on","its","crotch",")",",","and","put","its","image","among","images","of","Honda","'","s","ASIMO","and","Sony","'","s","QRIO","SDR-3X","for","juxtaposition","."],"labels":["O","O","O","O","O","O","O","O","B-product","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","O","B-product","O","B-organization","O","O","B-product","I-product","O","O","O"],"target_index":null,"target_label":null},"label_list":["product","metric","programming_language","algorithm","organization","university","researcher","country","person","conference","task","field","location"]}
{"id":"373","dataset":"crossner_ai","split":"test","instance":{"id":"373","prompt_labels":"There(O) are(O) also(O) many(O) programming(O) libraries(O) that(O) contain(O) neural(B-algorithm) network(I-algorithm) functionality(O) and(O) that(O) can(O) be(O) used(O) in(O) custom(O) implementations(O) ((O) such(O) as(O) TensorFlow(B-product) ,(O) Theano(B-product) ,(O) etc(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, university, task, field, organization, location, product, researcher, person, algorithm, conference, programming language, country and O.\nSentence: There are also many programming libraries that contain neural network functionality and that can be used in custom implementations ( such as TensorFlow , Theano , etc .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["There","are","also","many","programming","libraries","that","contain","neural","network","functionality","and","that","can","be","used","in","custom","implementations","(","such","as","TensorFlow",",","Theano",",","etc","."],"labels":["O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","B-product","O","B-product","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","university","task","field","organization","location","product","researcher","person","algorithm","conference","programming_language","country"]}
{"id":"374","dataset":"crossner_ai","split":"test","instance":{"id":"374","prompt_labels":"He(O) is(O) a(O) Fellow(O) of(O) the(O) Association(B-conference) for(I-conference) Computing(I-conference) Machinery(I-conference) ,(O) IEEE(B-organization) ,(O) American(B-conference) Association(I-conference) for(I-conference) the(I-conference) Advancement(I-conference) of(I-conference) Science(I-conference) ,(O) IAPR(B-conference) and(O) SPIE(B-conference) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, organization, product, task, metric, conference, field, person, location, programming language, country, researcher, university and O.\nSentence: He is a Fellow of the Association for Computing Machinery , IEEE , American Association for the Advancement of Science , IAPR and SPIE .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","is","a","Fellow","of","the","Association","for","Computing","Machinery",",","IEEE",",","American","Association","for","the","Advancement","of","Science",",","IAPR","and","SPIE","."],"labels":["O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","O","B-organization","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","B-conference","O","B-conference","O"],"target_index":null,"target_label":null},"label_list":["algorithm","organization","product","task","metric","conference","field","person","location","programming_language","country","researcher","university"]}
{"id":"375","dataset":"crossner_ai","split":"test","instance":{"id":"375","prompt_labels":"A(O) trial(O) by(O) RET(B-organization) in(O) 2011(O) with(O) Facial(B-product) recognition(I-product) system(I-product) cameras(O) mounted(O) on(O) trams(O) made(O) sure(O) that(O) people(O) were(O) banned(O) from(O) the(O) city(O) trams(O) did(O) not(O) sneak(O) on(O) anyway(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, product, university, programming language, algorithm, person, researcher, organization, task, field, location, conference, country and O.\nSentence: A trial by RET in 2011 with Facial recognition system cameras mounted on trams made sure that people were banned from the city trams did not sneak on anyway .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","trial","by","RET","in","2011","with","Facial","recognition","system","cameras","mounted","on","trams","made","sure","that","people","were","banned","from","the","city","trams","did","not","sneak","on","anyway","."],"labels":["O","O","O","B-organization","O","O","O","B-product","I-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","product","university","programming_language","algorithm","person","researcher","organization","task","field","location","conference","country"]}
{"id":"376","dataset":"crossner_ai","split":"test","instance":{"id":"376","prompt_labels":"The(O) film(O) ,(O) adapted(O) from(O) the(O) popular(O) Cole(B-person) Porter(I-person) Broadway(B-organization) musical(O) ,(O) starred(O) the(O) MGM(O) songbird(O) team(O) of(O) Howard(B-person) Keel(I-person) and(O) Kathryn(B-person) Grayson(I-person) as(O) the(O) leads(O) ,(O) supported(O) by(O) Ann(B-person) Miller(I-person) ,(O) Keenan(B-person) Wynn(I-person) ,(O) Bobby(B-person) Van(I-person) ,(O) James(B-person) Whitmore(I-person) ,(O) Kurt(B-person) Kasznar(I-person) and(O) Tommy(B-person) Rall(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, conference, task, person, algorithm, field, metric, programming language, researcher, organization, product, university and O.\nSentence: The film , adapted from the popular Cole Porter Broadway musical , starred the MGM songbird team of Howard Keel and Kathryn Grayson as the leads , supported by Ann Miller , Keenan Wynn , Bobby Van , James Whitmore , Kurt Kasznar and Tommy Rall .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","film",",","adapted","from","the","popular","Cole","Porter","Broadway","musical",",","starred","the","MGM","songbird","team","of","Howard","Keel","and","Kathryn","Grayson","as","the","leads",",","supported","by","Ann","Miller",",","Keenan","Wynn",",","Bobby","Van",",","James","Whitmore",",","Kurt","Kasznar","and","Tommy","Rall","."],"labels":["O","O","O","O","O","O","O","B-person","I-person","B-organization","O","O","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["location","country","conference","task","person","algorithm","field","metric","programming_language","researcher","organization","product","university"]}
{"id":"377","dataset":"crossner_ai","split":"test","instance":{"id":"377","prompt_labels":"Such(O) applications(O) should(O) streamline(O) the(O) call(O) flows(O) ,(O) minimize(O) prompts(O) ,(O) eliminate(O) unnecessary(O) iterations(O) and(O) allow(O) elaborate(O) mixed(B-product) initiative(I-product) dialog(I-product) system(I-product) ,(O) which(O) enable(O) callers(O) to(O) enter(O) several(O) pieces(O) of(O) information(O) in(O) a(O) single(O) utterance(O) and(O) in(O) any(O) order(O) or(O) combination(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, algorithm, person, field, organization, programming language, task, researcher, product, location, conference, university, metric and O.\nSentence: Such applications should streamline the call flows , minimize prompts , eliminate unnecessary iterations and allow elaborate mixed initiative dialog system , which enable callers to enter several pieces of information in a single utterance and in any order or combination .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Such","applications","should","streamline","the","call","flows",",","minimize","prompts",",","eliminate","unnecessary","iterations","and","allow","elaborate","mixed","initiative","dialog","system",",","which","enable","callers","to","enter","several","pieces","of","information","in","a","single","utterance","and","in","any","order","or","combination","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","I-product","I-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","algorithm","person","field","organization","programming_language","task","researcher","product","location","conference","university","metric"]}
{"id":"378","dataset":"crossner_ai","split":"test","instance":{"id":"378","prompt_labels":"As(O) such(O) ,(O) traditional(O) gradient(B-algorithm) descent(I-algorithm) ((O) or(O) Stochastic(B-algorithm) gradient(I-algorithm) descent(I-algorithm) )(O) methods(O) can(O) be(O) adapted(O) ,(O) where(O) of(O) taking(O) a(O) step(O) in(O) the(O) direction(O) of(O) the(O) function(O) 's(O) gradient(O) ,(O) a(O) step(O) is(O) taken(O) in(O) the(O) direction(O) of(O) a(O) vector(O) selected(O) from(O) the(O) function(O) 's(O) sub-gradient(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, researcher, country, programming language, person, location, task, conference, product, field, university, metric, organization and O.\nSentence: As such , traditional gradient descent ( or Stochastic gradient descent ) methods can be adapted , where of taking a step in the direction of the function 's gradient , a step is taken in the direction of a vector selected from the function 's sub-gradient .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["As","such",",","traditional","gradient","descent","(","or","Stochastic","gradient","descent",")","methods","can","be","adapted",",","where","of","taking","a","step","in","the","direction","of","the","function","'s","gradient",",","a","step","is","taken","in","the","direction","of","a","vector","selected","from","the","function","'s","sub-gradient","."],"labels":["O","O","O","O","B-algorithm","I-algorithm","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["algorithm","researcher","country","programming_language","person","location","task","conference","product","field","university","metric","organization"]}
{"id":"379","dataset":"crossner_ai","split":"test","instance":{"id":"379","prompt_labels":"If(O) it(O) is(O) assumed(O) that(O) distortion(O) is(O) measured(O) by(O) mean(B-metric) squared(I-metric) error(I-metric) ,(O) the(O) distortion(O) D(O) ,(O) is(O) given(O) by(O) :(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, person, researcher, programming language, task, country, location, product, metric, algorithm, organization, university, conference and O.\nSentence: If it is assumed that distortion is measured by mean squared error , the distortion D , is given by :","prediction_output":null,"prediction_outputs":null,"group":null,"words":["If","it","is","assumed","that","distortion","is","measured","by","mean","squared","error",",","the","distortion","D",",","is","given","by",":"],"labels":["O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","person","researcher","programming_language","task","country","location","product","metric","algorithm","organization","university","conference"]}
{"id":"381","dataset":"crossner_ai","split":"test","instance":{"id":"381","prompt_labels":"Allen(B-researcher) received(O) his(O) Ph.D.(O) from(O) the(O) University(B-university) of(I-university) Toronto(I-university) in(O) 1979(O) ,(O) under(O) the(O) supervision(O) of(O) C.(B-researcher) Raymond(I-researcher) Perrault(I-researcher) ,(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, organization, field, product, university, metric, conference, programming language, researcher, person, algorithm, task and O.\nSentence: Allen received his Ph.D. from the University of Toronto in 1979 , under the supervision of C. Raymond Perrault ,","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Allen","received","his","Ph.D.","from","the","University","of","Toronto","in","1979",",","under","the","supervision","of","C.","Raymond","Perrault",","],"labels":["B-researcher","O","O","O","O","O","B-university","I-university","I-university","O","O","O","O","O","O","O","B-researcher","I-researcher","I-researcher","O"],"target_index":null,"target_label":null},"label_list":["location","country","organization","field","product","university","metric","conference","programming_language","researcher","person","algorithm","task"]}
{"id":"382","dataset":"crossner_ai","split":"test","instance":{"id":"382","prompt_labels":"OpenCV(B-product) supports(O) some(O) models(O) from(O) deep(B-field) learning(I-field) frameworks(O) like(O) TensorFlow(B-product) ,(O) Torch(B-product) ,(O) PyTorch(B-product) ((O) after(O) converting(O) to(O) an(O) ONNX(B-product) model(O) )(O) and(O) Caffe(B-product) according(O) to(O) a(O) defined(O) list(O) of(O) supported(O) layers(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, programming language, researcher, country, field, organization, algorithm, person, location, product, conference, task, university and O.\nSentence: OpenCV supports some models from deep learning frameworks like TensorFlow , Torch , PyTorch ( after converting to an ONNX model ) and Caffe according to a defined list of supported layers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["OpenCV","supports","some","models","from","deep","learning","frameworks","like","TensorFlow",",","Torch",",","PyTorch","(","after","converting","to","an","ONNX","model",")","and","Caffe","according","to","a","defined","list","of","supported","layers","."],"labels":["B-product","O","O","O","O","B-field","I-field","O","O","B-product","O","B-product","O","B-product","O","O","O","O","O","B-product","O","O","O","B-product","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","programming_language","researcher","country","field","organization","algorithm","person","location","product","conference","task","university"]}
{"id":"383","dataset":"crossner_ai","split":"test","instance":{"id":"383","prompt_labels":"Previously(O) ,(O) Christensen(B-researcher) was(O) the(O) Founding(O) Chairman(O) of(O) European(B-organization) Robotics(I-organization) Research(I-organization) Network(I-organization) ((O) EURON(B-organization) )(O) and(O) an(O) IEEE(B-organization) Robotics(I-organization) and(I-organization) Automation(I-organization) Society(I-organization) Distinguished(O) Lecturer(O) in(O) Robotics(B-field) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, university, location, conference, organization, country, researcher, person, field, product, task, algorithm, programming language and O.\nSentence: Previously , Christensen was the Founding Chairman of European Robotics Research Network ( EURON ) and an IEEE Robotics and Automation Society Distinguished Lecturer in Robotics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Previously",",","Christensen","was","the","Founding","Chairman","of","European","Robotics","Research","Network","(","EURON",")","and","an","IEEE","Robotics","and","Automation","Society","Distinguished","Lecturer","in","Robotics","."],"labels":["O","O","B-researcher","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-field","O"],"target_index":null,"target_label":null},"label_list":["metric","university","location","conference","organization","country","researcher","person","field","product","task","algorithm","programming_language"]}
{"id":"385","dataset":"crossner_ai","split":"test","instance":{"id":"385","prompt_labels":"Increasingly(O) ,(O) however(O) ,(O) work(O) at(O) Cycorp(B-organization) involves(O) giving(O) the(O) Cyc(B-product) system(I-product) the(O) ability(O) to(O) communicate(O) with(O) end(O) users(O) in(O) natural(O) language(O) ,(O) and(O) to(O) assist(O) with(O) the(O) ongoing(O) knowledge(O) formation(O) process(O) via(O) machine(B-field) learning(I-field) and(O) natural(B-task) language(I-task) understanding(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, metric, university, task, programming language, organization, field, product, conference, country, algorithm, researcher and O.\nSentence: Increasingly , however , work at Cycorp involves giving the Cyc system the ability to communicate with end users in natural language , and to assist with the ongoing knowledge formation process via machine learning and natural language understanding .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Increasingly",",","however",",","work","at","Cycorp","involves","giving","the","Cyc","system","the","ability","to","communicate","with","end","users","in","natural","language",",","and","to","assist","with","the","ongoing","knowledge","formation","process","via","machine","learning","and","natural","language","understanding","."],"labels":["O","O","O","O","O","O","B-organization","O","O","O","B-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","B-task","I-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["location","person","metric","university","task","programming_language","organization","field","product","conference","country","algorithm","researcher"]}
{"id":"386","dataset":"crossner_ai","split":"test","instance":{"id":"386","prompt_labels":"For(O) example(O) ,(O) if(O) the(O) most(O) suitable(O) classifier(O) for(O) the(O) problem(O) is(O) sought(O) ,(O) the(O) training(O) dataset(O) is(O) used(O) to(O) train(O) the(O) candidate(O) algorithms(O) ,(O) the(O) validation(O) dataset(O) is(O) used(O) to(O) compare(O) their(O) performances(O) and(O) decide(O) which(O) one(O) to(O) take(O) and(O) ,(O) finally(O) ,(O) the(O) test(O) dataset(O) is(O) used(O) to(O) obtain(O) the(O) performance(O) characteristics(O) such(O) as(O) accuracy(B-metric) ,(O) sensitivity(B-metric) ,(O) specificity(B-metric) ,(O) F-measure(B-metric) ,(O) and(O) so(O) on(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, researcher, university, field, algorithm, conference, metric, product, organization, task, person, country, programming language and O.\nSentence: For example , if the most suitable classifier for the problem is sought , the training dataset is used to train the candidate algorithms , the validation dataset is used to compare their performances and decide which one to take and , finally , the test dataset is used to obtain the performance characteristics such as accuracy , sensitivity , specificity , F-measure , and so on .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","example",",","if","the","most","suitable","classifier","for","the","problem","is","sought",",","the","training","dataset","is","used","to","train","the","candidate","algorithms",",","the","validation","dataset","is","used","to","compare","their","performances","and","decide","which","one","to","take","and",",","finally",",","the","test","dataset","is","used","to","obtain","the","performance","characteristics","such","as","accuracy",",","sensitivity",",","specificity",",","F-measure",",","and","so","on","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","O","B-metric","O","B-metric","O","B-metric","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","researcher","university","field","algorithm","conference","metric","product","organization","task","person","country","programming_language"]}
{"id":"387","dataset":"crossner_ai","split":"test","instance":{"id":"387","prompt_labels":"The(O) Mean(B-metric) squared(I-metric) error(I-metric) is(O) 0.15(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, field, programming language, metric, product, researcher, location, organization, conference, person, task, algorithm, university and O.\nSentence: The Mean squared error is 0.15 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Mean","squared","error","is","0.15","."],"labels":["O","B-metric","I-metric","I-metric","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","field","programming_language","metric","product","researcher","location","organization","conference","person","task","algorithm","university"]}
{"id":"389","dataset":"crossner_ai","split":"test","instance":{"id":"389","prompt_labels":"The(O) Gabor(B-algorithm) space(I-algorithm) is(O) very(O) useful(O) in(O) image(B-field) processing(I-field) applications(O) such(O) as(O) optical(B-task) character(I-task) recognition(I-task) ,(O) iris(B-task) recognition(I-task) and(O) fingerprint(B-task) recognition(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, programming language, organization, conference, metric, product, country, university, location, researcher, field, person, task and O.\nSentence: The Gabor space is very useful in image processing applications such as optical character recognition , iris recognition and fingerprint recognition .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Gabor","space","is","very","useful","in","image","processing","applications","such","as","optical","character","recognition",",","iris","recognition","and","fingerprint","recognition","."],"labels":["O","B-algorithm","I-algorithm","O","O","O","O","B-field","I-field","O","O","O","B-task","I-task","I-task","O","B-task","I-task","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["algorithm","programming_language","organization","conference","metric","product","country","university","location","researcher","field","person","task"]}
{"id":"390","dataset":"crossner_ai","split":"test","instance":{"id":"390","prompt_labels":"or(O) via(O) high-level(O) interfaces(O) to(O) Java(B-programming language) and(O) Tcl(B-programming language) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, person, conference, algorithm, location, country, programming language, product, field, organization, researcher, task, metric and O.\nSentence: or via high-level interfaces to Java and Tcl .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["or","via","high-level","interfaces","to","Java","and","Tcl","."],"labels":["O","O","O","O","O","B-programming language","O","B-programming language","O"],"target_index":null,"target_label":null},"label_list":["university","person","conference","algorithm","location","country","programming_language","product","field","organization","researcher","task","metric"]}
{"id":"391","dataset":"crossner_ai","split":"test","instance":{"id":"391","prompt_labels":"In(O) recent(O) research(O) ,(O) kernel-based(O) methods(O) such(O) as(O) support(B-algorithm) vector(I-algorithm) machine(I-algorithm) s(O) have(O) shown(O) superior(O) performance(O) in(O) supervised(B-field) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, field, metric, organization, location, conference, researcher, product, task, programming language, algorithm, country, person and O.\nSentence: In recent research , kernel-based methods such as support vector machine s have shown superior performance in supervised .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","recent","research",",","kernel-based","methods","such","as","support","vector","machine","s","have","shown","superior","performance","in","supervised","."],"labels":["O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","B-field","O"],"target_index":null,"target_label":null},"label_list":["university","field","metric","organization","location","conference","researcher","product","task","programming_language","algorithm","country","person"]}
{"id":"392","dataset":"crossner_ai","split":"test","instance":{"id":"392","prompt_labels":"To(O) illustrate(O) the(O) basic(O) principles(O) of(O) bagging(O) ,(O) below(O) is(O) an(O) analysis(O) on(O) the(O) relationship(O) between(O) ozone(O) and(O) temperature(O) ((O) data(O) from(O) Rousseeuw(B-researcher) and(O) Leroy(B-researcher) ((O) 1986(O) )(O) ,(O) analysis(O) done(O) in(O) R(B-programming language) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, metric, location, algorithm, field, programming language, country, product, researcher, university, person, task, organization and O.\nSentence: To illustrate the basic principles of bagging , below is an analysis on the relationship between ozone and temperature ( data from Rousseeuw and Leroy ( 1986 ) , analysis done in R ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["To","illustrate","the","basic","principles","of","bagging",",","below","is","an","analysis","on","the","relationship","between","ozone","and","temperature","(","data","from","Rousseeuw","and","Leroy","(","1986",")",",","analysis","done","in","R",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-researcher","O","B-researcher","O","O","O","O","O","O","O","B-programming language","O","O"],"target_index":null,"target_label":null},"label_list":["conference","metric","location","algorithm","field","programming_language","country","product","researcher","university","person","task","organization"]}
{"id":"393","dataset":"crossner_ai","split":"test","instance":{"id":"393","prompt_labels":"Denso(B-organization) Wave(I-organization) is(O) a(O) subsidiary(O) that(O) produces(O) automatic(O) identification(O) products(O) ((O) bar-code(B-product) reader(I-product) s(O) and(O) related(O) products(O) )(O) ,(O) industrial(B-product) robot(I-product) s(O) and(O) programmable(B-product) logic(I-product) controller(I-product) s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, university, researcher, task, field, country, product, algorithm, person, metric, conference, programming language, location and O.\nSentence: Denso Wave is a subsidiary that produces automatic identification products ( bar-code reader s and related products ) , industrial robot s and programmable logic controller s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Denso","Wave","is","a","subsidiary","that","produces","automatic","identification","products","(","bar-code","reader","s","and","related","products",")",",","industrial","robot","s","and","programmable","logic","controller","s","."],"labels":["B-organization","I-organization","O","O","O","O","O","O","O","O","O","B-product","I-product","O","O","O","O","O","O","B-product","I-product","O","O","B-product","I-product","I-product","O","O"],"target_index":null,"target_label":null},"label_list":["organization","university","researcher","task","field","country","product","algorithm","person","metric","conference","programming_language","location"]}
{"id":"394","dataset":"crossner_ai","split":"test","instance":{"id":"394","prompt_labels":"Where(O) Bilingual(B-metric) evaluation(I-metric) understudy(I-metric) simply(O) calculates(O) n-gram(B-metric) precision(I-metric) adding(O) equal(O) weight(O) to(O) each(O) one(O) ,(O) NIST(B-metric) also(O) calculates(O) how(O) informative(O) a(O) particular(O) n-gram(O) is(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, conference, task, organization, university, country, researcher, metric, programming language, location, field, algorithm, person and O.\nSentence: Where Bilingual evaluation understudy simply calculates n-gram precision adding equal weight to each one , NIST also calculates how informative a particular n-gram is .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Where","Bilingual","evaluation","understudy","simply","calculates","n-gram","precision","adding","equal","weight","to","each","one",",","NIST","also","calculates","how","informative","a","particular","n-gram","is","."],"labels":["O","B-metric","I-metric","I-metric","O","O","B-metric","I-metric","O","O","O","O","O","O","O","B-metric","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["product","conference","task","organization","university","country","researcher","metric","programming_language","location","field","algorithm","person"]}
{"id":"395","dataset":"crossner_ai","split":"test","instance":{"id":"395","prompt_labels":"In(O) particular(O) ,(O) they(O) are(O) used(O) during(O) the(O) calculation(O) of(O) likelihood(O) of(O) a(O) tree(O) ((O) in(O) Bayesian(B-algorithm) and(O) maximum(B-algorithm) likelihood(I-algorithm) approaches(O) to(O) tree(O) estimation(O) )(O) and(O) they(O) are(O) used(O) to(O) estimate(O) the(O) evolutionary(O) distance(O) between(O) sequences(O) from(O) the(O) observed(O) differences(O) between(O) the(O) sequences(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, field, metric, location, algorithm, university, organization, person, country, researcher, conference, task, product and O.\nSentence: In particular , they are used during the calculation of likelihood of a tree ( in Bayesian and maximum likelihood approaches to tree estimation ) and they are used to estimate the evolutionary distance between sequences from the observed differences between the sequences .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","particular",",","they","are","used","during","the","calculation","of","likelihood","of","a","tree","(","in","Bayesian","and","maximum","likelihood","approaches","to","tree","estimation",")","and","they","are","used","to","estimate","the","evolutionary","distance","between","sequences","from","the","observed","differences","between","the","sequences","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","field","metric","location","algorithm","university","organization","person","country","researcher","conference","task","product"]}
{"id":"398","dataset":"crossner_ai","split":"test","instance":{"id":"398","prompt_labels":"In(O) red-green(O) anaglyph(O) ,(O) the(O) audience(O) was(O) presented(O) three(O) reels(O) of(O) tests(O) ,(O) which(O) included(O) rural(O) scenes(O) ,(O) test(O) shots(O) of(O) Marie(B-person) Doro(I-person) ,(O) a(O) segment(O) of(O) John(B-person) B.(I-person) Mason(I-person) playing(O) a(O) number(O) of(O) passages(O) from(O) Jim(B-person) the(I-person) Penman(I-person) ((O) a(O) film(O) released(O) by(O) Famous(B-organization) Players-Lasky(I-organization) that(O) year(O) ,(O) but(O) not(O) in(O) 3D(O) )(O) ,(O) Oriental(O) dancers(O) ,(O) and(O) a(O) reel(O) of(O) footage(O) of(O) Niagara(B-location) Falls(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, country, task, field, person, programming language, location, university, algorithm, organization, researcher, product, conference and O.\nSentence: In red-green anaglyph , the audience was presented three reels of tests , which included rural scenes , test shots of Marie Doro , a segment of John B. Mason playing a number of passages from Jim the Penman ( a film released by Famous Players-Lasky that year , but not in 3D ) , Oriental dancers , and a reel of footage of Niagara Falls .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","red-green","anaglyph",",","the","audience","was","presented","three","reels","of","tests",",","which","included","rural","scenes",",","test","shots","of","Marie","Doro",",","a","segment","of","John","B.","Mason","playing","a","number","of","passages","from","Jim","the","Penman","(","a","film","released","by","Famous","Players-Lasky","that","year",",","but","not","in","3D",")",",","Oriental","dancers",",","and","a","reel","of","footage","of","Niagara","Falls","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","B-person","I-person","I-person","O","O","O","O","O","O","B-person","I-person","I-person","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["metric","country","task","field","person","programming_language","location","university","algorithm","organization","researcher","product","conference"]}
{"id":"399","dataset":"crossner_ai","split":"test","instance":{"id":"399","prompt_labels":"This(O) is(O) a(O) particular(O) way(O) of(O) implementing(O) maximum(B-metric) likelihood(I-metric) estimation(I-metric) for(O) this(O) problem(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, metric, organization, conference, researcher, person, product, algorithm, location, country, field, university, programming language and O.\nSentence: This is a particular way of implementing maximum likelihood estimation for this problem .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","is","a","particular","way","of","implementing","maximum","likelihood","estimation","for","this","problem","."],"labels":["O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["task","metric","organization","conference","researcher","person","product","algorithm","location","country","field","university","programming_language"]}
{"id":"401","dataset":"crossner_ai","split":"test","instance":{"id":"401","prompt_labels":"It(O) is(O) covered(O) by(O) American(O) National(O) Standards(O) Institute(O) /(O) NISO(O) standard(O) Z39.50(O) ,(O) and(O) International(O) Organization(O) for(O) Standardization(O) standard(O) 23950(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, metric, location, person, university, field, product, organization, researcher, algorithm, programming language, country, conference and O.\nSentence: It is covered by American National Standards Institute / NISO standard Z39.50 , and International Organization for Standardization standard 23950 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","is","covered","by","American","National","Standards","Institute","/","NISO","standard","Z39.50",",","and","International","Organization","for","Standardization","standard","23950","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["task","metric","location","person","university","field","product","organization","researcher","algorithm","programming_language","country","conference"]}
{"id":"402","dataset":"crossner_ai","split":"test","instance":{"id":"402","prompt_labels":"The(O) encoder(O) and(O) decoder(O) are(O) trained(O) to(O) take(O) a(O) phrase(O) and(O) reproduce(O) the(O) one-hot(O) distribution(O) of(O) a(O) corresponding(O) paraphrase(O) by(O) minimizing(O) perplexity(B-metric) using(O) simple(O) stochastic(B-algorithm) gradient(I-algorithm) descent(I-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, algorithm, task, organization, field, programming language, researcher, university, metric, product, country, conference and O.\nSentence: The encoder and decoder are trained to take a phrase and reproduce the one-hot distribution of a corresponding paraphrase by minimizing perplexity using simple stochastic gradient descent .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","encoder","and","decoder","are","trained","to","take","a","phrase","and","reproduce","the","one-hot","distribution","of","a","corresponding","paraphrase","by","minimizing","perplexity","using","simple","stochastic","gradient","descent","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","O","O","B-algorithm","I-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["person","location","algorithm","task","organization","field","programming_language","researcher","university","metric","product","country","conference"]}
{"id":"403","dataset":"crossner_ai","split":"test","instance":{"id":"403","prompt_labels":"Other(O) typical(O) applications(O) of(O) pattern(B-field) recognition(I-field) techniques(O) are(O) automatic(B-task) speech(I-task) recognition(I-task) ,(O) classification(B-task) of(I-task) text(I-task) into(I-task) several(I-task) categories(I-task) ((O) e.g.(O) ,(O) spam(O) /(O) non-spam(O) email(O) messages(O) )(O) ,(O) the(O) handwriting(B-task) recognition(I-task) on(I-task) postal(I-task) envelopes(I-task) ,(O) automatic(B-task) recognition(I-task) of(I-task) images(I-task) of(I-task) human(I-task) faces(I-task) ,(O) or(O) handwriting(B-task) image(I-task) extraction(I-task) from(I-task) medical(I-task) forms(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, organization, task, product, location, university, field, country, programming language, metric, person, researcher, algorithm and O.\nSentence: Other typical applications of pattern recognition techniques are automatic speech recognition , classification of text into several categories ( e.g. , spam / non-spam email messages ) , the handwriting recognition on postal envelopes , automatic recognition of images of human faces , or handwriting image extraction from medical forms .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","typical","applications","of","pattern","recognition","techniques","are","automatic","speech","recognition",",","classification","of","text","into","several","categories","(","e.g.",",","spam","/","non-spam","email","messages",")",",","the","handwriting","recognition","on","postal","envelopes",",","automatic","recognition","of","images","of","human","faces",",","or","handwriting","image","extraction","from","medical","forms","."],"labels":["O","O","O","O","B-field","I-field","O","O","B-task","I-task","I-task","O","B-task","I-task","I-task","I-task","I-task","I-task","O","O","O","O","O","O","O","O","O","O","O","B-task","I-task","I-task","I-task","I-task","O","B-task","I-task","I-task","I-task","I-task","I-task","I-task","O","O","B-task","I-task","I-task","I-task","I-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["conference","organization","task","product","location","university","field","country","programming_language","metric","person","researcher","algorithm"]}
{"id":"404","dataset":"crossner_ai","split":"test","instance":{"id":"404","prompt_labels":"Artificial(B-algorithm) neural(I-algorithm) networks(I-algorithm) have(O) been(O) used(O) on(O) a(O) variety(O) of(O) tasks(O) ,(O) including(O) computer(B-field) vision(I-field) ,(O) speech(B-task) recognition(I-task) ,(O) machine(B-task) translation(I-task) ,(O) social(B-task) network(I-task) filtering(I-task) ,(O) playing(B-task) board(I-task) and(I-task) video(I-task) games(I-task) and(O) medical(B-task) diagnosis(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, person, country, product, field, metric, algorithm, university, programming language, conference, location, task, organization and O.\nSentence: Artificial neural networks have been used on a variety of tasks , including computer vision , speech recognition , machine translation , social network filtering , playing board and video games and medical diagnosis .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Artificial","neural","networks","have","been","used","on","a","variety","of","tasks",",","including","computer","vision",",","speech","recognition",",","machine","translation",",","social","network","filtering",",","playing","board","and","video","games","and","medical","diagnosis","."],"labels":["B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","I-task","O","B-task","I-task","I-task","I-task","I-task","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["researcher","person","country","product","field","metric","algorithm","university","programming_language","conference","location","task","organization"]}
{"id":"405","dataset":"crossner_ai","split":"test","instance":{"id":"405","prompt_labels":"Examples(O) include(O) Salford(B-organization) Systems(I-organization) CART(B-product) ((O) which(O) licensed(O) the(O) proprietary(O) code(O) of(O) the(O) original(O) CART(B-product) authors(O) )(O) ,(O) IBM(B-organization) SPSS(B-product) Modeler(I-product) ,(O) RapidMiner(B-product) ,(O) SAS(B-product) Enterprise(I-product) Miner(I-product) ,(O) Matlab(B-product) ,(O) R(B-programming language) ((O) an(O) open-source(O) software(O) environment(O) for(O) statistical(B-field) computing(I-field) ,(O) which(O) includes(O) several(O) CART(B-product) implementations(O) such(O) as(O) rpart(B-algorithm) ,(O) party(B-algorithm) and(O) randomForest(B-algorithm) packages(O) )(O) ,(O) Weka(B-product) ((O) a(O) free(O) and(O) open-source(O) data-mining(B-task) suite(O) ,(O) contains(O) many(O) decision(B-algorithm) tree(I-algorithm) algorithms(O) )(O) ,(O) Orange(B-product) ,(O) KNIME(B-product) ,(O) Microsoft(B-product) SQL(I-product) Server(I-product) programming(O) language(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, programming language, researcher, algorithm, person, conference, field, metric, university, organization, product, country, task and O.\nSentence: Examples include Salford Systems CART ( which licensed the proprietary code of the original CART authors ) , IBM SPSS Modeler , RapidMiner , SAS Enterprise Miner , Matlab , R ( an open-source software environment for statistical computing , which includes several CART implementations such as rpart , party and randomForest packages ) , Weka ( a free and open-source data-mining suite , contains many decision tree algorithms ) , Orange , KNIME , Microsoft SQL Server programming language ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Examples","include","Salford","Systems","CART","(","which","licensed","the","proprietary","code","of","the","original","CART","authors",")",",","IBM","SPSS","Modeler",",","RapidMiner",",","SAS","Enterprise","Miner",",","Matlab",",","R","(","an","open-source","software","environment","for","statistical","computing",",","which","includes","several","CART","implementations","such","as","rpart",",","party","and","randomForest","packages",")",",","Weka","(","a","free","and","open-source","data-mining","suite",",","contains","many","decision","tree","algorithms",")",",","Orange",",","KNIME",",","Microsoft","SQL","Server","programming","language",")","."],"labels":["O","O","B-organization","I-organization","B-product","O","O","O","O","O","O","O","O","O","B-product","O","O","O","B-organization","B-product","I-product","O","B-product","O","B-product","I-product","I-product","O","B-product","O","B-programming language","O","O","O","O","O","O","B-field","I-field","O","O","O","O","B-product","O","O","O","B-algorithm","O","B-algorithm","O","B-algorithm","O","O","O","B-product","O","O","O","O","O","B-task","O","O","O","O","B-algorithm","I-algorithm","O","O","O","B-product","O","B-product","O","B-product","I-product","I-product","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","programming_language","researcher","algorithm","person","conference","field","metric","university","organization","product","country","task"]}
{"id":"406","dataset":"crossner_ai","split":"test","instance":{"id":"406","prompt_labels":"Linear(B-algorithm) predictive(I-algorithm) coding(I-algorithm) ((O) LPC(B-algorithm) )(O) was(O) first(O) developed(O) by(O) Fumitada(B-researcher) Itakura(I-researcher) of(O) Nagoya(B-university) University(I-university) and(O) Shuzo(B-researcher) Saito(I-researcher) of(O) Nippon(B-organization) Telegraph(I-organization) and(I-organization) Telephone(I-organization) ((O) NTT(B-organization) )(O) in(O) 1966(O) ,(O) and(O) then(O) further(O) developed(O) by(O) Bishnu(B-researcher) S.(I-researcher) Atal(I-researcher) and(O) Manfred(B-researcher) R.(I-researcher) Schroeder(I-researcher) at(O) Bell(B-organization) Labs(I-organization) during(O) the(O) early-to-mid-1970s(O) ,(O) becoming(O) a(O) basis(O) for(O) the(O) first(O) speech(B-product) synthesizer(I-product) DSP(I-product) chips(I-product) in(O) the(O) late(O) 1970s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, country, task, product, location, organization, metric, programming language, algorithm, university, conference, person, field and O.\nSentence: Linear predictive coding ( LPC ) was first developed by Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone ( NTT ) in 1966 , and then further developed by Bishnu S. Atal and Manfred R. Schroeder at Bell Labs during the early-to-mid-1970s , becoming a basis for the first speech synthesizer DSP chips in the late 1970s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Linear","predictive","coding","(","LPC",")","was","first","developed","by","Fumitada","Itakura","of","Nagoya","University","and","Shuzo","Saito","of","Nippon","Telegraph","and","Telephone","(","NTT",")","in","1966",",","and","then","further","developed","by","Bishnu","S.","Atal","and","Manfred","R.","Schroeder","at","Bell","Labs","during","the","early-to-mid-1970s",",","becoming","a","basis","for","the","first","speech","synthesizer","DSP","chips","in","the","late","1970s","."],"labels":["B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","O","O","O","O","O","B-researcher","I-researcher","O","B-university","I-university","O","B-researcher","I-researcher","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","O","O","O","B-researcher","I-researcher","I-researcher","O","B-researcher","I-researcher","I-researcher","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","B-product","I-product","I-product","I-product","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","country","task","product","location","organization","metric","programming_language","algorithm","university","conference","person","field"]}
{"id":"407","dataset":"crossner_ai","split":"test","instance":{"id":"407","prompt_labels":"An(O) F-score(B-metric) is(O) a(O) combination(O) of(O) the(O) precision(B-metric) and(O) the(O) recall(B-metric) ,(O) providing(O) a(O) single(O) score(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, organization, country, task, researcher, metric, person, university, programming language, product, algorithm, location, conference and O.\nSentence: An F-score is a combination of the precision and the recall , providing a single score .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["An","F-score","is","a","combination","of","the","precision","and","the","recall",",","providing","a","single","score","."],"labels":["O","B-metric","O","O","O","O","O","B-metric","O","O","B-metric","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","organization","country","task","researcher","metric","person","university","programming_language","product","algorithm","location","conference"]}
{"id":"409","dataset":"crossner_ai","split":"test","instance":{"id":"409","prompt_labels":"The(O) special(O) case(O) of(O) linear(O) support-vector(B-algorithm) machines(I-algorithm) can(O) be(O) solved(O) more(O) efficiently(O) by(O) the(O) same(O) kind(O) of(O) algorithms(O) to(O) optimize(O) its(O) close(O) cousin(O) ,(O) logistic(B-algorithm) regression(I-algorithm) ;(O) this(O) class(O) of(O) algorithms(O) includes(O) Stochastic(B-algorithm) gradient(I-algorithm) descent(I-algorithm) ((O) e.g.(O) ,(O) PEGASOS(B-algorithm) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, location, organization, country, algorithm, metric, field, researcher, task, university, person, product, conference and O.\nSentence: The special case of linear support-vector machines can be solved more efficiently by the same kind of algorithms to optimize its close cousin , logistic regression ; this class of algorithms includes Stochastic gradient descent ( e.g. , PEGASOS ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","special","case","of","linear","support-vector","machines","can","be","solved","more","efficiently","by","the","same","kind","of","algorithms","to","optimize","its","close","cousin",",","logistic","regression",";","this","class","of","algorithms","includes","Stochastic","gradient","descent","(","e.g.",",","PEGASOS",")","."],"labels":["O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","B-algorithm","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","location","organization","country","algorithm","metric","field","researcher","task","university","person","product","conference"]}
{"id":"410","dataset":"crossner_ai","split":"test","instance":{"id":"410","prompt_labels":"When(O) Siri(B-product) on(O) an(O) iOS(B-product) device(O) is(O) asked(O) Do(O) you(O) have(O) a(O) pet(O) ?(O) ,(O) one(O) the(O) responses(O) is(O) I(O) used(O) to(O) have(O) an(O) AIBO(B-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, organization, conference, product, researcher, metric, task, university, programming language, country, location, person, algorithm and O.\nSentence: When Siri on an iOS device is asked Do you have a pet ? , one the responses is I used to have an AIBO .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["When","Siri","on","an","iOS","device","is","asked","Do","you","have","a","pet","?",",","one","the","responses","is","I","used","to","have","an","AIBO","."],"labels":["O","B-product","O","O","B-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","O"],"target_index":null,"target_label":null},"label_list":["field","organization","conference","product","researcher","metric","task","university","programming_language","country","location","person","algorithm"]}
{"id":"411","dataset":"crossner_ai","split":"test","instance":{"id":"411","prompt_labels":"In(O) information(B-task) retrieval(I-task) ,(O) the(O) positive(B-metric) predictive(I-metric) value(I-metric) is(O) called(O) precision(B-metric) ,(O) and(O) sensitivity(B-metric) is(O) called(O) recall(B-metric) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, algorithm, task, conference, researcher, country, person, organization, field, university, product, location, programming language and O.\nSentence: In information retrieval , the positive predictive value is called precision , and sensitivity is called recall .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","information","retrieval",",","the","positive","predictive","value","is","called","precision",",","and","sensitivity","is","called","recall","."],"labels":["O","B-task","I-task","O","O","B-metric","I-metric","I-metric","O","O","B-metric","O","O","B-metric","O","O","B-metric","O"],"target_index":null,"target_label":null},"label_list":["metric","algorithm","task","conference","researcher","country","person","organization","field","university","product","location","programming_language"]}
{"id":"412","dataset":"crossner_ai","split":"test","instance":{"id":"412","prompt_labels":"In(O) particular(O) ,(O) his(O) research(O) focused(O) on(O) areas(O) such(O) as(O) text(B-field) mining(I-field) ((O) extraction(B-task) ,(O) categorization(B-task) ,(O) novelty(B-task) detection(I-task) )(O) and(O) in(O) new(O) theoretical(O) frameworks(O) such(O) as(O) a(O) unified(O) utility-based(O) theory(O) bridging(O) information(B-task) retrieval(I-task) ,(O) Automatic(B-task) summarization(I-task) ,(O) free-text(B-task) Question(I-task) Answering(I-task) and(O) related(O) tasks(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, conference, researcher, organization, field, location, university, metric, product, programming language, country, task, algorithm and O.\nSentence: In particular , his research focused on areas such as text mining ( extraction , categorization , novelty detection ) and in new theoretical frameworks such as a unified utility-based theory bridging information retrieval , Automatic summarization , free-text Question Answering and related tasks .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","particular",",","his","research","focused","on","areas","such","as","text","mining","(","extraction",",","categorization",",","novelty","detection",")","and","in","new","theoretical","frameworks","such","as","a","unified","utility-based","theory","bridging","information","retrieval",",","Automatic","summarization",",","free-text","Question","Answering","and","related","tasks","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","B-task","O","B-task","O","B-task","I-task","O","O","O","O","O","O","O","O","O","O","O","O","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","I-task","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","conference","researcher","organization","field","location","university","metric","product","programming_language","country","task","algorithm"]}
{"id":"413","dataset":"crossner_ai","split":"test","instance":{"id":"413","prompt_labels":"Delta(B-product) robot(I-product) s(O) have(O) base-mounted(O) rotary(B-product) actuator(I-product) s(O) that(O) move(O) a(O) light(O) ,(O) stiff(O) ,(O) parallelogram(O) arm(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, programming language, location, metric, conference, researcher, algorithm, field, university, country, person, task, product and O.\nSentence: Delta robot s have base-mounted rotary actuator s that move a light , stiff , parallelogram arm .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Delta","robot","s","have","base-mounted","rotary","actuator","s","that","move","a","light",",","stiff",",","parallelogram","arm","."],"labels":["B-product","I-product","O","O","O","B-product","I-product","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","programming_language","location","metric","conference","researcher","algorithm","field","university","country","person","task","product"]}
{"id":"421","dataset":"crossner_ai","split":"test","instance":{"id":"421","prompt_labels":"Starting(O) as(O) a(O) curiosity(O) ,(O) the(O) speech(B-product) system(I-product) of(I-product) Apple(I-product) Macintosh(I-product) has(O) evolved(O) into(O) a(O) fully(O) supported(O) program(O) PlainTalk(B-product) ,(O) for(O) people(O) with(O) vision(O) problems(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, country, location, task, person, metric, programming language, university, field, algorithm, researcher, organization, conference and O.\nSentence: Starting as a curiosity , the speech system of Apple Macintosh has evolved into a fully supported program PlainTalk , for people with vision problems .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Starting","as","a","curiosity",",","the","speech","system","of","Apple","Macintosh","has","evolved","into","a","fully","supported","program","PlainTalk",",","for","people","with","vision","problems","."],"labels":["O","O","O","O","O","O","B-product","I-product","I-product","I-product","I-product","O","O","O","O","O","O","O","B-product","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["product","country","location","task","person","metric","programming_language","university","field","algorithm","researcher","organization","conference"]}
{"id":"422","dataset":"crossner_ai","split":"test","instance":{"id":"422","prompt_labels":"Other(O) areas(O) of(O) usage(O) for(O) ontologies(O) within(O) NLP(B-field) include(O) information(B-task) retrieval(I-task) ,(O) information(B-task) extraction(I-task) and(O) automatic(B-task) summarization(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, person, organization, task, algorithm, metric, university, programming language, field, country, conference, product, location and O.\nSentence: Other areas of usage for ontologies within NLP include information retrieval , information extraction and automatic summarization .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","areas","of","usage","for","ontologies","within","NLP","include","information","retrieval",",","information","extraction","and","automatic","summarization","."],"labels":["O","O","O","O","O","O","O","B-field","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["researcher","person","organization","task","algorithm","metric","university","programming_language","field","country","conference","product","location"]}
{"id":"423","dataset":"crossner_ai","split":"test","instance":{"id":"423","prompt_labels":"The(O) Institute(O) has(O) collaborated(O) closely(O) with(O) the(O) Janelia(B-organization) Farm(I-organization) Campus(I-organization) of(I-organization) Howard(I-organization) Hughes(I-organization) Medical(I-organization) Institute(I-organization) ,(O) the(O) Allen(B-organization) Institute(I-organization) for(I-organization) Brain(I-organization) Science(I-organization) and(O) the(O) National(B-organization) Institutes(I-organization) of(I-organization) Health(I-organization) to(O) develop(O) better(O) methods(O) of(O) reconstructing(O) neuronal(O) architectures(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, field, conference, country, researcher, location, organization, programming language, university, algorithm, metric, person, task and O.\nSentence: The Institute has collaborated closely with the Janelia Farm Campus of Howard Hughes Medical Institute , the Allen Institute for Brain Science and the National Institutes of Health to develop better methods of reconstructing neuronal architectures .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Institute","has","collaborated","closely","with","the","Janelia","Farm","Campus","of","Howard","Hughes","Medical","Institute",",","the","Allen","Institute","for","Brain","Science","and","the","National","Institutes","of","Health","to","develop","better","methods","of","reconstructing","neuronal","architectures","."],"labels":["O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["product","field","conference","country","researcher","location","organization","programming_language","university","algorithm","metric","person","task"]}
{"id":"424","dataset":"crossner_ai","split":"test","instance":{"id":"424","prompt_labels":"Recently(O) ,(O) Google(B-organization) announced(O) that(O) Google(B-product) Translate(I-product) translates(O) roughly(O) enough(O) text(O) to(O) fill(O) 1(O) million(O) books(O) in(O) one(O) day(O) ((O) 2012(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, field, university, researcher, product, conference, programming language, algorithm, task, organization, country, metric and O.\nSentence: Recently , Google announced that Google Translate translates roughly enough text to fill 1 million books in one day ( 2012 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Recently",",","Google","announced","that","Google","Translate","translates","roughly","enough","text","to","fill","1","million","books","in","one","day","(","2012",")","."],"labels":["O","O","B-organization","O","O","B-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","location","field","university","researcher","product","conference","programming_language","algorithm","task","organization","country","metric"]}
{"id":"425","dataset":"crossner_ai","split":"test","instance":{"id":"425","prompt_labels":"Events(O) are(O) held(O) worldwide(O) ,(O) and(O) are(O) most(O) popular(O) in(O) the(O) United(B-country) Kingdom(I-country) ,(O) United(B-country) States(I-country) ,(O) Japan(B-country) ,(O) Singapore(B-country) ,(O) India(B-country) ,(O) South(B-country) Korea(I-country) and(O) becoming(O) popular(O) in(O) subcontinent(O) countries(O) such(O) as(O) Sri(B-country) Lanka(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, researcher, organization, conference, location, programming language, university, field, metric, task, person, algorithm, product and O.\nSentence: Events are held worldwide , and are most popular in the United Kingdom , United States , Japan , Singapore , India , South Korea and becoming popular in subcontinent countries such as Sri Lanka .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Events","are","held","worldwide",",","and","are","most","popular","in","the","United","Kingdom",",","United","States",",","Japan",",","Singapore",",","India",",","South","Korea","and","becoming","popular","in","subcontinent","countries","such","as","Sri","Lanka","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O","B-country","I-country","O","B-country","O","B-country","O","B-country","O","B-country","I-country","O","O","O","O","O","O","O","O","B-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["country","researcher","organization","conference","location","programming_language","university","field","metric","task","person","algorithm","product"]}
{"id":"426","dataset":"crossner_ai","split":"test","instance":{"id":"426","prompt_labels":"These(O) packages(O) are(O) developed(O) primarily(O) in(O) R(B-programming language) ,(O) and(O) sometimes(O) in(O) Java(B-programming language) ,(O) C(B-programming language) ,(O) C(B-programming language) +(I-programming language) +(I-programming language) ,(O) and(O) Fortran(B-programming language) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, researcher, country, person, university, organization, field, location, algorithm, programming language, task, product, metric and O.\nSentence: These packages are developed primarily in R , and sometimes in Java , C , C + + , and Fortran .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["These","packages","are","developed","primarily","in","R",",","and","sometimes","in","Java",",","C",",","C","+","+",",","and","Fortran","."],"labels":["O","O","O","O","O","O","B-programming language","O","O","O","O","B-programming language","O","B-programming language","O","B-programming language","I-programming language","I-programming language","O","O","B-programming language","O"],"target_index":null,"target_label":null},"label_list":["conference","researcher","country","person","university","organization","field","location","algorithm","programming_language","task","product","metric"]}
{"id":"427","dataset":"crossner_ai","split":"test","instance":{"id":"427","prompt_labels":"As(O) part(O) of(O) the(O) 2006(B-conference) European(I-conference) Conference(I-conference) on(I-conference) Computer(I-conference) Vision(I-conference) ((O) ECCV(B-conference) )(O) ,(O) Dalal(B-researcher) and(O) Triggs(B-researcher) teamed(O) up(O) with(O) Cordelia(B-researcher) Schmid(I-researcher) to(O) apply(O) HOG(B-algorithm) detectors(I-algorithm) to(O) the(O) problem(O) of(O) human(B-task) detection(I-task) in(I-task) films(I-task) and(I-task) videos(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, field, university, algorithm, conference, product, metric, country, task, programming language, location, person, researcher and O.\nSentence: As part of the 2006 European Conference on Computer Vision ( ECCV ) , Dalal and Triggs teamed up with Cordelia Schmid to apply HOG detectors to the problem of human detection in films and videos .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["As","part","of","the","2006","European","Conference","on","Computer","Vision","(","ECCV",")",",","Dalal","and","Triggs","teamed","up","with","Cordelia","Schmid","to","apply","HOG","detectors","to","the","problem","of","human","detection","in","films","and","videos","."],"labels":["O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","B-conference","O","O","B-researcher","O","B-researcher","O","O","O","B-researcher","I-researcher","O","O","B-algorithm","I-algorithm","O","O","O","O","B-task","I-task","I-task","I-task","I-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["organization","field","university","algorithm","conference","product","metric","country","task","programming_language","location","person","researcher"]}
{"id":"430","dataset":"crossner_ai","split":"test","instance":{"id":"430","prompt_labels":"Further(O) ,(O) in(O) the(O) case(O) of(O) estimation(O) based(O) on(O) a(O) single(O) sample(O) ,(O) it(O) demonstrates(O) philosophical(O) issues(O) and(O) possible(O) misunderstandings(O) in(O) the(O) use(O) of(O) maximum(B-metric) likelihood(I-metric) estimators(I-metric) and(I-metric) likelihood(I-metric) functions(I-metric) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, metric, country, algorithm, university, field, organization, location, person, researcher, conference, task, programming language and O.\nSentence: Further , in the case of estimation based on a single sample , it demonstrates philosophical issues and possible misunderstandings in the use of maximum likelihood estimators and likelihood functions .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Further",",","in","the","case","of","estimation","based","on","a","single","sample",",","it","demonstrates","philosophical","issues","and","possible","misunderstandings","in","the","use","of","maximum","likelihood","estimators","and","likelihood","functions","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","I-metric","I-metric","I-metric","O"],"target_index":null,"target_label":null},"label_list":["product","metric","country","algorithm","university","field","organization","location","person","researcher","conference","task","programming_language"]}
{"id":"0","dataset":"crossner_literature","split":"test","instance":{"id":"0","prompt_labels":"Two(O) decades(O) after(O) Frank(B-writer) Herbert(I-writer) 's(O) death(O) ,(O) his(O) son(O) Brian(B-writer) Herbert(I-writer) ,(O) along(O) with(O) Kevin(B-writer) J.(I-writer) Anderson(I-writer) ,(O) published(O) two(O) sequel(O) s(O) -(O) Hunters(B-book) of(I-book) Dune(I-book) ((O) 2006(O) )(O) and(O) Sandworms(B-book) of(I-book) Dune(I-book) ((O) 2007(O) )(O) -(O) based(O) on(O) notes(O) left(O) behind(O) by(O) Frank(B-writer) Herbert(I-writer) for(O) what(O) he(O) referred(O) to(O) as(O) Dune(B-book) 7(I-book) ,(O) his(O) own(O) planned(O) seventh(O) novel(B-literary genre) in(O) the(O) Dune(O) series(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, poem, book, country, location, award, event, person, organization, literary genre, magazine and O.\nSentence: Two decades after Frank Herbert 's death , his son Brian Herbert , along with Kevin J. Anderson , published two sequel s - Hunters of Dune ( 2006 ) and Sandworms of Dune ( 2007 ) - based on notes left behind by Frank Herbert for what he referred to as Dune 7 , his own planned seventh novel in the Dune series .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Two","decades","after","Frank","Herbert","'s","death",",","his","son","Brian","Herbert",",","along","with","Kevin","J.","Anderson",",","published","two","sequel","s","-","Hunters","of","Dune","(","2006",")","and","Sandworms","of","Dune","(","2007",")","-","based","on","notes","left","behind","by","Frank","Herbert","for","what","he","referred","to","as","Dune","7",",","his","own","planned","seventh","novel","in","the","Dune","series","."],"labels":["O","O","O","B-writer","I-writer","O","O","O","O","O","B-writer","I-writer","O","O","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","B-book","I-book","O","O","O","O","O","B-literary genre","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["writer","poem","book","country","location","award","event","person","organization","literary_genre","magazine"]}
{"id":"1","dataset":"crossner_literature","split":"test","instance":{"id":"1","prompt_labels":"February(O) 2015(O) saw(O) Ellis(B-person) release(O) his(O) first(O) album(O) as(O) Zarelli(B-person) ,(O) Soft(O) Rains(O) on(O) the(O) Seris(B-organization) Aphnos(I-organization) label(I-organization) -(O) a(O) mostly(O) instrumental(O) synthesizer(O) record(O) ,(O) featuring(O) the(O) voice(O) of(O) Leonard(B-person) Nimoy(I-person) reading(O) the(O) Ray(B-writer) Bradbury(I-writer) short(B-literary genre) story(I-literary genre) There(B-poem) Will(I-poem) Come(I-poem) Soft(I-poem) Rains(I-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, book, writer, poem, person, organization, literary genre, award, event, magazine and O.\nSentence: February 2015 saw Ellis release his first album as Zarelli , Soft Rains on the Seris Aphnos label - a mostly instrumental synthesizer record , featuring the voice of Leonard Nimoy reading the Ray Bradbury short story There Will Come Soft Rains .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["February","2015","saw","Ellis","release","his","first","album","as","Zarelli",",","Soft","Rains","on","the","Seris","Aphnos","label","-","a","mostly","instrumental","synthesizer","record",",","featuring","the","voice","of","Leonard","Nimoy","reading","the","Ray","Bradbury","short","story","There","Will","Come","Soft","Rains","."],"labels":["O","O","O","B-person","O","O","O","O","O","B-person","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","B-writer","I-writer","B-literary genre","I-literary genre","B-poem","I-poem","I-poem","I-poem","I-poem","O"],"target_index":null,"target_label":null},"label_list":["location","country","book","writer","poem","person","organization","literary_genre","award","event","magazine"]}
{"id":"3","dataset":"crossner_literature","split":"test","instance":{"id":"3","prompt_labels":"Stoker(B-writer) 's(O) inspirations(O) for(O) the(O) story(O) ,(O) in(O) addition(O) to(O) Whitby(B-location) ,(O) may(O) have(O) included(O) a(O) visit(O) to(O) Slains(B-location) Castle(I-location) in(O) Aberdeenshire(B-location) ,(O) a(O) visit(O) to(O) the(O) crypts(O) of(O) St.(B-location) Michan(I-location) 's(I-location) Church(I-location) in(O) Dublin(B-location) ,(O) and(O) the(O) novella(B-literary genre) Carmilla(B-book) by(O) Sheridan(B-writer) Le(I-writer) Fanu(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, event, person, award, poem, magazine, organization, country, location, writer, book and O.\nSentence: Stoker 's inspirations for the story , in addition to Whitby , may have included a visit to Slains Castle in Aberdeenshire , a visit to the crypts of St. Michan 's Church in Dublin , and the novella Carmilla by Sheridan Le Fanu .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Stoker","'s","inspirations","for","the","story",",","in","addition","to","Whitby",",","may","have","included","a","visit","to","Slains","Castle","in","Aberdeenshire",",","a","visit","to","the","crypts","of","St.","Michan","'s","Church","in","Dublin",",","and","the","novella","Carmilla","by","Sheridan","Le","Fanu","."],"labels":["B-writer","O","O","O","O","O","O","O","O","O","B-location","O","O","O","O","O","O","O","B-location","I-location","O","B-location","O","O","O","O","O","O","O","B-location","I-location","I-location","I-location","O","B-location","O","O","O","B-literary genre","B-book","O","B-writer","I-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","event","person","award","poem","magazine","organization","country","location","writer","book"]}
{"id":"4","dataset":"crossner_literature","split":"test","instance":{"id":"4","prompt_labels":"The(O) film(O) was(O) completed(O) in(O) 1979(O) and(O) won(O) the(O) Prize(B-award) of(I-award) the(I-award) Ecumenical(I-award) Jury(I-award) at(O) the(O) Cannes(B-event) Film(I-event) Festival(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, literary genre, award, organization, location, poem, magazine, book, writer, event, person and O.\nSentence: The film was completed in 1979 and won the Prize of the Ecumenical Jury at the Cannes Film Festival .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","film","was","completed","in","1979","and","won","the","Prize","of","the","Ecumenical","Jury","at","the","Cannes","Film","Festival","."],"labels":["O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["country","literary_genre","award","organization","location","poem","magazine","book","writer","event","person"]}
{"id":"5","dataset":"crossner_literature","split":"test","instance":{"id":"5","prompt_labels":"The(B-book) Forsyte(I-book) Saga(I-book) ,(O) first(O) published(O) under(O) that(O) title(O) in(O) 1922(O) ,(O) is(O) a(O) series(O) of(O) three(O) novels(B-literary genre) and(O) two(O) interludes(O) published(O) between(O) 1906(O) and(O) 1921(O) by(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) -winning(O) English(O) author(O) John(B-writer) Galsworthy(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, book, magazine, country, literary genre, writer, event, location, person, organization, poem and O.\nSentence: The Forsyte Saga , first published under that title in 1922 , is a series of three novels and two interludes published between 1906 and 1921 by Nobel Prize in Literature -winning English author John Galsworthy .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Forsyte","Saga",",","first","published","under","that","title","in","1922",",","is","a","series","of","three","novels","and","two","interludes","published","between","1906","and","1921","by","Nobel","Prize","in","Literature","-winning","English","author","John","Galsworthy","."],"labels":["B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-literary genre","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["award","book","magazine","country","literary_genre","writer","event","location","person","organization","poem"]}
{"id":"6","dataset":"crossner_literature","split":"test","instance":{"id":"6","prompt_labels":"The(O) first(O) actual(O) missionary(O) in(O) Sweden(B-country) and(O) the(O) Nordic(B-location) countries(I-location) ((O) and(O) organizer(O) of(O) the(O) Catholic(O) church(O) therein(O) )(O) ,(O) Ansgar(B-writer) was(O) later(O) declared(O) Patron(O) of(O) Scandinavia(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, literary genre, location, event, book, writer, country, organization, person, award, poem and O.\nSentence: The first actual missionary in Sweden and the Nordic countries ( and organizer of the Catholic church therein ) , Ansgar was later declared Patron of Scandinavia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","first","actual","missionary","in","Sweden","and","the","Nordic","countries","(","and","organizer","of","the","Catholic","church","therein",")",",","Ansgar","was","later","declared","Patron","of","Scandinavia","."],"labels":["O","O","O","O","O","B-country","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","B-writer","O","O","O","O","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["magazine","literary_genre","location","event","book","writer","country","organization","person","award","poem"]}
{"id":"7","dataset":"crossner_literature","split":"test","instance":{"id":"7","prompt_labels":"In(O) late(O) November(O) 1851(O) ,(O) Dickens(B-writer) moved(O) into(O) Tavistock(B-location) House(I-location) where(O) he(O) wrote(O) Bleak(B-book) House(I-book) ((O) 1852-53(O) )(O) ,(O) Hard(B-book) Times(I-book) ((O) 1854(O) )(O) ,(O) and(O) Little(B-book) Dorrit(I-book) ((O) 1856(O) )(O) ..(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, location, literary genre, person, writer, event, magazine, organization, book, country, award and O.\nSentence: In late November 1851 , Dickens moved into Tavistock House where he wrote Bleak House ( 1852-53 ) , Hard Times ( 1854 ) , and Little Dorrit ( 1856 ) ..","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","late","November","1851",",","Dickens","moved","into","Tavistock","House","where","he","wrote","Bleak","House","(","1852-53",")",",","Hard","Times","(","1854",")",",","and","Little","Dorrit","(","1856",")",".."],"labels":["O","O","O","O","O","B-writer","O","O","B-location","I-location","O","O","O","B-book","I-book","O","O","O","O","B-book","I-book","O","O","O","O","O","B-book","I-book","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","location","literary_genre","person","writer","event","magazine","organization","book","country","award"]}
{"id":"8","dataset":"crossner_literature","split":"test","instance":{"id":"8","prompt_labels":"His(O) supernatural(O) thriller(O) film(O) Djinn(O) premiered(O) at(O) the(O) 2013(O) Abu(B-event) Dhabi(I-event) Film(I-event) Festival(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, poem, person, magazine, award, book, country, writer, literary genre, location and O.\nSentence: His supernatural thriller film Djinn premiered at the 2013 Abu Dhabi Film Festival .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","supernatural","thriller","film","Djinn","premiered","at","the","2013","Abu","Dhabi","Film","Festival","."],"labels":["O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["event","organization","poem","person","magazine","award","book","country","writer","literary_genre","location"]}
{"id":"10","dataset":"crossner_literature","split":"test","instance":{"id":"10","prompt_labels":"These(O) confessional(O) poems(B-literary genre) are(O) the(O) ones(O) that(O) document(O) Lowell(B-location) 's(O) struggle(O) with(O) mental(O) illness(O) and(O) include(O) pieces(O) like(O) Skunk(B-poem) Hour(I-poem) ,(O) Home(B-poem) After(I-poem) Three(I-poem) Months(I-poem) Away(I-poem) and(O) Waking(B-poem) in(I-poem) the(I-poem) Blue(I-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, location, magazine, book, person, literary genre, event, award, poem, writer and O.\nSentence: These confessional poems are the ones that document Lowell 's struggle with mental illness and include pieces like Skunk Hour , Home After Three Months Away and Waking in the Blue .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["These","confessional","poems","are","the","ones","that","document","Lowell","'s","struggle","with","mental","illness","and","include","pieces","like","Skunk","Hour",",","Home","After","Three","Months","Away","and","Waking","in","the","Blue","."],"labels":["O","O","B-literary genre","O","O","O","O","O","B-location","O","O","O","O","O","O","O","O","O","B-poem","I-poem","O","B-poem","I-poem","I-poem","I-poem","I-poem","O","B-poem","I-poem","I-poem","I-poem","O"],"target_index":null,"target_label":null},"label_list":["country","organization","location","magazine","book","person","literary_genre","event","award","poem","writer"]}
{"id":"12","dataset":"crossner_literature","split":"test","instance":{"id":"12","prompt_labels":"Rudyard(B-writer) Kipling(I-writer) was(O) born(O) on(O) 30(O) December(O) 1865(O) in(O) Bombay(B-location) ,(O) in(O) the(O) Bombay(B-location) Presidency(I-location) of(O) British(B-country) Raj(I-country) ,(O) to(O) Alice(B-writer) Kipling(I-writer) ((I-writer) ne(I-writer) MacDonald(I-writer) )(I-writer) and(O) John(B-writer) Lockwood(I-writer) Kipling(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, award, organization, country, location, literary genre, magazine, book, writer, person, event and O.\nSentence: Rudyard Kipling was born on 30 December 1865 in Bombay , in the Bombay Presidency of British Raj , to Alice Kipling ( ne MacDonald ) and John Lockwood Kipling .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Rudyard","Kipling","was","born","on","30","December","1865","in","Bombay",",","in","the","Bombay","Presidency","of","British","Raj",",","to","Alice","Kipling","(","ne","MacDonald",")","and","John","Lockwood","Kipling","."],"labels":["B-writer","I-writer","O","O","O","O","O","O","O","B-location","O","O","O","B-location","I-location","O","B-country","I-country","O","O","B-writer","I-writer","I-writer","I-writer","I-writer","I-writer","O","B-writer","I-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["poem","award","organization","country","location","literary_genre","magazine","book","writer","person","event"]}
{"id":"14","dataset":"crossner_literature","split":"test","instance":{"id":"14","prompt_labels":"In(O) a(O) letter(O) to(O) P.(B-writer) Schuyler(I-writer) Miller(I-writer) and(O) John(B-writer) Drury(I-writer) Clark(I-writer) in(O) 1936(O) ,(O) only(O) three(O) months(O) before(O) Howard(B-writer) 's(O) death(O) ,(O) Conan(B-person) is(O) described(O) as(O) standing(O) 6(O) ft(O) /(O) 183(O) cm(O) and(O) weighing(O) when(O) he(O) takes(O) part(O) in(O) an(O) attack(O) on(O) Venarium(O) at(O) only(O) 14(O) years(O) old(O) ,(O) though(O) being(O) far(O) from(O) fully(O) grown(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, poem, organization, writer, magazine, country, award, location, literary genre, person, event and O.\nSentence: In a letter to P. Schuyler Miller and John Drury Clark in 1936 , only three months before Howard 's death , Conan is described as standing 6 ft / 183 cm and weighing when he takes part in an attack on Venarium at only 14 years old , though being far from fully grown .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","a","letter","to","P.","Schuyler","Miller","and","John","Drury","Clark","in","1936",",","only","three","months","before","Howard","'s","death",",","Conan","is","described","as","standing","6","ft","/","183","cm","and","weighing","when","he","takes","part","in","an","attack","on","Venarium","at","only","14","years","old",",","though","being","far","from","fully","grown","."],"labels":["O","O","O","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","O","B-writer","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["book","poem","organization","writer","magazine","country","award","location","literary_genre","person","event"]}
{"id":"15","dataset":"crossner_literature","split":"test","instance":{"id":"15","prompt_labels":"Better(O) parts(O) followed(O) ,(O) including(O) The(B-book) Cider(I-book) House(I-book) Rules(I-book) ((O) 1999(O) )(O) ,(O) for(O) which(O) he(O) won(O) his(O) second(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Supporting(I-award) Actor(I-award) ..(O) BBC(B-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, magazine, person, writer, book, country, event, literary genre, organization, location, poem and O.\nSentence: Better parts followed , including The Cider House Rules ( 1999 ) , for which he won his second Academy Award for Best Supporting Actor .. BBC .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Better","parts","followed",",","including","The","Cider","House","Rules","(","1999",")",",","for","which","he","won","his","second","Academy","Award","for","Best","Supporting","Actor","..","BBC","."],"labels":["O","O","O","O","O","B-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-organization","O"],"target_index":null,"target_label":null},"label_list":["award","magazine","person","writer","book","country","event","literary_genre","organization","location","poem"]}
{"id":"18","dataset":"crossner_literature","split":"test","instance":{"id":"18","prompt_labels":"They(O) were(O) Rachel(O) ,(O) Rachel(O) ((O) 1968(O) )(O) ,(O) based(O) on(O) Margaret(B-writer) Laurence(I-writer) '(O) s(O) A(B-book) Jest(I-book) of(I-book) God(I-book) ,(O) the(O) screen(O) version(O) of(O) the(O) Pulitzer(B-award) Prize(I-award) -winning(O) play(O) The(B-book) Effect(I-book) of(I-book) Gamma(I-book) Rays(I-book) on(I-book) Man-in-the-Moon(I-book) Marigolds(I-book) ((O) 1972(O) )(O) ,(O) the(O) television(O) screen(O) version(O) of(O) the(O) Pulitzer(B-award) Prize-winning(I-award) play(O) The(B-book) Shadow(I-book) Box(I-book) ((O) 1980(O) )(O) ,(O) and(O) a(O) screen(O) version(O) of(O) Tennessee(B-writer) Williams(I-writer) '(O) The(B-book) Glass(I-book) Menagerie(I-book) ((O) 1987(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, event, book, location, poem, literary genre, magazine, writer, country, award and O.\nSentence: They were Rachel , Rachel ( 1968 ) , based on Margaret Laurence ' s A Jest of God , the screen version of the Pulitzer Prize -winning play The Effect of Gamma Rays on Man-in-the-Moon Marigolds ( 1972 ) , the television screen version of the Pulitzer Prize-winning play The Shadow Box ( 1980 ) , and a screen version of Tennessee Williams ' The Glass Menagerie ( 1987 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["They","were","Rachel",",","Rachel","(","1968",")",",","based","on","Margaret","Laurence","'","s","A","Jest","of","God",",","the","screen","version","of","the","Pulitzer","Prize","-winning","play","The","Effect","of","Gamma","Rays","on","Man-in-the-Moon","Marigolds","(","1972",")",",","the","television","screen","version","of","the","Pulitzer","Prize-winning","play","The","Shadow","Box","(","1980",")",",","and","a","screen","version","of","Tennessee","Williams","'","The","Glass","Menagerie","(","1987",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","B-book","I-book","I-book","I-book","O","O","O","O","O","O","B-award","I-award","O","O","B-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","B-book","I-book","I-book","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","organization","event","book","location","poem","literary_genre","magazine","writer","country","award"]}
{"id":"19","dataset":"crossner_literature","split":"test","instance":{"id":"19","prompt_labels":"Elwyn(B-writer) Brooks(I-writer) White(I-writer) ((O) July(O) 11(O) ,(O) 1899(O) -(O) October(O) 1(O) ,(O) 1985(O) )(O) In(O) addition(O) ,(O) he(O) was(O) a(O) contributor(O) to(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) magazine(O) ,(O) and(O) also(O) a(O) co-author(O) of(O) the(O) English(O) language(O) style(O) guide(O) The(B-book) Elements(I-book) of(I-book) Style(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, literary genre, poem, organization, book, writer, magazine, country, person, event, location and O.\nSentence: Elwyn Brooks White ( July 11 , 1899 - October 1 , 1985 ) In addition , he was a contributor to The New Yorker magazine , and also a co-author of the English language style guide The Elements of Style .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Elwyn","Brooks","White","(","July","11",",","1899","-","October","1",",","1985",")","In","addition",",","he","was","a","contributor","to","The","New","Yorker","magazine",",","and","also","a","co-author","of","the","English","language","style","guide","The","Elements","of","Style","."],"labels":["B-writer","I-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","I-magazine","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["award","literary_genre","poem","organization","book","writer","magazine","country","person","event","location"]}
{"id":"20","dataset":"crossner_literature","split":"test","instance":{"id":"20","prompt_labels":"Among(O) his(O) best-known(O) works(O) -(O) most(O) of(O) which(O) were(O) published(O) posthumously(O) -(O) are(O) Dulce(B-poem) et(I-poem) Decorum(I-poem) est(I-poem) ,(O) Insensibility(B-poem) ,(O) Anthem(B-poem) for(I-poem) Doomed(I-poem) Youth(I-poem) ,(O) Futility(B-poem) ,(O) Spring(B-poem) Offensive(I-poem) and(O) Strange(B-poem) Meeting(I-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, book, event, organization, poem, literary genre, location, writer, person, country, award and O.\nSentence: Among his best-known works - most of which were published posthumously - are Dulce et Decorum est , Insensibility , Anthem for Doomed Youth , Futility , Spring Offensive and Strange Meeting .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Among","his","best-known","works","-","most","of","which","were","published","posthumously","-","are","Dulce","et","Decorum","est",",","Insensibility",",","Anthem","for","Doomed","Youth",",","Futility",",","Spring","Offensive","and","Strange","Meeting","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-poem","I-poem","I-poem","I-poem","O","B-poem","O","B-poem","I-poem","I-poem","I-poem","O","B-poem","O","B-poem","I-poem","O","B-poem","I-poem","O"],"target_index":null,"target_label":null},"label_list":["magazine","book","event","organization","poem","literary_genre","location","writer","person","country","award"]}
{"id":"21","dataset":"crossner_literature","split":"test","instance":{"id":"21","prompt_labels":"Crooked(B-book) House(I-book) ((O) 1949(O) )(O) and(O) Ordeal(B-book) by(I-book) Innocence(I-book) ((O) 1957(O) )(O) ,(O) which(O) could(O) easily(O) have(O) been(O) Poirot(B-literary genre) novels(I-literary genre) ,(O) represent(O) a(O) logical(O) endpoint(O) of(O) the(O) general(O) diminution(O) of(O) his(O) presence(O) in(O) such(O) works(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, magazine, writer, poem, award, event, person, organization, country, literary genre, location and O.\nSentence: Crooked House ( 1949 ) and Ordeal by Innocence ( 1957 ) , which could easily have been Poirot novels , represent a logical endpoint of the general diminution of his presence in such works .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Crooked","House","(","1949",")","and","Ordeal","by","Innocence","(","1957",")",",","which","could","easily","have","been","Poirot","novels",",","represent","a","logical","endpoint","of","the","general","diminution","of","his","presence","in","such","works","."],"labels":["B-book","I-book","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["book","magazine","writer","poem","award","event","person","organization","country","literary_genre","location"]}
{"id":"22","dataset":"crossner_literature","split":"test","instance":{"id":"22","prompt_labels":"In(O) The(B-magazine) Nation(I-magazine) ,(O) reviewer(O) Katha(B-writer) Pollitt(I-writer) said(O) ,(O) Gibson(B-writer) has(O) violated(O) just(O) about(O) every(O) precept(O) of(O) the(O) ((O) United(B-organization) States(I-organization) Conference(I-organization) of(I-organization) Catholic(I-organization) Bishops(I-organization) )(O) conference(O) 's(O) own(O) 1988(O) '(O) Criteria(O) '(O) for(O) the(O) portrayal(O) of(O) Jews(O) in(O) dramatizations(O) of(O) the(O) Passion(O) ((O) no(O) bloodthirsty(O) Jews(O) ,(O) no(O) rabble(O) ,(O) no(O) use(O) of(O) Scripture(O) that(O) reinforces(O) negative(O) stereotypes(O) of(O) Jews(O) ,(O) etc(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, event, country, person, literary genre, award, writer, poem, book, location, organization and O.\nSentence: In The Nation , reviewer Katha Pollitt said , Gibson has violated just about every precept of the ( United States Conference of Catholic Bishops ) conference 's own 1988 ' Criteria ' for the portrayal of Jews in dramatizations of the Passion ( no bloodthirsty Jews , no rabble , no use of Scripture that reinforces negative stereotypes of Jews , etc .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","The","Nation",",","reviewer","Katha","Pollitt","said",",","Gibson","has","violated","just","about","every","precept","of","the","(","United","States","Conference","of","Catholic","Bishops",")","conference","'s","own","1988","'","Criteria","'","for","the","portrayal","of","Jews","in","dramatizations","of","the","Passion","(","no","bloodthirsty","Jews",",","no","rabble",",","no","use","of","Scripture","that","reinforces","negative","stereotypes","of","Jews",",","etc","."],"labels":["O","B-magazine","I-magazine","O","O","B-writer","I-writer","O","O","B-writer","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","event","country","person","literary_genre","award","writer","poem","book","location","organization"]}
{"id":"23","dataset":"crossner_literature","split":"test","instance":{"id":"23","prompt_labels":"The(B-book) Saga(I-book) of(I-book) Eric(I-book) Brighteyes(I-book) is(O) an(O) epic(B-literary genre) viking(I-literary genre) novel(I-literary genre) by(O) H.(B-writer) Rider(I-writer) Haggard(I-writer) that(O) concerns(O) the(O) adventures(O) of(O) its(O) eponymous(O) principal(O) character(O) in(O) 10th-century(O) Iceland(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, writer, poem, person, location, literary genre, country, event, book, organization, award and O.\nSentence: The Saga of Eric Brighteyes is an epic viking novel by H. Rider Haggard that concerns the adventures of its eponymous principal character in 10th-century Iceland .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Saga","of","Eric","Brighteyes","is","an","epic","viking","novel","by","H.","Rider","Haggard","that","concerns","the","adventures","of","its","eponymous","principal","character","in","10th-century","Iceland","."],"labels":["B-book","I-book","I-book","I-book","I-book","O","O","B-literary genre","I-literary genre","I-literary genre","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["magazine","writer","poem","person","location","literary_genre","country","event","book","organization","award"]}
{"id":"24","dataset":"crossner_literature","split":"test","instance":{"id":"24","prompt_labels":"Some(O) critics(O) ,(O) including(O) Donald(B-writer) Spoto(I-writer) and(O) Roger(B-writer) Ebert(I-writer) ,(O) agree(O) that(O) Vertigo(O) is(O) the(O) director(O) 's(O) most(O) personal(O) and(O) revealing(O) film(O) ,(O) dealing(O) with(O) the(O) Pygmalion(O) -like(O) obsessions(O) of(O) a(O) man(O) who(O) crafts(O) a(O) woman(O) into(O) the(O) woman(O) he(O) desires(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, literary genre, book, writer, award, poem, organization, country, event, person, location and O.\nSentence: Some critics , including Donald Spoto and Roger Ebert , agree that Vertigo is the director 's most personal and revealing film , dealing with the Pygmalion -like obsessions of a man who crafts a woman into the woman he desires .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","critics",",","including","Donald","Spoto","and","Roger","Ebert",",","agree","that","Vertigo","is","the","director","'s","most","personal","and","revealing","film",",","dealing","with","the","Pygmalion","-like","obsessions","of","a","man","who","crafts","a","woman","into","the","woman","he","desires","."],"labels":["O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","literary_genre","book","writer","award","poem","organization","country","event","person","location"]}
{"id":"25","dataset":"crossner_literature","split":"test","instance":{"id":"25","prompt_labels":"In(O) his(O) spare(O) time(O) he(O) also(O) enjoyed(O) reading(O) about(O) natural(O) science(O) s(O) and(O) popular(O) novels(B-literary genre) ,(O) such(O) as(O) Don(B-book) Quixote(I-book) and(O) Robinson(B-book) Crusoe(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, literary genre, award, location, book, person, magazine, writer, country, event, poem and O.\nSentence: In his spare time he also enjoyed reading about natural science s and popular novels , such as Don Quixote and Robinson Crusoe .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","his","spare","time","he","also","enjoyed","reading","about","natural","science","s","and","popular","novels",",","such","as","Don","Quixote","and","Robinson","Crusoe","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-literary genre","O","O","O","B-book","I-book","O","B-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["organization","literary_genre","award","location","book","person","magazine","writer","country","event","poem"]}
{"id":"26","dataset":"crossner_literature","split":"test","instance":{"id":"26","prompt_labels":"Kim(B-writer) Stanley(I-writer) Robinson(I-writer) '(O) s(O) novel(B-literary genre) ,(O) The(B-book) Years(I-book) of(I-book) Rice(I-book) and(I-book) Salt(I-book) ((O) 2002(O) )(O) ,(O) starts(O) at(O) the(O) point(O) of(O) divergence(O) with(O) Timur(B-person) turning(O) his(O) army(O) away(O) from(O) Europe(B-location) ,(O) and(O) the(O) Black(O) Death(O) has(O) killed(O) 99(O) %(O) of(O) Europe(B-location) 's(O) population(O) ,(O) instead(O) of(O) only(O) a(O) third(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, person, book, magazine, country, writer, award, organization, literary genre, event, location and O.\nSentence: Kim Stanley Robinson ' s novel , The Years of Rice and Salt ( 2002 ) , starts at the point of divergence with Timur turning his army away from Europe , and the Black Death has killed 99 % of Europe 's population , instead of only a third .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Kim","Stanley","Robinson","'","s","novel",",","The","Years","of","Rice","and","Salt","(","2002",")",",","starts","at","the","point","of","divergence","with","Timur","turning","his","army","away","from","Europe",",","and","the","Black","Death","has","killed","99","%","of","Europe","'s","population",",","instead","of","only","a","third","."],"labels":["B-writer","I-writer","I-writer","O","O","B-literary genre","O","B-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","B-person","O","O","O","O","O","B-location","O","O","O","O","O","O","O","O","O","O","B-location","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","person","book","magazine","country","writer","award","organization","literary_genre","event","location"]}
{"id":"27","dataset":"crossner_literature","split":"test","instance":{"id":"27","prompt_labels":"H.(B-writer) G.(I-writer) Wells(I-writer) praised(O) Growth(B-book) of(I-book) the(I-book) Soil(I-book) ((O) 1917(O) )(O) for(O) which(O) Hamsun(B-writer) was(O) awarded(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, organization, book, magazine, person, writer, poem, country, event, location, literary genre and O.\nSentence: H. G. Wells praised Growth of the Soil ( 1917 ) for which Hamsun was awarded the Nobel Prize in Literature .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["H.","G.","Wells","praised","Growth","of","the","Soil","(","1917",")","for","which","Hamsun","was","awarded","the","Nobel","Prize","in","Literature","."],"labels":["B-writer","I-writer","I-writer","O","B-book","I-book","I-book","I-book","O","O","O","O","O","B-writer","O","O","O","B-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["award","organization","book","magazine","person","writer","poem","country","event","location","literary_genre"]}
{"id":"29","dataset":"crossner_literature","split":"test","instance":{"id":"29","prompt_labels":"Later(O) ,(O) he(O) was(O) best(O) known(O) for(O) his(O) setting(O) the(O) Draumkvedet(B-poem) ((O) 1905(O) )(O) and(O) the(O) Poetic(B-book) Edda(I-book) ((O) 1908(O) )(O) into(O) modern(O) Norwegian(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, person, organization, event, country, writer, poem, magazine, book, literary genre and O.\nSentence: Later , he was best known for his setting the Draumkvedet ( 1905 ) and the Poetic Edda ( 1908 ) into modern Norwegian .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Later",",","he","was","best","known","for","his","setting","the","Draumkvedet","(","1905",")","and","the","Poetic","Edda","(","1908",")","into","modern","Norwegian","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-poem","O","O","O","O","O","B-book","I-book","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","location","person","organization","event","country","writer","poem","magazine","book","literary_genre"]}
{"id":"30","dataset":"crossner_literature","split":"test","instance":{"id":"30","prompt_labels":"She(O) later(O) formed(O) the(O) Pagan(B-book) Babies(I-book) with(O) friend(O) Kat(B-person) Bjelland(I-person) ,(O) whom(O) she(O) met(O) at(O) the(O) Satyricon(B-location) club(I-location) in(O) Portland(B-location) in(O) 1984(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, organization, poem, location, country, event, award, literary genre, magazine, person, writer and O.\nSentence: She later formed the Pagan Babies with friend Kat Bjelland , whom she met at the Satyricon club in Portland in 1984 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","later","formed","the","Pagan","Babies","with","friend","Kat","Bjelland",",","whom","she","met","at","the","Satyricon","club","in","Portland","in","1984","."],"labels":["O","O","O","O","B-book","I-book","O","O","B-person","I-person","O","O","O","O","O","O","B-location","I-location","O","B-location","O","O","O"],"target_index":null,"target_label":null},"label_list":["book","organization","poem","location","country","event","award","literary_genre","magazine","person","writer"]}
{"id":"31","dataset":"crossner_literature","split":"test","instance":{"id":"31","prompt_labels":"The(B-book) Faerie(I-book) Queene(I-book) by(O) Edmund(B-writer) Spenser(I-writer) ,(O) with(O) its(O) stanzas(O) of(O) eight(O) iambic(O) pentameter(O) lines(O) followed(O) by(O) one(O) alexandrine(O) ,(O) exemplifies(O) what(O) came(O) to(O) be(O) its(O) chief(O) role(O) :(O) as(O) a(O) somewhat(O) infrequent(O) variant(O) line(O) in(O) an(O) otherwise(O) iambic(O) pentameter(O) context(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, writer, person, organization, event, award, magazine, location, poem, literary genre, book and O.\nSentence: The Faerie Queene by Edmund Spenser , with its stanzas of eight iambic pentameter lines followed by one alexandrine , exemplifies what came to be its chief role : as a somewhat infrequent variant line in an otherwise iambic pentameter context .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Faerie","Queene","by","Edmund","Spenser",",","with","its","stanzas","of","eight","iambic","pentameter","lines","followed","by","one","alexandrine",",","exemplifies","what","came","to","be","its","chief","role",":","as","a","somewhat","infrequent","variant","line","in","an","otherwise","iambic","pentameter","context","."],"labels":["B-book","I-book","I-book","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","writer","person","organization","event","award","magazine","location","poem","literary_genre","book"]}
{"id":"32","dataset":"crossner_literature","split":"test","instance":{"id":"32","prompt_labels":"During(O) this(O) part(O) of(O) his(O) career(O) ,(O) he(O) was(O) often(O) seen(O) as(O) a(O) rival(O) to(O) the(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) '(O) s(O) Pauline(B-writer) Kael(I-writer) ,(O) who(O) had(O) originally(O) attacked(O) the(O) auteur(O) theory(O) in(O) her(O) essay(O) Circles(B-book) and(I-book) Squares(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, organization, country, location, writer, magazine, book, poem, event, person, literary genre and O.\nSentence: During this part of his career , he was often seen as a rival to the The New Yorker ' s Pauline Kael , who had originally attacked the auteur theory in her essay Circles and Squares .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","this","part","of","his","career",",","he","was","often","seen","as","a","rival","to","the","The","New","Yorker","'","s","Pauline","Kael",",","who","had","originally","attacked","the","auteur","theory","in","her","essay","Circles","and","Squares","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","I-magazine","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["award","organization","country","location","writer","magazine","book","poem","event","person","literary_genre"]}
{"id":"33","dataset":"crossner_literature","split":"test","instance":{"id":"33","prompt_labels":"5(O) Smith(B-writer) later(O) taught(O) himself(O) French(O) and(O) Spanish(O) to(O) translate(O) verse(B-literary genre) out(O) of(O) those(O) languages(O) ,(O) including(O) works(O) by(O) Grard(B-writer) de(I-writer) Nerval(I-writer) ,(O) Paul(B-writer) Verlaine(I-writer) ,(O) and(O) all(O) but(O) 6(O) of(O) Charles(B-writer) Baudelaire(I-writer) '(O) s(O) 157(O) poems(B-literary genre) in(O) Les(B-book) Fleurs(I-book) du(I-book) mal(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, award, location, writer, person, poem, event, magazine, organization, country, book and O.\nSentence: 5 Smith later taught himself French and Spanish to translate verse out of those languages , including works by Grard de Nerval , Paul Verlaine , and all but 6 of Charles Baudelaire ' s 157 poems in Les Fleurs du mal .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["5","Smith","later","taught","himself","French","and","Spanish","to","translate","verse","out","of","those","languages",",","including","works","by","Grard","de","Nerval",",","Paul","Verlaine",",","and","all","but","6","of","Charles","Baudelaire","'","s","157","poems","in","Les","Fleurs","du","mal","."],"labels":["O","B-writer","O","O","O","O","O","O","O","O","B-literary genre","O","O","O","O","O","O","O","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","O","O","O","O","O","O","B-writer","I-writer","O","O","O","B-literary genre","O","B-book","I-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","award","location","writer","person","poem","event","magazine","organization","country","book"]}
{"id":"34","dataset":"crossner_literature","split":"test","instance":{"id":"34","prompt_labels":"Together(O) with(O) Andersson(B-writer) ,(O) Ulvaeus(B-writer) was(O) nominated(O) for(O) the(O) Drama(B-award) Desk(I-award) Award(I-award) in(O) the(O) category(O) Outstanding(O) Music(O) ((O) for(O) the(O) musical(O) Chess(O) )(O) ,(O) and(O) for(O) a(O) Tony(B-award) Award(I-award) in(O) a(O) category(O) Best(O) Orchestrations(O) ((O) for(O) the(O) musical(O) Mamma(O) Mia(O) !(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, organization, literary genre, country, writer, event, award, person, poem, magazine, location and O.\nSentence: Together with Andersson , Ulvaeus was nominated for the Drama Desk Award in the category Outstanding Music ( for the musical Chess ) , and for a Tony Award in a category Best Orchestrations ( for the musical Mamma Mia ! ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Together","with","Andersson",",","Ulvaeus","was","nominated","for","the","Drama","Desk","Award","in","the","category","Outstanding","Music","(","for","the","musical","Chess",")",",","and","for","a","Tony","Award","in","a","category","Best","Orchestrations","(","for","the","musical","Mamma","Mia","!",")","."],"labels":["O","O","B-writer","O","B-writer","O","O","O","O","B-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["book","organization","literary_genre","country","writer","event","award","person","poem","magazine","location"]}
{"id":"35","dataset":"crossner_literature","split":"test","instance":{"id":"35","prompt_labels":"In(O) March(O) 1938(O) ,(O) Huxley(B-writer) 's(O) friend(O) Anita(B-writer) Loos(I-writer) ,(O) a(O) novelist(O) and(O) screenwriter(O) ,(O) put(O) him(O) in(O) touch(O) with(O) Metro-Goldwyn-Mayer(B-organization) ((O) MGM(B-organization) )(O) ,(O) which(O) hired(O) him(O) for(O) Madame(O) Curie(O) which(O) was(O) originally(O) to(O) star(O) Greta(B-person) Garbo(I-person) and(O) be(O) directed(O) by(O) George(B-person) Cukor(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, event, person, poem, book, literary genre, location, country, organization, magazine, award and O.\nSentence: In March 1938 , Huxley 's friend Anita Loos , a novelist and screenwriter , put him in touch with Metro-Goldwyn-Mayer ( MGM ) , which hired him for Madame Curie which was originally to star Greta Garbo and be directed by George Cukor .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","March","1938",",","Huxley","'s","friend","Anita","Loos",",","a","novelist","and","screenwriter",",","put","him","in","touch","with","Metro-Goldwyn-Mayer","(","MGM",")",",","which","hired","him","for","Madame","Curie","which","was","originally","to","star","Greta","Garbo","and","be","directed","by","George","Cukor","."],"labels":["O","O","O","O","B-writer","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["writer","event","person","poem","book","literary_genre","location","country","organization","magazine","award"]}
{"id":"37","dataset":"crossner_literature","split":"test","instance":{"id":"37","prompt_labels":"Writing(O) for(O) The(B-magazine) Spectator(I-magazine) in(O) the(O) UK(B-country) ,(O) Graham(B-writer) Greene(I-writer) gave(O) the(O) film(O) a(O) mixed(O) good(O) review(O) ,(O) characterizing(O) the(O) first(O) half(O) of(O) the(O) film(O) as(O) simple(O) and(O) direct(O) and(O) TRUE(O) ,(O) but(O) complaining(O) that(O) the(O) second(O) half(O) displays(O) a(O) little(O) less(O) than(O) life(O) and(O) that(O) the(O) last(O) hour(O) was(O) permeated(O) by(O) banality(O) and(O) ennui(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, award, book, poem, person, country, event, magazine, location, literary genre, organization and O.\nSentence: Writing for The Spectator in the UK , Graham Greene gave the film a mixed good review , characterizing the first half of the film as simple and direct and TRUE , but complaining that the second half displays a little less than life and that the last hour was permeated by banality and ennui .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Writing","for","The","Spectator","in","the","UK",",","Graham","Greene","gave","the","film","a","mixed","good","review",",","characterizing","the","first","half","of","the","film","as","simple","and","direct","and","TRUE",",","but","complaining","that","the","second","half","displays","a","little","less","than","life","and","that","the","last","hour","was","permeated","by","banality","and","ennui","."],"labels":["O","O","B-magazine","I-magazine","O","O","B-country","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["writer","award","book","poem","person","country","event","magazine","location","literary_genre","organization"]}
{"id":"38","dataset":"crossner_literature","split":"test","instance":{"id":"38","prompt_labels":"The(O) rights(O) to(O) A.(B-writer) A.(I-writer) Milne(I-writer) 's(O) Pooh(B-book) books(O) were(O) left(O) to(O) four(O) beneficiaries(O) :(O) his(O) family(O) ,(O) the(O) Royal(B-organization) Literary(I-organization) Fund(I-organization) ,(O) Westminster(B-organization) School(I-organization) and(O) the(O) Garrick(B-organization) Club(I-organization) at(O) the(O) beginning(O) of(O) the(O) year(O) after(O) the(O) 70th(O) anniversary(O) of(O) the(O) author(O) 's(O) death(O) ((O) PMA(O) -70(O) )(O) ,(O) and(O) has(O) already(O) expired(O) in(O) those(O) countries(O) with(O) a(O) PMA-50(O) rule(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, magazine, person, organization, award, poem, event, literary genre, country, writer, location and O.\nSentence: The rights to A. A. Milne 's Pooh books were left to four beneficiaries : his family , the Royal Literary Fund , Westminster School and the Garrick Club at the beginning of the year after the 70th anniversary of the author 's death ( PMA -70 ) , and has already expired in those countries with a PMA-50 rule .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","rights","to","A.","A.","Milne","'s","Pooh","books","were","left","to","four","beneficiaries",":","his","family",",","the","Royal","Literary","Fund",",","Westminster","School","and","the","Garrick","Club","at","the","beginning","of","the","year","after","the","70th","anniversary","of","the","author","'s","death","(","PMA","-70",")",",","and","has","already","expired","in","those","countries","with","a","PMA-50","rule","."],"labels":["O","O","O","B-writer","I-writer","I-writer","O","B-book","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["book","magazine","person","organization","award","poem","event","literary_genre","country","writer","location"]}
{"id":"39","dataset":"crossner_literature","split":"test","instance":{"id":"39","prompt_labels":"She(O) portrayed(O) Nora(B-person) in(O) Henrik(B-writer) Ibsen(I-writer) '(O) s(O) A(B-book) Doll(I-book) 's(I-book) House(I-book) at(O) the(O) Donmar(B-location) Warehouse(I-location) in(O) London(B-location) 's(I-location) West(I-location) End(I-location) during(O) a(O) limited(O) engagement(O) which(O) ran(O) from(O) May(O) 14(O) ,(O) 2009(O) ,(O) until(O) July(O) 18(O) ,(O) 2009(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, literary genre, country, poem, writer, magazine, book, person, award, event and O.\nSentence: She portrayed Nora in Henrik Ibsen ' s A Doll 's House at the Donmar Warehouse in London 's West End during a limited engagement which ran from May 14 , 2009 , until July 18 , 2009 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","portrayed","Nora","in","Henrik","Ibsen","'","s","A","Doll","'s","House","at","the","Donmar","Warehouse","in","London","'s","West","End","during","a","limited","engagement","which","ran","from","May","14",",","2009",",","until","July","18",",","2009","."],"labels":["O","O","B-person","O","B-writer","I-writer","O","O","B-book","I-book","I-book","I-book","O","O","B-location","I-location","O","B-location","I-location","I-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","organization","literary_genre","country","poem","writer","magazine","book","person","award","event"]}
{"id":"40","dataset":"crossner_literature","split":"test","instance":{"id":"40","prompt_labels":"Carmilla(B-book) is(O) an(O) 1872(O) Gothic(B-literary genre) novel(I-literary genre) la(O) by(O) Irish(O) author(O) Sheridan(B-writer) Le(I-writer) Fanu(I-writer) and(O) one(O) of(O) the(O) early(O) works(O) of(O) vampire(B-literary genre) fiction(I-literary genre) ,(O) predating(O) Bram(B-writer) Stoker(I-writer) '(O) s(O) Dracula(B-book) ((O) 1897(O) )(O) by(O) 26(O) years(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, book, person, literary genre, organization, poem, writer, magazine, location, event, award and O.\nSentence: Carmilla is an 1872 Gothic novel la by Irish author Sheridan Le Fanu and one of the early works of vampire fiction , predating Bram Stoker ' s Dracula ( 1897 ) by 26 years .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Carmilla","is","an","1872","Gothic","novel","la","by","Irish","author","Sheridan","Le","Fanu","and","one","of","the","early","works","of","vampire","fiction",",","predating","Bram","Stoker","'","s","Dracula","(","1897",")","by","26","years","."],"labels":["B-book","O","O","O","B-literary genre","I-literary genre","O","O","O","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","B-writer","I-writer","O","O","B-book","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","book","person","literary_genre","organization","poem","writer","magazine","location","event","award"]}
{"id":"41","dataset":"crossner_literature","split":"test","instance":{"id":"41","prompt_labels":"She(O) appeared(O) in(O) Catherine(B-writer) Cookson(I-writer) '(O) s(O) The(B-book) Fifteen(I-book) Streets(I-book) ,(O) alongside(O) Sean(B-person) Bean(I-person) and(O) Owen(B-person) Teale(I-person) in(O) 1989(O) ;(O) Our(B-book) Own(I-book) Kind(I-book) ((O) Bush(B-person) ,(O) 1991(O) )(O) ;(O) Deadly(B-book) Advice(I-book) ((O) Fletcher(B-person) ,(O) 1993(O) )(O) ;(O) Cabaret(B-book) ((O) Donmar(B-location) Warehouse(I-location) 1994(O) )(O) ;(O) Macbeth(B-book) ((O) Greenwich(B-location) Theatre(I-location) ,(O) 1995(O) )(O) ;(O) and(O) Absurd(B-book) Person(I-book) Singular(I-book) ((O) Garrick(B-location) Theatre(I-location) ,(O) 2007(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, poem, writer, organization, magazine, book, person, location, award, country, literary genre and O.\nSentence: She appeared in Catherine Cookson ' s The Fifteen Streets , alongside Sean Bean and Owen Teale in 1989 ; Our Own Kind ( Bush , 1991 ) ; Deadly Advice ( Fletcher , 1993 ) ; Cabaret ( Donmar Warehouse 1994 ) ; Macbeth ( Greenwich Theatre , 1995 ) ; and Absurd Person Singular ( Garrick Theatre , 2007 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","appeared","in","Catherine","Cookson","'","s","The","Fifteen","Streets",",","alongside","Sean","Bean","and","Owen","Teale","in","1989",";","Our","Own","Kind","(","Bush",",","1991",")",";","Deadly","Advice","(","Fletcher",",","1993",")",";","Cabaret","(","Donmar","Warehouse","1994",")",";","Macbeth","(","Greenwich","Theatre",",","1995",")",";","and","Absurd","Person","Singular","(","Garrick","Theatre",",","2007",")","."],"labels":["O","O","O","B-writer","I-writer","O","O","B-book","I-book","I-book","O","O","B-person","I-person","O","B-person","I-person","O","O","O","B-book","I-book","I-book","O","B-person","O","O","O","O","B-book","I-book","O","B-person","O","O","O","O","B-book","O","B-location","I-location","O","O","O","B-book","O","B-location","I-location","O","O","O","O","O","B-book","I-book","I-book","O","B-location","I-location","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","poem","writer","organization","magazine","book","person","location","award","country","literary_genre"]}
{"id":"43","dataset":"crossner_literature","split":"test","instance":{"id":"43","prompt_labels":"In(O) 1945(O) ,(O) she(O) won(O) the(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) for(O) her(O) work(O) in(O) Mildred(O) Pierce(O) ,(O) and(O) received(O) Best(B-award) Actress(I-award) nominations(O) for(O) Possessed(O) ((O) 1947(O) )(O) and(O) Sudden(O) Fear(O) ((O) 1952(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, writer, book, location, literary genre, magazine, person, event, poem, country, organization and O.\nSentence: In 1945 , she won the Academy Award for Best Actress for her work in Mildred Pierce , and received Best Actress nominations for Possessed ( 1947 ) and Sudden Fear ( 1952 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1945",",","she","won","the","Academy","Award","for","Best","Actress","for","her","work","in","Mildred","Pierce",",","and","received","Best","Actress","nominations","for","Possessed","(","1947",")","and","Sudden","Fear","(","1952",")","."],"labels":["O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","writer","book","location","literary_genre","magazine","person","event","poem","country","organization"]}
{"id":"44","dataset":"crossner_literature","split":"test","instance":{"id":"44","prompt_labels":"Four(O) more(O) children(O) followed(O) :(O) Charlotte(B-writer) Bront(I-writer) ,(O) ((O) 1816-1855(O) )(O) ,(O) Branwell(B-writer) Bront(I-writer) ((O) 1817-1848(O) )(O) ,(O) Emily(B-writer) Bront(I-writer) ,(O) ((O) 1818-1848(O) )(O) and(O) Anne(O) ((O) 1820-1849(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, book, magazine, location, writer, award, literary genre, organization, country, poem and O.\nSentence: Four more children followed : Charlotte Bront , ( 1816-1855 ) , Branwell Bront ( 1817-1848 ) , Emily Bront , ( 1818-1848 ) and Anne ( 1820-1849 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Four","more","children","followed",":","Charlotte","Bront",",","(","1816-1855",")",",","Branwell","Bront","(","1817-1848",")",",","Emily","Bront",",","(","1818-1848",")","and","Anne","(","1820-1849",")","."],"labels":["O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","B-writer","I-writer","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","person","book","magazine","location","writer","award","literary_genre","organization","country","poem"]}
{"id":"46","dataset":"crossner_literature","split":"test","instance":{"id":"46","prompt_labels":"His(O) best(O) known(O) works(O) include(O) The(B-book) Day(I-book) of(I-book) the(I-book) Triffids(I-book) ((O) 1951(O) )(O) and(O) The(B-book) Midwich(I-book) Cuckoos(I-book) ((O) 1957(O) )(O) ,(O) the(O) latter(O) filmed(O) twice(O) as(O) Village(O) of(O) the(O) Damned(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, book, literary genre, country, organization, event, award, magazine, writer, poem, person and O.\nSentence: His best known works include The Day of the Triffids ( 1951 ) and The Midwich Cuckoos ( 1957 ) , the latter filmed twice as Village of the Damned .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","best","known","works","include","The","Day","of","the","Triffids","(","1951",")","and","The","Midwich","Cuckoos","(","1957",")",",","the","latter","filmed","twice","as","Village","of","the","Damned","."],"labels":["O","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","book","literary_genre","country","organization","event","award","magazine","writer","poem","person"]}
{"id":"48","dataset":"crossner_literature","split":"test","instance":{"id":"48","prompt_labels":"Other(O) well-known(O) works(O) are(O) the(O) short-story(B-literary genre) collection(O) Dubliners(B-book) ((O) 1914(O) )(O) ,(O) and(O) the(O) novels(B-literary genre) A(B-book) Portrait(I-book) of(I-book) the(I-book) Artist(I-book) as(I-book) a(I-book) Young(I-book) Man(I-book) ((O) 1916(O) )(O) and(O) Finnegans(B-book) Wake(I-book) ((O) 1939(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, magazine, person, organization, event, book, award, country, writer, literary genre, poem and O.\nSentence: Other well-known works are the short-story collection Dubliners ( 1914 ) , and the novels A Portrait of the Artist as a Young Man ( 1916 ) and Finnegans Wake ( 1939 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","well-known","works","are","the","short-story","collection","Dubliners","(","1914",")",",","and","the","novels","A","Portrait","of","the","Artist","as","a","Young","Man","(","1916",")","and","Finnegans","Wake","(","1939",")","."],"labels":["O","O","O","O","O","B-literary genre","O","B-book","O","O","O","O","O","O","B-literary genre","B-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","magazine","person","organization","event","book","award","country","writer","literary_genre","poem"]}
{"id":"49","dataset":"crossner_literature","split":"test","instance":{"id":"49","prompt_labels":"In(O) addition(O) to(O) his(O) Best(B-award) Picture(I-award) Awards(I-award) ,(O) he(O) received(O) an(O) Academy(B-award) Honorary(I-award) Award(I-award) for(O) his(O) film(O) contributions(O) ,(O) the(O) Palme(B-award) d(I-award) 'Or(I-award) ((O) posthumously(O) )(O) for(O) Union(B-organization) Pacific(I-organization) ((O) 1939(O) )(O) ,(O) a(O) Directors(B-award) Guild(I-award) of(I-award) America(I-award) Award(I-award) for(I-award) Lifetime(I-award) Achievement(I-award) ,(O) and(O) the(O) Irving(B-award) G.(I-award) Thalberg(I-award) Memorial(I-award) Award(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, person, event, location, poem, organization, magazine, literary genre, writer, book and O.\nSentence: In addition to his Best Picture Awards , he received an Academy Honorary Award for his film contributions , the Palme d 'Or ( posthumously ) for Union Pacific ( 1939 ) , a Directors Guild of America Award for Lifetime Achievement , and the Irving G. Thalberg Memorial Award .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","addition","to","his","Best","Picture","Awards",",","he","received","an","Academy","Honorary","Award","for","his","film","contributions",",","the","Palme","d","'Or","(","posthumously",")","for","Union","Pacific","(","1939",")",",","a","Directors","Guild","of","America","Award","for","Lifetime","Achievement",",","and","the","Irving","G.","Thalberg","Memorial","Award","."],"labels":["O","O","O","O","B-award","I-award","I-award","O","O","O","O","B-award","I-award","I-award","O","O","O","O","O","O","B-award","I-award","I-award","O","O","O","O","B-organization","I-organization","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["country","award","person","event","location","poem","organization","magazine","literary_genre","writer","book"]}
{"id":"54","dataset":"crossner_literature","split":"test","instance":{"id":"54","prompt_labels":"Among(O) his(O) best-known(O) works(O) are(O) Moby-Dick(B-book) ((O) 1851(O) )(O) ,(O) Typee(B-book) ((O) 1846(O) )(O) ,(O) a(O) romanticized(O) account(O) of(O) his(O) experiences(O) in(O) Polynesia(B-country) ,(O) and(O) Billy(B-book) Budd(I-book) ,(O) a(O) posthumously(O) published(O) novella(B-literary genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, literary genre, writer, person, location, award, book, poem, magazine, event and O.\nSentence: Among his best-known works are Moby-Dick ( 1851 ) , Typee ( 1846 ) , a romanticized account of his experiences in Polynesia , and Billy Budd , a posthumously published novella .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Among","his","best-known","works","are","Moby-Dick","(","1851",")",",","Typee","(","1846",")",",","a","romanticized","account","of","his","experiences","in","Polynesia",",","and","Billy","Budd",",","a","posthumously","published","novella","."],"labels":["O","O","O","O","O","B-book","O","O","O","O","B-book","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","B-book","I-book","O","O","O","O","B-literary genre","O"],"target_index":null,"target_label":null},"label_list":["organization","country","literary_genre","writer","person","location","award","book","poem","magazine","event"]}
{"id":"55","dataset":"crossner_literature","split":"test","instance":{"id":"55","prompt_labels":"The(O) poem(B-literary genre) was(O) translated(O) into(O) a(O) number(O) of(O) languages(O) ,(O) including(O) the(O) Middle(O) English(O) Ywain(B-poem) and(I-poem) Gawain(I-poem) ;(O) the(O) Old(O) Norwegian(O) Chivaldric(O) vens(B-poem) saga(I-poem) ,(O) and(O) the(O) Old(O) Swedish(O) Herr(B-poem) Ivan(I-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, poem, magazine, organization, location, literary genre, award, person, country, event, writer and O.\nSentence: The poem was translated into a number of languages , including the Middle English Ywain and Gawain ; the Old Norwegian Chivaldric vens saga , and the Old Swedish Herr Ivan .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","poem","was","translated","into","a","number","of","languages",",","including","the","Middle","English","Ywain","and","Gawain",";","the","Old","Norwegian","Chivaldric","vens","saga",",","and","the","Old","Swedish","Herr","Ivan","."],"labels":["O","B-literary genre","O","O","O","O","O","O","O","O","O","O","O","O","B-poem","I-poem","I-poem","O","O","O","O","O","B-poem","I-poem","O","O","O","O","O","B-poem","I-poem","O"],"target_index":null,"target_label":null},"label_list":["book","poem","magazine","organization","location","literary_genre","award","person","country","event","writer"]}
{"id":"56","dataset":"crossner_literature","split":"test","instance":{"id":"56","prompt_labels":"Among(O) the(O) major(O) surviving(O) Ancient(B-country) Rome(I-country) poets(O) of(O) the(O) classical(O) period(O) ,(O) only(O) Catullus(B-writer) ((O) N(O) (O) 11(O) ,(O) 17(O) ,(O) 30(O) ,(O) 34(O) ,(O) 51(O) ,(O) 61(O) )(O) and(O) Horace(B-writer) ((O) Odes(B-poem) )(O) wrote(O) lyric(B-literary genre) poetry(I-literary genre) ,(O) which(O) in(O) the(O) disputed(O) view(O) of(O) some(O) commentatorsS(O) Lyons(B-writer) '(O) Music(B-book) in(I-book) the(I-book) Odes(I-book) of(I-book) Horace(I-book) ((O) 2010(O) )(O) provides(O) evidence(O) to(O) the(O) contrary.(O) was(O) no(O) longer(O) meant(O) to(O) be(O) sung(O) but(O) instead(O) read(O) or(O) recited(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, magazine, literary genre, event, organization, country, award, location, writer, person, book and O.\nSentence: Among the major surviving Ancient Rome poets of the classical period , only Catullus ( N  11 , 17 , 30 , 34 , 51 , 61 ) and Horace ( Odes ) wrote lyric poetry , which in the disputed view of some commentatorsS Lyons ' Music in the Odes of Horace ( 2010 ) provides evidence to the contrary. was no longer meant to be sung but instead read or recited .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Among","the","major","surviving","Ancient","Rome","poets","of","the","classical","period",",","only","Catullus","(","N","","11",",","17",",","30",",","34",",","51",",","61",")","and","Horace","(","Odes",")","wrote","lyric","poetry",",","which","in","the","disputed","view","of","some","commentatorsS","Lyons","'","Music","in","the","Odes","of","Horace","(","2010",")","provides","evidence","to","the","contrary.","was","no","longer","meant","to","be","sung","but","instead","read","or","recited","."],"labels":["O","O","O","O","B-country","I-country","O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","O","B-poem","O","O","B-literary genre","I-literary genre","O","O","O","O","O","O","O","O","O","B-writer","O","B-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","magazine","literary_genre","event","organization","country","award","location","writer","person","book"]}
{"id":"57","dataset":"crossner_literature","split":"test","instance":{"id":"57","prompt_labels":"Mann(B-writer) was(O) awarded(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) in(O) 1929(O) ,(O) after(O) he(O) had(O) been(O) nominated(O) by(O) Anders(B-writer) sterling(I-writer) ,(O) member(O) of(O) the(O) Swedish(B-organization) Academy(I-organization) ,(O) principally(O) in(O) recognition(O) of(O) his(O) popular(O) achievement(O) with(O) the(O) epic(O) Buddenbrooks(B-book) ((O) 1901(O) )(O) ,(O) The(B-book) Magic(I-book) Mountain(I-book) ((O) Der(B-book) Zauberberg(I-book) ,(O) 1924(O) )(O) and(O) his(O) numerous(O) short(B-literary genre) stories(I-literary genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, award, organization, magazine, person, writer, event, literary genre, country, location, book and O.\nSentence: Mann was awarded the Nobel Prize in Literature in 1929 , after he had been nominated by Anders sterling , member of the Swedish Academy , principally in recognition of his popular achievement with the epic Buddenbrooks ( 1901 ) , The Magic Mountain ( Der Zauberberg , 1924 ) and his numerous short stories .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Mann","was","awarded","the","Nobel","Prize","in","Literature","in","1929",",","after","he","had","been","nominated","by","Anders","sterling",",","member","of","the","Swedish","Academy",",","principally","in","recognition","of","his","popular","achievement","with","the","epic","Buddenbrooks","(","1901",")",",","The","Magic","Mountain","(","Der","Zauberberg",",","1924",")","and","his","numerous","short","stories","."],"labels":["B-writer","O","O","O","B-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","B-book","O","O","O","O","B-book","I-book","I-book","O","B-book","I-book","O","O","O","O","O","O","B-literary genre","I-literary genre","O"],"target_index":null,"target_label":null},"label_list":["poem","award","organization","magazine","person","writer","event","literary_genre","country","location","book"]}
{"id":"60","dataset":"crossner_literature","split":"test","instance":{"id":"60","prompt_labels":"As(O) a(O) writer(O) ,(O) Orwell(B-writer) produced(O) literary(B-literary genre) criticism(I-literary genre) and(O) poetry(B-literary genre) ,(O) fiction(B-literary genre) and(O) polemic(B-literary genre) al(O) journalism(O) ;(O) and(O) is(O) best(O) known(O) for(O) the(O) allegorical(O) novella(O) Animal(B-book) Farm(I-book) ((O) 1945(O) )(O) and(O) the(O) dystopian(B-literary genre) novel(I-literary genre) Nineteen(B-book) Eighty-Four(I-book) ((O) 1949(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, magazine, writer, literary genre, book, poem, country, location, event, person and O.\nSentence: As a writer , Orwell produced literary criticism and poetry , fiction and polemic al journalism ; and is best known for the allegorical novella Animal Farm ( 1945 ) and the dystopian novel Nineteen Eighty-Four ( 1949 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["As","a","writer",",","Orwell","produced","literary","criticism","and","poetry",",","fiction","and","polemic","al","journalism",";","and","is","best","known","for","the","allegorical","novella","Animal","Farm","(","1945",")","and","the","dystopian","novel","Nineteen","Eighty-Four","(","1949",")","."],"labels":["O","O","O","O","B-writer","O","B-literary genre","I-literary genre","O","B-literary genre","O","B-literary genre","O","B-literary genre","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","O","O","O","O","O","B-literary genre","I-literary genre","B-book","I-book","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","award","magazine","writer","literary_genre","book","poem","country","location","event","person"]}
{"id":"62","dataset":"crossner_literature","split":"test","instance":{"id":"62","prompt_labels":"The(O) Reference(O) Library(O) ,(O) Astounding(B-magazine) Science(I-magazine) Fiction(I-magazine) ,(O) March(O) 1952(O) ,(O) pp.159(O) In(O) his(O) Books(O) column(O) for(O) The(B-magazine) Magazine(I-magazine) of(I-magazine) Fantasy(I-magazine) &(I-magazine) Science(I-magazine) Fiction(I-magazine) ,(O) Damon(B-writer) Knight(I-writer) selected(O) the(O) novel(B-literary genre) as(O) one(O) of(O) the(O) 10(O) best(O) SF(B-literary genre) books(O) of(O) the(O) 1950s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, event, award, writer, magazine, person, country, literary genre, organization, location, book and O.\nSentence: The Reference Library , Astounding Science Fiction , March 1952 , pp.159 In his Books column for The Magazine of Fantasy & Science Fiction , Damon Knight selected the novel as one of the 10 best SF books of the 1950s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Reference","Library",",","Astounding","Science","Fiction",",","March","1952",",","pp.159","In","his","Books","column","for","The","Magazine","of","Fantasy","&","Science","Fiction",",","Damon","Knight","selected","the","novel","as","one","of","the","10","best","SF","books","of","the","1950s","."],"labels":["O","O","O","O","B-magazine","I-magazine","I-magazine","O","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","I-magazine","I-magazine","I-magazine","I-magazine","I-magazine","O","B-writer","I-writer","O","O","B-literary genre","O","O","O","O","O","O","B-literary genre","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","event","award","writer","magazine","person","country","literary_genre","organization","location","book"]}
{"id":"64","dataset":"crossner_literature","split":"test","instance":{"id":"64","prompt_labels":"The(O) first(O) two(O) Sherlock(B-book) Holmes(I-book) stories(I-book) ,(O) the(O) novels(B-literary genre) A(B-book) Study(I-book) in(I-book) Scarlet(I-book) ((O) 1887(O) )(O) and(O) The(B-book) Sign(I-book) of(I-book) the(I-book) Four(I-book) ((O) 1890(O) )(O) ,(O) were(O) moderately(O) well(O) received(O) ,(O) but(O) Holmes(B-book) first(O) became(O) widely(O) popular(O) early(O) in(O) 1891(O) ,(O) when(O) the(O) first(O) six(O) short(O) stories(O) featuring(O) the(O) character(O) were(O) published(O) in(O) The(B-magazine) Strand(I-magazine) Magazine(I-magazine) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, poem, writer, literary genre, country, award, organization, person, location, magazine, event and O.\nSentence: The first two Sherlock Holmes stories , the novels A Study in Scarlet ( 1887 ) and The Sign of the Four ( 1890 ) , were moderately well received , but Holmes first became widely popular early in 1891 , when the first six short stories featuring the character were published in The Strand Magazine .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","first","two","Sherlock","Holmes","stories",",","the","novels","A","Study","in","Scarlet","(","1887",")","and","The","Sign","of","the","Four","(","1890",")",",","were","moderately","well","received",",","but","Holmes","first","became","widely","popular","early","in","1891",",","when","the","first","six","short","stories","featuring","the","character","were","published","in","The","Strand","Magazine","."],"labels":["O","O","O","B-book","I-book","I-book","O","O","B-literary genre","B-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","B-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","I-magazine","O"],"target_index":null,"target_label":null},"label_list":["book","poem","writer","literary_genre","country","award","organization","person","location","magazine","event"]}
{"id":"65","dataset":"crossner_literature","split":"test","instance":{"id":"65","prompt_labels":"Smith(B-writer) briefly(O) moved(O) among(O) the(O) circle(O) that(O) included(O) Ambrose(B-writer) Bierce(I-writer) and(O) Jack(B-writer) London(I-writer) ,(O) but(O) his(O) early(O) fame(O) soon(O) faded(O) away(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, writer, poem, book, location, country, magazine, event, organization, literary genre and O.\nSentence: Smith briefly moved among the circle that included Ambrose Bierce and Jack London , but his early fame soon faded away .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Smith","briefly","moved","among","the","circle","that","included","Ambrose","Bierce","and","Jack","London",",","but","his","early","fame","soon","faded","away","."],"labels":["B-writer","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","person","writer","poem","book","location","country","magazine","event","organization","literary_genre"]}
{"id":"66","dataset":"crossner_literature","split":"test","instance":{"id":"66","prompt_labels":"Parodies(O) of(O) courtly(B-literary genre) poetry(I-literary genre) also(O) exist(O) ,(O) among(O) them(O) Der(B-poem) Weinschwelg(I-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, literary genre, country, magazine, location, person, book, organization, event, writer, award and O.\nSentence: Parodies of courtly poetry also exist , among them Der Weinschwelg .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Parodies","of","courtly","poetry","also","exist",",","among","them","Der","Weinschwelg","."],"labels":["O","O","B-literary genre","I-literary genre","O","O","O","O","O","B-poem","I-poem","O"],"target_index":null,"target_label":null},"label_list":["poem","literary_genre","country","magazine","location","person","book","organization","event","writer","award"]}
{"id":"67","dataset":"crossner_literature","split":"test","instance":{"id":"67","prompt_labels":"Her(O) most(O) notable(O) accomplishment(O) was(O) the(O) translation(O) of(O) Roman(B-poem) Triptych(I-poem) ((O) Trittico(B-poem) romano(I-poem) ((I-poem) Meditazioni(I-poem) )(I-poem) by(O) Pope(B-writer) John(I-writer) Paul(I-writer) II(I-writer) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, event, magazine, person, book, organization, literary genre, poem, location, writer and O.\nSentence: Her most notable accomplishment was the translation of Roman Triptych ( Trittico romano ( Meditazioni ) by Pope John Paul II ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Her","most","notable","accomplishment","was","the","translation","of","Roman","Triptych","(","Trittico","romano","(","Meditazioni",")","by","Pope","John","Paul","II",")","."],"labels":["O","O","O","O","O","O","O","O","B-poem","I-poem","O","B-poem","I-poem","I-poem","I-poem","I-poem","O","B-writer","I-writer","I-writer","I-writer","O","O"],"target_index":null,"target_label":null},"label_list":["country","award","event","magazine","person","book","organization","literary_genre","poem","location","writer"]}
{"id":"70","dataset":"crossner_literature","split":"test","instance":{"id":"70","prompt_labels":"Classicist(O) Bernard(B-writer) Knox(I-writer) made(O) direct(O) reference(O) to(O) this(O) topic(O) when(O) he(O) delivered(O) his(O) 1992(O) Jefferson(B-award) Lecture(I-award) ((O) the(O) U.S.(B-country) federal(O) government(O) 's(O) highest(O) honor(O) for(O) achievement(O) in(O) the(O) humanities(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, person, book, literary genre, event, country, organization, poem, award, location, magazine and O.\nSentence: Classicist Bernard Knox made direct reference to this topic when he delivered his 1992 Jefferson Lecture ( the U.S. federal government 's highest honor for achievement in the humanities ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Classicist","Bernard","Knox","made","direct","reference","to","this","topic","when","he","delivered","his","1992","Jefferson","Lecture","(","the","U.S.","federal","government","'s","highest","honor","for","achievement","in","the","humanities",")","."],"labels":["O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["writer","person","book","literary_genre","event","country","organization","poem","award","location","magazine"]}
{"id":"71","dataset":"crossner_literature","split":"test","instance":{"id":"71","prompt_labels":"In(O) The(B-magazine) New(I-magazine) York(I-magazine) Review(I-magazine) of(I-magazine) Books(I-magazine) Christopher(B-writer) Ricks(I-writer) wrote(O) of(O) the(O) refinement(O) of(O) self-consciousness(O) ,(O) usually(O) flawless(O) in(O) its(O) execution(O) and(O) Larkin(B-writer) 's(O) summoning(O) up(O) of(O) the(O) world(O) of(O) all(O) of(O) us(O) ,(O) the(O) place(O) where(O) ,(O) in(O) the(O) end(O) ,(O) we(O) find(O) our(O) happiness(O) ,(O) or(O) not(O) at(O) all(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, organization, event, award, book, person, literary genre, writer, magazine, poem and O.\nSentence: In The New York Review of Books Christopher Ricks wrote of the refinement of self-consciousness , usually flawless in its execution and Larkin 's summoning up of the world of all of us , the place where , in the end , we find our happiness , or not at all .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","The","New","York","Review","of","Books","Christopher","Ricks","wrote","of","the","refinement","of","self-consciousness",",","usually","flawless","in","its","execution","and","Larkin","'s","summoning","up","of","the","world","of","all","of","us",",","the","place","where",",","in","the","end",",","we","find","our","happiness",",","or","not","at","all","."],"labels":["O","B-magazine","I-magazine","I-magazine","I-magazine","I-magazine","I-magazine","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","location","organization","event","award","book","person","literary_genre","writer","magazine","poem"]}
{"id":"72","dataset":"crossner_literature","split":"test","instance":{"id":"72","prompt_labels":"His(O) original(O) works(O) include(O) the(O) Saga(B-book) of(I-book) Seven(I-book) Suns(I-book) series(O) and(O) the(O) Nebula(B-award) Award(I-award) -nominated(O) Assemblers(B-book) of(I-book) Infinity(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, book, location, event, magazine, award, writer, literary genre, poem, organization and O.\nSentence: His original works include the Saga of Seven Suns series and the Nebula Award -nominated Assemblers of Infinity .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","original","works","include","the","Saga","of","Seven","Suns","series","and","the","Nebula","Award","-nominated","Assemblers","of","Infinity","."],"labels":["O","O","O","O","O","B-book","I-book","I-book","I-book","O","O","O","B-award","I-award","O","B-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["person","country","book","location","event","magazine","award","writer","literary_genre","poem","organization"]}
{"id":"74","dataset":"crossner_literature","split":"test","instance":{"id":"74","prompt_labels":"Alternate(B-literary genre) history(I-literary genre) has(O) long(O) been(O) a(O) staple(O) of(O) Japanese(O) speculative(O) fiction(B-literary genre) with(O) such(O) authors(O) as(O) Futaro(B-writer) Yamada(I-writer) and(O) Ry(B-writer) Hanmura(I-writer) writing(O) novels(B-literary genre) set(O) in(O) recognizable(O) historical(O) settings(O) with(O) supernatural(B-literary genre) or(O) science(B-literary genre) fiction(I-literary genre) elements(O) present(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, writer, event, country, literary genre, organization, award, location, magazine, poem, book and O.\nSentence: Alternate history has long been a staple of Japanese speculative fiction with such authors as Futaro Yamada and Ry Hanmura writing novels set in recognizable historical settings with supernatural or science fiction elements present .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Alternate","history","has","long","been","a","staple","of","Japanese","speculative","fiction","with","such","authors","as","Futaro","Yamada","and","Ry","Hanmura","writing","novels","set","in","recognizable","historical","settings","with","supernatural","or","science","fiction","elements","present","."],"labels":["B-literary genre","I-literary genre","O","O","O","O","O","O","O","O","B-literary genre","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-literary genre","O","O","O","O","O","O","B-literary genre","O","B-literary genre","I-literary genre","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","writer","event","country","literary_genre","organization","award","location","magazine","poem","book"]}
{"id":"76","dataset":"crossner_literature","split":"test","instance":{"id":"76","prompt_labels":"Dick(B-writer) said(O) he(O) conceived(O) The(B-book) Man(I-book) in(I-book) the(I-book) High(I-book) Castle(I-book) when(O) reading(O) Bring(B-book) the(I-book) Jubilee(I-book) ((O) 1953(O) )(O) ,(O) by(O) Ward(B-writer) Moore(I-writer) ,(O) which(O) occurs(O) mainly(O) in(O) an(O) alternative(O) 20th-century(O) US(B-country) wherein(O) the(O) Confederate(B-organization) States(I-organization) of(I-organization) America(I-organization) won(O) the(O) American(B-event) Civil(I-event) War(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, book, literary genre, country, organization, person, location, writer, event, magazine, poem and O.\nSentence: Dick said he conceived The Man in the High Castle when reading Bring the Jubilee ( 1953 ) , by Ward Moore , which occurs mainly in an alternative 20th-century US wherein the Confederate States of America won the American Civil War .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Dick","said","he","conceived","The","Man","in","the","High","Castle","when","reading","Bring","the","Jubilee","(","1953",")",",","by","Ward","Moore",",","which","occurs","mainly","in","an","alternative","20th-century","US","wherein","the","Confederate","States","of","America","won","the","American","Civil","War","."],"labels":["B-writer","O","O","O","B-book","I-book","I-book","I-book","I-book","I-book","O","O","B-book","I-book","I-book","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","B-country","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["award","book","literary_genre","country","organization","person","location","writer","event","magazine","poem"]}
{"id":"78","dataset":"crossner_literature","split":"test","instance":{"id":"78","prompt_labels":"In(O) the(O) service(O) of(O) the(O) Russian(B-country) Empire(I-country) ,(O) Clausewitz(B-writer) helped(O) negotiate(O) the(O) Convention(O) of(O) Tauroggen(O) ((O) 1812(O) )(O) ,(O) which(O) prepared(O) the(O) way(O) for(O) the(O) coalition(B-organization) of(I-organization) Prussia(I-organization) ,(I-organization) Russia(I-organization) ,(O) and(O) the(O) United(B-country) Kingdom(I-country) of(I-country) Great(I-country) Britain(I-country) and(I-country) Ireland(I-country) that(O) ultimately(O) defeated(O) Napoleon(B-person) and(O) his(O) allies(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, book, person, magazine, literary genre, country, award, writer, location, organization, poem and O.\nSentence: In the service of the Russian Empire , Clausewitz helped negotiate the Convention of Tauroggen ( 1812 ) , which prepared the way for the coalition of Prussia , Russia , and the United Kingdom of Great Britain and Ireland that ultimately defeated Napoleon and his allies .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","service","of","the","Russian","Empire",",","Clausewitz","helped","negotiate","the","Convention","of","Tauroggen","(","1812",")",",","which","prepared","the","way","for","the","coalition","of","Prussia",",","Russia",",","and","the","United","Kingdom","of","Great","Britain","and","Ireland","that","ultimately","defeated","Napoleon","and","his","allies","."],"labels":["O","O","O","O","O","B-country","I-country","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-country","I-country","I-country","I-country","I-country","I-country","I-country","O","O","O","B-person","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","book","person","magazine","literary_genre","country","award","writer","location","organization","poem"]}
{"id":"79","dataset":"crossner_literature","split":"test","instance":{"id":"79","prompt_labels":"The(O) second(O) volume(O) ((O) The(B-book) Black(I-book) Cauldron(I-book) )(O) was(O) a(O) runner-up(O) for(O) the(O) 1966(O) Newbery(B-award) Medal(I-award) ;(O) the(O) fourth(O) ((O) Taran(B-book) Wanderer(I-book) )(O) was(O) a(O) School(B-magazine) Library(I-magazine) Journal(I-magazine) Best(B-award) Book(I-award) of(I-award) the(I-award) Year(I-award) ;(O) the(O) fifth(O) and(O) concluding(O) volume(O) ((O) The(B-book) High(I-book) King(I-book) )(O) won(O) the(O) 1969(O) Newbery(B-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, event, writer, location, award, country, organization, magazine, book, literary genre, person and O.\nSentence: The second volume ( The Black Cauldron ) was a runner-up for the 1966 Newbery Medal ; the fourth ( Taran Wanderer ) was a School Library Journal Best Book of the Year ; the fifth and concluding volume ( The High King ) won the 1969 Newbery .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","second","volume","(","The","Black","Cauldron",")","was","a","runner-up","for","the","1966","Newbery","Medal",";","the","fourth","(","Taran","Wanderer",")","was","a","School","Library","Journal","Best","Book","of","the","Year",";","the","fifth","and","concluding","volume","(","The","High","King",")","won","the","1969","Newbery","."],"labels":["O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","B-award","I-award","O","O","O","O","B-book","I-book","O","O","O","B-magazine","I-magazine","I-magazine","B-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","B-award","O"],"target_index":null,"target_label":null},"label_list":["poem","event","writer","location","award","country","organization","magazine","book","literary_genre","person"]}
{"id":"80","dataset":"crossner_literature","split":"test","instance":{"id":"80","prompt_labels":"The(B-poem) Gentle(I-poem) Shepherd(I-poem) ,(O) first(O) published(O) in(O) 1725(O) ,(O) was(O) dedicated(O) to(O) her(O) by(O) Allan(B-writer) Ramsay(I-writer) and(O) Hamilton(B-writer) of(I-writer) Bangour(I-writer) wrote(O) flattering(O) verse(B-literary genre) to(O) Susanna(B-person) ,(I-person) Lady(I-person) Eglinton(I-person) and(O) her(O) daughters(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, book, location, country, magazine, award, poem, organization, person, event, writer and O.\nSentence: The Gentle Shepherd , first published in 1725 , was dedicated to her by Allan Ramsay and Hamilton of Bangour wrote flattering verse to Susanna , Lady Eglinton and her daughters .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Gentle","Shepherd",",","first","published","in","1725",",","was","dedicated","to","her","by","Allan","Ramsay","and","Hamilton","of","Bangour","wrote","flattering","verse","to","Susanna",",","Lady","Eglinton","and","her","daughters","."],"labels":["B-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","B-literary genre","O","B-person","I-person","I-person","I-person","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","book","location","country","magazine","award","poem","organization","person","event","writer"]}
{"id":"82","dataset":"crossner_literature","split":"test","instance":{"id":"82","prompt_labels":"Wilder(B-person) earned(O) the(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Adapted(I-award) Screenplay(I-award) Academy(B-award) Awards(I-award) for(O) the(O) adaptation(O) of(O) a(O) Charles(B-writer) R.(I-writer) Jackson(I-writer) story(O) ,(O) The(B-book) Lost(I-book) Weekend(I-book) ((O) 1945(O) )(O) ,(O) about(O) alcoholism(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, event, magazine, award, poem, person, location, literary genre, country, book, organization and O.\nSentence: Wilder earned the Academy Award for Best Director and Academy Award for Best Adapted Screenplay Academy Awards for the adaptation of a Charles R. Jackson story , The Lost Weekend ( 1945 ) , about alcoholism .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Wilder","earned","the","Academy","Award","for","Best","Director","and","Academy","Award","for","Best","Adapted","Screenplay","Academy","Awards","for","the","adaptation","of","a","Charles","R.","Jackson","story",",","The","Lost","Weekend","(","1945",")",",","about","alcoholism","."],"labels":["B-person","O","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","B-award","I-award","O","O","O","O","O","B-writer","I-writer","I-writer","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["writer","event","magazine","award","poem","person","location","literary_genre","country","book","organization"]}
{"id":"83","dataset":"crossner_literature","split":"test","instance":{"id":"83","prompt_labels":"122(O) The(B-poem) Public(I-poem) Square(I-poem) illustrates(O) this(O) quality(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, country, award, organization, writer, person, book, event, magazine, location, literary genre and O.\nSentence: 122 The Public Square illustrates this quality .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["122","The","Public","Square","illustrates","this","quality","."],"labels":["O","B-poem","I-poem","I-poem","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","country","award","organization","writer","person","book","event","magazine","location","literary_genre"]}
{"id":"84","dataset":"crossner_literature","split":"test","instance":{"id":"84","prompt_labels":"Some(O) of(O) his(O) best(O) known(O) poems(O) are(O) about(O) love(B-literary genre) ,(O) such(O) as(O) Funeral(B-poem) Blues(I-poem) ;(O) on(O) political(B-literary genre) and(I-literary genre) social(I-literary genre) themes(I-literary genre) ,(O) such(O) as(O) September(B-poem) 1(I-poem) ,(I-poem) 1939(I-poem) and(O) The(B-poem) Shield(I-poem) of(I-poem) Achilles(I-poem) ;(O) on(O) cultural(B-literary genre) and(I-literary genre) psychological(I-literary genre) themes(I-literary genre) ,(O) such(O) as(O) The(B-poem) Age(I-poem) of(I-poem) Anxiety(I-poem) ;(O) and(O) on(O) religious(B-literary genre) themes(I-literary genre) such(O) as(O) For(B-poem) the(I-poem) Time(I-poem) Being(I-poem) and(O) Horae(B-poem) Canonicae(I-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, location, writer, person, literary genre, magazine, book, award, organization, poem and O.\nSentence: Some of his best known poems are about love , such as Funeral Blues ; on political and social themes , such as September 1 , 1939 and The Shield of Achilles ; on cultural and psychological themes , such as The Age of Anxiety ; and on religious themes such as For the Time Being and Horae Canonicae .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","of","his","best","known","poems","are","about","love",",","such","as","Funeral","Blues",";","on","political","and","social","themes",",","such","as","September","1",",","1939","and","The","Shield","of","Achilles",";","on","cultural","and","psychological","themes",",","such","as","The","Age","of","Anxiety",";","and","on","religious","themes","such","as","For","the","Time","Being","and","Horae","Canonicae","."],"labels":["O","O","O","O","O","O","O","O","B-literary genre","O","O","O","B-poem","I-poem","O","O","B-literary genre","I-literary genre","I-literary genre","I-literary genre","O","O","O","B-poem","I-poem","I-poem","I-poem","O","B-poem","I-poem","I-poem","I-poem","O","O","B-literary genre","I-literary genre","I-literary genre","I-literary genre","O","O","O","B-poem","I-poem","I-poem","I-poem","O","O","O","B-literary genre","I-literary genre","O","O","B-poem","I-poem","I-poem","I-poem","O","B-poem","I-poem","O"],"target_index":null,"target_label":null},"label_list":["event","country","location","writer","person","literary_genre","magazine","book","award","organization","poem"]}
{"id":"85","dataset":"crossner_literature","split":"test","instance":{"id":"85","prompt_labels":"As(O) a(O) Barnard(B-organization) student(O) ,(O) Elise(B-writer) Cowen(I-writer) extensively(O) read(O) the(O) poetry(B-literary genre) of(O) Ezra(B-writer) Pound(I-writer) and(O) T.(B-writer) S.(I-writer) Eliot(I-writer) ,(O) when(O) she(O) met(O) Joyce(B-writer) Johnson(I-writer) and(O) Leo(B-writer) Skir(I-writer) ,(O) among(O) other(O) Beat(B-event) players(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, country, person, location, literary genre, award, writer, magazine, book, event, organization and O.\nSentence: As a Barnard student , Elise Cowen extensively read the poetry of Ezra Pound and T. S. Eliot , when she met Joyce Johnson and Leo Skir , among other Beat players .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["As","a","Barnard","student",",","Elise","Cowen","extensively","read","the","poetry","of","Ezra","Pound","and","T.","S.","Eliot",",","when","she","met","Joyce","Johnson","and","Leo","Skir",",","among","other","Beat","players","."],"labels":["O","O","B-organization","O","O","B-writer","I-writer","O","O","O","B-literary genre","O","B-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","O","O","B-event","O","O"],"target_index":null,"target_label":null},"label_list":["poem","country","person","location","literary_genre","award","writer","magazine","book","event","organization"]}
{"id":"86","dataset":"crossner_literature","split":"test","instance":{"id":"86","prompt_labels":"After(O) some(O) unsuccessful(O) films(O) ,(O) she(O) had(O) her(O) critical(O) breakthrough(O) playing(O) a(O) vulgar(O) waitress(O) in(O) Of(B-book) Human(I-book) Bondage(I-book) ((O) 1934(O) )(O) ,(O) although(O) ,(O) contentiously(O) ,(O) she(O) was(O) not(O) among(O) the(O) three(O) nominees(O) for(O) the(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) that(O) year(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, person, literary genre, location, magazine, writer, event, poem, organization, award, country and O.\nSentence: After some unsuccessful films , she had her critical breakthrough playing a vulgar waitress in Of Human Bondage ( 1934 ) , although , contentiously , she was not among the three nominees for the Academy Award for Best Actress that year .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","some","unsuccessful","films",",","she","had","her","critical","breakthrough","playing","a","vulgar","waitress","in","Of","Human","Bondage","(","1934",")",",","although",",","contentiously",",","she","was","not","among","the","three","nominees","for","the","Academy","Award","for","Best","Actress","that","year","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O"],"target_index":null,"target_label":null},"label_list":["book","person","literary_genre","location","magazine","writer","event","poem","organization","award","country"]}
{"id":"87","dataset":"crossner_literature","split":"test","instance":{"id":"87","prompt_labels":"Soon(O) after(O) his(O) return(O) to(O) England(B-country) ,(O) Dickens(B-writer) began(O) work(O) on(O) the(O) first(O) of(O) his(O) Christmas(B-event) stories(O) ,(O) A(B-book) Christmas(I-book) Carol(I-book) ,(O) written(O) in(O) 1843(O) ,(O) which(O) was(O) followed(O) by(O) The(B-book) Chimes(I-book) in(O) 1844(O) and(O) The(B-book) Cricket(I-book) on(I-book) the(I-book) Hearth(I-book) in(O) 1845(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, poem, book, location, literary genre, writer, event, organization, award, magazine, country and O.\nSentence: Soon after his return to England , Dickens began work on the first of his Christmas stories , A Christmas Carol , written in 1843 , which was followed by The Chimes in 1844 and The Cricket on the Hearth in 1845 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Soon","after","his","return","to","England",",","Dickens","began","work","on","the","first","of","his","Christmas","stories",",","A","Christmas","Carol",",","written","in","1843",",","which","was","followed","by","The","Chimes","in","1844","and","The","Cricket","on","the","Hearth","in","1845","."],"labels":["O","O","O","O","O","B-country","O","B-writer","O","O","O","O","O","O","O","B-event","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","B-book","I-book","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","poem","book","location","literary_genre","writer","event","organization","award","magazine","country"]}
{"id":"88","dataset":"crossner_literature","split":"test","instance":{"id":"88","prompt_labels":"These(O) were(O) later(O) collected(O) and(O) published(O) in(O) book(O) form(O) as(O) My(B-book) Disillusionment(I-book) in(I-book) Russia(I-book) ((O) 1923(O) )(O) and(O) My(B-book) Further(I-book) Disillusionment(I-book) in(I-book) Russia(I-book) ((O) 1924(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, organization, event, literary genre, poem, book, writer, award, magazine, location and O.\nSentence: These were later collected and published in book form as My Disillusionment in Russia ( 1923 ) and My Further Disillusionment in Russia ( 1924 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["These","were","later","collected","and","published","in","book","form","as","My","Disillusionment","in","Russia","(","1923",")","and","My","Further","Disillusionment","in","Russia","(","1924",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","country","organization","event","literary_genre","poem","book","writer","award","magazine","location"]}
{"id":"89","dataset":"crossner_literature","split":"test","instance":{"id":"89","prompt_labels":"His(O) name(O) had(O) sometimes(O) been(O) mentioned(O) as(O) a(O) contender(O) for(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) ,(O) but(O) in(O) 1971(O) ,(O) after(O) losing(O) to(O) Aleksandr(B-writer) Solzhenitsyn(I-writer) ,(O) he(O) wrote(O) to(O) a(O) friend(O) :(O) That(O) Nobel(B-award) Prize(I-award) !(O) I(O) hope(O) I(O) never(O) hear(O) it(O) mentioned(O) again(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, organization, writer, magazine, location, book, poem, award, person, literary genre and O.\nSentence: His name had sometimes been mentioned as a contender for the Nobel Prize in Literature , but in 1971 , after losing to Aleksandr Solzhenitsyn , he wrote to a friend : That Nobel Prize ! I hope I never hear it mentioned again .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","name","had","sometimes","been","mentioned","as","a","contender","for","the","Nobel","Prize","in","Literature",",","but","in","1971",",","after","losing","to","Aleksandr","Solzhenitsyn",",","he","wrote","to","a","friend",":","That","Nobel","Prize","!","I","hope","I","never","hear","it","mentioned","again","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","B-award","I-award","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","event","organization","writer","magazine","location","book","poem","award","person","literary_genre"]}
{"id":"90","dataset":"crossner_literature","split":"test","instance":{"id":"90","prompt_labels":"Sir(O) Walter(B-writer) Scott(I-writer) published(O) Harold(B-poem) the(I-poem) Dauntless(I-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, literary genre, location, poem, book, award, event, writer, magazine, country and O.\nSentence: Sir Walter Scott published Harold the Dauntless .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Sir","Walter","Scott","published","Harold","the","Dauntless","."],"labels":["O","B-writer","I-writer","O","B-poem","I-poem","I-poem","O"],"target_index":null,"target_label":null},"label_list":["person","organization","literary_genre","location","poem","book","award","event","writer","magazine","country"]}
{"id":"92","dataset":"crossner_literature","split":"test","instance":{"id":"92","prompt_labels":"In(O) his(O) London(O) Letter(O) on(O) 17(O) April(O) 1944(O) for(O) Partisan(B-magazine) Review(I-magazine) ,(O) Orwell(B-writer) wrote(O) that(O) it(O) was(O) now(O) next(O) door(O) to(O) impossible(O) to(O) get(O) anything(O) overtly(O) anti-Russian(O) printed(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, writer, book, person, organization, event, magazine, literary genre, country, poem and O.\nSentence: In his London Letter on 17 April 1944 for Partisan Review , Orwell wrote that it was now next door to impossible to get anything overtly anti-Russian printed .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","his","London","Letter","on","17","April","1944","for","Partisan","Review",",","Orwell","wrote","that","it","was","now","next","door","to","impossible","to","get","anything","overtly","anti-Russian","printed","."],"labels":["O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","award","writer","book","person","organization","event","magazine","literary_genre","country","poem"]}
{"id":"93","dataset":"crossner_literature","split":"test","instance":{"id":"93","prompt_labels":"Bernardo(B-writer) Bertolucci(I-writer) ((O) ;(O) 16(O) March(O) 1941(O) -(O) 26(O) November(O) 2018(O) )(O) was(O) an(O) Italian(O) director(O) and(O) screenwriter(O) ,(O) whose(O) films(O) include(O) The(O) Conformist(O) ,(O) Last(O) Tango(O) in(O) Paris(O) ,(O) 1900(O) ,(O) The(O) Last(O) Emperor(O) ((O) for(O) which(O) he(O) won(O) the(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) and(O) the(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Adapted(I-award) Screenplay(I-award) )(O) ,(O) The(O) Sheltering(O) Sky(O) ,(O) Little(O) Buddha(O) ,(O) Stealing(O) Beauty(O) and(O) The(O) Dreamers(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, book, country, magazine, event, location, writer, literary genre, person, award, organization and O.\nSentence: Bernardo Bertolucci ( ; 16 March 1941 - 26 November 2018 ) was an Italian director and screenwriter , whose films include The Conformist , Last Tango in Paris , 1900 , The Last Emperor ( for which he won the Academy Award for Best Director and the Academy Award for Best Adapted Screenplay ) , The Sheltering Sky , Little Buddha , Stealing Beauty and The Dreamers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Bernardo","Bertolucci","(",";","16","March","1941","-","26","November","2018",")","was","an","Italian","director","and","screenwriter",",","whose","films","include","The","Conformist",",","Last","Tango","in","Paris",",","1900",",","The","Last","Emperor","(","for","which","he","won","the","Academy","Award","for","Best","Director","and","the","Academy","Award","for","Best","Adapted","Screenplay",")",",","The","Sheltering","Sky",",","Little","Buddha",",","Stealing","Beauty","and","The","Dreamers","."],"labels":["B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","book","country","magazine","event","location","writer","literary_genre","person","award","organization"]}
{"id":"95","dataset":"crossner_literature","split":"test","instance":{"id":"95","prompt_labels":"She(O) exhibited(O) a(O) marble(O) statue(O) of(O) Maud(B-poem) Muller(I-poem) at(O) the(O) 1876(B-event) Centennial(I-event) Exposition(I-event) in(O) Philadelphia(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, poem, magazine, award, literary genre, person, book, event, writer, organization and O.\nSentence: She exhibited a marble statue of Maud Muller at the 1876 Centennial Exposition in Philadelphia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","exhibited","a","marble","statue","of","Maud","Muller","at","the","1876","Centennial","Exposition","in","Philadelphia","."],"labels":["O","O","O","O","O","O","B-poem","I-poem","O","O","B-event","I-event","I-event","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["country","location","poem","magazine","award","literary_genre","person","book","event","writer","organization"]}
{"id":"96","dataset":"crossner_literature","split":"test","instance":{"id":"96","prompt_labels":"The(B-book) Color(I-book) Purple(I-book) is(O) a(O) 1982(O) epistolary(B-literary genre) novel(I-literary genre) by(O) American(O) author(O) Alice(B-writer) Walker(I-writer) which(O) won(O) the(O) 1983(O) Pulitzer(B-award) Prize(I-award) for(I-award) Fiction(I-award) and(O) the(O) National(B-award) Book(I-award) Award(I-award) for(I-award) Fiction(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, event, magazine, literary genre, country, award, writer, person, organization, location, book and O.\nSentence: The Color Purple is a 1982 epistolary novel by American author Alice Walker which won the 1983 Pulitzer Prize for Fiction and the National Book Award for Fiction .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Color","Purple","is","a","1982","epistolary","novel","by","American","author","Alice","Walker","which","won","the","1983","Pulitzer","Prize","for","Fiction","and","the","National","Book","Award","for","Fiction","."],"labels":["B-book","I-book","I-book","O","O","O","B-literary genre","I-literary genre","O","O","O","B-writer","I-writer","O","O","O","O","B-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["poem","event","magazine","literary_genre","country","award","writer","person","organization","location","book"]}
{"id":"97","dataset":"crossner_literature","split":"test","instance":{"id":"97","prompt_labels":"During(O) his(O) visits(O) to(O) England(B-country) in(O) these(O) years(O) ,(O) Swift(B-writer) published(O) A(B-book) Tale(I-book) of(I-book) a(I-book) Tub(I-book) and(O) The(B-book) Battle(I-book) of(I-book) the(I-book) Books(I-book) ((O) 1704(O) )(O) and(O) began(O) to(O) gain(O) a(O) reputation(O) as(O) a(O) writer(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, literary genre, writer, country, poem, organization, magazine, book, award, event, location and O.\nSentence: During his visits to England in these years , Swift published A Tale of a Tub and The Battle of the Books ( 1704 ) and began to gain a reputation as a writer .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","his","visits","to","England","in","these","years",",","Swift","published","A","Tale","of","a","Tub","and","The","Battle","of","the","Books","(","1704",")","and","began","to","gain","a","reputation","as","a","writer","."],"labels":["O","O","O","O","B-country","O","O","O","O","B-writer","O","B-book","I-book","I-book","I-book","I-book","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","literary_genre","writer","country","poem","organization","magazine","book","award","event","location"]}
{"id":"98","dataset":"crossner_literature","split":"test","instance":{"id":"98","prompt_labels":"Humorist(O) Henry(B-person) Morgan(I-person) had(O) a(O) recurring(O) role(O) as(O) a(O) humor(O) writer(O) for(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) ,(O) which(O) was(O) said(O) to(O) be(O) based(O) on(O) real-life(O) humorist(O) /(O) actor(O) Robert(B-writer) Benchley(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, literary genre, organization, writer, award, magazine, book, country, person, location, event and O.\nSentence: Humorist Henry Morgan had a recurring role as a humor writer for The New Yorker , which was said to be based on real-life humorist / actor Robert Benchley .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Humorist","Henry","Morgan","had","a","recurring","role","as","a","humor","writer","for","The","New","Yorker",",","which","was","said","to","be","based","on","real-life","humorist","/","actor","Robert","Benchley","."],"labels":["O","B-person","I-person","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","I-magazine","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["poem","literary_genre","organization","writer","award","magazine","book","country","person","location","event"]}
{"id":"100","dataset":"crossner_literature","split":"test","instance":{"id":"100","prompt_labels":"Marilynne(B-writer) Robinson(I-writer) ,(O) On(O) Edgar(B-writer) Allan(I-writer) Poe(I-writer) ,(O) The(B-magazine) New(I-magazine) York(I-magazine) Review(I-magazine) of(I-magazine) Books(I-magazine) ,(O) vol(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, location, award, writer, organization, book, person, country, literary genre, poem, event and O.\nSentence: Marilynne Robinson , On Edgar Allan Poe , The New York Review of Books , vol .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Marilynne","Robinson",",","On","Edgar","Allan","Poe",",","The","New","York","Review","of","Books",",","vol","."],"labels":["B-writer","I-writer","O","O","B-writer","I-writer","I-writer","O","B-magazine","I-magazine","I-magazine","I-magazine","I-magazine","I-magazine","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","location","award","writer","organization","book","person","country","literary_genre","poem","event"]}
{"id":"102","dataset":"crossner_literature","split":"test","instance":{"id":"102","prompt_labels":"1283(O) )(O) ,(O) was(O) commemorated(O) in(O) the(O) poem(B-literary genre) The(B-poem) Bards(I-poem) of(I-poem) Wales(I-poem) by(O) the(O) Kingdom(B-country) of(I-country) Hungary(I-country) poet(O) Jnos(B-writer) Arany(I-writer) in(O) 1857(O) ,(O) as(O) a(O) way(O) of(O) encoded(O) resistance(O) to(O) the(O) suppressive(O) politics(O) of(O) his(O) own(O) time(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, location, literary genre, poem, book, organization, writer, country, award, event, person and O.\nSentence: 1283 ) , was commemorated in the poem The Bards of Wales by the Kingdom of Hungary poet Jnos Arany in 1857 , as a way of encoded resistance to the suppressive politics of his own time .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["1283",")",",","was","commemorated","in","the","poem","The","Bards","of","Wales","by","the","Kingdom","of","Hungary","poet","Jnos","Arany","in","1857",",","as","a","way","of","encoded","resistance","to","the","suppressive","politics","of","his","own","time","."],"labels":["O","O","O","O","O","O","O","B-literary genre","B-poem","I-poem","I-poem","I-poem","O","O","B-country","I-country","I-country","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","location","literary_genre","poem","book","organization","writer","country","award","event","person"]}
{"id":"103","dataset":"crossner_literature","split":"test","instance":{"id":"103","prompt_labels":"Xuanxue(O) was(O) a(O) philosophical(O) school(O) that(O) combined(O) elements(O) of(O) Confucianism(O) and(O) Taoism(O) to(O) reinterpret(O) the(O) I(B-book) Ching(I-book) ,(O) Tao(B-book) Te(I-book) Ching(I-book) ,(O) and(O) Zhuangzi(B-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, book, magazine, person, award, event, literary genre, poem, location, country, organization and O.\nSentence: Xuanxue was a philosophical school that combined elements of Confucianism and Taoism to reinterpret the I Ching , Tao Te Ching , and Zhuangzi .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Xuanxue","was","a","philosophical","school","that","combined","elements","of","Confucianism","and","Taoism","to","reinterpret","the","I","Ching",",","Tao","Te","Ching",",","and","Zhuangzi","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","O","B-book","I-book","I-book","O","O","B-book","O"],"target_index":null,"target_label":null},"label_list":["writer","book","magazine","person","award","event","literary_genre","poem","location","country","organization"]}
{"id":"106","dataset":"crossner_literature","split":"test","instance":{"id":"106","prompt_labels":"Initially(O) ,(O) after(O) the(O) Roman(O) appeared(O) ,(O) other(O) authors(O) who(O) refer(O) to(O) the(O) story(O) ,(O) for(O) example(O) ,(O) Azalais(O) d(O) 'Altier(O) in(O) her(O) poem(B-literary genre) Tanz(B-poem) salutz(I-poem) e(I-poem) tantas(I-poem) amors(I-poem) and(O) Guido(B-writer) delle(I-writer) Colonne(I-writer) in(O) his(O) Historia(B-poem) destructionis(I-poem) Troiae(I-poem) ,(O) continue(O) to(O) use(O) names(O) derived(O) from(O) that(O) of(O) Briseis(B-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, magazine, poem, writer, person, location, event, book, award, organization, country and O.\nSentence: Initially , after the Roman appeared , other authors who refer to the story , for example , Azalais d 'Altier in her poem Tanz salutz e tantas amors and Guido delle Colonne in his Historia destructionis Troiae , continue to use names derived from that of Briseis .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Initially",",","after","the","Roman","appeared",",","other","authors","who","refer","to","the","story",",","for","example",",","Azalais","d","'Altier","in","her","poem","Tanz","salutz","e","tantas","amors","and","Guido","delle","Colonne","in","his","Historia","destructionis","Troiae",",","continue","to","use","names","derived","from","that","of","Briseis","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-literary genre","B-poem","I-poem","I-poem","I-poem","I-poem","O","B-writer","I-writer","I-writer","O","O","B-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","B-person","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","magazine","poem","writer","person","location","event","book","award","organization","country"]}
{"id":"108","dataset":"crossner_literature","split":"test","instance":{"id":"108","prompt_labels":"Alternate(B-literary genre) history(I-literary genre) shades(O) off(O) into(O) other(O) fantasy(B-literary genre) subgenres(O) when(O) the(O) use(O) of(O) actual(O) ,(O) though(O) altered(O) ,(O) history(O) and(O) geography(O) decreases(O) ,(O) although(O) a(O) culture(O) may(O) still(O) be(O) clearly(O) the(O) original(O) source(O) ;(O) Barry(B-writer) Hughart(I-writer) '(O) s(O) Bridge(B-book) of(I-book) Birds(I-book) and(O) its(O) sequels(O) take(O) place(O) in(O) a(O) fantasy(O) world(O) ,(O) albeit(O) one(O) clearly(O) based(O) on(O) China(B-country) ,(O) and(O) with(O) allusions(O) to(O) actual(O) Chinese(O) history(O) ,(O) such(O) as(O) the(O) Empress(B-person) Wu(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, country, award, organization, writer, literary genre, event, location, poem, magazine, person and O.\nSentence: Alternate history shades off into other fantasy subgenres when the use of actual , though altered , history and geography decreases , although a culture may still be clearly the original source ; Barry Hughart ' s Bridge of Birds and its sequels take place in a fantasy world , albeit one clearly based on China , and with allusions to actual Chinese history , such as the Empress Wu .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Alternate","history","shades","off","into","other","fantasy","subgenres","when","the","use","of","actual",",","though","altered",",","history","and","geography","decreases",",","although","a","culture","may","still","be","clearly","the","original","source",";","Barry","Hughart","'","s","Bridge","of","Birds","and","its","sequels","take","place","in","a","fantasy","world",",","albeit","one","clearly","based","on","China",",","and","with","allusions","to","actual","Chinese","history",",","such","as","the","Empress","Wu","."],"labels":["B-literary genre","I-literary genre","O","O","O","O","B-literary genre","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["book","country","award","organization","writer","literary_genre","event","location","poem","magazine","person"]}
{"id":"109","dataset":"crossner_literature","split":"test","instance":{"id":"109","prompt_labels":"Media(O) coverage(O) of(O) Lewis(B-writer) 's(O) death(O) was(O) almost(O) completely(O) overshadowed(O) by(O) news(O) of(O) the(O) assassination(O) of(O) US(B-country) President(O) John(B-person) F.(I-person) Kennedy(I-person) ,(O) which(O) occurred(O) on(O) the(O) same(O) day(O) ((O) approximately(O) 55(O) minutes(O) following(O) Lewis(B-writer) 's(O) collapse(O) )(O) ,(O) as(O) did(O) the(O) death(O) of(O) English(O) writer(O) Aldous(B-writer) Huxley(I-writer) ,(O) author(O) of(O) Brave(B-book) New(I-book) World(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, magazine, organization, person, award, literary genre, country, writer, poem, location, book and O.\nSentence: Media coverage of Lewis 's death was almost completely overshadowed by news of the assassination of US President John F. Kennedy , which occurred on the same day ( approximately 55 minutes following Lewis 's collapse ) , as did the death of English writer Aldous Huxley , author of Brave New World .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Media","coverage","of","Lewis","'s","death","was","almost","completely","overshadowed","by","news","of","the","assassination","of","US","President","John","F.","Kennedy",",","which","occurred","on","the","same","day","(","approximately","55","minutes","following","Lewis","'s","collapse",")",",","as","did","the","death","of","English","writer","Aldous","Huxley",",","author","of","Brave","New","World","."],"labels":["O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","B-person","I-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","B-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["event","magazine","organization","person","award","literary_genre","country","writer","poem","location","book"]}
{"id":"110","dataset":"crossner_literature","split":"test","instance":{"id":"110","prompt_labels":"He(O) eventually(O) returned(O) to(O) Christianity(O) ,(O) having(O) been(O) influenced(O) by(O) arguments(O) with(O) his(O) Oxford(B-organization) colleague(O) and(O) Christian(O) friend(O) J.(B-writer) R.(I-writer) R.(I-writer) Tolkien(I-writer) ,(O) whom(O) he(O) seems(O) to(O) have(O) met(O) for(O) the(O) first(O) time(O) on(O) 11(O) May(O) 1926(O) ,(O) and(O) the(O) book(O) The(B-book) Everlasting(I-book) Man(I-book) by(O) G.(B-writer) K.(I-writer) Chesterton(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, organization, award, poem, magazine, event, book, person, writer, literary genre and O.\nSentence: He eventually returned to Christianity , having been influenced by arguments with his Oxford colleague and Christian friend J. R. R. Tolkien , whom he seems to have met for the first time on 11 May 1926 , and the book The Everlasting Man by G. K. Chesterton .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","eventually","returned","to","Christianity",",","having","been","influenced","by","arguments","with","his","Oxford","colleague","and","Christian","friend","J.","R.","R.","Tolkien",",","whom","he","seems","to","have","met","for","the","first","time","on","11","May","1926",",","and","the","book","The","Everlasting","Man","by","G.","K.","Chesterton","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","O","O","O","B-writer","I-writer","I-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","O","B-writer","I-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["location","country","organization","award","poem","magazine","event","book","person","writer","literary_genre"]}
{"id":"111","dataset":"crossner_literature","split":"test","instance":{"id":"111","prompt_labels":"Landing(O) in(O) Boston(B-location) ,(O) he(O) devoted(O) the(O) rest(O) of(O) the(O) month(O) to(O) a(O) round(O) of(O) dinners(O) with(O) such(O) notables(O) as(O) Ralph(B-writer) Waldo(I-writer) Emerson(I-writer) ,(O) Henry(B-writer) Wadsworth(I-writer) Longfellow(I-writer) ,(O) and(O) his(O) American(O) publisher(O) ,(O) James(B-writer) Thomas(I-writer) Fields(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, literary genre, book, event, organization, person, country, writer, award, magazine, poem and O.\nSentence: Landing in Boston , he devoted the rest of the month to a round of dinners with such notables as Ralph Waldo Emerson , Henry Wadsworth Longfellow , and his American publisher , James Thomas Fields .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Landing","in","Boston",",","he","devoted","the","rest","of","the","month","to","a","round","of","dinners","with","such","notables","as","Ralph","Waldo","Emerson",",","Henry","Wadsworth","Longfellow",",","and","his","American","publisher",",","James","Thomas","Fields","."],"labels":["O","O","B-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","B-writer","I-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["location","literary_genre","book","event","organization","person","country","writer","award","magazine","poem"]}
{"id":"112","dataset":"crossner_literature","split":"test","instance":{"id":"112","prompt_labels":"She(O) visited(O) Hill(B-location) Top(I-location) at(O) every(O) opportunity(O) ,(O) and(O) her(O) books(O) written(O) during(O) this(O) period(O) ((O) such(O) as(O) The(B-book) Tale(I-book) of(I-book) Ginger(I-book) and(I-book) Pickles(I-book) ,(O) about(O) the(O) local(O) shop(O) in(O) Near(B-location) Sawrey(I-location) and(O) The(B-book) Tale(I-book) of(I-book) Mrs.(I-book) Tittlemouse(I-book) ,(O) a(O) wood(O) mouse(O) )(O) reflect(O) her(O) increasing(O) participation(O) in(O) village(O) life(O) and(O) her(O) delight(O) in(O) country(O) living.Taylor(O) ,(O) ed(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, poem, person, literary genre, magazine, award, event, book, location, organization, writer and O.\nSentence: She visited Hill Top at every opportunity , and her books written during this period ( such as The Tale of Ginger and Pickles , about the local shop in Near Sawrey and The Tale of Mrs. Tittlemouse , a wood mouse ) reflect her increasing participation in village life and her delight in country living.Taylor , ed .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","visited","Hill","Top","at","every","opportunity",",","and","her","books","written","during","this","period","(","such","as","The","Tale","of","Ginger","and","Pickles",",","about","the","local","shop","in","Near","Sawrey","and","The","Tale","of","Mrs.","Tittlemouse",",","a","wood","mouse",")","reflect","her","increasing","participation","in","village","life","and","her","delight","in","country","living.Taylor",",","ed","."],"labels":["O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","B-location","I-location","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","poem","person","literary_genre","magazine","award","event","book","location","organization","writer"]}
{"id":"113","dataset":"crossner_literature","split":"test","instance":{"id":"113","prompt_labels":"His(O) associations(O) were(O) numerous(O) ,(O) including(O) Gustave(B-person) Courbet(I-person) ,(O) Honor(B-person) Daumier(I-person) ,(O) Flicien(B-person) Rops(I-person) ,(O) Franz(B-person) Liszt(I-person) ,(O) Champfleury(B-writer) ,(O) Victor(B-writer) Hugo(I-writer) ,(O) Gustave(B-writer) Flaubert(I-writer) ,(O) and(O) Balzac(B-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, award, organization, poem, book, location, person, country, event, literary genre, writer and O.\nSentence: His associations were numerous , including Gustave Courbet , Honor Daumier , Flicien Rops , Franz Liszt , Champfleury , Victor Hugo , Gustave Flaubert , and Balzac .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","associations","were","numerous",",","including","Gustave","Courbet",",","Honor","Daumier",",","Flicien","Rops",",","Franz","Liszt",",","Champfleury",",","Victor","Hugo",",","Gustave","Flaubert",",","and","Balzac","."],"labels":["O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","O","B-writer","O"],"target_index":null,"target_label":null},"label_list":["magazine","award","organization","poem","book","location","person","country","event","literary_genre","writer"]}
{"id":"114","dataset":"crossner_literature","split":"test","instance":{"id":"114","prompt_labels":"Kingsley(B-writer) Amis(I-writer) set(O) his(O) novel(B-literary genre) ,(O) The(B-book) Alteration(I-book) ((O) 1976(O) )(O) ,(O) in(O) the(O) 20th(O) century(O) ,(O) but(O) major(O) events(O) in(O) the(B-event) Reformation(I-event) did(O) not(O) take(O) place(O) ,(O) and(O) Protestantism(O) is(O) limited(O) to(O) the(O) breakaway(O) Republic(B-country) of(I-country) New(I-country) England(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, book, location, person, award, poem, event, writer, magazine, literary genre, country and O.\nSentence: Kingsley Amis set his novel , The Alteration ( 1976 ) , in the 20th century , but major events in the Reformation did not take place , and Protestantism is limited to the breakaway Republic of New England .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Kingsley","Amis","set","his","novel",",","The","Alteration","(","1976",")",",","in","the","20th","century",",","but","major","events","in","the","Reformation","did","not","take","place",",","and","Protestantism","is","limited","to","the","breakaway","Republic","of","New","England","."],"labels":["B-writer","I-writer","O","O","B-literary genre","O","B-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","I-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["organization","book","location","person","award","poem","event","writer","magazine","literary_genre","country"]}
{"id":"116","dataset":"crossner_literature","split":"test","instance":{"id":"116","prompt_labels":"His(O) film(O) Night(O) Train(O) to(O) Lisbon(O) ((O) 2013(O) )(O) premiered(O) out(O) of(O) competition(O) at(O) the(O) 63rd(B-event) Berlin(I-event) International(I-event) Film(I-event) Festival(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, magazine, award, event, poem, literary genre, country, writer, book, person, organization and O.\nSentence: His film Night Train to Lisbon ( 2013 ) premiered out of competition at the 63rd Berlin International Film Festival .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","film","Night","Train","to","Lisbon","(","2013",")","premiered","out","of","competition","at","the","63rd","Berlin","International","Film","Festival","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["location","magazine","award","event","poem","literary_genre","country","writer","book","person","organization"]}
{"id":"117","dataset":"crossner_literature","split":"test","instance":{"id":"117","prompt_labels":"His(O) most(O) famous(O) works(O) were(O) his(O) longer(O) and(O) more(O) moralistic(O) Troy(B-poem) Book(I-poem) ((O) 1412-20(O) )(O) ,(O) a(O) 30,000(O) line(O) translation(O) of(O) the(O) Latin(B-literary genre) prose(I-literary genre) narrative(O) by(O) Guido(B-writer) delle(I-writer) Colonne(I-writer) ,(O) Historia(B-poem) destructionis(I-poem) Troiae(I-poem) ,(O) the(O) Siege(B-poem) of(I-poem) Thebes(I-poem) which(O) was(O) translated(O) from(O) a(O) French(B-literary genre) prose(I-literary genre) redaction(O) of(O) the(O) Roman(B-poem) de(I-poem) Thebes(I-poem) and(O) the(O) Fall(B-poem) of(I-poem) Princes(I-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, organization, book, literary genre, location, writer, person, award, poem, magazine and O.\nSentence: His most famous works were his longer and more moralistic Troy Book ( 1412-20 ) , a 30,000 line translation of the Latin prose narrative by Guido delle Colonne , Historia destructionis Troiae , the Siege of Thebes which was translated from a French prose redaction of the Roman de Thebes and the Fall of Princes .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","most","famous","works","were","his","longer","and","more","moralistic","Troy","Book","(","1412-20",")",",","a","30,000","line","translation","of","the","Latin","prose","narrative","by","Guido","delle","Colonne",",","Historia","destructionis","Troiae",",","the","Siege","of","Thebes","which","was","translated","from","a","French","prose","redaction","of","the","Roman","de","Thebes","and","the","Fall","of","Princes","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-poem","I-poem","O","O","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","B-writer","I-writer","I-writer","O","B-poem","I-poem","I-poem","O","O","B-poem","I-poem","I-poem","O","O","O","O","O","B-literary genre","I-literary genre","O","O","O","B-poem","I-poem","I-poem","O","O","B-poem","I-poem","I-poem","O"],"target_index":null,"target_label":null},"label_list":["event","country","organization","book","literary_genre","location","writer","person","award","poem","magazine"]}
{"id":"118","dataset":"crossner_literature","split":"test","instance":{"id":"118","prompt_labels":"Nigerian(O) Wole(B-writer) Soyinka(I-writer) was(O) the(O) first(O) African(O) to(O) win(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) in(O) 1986(O) ,(O) and(O) American(O) Toni(B-writer) Morrison(I-writer) was(O) the(O) first(O) black(O) woman(O) to(O) win(O) in(O) 1993(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, book, person, country, magazine, award, writer, literary genre, location, poem, organization and O.\nSentence: Nigerian Wole Soyinka was the first African to win the Nobel Prize in Literature in 1986 , and American Toni Morrison was the first black woman to win in 1993 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Nigerian","Wole","Soyinka","was","the","first","African","to","win","the","Nobel","Prize","in","Literature","in","1986",",","and","American","Toni","Morrison","was","the","first","black","woman","to","win","in","1993","."],"labels":["O","B-writer","I-writer","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","book","person","country","magazine","award","writer","literary_genre","location","poem","organization"]}
{"id":"119","dataset":"crossner_literature","split":"test","instance":{"id":"119","prompt_labels":"His(O) philosophy(O) was(O) expressed(O) in(O) many(O) ways(O) ,(O) but(O) most(O) famously(O) in(O) founding(O) and(O) sustaining(O) the(O) Albert(B-location) Schweitzer(I-location) Hospital(I-location) in(O) Lambarn(B-location) ,(O) which(O) up(O) to(O) 1958(O) was(O) situated(O) in(O) French(B-location) Equatorial(I-location) Africa(I-location) ,(O) and(O) after(O) this(O) in(O) Gabon(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, organization, country, writer, award, book, location, person, literary genre, poem, event and O.\nSentence: His philosophy was expressed in many ways , but most famously in founding and sustaining the Albert Schweitzer Hospital in Lambarn , which up to 1958 was situated in French Equatorial Africa , and after this in Gabon .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","philosophy","was","expressed","in","many","ways",",","but","most","famously","in","founding","and","sustaining","the","Albert","Schweitzer","Hospital","in","Lambarn",",","which","up","to","1958","was","situated","in","French","Equatorial","Africa",",","and","after","this","in","Gabon","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","O","B-location","O","O","O","O","O","O","O","O","B-location","I-location","I-location","O","O","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["magazine","organization","country","writer","award","book","location","person","literary_genre","poem","event"]}
{"id":"120","dataset":"crossner_literature","split":"test","instance":{"id":"120","prompt_labels":"Certain(O) works(O) ,(O) though(O) published(O) legally(O) by(O) the(O) State-controlled(O) media(O) ,(O) were(O) practically(O) impossible(O) to(O) find(O) in(O) bookshops(O) and(O) libraries(O) ,(O) and(O) found(O) their(O) way(O) into(O) samizdat(O) :(O) for(O) example(O) Aleksandr(B-writer) Solzhenitsyn(I-writer) '(O) s(O) novel(B-literary genre) One(B-book) Day(I-book) in(I-book) the(I-book) Life(I-book) of(I-book) Ivan(I-book) Denisovich(I-book) was(O) widely(O) distributed(O) via(O) samizdat(O) .(O) November(O) 1962(O) issue(O) of(O) the(O) Novy(B-magazine) Mir(I-magazine) literary(O) magazine(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, event, country, writer, book, award, organization, person, literary genre, magazine, location and O.\nSentence: Certain works , though published legally by the State-controlled media , were practically impossible to find in bookshops and libraries , and found their way into samizdat : for example Aleksandr Solzhenitsyn ' s novel One Day in the Life of Ivan Denisovich was widely distributed via samizdat . November 1962 issue of the Novy Mir literary magazine","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Certain","works",",","though","published","legally","by","the","State-controlled","media",",","were","practically","impossible","to","find","in","bookshops","and","libraries",",","and","found","their","way","into","samizdat",":","for","example","Aleksandr","Solzhenitsyn","'","s","novel","One","Day","in","the","Life","of","Ivan","Denisovich","was","widely","distributed","via","samizdat",".","November","1962","issue","of","the","Novy","Mir","literary","magazine"],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","B-literary genre","B-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","O","O"],"target_index":null,"target_label":null},"label_list":["poem","event","country","writer","book","award","organization","person","literary_genre","magazine","location"]}
{"id":"121","dataset":"crossner_literature","split":"test","instance":{"id":"121","prompt_labels":"The(O) tradition(O) of(O) regularly(O) assembling(O) bards(O) at(O) an(O) eisteddfod(O) never(O) lapsed(O) ,(O) and(O) was(O) strengthened(O) by(O) formation(O) of(O) the(O) Gorsedd(B-organization) by(O) Iolo(B-writer) Morganwg(I-writer) in(O) 1792(O) ,(O) establishing(O) Wales(B-location) as(O) the(O) major(O) Celtic(O) upholder(O) of(O) bardic(O) tradition(O) in(O) the(O) 21st(O) century(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, literary genre, country, writer, poem, event, organization, magazine, award, location, book and O.\nSentence: The tradition of regularly assembling bards at an eisteddfod never lapsed , and was strengthened by formation of the Gorsedd by Iolo Morganwg in 1792 , establishing Wales as the major Celtic upholder of bardic tradition in the 21st century .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","tradition","of","regularly","assembling","bards","at","an","eisteddfod","never","lapsed",",","and","was","strengthened","by","formation","of","the","Gorsedd","by","Iolo","Morganwg","in","1792",",","establishing","Wales","as","the","major","Celtic","upholder","of","bardic","tradition","in","the","21st","century","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","B-writer","I-writer","O","O","O","O","B-location","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","literary_genre","country","writer","poem","event","organization","magazine","award","location","book"]}
{"id":"122","dataset":"crossner_literature","split":"test","instance":{"id":"122","prompt_labels":"In(O) 1929(O) ,(O) Mann(B-writer) had(O) a(O) cottage(O) built(O) in(O) the(O) fishing(O) village(O) of(O) Nidden(B-location) ,(O) Memel(B-location) Territory(I-location) ((O) now(O) Nida(B-location) ,(O) Lithuania(B-country) )(O) on(O) the(O) Curonian(B-location) Spit(I-location) ,(O) where(O) there(O) was(O) a(O) German(O) art(O) colony(O) and(O) where(O) he(O) spent(O) the(O) summers(O) of(O) 1930-1932(O) working(O) on(O) Joseph(B-book) and(I-book) His(I-book) Brothers(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, book, literary genre, person, country, magazine, location, poem, event, organization, award and O.\nSentence: In 1929 , Mann had a cottage built in the fishing village of Nidden , Memel Territory ( now Nida , Lithuania ) on the Curonian Spit , where there was a German art colony and where he spent the summers of 1930-1932 working on Joseph and His Brothers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1929",",","Mann","had","a","cottage","built","in","the","fishing","village","of","Nidden",",","Memel","Territory","(","now","Nida",",","Lithuania",")","on","the","Curonian","Spit",",","where","there","was","a","German","art","colony","and","where","he","spent","the","summers","of","1930-1932","working","on","Joseph","and","His","Brothers","."],"labels":["O","O","O","B-writer","O","O","O","O","O","O","O","O","O","B-location","O","B-location","I-location","O","O","B-location","O","B-country","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["writer","book","literary_genre","person","country","magazine","location","poem","event","organization","award"]}
{"id":"123","dataset":"crossner_literature","split":"test","instance":{"id":"123","prompt_labels":"However(O) ,(O) in(O) late(O) 1970(O) ,(O) Pauline(B-writer) Kael(I-writer) ,(O) in(O) her(O) negative(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) review(O) of(O) the(O) Maysles(B-person) '(O) subsequent(O) documentary(O) Gimme(O) Shelter(O) ,(O) alleged(O) that(O) Salesman(O) was(O) set(O) up(O) and(O) acted(O) by(O) its(O) principals(O) ,(O) rather(O) than(O) actually(O) being(O) direct(O) cinema(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, book, literary genre, country, writer, person, poem, award, location, organization, magazine and O.\nSentence: However , in late 1970 , Pauline Kael , in her negative The New Yorker review of the Maysles ' subsequent documentary Gimme Shelter , alleged that Salesman was set up and acted by its principals , rather than actually being direct cinema .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["However",",","in","late","1970",",","Pauline","Kael",",","in","her","negative","The","New","Yorker","review","of","the","Maysles","'","subsequent","documentary","Gimme","Shelter",",","alleged","that","Salesman","was","set","up","and","acted","by","its","principals",",","rather","than","actually","being","direct","cinema","."],"labels":["O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","B-magazine","I-magazine","I-magazine","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","book","literary_genre","country","writer","person","poem","award","location","organization","magazine"]}
{"id":"124","dataset":"crossner_literature","split":"test","instance":{"id":"124","prompt_labels":"Hunter(B-writer) S.(I-writer) Thompson(I-writer) wrote(O) a(O) scathing(O) piece(O) denouncing(O) Nixon(B-person) for(O) Rolling(B-magazine) Stone(I-magazine) ,(O) entitled(O) He(O) Was(O) a(O) Crook(O) ((O) which(O) also(O) appeared(O) a(O) month(O) later(O) in(O) The(B-magazine) Atlantic(I-magazine) )(O) .ref(O) name(O) =(O) atlantic(O) In(O) his(O) article(O) ,(O) Thompson(B-writer) described(O) Nixon(B-person) as(O) a(O) political(O) monster(O) straight(O) out(O) of(O) Grendel(O) and(O) a(O) very(O) dangerous(O) enemy(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, magazine, organization, poem, literary genre, country, location, book, writer, award and O.\nSentence: Hunter S. Thompson wrote a scathing piece denouncing Nixon for Rolling Stone , entitled He Was a Crook ( which also appeared a month later in The Atlantic ) .ref name = atlantic In his article , Thompson described Nixon as a political monster straight out of Grendel and a very dangerous enemy .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Hunter","S.","Thompson","wrote","a","scathing","piece","denouncing","Nixon","for","Rolling","Stone",",","entitled","He","Was","a","Crook","(","which","also","appeared","a","month","later","in","The","Atlantic",")",".ref","name","=","atlantic","In","his","article",",","Thompson","described","Nixon","as","a","political","monster","straight","out","of","Grendel","and","a","very","dangerous","enemy","."],"labels":["B-writer","I-writer","I-writer","O","O","O","O","O","B-person","O","B-magazine","I-magazine","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","O","O","O","O","O","O","O","O","O","B-writer","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","person","magazine","organization","poem","literary_genre","country","location","book","writer","award"]}
{"id":"126","dataset":"crossner_literature","split":"test","instance":{"id":"126","prompt_labels":"Philip(B-writer) Lamantia(I-writer) introduced(O) him(O) to(O) other(O) Surrealists(O) and(O) Surrealism(O) continued(O) to(O) be(O) an(O) influence(O) ((O) for(O) example(O) ,(O) sections(O) of(O) Kaddish(O) were(O) inspired(O) by(O) Andr(B-writer) Breton(I-writer) '(O) s(O) Free(B-poem) Union(I-poem) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, magazine, event, location, book, writer, poem, person, literary genre, award and O.\nSentence: Philip Lamantia introduced him to other Surrealists and Surrealism continued to be an influence ( for example , sections of Kaddish were inspired by Andr Breton ' s Free Union ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Philip","Lamantia","introduced","him","to","other","Surrealists","and","Surrealism","continued","to","be","an","influence","(","for","example",",","sections","of","Kaddish","were","inspired","by","Andr","Breton","'","s","Free","Union",")","."],"labels":["B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","B-poem","I-poem","O","O"],"target_index":null,"target_label":null},"label_list":["country","organization","magazine","event","location","book","writer","poem","person","literary_genre","award"]}
{"id":"127","dataset":"crossner_literature","split":"test","instance":{"id":"127","prompt_labels":"Susan(B-writer) Cooper(I-writer) related(O) that(O) The(O) power(O) and(O) range(O) of(O) Alan(B-writer) Garner(I-writer) 's(O) astounding(O) talent(O) has(O) grown(O) with(O) every(O) book(O) he(O) 's(O) written(O) ,(O) whilst(O) David(B-writer) Almond(I-writer) called(O) him(O) one(O) of(O) Britain(B-country) 's(O) greatest(O) writers(O) whose(O) works(O) really(O) matter(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, poem, literary genre, award, country, organization, person, magazine, event, book, writer and O.\nSentence: Susan Cooper related that The power and range of Alan Garner 's astounding talent has grown with every book he 's written , whilst David Almond called him one of Britain 's greatest writers whose works really matter .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Susan","Cooper","related","that","The","power","and","range","of","Alan","Garner","'s","astounding","talent","has","grown","with","every","book","he","'s","written",",","whilst","David","Almond","called","him","one","of","Britain","'s","greatest","writers","whose","works","really","matter","."],"labels":["B-writer","I-writer","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","B-country","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","poem","literary_genre","award","country","organization","person","magazine","event","book","writer"]}
{"id":"128","dataset":"crossner_literature","split":"test","instance":{"id":"128","prompt_labels":"Have(B-book) Space(I-book) Suit(I-book) -(I-book) Will(I-book) Travel(I-book) is(O) a(O) science(B-literary genre) fiction(I-literary genre) novel(I-literary genre) for(O) young(O) readers(O) by(O) American(O) writer(O) Robert(B-writer) A.(I-writer) Heinlein(I-writer) ,(O) originally(O) serialised(O) in(O) The(B-magazine) Magazine(I-magazine) of(I-magazine) Fantasy(I-magazine) &(I-magazine) Science(I-magazine) Fiction(I-magazine) ((O) August(O) ,(O) September(O) ,(O) October(O) 1958(O) )(O) and(O) published(O) by(O) Scribner(B-organization) 's(O) in(O) hardcover(O) in(O) 1958(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, country, event, poem, person, writer, organization, magazine, book, location, literary genre and O.\nSentence: Have Space Suit - Will Travel is a science fiction novel for young readers by American writer Robert A. Heinlein , originally serialised in The Magazine of Fantasy & Science Fiction ( August , September , October 1958 ) and published by Scribner 's in hardcover in 1958 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Have","Space","Suit","-","Will","Travel","is","a","science","fiction","novel","for","young","readers","by","American","writer","Robert","A.","Heinlein",",","originally","serialised","in","The","Magazine","of","Fantasy","&","Science","Fiction","(","August",",","September",",","October","1958",")","and","published","by","Scribner","'s","in","hardcover","in","1958","."],"labels":["B-book","I-book","I-book","I-book","I-book","I-book","O","O","B-literary genre","I-literary genre","I-literary genre","O","O","O","O","O","O","B-writer","I-writer","I-writer","O","O","O","O","B-magazine","I-magazine","I-magazine","I-magazine","I-magazine","I-magazine","I-magazine","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","country","event","poem","person","writer","organization","magazine","book","location","literary_genre"]}
{"id":"129","dataset":"crossner_literature","split":"test","instance":{"id":"129","prompt_labels":"Prahldacharita(B-poem) a(O) Sanskrit(O) work(O) written(O) by(O) Rama(B-writer) Varma(I-writer) Parikshith(I-writer) Thampuran(I-writer) ,(O) former(O) Maharaja(O) of(O) Cochin(B-country) is(O) in(O) Champu(B-literary genre) style(I-literary genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, event, location, book, magazine, award, person, organization, writer, literary genre, country and O.\nSentence: Prahldacharita a Sanskrit work written by Rama Varma Parikshith Thampuran , former Maharaja of Cochin is in Champu style .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Prahldacharita","a","Sanskrit","work","written","by","Rama","Varma","Parikshith","Thampuran",",","former","Maharaja","of","Cochin","is","in","Champu","style","."],"labels":["B-poem","O","O","O","O","O","B-writer","I-writer","I-writer","I-writer","O","O","O","O","B-country","O","O","B-literary genre","I-literary genre","O"],"target_index":null,"target_label":null},"label_list":["poem","event","location","book","magazine","award","person","organization","writer","literary_genre","country"]}
{"id":"130","dataset":"crossner_literature","split":"test","instance":{"id":"130","prompt_labels":"In(O) 1977(O) ,(O) he(O) took(O) various(O) parts(O) in(O) the(O) Joint(B-organization) Stock(I-organization) Theatre(I-organization) Company(I-organization) '(O) s(O) production(O) of(O) Epsom(O) Downs(O) and(O) in(O) 1979(O) ,(O) he(O) starred(O) in(O) Snoo(B-writer) Wilson(I-writer) '(O) s(O) The(B-book) Soul(I-book) of(I-book) the(I-book) White(I-book) Ant(I-book) at(O) the(O) Soho(B-location) Theatre(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, person, location, magazine, writer, poem, literary genre, organization, country, event, award and O.\nSentence: In 1977 , he took various parts in the Joint Stock Theatre Company ' s production of Epsom Downs and in 1979 , he starred in Snoo Wilson ' s The Soul of the White Ant at the Soho Theatre .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1977",",","he","took","various","parts","in","the","Joint","Stock","Theatre","Company","'","s","production","of","Epsom","Downs","and","in","1979",",","he","starred","in","Snoo","Wilson","'","s","The","Soul","of","the","White","Ant","at","the","Soho","Theatre","."],"labels":["O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","B-book","I-book","I-book","I-book","I-book","I-book","O","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["book","person","location","magazine","writer","poem","literary_genre","organization","country","event","award"]}
{"id":"131","dataset":"crossner_literature","split":"test","instance":{"id":"131","prompt_labels":"Ginsberg(B-writer) won(O) a(O) 1974(O) National(B-award) Book(I-award) Award(I-award) for(O) The(B-book) Fall(I-book) of(I-book) America(I-book) ((O) split(O) with(O) Adrienne(B-writer) Rich(I-writer) ,(O) Diving(B-poem) into(I-poem) the(I-poem) Wreck(I-poem) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, organization, literary genre, award, event, poem, magazine, location, writer, book and O.\nSentence: Ginsberg won a 1974 National Book Award for The Fall of America ( split with Adrienne Rich , Diving into the Wreck ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Ginsberg","won","a","1974","National","Book","Award","for","The","Fall","of","America","(","split","with","Adrienne","Rich",",","Diving","into","the","Wreck",")","."],"labels":["B-writer","O","O","O","B-award","I-award","I-award","O","B-book","I-book","I-book","I-book","O","O","O","B-writer","I-writer","O","B-poem","I-poem","I-poem","I-poem","O","O"],"target_index":null,"target_label":null},"label_list":["person","country","organization","literary_genre","award","event","poem","magazine","location","writer","book"]}
{"id":"132","dataset":"crossner_literature","split":"test","instance":{"id":"132","prompt_labels":"In(O) 1992(O) ,(O) he(O) narrated(O) Dr.(B-writer) Seuss(I-writer) Video(O) Classics(O) :(O) Horton(B-book) Hatches(I-book) the(I-book) Egg(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, poem, organization, book, event, writer, location, literary genre, magazine, person and O.\nSentence: In 1992 , he narrated Dr. Seuss Video Classics : Horton Hatches the Egg .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1992",",","he","narrated","Dr.","Seuss","Video","Classics",":","Horton","Hatches","the","Egg","."],"labels":["O","O","O","O","O","B-writer","I-writer","O","O","O","B-book","I-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["country","award","poem","organization","book","event","writer","location","literary_genre","magazine","person"]}
{"id":"133","dataset":"crossner_literature","split":"test","instance":{"id":"133","prompt_labels":"The(O) Samanid(B-country) Empire(I-country) king(O) ,(O) Mansur(B-person) I(I-person) ((O) 961-976(O) )(O) ,(O) ordered(O) a(O) group(O) of(O) scholars(O) from(O) Khorasan(B-location) to(O) translate(O) the(O) Tafsir(B-book) al-Tabari(I-book) ,(O) originally(O) in(O) Arabic(O) ,(O) into(O) Persian(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, writer, country, magazine, organization, literary genre, poem, award, book, event and O.\nSentence: The Samanid Empire king , Mansur I ( 961-976 ) , ordered a group of scholars from Khorasan to translate the Tafsir al-Tabari , originally in Arabic , into Persian .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Samanid","Empire","king",",","Mansur","I","(","961-976",")",",","ordered","a","group","of","scholars","from","Khorasan","to","translate","the","Tafsir","al-Tabari",",","originally","in","Arabic",",","into","Persian","."],"labels":["O","B-country","I-country","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","B-location","O","O","O","B-book","I-book","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","location","writer","country","magazine","organization","literary_genre","poem","award","book","event"]}
{"id":"134","dataset":"crossner_literature","split":"test","instance":{"id":"134","prompt_labels":"The(O) Zabur-e-Ajam(B-book) ((O) Persian(B-book) Psalms(I-book) )(O) ,(O) published(O) in(O) 1927(O) ,(O) includes(O) the(O) poems(B-literary genre) Gulshan-e-Raz-e-Jadeed(B-poem) ((O) Garden(B-poem) of(I-poem) New(I-poem) Secrets(I-poem) )(O) and(O) Bandagi(B-poem) Nama(I-poem) ((O) Book(B-poem) of(I-poem) Slavery(I-poem) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, writer, person, country, magazine, organization, award, book, event, literary genre, location and O.\nSentence: The Zabur-e-Ajam ( Persian Psalms ) , published in 1927 , includes the poems Gulshan-e-Raz-e-Jadeed ( Garden of New Secrets ) and Bandagi Nama ( Book of Slavery ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Zabur-e-Ajam","(","Persian","Psalms",")",",","published","in","1927",",","includes","the","poems","Gulshan-e-Raz-e-Jadeed","(","Garden","of","New","Secrets",")","and","Bandagi","Nama","(","Book","of","Slavery",")","."],"labels":["O","B-book","O","B-book","I-book","O","O","O","O","O","O","O","O","B-literary genre","B-poem","O","B-poem","I-poem","I-poem","I-poem","O","O","B-poem","I-poem","O","B-poem","I-poem","I-poem","O","O"],"target_index":null,"target_label":null},"label_list":["poem","writer","person","country","magazine","organization","award","book","event","literary_genre","location"]}
{"id":"135","dataset":"crossner_literature","split":"test","instance":{"id":"135","prompt_labels":"His(O) books(O) Difference(B-book) and(I-book) Repetition(I-book) ((O) 1968(O) )(O) and(O) The(B-book) Logic(I-book) of(I-book) Sense(I-book) ((O) 1969(O) )(O) led(O) Michel(B-writer) Foucault(I-writer) to(O) declare(O) that(O) one(O) day(O) ,(O) perhaps(O) ,(O) this(O) century(O) will(O) be(O) called(O) Deleuzian(B-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, person, award, writer, literary genre, event, book, location, magazine, country, organization and O.\nSentence: His books Difference and Repetition ( 1968 ) and The Logic of Sense ( 1969 ) led Michel Foucault to declare that one day , perhaps , this century will be called Deleuzian .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","books","Difference","and","Repetition","(","1968",")","and","The","Logic","of","Sense","(","1969",")","led","Michel","Foucault","to","declare","that","one","day",",","perhaps",",","this","century","will","be","called","Deleuzian","."],"labels":["O","O","B-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","O"],"target_index":null,"target_label":null},"label_list":["poem","person","award","writer","literary_genre","event","book","location","magazine","country","organization"]}
{"id":"136","dataset":"crossner_literature","split":"test","instance":{"id":"136","prompt_labels":"Paradoxically(O) ,(O) one(O) of(O) his(O) most(O) famous(O) works(O) ,(O) a(O) book(O) called(O) Safahat(B-poem) ,(O) was(O) not(O) widely(O) read(O) or(O) published(O) until(O) recently(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, book, award, location, organization, country, person, literary genre, event, writer, magazine and O.\nSentence: Paradoxically , one of his most famous works , a book called Safahat , was not widely read or published until recently .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Paradoxically",",","one","of","his","most","famous","works",",","a","book","called","Safahat",",","was","not","widely","read","or","published","until","recently","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-poem","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","book","award","location","organization","country","person","literary_genre","event","writer","magazine"]}
{"id":"138","dataset":"crossner_literature","split":"test","instance":{"id":"138","prompt_labels":"As(O) well(O) ,(O) several(O) ((O) though(O) not(O) all(O) )(O) of(O) the(O) stories(O) that(O) were(O) compiled(O) to(O) make(O) up(O) the(O) novels(B-literary genre) The(B-book) Weapon(I-book) Shops(I-book) of(I-book) Isher(I-book) ,(O) The(B-book) Mixed(I-book) Men(I-book) and(O) The(B-book) War(I-book) Against(I-book) the(I-book) Rull(I-book) were(O) also(O) published(O) during(O) this(O) time(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, literary genre, poem, person, magazine, organization, writer, award, country, book, location and O.\nSentence: As well , several ( though not all ) of the stories that were compiled to make up the novels The Weapon Shops of Isher , The Mixed Men and The War Against the Rull were also published during this time .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["As","well",",","several","(","though","not","all",")","of","the","stories","that","were","compiled","to","make","up","the","novels","The","Weapon","Shops","of","Isher",",","The","Mixed","Men","and","The","War","Against","the","Rull","were","also","published","during","this","time","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-literary genre","B-book","I-book","I-book","I-book","I-book","O","B-book","I-book","I-book","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","literary_genre","poem","person","magazine","organization","writer","award","country","book","location"]}
{"id":"141","dataset":"crossner_literature","split":"test","instance":{"id":"141","prompt_labels":"Alan(B-writer) Moore(I-writer) had(O) this(O) to(O) say(O) about(O) the(O) use(O) of(O) the(O) Guy(B-person) Fawkes(I-person) motif(O) adopted(O) from(O) his(O) comic(O) V(B-book) for(I-book) Vendetta(I-book) ,(O) in(O) an(O) interview(O) with(O) Entertainment(B-magazine) Weekly(I-magazine) :(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, literary genre, magazine, book, event, award, poem, country, writer, person, organization and O.\nSentence: Alan Moore had this to say about the use of the Guy Fawkes motif adopted from his comic V for Vendetta , in an interview with Entertainment Weekly :","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Alan","Moore","had","this","to","say","about","the","use","of","the","Guy","Fawkes","motif","adopted","from","his","comic","V","for","Vendetta",",","in","an","interview","with","Entertainment","Weekly",":"],"labels":["B-writer","I-writer","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","B-magazine","I-magazine","O"],"target_index":null,"target_label":null},"label_list":["location","literary_genre","magazine","book","event","award","poem","country","writer","person","organization"]}
{"id":"142","dataset":"crossner_literature","split":"test","instance":{"id":"142","prompt_labels":"A(O) few(O) of(O) Hardy(B-writer) 's(O) poems(B-literary genre) ,(O) such(O) as(O) The(B-poem) Blinded(I-poem) Bird(I-poem) ,(O) a(O) melancholy(O) polemic(O) against(O) the(O) sport(O) of(O) ,(O) reflect(O) his(O) firm(O) stance(O) against(O) animal(O) cruelty(O) ,(O) exhibited(O) in(O) his(O) antivivisectionist(O) views(O) and(O) his(O) membership(O) in(O) The(O) Royal(B-organization) Society(I-organization) for(I-organization) the(I-organization) Prevention(I-organization) of(I-organization) Cruelty(I-organization) to(I-organization) Animals(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, magazine, event, award, location, book, poem, person, literary genre, country, writer and O.\nSentence: A few of Hardy 's poems , such as The Blinded Bird , a melancholy polemic against the sport of , reflect his firm stance against animal cruelty , exhibited in his antivivisectionist views and his membership in The Royal Society for the Prevention of Cruelty to Animals .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","few","of","Hardy","'s","poems",",","such","as","The","Blinded","Bird",",","a","melancholy","polemic","against","the","sport","of",",","reflect","his","firm","stance","against","animal","cruelty",",","exhibited","in","his","antivivisectionist","views","and","his","membership","in","The","Royal","Society","for","the","Prevention","of","Cruelty","to","Animals","."],"labels":["O","O","O","B-writer","O","B-literary genre","O","O","O","B-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["organization","magazine","event","award","location","book","poem","person","literary_genre","country","writer"]}
{"id":"143","dataset":"crossner_literature","split":"test","instance":{"id":"143","prompt_labels":"Aleksandr(B-writer) Solzhenitsyn(I-writer) ,(O) the(O) Russian(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) -winning(O) anti-Communist(O) author(O) of(O) The(B-book) Gulag(I-book) Archipelago(I-book) ,(O) argued(O) that(O) Soviet(O) communism(O) needed(O) enslavement(O) and(O) forced(O) labour(O) to(O) survive(O) ,(O) and(O) that(O) this(O) had(O) been(O) ...(O) foreseen(O) as(O) far(O) back(O) as(O) Thomas(O) More(O) ,(O) in(O) his(O) Utopia(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, magazine, location, book, writer, poem, organization, award, person, literary genre and O.\nSentence: Aleksandr Solzhenitsyn , the Russian Nobel Prize in Literature -winning anti-Communist author of The Gulag Archipelago , argued that Soviet communism needed enslavement and forced labour to survive , and that this had been ... foreseen as far back as Thomas More , in his Utopia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Aleksandr","Solzhenitsyn",",","the","Russian","Nobel","Prize","in","Literature","-winning","anti-Communist","author","of","The","Gulag","Archipelago",",","argued","that","Soviet","communism","needed","enslavement","and","forced","labour","to","survive",",","and","that","this","had","been","...","foreseen","as","far","back","as","Thomas","More",",","in","his","Utopia","."],"labels":["B-writer","I-writer","O","O","O","B-award","I-award","I-award","I-award","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","event","magazine","location","book","writer","poem","organization","award","person","literary_genre"]}
{"id":"144","dataset":"crossner_literature","split":"test","instance":{"id":"144","prompt_labels":"He(O) won(O) three(O) Pulitzer(B-award) Prizes(I-award) -(O) for(O) the(O) novel(B-literary genre) The(B-book) Bridge(I-book) of(I-book) San(I-book) Luis(I-book) Rey(I-book) ,(O) and(O) for(O) the(O) plays(O) Our(B-book) Town(I-book) and(O) The(B-book) Skin(I-book) of(I-book) Our(I-book) Teeth(I-book) -(O) and(O) a(O) U.S.(B-award) National(I-award) Book(I-award) Award(I-award) for(O) the(O) novel(B-literary genre) The(B-book) Eighth(I-book) Day(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, event, writer, location, poem, country, award, magazine, book, organization, person and O.\nSentence: He won three Pulitzer Prizes - for the novel The Bridge of San Luis Rey , and for the plays Our Town and The Skin of Our Teeth - and a U.S. National Book Award for the novel The Eighth Day .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","won","three","Pulitzer","Prizes","-","for","the","novel","The","Bridge","of","San","Luis","Rey",",","and","for","the","plays","Our","Town","and","The","Skin","of","Our","Teeth","-","and","a","U.S.","National","Book","Award","for","the","novel","The","Eighth","Day","."],"labels":["O","O","O","B-award","I-award","O","O","O","B-literary genre","B-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","O","B-book","I-book","O","B-book","I-book","I-book","I-book","I-book","O","O","O","B-award","I-award","I-award","I-award","O","O","B-literary genre","B-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","event","writer","location","poem","country","award","magazine","book","organization","person"]}
{"id":"145","dataset":"crossner_literature","split":"test","instance":{"id":"145","prompt_labels":"During(O) this(O) period(O) ,(O) Stoker(B-writer) was(O) part(O) of(O) the(O) literary(O) staff(O) of(O) The(B-organization) Daily(I-organization) Telegraph(I-organization) in(O) London(B-location) ,(O) and(O) he(O) wrote(O) other(O) fiction(O) ,(O) including(O) the(O) horror(B-literary genre) novels(I-literary genre) The(B-book) Lady(I-book) of(I-book) the(I-book) Shroud(I-book) ((O) 1909(O) )(O) and(O) The(B-book) Lair(I-book) of(I-book) the(I-book) White(I-book) Worm(I-book) ((O) 1911(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, writer, person, magazine, book, literary genre, poem, location, organization, award and O.\nSentence: During this period , Stoker was part of the literary staff of The Daily Telegraph in London , and he wrote other fiction , including the horror novels The Lady of the Shroud ( 1909 ) and The Lair of the White Worm ( 1911 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","this","period",",","Stoker","was","part","of","the","literary","staff","of","The","Daily","Telegraph","in","London",",","and","he","wrote","other","fiction",",","including","the","horror","novels","The","Lady","of","the","Shroud","(","1909",")","and","The","Lair","of","the","White","Worm","(","1911",")","."],"labels":["O","O","O","O","B-writer","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-location","O","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","event","writer","person","magazine","book","literary_genre","poem","location","organization","award"]}
{"id":"146","dataset":"crossner_literature","split":"test","instance":{"id":"146","prompt_labels":"On(O) 22(O) December(O) Gabriela(B-writer) Mistral(I-writer) ,(O) who(O) took(O) her(O) pen(O) name(O) from(O) Occitan(O) poet(O) Frdric(B-writer) Mistral(I-writer) ,(O) won(O) top(O) prize(O) for(O) her(O) Sonetos(B-poem) de(I-poem) la(I-poem) Muerte(I-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, country, book, person, event, poem, magazine, location, organization, award, literary genre and O.\nSentence: On 22 December Gabriela Mistral , who took her pen name from Occitan poet Frdric Mistral , won top prize for her Sonetos de la Muerte .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","22","December","Gabriela","Mistral",",","who","took","her","pen","name","from","Occitan","poet","Frdric","Mistral",",","won","top","prize","for","her","Sonetos","de","la","Muerte","."],"labels":["O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","B-poem","I-poem","I-poem","I-poem","O"],"target_index":null,"target_label":null},"label_list":["writer","country","book","person","event","poem","magazine","location","organization","award","literary_genre"]}
{"id":"147","dataset":"crossner_literature","split":"test","instance":{"id":"147","prompt_labels":"After(O) his(O) death(O) ,(O) some(O) of(O) his(O) poems(O) ,(O) notably(O) Funeral(B-poem) Blues(I-poem) ,(O) Muse(B-poem) des(I-poem) Beaux(I-poem) Arts(I-poem) ,(O) Refugee(B-poem) Blues(I-poem) ,(O) The(B-poem) Unknown(I-poem) Citizen(I-poem) ,(O) and(O) September(B-poem) 1(I-poem) ,(I-poem) 1939(I-poem) ,(O) became(O) known(O) to(O) a(O) much(O) wider(O) public(O) than(O) during(O) his(O) lifetime(O) through(O) films(O) ,(O) broadcasts(O) ,(O) and(O) popular(O) media(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, country, event, book, literary genre, location, organization, person, writer, award, poem and O.\nSentence: After his death , some of his poems , notably Funeral Blues , Muse des Beaux Arts , Refugee Blues , The Unknown Citizen , and September 1 , 1939 , became known to a much wider public than during his lifetime through films , broadcasts , and popular media .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","his","death",",","some","of","his","poems",",","notably","Funeral","Blues",",","Muse","des","Beaux","Arts",",","Refugee","Blues",",","The","Unknown","Citizen",",","and","September","1",",","1939",",","became","known","to","a","much","wider","public","than","during","his","lifetime","through","films",",","broadcasts",",","and","popular","media","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-poem","I-poem","O","B-poem","I-poem","I-poem","I-poem","O","B-poem","I-poem","O","B-poem","I-poem","I-poem","O","O","B-poem","I-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","country","event","book","literary_genre","location","organization","person","writer","award","poem"]}
{"id":"148","dataset":"crossner_literature","split":"test","instance":{"id":"148","prompt_labels":"All(B-book) the(I-book) Light(I-book) We(I-book) Cannot(I-book) See(I-book) ,(O) a(O) Pulitzer(B-award) prize(I-award) winning(O) novel(B-literary genre) by(O) Anthony(B-writer) Doerr(I-writer) ,(O) tells(O) the(O) story(O) of(O) Marie-Laure(B-person) LeBlanc(I-person) ,(O) a(O) young(O) girl(O) who(O) has(O) gone(O) completely(O) blind(O) due(O) to(O) cataract(O) s(O) at(O) the(O) age(O) of(O) 6(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, event, person, literary genre, poem, location, writer, award, country, magazine, organization and O.\nSentence: All the Light We Cannot See , a Pulitzer prize winning novel by Anthony Doerr , tells the story of Marie-Laure LeBlanc , a young girl who has gone completely blind due to cataract s at the age of 6 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["All","the","Light","We","Cannot","See",",","a","Pulitzer","prize","winning","novel","by","Anthony","Doerr",",","tells","the","story","of","Marie-Laure","LeBlanc",",","a","young","girl","who","has","gone","completely","blind","due","to","cataract","s","at","the","age","of","6","."],"labels":["B-book","I-book","I-book","I-book","I-book","I-book","O","O","B-award","I-award","O","B-literary genre","O","B-writer","I-writer","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["book","event","person","literary_genre","poem","location","writer","award","country","magazine","organization"]}
{"id":"149","dataset":"crossner_literature","split":"test","instance":{"id":"149","prompt_labels":"In(O) April(O) he(O) chaired(O) the(O) South(B-organization) Side(I-organization) Writers(I-organization) Group(I-organization) ,(O) whose(O) members(O) included(O) Arna(B-writer) Bontemps(I-writer) and(O) Margaret(B-writer) Walker(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, poem, country, writer, literary genre, award, magazine, book, event, location, organization and O.\nSentence: In April he chaired the South Side Writers Group , whose members included Arna Bontemps and Margaret Walker .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","April","he","chaired","the","South","Side","Writers","Group",",","whose","members","included","Arna","Bontemps","and","Margaret","Walker","."],"labels":["O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["person","poem","country","writer","literary_genre","award","magazine","book","event","location","organization"]}
{"id":"150","dataset":"crossner_literature","split":"test","instance":{"id":"150","prompt_labels":"A(O) final(O) folktale(B-literary genre) ,(O) Wag(B-book) by(I-book) Wall(I-book) ,(O) was(O) published(O) posthumously(O) by(O) The(B-magazine) Horn(I-magazine) Book(I-magazine) Magazine(I-magazine) in(O) 1944(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, writer, poem, event, magazine, location, country, award, book, person, literary genre and O.\nSentence: A final folktale , Wag by Wall , was published posthumously by The Horn Book Magazine in 1944 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","final","folktale",",","Wag","by","Wall",",","was","published","posthumously","by","The","Horn","Book","Magazine","in","1944","."],"labels":["O","O","B-literary genre","O","B-book","I-book","I-book","O","O","O","O","O","B-magazine","I-magazine","I-magazine","I-magazine","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","writer","poem","event","magazine","location","country","award","book","person","literary_genre"]}
{"id":"151","dataset":"crossner_literature","split":"test","instance":{"id":"151","prompt_labels":"Novels(B-literary genre) such(O) as(O) Kipps(B-book) and(O) The(B-book) History(I-book) of(I-book) Mr(I-book) Polly(I-book) ,(O) which(O) describe(O) lower-middle-class(O) life(O) ,(O) led(O) to(O) the(O) suggestion(O) that(O) he(O) was(O) a(O) worthy(O) successor(O) to(O) Charles(B-writer) Dickens(I-writer) ,(O) Vincent(B-writer) Brome(I-writer) ,(O) H.(B-book) G.(I-book) Wells(I-book) :(I-book) A(I-book) Biography(I-book) ((O) London(B-location) ,(O) New(B-location) York(I-location) ,(O) and(O) Toronto(B-location) :(O) Longmans(B-organization) ,(I-organization) Green(I-organization) ,(O) 1951(O) )(O) ,(O) p(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, writer, magazine, literary genre, country, award, book, organization, location, person, event and O.\nSentence: Novels such as Kipps and The History of Mr Polly , which describe lower-middle-class life , led to the suggestion that he was a worthy successor to Charles Dickens , Vincent Brome , H. G. Wells : A Biography ( London , New York , and Toronto : Longmans , Green , 1951 ) , p .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Novels","such","as","Kipps","and","The","History","of","Mr","Polly",",","which","describe","lower-middle-class","life",",","led","to","the","suggestion","that","he","was","a","worthy","successor","to","Charles","Dickens",",","Vincent","Brome",",","H.","G.","Wells",":","A","Biography","(","London",",","New","York",",","and","Toronto",":","Longmans",",","Green",",","1951",")",",","p","."],"labels":["B-literary genre","O","O","B-book","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-book","I-book","I-book","I-book","I-book","I-book","O","B-location","O","B-location","I-location","O","O","B-location","O","B-organization","I-organization","I-organization","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","writer","magazine","literary_genre","country","award","book","organization","location","person","event"]}
{"id":"152","dataset":"crossner_literature","split":"test","instance":{"id":"152","prompt_labels":"During(O) this(O) time(O) ,(O) he(O) made(O) the(O) short(O) film(O) I(B-poem) Am(I-poem) Joaquin(I-poem) based(O) on(O) the(O) legendary(O) poem(B-literary genre) by(O) Rodolfo(B-writer) Corky(I-writer) Gonzles(I-writer) ((O) it(O) was(O) later(O) inducted(O) into(O) the(O) National(B-organization) Film(I-organization) Registry(I-organization) in(O) 2010(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, award, location, magazine, event, poem, country, person, book, literary genre, organization and O.\nSentence: During this time , he made the short film I Am Joaquin based on the legendary poem by Rodolfo Corky Gonzles ( it was later inducted into the National Film Registry in 2010 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","this","time",",","he","made","the","short","film","I","Am","Joaquin","based","on","the","legendary","poem","by","Rodolfo","Corky","Gonzles","(","it","was","later","inducted","into","the","National","Film","Registry","in","2010",")","."],"labels":["O","O","O","O","O","O","O","O","O","B-poem","I-poem","I-poem","O","O","O","O","B-literary genre","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["writer","award","location","magazine","event","poem","country","person","book","literary_genre","organization"]}
{"id":"155","dataset":"crossner_literature","split":"test","instance":{"id":"155","prompt_labels":"In(O) the(O) mid-1970s(O) ,(O) Pohl(B-writer) acquired(O) and(O) edited(O) novels(B-literary genre) for(O) Bantam(B-organization) Books(I-organization) ,(O) published(O) as(O) Frederik(B-writer) Pohl(I-writer) Selections(O) ;(O) these(O) included(O) Samuel(B-writer) R.(I-writer) Delany(I-writer) '(O) s(O) Dhalgren(B-book) and(O) Joanna(B-writer) Russ(I-writer) '(O) s(O) The(B-book) Female(I-book) Man(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, organization, writer, person, literary genre, location, magazine, poem, award, event, country and O.\nSentence: In the mid-1970s , Pohl acquired and edited novels for Bantam Books , published as Frederik Pohl Selections ; these included Samuel R. Delany ' s Dhalgren and Joanna Russ ' s The Female Man .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","mid-1970s",",","Pohl","acquired","and","edited","novels","for","Bantam","Books",",","published","as","Frederik","Pohl","Selections",";","these","included","Samuel","R.","Delany","'","s","Dhalgren","and","Joanna","Russ","'","s","The","Female","Man","."],"labels":["O","O","O","O","B-writer","O","O","O","B-literary genre","O","B-organization","I-organization","O","O","O","B-writer","I-writer","O","O","O","O","B-writer","I-writer","I-writer","O","O","B-book","O","B-writer","I-writer","O","O","B-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["book","organization","writer","person","literary_genre","location","magazine","poem","award","event","country"]}
{"id":"157","dataset":"crossner_literature","split":"test","instance":{"id":"157","prompt_labels":"Faulkner(B-writer) was(O) awarded(O) two(O) Pulitzer(B-award) Prize(I-award) s(O) for(O) what(O) are(O) considered(O) minor(O) novels(B-literary genre) :(O) his(O) 1954(O) novel(B-literary genre) A(B-book) Fable(I-book) ,(O) which(O) took(O) the(O) Pulitzer(B-award) in(O) 1955(O) ,(O) and(O) the(O) 1962(O) novel(B-literary genre) ,(O) The(B-book) Reivers(I-book) ,(O) which(O) was(O) posthumously(O) awarded(O) the(O) Pulitzer(B-award) in(O) 1963(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, writer, country, book, event, award, magazine, organization, poem, literary genre and O.\nSentence: Faulkner was awarded two Pulitzer Prize s for what are considered minor novels : his 1954 novel A Fable , which took the Pulitzer in 1955 , and the 1962 novel , The Reivers , which was posthumously awarded the Pulitzer in 1963 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Faulkner","was","awarded","two","Pulitzer","Prize","s","for","what","are","considered","minor","novels",":","his","1954","novel","A","Fable",",","which","took","the","Pulitzer","in","1955",",","and","the","1962","novel",",","The","Reivers",",","which","was","posthumously","awarded","the","Pulitzer","in","1963","."],"labels":["B-writer","O","O","O","B-award","I-award","O","O","O","O","O","O","B-literary genre","O","O","O","B-literary genre","B-book","I-book","O","O","O","O","B-award","O","O","O","O","O","O","B-literary genre","O","B-book","I-book","O","O","O","O","O","O","B-award","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","person","writer","country","book","event","award","magazine","organization","poem","literary_genre"]}
{"id":"158","dataset":"crossner_literature","split":"test","instance":{"id":"158","prompt_labels":"He(O) has(O) received(O) five(O) Robert(B-award) Awards(I-award) ((O) including(O) Best(B-award) Film(I-award) and(O) Best(B-award) Director(I-award) )(O) and(O) three(O) Bodil(B-award) Awards(I-award) for(O) Best(B-award) Danish(I-award) Film(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, event, person, writer, organization, literary genre, poem, magazine, award, book and O.\nSentence: He has received five Robert Awards ( including Best Film and Best Director ) and three Bodil Awards for Best Danish Film .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","has","received","five","Robert","Awards","(","including","Best","Film","and","Best","Director",")","and","three","Bodil","Awards","for","Best","Danish","Film","."],"labels":["O","O","O","O","B-award","I-award","O","O","B-award","I-award","O","B-award","I-award","O","O","O","B-award","I-award","O","B-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["location","country","event","person","writer","organization","literary_genre","poem","magazine","award","book"]}
{"id":"159","dataset":"crossner_literature","split":"test","instance":{"id":"159","prompt_labels":"The(O) film(O) was(O) overwhelmingly(O) lauded(O) by(O) critics(O) when(O) it(O) finally(O) appeared(O) in(O) 1979(O) and(O) was(O) selected(O) for(O) the(O) 1979(O) Cannes(B-event) Film(I-event) Festival(I-event) ,(O) winning(O) the(O) Palme(B-award) d(I-award) 'Or(I-award) along(O) with(O) The(O) Tin(O) Drum(O) ,(O) directed(O) by(O) Volker(B-person) Schlndorff(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, book, person, award, location, literary genre, magazine, poem, country, writer, organization and O.\nSentence: The film was overwhelmingly lauded by critics when it finally appeared in 1979 and was selected for the 1979 Cannes Film Festival , winning the Palme d 'Or along with The Tin Drum , directed by Volker Schlndorff .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","film","was","overwhelmingly","lauded","by","critics","when","it","finally","appeared","in","1979","and","was","selected","for","the","1979","Cannes","Film","Festival",",","winning","the","Palme","d","'Or","along","with","The","Tin","Drum",",","directed","by","Volker","Schlndorff","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","B-award","I-award","I-award","O","O","O","O","O","O","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["event","book","person","award","location","literary_genre","magazine","poem","country","writer","organization"]}
{"id":"162","dataset":"crossner_literature","split":"test","instance":{"id":"162","prompt_labels":"Moving(O) to(O) London(B-location) in(O) 1945(O) ,(O) he(O) became(O) intent(O) on(O) propagating(O) this(O) religion(O) ,(O) attracting(O) media(O) attention(O) and(O) writing(O) about(O) it(O) in(O) High(B-book) Magic(I-book) 's(I-book) Aid(I-book) ((O) 1949(O) )(O) ,(O) Witchcraft(B-book) Today(I-book) ((O) 1954(O) )(O) and(O) The(B-book) Meaning(I-book) of(I-book) Witchcraft(I-book) ((O) 1959(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, event, award, person, organization, literary genre, location, magazine, poem, country, book and O.\nSentence: Moving to London in 1945 , he became intent on propagating this religion , attracting media attention and writing about it in High Magic 's Aid ( 1949 ) , Witchcraft Today ( 1954 ) and The Meaning of Witchcraft ( 1959 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Moving","to","London","in","1945",",","he","became","intent","on","propagating","this","religion",",","attracting","media","attention","and","writing","about","it","in","High","Magic","'s","Aid","(","1949",")",",","Witchcraft","Today","(","1954",")","and","The","Meaning","of","Witchcraft","(","1959",")","."],"labels":["O","O","B-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["writer","event","award","person","organization","literary_genre","location","magazine","poem","country","book"]}
{"id":"164","dataset":"crossner_literature","split":"test","instance":{"id":"164","prompt_labels":"From(O) Adam(B-book) Bede(I-book) to(O) The(B-book) Mill(I-book) on(I-book) the(I-book) Floss(I-book) and(O) Silas(B-book) Marner(I-book) ,(O) Eliot(B-writer) presented(O) the(O) cases(O) of(O) social(O) outsiders(O) and(O) small-town(O) persecution(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, poem, organization, event, book, literary genre, person, magazine, location, country, writer and O.\nSentence: From Adam Bede to The Mill on the Floss and Silas Marner , Eliot presented the cases of social outsiders and small-town persecution .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["From","Adam","Bede","to","The","Mill","on","the","Floss","and","Silas","Marner",",","Eliot","presented","the","cases","of","social","outsiders","and","small-town","persecution","."],"labels":["O","B-book","I-book","O","B-book","I-book","I-book","I-book","I-book","O","B-book","I-book","O","B-writer","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","poem","organization","event","book","literary_genre","person","magazine","location","country","writer"]}
{"id":"165","dataset":"crossner_literature","split":"test","instance":{"id":"165","prompt_labels":"His(O) 1974(O) short(B-literary genre) story(I-literary genre) The(B-book) Four-Hour(I-book) Fugue(I-book) was(O) nominated(O) for(O) a(O) Hugo(B-award) Award(I-award) ,(O) and(O) Nebula(B-award) Award(I-award) nominations(O) for(O) his(O) 1975(O) novel(B-literary genre) The(B-book) Computer(I-book) Connection(I-book) ((O) titled(O) The(B-book) Indian(I-book) Giver(I-book) as(O) a(O) magazine(O) serial(O) and(O) later(O) reprinted(O) as(O) Extro(B-book) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, poem, book, person, writer, event, magazine, organization, location, award, country and O.\nSentence: His 1974 short story The Four-Hour Fugue was nominated for a Hugo Award , and Nebula Award nominations for his 1975 novel The Computer Connection ( titled The Indian Giver as a magazine serial and later reprinted as Extro ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","1974","short","story","The","Four-Hour","Fugue","was","nominated","for","a","Hugo","Award",",","and","Nebula","Award","nominations","for","his","1975","novel","The","Computer","Connection","(","titled","The","Indian","Giver","as","a","magazine","serial","and","later","reprinted","as","Extro",")","."],"labels":["O","O","B-literary genre","I-literary genre","B-book","I-book","I-book","O","O","O","O","B-award","I-award","O","O","B-award","I-award","O","O","O","O","B-literary genre","B-book","I-book","I-book","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","B-book","O","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","poem","book","person","writer","event","magazine","organization","location","award","country"]}
{"id":"166","dataset":"crossner_literature","split":"test","instance":{"id":"166","prompt_labels":"While(O) at(O) Columbia(B-location) ,(O) Ginsberg(B-writer) contributed(O) to(O) the(O) Columbia(B-magazine) Review(I-magazine) literary(O) journal(O) ,(O) the(O) Jester(B-magazine) of(I-magazine) Columbia(I-magazine) humor(O) magazine(O) ,(O) won(O) the(O) Woodberry(B-award) Poetry(I-award) Prize(I-award) ,(O) served(O) as(O) president(O) of(O) the(O) Philolexian(B-organization) Society(I-organization) ((O) literary(O) and(O) debate(O) group(O) )(O) ,(O) and(O) joined(O) Boar(B-organization) 's(I-organization) Head(I-organization) Society(I-organization) ((O) poetry(B-organization) society(I-organization) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, literary genre, writer, book, organization, location, person, magazine, award, poem, event and O.\nSentence: While at Columbia , Ginsberg contributed to the Columbia Review literary journal , the Jester of Columbia humor magazine , won the Woodberry Poetry Prize , served as president of the Philolexian Society ( literary and debate group ) , and joined Boar 's Head Society ( poetry society ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["While","at","Columbia",",","Ginsberg","contributed","to","the","Columbia","Review","literary","journal",",","the","Jester","of","Columbia","humor","magazine",",","won","the","Woodberry","Poetry","Prize",",","served","as","president","of","the","Philolexian","Society","(","literary","and","debate","group",")",",","and","joined","Boar","'s","Head","Society","(","poetry","society",")","."],"labels":["O","O","B-location","O","B-writer","O","O","O","B-magazine","I-magazine","O","O","O","O","B-magazine","I-magazine","I-magazine","O","O","O","O","O","B-award","I-award","I-award","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","O","O"],"target_index":null,"target_label":null},"label_list":["country","literary_genre","writer","book","organization","location","person","magazine","award","poem","event"]}
{"id":"167","dataset":"crossner_literature","split":"test","instance":{"id":"167","prompt_labels":"Cuarn(B-writer) shared(O) an(O) Academy(B-award) Awards(I-award) nomination(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Screenplay(I-award) with(O) co-writer(O) and(O) brother(O) Carlos(B-writer) Cuarn(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, country, award, poem, writer, person, book, magazine, location, literary genre and O.\nSentence: Cuarn shared an Academy Awards nomination for Academy Award for Best Original Screenplay with co-writer and brother Carlos Cuarn .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Cuarn","shared","an","Academy","Awards","nomination","for","Academy","Award","for","Best","Original","Screenplay","with","co-writer","and","brother","Carlos","Cuarn","."],"labels":["B-writer","O","O","B-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["organization","event","country","award","poem","writer","person","book","magazine","location","literary_genre"]}
{"id":"168","dataset":"crossner_literature","split":"test","instance":{"id":"168","prompt_labels":"He(O) has(O) directed(O) film(O) versions(O) of(O) his(O) plays(O) The(B-book) Grey(I-book) Zone(I-book) and(O) Eye(B-book) of(I-book) God(I-book) ((O) for(O) which(O) he(O) received(O) an(O) Independent(B-award) Spirit(I-award) Awards(I-award) nomination(O) for(O) the(O) Someone(B-award) to(I-award) Watch(I-award) Award(I-award) )(O) ,(O) as(O) well(O) as(O) writing(O) and(O) directing(O) two(O) original(O) screenplays(O) :(O) 1998(B-book) 's(I-book) Kansas(I-book) and(O) Leaves(B-book) of(I-book) Grass(I-book) ,(O) which(O) was(O) released(O) in(O) 2009(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, magazine, literary genre, book, person, location, organization, writer, poem, event and O.\nSentence: He has directed film versions of his plays The Grey Zone and Eye of God ( for which he received an Independent Spirit Awards nomination for the Someone to Watch Award ) , as well as writing and directing two original screenplays : 1998 's Kansas and Leaves of Grass , which was released in 2009 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","has","directed","film","versions","of","his","plays","The","Grey","Zone","and","Eye","of","God","(","for","which","he","received","an","Independent","Spirit","Awards","nomination","for","the","Someone","to","Watch","Award",")",",","as","well","as","writing","and","directing","two","original","screenplays",":","1998","'s","Kansas","and","Leaves","of","Grass",",","which","was","released","in","2009","."],"labels":["O","O","O","O","O","O","O","O","B-book","I-book","I-book","O","B-book","I-book","I-book","O","O","O","O","O","O","B-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","O","B-book","I-book","I-book","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","award","magazine","literary_genre","book","person","location","organization","writer","poem","event"]}
{"id":"169","dataset":"crossner_literature","split":"test","instance":{"id":"169","prompt_labels":"Following(O) the(O) introduction(O) of(O) the(O) talkies(O) ,(O) Dwan(B-person) directed(O) child-star(O) Shirley(B-person) Temple(I-person) in(O) Heidi(O) ((O) 1937(O) )(O) and(O) Rebecca(O) of(O) Sunnybrook(O) Farm(O) ((O) 1938(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, event, country, location, literary genre, person, poem, writer, book, magazine and O.\nSentence: Following the introduction of the talkies , Dwan directed child-star Shirley Temple in Heidi ( 1937 ) and Rebecca of Sunnybrook Farm ( 1938 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Following","the","introduction","of","the","talkies",",","Dwan","directed","child-star","Shirley","Temple","in","Heidi","(","1937",")","and","Rebecca","of","Sunnybrook","Farm","(","1938",")","."],"labels":["O","O","O","O","O","O","O","B-person","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","award","event","country","location","literary_genre","person","poem","writer","book","magazine"]}
{"id":"170","dataset":"crossner_literature","split":"test","instance":{"id":"170","prompt_labels":"The(B-book) Rape(I-book) of(I-book) Nanking(I-book) :(I-book) The(I-book) Forgotten(I-book) Holocaust(I-book) of(I-book) World(I-book) War(I-book) II(I-book) is(O) a(O) bestselling(O) 1997(O) non-fiction(O) book(O) written(O) by(O) Iris(B-writer) Chang(I-writer) about(O) the(O) 1937-1938(B-event) Nanking(I-event) Massacre(I-event) ,(O) the(O) massacre(O) and(O) atrocities(O) committed(O) by(O) the(O) Imperial(O) Japanese(O) Army(O) after(O) it(O) captured(O) Nanjing(B-location) ,(O) then(O) capital(O) of(O) China(B-country) ,(O) during(O) the(O) Second(B-event) Sino-Japanese(I-event) War(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, literary genre, writer, magazine, person, location, country, organization, book, award, event and O.\nSentence: The Rape of Nanking : The Forgotten Holocaust of World War II is a bestselling 1997 non-fiction book written by Iris Chang about the 1937-1938 Nanking Massacre , the massacre and atrocities committed by the Imperial Japanese Army after it captured Nanjing , then capital of China , during the Second Sino-Japanese War .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Rape","of","Nanking",":","The","Forgotten","Holocaust","of","World","War","II","is","a","bestselling","1997","non-fiction","book","written","by","Iris","Chang","about","the","1937-1938","Nanking","Massacre",",","the","massacre","and","atrocities","committed","by","the","Imperial","Japanese","Army","after","it","captured","Nanjing",",","then","capital","of","China",",","during","the","Second","Sino-Japanese","War","."],"labels":["B-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","O","O","O","B-country","O","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["poem","literary_genre","writer","magazine","person","location","country","organization","book","award","event"]}
{"id":"172","dataset":"crossner_literature","split":"test","instance":{"id":"172","prompt_labels":"However(O) ,(O) in(O) the(O) parallel(B-literary genre) poem(I-literary genre) The(B-poem) Greene(I-poem) Knight(I-poem) ,(O) the(O) lace(O) is(O) white(O) ,(O) not(O) green(O) ,(O) and(O) is(O) considered(O) the(O) origin(O) of(O) the(O) collar(O) worn(O) by(O) the(O) knights(O) of(O) the(O) Bath(O) ,(O) not(O) the(O) Order(O) of(O) the(O) Garter(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, poem, person, literary genre, organization, magazine, book, writer, country, event and O.\nSentence: However , in the parallel poem The Greene Knight , the lace is white , not green , and is considered the origin of the collar worn by the knights of the Bath , not the Order of the Garter .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["However",",","in","the","parallel","poem","The","Greene","Knight",",","the","lace","is","white",",","not","green",",","and","is","considered","the","origin","of","the","collar","worn","by","the","knights","of","the","Bath",",","not","the","Order","of","the","Garter","."],"labels":["O","O","O","O","B-literary genre","I-literary genre","B-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","award","poem","person","literary_genre","organization","magazine","book","writer","country","event"]}
{"id":"176","dataset":"crossner_literature","split":"test","instance":{"id":"176","prompt_labels":"In(O) 1967(O) Algis(B-writer) Budrys(I-writer) listed(O) Aldiss(O) ,(O) J.(B-writer) G.(I-writer) Ballard(I-writer) ,(O) Roger(B-writer) Zelazny(I-writer) ,(O) and(O) Samuel(B-writer) R.(I-writer) Delany(I-writer) as(O) an(O) earthshaking(O) new(O) kind(O) of(O) writers(O) ,(O) and(O) leaders(O) of(O) the(O) New(O) Wave(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, person, location, award, poem, event, writer, magazine, organization, book, country and O.\nSentence: In 1967 Algis Budrys listed Aldiss , J. G. Ballard , Roger Zelazny , and Samuel R. Delany as an earthshaking new kind of writers , and leaders of the New Wave .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1967","Algis","Budrys","listed","Aldiss",",","J.","G.","Ballard",",","Roger","Zelazny",",","and","Samuel","R.","Delany","as","an","earthshaking","new","kind","of","writers",",","and","leaders","of","the","New","Wave","."],"labels":["O","O","B-writer","I-writer","O","O","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","O","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","person","location","award","poem","event","writer","magazine","organization","book","country"]}
{"id":"177","dataset":"crossner_literature","split":"test","instance":{"id":"177","prompt_labels":"They(O) published(O) numerous(O) criticisms(O) in(O) the(O) 1950s(O) and(O) 1960s(O) by(O) Whittaker(B-writer) Chambers(I-writer) ,(O) Garry(B-writer) Wills(I-writer) ,(O) and(O) M.(B-writer) Stanton(I-writer) Evans(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, book, event, award, poem, organization, country, location, writer, literary genre, magazine and O.\nSentence: They published numerous criticisms in the 1950s and 1960s by Whittaker Chambers , Garry Wills , and M. Stanton Evans .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["They","published","numerous","criticisms","in","the","1950s","and","1960s","by","Whittaker","Chambers",",","Garry","Wills",",","and","M.","Stanton","Evans","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","O","B-writer","I-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["person","book","event","award","poem","organization","country","location","writer","literary_genre","magazine"]}
{"id":"178","dataset":"crossner_literature","split":"test","instance":{"id":"178","prompt_labels":"Three(O) of(O) his(O) films(O) -(O) Andrei(O) Rublev(O) ,(O) Mirror(O) ,(O) and(O) Stalker(O) -(O) featured(O) in(O) Sight(B-magazine) &(I-magazine) Sound(I-magazine) s(O) 2012(B-award) poll(I-award) of(I-award) the(I-award) 50(I-award) greatest(I-award) films(I-award) of(I-award) all(I-award) time(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, literary genre, magazine, poem, organization, award, person, location, country, writer, event and O.\nSentence: Three of his films - Andrei Rublev , Mirror , and Stalker - featured in Sight & Sound s 2012 poll of the 50 greatest films of all time .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Three","of","his","films","-","Andrei","Rublev",",","Mirror",",","and","Stalker","-","featured","in","Sight","&","Sound","s","2012","poll","of","the","50","greatest","films","of","all","time","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","I-magazine","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["book","literary_genre","magazine","poem","organization","award","person","location","country","writer","event"]}
{"id":"179","dataset":"crossner_literature","split":"test","instance":{"id":"179","prompt_labels":"Early(O) in(O) his(O) career(O) ,(O) he(O) published(O) short(B-literary genre) stories(I-literary genre) and(O) poetry(B-literary genre) and(O) edited(O) the(O) literary(O) magazine(O) Oxford(B-magazine) Poetry(I-magazine) ,(O) before(O) going(O) on(O) to(O) publish(O) travel(O) writing(O) ,(O) satire(B-literary genre) ,(O) and(O) screenplays(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, location, literary genre, writer, book, event, organization, country, award, person, poem and O.\nSentence: Early in his career , he published short stories and poetry and edited the literary magazine Oxford Poetry , before going on to publish travel writing , satire , and screenplays .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Early","in","his","career",",","he","published","short","stories","and","poetry","and","edited","the","literary","magazine","Oxford","Poetry",",","before","going","on","to","publish","travel","writing",",","satire",",","and","screenplays","."],"labels":["O","O","O","O","O","O","O","B-literary genre","I-literary genre","O","B-literary genre","O","O","O","O","O","B-magazine","I-magazine","O","O","O","O","O","O","O","O","O","B-literary genre","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","location","literary_genre","writer","book","event","organization","country","award","person","poem"]}
{"id":"180","dataset":"crossner_literature","split":"test","instance":{"id":"180","prompt_labels":"The(B-book) English(I-book) Patient(I-book) ((O) 1992(O) )(O) won(O) the(O) Booker(B-award) Prize(I-award) ,(O) the(O) Canada(B-award) Australia(I-award) Prize(I-award) ,(O) and(O) the(O) Governor(B-award) General(I-award) 's(I-award) Award(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, person, poem, organization, event, award, magazine, book, country, location, literary genre and O.\nSentence: The English Patient ( 1992 ) won the Booker Prize , the Canada Australia Prize , and the Governor General 's Award .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","English","Patient","(","1992",")","won","the","Booker","Prize",",","the","Canada","Australia","Prize",",","and","the","Governor","General","'s","Award","."],"labels":["B-book","I-book","I-book","O","O","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["writer","person","poem","organization","event","award","magazine","book","country","location","literary_genre"]}
{"id":"181","dataset":"crossner_literature","split":"test","instance":{"id":"181","prompt_labels":"The(O) character(O) is(O) brought(O) back(O) as(O) a(O) ghola(O) in(O) the(O) Herbert(B-person) /(I-person) Anderson(I-person) sequels(O) which(O) conclude(O) the(O) original(O) series(O) ,(O) Hunters(B-book) of(I-book) Dune(I-book) ((O) 2006(O) )(O) and(O) Sandworms(B-book) of(I-book) Dune(I-book) ((O) 2007(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, writer, country, magazine, location, book, literary genre, person, organization, poem, award and O.\nSentence: The character is brought back as a ghola in the Herbert / Anderson sequels which conclude the original series , Hunters of Dune ( 2006 ) and Sandworms of Dune ( 2007 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","character","is","brought","back","as","a","ghola","in","the","Herbert","/","Anderson","sequels","which","conclude","the","original","series",",","Hunters","of","Dune","(","2006",")","and","Sandworms","of","Dune","(","2007",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-person","I-person","I-person","O","O","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","writer","country","magazine","location","book","literary_genre","person","organization","poem","award"]}
{"id":"182","dataset":"crossner_literature","split":"test","instance":{"id":"182","prompt_labels":"In(O) Patricia(B-writer) Wrede(I-writer) '(O) s(O) Regency(B-book) fantasies(I-book) ,(O) Great(O) Britain(O) has(O) a(O) Royal(B-organization) Society(I-organization) of(I-organization) Wizards(I-organization) ,(O) and(O) in(O) Poul(B-writer) Anderson(I-writer) 's(O) A(B-book) Midsummer(I-book) Tempest(I-book) William(B-writer) Shakespeare(I-writer) is(O) remembered(O) as(O) the(O) Great(O) Historian(O) ,(O) with(O) the(O) novel(B-literary genre) itself(O) taking(O) place(O) in(O) the(O) era(O) of(O) Oliver(B-person) Cromwell(I-person) and(O) Charles(B-person) I(I-person) ,(O) with(O) an(O) alternate(O) outcome(O) for(O) the(O) English(B-event) Civil(I-event) War(I-event) and(O) an(O) earlier(O) Industrial(B-event) Revolution(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, poem, person, writer, organization, country, location, event, award, book, magazine and O.\nSentence: In Patricia Wrede ' s Regency fantasies , Great Britain has a Royal Society of Wizards , and in Poul Anderson 's A Midsummer Tempest William Shakespeare is remembered as the Great Historian , with the novel itself taking place in the era of Oliver Cromwell and Charles I , with an alternate outcome for the English Civil War and an earlier Industrial Revolution .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","Patricia","Wrede","'","s","Regency","fantasies",",","Great","Britain","has","a","Royal","Society","of","Wizards",",","and","in","Poul","Anderson","'s","A","Midsummer","Tempest","William","Shakespeare","is","remembered","as","the","Great","Historian",",","with","the","novel","itself","taking","place","in","the","era","of","Oliver","Cromwell","and","Charles","I",",","with","an","alternate","outcome","for","the","English","Civil","War","and","an","earlier","Industrial","Revolution","."],"labels":["O","B-writer","I-writer","O","O","B-book","I-book","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","B-writer","I-writer","O","B-book","I-book","I-book","B-writer","I-writer","O","O","O","O","O","O","O","O","O","B-literary genre","O","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","B-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","poem","person","writer","organization","country","location","event","award","book","magazine"]}
{"id":"183","dataset":"crossner_literature","split":"test","instance":{"id":"183","prompt_labels":"It(O) was(O) followed(O) the(O) next(O) year(O) by(O) The(B-book) Tale(I-book) of(I-book) Squirrel(I-book) Nutkin(I-book) and(O) The(B-book) Tailor(I-book) of(I-book) Gloucester(I-book) ,(O) which(O) had(O) also(O) first(O) been(O) written(O) as(O) picture(O) letters(O) to(O) the(O) Moore(O) children(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, location, person, event, award, country, book, organization, poem, writer, magazine and O.\nSentence: It was followed the next year by The Tale of Squirrel Nutkin and The Tailor of Gloucester , which had also first been written as picture letters to the Moore children .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","followed","the","next","year","by","The","Tale","of","Squirrel","Nutkin","and","The","Tailor","of","Gloucester",",","which","had","also","first","been","written","as","picture","letters","to","the","Moore","children","."],"labels":["O","O","O","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","B-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","location","person","event","award","country","book","organization","poem","writer","magazine"]}
{"id":"185","dataset":"crossner_literature","split":"test","instance":{"id":"185","prompt_labels":"See(O) Gubbinal(B-poem) and(O) The(B-poem) Snow(I-poem) Man(I-poem) for(O) other(O) experiments(O) in(O) perspective(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, country, person, writer, event, organization, location, literary genre, poem, award, book and O.\nSentence: See Gubbinal and The Snow Man for other experiments in perspective .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["See","Gubbinal","and","The","Snow","Man","for","other","experiments","in","perspective","."],"labels":["O","B-poem","O","B-poem","I-poem","I-poem","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","country","person","writer","event","organization","location","literary_genre","poem","award","book"]}
{"id":"186","dataset":"crossner_literature","split":"test","instance":{"id":"186","prompt_labels":"We(O) can(O) see(O) its(O) echo(O) in(O) the(O) works(O) of(O) such(O) very(O) different(O) writers(O) as(O) Daniel(B-writer) Defoe(I-writer) ,(O) William(B-writer) Makepeace(I-writer) Thackeray(I-writer) ,(O) the(O) Bronts(B-writer) ,(O) Samuel(B-writer) Taylor(I-writer) Coleridge(I-writer) ,(O) T.(B-writer) S.(I-writer) Eliot(I-writer) and(O) even(O) Dorothy(B-writer) L.(I-writer) Sayers(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, person, magazine, poem, writer, organization, literary genre, book, event, country and O.\nSentence: We can see its echo in the works of such very different writers as Daniel Defoe , William Makepeace Thackeray , the Bronts , Samuel Taylor Coleridge , T. S. Eliot and even Dorothy L. Sayers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["We","can","see","its","echo","in","the","works","of","such","very","different","writers","as","Daniel","Defoe",",","William","Makepeace","Thackeray",",","the","Bronts",",","Samuel","Taylor","Coleridge",",","T.","S.","Eliot","and","even","Dorothy","L.","Sayers","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","B-writer","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","B-writer","I-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["award","location","person","magazine","poem","writer","organization","literary_genre","book","event","country"]}
{"id":"187","dataset":"crossner_literature","split":"test","instance":{"id":"187","prompt_labels":"Following(O) the(O) American(O) publication(O) ,(O) James(B-writer) T.(I-writer) Farrell(I-writer) ,(O) writing(O) in(O) The(B-magazine) New(I-magazine) Republic(I-magazine) ,(O) called(O) it(O) genuine(O) ,(O) unexaggerated(O) and(O) intelligent(O) ,(O) while(O) Herbert(B-writer) Gorman(I-writer) wrote(O) for(O) the(O) New(B-magazine) York(I-magazine) Times(I-magazine) Book(I-magazine) Review(I-magazine) ,(O) He(O) possesses(O) a(O) keen(O) eye(O) for(O) character(O) and(O) a(O) rough-and-ready(O) '(O) styleless(O) style(O) '(O) that(O) plunges(O) along(O) and(O) makes(O) the(O) reader(O) see(O) what(O) the(O) author(O) wants(O) him(O) to(O) see(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, award, book, literary genre, country, magazine, location, event, writer, poem and O.\nSentence: Following the American publication , James T. Farrell , writing in The New Republic , called it genuine , unexaggerated and intelligent , while Herbert Gorman wrote for the New York Times Book Review , He possesses a keen eye for character and a rough-and-ready ' styleless style ' that plunges along and makes the reader see what the author wants him to see .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Following","the","American","publication",",","James","T.","Farrell",",","writing","in","The","New","Republic",",","called","it","genuine",",","unexaggerated","and","intelligent",",","while","Herbert","Gorman","wrote","for","the","New","York","Times","Book","Review",",","He","possesses","a","keen","eye","for","character","and","a","rough-and-ready","'","styleless","style","'","that","plunges","along","and","makes","the","reader","see","what","the","author","wants","him","to","see","."],"labels":["O","O","O","O","O","B-writer","I-writer","I-writer","O","O","O","B-magazine","I-magazine","I-magazine","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","B-magazine","I-magazine","I-magazine","I-magazine","I-magazine","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","organization","award","book","literary_genre","country","magazine","location","event","writer","poem"]}
{"id":"188","dataset":"crossner_literature","split":"test","instance":{"id":"188","prompt_labels":"In(O) 1962(O) ,(O) two(O) major(O) anthologies(O) of(O) Borges(B-writer) 's(O) writings(O) were(O) published(O) in(O) English(O) by(O) New(B-organization) York(I-organization) presses(I-organization) :(O) Ficciones(B-book) and(O) Labyrinths(B-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, book, organization, person, poem, event, award, literary genre, country, magazine, location and O.\nSentence: In 1962 , two major anthologies of Borges 's writings were published in English by New York presses : Ficciones and Labyrinths .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1962",",","two","major","anthologies","of","Borges","'s","writings","were","published","in","English","by","New","York","presses",":","Ficciones","and","Labyrinths","."],"labels":["O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-book","O","B-book","O"],"target_index":null,"target_label":null},"label_list":["writer","book","organization","person","poem","event","award","literary_genre","country","magazine","location"]}
{"id":"189","dataset":"crossner_literature","split":"test","instance":{"id":"189","prompt_labels":"Chaplin(B-writer) received(O) three(O) Academy(B-award) Awards(I-award) :(O) an(O) Academy(B-award) Honorary(I-award) Award(I-award) for(O) versatility(O) and(O) genius(O) in(O) acting(O) ,(O) writing(O) ,(O) directing(O) ,(O) and(O) producing(O) The(O) Circus(O) in(O) 1929(O) ,(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, writer, book, poem, award, literary genre, location, event, organization, magazine and O.\nSentence: Chaplin received three Academy Awards : an Academy Honorary Award for versatility and genius in acting , writing , directing , and producing The Circus in 1929 ,","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Chaplin","received","three","Academy","Awards",":","an","Academy","Honorary","Award","for","versatility","and","genius","in","acting",",","writing",",","directing",",","and","producing","The","Circus","in","1929",","],"labels":["B-writer","O","O","B-award","I-award","O","O","B-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","country","writer","book","poem","award","literary_genre","location","event","organization","magazine"]}
{"id":"191","dataset":"crossner_literature","split":"test","instance":{"id":"191","prompt_labels":"This(O) group(O) ((O) jokingly(O) designated(O) The(B-organization) Collective(I-organization) )(O) included(O) future(O) Federal(B-organization) Reserve(I-organization) Chairman(O) Alan(B-person) Greenspan(I-person) ,(O) a(O) young(O) psychology(O) student(O) named(O) Nathan(B-person) Blumenthal(I-person) ((O) later(O) Nathaniel(B-person) Branden(I-person) )(O) and(O) his(O) wife(O) Barbara(B-writer) Branden(I-writer) and(O) Barbara(B-writer) 's(O) cousin(O) Leonard(B-person) Peikoff(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, book, award, writer, event, literary genre, organization, person, country, poem, magazine and O.\nSentence: This group ( jokingly designated The Collective ) included future Federal Reserve Chairman Alan Greenspan , a young psychology student named Nathan Blumenthal ( later Nathaniel Branden ) and his wife Barbara Branden and Barbara 's cousin Leonard Peikoff .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","group","(","jokingly","designated","The","Collective",")","included","future","Federal","Reserve","Chairman","Alan","Greenspan",",","a","young","psychology","student","named","Nathan","Blumenthal","(","later","Nathaniel","Branden",")","and","his","wife","Barbara","Branden","and","Barbara","'s","cousin","Leonard","Peikoff","."],"labels":["O","O","O","O","O","B-organization","I-organization","O","O","O","B-organization","I-organization","O","B-person","I-person","O","O","O","O","O","O","B-person","I-person","O","O","B-person","I-person","O","O","O","O","B-writer","I-writer","O","B-writer","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["location","book","award","writer","event","literary_genre","organization","person","country","poem","magazine"]}
{"id":"192","dataset":"crossner_literature","split":"test","instance":{"id":"192","prompt_labels":"Ti-Grace(B-writer) Atkinson(I-writer) ,(O) the(O) New(B-location) York(I-location) chapter(O) president(O) of(O) the(O) National(B-organization) Organization(I-organization) for(I-organization) Women(I-organization) ((O) NOW(B-organization) )(O) ,(O) described(O) Solanas(B-writer) as(O) the(O) first(O) outstanding(O) champion(O) of(O) women(O) 's(O) rights(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, magazine, person, literary genre, country, location, event, poem, award, book, writer and O.\nSentence: Ti-Grace Atkinson , the New York chapter president of the National Organization for Women ( NOW ) , described Solanas as the first outstanding champion of women 's rights","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Ti-Grace","Atkinson",",","the","New","York","chapter","president","of","the","National","Organization","for","Women","(","NOW",")",",","described","Solanas","as","the","first","outstanding","champion","of","women","'s","rights"],"labels":["B-writer","I-writer","O","O","B-location","I-location","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","B-writer","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","magazine","person","literary_genre","country","location","event","poem","award","book","writer"]}
{"id":"193","dataset":"crossner_literature","split":"test","instance":{"id":"193","prompt_labels":"Two(O) years(O) later(O) ,(O) Wilder(B-writer) earned(O) the(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Adapted(I-award) Screenplay(I-award) Academy(B-award) Awards(I-award) for(O) the(O) adaptation(O) of(O) a(O) Charles(B-writer) R.(I-writer) Jackson(I-writer) story(O) ,(O) The(B-book) Lost(I-book) Weekend(I-book) ((O) 1945(O) )(O) ,(O) the(O) first(O) major(O) American(O) film(O) to(O) make(O) a(O) serious(O) examination(O) of(O) alcoholism(O) ,(O) another(O) difficult(O) theme(O) under(O) the(O) Production(O) Code(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, event, magazine, poem, country, location, literary genre, person, writer, organization, award and O.\nSentence: Two years later , Wilder earned the Academy Award for Best Director and Academy Award for Best Adapted Screenplay Academy Awards for the adaptation of a Charles R. Jackson story , The Lost Weekend ( 1945 ) , the first major American film to make a serious examination of alcoholism , another difficult theme under the Production Code .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Two","years","later",",","Wilder","earned","the","Academy","Award","for","Best","Director","and","Academy","Award","for","Best","Adapted","Screenplay","Academy","Awards","for","the","adaptation","of","a","Charles","R.","Jackson","story",",","The","Lost","Weekend","(","1945",")",",","the","first","major","American","film","to","make","a","serious","examination","of","alcoholism",",","another","difficult","theme","under","the","Production","Code","."],"labels":["O","O","O","O","B-writer","O","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","B-award","I-award","O","O","O","O","O","B-writer","I-writer","I-writer","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["book","event","magazine","poem","country","location","literary_genre","person","writer","organization","award"]}
{"id":"194","dataset":"crossner_literature","split":"test","instance":{"id":"194","prompt_labels":"He(O) associated(O) with(O) a(O) variety(O) of(O) figures(O) in(O) Britain(B-country) 's(O) intelligence(O) community(O) at(O) the(O) time(O) ,(O) including(O) Dennis(B-writer) Wheatley(I-writer) ,(O) Roald(B-writer) Dahl(I-writer) ,(O) Ian(B-writer) Fleming(I-writer) ,(O) and(O) Maxwell(B-person) Knight(I-person) ,(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, writer, person, magazine, poem, location, country, literary genre, organization, book, award and O.\nSentence: He associated with a variety of figures in Britain 's intelligence community at the time , including Dennis Wheatley , Roald Dahl , Ian Fleming , and Maxwell Knight ,","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","associated","with","a","variety","of","figures","in","Britain","'s","intelligence","community","at","the","time",",","including","Dennis","Wheatley",",","Roald","Dahl",",","Ian","Fleming",",","and","Maxwell","Knight",","],"labels":["O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["event","writer","person","magazine","poem","location","country","literary_genre","organization","book","award"]}
{"id":"195","dataset":"crossner_literature","split":"test","instance":{"id":"195","prompt_labels":"In(O) the(O) 1970s(O) ,(O) Pohl(B-writer) re-emerged(O) as(O) a(O) novel(B-literary genre) writer(O) in(O) his(O) own(O) right(O) ,(O) with(O) books(O) such(O) as(O) Man(B-book) Plus(I-book) and(O) the(O) Heechee(B-book) series(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, book, poem, literary genre, award, location, organization, magazine, writer, person and O.\nSentence: In the 1970s , Pohl re-emerged as a novel writer in his own right , with books such as Man Plus and the Heechee series .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","1970s",",","Pohl","re-emerged","as","a","novel","writer","in","his","own","right",",","with","books","such","as","Man","Plus","and","the","Heechee","series","."],"labels":["O","O","O","O","B-writer","O","O","O","B-literary genre","O","O","O","O","O","O","O","O","O","O","B-book","I-book","O","O","B-book","O","O"],"target_index":null,"target_label":null},"label_list":["event","country","book","poem","literary_genre","award","location","organization","magazine","writer","person"]}
{"id":"196","dataset":"crossner_literature","split":"test","instance":{"id":"196","prompt_labels":"After(O) the(O) war(O) ,(O) Geisel(B-writer) returned(O) to(O) writing(O) children(O) 's(O) books(O) ,(O) writing(O) classics(B-literary genre) like(O) If(B-book) I(I-book) Ran(I-book) the(I-book) Zoo(I-book) ((O) 1950(O) )(O) ,(O) Horton(B-book) Hears(I-book) a(I-book) Who(I-book) !(I-book) ((O) 1955(O) )(O) ,(O) If(B-book) I(I-book) Ran(I-book) the(I-book) Circus(I-book) ((O) 1956(O) )(O) ,(O) The(B-book) Cat(I-book) in(I-book) the(I-book) Hat(I-book) ((O) 1957(O) )(O) ,(O) How(B-book) the(I-book) Grinch(I-book) Stole(I-book) Christmas(I-book) !(I-book) ((O) 1957(O) )(O) ,(O) and(O) Green(B-book) Eggs(I-book) and(I-book) Ham(I-book) ((O) 1960(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, book, organization, writer, event, literary genre, country, poem, magazine, location and O.\nSentence: After the war , Geisel returned to writing children 's books , writing classics like If I Ran the Zoo ( 1950 ) , Horton Hears a Who ! ( 1955 ) , If I Ran the Circus ( 1956 ) , The Cat in the Hat ( 1957 ) , How the Grinch Stole Christmas ! ( 1957 ) , and Green Eggs and Ham ( 1960 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","the","war",",","Geisel","returned","to","writing","children","'s","books",",","writing","classics","like","If","I","Ran","the","Zoo","(","1950",")",",","Horton","Hears","a","Who","!","(","1955",")",",","If","I","Ran","the","Circus","(","1956",")",",","The","Cat","in","the","Hat","(","1957",")",",","How","the","Grinch","Stole","Christmas","!","(","1957",")",",","and","Green","Eggs","and","Ham","(","1960",")","."],"labels":["O","O","O","O","B-writer","O","O","O","O","O","O","O","O","B-literary genre","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","O","B-book","I-book","I-book","I-book","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","award","book","organization","writer","event","literary_genre","country","poem","magazine","location"]}
{"id":"197","dataset":"crossner_literature","split":"test","instance":{"id":"197","prompt_labels":"Its(O) sequel(O) ,(O) Nova(B-book) Swing(I-book) ((O) 2006(O) )(O) ,(O) won(O) the(O) Arthur(B-award) C.(I-award) Clarke(I-award) Award(I-award) in(O) 2007(O) Ansible(B-organization) newsletter(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, location, person, award, country, writer, magazine, literary genre, event, poem, organization and O.\nSentence: Its sequel , Nova Swing ( 2006 ) , won the Arthur C. Clarke Award in 2007 Ansible newsletter","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Its","sequel",",","Nova","Swing","(","2006",")",",","won","the","Arthur","C.","Clarke","Award","in","2007","Ansible","newsletter"],"labels":["O","O","O","B-book","I-book","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","B-organization","O"],"target_index":null,"target_label":null},"label_list":["book","location","person","award","country","writer","magazine","literary_genre","event","poem","organization"]}
{"id":"198","dataset":"crossner_literature","split":"test","instance":{"id":"198","prompt_labels":"Robert(B-writer) B.(I-writer) Parker(I-writer) ,(O) The(B-book) Godwulf(I-book) Manuscript(I-book) ,(O) Dell(B-organization) Books(I-organization) ,(O) 1987(O) ,(O) page(O) 163(O) :(O) I(O) looked(O) at(O) them(O) obliquely(O) as(O) I(O) 'd(O) learned(O) to(O) do(O) a(O) long(O) time(O) ago(O) in(O) Korea(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, book, organization, award, writer, event, magazine, location, person, country, literary genre and O.\nSentence: Robert B. Parker , The Godwulf Manuscript , Dell Books , 1987 , page 163 : I looked at them obliquely as I 'd learned to do a long time ago in Korea .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Robert","B.","Parker",",","The","Godwulf","Manuscript",",","Dell","Books",",","1987",",","page","163",":","I","looked","at","them","obliquely","as","I","'d","learned","to","do","a","long","time","ago","in","Korea","."],"labels":["B-writer","I-writer","I-writer","O","B-book","I-book","I-book","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["poem","book","organization","award","writer","event","magazine","location","person","country","literary_genre"]}
{"id":"200","dataset":"crossner_literature","split":"test","instance":{"id":"200","prompt_labels":"In(O) 1930(O) ,(O) an(O) American(O) film(O) of(O) the(O) novel(B-literary genre) was(O) made(O) ,(O) directed(O) by(O) Lewis(B-person) Milestone(I-person) ;(O) with(O) a(O) screenplay(O) by(O) Maxwell(B-writer) Anderson(I-writer) ,(O) George(B-writer) Abbott(I-writer) ,(O) Del(B-writer) Andrews(I-writer) ,(O) C.(B-writer) Gardner(I-writer) Sullivan(I-writer) ;(O) and(O) with(O) uncredited(O) work(O) by(O) Walter(B-writer) Anthony(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, person, book, writer, poem, award, magazine, organization, literary genre, country and O.\nSentence: In 1930 , an American film of the novel was made , directed by Lewis Milestone ; with a screenplay by Maxwell Anderson , George Abbott , Del Andrews , C. Gardner Sullivan ; and with uncredited work by Walter Anthony .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1930",",","an","American","film","of","the","novel","was","made",",","directed","by","Lewis","Milestone",";","with","a","screenplay","by","Maxwell","Anderson",",","George","Abbott",",","Del","Andrews",",","C.","Gardner","Sullivan",";","and","with","uncredited","work","by","Walter","Anthony","."],"labels":["O","O","O","O","O","O","O","O","B-literary genre","O","O","O","O","O","B-person","I-person","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["location","event","person","book","writer","poem","award","magazine","organization","literary_genre","country"]}
{"id":"205","dataset":"crossner_literature","split":"test","instance":{"id":"205","prompt_labels":"Owen(B-writer) 's(O) experiences(O) with(O) religion(O) also(O) heavily(O) influenced(O) his(O) poetry(B-literary genre) ,(O) notably(O) in(O) poems(B-literary genre) such(O) as(O) Anthem(B-poem) for(I-poem) Doomed(I-poem) Youth(I-poem) ,(O) in(O) which(O) the(O) ceremony(O) of(O) a(O) funeral(O) is(O) re-enacted(O) not(O) in(O) a(O) church(O) ,(O) but(O) on(O) the(O) battlefield(O) itself(O) ,(O) and(O) At(B-poem) a(I-poem) Calvary(I-poem) near(I-poem) the(I-poem) Ancre(I-poem) ,(O) which(O) comments(O) on(O) the(O) Crucifixion(O) of(O) Christ(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, magazine, writer, literary genre, poem, person, organization, award, book, event, country and O.\nSentence: Owen 's experiences with religion also heavily influenced his poetry , notably in poems such as Anthem for Doomed Youth , in which the ceremony of a funeral is re-enacted not in a church , but on the battlefield itself , and At a Calvary near the Ancre , which comments on the Crucifixion of Christ .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Owen","'s","experiences","with","religion","also","heavily","influenced","his","poetry",",","notably","in","poems","such","as","Anthem","for","Doomed","Youth",",","in","which","the","ceremony","of","a","funeral","is","re-enacted","not","in","a","church",",","but","on","the","battlefield","itself",",","and","At","a","Calvary","near","the","Ancre",",","which","comments","on","the","Crucifixion","of","Christ","."],"labels":["B-writer","O","O","O","O","O","O","O","O","B-literary genre","O","O","O","B-literary genre","O","O","B-poem","I-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-poem","I-poem","I-poem","I-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","magazine","writer","literary_genre","poem","person","organization","award","book","event","country"]}
{"id":"206","dataset":"crossner_literature","split":"test","instance":{"id":"206","prompt_labels":"As(O) of(O) June(O) 2018(O) ,(O) Amnesty(B-organization) International(I-organization) and(O) the(O) Inter-American(B-organization) Commission(I-organization) on(I-organization) Human(I-organization) Rights(I-organization) of(O) the(O) Organization(B-organization) of(I-organization) American(I-organization) States(I-organization) have(O) reported(O) that(O) Ortega(B-person) has(O) engaged(O) in(O) a(O) violent(O) oppression(O) campaign(O) against(O) protesters(O) in(O) response(O) to(O) anti-Ortega(B-event) protests(I-event) since(O) April(O) 2018(O) ,(O) while(O) government(O) officials(O) and(O) government-owned(O) media(O) have(O) denied(O) such(O) actions(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, literary genre, country, book, writer, location, poem, award, organization, magazine, event and O.\nSentence: As of June 2018 , Amnesty International and the Inter-American Commission on Human Rights of the Organization of American States have reported that Ortega has engaged in a violent oppression campaign against protesters in response to anti-Ortega protests since April 2018 , while government officials and government-owned media have denied such actions .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["As","of","June","2018",",","Amnesty","International","and","the","Inter-American","Commission","on","Human","Rights","of","the","Organization","of","American","States","have","reported","that","Ortega","has","engaged","in","a","violent","oppression","campaign","against","protesters","in","response","to","anti-Ortega","protests","since","April","2018",",","while","government","officials","and","government-owned","media","have","denied","such","actions","."],"labels":["O","O","O","O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","literary_genre","country","book","writer","location","poem","award","organization","magazine","event"]}
{"id":"208","dataset":"crossner_literature","split":"test","instance":{"id":"208","prompt_labels":"During(O) the(O) Commonwealth(B-organization) of(I-organization) England(I-organization) period(O) he(O) was(O) a(O) colleague(O) and(O) friend(O) of(O) John(B-writer) Milton(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, book, poem, event, person, country, magazine, literary genre, award, writer, organization and O.\nSentence: During the Commonwealth of England period he was a colleague and friend of John Milton .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","the","Commonwealth","of","England","period","he","was","a","colleague","and","friend","of","John","Milton","."],"labels":["O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["location","book","poem","event","person","country","magazine","literary_genre","award","writer","organization"]}
{"id":"210","dataset":"crossner_literature","split":"test","instance":{"id":"210","prompt_labels":"He(O) was(O) also(O) a(O) prolific(O) author(O) who(O) wrote(O) on(O) both(O) scientific(B-literary genre) and(I-literary genre) social(I-literary genre) issues(I-literary genre) ;(O) his(O) account(O) of(O) his(O) adventures(O) and(O) observations(O) during(O) his(O) explorations(O) in(O) Singapore(B-country) ,(O) Indonesia(B-country) and(O) Malaysia(B-country) ,(O) The(B-book) Malay(I-book) Archipelago(I-book) ,(O) was(O) both(O) popular(O) and(O) highly(O) regarded(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, writer, country, person, poem, literary genre, award, book, location, organization, magazine and O.\nSentence: He was also a prolific author who wrote on both scientific and social issues ; his account of his adventures and observations during his explorations in Singapore , Indonesia and Malaysia , The Malay Archipelago , was both popular and highly regarded .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","also","a","prolific","author","who","wrote","on","both","scientific","and","social","issues",";","his","account","of","his","adventures","and","observations","during","his","explorations","in","Singapore",",","Indonesia","and","Malaysia",",","The","Malay","Archipelago",",","was","both","popular","and","highly","regarded","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","I-literary genre","I-literary genre","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","B-country","O","B-country","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","writer","country","person","poem","literary_genre","award","book","location","organization","magazine"]}
{"id":"211","dataset":"crossner_literature","split":"test","instance":{"id":"211","prompt_labels":"Recommended(O) Reading(O) ,(O) The(B-magazine) Magazine(I-magazine) of(I-magazine) Fantasy(I-magazine) &(I-magazine) Science(I-magazine) Fiction(I-magazine) ,(O) February(O) 1952(O) ,(O) p.105(O) P.(B-writer) Schuyler(I-writer) Miller(I-writer) ,(O) noting(O) that(O) the(O) novel(B-literary genre) 's(O) climactic(O) situations(O) seem(O) to(O) be(O) telegraphed(O) ,(O) suggested(O) that(O) Heinlein(B-writer) presented(O) his(O) background(O) situations(O) so(O) effectively(O) that(O) readers(O) solve(O) the(O) story(O) 's(O) mysteries(O) more(O) quickly(O) than(O) Heinlein(B-writer) allowed(O) his(O) characters(O) to(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, writer, magazine, location, literary genre, award, organization, person, poem, country, book and O.\nSentence: Recommended Reading , The Magazine of Fantasy & Science Fiction , February 1952 , p.105 P. Schuyler Miller , noting that the novel 's climactic situations seem to be telegraphed , suggested that Heinlein presented his background situations so effectively that readers solve the story 's mysteries more quickly than Heinlein allowed his characters to .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Recommended","Reading",",","The","Magazine","of","Fantasy","&","Science","Fiction",",","February","1952",",","p.105","P.","Schuyler","Miller",",","noting","that","the","novel","'s","climactic","situations","seem","to","be","telegraphed",",","suggested","that","Heinlein","presented","his","background","situations","so","effectively","that","readers","solve","the","story","'s","mysteries","more","quickly","than","Heinlein","allowed","his","characters","to","."],"labels":["O","O","O","B-magazine","I-magazine","I-magazine","I-magazine","I-magazine","I-magazine","I-magazine","O","O","O","O","O","B-writer","I-writer","I-writer","O","O","O","O","B-literary genre","O","O","O","O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","writer","magazine","location","literary_genre","award","organization","person","poem","country","book"]}
{"id":"213","dataset":"crossner_literature","split":"test","instance":{"id":"213","prompt_labels":"Although(O) German(O) language(O) writers(O) such(O) as(O) Hilde(B-writer) Domin(I-writer) ,(O) Luise(B-writer) Rinser(I-writer) and(O) Nelly(B-writer) Sachs(I-writer) had(O) published(O) notable(O) works(O) on(O) women(O) 's(O) issues(O) in(O) the(O) post-war(O) period(O) it(O) was(O) only(O) in(O) the(O) 1970s(O) that(O) a(O) feminist(O) movement(O) emerged(O) in(O) West(B-country) Germany(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, writer, literary genre, magazine, organization, award, book, location, person, poem and O.\nSentence: Although German language writers such as Hilde Domin , Luise Rinser and Nelly Sachs had published notable works on women 's issues in the post-war period it was only in the 1970s that a feminist movement emerged in West Germany .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Although","German","language","writers","such","as","Hilde","Domin",",","Luise","Rinser","and","Nelly","Sachs","had","published","notable","works","on","women","'s","issues","in","the","post-war","period","it","was","only","in","the","1970s","that","a","feminist","movement","emerged","in","West","Germany","."],"labels":["O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["event","country","writer","literary_genre","magazine","organization","award","book","location","person","poem"]}
{"id":"214","dataset":"crossner_literature","split":"test","instance":{"id":"214","prompt_labels":"Lunar(B-poem) Paraphrase(I-poem) can(O) be(O) read(O) as(O) such(O) a(O) response(O) ,(O) despite(O) its(O) mention(O) of(O) religious(O) figures(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, book, country, award, event, writer, organization, literary genre, magazine, location, person and O.\nSentence: Lunar Paraphrase can be read as such a response , despite its mention of religious figures .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Lunar","Paraphrase","can","be","read","as","such","a","response",",","despite","its","mention","of","religious","figures","."],"labels":["B-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","book","country","award","event","writer","organization","literary_genre","magazine","location","person"]}
{"id":"215","dataset":"crossner_literature","split":"test","instance":{"id":"215","prompt_labels":"William(B-writer) wrote(O) the(O) Arthurian(O) epic(O) Gesta(B-poem) Regum(I-poem) Britanniae(I-poem) ,(O) in(O) Latin(O) hexameters(O) ,(O) which(O) he(O) completed(O) just(O) after(O) 1236(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, book, location, literary genre, country, person, magazine, organization, event, award, writer and O.\nSentence: William wrote the Arthurian epic Gesta Regum Britanniae , in Latin hexameters , which he completed just after 1236 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["William","wrote","the","Arthurian","epic","Gesta","Regum","Britanniae",",","in","Latin","hexameters",",","which","he","completed","just","after","1236","."],"labels":["B-writer","O","O","O","O","B-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","book","location","literary_genre","country","person","magazine","organization","event","award","writer"]}
{"id":"216","dataset":"crossner_literature","split":"test","instance":{"id":"216","prompt_labels":"The(O) first(O) professional(O) critic(O) to(O) comment(O) on(O) Howard(B-writer) 's(O) work(O) was(O) Hoffman(B-person) Reynolds(I-person) Hays(I-person) ,(O) reviewing(O) the(O) Arkham(B-organization) House(I-organization) collection(O) Skull-Face(B-book) and(I-book) Others(I-book) in(O) The(B-magazine) New(I-magazine) York(I-magazine) Times(I-magazine) Book(I-magazine) Review(I-magazine) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, literary genre, poem, person, book, event, organization, magazine, location, writer and O.\nSentence: The first professional critic to comment on Howard 's work was Hoffman Reynolds Hays , reviewing the Arkham House collection Skull-Face and Others in The New York Times Book Review .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","first","professional","critic","to","comment","on","Howard","'s","work","was","Hoffman","Reynolds","Hays",",","reviewing","the","Arkham","House","collection","Skull-Face","and","Others","in","The","New","York","Times","Book","Review","."],"labels":["O","O","O","O","O","O","O","B-writer","O","O","O","B-person","I-person","I-person","O","O","O","B-organization","I-organization","O","B-book","I-book","I-book","O","B-magazine","I-magazine","I-magazine","I-magazine","I-magazine","I-magazine","O"],"target_index":null,"target_label":null},"label_list":["country","award","literary_genre","poem","person","book","event","organization","magazine","location","writer"]}
{"id":"217","dataset":"crossner_literature","split":"test","instance":{"id":"217","prompt_labels":"She(O) subsequently(O) received(O) critical(O) acclaim(O) for(O) her(O) performances(O) in(O) films(O) such(O) as(O) Silkwood(O) ((O) 1983(O) )(O) ,(O) Mask(O) ((O) 1985(O) )(O) ,(O) The(O) Witches(O) of(O) Eastwick(O) ((O) 1987(O) )(O) ,(O) and(O) Moonstruck(O) ((O) 1987(O) )(O) ,(O) with(O) the(O) latter(O) having(O) earned(O) her(O) the(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, poem, person, magazine, writer, location, literary genre, book, event, country, award and O.\nSentence: She subsequently received critical acclaim for her performances in films such as Silkwood ( 1983 ) , Mask ( 1985 ) , The Witches of Eastwick ( 1987 ) , and Moonstruck ( 1987 ) , with the latter having earned her the Academy Award for Best Actress .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","subsequently","received","critical","acclaim","for","her","performances","in","films","such","as","Silkwood","(","1983",")",",","Mask","(","1985",")",",","The","Witches","of","Eastwick","(","1987",")",",","and","Moonstruck","(","1987",")",",","with","the","latter","having","earned","her","the","Academy","Award","for","Best","Actress","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["organization","poem","person","magazine","writer","location","literary_genre","book","event","country","award"]}
{"id":"218","dataset":"crossner_literature","split":"test","instance":{"id":"218","prompt_labels":"Slaughterhouse-Five(B-book) received(O) generally(O) positive(O) reviews(O) ,(O) with(O) Michael(B-writer) Crichton(I-writer) writing(O) in(O) The(B-magazine) New(I-magazine) Republic(I-magazine) ,(O) he(O) writes(O) about(O) the(O) most(O) excruciatingly(O) painful(O) things(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, book, magazine, award, person, country, writer, literary genre, poem, organization, event and O.\nSentence: Slaughterhouse-Five received generally positive reviews , with Michael Crichton writing in The New Republic , he writes about the most excruciatingly painful things .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Slaughterhouse-Five","received","generally","positive","reviews",",","with","Michael","Crichton","writing","in","The","New","Republic",",","he","writes","about","the","most","excruciatingly","painful","things","."],"labels":["B-book","O","O","O","O","O","O","B-writer","I-writer","O","O","B-magazine","I-magazine","I-magazine","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","book","magazine","award","person","country","writer","literary_genre","poem","organization","event"]}
{"id":"219","dataset":"crossner_literature","split":"test","instance":{"id":"219","prompt_labels":"He(O) continued(O) his(O) Dune(B-book) saga(I-book) ,(O) following(O) it(O) with(O) Dune(B-book) Messiah(I-book) ,(O) Children(B-book) of(I-book) Dune(I-book) ,(O) and(O) God(B-book) Emperor(I-book) of(I-book) Dune(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, organization, writer, magazine, person, country, location, event, literary genre, award, book and O.\nSentence: He continued his Dune saga , following it with Dune Messiah , Children of Dune , and God Emperor of Dune .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","continued","his","Dune","saga",",","following","it","with","Dune","Messiah",",","Children","of","Dune",",","and","God","Emperor","of","Dune","."],"labels":["O","O","O","B-book","I-book","O","O","O","O","B-book","I-book","O","B-book","I-book","I-book","O","O","B-book","I-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["poem","organization","writer","magazine","person","country","location","event","literary_genre","award","book"]}
{"id":"222","dataset":"crossner_literature","split":"test","instance":{"id":"222","prompt_labels":"Barry(O) Lyndon(O) is(O) a(O) 1975(O) period(O) drama(O) film(O) written(O) ,(O) directed(O) and(O) produced(O) by(O) Stanley(B-writer) Kubrick(I-writer) ,(O) based(O) on(O) the(O) 1844(O) novel(B-literary genre) The(B-book) Luck(I-book) of(I-book) Barry(I-book) Lyndon(I-book) by(O) William(B-writer) Makepeace(I-writer) Thackeray(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, writer, literary genre, event, magazine, organization, person, country, book, location, award and O.\nSentence: Barry Lyndon is a 1975 period drama film written , directed and produced by Stanley Kubrick , based on the 1844 novel The Luck of Barry Lyndon by William Makepeace Thackeray .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Barry","Lyndon","is","a","1975","period","drama","film","written",",","directed","and","produced","by","Stanley","Kubrick",",","based","on","the","1844","novel","The","Luck","of","Barry","Lyndon","by","William","Makepeace","Thackeray","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","B-literary genre","B-book","I-book","I-book","I-book","I-book","O","B-writer","I-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["poem","writer","literary_genre","event","magazine","organization","person","country","book","location","award"]}
{"id":"223","dataset":"crossner_literature","split":"test","instance":{"id":"223","prompt_labels":"In(O) the(O) Swahili(O) and(O) Indonesian(O) culture(O) ,(O) many(O) of(O) his(O) stories(O) are(O) being(O) told(O) under(O) the(O) name(O) of(O) Abunuwasi(B-person) or(O) Abunawas(B-person) ,(O) though(O) this(O) confuses(O) Nasreddin(B-person) with(O) an(O) entirely(O) different(O) man(O) -(O) the(O) poet(O) Abu(B-writer) Nuwas(I-writer) ,(O) known(O) for(O) homoerotic(B-literary genre) verse(I-literary genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, event, writer, magazine, literary genre, organization, location, award, person, poem, country and O.\nSentence: In the Swahili and Indonesian culture , many of his stories are being told under the name of Abunuwasi or Abunawas , though this confuses Nasreddin with an entirely different man - the poet Abu Nuwas , known for homoerotic verse .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","Swahili","and","Indonesian","culture",",","many","of","his","stories","are","being","told","under","the","name","of","Abunuwasi","or","Abunawas",",","though","this","confuses","Nasreddin","with","an","entirely","different","man","-","the","poet","Abu","Nuwas",",","known","for","homoerotic","verse","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","O","B-person","O","O","O","O","B-person","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","B-literary genre","I-literary genre","O"],"target_index":null,"target_label":null},"label_list":["book","event","writer","magazine","literary_genre","organization","location","award","person","poem","country"]}
{"id":"224","dataset":"crossner_literature","split":"test","instance":{"id":"224","prompt_labels":"In(O) 1998(O) ,(O) he(O) was(O) a(O) member(O) of(O) the(O) jury(O) at(O) the(O) 48th(B-event) Berlin(I-event) International(I-event) Film(I-event) Festival(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, award, country, magazine, event, organization, book, literary genre, writer, location, person and O.\nSentence: In 1998 , he was a member of the jury at the 48th Berlin International Film Festival .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1998",",","he","was","a","member","of","the","jury","at","the","48th","Berlin","International","Film","Festival","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["poem","award","country","magazine","event","organization","book","literary_genre","writer","location","person"]}
{"id":"225","dataset":"crossner_literature","split":"test","instance":{"id":"225","prompt_labels":"In(O) the(O) 1990(O) steampunk(B-literary genre) novel(I-literary genre) The(B-book) Difference(I-book) Engine(I-book) by(O) William(B-writer) Gibson(I-writer) and(O) Bruce(B-writer) Sterling(I-writer) ,(O) Ada(B-person) Lovelace(I-person) delivers(O) a(O) lecture(O) on(O) the(O) punched(O) cards(O) programme(O) that(O) proves(O) two(O) theorems(O) ,(O) the(O) discovery(O) that(O) ,(O) in(O) reality(O) ,(O) would(O) not(O) be(O) made(O) until(O) 1931(O) by(O) Kurt(B-person) Gdel(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, country, writer, award, location, book, organization, poem, person, magazine, event and O.\nSentence: In the 1990 steampunk novel The Difference Engine by William Gibson and Bruce Sterling , Ada Lovelace delivers a lecture on the punched cards programme that proves two theorems , the discovery that , in reality , would not be made until 1931 by Kurt Gdel .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","1990","steampunk","novel","The","Difference","Engine","by","William","Gibson","and","Bruce","Sterling",",","Ada","Lovelace","delivers","a","lecture","on","the","punched","cards","programme","that","proves","two","theorems",",","the","discovery","that",",","in","reality",",","would","not","be","made","until","1931","by","Kurt","Gdel","."],"labels":["O","O","O","B-literary genre","I-literary genre","B-book","I-book","I-book","O","B-writer","I-writer","O","B-writer","I-writer","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","country","writer","award","location","book","organization","poem","person","magazine","event"]}
{"id":"226","dataset":"crossner_literature","split":"test","instance":{"id":"226","prompt_labels":"He(O) directed(O) Mac(O) ((O) 1992(O) )(O) ,(O) which(O) won(O) the(O) Golden(B-award) Camera(I-award) Award(I-award) at(O) the(O) Cannes(B-event) Film(I-event) Festival(I-event) ,(O) Illuminata(O) ((O) 1998(O) )(O) ,(O) and(O) Romance(O) and(O) Cigarettes(O) ((O) 2005(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, event, book, literary genre, award, writer, magazine, poem, country, location and O.\nSentence: He directed Mac ( 1992 ) , which won the Golden Camera Award at the Cannes Film Festival , Illuminata ( 1998 ) , and Romance and Cigarettes ( 2005 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","directed","Mac","(","1992",")",",","which","won","the","Golden","Camera","Award","at","the","Cannes","Film","Festival",",","Illuminata","(","1998",")",",","and","Romance","and","Cigarettes","(","2005",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","person","event","book","literary_genre","award","writer","magazine","poem","country","location"]}
{"id":"227","dataset":"crossner_literature","split":"test","instance":{"id":"227","prompt_labels":"Boethius(B-person) and(O) Consolatio(B-book) Philosophiae(I-book) are(O) cited(O) frequently(O) by(O) the(O) main(O) character(O) Ignatius(B-person) J.(I-person) Reilly(I-person) in(O) the(O) Pulitzer(B-award) Prize(I-award) -winning(O) A(B-book) Confederacy(I-book) of(I-book) Dunces(I-book) ((O) 1980(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, literary genre, writer, poem, book, award, event, organization, magazine, location, person and O.\nSentence: Boethius and Consolatio Philosophiae are cited frequently by the main character Ignatius J. Reilly in the Pulitzer Prize -winning A Confederacy of Dunces ( 1980 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Boethius","and","Consolatio","Philosophiae","are","cited","frequently","by","the","main","character","Ignatius","J.","Reilly","in","the","Pulitzer","Prize","-winning","A","Confederacy","of","Dunces","(","1980",")","."],"labels":["B-person","O","B-book","I-book","O","O","O","O","O","O","O","B-person","I-person","I-person","O","O","B-award","I-award","O","B-book","I-book","I-book","I-book","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","literary_genre","writer","poem","book","award","event","organization","magazine","location","person"]}
{"id":"231","dataset":"crossner_literature","split":"test","instance":{"id":"231","prompt_labels":"Andy(B-writer) Warhol(I-writer) was(O) commissioned(O) in(O) 1984(O) by(O) collector(O) and(O) gallerist(O) Alexander(B-person) Iolas(I-person) to(O) produce(O) work(O) based(O) on(O) Leonardo(B-person) da(I-person) Vinci(I-person) '(O) s(O) The(O) Last(O) Supper(O) for(O) an(O) exhibition(O) at(O) the(O) old(O) refectory(O) of(O) the(O) Palazzo(B-location) delle(I-location) Stelline(I-location) in(O) Milan(B-location) ,(O) opposite(O) from(O) the(O) Santa(B-location) Maria(I-location) delle(I-location) Grazie(I-location) where(O) Leonardo(B-person) da(I-person) Vinci(I-person) 's(O) mural(O) can(O) be(O) seen(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, country, poem, writer, organization, event, literary genre, person, award, location, magazine and O.\nSentence: Andy Warhol was commissioned in 1984 by collector and gallerist Alexander Iolas to produce work based on Leonardo da Vinci ' s The Last Supper for an exhibition at the old refectory of the Palazzo delle Stelline in Milan , opposite from the Santa Maria delle Grazie where Leonardo da Vinci 's mural can be seen .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Andy","Warhol","was","commissioned","in","1984","by","collector","and","gallerist","Alexander","Iolas","to","produce","work","based","on","Leonardo","da","Vinci","'","s","The","Last","Supper","for","an","exhibition","at","the","old","refectory","of","the","Palazzo","delle","Stelline","in","Milan",",","opposite","from","the","Santa","Maria","delle","Grazie","where","Leonardo","da","Vinci","'s","mural","can","be","seen","."],"labels":["B-writer","I-writer","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","B-person","I-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","O","B-location","O","O","O","O","B-location","I-location","I-location","I-location","O","B-person","I-person","I-person","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["book","country","poem","writer","organization","event","literary_genre","person","award","location","magazine"]}
{"id":"232","dataset":"crossner_literature","split":"test","instance":{"id":"232","prompt_labels":"During(O) his(O) life(O) ,(O) Cocteau(B-writer) was(O) commander(O) of(O) the(O) Legion(B-organization) of(I-organization) Honor(I-organization) ,(O) Member(O) of(O) the(O) Mallarm(B-organization) Academy(I-organization) ,(O) German(B-organization) Academy(I-organization) ((O) Berlin(B-location) )(O) ,(O) American(B-organization) Academy(I-organization) ,(O) Mark(B-organization) Twain(I-organization) ((I-organization) U.S.A(I-organization) )(I-organization) Academy(I-organization) ,(O) Honorary(O) President(O) of(O) the(O) Cannes(B-event) Film(I-event) Festival(I-event) ,(O) Honorary(O) President(O) of(O) the(O) France-Hungary(B-organization) Association(I-organization) and(O) President(O) of(O) the(O) Jazz(B-organization) Academy(I-organization) and(O) of(O) the(O) Academy(B-organization) of(I-organization) the(I-organization) Disc(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, award, person, magazine, location, organization, book, poem, event, country, writer and O.\nSentence: During his life , Cocteau was commander of the Legion of Honor , Member of the Mallarm Academy , German Academy ( Berlin ) , American Academy , Mark Twain ( U.S.A ) Academy , Honorary President of the Cannes Film Festival , Honorary President of the France-Hungary Association and President of the Jazz Academy and of the Academy of the Disc .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","his","life",",","Cocteau","was","commander","of","the","Legion","of","Honor",",","Member","of","the","Mallarm","Academy",",","German","Academy","(","Berlin",")",",","American","Academy",",","Mark","Twain","(","U.S.A",")","Academy",",","Honorary","President","of","the","Cannes","Film","Festival",",","Honorary","President","of","the","France-Hungary","Association","and","President","of","the","Jazz","Academy","and","of","the","Academy","of","the","Disc","."],"labels":["O","O","O","O","B-writer","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","B-organization","I-organization","O","B-organization","I-organization","O","B-location","O","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","B-organization","I-organization","O","O","O","O","B-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","award","person","magazine","location","organization","book","poem","event","country","writer"]}
{"id":"233","dataset":"crossner_literature","split":"test","instance":{"id":"233","prompt_labels":"After(O) making(O) a(O) series(O) of(O) westerns(O) and(O) comedies(O) ,(O) Dwan(B-person) directed(O) fellow(O) Canadian-American(O) Mary(B-person) Pickford(I-person) in(O) several(O) very(O) successful(O) movies(O) as(O) well(O) as(O) her(O) husband(O) ,(O) Douglas(B-person) Fairbanks(I-person) ,(O) notably(O) in(O) the(O) acclaimed(O) 1922(O) Robin(O) Hood(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, poem, country, book, writer, person, award, location, event, literary genre, organization and O.\nSentence: After making a series of westerns and comedies , Dwan directed fellow Canadian-American Mary Pickford in several very successful movies as well as her husband , Douglas Fairbanks , notably in the acclaimed 1922 Robin Hood .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","making","a","series","of","westerns","and","comedies",",","Dwan","directed","fellow","Canadian-American","Mary","Pickford","in","several","very","successful","movies","as","well","as","her","husband",",","Douglas","Fairbanks",",","notably","in","the","acclaimed","1922","Robin","Hood","."],"labels":["O","O","O","O","O","O","O","O","O","B-person","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","poem","country","book","writer","person","award","location","event","literary_genre","organization"]}
{"id":"234","dataset":"crossner_literature","split":"test","instance":{"id":"234","prompt_labels":"Years(O) later(O) ,(O) Lead(O) Me(O) On(O) would(O) be(O) chosen(O) as(O) the(O) greatest(O) Contemporary(O) Christian(O) album(O) of(O) all(O) time(O) by(O) CCM(B-magazine) Magazine(I-magazine) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, book, country, literary genre, person, organization, magazine, writer, location, event, poem and O.\nSentence: Years later , Lead Me On would be chosen as the greatest Contemporary Christian album of all time by CCM Magazine .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Years","later",",","Lead","Me","On","would","be","chosen","as","the","greatest","Contemporary","Christian","album","of","all","time","by","CCM","Magazine","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","O"],"target_index":null,"target_label":null},"label_list":["award","book","country","literary_genre","person","organization","magazine","writer","location","event","poem"]}
{"id":"236","dataset":"crossner_literature","split":"test","instance":{"id":"236","prompt_labels":"Shot(O) in(O) January(O) ,(O) it(O) premired(O) at(O) the(O) Cannes(B-event) Film(I-event) Festival(I-event) on(O) 27(O) May(O) 2007(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, country, event, writer, award, person, magazine, book, location, organization, poem and O.\nSentence: Shot in January , it premired at the Cannes Film Festival on 27 May 2007 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Shot","in","January",",","it","premired","at","the","Cannes","Film","Festival","on","27","May","2007","."],"labels":["O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","country","event","writer","award","person","magazine","book","location","organization","poem"]}
{"id":"238","dataset":"crossner_literature","split":"test","instance":{"id":"238","prompt_labels":"According(O) to(O) Henry(B-writer) Littlefield(I-writer) 's(O) 1964(O) article(O) ,(O) L.(B-writer) Frank(I-writer) Baum(I-writer) '(O) s(O) The(B-book) Wonderful(I-book) Wizard(I-book) of(I-book) Oz(I-book) ,(O) may(O) be(O) readily(O) understood(O) as(O) a(O) plot-driven(O) fantasy(B-literary genre) narrative(I-literary genre) in(O) an(O) extended(O) fable(O) with(O) talking(O) animals(O) and(O) broadly(O) sketched(O) characters(O) ,(O) intended(O) to(O) discuss(O) the(O) politics(O) of(O) the(O) time(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, literary genre, organization, writer, magazine, poem, book, event, person, country, location and O.\nSentence: According to Henry Littlefield 's 1964 article , L. Frank Baum ' s The Wonderful Wizard of Oz , may be readily understood as a plot-driven fantasy narrative in an extended fable with talking animals and broadly sketched characters , intended to discuss the politics of the time .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["According","to","Henry","Littlefield","'s","1964","article",",","L.","Frank","Baum","'","s","The","Wonderful","Wizard","of","Oz",",","may","be","readily","understood","as","a","plot-driven","fantasy","narrative","in","an","extended","fable","with","talking","animals","and","broadly","sketched","characters",",","intended","to","discuss","the","politics","of","the","time","."],"labels":["O","O","B-writer","I-writer","O","O","O","O","B-writer","I-writer","I-writer","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","literary_genre","organization","writer","magazine","poem","book","event","person","country","location"]}
{"id":"239","dataset":"crossner_literature","split":"test","instance":{"id":"239","prompt_labels":"He(O) is(O) a(O) key(O) figure(O) in(O) the(O) 17th-century(B-event) scientific(I-event) revolution(I-event) ,(O) best(O) known(O) for(O) his(O) laws(O) of(O) planetary(O) motion(O) ,(O) and(O) his(O) books(O) Astronomia(B-book) nova(I-book) ,(O) Harmonices(B-book) Mundi(I-book) ,(O) and(O) Epitome(B-book) Astronomiae(I-book) Copernicanae(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, country, organization, person, award, poem, magazine, literary genre, writer, event, location and O.\nSentence: He is a key figure in the 17th-century scientific revolution , best known for his laws of planetary motion , and his books Astronomia nova , Harmonices Mundi , and Epitome Astronomiae Copernicanae .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","is","a","key","figure","in","the","17th-century","scientific","revolution",",","best","known","for","his","laws","of","planetary","motion",",","and","his","books","Astronomia","nova",",","Harmonices","Mundi",",","and","Epitome","Astronomiae","Copernicanae","."],"labels":["O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","O","B-book","I-book","O","O","B-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["book","country","organization","person","award","poem","magazine","literary_genre","writer","event","location"]}
{"id":"242","dataset":"crossner_literature","split":"test","instance":{"id":"242","prompt_labels":"This(O) heartbreak(O) was(O) reflected(O) in(O) her(O) early(O) poetry(B-literary genre) and(O) earned(O) Mistral(B-writer) her(O) first(O) recognized(O) literary(O) work(O) in(O) 1914(O) with(O) Sonetos(B-poem) de(I-poem) la(I-poem) Muerte(I-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, country, poem, literary genre, organization, magazine, book, event, award, writer and O.\nSentence: This heartbreak was reflected in her early poetry and earned Mistral her first recognized literary work in 1914 with Sonetos de la Muerte .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","heartbreak","was","reflected","in","her","early","poetry","and","earned","Mistral","her","first","recognized","literary","work","in","1914","with","Sonetos","de","la","Muerte","."],"labels":["O","O","O","O","O","O","O","B-literary genre","O","O","B-writer","O","O","O","O","O","O","O","O","B-poem","I-poem","I-poem","I-poem","O"],"target_index":null,"target_label":null},"label_list":["location","person","country","poem","literary_genre","organization","magazine","book","event","award","writer"]}
{"id":"245","dataset":"crossner_literature","split":"test","instance":{"id":"245","prompt_labels":"In(O) 2005(O) ,(O) McEntire(B-person) starred(O) as(O) Nellie(B-person) Forbush(I-person) in(O) the(O) Carnegie(B-location) Hall(I-location) concert(I-location) production(O) of(O) the(O) Broadway(B-organization) musical(O) South(O) Pacific(O) with(O) Alec(B-person) Baldwin(I-person) as(O) Luther(B-person) Billis(I-person) and(O) Brian(B-person) Stokes(I-person) Mitchell(I-person) as(O) Emile(B-person) de(I-person) Becque(I-person) ,(O) directed(O) by(O) Walter(B-person) Bobbie(I-person) and(O) with(O) an(O) adapted(O) script(O) by(O) David(B-writer) Ives(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, location, event, book, literary genre, country, writer, organization, poem, magazine and O.\nSentence: In 2005 , McEntire starred as Nellie Forbush in the Carnegie Hall concert production of the Broadway musical South Pacific with Alec Baldwin as Luther Billis and Brian Stokes Mitchell as Emile de Becque , directed by Walter Bobbie and with an adapted script by David Ives .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2005",",","McEntire","starred","as","Nellie","Forbush","in","the","Carnegie","Hall","concert","production","of","the","Broadway","musical","South","Pacific","with","Alec","Baldwin","as","Luther","Billis","and","Brian","Stokes","Mitchell","as","Emile","de","Becque",",","directed","by","Walter","Bobbie","and","with","an","adapted","script","by","David","Ives","."],"labels":["O","O","O","B-person","O","O","B-person","I-person","O","O","B-location","I-location","I-location","O","O","O","B-organization","O","O","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","I-person","O","B-person","I-person","I-person","O","O","O","B-person","I-person","O","O","O","O","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["award","person","location","event","book","literary_genre","country","writer","organization","poem","magazine"]}
{"id":"247","dataset":"crossner_literature","split":"test","instance":{"id":"247","prompt_labels":"Another(O) well-known(O) version(O) of(O) the(O) play(O) is(O) Jedermann(B-book) by(O) the(O) Austrian(O) playwright(O) Hugo(B-writer) von(I-writer) Hofmannsthal(I-writer) ,(O) which(O) has(O) been(O) performed(O) annually(O) at(O) the(O) Salzburg(B-event) Festival(I-event) since(O) 1920(O) ,(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, poem, literary genre, country, writer, location, person, book, magazine, organization and O.\nSentence: Another well-known version of the play is Jedermann by the Austrian playwright Hugo von Hofmannsthal , which has been performed annually at the Salzburg Festival since 1920 , .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Another","well-known","version","of","the","play","is","Jedermann","by","the","Austrian","playwright","Hugo","von","Hofmannsthal",",","which","has","been","performed","annually","at","the","Salzburg","Festival","since","1920",",","."],"labels":["O","O","O","O","O","O","O","B-book","O","O","O","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","O","O","B-event","I-event","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","award","poem","literary_genre","country","writer","location","person","book","magazine","organization"]}
{"id":"249","dataset":"crossner_literature","split":"test","instance":{"id":"249","prompt_labels":"On(O) November(O) 17(O) ,(O) 2010(O) ,(O) Smith(B-writer) won(O) the(O) National(B-award) Book(I-award) Award(I-award) for(O) her(O) memoir(O) Just(B-book) Kids(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, literary genre, poem, location, award, magazine, writer, book, person, event and O.\nSentence: On November 17 , 2010 , Smith won the National Book Award for her memoir Just Kids .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","November","17",",","2010",",","Smith","won","the","National","Book","Award","for","her","memoir","Just","Kids","."],"labels":["O","O","O","O","O","O","B-writer","O","O","B-award","I-award","I-award","O","O","O","B-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["country","organization","literary_genre","poem","location","award","magazine","writer","book","person","event"]}
{"id":"250","dataset":"crossner_literature","split":"test","instance":{"id":"250","prompt_labels":"Writing(O) for(O) The(B-magazine) Spectator(I-magazine) ,(O) Graham(B-writer) Greene(I-writer) expressed(O) similar(O) views(O) ,(O) acerbically(O) noting(O) of(O) the(O) film(O) that(O) it(O) goes(O) on(O) too(O) long(O) ,(O) otherwise(O) it(O) might(O) have(O) been(O) the(O) funniest(O) film(O) since(O) The(O) Crusades(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, literary genre, country, poem, person, magazine, book, organization, event, award, location and O.\nSentence: Writing for The Spectator , Graham Greene expressed similar views , acerbically noting of the film that it goes on too long , otherwise it might have been the funniest film since The Crusades .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Writing","for","The","Spectator",",","Graham","Greene","expressed","similar","views",",","acerbically","noting","of","the","film","that","it","goes","on","too","long",",","otherwise","it","might","have","been","the","funniest","film","since","The","Crusades","."],"labels":["O","O","B-magazine","I-magazine","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["writer","literary_genre","country","poem","person","magazine","book","organization","event","award","location"]}
{"id":"252","dataset":"crossner_literature","split":"test","instance":{"id":"252","prompt_labels":"In(O) addition(O) to(O) the(O) Vietnam(B-event) War(I-event) ,(O) he(O) has(O) also(O) been(O) through(O) the(O) Iran(B-event) hostage(I-event) crisis(I-event) and(O) the(O) Gulf(B-event) War(I-event) ,(O) plus(O) a(O) number(O) of(O) missions(O) in(O) the(O) Soviet(B-country) Union(I-country) ,(O) and(O) claims(O) to(O) have(O) had(O) Abu(B-person) Nidal(I-person) '(O) s(O) head(O) in(O) my(O) gunsights(O) ,(O) but(O) never(O) got(O) the(O) green(O) light(O) allowing(O) him(O) to(O) kill(O) the(O) man(O) ((O) Clear(B-book) and(I-book) Present(I-book) Danger(I-book) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, country, location, organization, event, magazine, award, poem, literary genre, person, writer and O.\nSentence: In addition to the Vietnam War , he has also been through the Iran hostage crisis and the Gulf War , plus a number of missions in the Soviet Union , and claims to have had Abu Nidal ' s head in my gunsights , but never got the green light allowing him to kill the man ( Clear and Present Danger ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","addition","to","the","Vietnam","War",",","he","has","also","been","through","the","Iran","hostage","crisis","and","the","Gulf","War",",","plus","a","number","of","missions","in","the","Soviet","Union",",","and","claims","to","have","had","Abu","Nidal","'","s","head","in","my","gunsights",",","but","never","got","the","green","light","allowing","him","to","kill","the","man","(","Clear","and","Present","Danger",")","."],"labels":["O","O","O","O","B-event","I-event","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","B-event","I-event","O","O","O","O","O","O","O","O","B-country","I-country","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","I-book","O","O"],"target_index":null,"target_label":null},"label_list":["book","country","location","organization","event","magazine","award","poem","literary_genre","person","writer"]}
{"id":"254","dataset":"crossner_literature","split":"test","instance":{"id":"254","prompt_labels":"Le(B-writer) Guin(I-writer) refused(O) a(O) Nebula(B-award) Award(I-award) for(I-award) Best(I-award) Novelette(I-award) for(O) her(O) story(O) The(B-book) Diary(I-book) of(I-book) the(I-book) Rose(I-book) in(O) 1977(O) ,(O) in(O) protest(O) at(O) the(O) Science(B-organization) Fiction(I-organization) Writers(I-organization) of(I-organization) America(I-organization) '(O) s(O) revocation(O) of(O) Stanisaw(B-writer) Lem(I-writer) '(O) s(O) membership(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, literary genre, person, writer, award, poem, magazine, location, book, country, event and O.\nSentence: Le Guin refused a Nebula Award for Best Novelette for her story The Diary of the Rose in 1977 , in protest at the Science Fiction Writers of America ' s revocation of Stanisaw Lem ' s membership .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Le","Guin","refused","a","Nebula","Award","for","Best","Novelette","for","her","story","The","Diary","of","the","Rose","in","1977",",","in","protest","at","the","Science","Fiction","Writers","of","America","'","s","revocation","of","Stanisaw","Lem","'","s","membership","."],"labels":["B-writer","I-writer","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","B-writer","I-writer","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","literary_genre","person","writer","award","poem","magazine","location","book","country","event"]}
{"id":"255","dataset":"crossner_literature","split":"test","instance":{"id":"255","prompt_labels":"Sometimes(B-book) a(I-book) Great(I-book) Notion(I-book) inspired(O) a(O) 1970(O) film(O) starring(O) and(O) directed(O) by(O) Paul(B-person) Newman(I-person) ;(O) it(O) was(O) nominated(O) for(O) two(O) Academy(B-award) Awards(I-award) ,(O) and(O) in(O) 1972(O) was(O) the(O) first(O) film(O) shown(O) by(O) the(O) new(O) television(O) network(O) HBO(B-organization) ,(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, country, magazine, award, organization, book, literary genre, event, poem, person, location and O.\nSentence: Sometimes a Great Notion inspired a 1970 film starring and directed by Paul Newman ; it was nominated for two Academy Awards , and in 1972 was the first film shown by the new television network HBO ,","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Sometimes","a","Great","Notion","inspired","a","1970","film","starring","and","directed","by","Paul","Newman",";","it","was","nominated","for","two","Academy","Awards",",","and","in","1972","was","the","first","film","shown","by","the","new","television","network","HBO",","],"labels":["B-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","B-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","O"],"target_index":null,"target_label":null},"label_list":["writer","country","magazine","award","organization","book","literary_genre","event","poem","person","location"]}
{"id":"256","dataset":"crossner_literature","split":"test","instance":{"id":"256","prompt_labels":"At(O) the(O) 2012(O) Pride(B-award) of(I-award) Britain(I-award) Awards(I-award) shown(O) on(O) ITV(O) on(O) 30(O) October(O) ,(O) Fry(B-person) ,(O) along(O) with(O) Michael(B-person) Caine(I-person) ,(O) Elton(B-person) John(I-person) ,(O) Richard(B-person) Branson(I-person) and(O) Simon(B-person) Cowell(I-person) ,(O) recited(O) Rudyard(B-writer) Kipling(I-writer) '(O) s(O) poem(B-literary genre) If(B-poem) -(I-poem) in(O) tribute(O) to(O) the(O) 2012(O) British(O) Olympic(O) and(O) Paralympic(O) athletes(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, literary genre, book, organization, magazine, country, event, writer, poem, person and O.\nSentence: At the 2012 Pride of Britain Awards shown on ITV on 30 October , Fry , along with Michael Caine , Elton John , Richard Branson and Simon Cowell , recited Rudyard Kipling ' s poem If - in tribute to the 2012 British Olympic and Paralympic athletes .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["At","the","2012","Pride","of","Britain","Awards","shown","on","ITV","on","30","October",",","Fry",",","along","with","Michael","Caine",",","Elton","John",",","Richard","Branson","and","Simon","Cowell",",","recited","Rudyard","Kipling","'","s","poem","If","-","in","tribute","to","the","2012","British","Olympic","and","Paralympic","athletes","."],"labels":["O","O","O","B-award","I-award","I-award","I-award","O","O","O","O","O","O","O","B-person","O","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","O","B-writer","I-writer","O","O","B-literary genre","B-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","location","literary_genre","book","organization","magazine","country","event","writer","poem","person"]}
{"id":"259","dataset":"crossner_literature","split":"test","instance":{"id":"259","prompt_labels":"Director(O) Anatole(B-person) Litvak(I-person) ,(O) unhappy(O) with(O) the(O) script(O) submitted(O) by(O) Frank(B-writer) Partos(I-writer) and(O) Millen(B-writer) Brand(I-writer) for(O) The(O) Snake(O) Pit(O) ((O) 1948(O) )(O) ,(O) hired(O) Laurents(B-writer) to(O) rewrite(O) it(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, person, magazine, location, award, poem, organization, literary genre, writer, event, country and O.\nSentence: Director Anatole Litvak , unhappy with the script submitted by Frank Partos and Millen Brand for The Snake Pit ( 1948 ) , hired Laurents to rewrite it .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Director","Anatole","Litvak",",","unhappy","with","the","script","submitted","by","Frank","Partos","and","Millen","Brand","for","The","Snake","Pit","(","1948",")",",","hired","Laurents","to","rewrite","it","."],"labels":["O","B-person","I-person","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","B-writer","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["book","person","magazine","location","award","poem","organization","literary_genre","writer","event","country"]}
{"id":"260","dataset":"crossner_literature","split":"test","instance":{"id":"260","prompt_labels":"Writing(O) for(O) The(B-magazine) Spectator(I-magazine) in(O) 1936(O) ,(O) Graham(B-writer) Greene(I-writer) gave(O) the(O) film(O) a(O) tepid(O) review(O) ,(O) describing(O) it(O) on(O) the(O) one(O) hand(O) as(O) his(O) favorite(O) of(O) the(O) films(O) he(O) reviewed(O) that(O) week(O) ,(O) but(O) describing(O) it(O) as(O) a(O) fine(O) spirited(O) mix-up(O) and(O) making(O) pointed(O) note(O) of(O) the(O) magnificently(O) wrong(O) characterization(O) of(O) bad(O) King(B-person) James(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, writer, location, poem, event, book, magazine, country, literary genre, organization, person and O.\nSentence: Writing for The Spectator in 1936 , Graham Greene gave the film a tepid review , describing it on the one hand as his favorite of the films he reviewed that week , but describing it as a fine spirited mix-up and making pointed note of the magnificently wrong characterization of bad King James .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Writing","for","The","Spectator","in","1936",",","Graham","Greene","gave","the","film","a","tepid","review",",","describing","it","on","the","one","hand","as","his","favorite","of","the","films","he","reviewed","that","week",",","but","describing","it","as","a","fine","spirited","mix-up","and","making","pointed","note","of","the","magnificently","wrong","characterization","of","bad","King","James","."],"labels":["O","O","B-magazine","I-magazine","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["award","writer","location","poem","event","book","magazine","country","literary_genre","organization","person"]}
{"id":"261","dataset":"crossner_literature","split":"test","instance":{"id":"261","prompt_labels":"The(O) novels(B-literary genre) The(B-book) Book(I-book) of(I-book) Ptath(I-book) and(O) The(B-book) Weapon(I-book) Makers(I-book) both(O) appeared(O) in(O) magazines(O) in(O) serial(O) form(O) during(O) this(O) era(O) ;(O) they(O) were(O) later(O) published(O) in(O) book(O) form(O) after(O) World(B-event) War(I-event) II(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, literary genre, location, award, person, organization, country, writer, magazine, event, poem and O.\nSentence: The novels The Book of Ptath and The Weapon Makers both appeared in magazines in serial form during this era ; they were later published in book form after World War II .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","novels","The","Book","of","Ptath","and","The","Weapon","Makers","both","appeared","in","magazines","in","serial","form","during","this","era",";","they","were","later","published","in","book","form","after","World","War","II","."],"labels":["O","B-literary genre","B-book","I-book","I-book","I-book","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["book","literary_genre","location","award","person","organization","country","writer","magazine","event","poem"]}
{"id":"262","dataset":"crossner_literature","split":"test","instance":{"id":"262","prompt_labels":"Romanticism(B-literary genre) began(O) in(O) Portugal(B-country) with(O) the(O) publication(O) of(O) the(O) poem(B-literary genre) Cames(B-book) ((O) 1825(O) )(O) ,(O) by(O) Almeida(B-writer) Garrett(I-writer) ,(O) who(O) was(O) raised(O) by(O) his(O) uncle(O) D.(B-person) Alexandre(I-person) ,(O) bishop(O) of(O) Angra(O) ,(O) in(O) the(O) precepts(O) of(O) Neoclassicism(B-literary genre) ,(O) which(O) can(O) be(O) observed(O) in(O) his(O) early(O) work(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, book, location, country, award, person, literary genre, magazine, writer, poem and O.\nSentence: Romanticism began in Portugal with the publication of the poem Cames ( 1825 ) , by Almeida Garrett , who was raised by his uncle D. Alexandre , bishop of Angra , in the precepts of Neoclassicism , which can be observed in his early work .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Romanticism","began","in","Portugal","with","the","publication","of","the","poem","Cames","(","1825",")",",","by","Almeida","Garrett",",","who","was","raised","by","his","uncle","D.","Alexandre",",","bishop","of","Angra",",","in","the","precepts","of","Neoclassicism",",","which","can","be","observed","in","his","early","work","."],"labels":["B-literary genre","O","O","B-country","O","O","O","O","O","B-literary genre","B-book","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","B-literary genre","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","organization","book","location","country","award","person","literary_genre","magazine","writer","poem"]}
{"id":"263","dataset":"crossner_literature","split":"test","instance":{"id":"263","prompt_labels":"The(O) film(O) is(O) about(O) feuding(O) gangsters(O) in(O) the(O) Prohibition(O) era(O) ,(O) inspired(O) by(O) Dashiell(B-writer) Hammett(I-writer) '(O) s(O) novels(B-literary genre) Red(B-book) Harvest(I-book) ((O) 1929(O) )(O) and(O) The(B-book) Glass(I-book) Key(I-book) ((O) serialized(O) in(O) 1930(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, location, book, magazine, award, person, literary genre, event, writer, poem and O.\nSentence: The film is about feuding gangsters in the Prohibition era , inspired by Dashiell Hammett ' s novels Red Harvest ( 1929 ) and The Glass Key ( serialized in 1930 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","film","is","about","feuding","gangsters","in","the","Prohibition","era",",","inspired","by","Dashiell","Hammett","'","s","novels","Red","Harvest","(","1929",")","and","The","Glass","Key","(","serialized","in","1930",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","B-literary genre","B-book","I-book","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","organization","location","book","magazine","award","person","literary_genre","event","writer","poem"]}
{"id":"264","dataset":"crossner_literature","split":"test","instance":{"id":"264","prompt_labels":"A(O) different(O) account(O) in(O) Virgil(B-writer) '(O) s(O) Aeneid(B-poem) ((O) 3.163f(O) )(O) has(O) Aeneas(B-person) in(O) a(O) dream(O) learn(O) from(O) his(O) ancestral(O) Penates(B-person) that(O) Dardanus(B-person) and(O) Father(B-person) Iasius(I-person) and(O) the(O) Penates(B-person) themselves(O) originally(O) came(O) from(O) Hesperia(B-location) ,(O) afterwards(O) renamed(O) as(O) Italy(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, organization, award, location, person, poem, writer, magazine, event, book, country and O.\nSentence: A different account in Virgil ' s Aeneid ( 3.163f ) has Aeneas in a dream learn from his ancestral Penates that Dardanus and Father Iasius and the Penates themselves originally came from Hesperia , afterwards renamed as Italy .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","different","account","in","Virgil","'","s","Aeneid","(","3.163f",")","has","Aeneas","in","a","dream","learn","from","his","ancestral","Penates","that","Dardanus","and","Father","Iasius","and","the","Penates","themselves","originally","came","from","Hesperia",",","afterwards","renamed","as","Italy","."],"labels":["O","O","O","O","B-writer","O","O","B-poem","O","O","O","O","B-person","O","O","O","O","O","O","O","B-person","O","B-person","O","B-person","I-person","O","O","B-person","O","O","O","O","B-location","O","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","organization","award","location","person","poem","writer","magazine","event","book","country"]}
{"id":"265","dataset":"crossner_literature","split":"test","instance":{"id":"265","prompt_labels":"Barry(O) Lyndon(O) won(O) four(O) Oscars(B-award) at(O) the(O) 48th(B-event) Academy(I-event) Awards(I-event) :(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Score(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Costume(I-award) Design(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Production(I-award) Design(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Cinematography(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, literary genre, poem, magazine, event, organization, book, writer, country, location and O.\nSentence: Barry Lyndon won four Oscars at the 48th Academy Awards : Academy Award for Best Original Score , Academy Award for Best Costume Design , Academy Award for Best Production Design and Academy Award for Best Cinematography .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Barry","Lyndon","won","four","Oscars","at","the","48th","Academy","Awards",":","Academy","Award","for","Best","Original","Score",",","Academy","Award","for","Best","Costume","Design",",","Academy","Award","for","Best","Production","Design","and","Academy","Award","for","Best","Cinematography","."],"labels":["O","O","O","O","B-award","O","O","B-event","I-event","I-event","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["person","award","literary_genre","poem","magazine","event","organization","book","writer","country","location"]}
{"id":"266","dataset":"crossner_literature","split":"test","instance":{"id":"266","prompt_labels":"The(O) sequel(O) The(B-book) Voyages(I-book) of(I-book) Doctor(I-book) Dolittle(I-book) ((O) 1922(O) )(O) won(O) Lofting(B-award) the(O) prestigious(O) Newbery(B-award) Medal(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, writer, award, poem, organization, magazine, literary genre, event, country, book and O.\nSentence: The sequel The Voyages of Doctor Dolittle ( 1922 ) won Lofting the prestigious Newbery Medal .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","sequel","The","Voyages","of","Doctor","Dolittle","(","1922",")","won","Lofting","the","prestigious","Newbery","Medal","."],"labels":["O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-award","O","O","B-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["location","person","writer","award","poem","organization","magazine","literary_genre","event","country","book"]}
{"id":"267","dataset":"crossner_literature","split":"test","instance":{"id":"267","prompt_labels":"The(O) episode(O) depicts(O) the(O) attempted(O) assassination(O) of(O) Warhol(B-writer) by(O) Valerie(B-writer) Solanas(I-writer) ((O) Lena(B-person) Dunham(I-person) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, location, country, literary genre, award, event, book, poem, organization, writer, person and O.\nSentence: The episode depicts the attempted assassination of Warhol by Valerie Solanas ( Lena Dunham ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","episode","depicts","the","attempted","assassination","of","Warhol","by","Valerie","Solanas","(","Lena","Dunham",")","."],"labels":["O","O","O","O","O","O","O","B-writer","O","B-writer","I-writer","O","B-person","I-person","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","location","country","literary_genre","award","event","book","poem","organization","writer","person"]}
{"id":"268","dataset":"crossner_literature","split":"test","instance":{"id":"268","prompt_labels":"The(O) series(O) was(O) greatly(O) admired(O) by(O) such(O) notable(O) writers(O) and(O) critics(O) of(O) mystery(B-literary genre) and(I-literary genre) detective(I-literary genre) fiction(I-literary genre) as(O) Ellery(B-writer) Queen(I-writer) ((O) Frederic(B-writer) Dannay(I-writer) )(O) ,(O) Anthony(B-writer) Boucher(I-writer) ,(O) Vincent(B-writer) Starrett(I-writer) and(O) Howard(B-writer) Haycraft(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, organization, literary genre, award, country, poem, writer, event, person, location, book and O.\nSentence: The series was greatly admired by such notable writers and critics of mystery and detective fiction as Ellery Queen ( Frederic Dannay ) , Anthony Boucher , Vincent Starrett and Howard Haycraft .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","series","was","greatly","admired","by","such","notable","writers","and","critics","of","mystery","and","detective","fiction","as","Ellery","Queen","(","Frederic","Dannay",")",",","Anthony","Boucher",",","Vincent","Starrett","and","Howard","Haycraft","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","I-literary genre","I-literary genre","O","B-writer","I-writer","O","B-writer","I-writer","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["magazine","organization","literary_genre","award","country","poem","writer","event","person","location","book"]}
{"id":"270","dataset":"crossner_literature","split":"test","instance":{"id":"270","prompt_labels":"Using(O) donations(O) and(O) sales(O) of(O) their(O) magazine(O) ,(O) Skeptical(B-magazine) Inquirer(I-magazine) ,(O) they(O) and(O) secular(O) humanist(O) philosopher(O) Paul(B-person) Kurtz(I-person) took(O) seats(O) on(O) the(O) executive(O) board(O) ,(O) with(O) Isaac(B-writer) Asimov(I-writer) and(O) Carl(B-person) Sagan(I-person) joining(O) as(O) founding(O) members(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, organization, magazine, award, book, person, writer, literary genre, poem, location and O.\nSentence: Using donations and sales of their magazine , Skeptical Inquirer , they and secular humanist philosopher Paul Kurtz took seats on the executive board , with Isaac Asimov and Carl Sagan joining as founding members .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Using","donations","and","sales","of","their","magazine",",","Skeptical","Inquirer",",","they","and","secular","humanist","philosopher","Paul","Kurtz","took","seats","on","the","executive","board",",","with","Isaac","Asimov","and","Carl","Sagan","joining","as","founding","members","."],"labels":["O","O","O","O","O","O","O","O","B-magazine","I-magazine","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","B-writer","I-writer","O","B-person","I-person","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","event","organization","magazine","award","book","person","writer","literary_genre","poem","location"]}
{"id":"271","dataset":"crossner_literature","split":"test","instance":{"id":"271","prompt_labels":"Gygax(B-writer) wrote(O) the(O) supplements(O) Greyhawk(B-book) ,(O) Eldritch(B-book) Wizardry(I-book) ,(O) and(O) Swords(B-book) &(I-book) Spells(I-book) for(O) the(O) original(O) D(O) &(O) D(O) game(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, literary genre, book, award, person, event, location, magazine, writer, poem and O.\nSentence: Gygax wrote the supplements Greyhawk , Eldritch Wizardry , and Swords & Spells for the original D & D game .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Gygax","wrote","the","supplements","Greyhawk",",","Eldritch","Wizardry",",","and","Swords","&","Spells","for","the","original","D","&","D","game","."],"labels":["B-writer","O","O","O","B-book","O","B-book","I-book","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","country","literary_genre","book","award","person","event","location","magazine","writer","poem"]}
{"id":"273","dataset":"crossner_literature","split":"test","instance":{"id":"273","prompt_labels":"His(O) first(O) story(O) to(O) attract(O) major(O) attention(O) was(O) A(B-book) Rose(I-book) for(I-book) Ecclesiastes(I-book) ,(O) published(O) in(O) The(B-magazine) Magazine(I-magazine) of(I-magazine) Fantasy(I-magazine) &(I-magazine) Science(I-magazine) Fiction(I-magazine) ,(O) with(O) cover(O) art(O) by(O) Hannes(B-writer) Bok(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, writer, book, event, poem, organization, location, award, person, country, magazine and O.\nSentence: His first story to attract major attention was A Rose for Ecclesiastes , published in The Magazine of Fantasy & Science Fiction , with cover art by Hannes Bok .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","first","story","to","attract","major","attention","was","A","Rose","for","Ecclesiastes",",","published","in","The","Magazine","of","Fantasy","&","Science","Fiction",",","with","cover","art","by","Hannes","Bok","."],"labels":["O","O","O","O","O","O","O","O","B-book","I-book","I-book","I-book","O","O","O","B-magazine","I-magazine","I-magazine","I-magazine","I-magazine","I-magazine","I-magazine","O","O","O","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","writer","book","event","poem","organization","location","award","person","country","magazine"]}
{"id":"274","dataset":"crossner_literature","split":"test","instance":{"id":"274","prompt_labels":"Capp(B-writer) also(O) freelanced(O) very(O) successfully(O) as(O) a(O) magazine(O) writer(O) and(O) newspaper(O) columnist(O) ,(O) in(O) a(O) wide(O) variety(O) of(O) publications(O) including(O) Life(B-magazine) ,(O) Show(B-magazine) ,(O) Pageant(B-magazine) ,(O) The(B-magazine) Atlantic(I-magazine) ,(O) Esquire(B-magazine) ,(O) Coronet(B-magazine) ,(O) and(O) The(B-magazine) Saturday(I-magazine) Evening(I-magazine) Post(I-magazine) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, literary genre, country, event, book, writer, magazine, poem, location, organization and O.\nSentence: Capp also freelanced very successfully as a magazine writer and newspaper columnist , in a wide variety of publications including Life , Show , Pageant , The Atlantic , Esquire , Coronet , and The Saturday Evening Post .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Capp","also","freelanced","very","successfully","as","a","magazine","writer","and","newspaper","columnist",",","in","a","wide","variety","of","publications","including","Life",",","Show",",","Pageant",",","The","Atlantic",",","Esquire",",","Coronet",",","and","The","Saturday","Evening","Post","."],"labels":["B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-magazine","O","B-magazine","O","B-magazine","O","B-magazine","I-magazine","O","B-magazine","O","B-magazine","O","O","B-magazine","I-magazine","I-magazine","I-magazine","O"],"target_index":null,"target_label":null},"label_list":["award","person","literary_genre","country","event","book","writer","magazine","poem","location","organization"]}
{"id":"277","dataset":"crossner_literature","split":"test","instance":{"id":"277","prompt_labels":"Pauline(B-writer) Kael(I-writer) of(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) wrote(O) that(O) Kubrick(B-writer) has(O) taken(O) a(O) quick-witted(O) story(O) and(O) controlled(O) it(O) so(O) meticulously(O) that(O) he(O) 's(O) drained(O) the(O) blood(O) out(O) of(O) it(O) ,(O) adding(O) ,(O) It(O) 's(O) a(O) coffee-table(O) movie(O) ;(O) we(O) might(O) as(O) well(O) be(O) at(O) a(O) three-hour(O) slide(O) show(O) for(O) art-history(O) majors(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, poem, event, book, country, person, magazine, writer, location, literary genre and O.\nSentence: Pauline Kael of The New Yorker wrote that Kubrick has taken a quick-witted story and controlled it so meticulously that he 's drained the blood out of it , adding , It 's a coffee-table movie ; we might as well be at a three-hour slide show for art-history majors .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Pauline","Kael","of","The","New","Yorker","wrote","that","Kubrick","has","taken","a","quick-witted","story","and","controlled","it","so","meticulously","that","he","'s","drained","the","blood","out","of","it",",","adding",",","It","'s","a","coffee-table","movie",";","we","might","as","well","be","at","a","three-hour","slide","show","for","art-history","majors","."],"labels":["B-writer","I-writer","O","B-magazine","I-magazine","I-magazine","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","award","poem","event","book","country","person","magazine","writer","location","literary_genre"]}
{"id":"280","dataset":"crossner_literature","split":"test","instance":{"id":"280","prompt_labels":"Gaiman(B-writer) 's(O) 2009(O) Newbery(B-award) Medal(I-award) winning(O) book(O) The(B-book) Graveyard(I-book) Book(I-book) will(O) be(O) made(O) into(O) a(O) movie(O) ,(O) with(O) Ron(B-person) Howard(I-person) as(O) the(O) director(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, magazine, event, writer, poem, country, literary genre, location, award, person, organization and O.\nSentence: Gaiman 's 2009 Newbery Medal winning book The Graveyard Book will be made into a movie , with Ron Howard as the director .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Gaiman","'s","2009","Newbery","Medal","winning","book","The","Graveyard","Book","will","be","made","into","a","movie",",","with","Ron","Howard","as","the","director","."],"labels":["B-writer","O","O","B-award","I-award","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["book","magazine","event","writer","poem","country","literary_genre","location","award","person","organization"]}
{"id":"281","dataset":"crossner_literature","split":"test","instance":{"id":"281","prompt_labels":"In(O) 1988(O) ,(O) Berkoff(B-person) directed(O) an(O) interpretation(O) of(O) Salome(O) by(O) Oscar(B-writer) Wilde(I-writer) ,(O) performed(O) in(O) slow(O) motion(O) ,(O) at(O) the(O) Gate(B-location) Theatre(I-location) ,(O) Dublin(B-location) ..(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, poem, location, writer, literary genre, organization, person, award, magazine, book, event and O.\nSentence: In 1988 , Berkoff directed an interpretation of Salome by Oscar Wilde , performed in slow motion , at the Gate Theatre , Dublin ..","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1988",",","Berkoff","directed","an","interpretation","of","Salome","by","Oscar","Wilde",",","performed","in","slow","motion",",","at","the","Gate","Theatre",",","Dublin",".."],"labels":["O","O","O","B-person","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["country","poem","location","writer","literary_genre","organization","person","award","magazine","book","event"]}
{"id":"282","dataset":"crossner_literature","split":"test","instance":{"id":"282","prompt_labels":"The(O) Professor(O) and(O) the(O) Woggle-Bug(B-person) try(O) to(O) dissuade(O) the(O) girls(O) from(O) war(O) ,(O) the(O) Woggle-Bug(B-person) saying(O) that(O) it(O) is(O) better(O) to(O) be(O) a(O) Maud(B-poem) Muller(I-poem) than(O) a(O) Carrie(B-person) Nation(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, writer, event, literary genre, country, person, award, book, poem, magazine, location and O.\nSentence: The Professor and the Woggle-Bug try to dissuade the girls from war , the Woggle-Bug saying that it is better to be a Maud Muller than a Carrie Nation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Professor","and","the","Woggle-Bug","try","to","dissuade","the","girls","from","war",",","the","Woggle-Bug","saying","that","it","is","better","to","be","a","Maud","Muller","than","a","Carrie","Nation","."],"labels":["O","O","O","O","B-person","O","O","O","O","O","O","O","O","O","B-person","O","O","O","O","O","O","O","O","B-poem","I-poem","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["organization","writer","event","literary_genre","country","person","award","book","poem","magazine","location"]}
{"id":"283","dataset":"crossner_literature","split":"test","instance":{"id":"283","prompt_labels":"They(O) continued(O) to(O) work(O) in(O) sf(B-literary genre) and(O) fantasy(B-literary genre) ,(O) and(O) their(O) works(O) include(O) two(O) frequently(O) anthologized(O) sf(B-literary genre) classics(O) :(O) Mimsy(B-book) Were(I-book) the(I-book) Borogoves(I-book) ((O) February(O) 1943(O) )(O) ,(O) the(O) basis(O) for(O) the(O) film(O) The(O) Last(O) Mimzy(O) ((O) 2007(O) )(O) ,(O) and(O) Vintage(O) Season(O) ((O) September(O) 1946(O) )(O) ,(O) the(O) basis(O) for(O) the(O) film(O) Timescape(O) ((O) 1992(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, literary genre, writer, location, book, magazine, country, person, poem, award and O.\nSentence: They continued to work in sf and fantasy , and their works include two frequently anthologized sf classics : Mimsy Were the Borogoves ( February 1943 ) , the basis for the film The Last Mimzy ( 2007 ) , and Vintage Season ( September 1946 ) , the basis for the film Timescape ( 1992 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["They","continued","to","work","in","sf","and","fantasy",",","and","their","works","include","two","frequently","anthologized","sf","classics",":","Mimsy","Were","the","Borogoves","(","February","1943",")",",","the","basis","for","the","film","The","Last","Mimzy","(","2007",")",",","and","Vintage","Season","(","September","1946",")",",","the","basis","for","the","film","Timescape","(","1992",")","."],"labels":["O","O","O","O","O","B-literary genre","O","B-literary genre","O","O","O","O","O","O","O","O","B-literary genre","O","O","B-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","organization","literary_genre","writer","location","book","magazine","country","person","poem","award"]}
{"id":"288","dataset":"crossner_literature","split":"test","instance":{"id":"288","prompt_labels":"These(O) styles(O) included(O) the(O) historical(B-literary genre) fiction(I-literary genre) of(O) Memories(B-book) of(I-book) the(I-book) Ford(I-book) Administration(I-book) ((O) 1992(O) )(O) ,(O) the(O) magical(B-literary genre) realism(I-literary genre) of(O) Brazil(B-book) ((O) 1994(O) )(O) ,(O) the(O) science(B-literary genre) fiction(I-literary genre) of(O) Toward(B-book) the(I-book) End(I-book) of(I-book) Time(I-book) ((O) 1997(O) )(O) ,(O) the(O) postmodernism(B-literary genre) of(O) Gertrude(B-book) and(I-book) Claudius(I-book) ((O) 2000(O) )(O) ,(O) and(O) the(O) experimental(B-literary genre) fiction(I-literary genre) of(O) Seek(B-book) My(I-book) Face(I-book) ((O) 2002(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, person, location, event, award, organization, writer, magazine, literary genre, country, book and O.\nSentence: These styles included the historical fiction of Memories of the Ford Administration ( 1992 ) , the magical realism of Brazil ( 1994 ) , the science fiction of Toward the End of Time ( 1997 ) , the postmodernism of Gertrude and Claudius ( 2000 ) , and the experimental fiction of Seek My Face ( 2002 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["These","styles","included","the","historical","fiction","of","Memories","of","the","Ford","Administration","(","1992",")",",","the","magical","realism","of","Brazil","(","1994",")",",","the","science","fiction","of","Toward","the","End","of","Time","(","1997",")",",","the","postmodernism","of","Gertrude","and","Claudius","(","2000",")",",","and","the","experimental","fiction","of","Seek","My","Face","(","2002",")","."],"labels":["O","O","O","O","B-literary genre","I-literary genre","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","O","B-literary genre","I-literary genre","O","B-book","O","O","O","O","O","B-literary genre","I-literary genre","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","O","B-literary genre","O","B-book","I-book","I-book","O","O","O","O","O","O","B-literary genre","I-literary genre","O","B-book","I-book","I-book","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","person","location","event","award","organization","writer","magazine","literary_genre","country","book"]}
{"id":"290","dataset":"crossner_literature","split":"test","instance":{"id":"290","prompt_labels":"Although(O) Friedman(B-writer) never(O) visited(O) Estonia(B-country) ,(O) his(O) book(O) Free(B-book) to(I-book) Choose(I-book) exercised(O) a(O) great(O) influence(O) on(O) that(O) nation(O) 's(O) then(O) 32-year-old(O) prime(O) minister(O) ,(O) Mart(B-person) Laar(I-person) ,(O) who(O) has(O) claimed(O) that(O) it(O) was(O) the(O) only(O) book(O) on(O) economics(O) he(O) had(O) read(O) before(O) taking(O) office(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, book, writer, award, country, poem, literary genre, organization, person, magazine and O.\nSentence: Although Friedman never visited Estonia , his book Free to Choose exercised a great influence on that nation 's then 32-year-old prime minister , Mart Laar , who has claimed that it was the only book on economics he had read before taking office .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Although","Friedman","never","visited","Estonia",",","his","book","Free","to","Choose","exercised","a","great","influence","on","that","nation","'s","then","32-year-old","prime","minister",",","Mart","Laar",",","who","has","claimed","that","it","was","the","only","book","on","economics","he","had","read","before","taking","office","."],"labels":["O","B-writer","O","O","B-country","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","event","book","writer","award","country","poem","literary_genre","organization","person","magazine"]}
{"id":"291","dataset":"crossner_literature","split":"test","instance":{"id":"291","prompt_labels":"St.(B-organization) Columba(I-organization) 's(I-organization) School(I-organization) one(O) of(O) the(O) most(O) prominent(O) English-Medium(O) schools(O) in(O) India(B-country) run(O) by(O) the(O) Congregation(B-organization) of(I-organization) Christian(I-organization) Brothers(I-organization) is(O) also(O) named(O) after(O) the(O) saint(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, poem, book, literary genre, location, magazine, writer, organization, country, person, award and O.\nSentence: St. Columba 's School one of the most prominent English-Medium schools in India run by the Congregation of Christian Brothers is also named after the saint .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["St.","Columba","'s","School","one","of","the","most","prominent","English-Medium","schools","in","India","run","by","the","Congregation","of","Christian","Brothers","is","also","named","after","the","saint","."],"labels":["B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","B-country","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","poem","book","literary_genre","location","magazine","writer","organization","country","person","award"]}
{"id":"292","dataset":"crossner_literature","split":"test","instance":{"id":"292","prompt_labels":"Among(O) lyric(O) poets(O) ,(O) the(O) most(O) important(O) figures(O) are(O) Anne(B-writer) Bradstreet(I-writer) ,(O) who(O) wrote(O) personal(O) poems(O) about(O) her(O) family(O) and(O) homelife(O) ;(O) pastor(O) Edward(B-writer) Taylor(I-writer) ,(O) whose(O) best(O) poems(O) ,(O) the(O) Preparatory(B-poem) Meditations(I-poem) ,(O) were(O) written(O) to(O) help(O) him(O) prepare(O) for(O) leading(O) worship(O) ;(O) and(O) Michael(B-writer) Wigglesworth(I-writer) ,(O) whose(O) best-selling(O) poem(B-literary genre) ,(O) The(B-poem) Day(I-poem) of(I-poem) Doom(I-poem) ((O) 1660(O) )(O) ,(O) describes(O) the(O) time(O) of(O) judgment(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, writer, country, award, organization, event, book, location, literary genre, person, poem and O.\nSentence: Among lyric poets , the most important figures are Anne Bradstreet , who wrote personal poems about her family and homelife ; pastor Edward Taylor , whose best poems , the Preparatory Meditations , were written to help him prepare for leading worship ; and Michael Wigglesworth , whose best-selling poem , The Day of Doom ( 1660 ) , describes the time of judgment .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Among","lyric","poets",",","the","most","important","figures","are","Anne","Bradstreet",",","who","wrote","personal","poems","about","her","family","and","homelife",";","pastor","Edward","Taylor",",","whose","best","poems",",","the","Preparatory","Meditations",",","were","written","to","help","him","prepare","for","leading","worship",";","and","Michael","Wigglesworth",",","whose","best-selling","poem",",","The","Day","of","Doom","(","1660",")",",","describes","the","time","of","judgment","."],"labels":["O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","B-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","B-literary genre","O","B-poem","I-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","writer","country","award","organization","event","book","location","literary_genre","person","poem"]}
{"id":"293","dataset":"crossner_literature","split":"test","instance":{"id":"293","prompt_labels":"197(O) ,(O) Oxford(B-organization) University(I-organization) Press(I-organization) ,(O) 1969(O) and(O) gave(O) lectures(O) on(O) Human(B-book) Potentialities(I-book) both(O) at(O) the(O) UCSF(B-location) Medical(I-location) Center(I-location) and(O) at(O) the(O) Esalen(B-location) Institute(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, award, book, location, magazine, organization, literary genre, writer, country, poem and O.\nSentence: 197 , Oxford University Press , 1969 and gave lectures on Human Potentialities both at the UCSF Medical Center and at the Esalen Institute .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["197",",","Oxford","University","Press",",","1969","and","gave","lectures","on","Human","Potentialities","both","at","the","UCSF","Medical","Center","and","at","the","Esalen","Institute","."],"labels":["O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","B-book","I-book","O","O","O","B-location","I-location","I-location","O","O","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["event","person","award","book","location","magazine","organization","literary_genre","writer","country","poem"]}
{"id":"296","dataset":"crossner_literature","split":"test","instance":{"id":"296","prompt_labels":"Frances(B-writer) Yates(I-writer) in(O) her(O) 1966(O) study(O) The(B-book) Art(I-book) of(I-book) Memory(I-book) argues(O) that(O) a(O) brief(O) passage(O) of(O) the(O) Confessions(B-book) ,(O) 10.8.12(O) ,(O) in(O) which(O) Augustine(B-writer) writes(O) of(O) walking(O) up(O) a(O) flight(O) of(O) stairs(O) and(O) entering(O) the(O) vast(O) fields(O) of(O) memory(O) technique(O) for(O) organizing(O) large(O) amounts(O) of(O) information(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, country, poem, person, event, award, organization, magazine, writer, location, literary genre and O.\nSentence: Frances Yates in her 1966 study The Art of Memory argues that a brief passage of the Confessions , 10.8.12 , in which Augustine writes of walking up a flight of stairs and entering the vast fields of memory technique for organizing large amounts of information .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Frances","Yates","in","her","1966","study","The","Art","of","Memory","argues","that","a","brief","passage","of","the","Confessions",",","10.8.12",",","in","which","Augustine","writes","of","walking","up","a","flight","of","stairs","and","entering","the","vast","fields","of","memory","technique","for","organizing","large","amounts","of","information","."],"labels":["B-writer","I-writer","O","O","O","O","B-book","I-book","I-book","I-book","O","O","O","O","O","O","O","B-book","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["book","country","poem","person","event","award","organization","magazine","writer","location","literary_genre"]}
{"id":"297","dataset":"crossner_literature","split":"test","instance":{"id":"297","prompt_labels":"Daniel(B-book) 11(I-book) :(O) A(O) future(O) king(O) of(O) Persia(O) will(O) make(O) war(O) on(O) the(O) king(O) of(O) Greece(B-country) ,(O) a(O) mighty(O) king(O) will(O) arise(O) and(O) wield(O) power(O) until(O) his(O) empire(O) is(O) broken(O) up(O) and(O) given(O) to(O) others(O) ,(O) and(O) finally(O) the(O) king(O) of(O) the(O) south(O) ((O) identified(O) in(O) verse(O) 8(O) as(O) Egypt(B-country) )(O) will(O) go(O) to(O) war(O) with(O) the(O) king(O) of(O) the(O) north(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, organization, country, location, writer, person, event, magazine, poem, book, literary genre and O.\nSentence: Daniel 11 : A future king of Persia will make war on the king of Greece , a mighty king will arise and wield power until his empire is broken up and given to others , and finally the king of the south ( identified in verse 8 as Egypt ) will go to war with the king of the north .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Daniel","11",":","A","future","king","of","Persia","will","make","war","on","the","king","of","Greece",",","a","mighty","king","will","arise","and","wield","power","until","his","empire","is","broken","up","and","given","to","others",",","and","finally","the","king","of","the","south","(","identified","in","verse","8","as","Egypt",")","will","go","to","war","with","the","king","of","the","north","."],"labels":["B-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","organization","country","location","writer","person","event","magazine","poem","book","literary_genre"]}
{"id":"298","dataset":"crossner_literature","split":"test","instance":{"id":"298","prompt_labels":"Penelope(B-writer) Gilliatt(I-writer) of(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) was(O) negative(O) ,(O) writing(O) ,(O) This(O) pompously(O) prophetic(O) thing(O) of(O) a(O) film(O) hasn(O) 't(O) a(O) brain(O) in(O) its(O) beanbag(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, poem, literary genre, country, writer, book, magazine, event, organization, location, award and O.\nSentence: Penelope Gilliatt of The New Yorker was negative , writing , This pompously prophetic thing of a film hasn 't a brain in its beanbag .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Penelope","Gilliatt","of","The","New","Yorker","was","negative",",","writing",",","This","pompously","prophetic","thing","of","a","film","hasn","'t","a","brain","in","its","beanbag","."],"labels":["B-writer","I-writer","O","B-magazine","I-magazine","I-magazine","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","poem","literary_genre","country","writer","book","magazine","event","organization","location","award"]}
{"id":"299","dataset":"crossner_literature","split":"test","instance":{"id":"299","prompt_labels":"His(O) poem(B-literary genre) ,(O) Jai(B-poem) Jai(I-poem) Garavi(I-poem) Gujarat(I-poem) ((O) 1873(O) )(O) ,(O) is(O) used(O) as(O) a(O) de(O) facto(O) state(O) song(O) for(O) Gujarat(B-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, literary genre, location, magazine, person, writer, country, book, poem, event, organization and O.\nSentence: His poem , Jai Jai Garavi Gujarat ( 1873 ) , is used as a de facto state song for Gujarat .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","poem",",","Jai","Jai","Garavi","Gujarat","(","1873",")",",","is","used","as","a","de","facto","state","song","for","Gujarat","."],"labels":["O","B-literary genre","O","B-poem","I-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","O","O","B-poem","O"],"target_index":null,"target_label":null},"label_list":["award","literary_genre","location","magazine","person","writer","country","book","poem","event","organization"]}
{"id":"300","dataset":"crossner_literature","split":"test","instance":{"id":"300","prompt_labels":"Only(O) three(O) Middle(O) English(O) texts(O) have(O) survived(O) in(O) more(O) copies(O) :(O) Piers(B-poem) Plowman(I-poem) ,(O) The(B-book) Canterbury(I-book) Tales(I-book) and(O) the(O) Prick(B-poem) of(I-poem) Conscience(I-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, organization, person, country, event, book, writer, poem, literary genre, magazine, location and O.\nSentence: Only three Middle English texts have survived in more copies : Piers Plowman , The Canterbury Tales and the Prick of Conscience .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Only","three","Middle","English","texts","have","survived","in","more","copies",":","Piers","Plowman",",","The","Canterbury","Tales","and","the","Prick","of","Conscience","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-poem","I-poem","O","B-book","I-book","I-book","O","O","B-poem","I-poem","I-poem","O"],"target_index":null,"target_label":null},"label_list":["award","organization","person","country","event","book","writer","poem","literary_genre","magazine","location"]}
{"id":"301","dataset":"crossner_literature","split":"test","instance":{"id":"301","prompt_labels":"During(O) the(O) 1980s(O) ,(O) Rumsfeld(B-person) became(O) a(O) member(O) of(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Public(I-organization) Administration(I-organization) ,(O) and(O) was(O) named(O) a(O) member(O) of(O) the(O) boards(O) of(O) trustees(O) of(O) the(O) Gerald(B-organization) R.(I-organization) Ford(I-organization) Foundation(I-organization) ,(O) the(O) Eisenhower(B-organization) Exchange(I-organization) Fellowships(I-organization) ,(O) the(O) Hoover(B-organization) Institution(I-organization) at(O) Stanford(B-organization) University(I-organization) and(O) the(O) National(B-organization) Park(I-organization) Foundation(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, literary genre, writer, event, organization, person, award, magazine, location, poem, book and O.\nSentence: During the 1980s , Rumsfeld became a member of the National Academy of Public Administration , and was named a member of the boards of trustees of the Gerald R. Ford Foundation , the Eisenhower Exchange Fellowships , the Hoover Institution at Stanford University and the National Park Foundation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","the","1980s",",","Rumsfeld","became","a","member","of","the","National","Academy","of","Public","Administration",",","and","was","named","a","member","of","the","boards","of","trustees","of","the","Gerald","R.","Ford","Foundation",",","the","Eisenhower","Exchange","Fellowships",",","the","Hoover","Institution","at","Stanford","University","and","the","National","Park","Foundation","."],"labels":["O","O","O","O","B-person","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["country","literary_genre","writer","event","organization","person","award","magazine","location","poem","book"]}
{"id":"302","dataset":"crossner_literature","split":"test","instance":{"id":"302","prompt_labels":"Naguib(B-writer) Mahfouz(I-writer) ((O) 1911(O) &(O) ndash(O) ;(O) 2006(O) )(O) was(O) an(O) Egyptian(O) writer(O) who(O) won(O) the(O) 1988(O) Nobel(B-award) Prize(I-award) for(I-award) Literature(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, book, person, magazine, organization, award, location, writer, country, poem, event and O.\nSentence: Naguib Mahfouz ( 1911 & ndash ; 2006 ) was an Egyptian writer who won the 1988 Nobel Prize for Literature .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Naguib","Mahfouz","(","1911","&","ndash",";","2006",")","was","an","Egyptian","writer","who","won","the","1988","Nobel","Prize","for","Literature","."],"labels":["B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","book","person","magazine","organization","award","location","writer","country","poem","event"]}
{"id":"303","dataset":"crossner_literature","split":"test","instance":{"id":"303","prompt_labels":"Chaucer(B-writer) 's(O) sources(O) for(O) the(O) legends(O) include(O) :(O) Virgil(B-writer) '(O) s(O) Aeneid(B-poem) ,(O) Vincent(B-writer) of(I-writer) Beauvais(I-writer) ,(O) Guido(B-writer) delle(I-writer) Colonne(I-writer) '(O) s(O) Historia(B-poem) destructionis(I-poem) Troiae(I-poem) ,(O) Gaius(B-writer) Julius(I-writer) Hyginus(I-writer) '(O) Fabulae(B-poem) and(O) Ovid(B-writer) '(O) s(O) Metamorphoses(B-poem) and(O) Heroides(B-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, organization, person, poem, event, award, country, literary genre, location, writer, book and O.\nSentence: Chaucer 's sources for the legends include : Virgil ' s Aeneid , Vincent of Beauvais , Guido delle Colonne ' s Historia destructionis Troiae , Gaius Julius Hyginus ' Fabulae and Ovid ' s Metamorphoses and Heroides .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Chaucer","'s","sources","for","the","legends","include",":","Virgil","'","s","Aeneid",",","Vincent","of","Beauvais",",","Guido","delle","Colonne","'","s","Historia","destructionis","Troiae",",","Gaius","Julius","Hyginus","'","Fabulae","and","Ovid","'","s","Metamorphoses","and","Heroides","."],"labels":["B-writer","O","O","O","O","O","O","O","B-writer","O","O","B-poem","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","B-poem","I-poem","I-poem","O","B-writer","I-writer","I-writer","O","B-poem","O","B-writer","O","O","B-poem","O","B-poem","O"],"target_index":null,"target_label":null},"label_list":["magazine","organization","person","poem","event","award","country","literary_genre","location","writer","book"]}
{"id":"304","dataset":"crossner_literature","split":"test","instance":{"id":"304","prompt_labels":"He(O) was(O) commissioned(O) by(O) Walt(B-person) Disney(I-person) in(O) 1945(O) to(O) write(O) a(O) script(O) based(O) on(O) Alice(B-book) 's(I-book) Adventures(I-book) in(I-book) Wonderland(I-book) and(O) the(O) biography(B-literary genre) of(O) the(O) story(O) 's(O) author(O) ,(O) Lewis(B-writer) Carroll(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, country, location, literary genre, event, book, award, poem, writer, organization, person and O.\nSentence: He was commissioned by Walt Disney in 1945 to write a script based on Alice 's Adventures in Wonderland and the biography of the story 's author , Lewis Carroll .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","commissioned","by","Walt","Disney","in","1945","to","write","a","script","based","on","Alice","'s","Adventures","in","Wonderland","and","the","biography","of","the","story","'s","author",",","Lewis","Carroll","."],"labels":["O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","B-literary genre","O","O","O","O","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["magazine","country","location","literary_genre","event","book","award","poem","writer","organization","person"]}
{"id":"306","dataset":"crossner_literature","split":"test","instance":{"id":"306","prompt_labels":"Modern(O) readers(O) are(O) more(O) often(O) introduced(O) to(O) Orwell(B-writer) as(O) a(O) novelist(O) ,(O) particularly(O) through(O) his(O) enormously(O) successful(O) titles(O) Animal(B-book) Farm(I-book) and(O) Nineteen(B-book) Eighty-Four(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, literary genre, country, writer, location, magazine, poem, person, organization, book and O.\nSentence: Modern readers are more often introduced to Orwell as a novelist , particularly through his enormously successful titles Animal Farm and Nineteen Eighty-Four .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Modern","readers","are","more","often","introduced","to","Orwell","as","a","novelist",",","particularly","through","his","enormously","successful","titles","Animal","Farm","and","Nineteen","Eighty-Four","."],"labels":["O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","B-book","I-book","O","B-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["award","event","literary_genre","country","writer","location","magazine","poem","person","organization","book"]}
{"id":"308","dataset":"crossner_literature","split":"test","instance":{"id":"308","prompt_labels":"Rushdie(B-writer) wrote(O) a(O) non-fiction(O) book(O) about(O) Nicaragua(B-country) in(O) 1987(O) called(O) The(B-book) Jaguar(I-book) Smile(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, writer, poem, magazine, literary genre, organization, award, country, book, person and O.\nSentence: Rushdie wrote a non-fiction book about Nicaragua in 1987 called The Jaguar Smile .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Rushdie","wrote","a","non-fiction","book","about","Nicaragua","in","1987","called","The","Jaguar","Smile","."],"labels":["B-writer","O","O","O","O","O","B-country","O","O","O","B-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["location","event","writer","poem","magazine","literary_genre","organization","award","country","book","person"]}
{"id":"309","dataset":"crossner_literature","split":"test","instance":{"id":"309","prompt_labels":"Thorpe(B-person) began(O) competition(O) in(O) 2002(O) at(O) the(O) Australian(O) Championships(O) in(O) Brisbane(B-location) in(O) March(O) ,(O) which(O) were(O) used(O) to(O) select(O) the(O) team(O) for(O) the(O) 2002(B-event) Commonwealth(I-event) Games(I-event) in(O) Manchester(B-location) and(O) the(O) 2002(B-event) Pan(I-event) Pacific(I-event) Swimming(I-event) Championships(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, organization, literary genre, magazine, poem, country, location, person, award, writer, event and O.\nSentence: Thorpe began competition in 2002 at the Australian Championships in Brisbane in March , which were used to select the team for the 2002 Commonwealth Games in Manchester and the 2002 Pan Pacific Swimming Championships .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Thorpe","began","competition","in","2002","at","the","Australian","Championships","in","Brisbane","in","March",",","which","were","used","to","select","the","team","for","the","2002","Commonwealth","Games","in","Manchester","and","the","2002","Pan","Pacific","Swimming","Championships","."],"labels":["B-person","O","O","O","O","O","O","O","O","O","B-location","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","B-location","O","O","B-event","I-event","I-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["book","organization","literary_genre","magazine","poem","country","location","person","award","writer","event"]}
{"id":"310","dataset":"crossner_literature","split":"test","instance":{"id":"310","prompt_labels":"Al(B-writer) Capp(I-writer) Admits(O) One(O) Morals(O) Count(O) ;(O) Pays(O) $(O) 500(O) Fine(O) ;(O) The(B-organization) Capital(I-organization) Times(I-organization) ,(O) February(O) 12(O) ,(O) 1972(O) In(O) a(O) December(O) 1992(O) article(O) for(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) ,(O) Seymour(B-writer) Hersh(I-writer) reported(O) that(O) President(O) Richard(B-person) Nixon(I-person) and(O) Charles(B-person) Colson(I-person) had(O) repeatedly(O) discussed(O) the(O) Capp(B-writer) case(O) in(O) Oval(B-location) Office(I-location) recordings(O) that(O) had(O) recently(O) been(O) made(O) available(O) by(O) the(B-organization) National(I-organization) Archives(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, organization, literary genre, award, poem, book, writer, person, country, magazine and O.\nSentence: Al Capp Admits One Morals Count ; Pays $ 500 Fine ; The Capital Times , February 12 , 1972 In a December 1992 article for The New Yorker , Seymour Hersh reported that President Richard Nixon and Charles Colson had repeatedly discussed the Capp case in Oval Office recordings that had recently been made available by the National Archives .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Al","Capp","Admits","One","Morals","Count",";","Pays","$","500","Fine",";","The","Capital","Times",",","February","12",",","1972","In","a","December","1992","article","for","The","New","Yorker",",","Seymour","Hersh","reported","that","President","Richard","Nixon","and","Charles","Colson","had","repeatedly","discussed","the","Capp","case","in","Oval","Office","recordings","that","had","recently","been","made","available","by","the","National","Archives","."],"labels":["B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","I-magazine","O","B-writer","I-writer","O","O","O","B-person","I-person","O","B-person","I-person","O","O","O","O","B-writer","O","O","B-location","I-location","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["location","event","organization","literary_genre","award","poem","book","writer","person","country","magazine"]}
{"id":"311","dataset":"crossner_literature","split":"test","instance":{"id":"311","prompt_labels":"The(O) poem(B-literary genre) was(O) inspired(O) by(O) The(B-poem) Rodiad(I-poem) ((O) 1871(O) )(O) ,(O) falsely(O) ascribed(O) to(O) George(B-writer) Colman(I-writer) the(I-writer) Younger(I-writer) ,(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, organization, location, literary genre, writer, country, event, book, magazine, poem, person and O.\nSentence: The poem was inspired by The Rodiad ( 1871 ) , falsely ascribed to George Colman the Younger ,","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","poem","was","inspired","by","The","Rodiad","(","1871",")",",","falsely","ascribed","to","George","Colman","the","Younger",","],"labels":["O","B-literary genre","O","O","O","B-poem","I-poem","O","O","O","O","O","O","O","B-writer","I-writer","I-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["award","organization","location","literary_genre","writer","country","event","book","magazine","poem","person"]}
{"id":"313","dataset":"crossner_literature","split":"test","instance":{"id":"313","prompt_labels":"This(O) included(O) The(B-book) 500(I-book) Hats(I-book) of(I-book) Bartholomew(I-book) Cubbins(I-book) in(O) 1938(O) ,(O) as(O) well(O) as(O) The(B-book) King(I-book) 's(I-book) Stilts(I-book) and(O) The(B-book) Seven(I-book) Lady(I-book) Godivas(I-book) in(O) 1939(O) ,(O) all(O) of(O) which(O) were(O) in(O) prose(O) ,(O) atypically(O) for(O) him(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, literary genre, location, country, award, magazine, event, person, poem, writer, organization and O.\nSentence: This included The 500 Hats of Bartholomew Cubbins in 1938 , as well as The King 's Stilts and The Seven Lady Godivas in 1939 , all of which were in prose , atypically for him .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","included","The","500","Hats","of","Bartholomew","Cubbins","in","1938",",","as","well","as","The","King","'s","Stilts","and","The","Seven","Lady","Godivas","in","1939",",","all","of","which","were","in","prose",",","atypically","for","him","."],"labels":["O","O","B-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","B-book","I-book","I-book","I-book","O","B-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["book","literary_genre","location","country","award","magazine","event","person","poem","writer","organization"]}
{"id":"314","dataset":"crossner_literature","split":"test","instance":{"id":"314","prompt_labels":"He(O) also(O) edited(O) works(O) by(O) Juvenal(B-writer) ((O) 1905(O) )(O) and(O) Lucan(B-writer) ((O) 1926(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, person, award, organization, country, book, magazine, literary genre, writer, poem and O.\nSentence: He also edited works by Juvenal ( 1905 ) and Lucan ( 1926 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","also","edited","works","by","Juvenal","(","1905",")","and","Lucan","(","1926",")","."],"labels":["O","O","O","O","O","B-writer","O","O","O","O","B-writer","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","event","person","award","organization","country","book","magazine","literary_genre","writer","poem"]}
{"id":"315","dataset":"crossner_literature","split":"test","instance":{"id":"315","prompt_labels":"The(O) phrase(O) ,(O) from(O) Ode(B-poem) on(I-poem) a(I-poem) Distant(I-poem) Prospect(I-poem) of(I-poem) Eton(I-poem) College(I-poem) ,(O) is(O) possibly(O) one(O) of(O) the(O) most(O) misconstrued(O) phrases(O) in(O) English(O) literature(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, literary genre, book, organization, location, person, magazine, event, award, poem, writer and O.\nSentence: The phrase , from Ode on a Distant Prospect of Eton College , is possibly one of the most misconstrued phrases in English literature .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","phrase",",","from","Ode","on","a","Distant","Prospect","of","Eton","College",",","is","possibly","one","of","the","most","misconstrued","phrases","in","English","literature","."],"labels":["O","O","O","O","B-poem","I-poem","I-poem","I-poem","I-poem","I-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","literary_genre","book","organization","location","person","magazine","event","award","poem","writer"]}
{"id":"316","dataset":"crossner_literature","split":"test","instance":{"id":"316","prompt_labels":"Conversely(O) ,(O) Joe(B-writer) Haldeman(I-writer) '(O) s(O) 1974(O) anti-war(O) ,(O) Hugo(B-award) Award(I-award) -(O) and(O) Nebula(B-award) Award(I-award) -winning(O) science(B-literary genre) fiction(I-literary genre) novel(I-literary genre) The(B-book) Forever(I-book) War(I-book) is(O) popularly(O) thought(O) to(O) be(O) a(O) direct(O) reply(O) to(O) Starship(B-book) Troopers(I-book) ,(O) and(O) though(O) Haldeman(B-writer) has(O) stated(O) that(O) it(O) is(O) actually(O) a(O) result(O) of(O) his(O) personal(O) experiences(O) in(O) the(O) Vietnam(B-event) War(I-event) ,(O) he(O) has(O) admitted(O) to(O) being(O) influenced(O) by(O) Starship(B-book) Troopers(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, literary genre, country, book, person, writer, location, event, organization, poem, magazine and O.\nSentence: Conversely , Joe Haldeman ' s 1974 anti-war , Hugo Award - and Nebula Award -winning science fiction novel The Forever War is popularly thought to be a direct reply to Starship Troopers , and though Haldeman has stated that it is actually a result of his personal experiences in the Vietnam War , he has admitted to being influenced by Starship Troopers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Conversely",",","Joe","Haldeman","'","s","1974","anti-war",",","Hugo","Award","-","and","Nebula","Award","-winning","science","fiction","novel","The","Forever","War","is","popularly","thought","to","be","a","direct","reply","to","Starship","Troopers",",","and","though","Haldeman","has","stated","that","it","is","actually","a","result","of","his","personal","experiences","in","the","Vietnam","War",",","he","has","admitted","to","being","influenced","by","Starship","Troopers","."],"labels":["O","O","B-writer","I-writer","O","O","O","O","O","B-award","I-award","O","O","B-award","I-award","O","B-literary genre","I-literary genre","I-literary genre","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","B-book","I-book","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","O","O","O","O","O","O","O","O","B-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["award","literary_genre","country","book","person","writer","location","event","organization","poem","magazine"]}
{"id":"317","dataset":"crossner_literature","split":"test","instance":{"id":"317","prompt_labels":"In(O) 1995(O) ,(O) Cuarn(B-writer) released(O) his(O) first(O) feature(O) film(O) produced(O) in(O) the(O) United(B-country) States(I-country) ,(O) A(B-book) Little(I-book) Princess(I-book) ,(O) an(O) adaptation(O) of(O) Frances(B-writer) Hodgson(I-writer) Burnett(I-writer) '(O) s(O) classic(B-literary genre) novel(I-literary genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, literary genre, poem, award, organization, person, magazine, writer, event, book and O.\nSentence: In 1995 , Cuarn released his first feature film produced in the United States , A Little Princess , an adaptation of Frances Hodgson Burnett ' s classic novel .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1995",",","Cuarn","released","his","first","feature","film","produced","in","the","United","States",",","A","Little","Princess",",","an","adaptation","of","Frances","Hodgson","Burnett","'","s","classic","novel","."],"labels":["O","O","O","B-writer","O","O","O","O","O","O","O","O","B-country","I-country","O","B-book","I-book","I-book","O","O","O","O","B-writer","I-writer","I-writer","O","O","B-literary genre","I-literary genre","O"],"target_index":null,"target_label":null},"label_list":["country","location","literary_genre","poem","award","organization","person","magazine","writer","event","book"]}
{"id":"318","dataset":"crossner_literature","split":"test","instance":{"id":"318","prompt_labels":"He(O) began(O) acting(O) on-screen(O) in(O) the(O) early(O) 1980s(O) ,(O) with(O) his(O) mainstream(O) breakthrough(O) coming(O) with(O) Lee(B-person) 's(O) Do(O) the(O) Right(O) Thing(O) ((O) 1989(O) )(O) and(O) the(O) Coens(B-person) '(O) Miller(O) 's(O) Crossing(O) ((O) 1990(O) )(O) and(O) Barton(O) Fink(O) ((O) 1991(O) )(O) ,(O) for(O) which(O) he(O) won(O) the(O) Cannes(B-award) Film(I-award) Festival(I-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) Award(O) at(O) the(O) Cannes(B-event) Film(I-event) Festival(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, organization, book, event, country, magazine, person, location, literary genre, writer, award and O.\nSentence: He began acting on-screen in the early 1980s , with his mainstream breakthrough coming with Lee 's Do the Right Thing ( 1989 ) and the Coens ' Miller 's Crossing ( 1990 ) and Barton Fink ( 1991 ) , for which he won the Cannes Film Festival Award for Best Actor Award at the Cannes Film Festival .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","began","acting","on-screen","in","the","early","1980s",",","with","his","mainstream","breakthrough","coming","with","Lee","'s","Do","the","Right","Thing","(","1989",")","and","the","Coens","'","Miller","'s","Crossing","(","1990",")","and","Barton","Fink","(","1991",")",",","for","which","he","won","the","Cannes","Film","Festival","Award","for","Best","Actor","Award","at","the","Cannes","Film","Festival","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["poem","organization","book","event","country","magazine","person","location","literary_genre","writer","award"]}
{"id":"319","dataset":"crossner_literature","split":"test","instance":{"id":"319","prompt_labels":"The(O) book(O) and(O) its(O) sequel(O) ,(O) The(B-book) Road(I-book) Back(I-book) ((O) 1930(O) )(O) ,(O) were(O) among(O) the(O) books(O) banned(O) and(O) burned(O) in(O) Nazi(B-country) Germany(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, magazine, literary genre, book, writer, poem, organization, person, award, event, location and O.\nSentence: The book and its sequel , The Road Back ( 1930 ) , were among the books banned and burned in Nazi Germany .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","book","and","its","sequel",",","The","Road","Back","(","1930",")",",","were","among","the","books","banned","and","burned","in","Nazi","Germany","."],"labels":["O","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["country","magazine","literary_genre","book","writer","poem","organization","person","award","event","location"]}
{"id":"320","dataset":"crossner_literature","split":"test","instance":{"id":"320","prompt_labels":"Too(O) old(O) to(O) enlist(O) when(O) the(O) First(B-event) World(I-event) War(I-event) broke(O) out(O) ,(O) he(O) served(O) in(O) France(B-country) as(O) a(O) member(O) of(O) the(O) British(B-organization) Red(I-organization) Cross(I-organization) '(O) s(O) so-called(O) Literary(B-organization) Ambulance(I-organization) Drivers(I-organization) ,(O) a(O) group(O) of(O) some(O) 24(O) well-known(O) writers(O) ,(O) including(O) the(O) Americans(O) John(B-writer) Dos(I-writer) Passos(I-writer) ,(O) E.(B-writer) E.(I-writer) Cummings(I-writer) ,(O) and(O) Ernest(B-writer) Hemingway(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, writer, poem, organization, event, literary genre, award, book, magazine, person, location and O.\nSentence: Too old to enlist when the First World War broke out , he served in France as a member of the British Red Cross ' s so-called Literary Ambulance Drivers , a group of some 24 well-known writers , including the Americans John Dos Passos , E. E. Cummings , and Ernest Hemingway .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Too","old","to","enlist","when","the","First","World","War","broke","out",",","he","served","in","France","as","a","member","of","the","British","Red","Cross","'","s","so-called","Literary","Ambulance","Drivers",",","a","group","of","some","24","well-known","writers",",","including","the","Americans","John","Dos","Passos",",","E.","E.","Cummings",",","and","Ernest","Hemingway","."],"labels":["O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","B-country","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["country","writer","poem","organization","event","literary_genre","award","book","magazine","person","location"]}
{"id":"321","dataset":"crossner_literature","split":"test","instance":{"id":"321","prompt_labels":"Rashomon(O) ,(O) which(O) premiered(O) in(O) Tokyo(O) ,(O) became(O) the(O) surprise(O) winner(O) of(O) the(O) Golden(B-award) Lion(I-award) at(O) the(O) 1951(O) Venice(B-event) Film(I-event) Festival(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, country, location, magazine, award, poem, book, organization, event, person, writer and O.\nSentence: Rashomon , which premiered in Tokyo , became the surprise winner of the Golden Lion at the 1951 Venice Film Festival .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Rashomon",",","which","premiered","in","Tokyo",",","became","the","surprise","winner","of","the","Golden","Lion","at","the","1951","Venice","Film","Festival","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","country","location","magazine","award","poem","book","organization","event","person","writer"]}
{"id":"322","dataset":"crossner_literature","split":"test","instance":{"id":"322","prompt_labels":"Sterling(B-writer) is(O) one(O) of(O) the(O) founders(O) of(O) the(O) cyberpunk(O) movement(O) in(O) science(B-literary genre) fiction(I-literary genre) ,(O) along(O) with(O) William(B-writer) Gibson(I-writer) ,(O) Rudy(B-writer) Rucker(I-writer) ,(O) John(B-writer) Shirley(I-writer) ,(O) Lewis(B-writer) Shiner(I-writer) ,(O) and(O) Pat(B-writer) Cadigan(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, writer, country, award, person, magazine, poem, location, book, literary genre and O.\nSentence: Sterling is one of the founders of the cyberpunk movement in science fiction , along with William Gibson , Rudy Rucker , John Shirley , Lewis Shiner , and Pat Cadigan .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Sterling","is","one","of","the","founders","of","the","cyberpunk","movement","in","science","fiction",",","along","with","William","Gibson",",","Rudy","Rucker",",","John","Shirley",",","Lewis","Shiner",",","and","Pat","Cadigan","."],"labels":["B-writer","O","O","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["event","organization","writer","country","award","person","magazine","poem","location","book","literary_genre"]}
{"id":"323","dataset":"crossner_literature","split":"test","instance":{"id":"323","prompt_labels":"Controversial(O) wars(O) in(O) Afghanistan(B-country) and(O) South(B-country) Africa(I-country) undermined(O) his(O) public(O) support(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, writer, book, magazine, event, person, literary genre, organization, location, award, country and O.\nSentence: Controversial wars in Afghanistan and South Africa undermined his public support .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Controversial","wars","in","Afghanistan","and","South","Africa","undermined","his","public","support","."],"labels":["O","O","O","B-country","O","B-country","I-country","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","writer","book","magazine","event","person","literary_genre","organization","location","award","country"]}
{"id":"325","dataset":"crossner_literature","split":"test","instance":{"id":"325","prompt_labels":"Tallulah(O) premiered(O) at(O) the(O) 2016(B-event) Sundance(I-event) Film(I-event) Festival(I-event) on(O) January(O) 23(O) ,(O) 2016(O) and(O) was(O) released(O) on(O) Netflix(B-organization) on(O) July(O) 29(O) ,(O) 2016(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, poem, organization, person, magazine, country, location, book, writer, event, literary genre and O.\nSentence: Tallulah premiered at the 2016 Sundance Film Festival on January 23 , 2016 and was released on Netflix on July 29 , 2016 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Tallulah","premiered","at","the","2016","Sundance","Film","Festival","on","January","23",",","2016","and","was","released","on","Netflix","on","July","29",",","2016","."],"labels":["O","O","O","O","B-event","I-event","I-event","I-event","O","O","O","O","O","O","O","O","O","B-organization","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","poem","organization","person","magazine","country","location","book","writer","event","literary_genre"]}
{"id":"326","dataset":"crossner_literature","split":"test","instance":{"id":"326","prompt_labels":"Although(O) the(O) prophets(O) urged(O) the(O) people(O) of(O) Kingdom(B-country) of(I-country) Judah(I-country) and(O) Israel(B-country) to(O) see(O) their(O) exile(O) as(O) punishment(O) for(O) failing(O) to(O) uphold(O) their(O) covenant(O) with(O) God(O) ,(O) it(O) was(O) not(O) long(O) after(O) they(O) had(O) been(O) restored(O) to(O) the(O) land(O) and(O) to(O) Temple(O) worship(O) that(O) the(O) people(O) 's(O) commitment(O) to(O) their(O) God(O) began(O) ,(O) once(O) again(O) ,(O) to(O) wane(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, event, magazine, writer, poem, literary genre, country, book, award, person and O.\nSentence: Although the prophets urged the people of Kingdom of Judah and Israel to see their exile as punishment for failing to uphold their covenant with God , it was not long after they had been restored to the land and to Temple worship that the people 's commitment to their God began , once again , to wane .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Although","the","prophets","urged","the","people","of","Kingdom","of","Judah","and","Israel","to","see","their","exile","as","punishment","for","failing","to","uphold","their","covenant","with","God",",","it","was","not","long","after","they","had","been","restored","to","the","land","and","to","Temple","worship","that","the","people","'s","commitment","to","their","God","began",",","once","again",",","to","wane","."],"labels":["O","O","O","O","O","O","O","B-country","I-country","I-country","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","organization","event","magazine","writer","poem","literary_genre","country","book","award","person"]}
{"id":"328","dataset":"crossner_literature","split":"test","instance":{"id":"328","prompt_labels":"He(O) wrote(O) that(O) The(B-book) Matarese(I-book) Circle(I-book) was(O) inspired(O) by(O) rumors(O) about(O) the(O) Trilateral(B-organization) Commission(I-organization) ,(O) and(O) it(O) was(O) published(O) only(O) a(O) few(O) years(O) after(O) the(O) commission(O) was(O) founded(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, award, poem, person, book, country, location, literary genre, organization, writer, event and O.\nSentence: He wrote that The Matarese Circle was inspired by rumors about the Trilateral Commission , and it was published only a few years after the commission was founded .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","wrote","that","The","Matarese","Circle","was","inspired","by","rumors","about","the","Trilateral","Commission",",","and","it","was","published","only","a","few","years","after","the","commission","was","founded","."],"labels":["O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","award","poem","person","book","country","location","literary_genre","organization","writer","event"]}
{"id":"329","dataset":"crossner_literature","split":"test","instance":{"id":"329","prompt_labels":"They(O) were(O) honored(O) with(O) the(O) rank(O) of(O) a(O) marquis(O) 35(O) times(O) since(O) Gaozu(B-person) of(O) the(O) Han(B-country) dynasty(I-country) ,(O) and(O) they(O) were(O) promoted(O) to(O) the(O) rank(O) of(O) duke(O) 42(O) times(O) from(O) the(O) Tang(B-country) dynasty(I-country) to(O) the(O) Qing(B-country) dynasty(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, poem, book, country, award, location, literary genre, event, organization, writer, magazine and O.\nSentence: They were honored with the rank of a marquis 35 times since Gaozu of the Han dynasty , and they were promoted to the rank of duke 42 times from the Tang dynasty to the Qing dynasty .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["They","were","honored","with","the","rank","of","a","marquis","35","times","since","Gaozu","of","the","Han","dynasty",",","and","they","were","promoted","to","the","rank","of","duke","42","times","from","the","Tang","dynasty","to","the","Qing","dynasty","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-person","O","O","B-country","I-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O","O","B-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["person","poem","book","country","award","location","literary_genre","event","organization","writer","magazine"]}
{"id":"330","dataset":"crossner_literature","split":"test","instance":{"id":"330","prompt_labels":"Cuarn(B-writer) 's(O) next(O) feature(O) was(O) also(O) a(O) literary(O) adaptation(O) ,(O) a(O) modernized(O) version(O) of(O) Charles(B-writer) Dickens(I-writer) '(O) s(O) Great(B-book) Expectations(I-book) starring(O) Ethan(B-person) Hawke(I-person) ,(O) Gwyneth(B-person) Paltrow(I-person) and(O) Robert(B-person) De(I-person) Niro(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, country, literary genre, poem, event, magazine, book, award, writer, location and O.\nSentence: Cuarn 's next feature was also a literary adaptation , a modernized version of Charles Dickens ' s Great Expectations starring Ethan Hawke , Gwyneth Paltrow and Robert De Niro .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Cuarn","'s","next","feature","was","also","a","literary","adaptation",",","a","modernized","version","of","Charles","Dickens","'","s","Great","Expectations","starring","Ethan","Hawke",",","Gwyneth","Paltrow","and","Robert","De","Niro","."],"labels":["B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","B-book","I-book","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["organization","person","country","literary_genre","poem","event","magazine","book","award","writer","location"]}
{"id":"331","dataset":"crossner_literature","split":"test","instance":{"id":"331","prompt_labels":"Its(O) glory(O) rests(O) chiefly(O) on(O) three(O) works(O) ...(O) '(O) A(B-poem) song(I-poem) of(I-poem) sorrow(I-poem) inside(I-poem) the(I-poem) royal(I-poem) harem(I-poem) '(O) ...(O) by(O) Nguyn(B-writer) Gia(I-writer) Thiu(I-writer) ,(O) '(O) Calling(B-poem) all(I-poem) souls(I-poem) '(O) ...(O) by(O) Nguyn(B-writer) Du(I-writer) ,(O) and(O) '(O) Chinh(B-poem) ph(I-poem) ngm(I-poem) '(O) ...(O) by(O) Phan(B-writer) Huy(I-writer) ch(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, poem, person, book, country, literary genre, organization, writer, event, location, award and O.\nSentence: Its glory rests chiefly on three works ... ' A song of sorrow inside the royal harem ' ... by Nguyn Gia Thiu , ' Calling all souls ' ... by Nguyn Du , and ' Chinh ph ngm ' ... by Phan Huy ch .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Its","glory","rests","chiefly","on","three","works","...","'","A","song","of","sorrow","inside","the","royal","harem","'","...","by","Nguyn","Gia","Thiu",",","'","Calling","all","souls","'","...","by","Nguyn","Du",",","and","'","Chinh","ph","ngm","'","...","by","Phan","Huy","ch","."],"labels":["O","O","O","O","O","O","O","O","O","B-poem","I-poem","I-poem","I-poem","I-poem","I-poem","I-poem","I-poem","O","O","O","B-writer","I-writer","I-writer","O","O","B-poem","I-poem","I-poem","O","O","O","B-writer","I-writer","O","O","O","B-poem","I-poem","I-poem","O","O","O","B-writer","I-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["magazine","poem","person","book","country","literary_genre","organization","writer","event","location","award"]}
{"id":"332","dataset":"crossner_literature","split":"test","instance":{"id":"332","prompt_labels":"The(O) contest(O) over(O) the(O) authenticity(O) of(O) Macpherson(B-writer) 's(O) pseudo-Gaelic(O) productions(O) ,(O) Curley(B-person) asserts(O) ,(O) became(O) a(O) seismograph(O) of(O) the(O) fragile(O) unity(O) within(O) restive(O) diversity(O) of(O) British(B-country) Empire(I-country) Kingdom(B-country) of(I-country) Great(I-country) Britain(I-country) in(O) the(O) age(O) of(O) Samuel(B-writer) Johnson(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, location, writer, country, person, book, organization, poem, event, magazine, award and O.\nSentence: The contest over the authenticity of Macpherson 's pseudo-Gaelic productions , Curley asserts , became a seismograph of the fragile unity within restive diversity of British Empire Kingdom of Great Britain in the age of Samuel Johnson .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","contest","over","the","authenticity","of","Macpherson","'s","pseudo-Gaelic","productions",",","Curley","asserts",",","became","a","seismograph","of","the","fragile","unity","within","restive","diversity","of","British","Empire","Kingdom","of","Great","Britain","in","the","age","of","Samuel","Johnson","."],"labels":["O","O","O","O","O","O","B-writer","O","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","B-country","I-country","I-country","I-country","O","O","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","location","writer","country","person","book","organization","poem","event","magazine","award"]}
{"id":"335","dataset":"crossner_literature","split":"test","instance":{"id":"335","prompt_labels":"In(O) 1993(O) ,(O) Thornton(B-person) married(O) Playboy(B-magazine) model(O) Pietra(B-person) Dawn(I-person) Cherniak(I-person) ,(O) with(O) whom(O) he(O) had(O) two(O) sons(O) ,(O) Harry(B-person) James(I-person) and(O) William(B-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, person, writer, organization, country, literary genre, poem, magazine, award, book and O.\nSentence: In 1993 , Thornton married Playboy model Pietra Dawn Cherniak , with whom he had two sons , Harry James and William .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1993",",","Thornton","married","Playboy","model","Pietra","Dawn","Cherniak",",","with","whom","he","had","two","sons",",","Harry","James","and","William","."],"labels":["O","O","O","B-person","O","B-magazine","O","B-person","I-person","I-person","O","O","O","O","O","O","O","O","B-person","I-person","O","B-person","O"],"target_index":null,"target_label":null},"label_list":["event","location","person","writer","organization","country","literary_genre","poem","magazine","award","book"]}
{"id":"336","dataset":"crossner_literature","split":"test","instance":{"id":"336","prompt_labels":"His(O) works(O) have(O) been(O) short(O) listed(O) for(O) the(O) Booker(B-award) Prize(I-award) five(O) times(O) ,(O) in(O) 1981(O) for(O) Midnight(B-book) 's(I-book) Children(I-book) ,(O) 1983(O) for(O) Shame(B-book) ,(O) 1988(O) for(O) The(B-book) Satanic(I-book) Verses(I-book) ,(O) 1995(O) for(O) The(B-book) Moor(I-book) 's(I-book) Last(I-book) Sigh(I-book) ,(O) 2019(O) for(O) Quichotte(B-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, book, person, magazine, literary genre, country, organization, award, writer, location, event and O.\nSentence: His works have been short listed for the Booker Prize five times , in 1981 for Midnight 's Children , 1983 for Shame , 1988 for The Satanic Verses , 1995 for The Moor 's Last Sigh , 2019 for Quichotte .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","works","have","been","short","listed","for","the","Booker","Prize","five","times",",","in","1981","for","Midnight","'s","Children",",","1983","for","Shame",",","1988","for","The","Satanic","Verses",",","1995","for","The","Moor","'s","Last","Sigh",",","2019","for","Quichotte","."],"labels":["O","O","O","O","O","O","O","O","B-award","I-award","O","O","O","O","O","O","B-book","I-book","I-book","O","O","O","B-book","O","O","O","B-book","I-book","I-book","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","B-book","O"],"target_index":null,"target_label":null},"label_list":["poem","book","person","magazine","literary_genre","country","organization","award","writer","location","event"]}
{"id":"337","dataset":"crossner_literature","split":"test","instance":{"id":"337","prompt_labels":"Baudelaire(B-writer) 's(O) highly(O) original(O) style(O) of(O) prose-poetry(B-literary genre) influenced(O) a(O) whole(O) generation(O) of(O) poets(O) including(O) Paul(B-writer) Verlaine(I-writer) ,(O) Arthur(B-writer) Rimbaud(I-writer) and(O) Stphane(B-writer) Mallarm(I-writer) ,(O) among(O) many(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, award, poem, literary genre, event, country, location, magazine, organization, person, book and O.\nSentence: Baudelaire 's highly original style of prose-poetry influenced a whole generation of poets including Paul Verlaine , Arthur Rimbaud and Stphane Mallarm , among many others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Baudelaire","'s","highly","original","style","of","prose-poetry","influenced","a","whole","generation","of","poets","including","Paul","Verlaine",",","Arthur","Rimbaud","and","Stphane","Mallarm",",","among","many","others","."],"labels":["B-writer","O","O","O","O","O","B-literary genre","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["writer","award","poem","literary_genre","event","country","location","magazine","organization","person","book"]}
{"id":"341","dataset":"crossner_literature","split":"test","instance":{"id":"341","prompt_labels":"Much(O) of(O) her(O) work(O) ,(O) including(O) the(O) collections(O) of(O) erotica(O) Delta(B-book) of(I-book) Venus(I-book) and(O) Little(B-book) Birds(I-book) ,(O) was(O) published(O) posthumously(O) amid(O) renewed(O) critical(O) interest(O) in(O) her(O) life(O) and(O) work(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, award, event, writer, country, location, person, literary genre, organization, poem, book and O.\nSentence: Much of her work , including the collections of erotica Delta of Venus and Little Birds , was published posthumously amid renewed critical interest in her life and work .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Much","of","her","work",",","including","the","collections","of","erotica","Delta","of","Venus","and","Little","Birds",",","was","published","posthumously","amid","renewed","critical","interest","in","her","life","and","work","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","O","B-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","award","event","writer","country","location","person","literary_genre","organization","poem","book"]}
{"id":"342","dataset":"crossner_literature","split":"test","instance":{"id":"342","prompt_labels":"Angel(B-writer) is(O) skeptical(O) about(O) the(O) efficacy(O) of(O) astrology(B-literary genre) ,(O) and(O) believes(O) that(O) the(O) unfolding(O) existential(O) situation(O) of(O) Tim(B-person) and(O) Kirsten(B-person) is(O) akin(O) to(O) Friedrich(B-writer) Schiller(I-writer) '(O) s(O) Germany(B-literary genre) Romanticism(I-literary genre) era(O) masterpiece(O) ,(O) the(O) Wallenstein(B-book) trilogy(I-book) ((O) insofar(O) as(O) their(O) credulity(O) reflects(O) the(O) loss(O) of(O) rational(O) belief(O) in(O) contemporary(O) consensual(O) reality(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, organization, award, country, event, book, location, magazine, person, literary genre, writer and O.\nSentence: Angel is skeptical about the efficacy of astrology , and believes that the unfolding existential situation of Tim and Kirsten is akin to Friedrich Schiller ' s Germany Romanticism era masterpiece , the Wallenstein trilogy ( insofar as their credulity reflects the loss of rational belief in contemporary consensual reality ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Angel","is","skeptical","about","the","efficacy","of","astrology",",","and","believes","that","the","unfolding","existential","situation","of","Tim","and","Kirsten","is","akin","to","Friedrich","Schiller","'","s","Germany","Romanticism","era","masterpiece",",","the","Wallenstein","trilogy","(","insofar","as","their","credulity","reflects","the","loss","of","rational","belief","in","contemporary","consensual","reality",")","."],"labels":["B-writer","O","O","O","O","O","O","B-literary genre","O","O","O","O","O","O","O","O","O","B-person","O","B-person","O","O","O","B-writer","I-writer","O","O","B-literary genre","I-literary genre","O","O","O","O","B-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","organization","award","country","event","book","location","magazine","person","literary_genre","writer"]}
{"id":"343","dataset":"crossner_literature","split":"test","instance":{"id":"343","prompt_labels":"Love(B-writer) subsequently(O) attracted(O) media(O) attention(O) in(O) May(O) 1998(O) after(O) punching(O) journalist(O) Belissa(B-writer) Cohen(I-writer) in(O) the(O) face(O) at(O) a(O) party(O) ;(O) the(O) suit(O) was(O) settled(O) out(O) of(O) court(O) for(O) an(O) undisclosed(O) sum.(O) and(O) were(O) subsequently(O) nominated(O) for(O) three(O) Grammy(B-award) Award(I-award) s(O) at(O) the(O) 41st(B-event) Annual(I-event) Grammy(I-event) Awards(I-event) ceremony(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, organization, poem, country, event, magazine, book, literary genre, location, writer, person and O.\nSentence: Love subsequently attracted media attention in May 1998 after punching journalist Belissa Cohen in the face at a party ; the suit was settled out of court for an undisclosed sum. and were subsequently nominated for three Grammy Award s at the 41st Annual Grammy Awards ceremony .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Love","subsequently","attracted","media","attention","in","May","1998","after","punching","journalist","Belissa","Cohen","in","the","face","at","a","party",";","the","suit","was","settled","out","of","court","for","an","undisclosed","sum.","and","were","subsequently","nominated","for","three","Grammy","Award","s","at","the","41st","Annual","Grammy","Awards","ceremony","."],"labels":["B-writer","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","O","B-event","I-event","I-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["award","organization","poem","country","event","magazine","book","literary_genre","location","writer","person"]}
{"id":"345","dataset":"crossner_literature","split":"test","instance":{"id":"345","prompt_labels":"On(O) June(O) 3(O) ,(O) 1968(O) ,(O) radical(O) feminist(O) writer(O) Valerie(B-writer) Solanas(I-writer) shot(O) Warhol(B-writer) and(O) Mario(B-writer) Amaya(I-writer) ,(O) art(O) critic(O) and(O) curator(O) ,(O) at(O) Warhol(B-writer) 's(O) studio.(O) a(O) separatist(O) feminist(O) tract(O) that(O) advocated(O) the(O) elimination(O) of(O) men(O) ;(O) and(O) appeared(O) in(O) the(O) 1968(O) Warhol(B-writer) film(O) I(O) ,(O) a(O) Man(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, person, country, organization, poem, award, location, book, literary genre, writer, event and O.\nSentence: On June 3 , 1968 , radical feminist writer Valerie Solanas shot Warhol and Mario Amaya , art critic and curator , at Warhol 's studio. a separatist feminist tract that advocated the elimination of men ; and appeared in the 1968 Warhol film I , a Man .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","June","3",",","1968",",","radical","feminist","writer","Valerie","Solanas","shot","Warhol","and","Mario","Amaya",",","art","critic","and","curator",",","at","Warhol","'s","studio.","a","separatist","feminist","tract","that","advocated","the","elimination","of","men",";","and","appeared","in","the","1968","Warhol","film","I",",","a","Man","."],"labels":["O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","O","B-writer","I-writer","O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","person","country","organization","poem","award","location","book","literary_genre","writer","event"]}
{"id":"347","dataset":"crossner_literature","split":"test","instance":{"id":"347","prompt_labels":"Adapted(O) by(O) Altman(B-writer) and(O) Sam(B-writer) Shepard(I-writer) for(O) The(B-organization) Cannon(I-organization) Group(I-organization) from(O) the(O) latter(O) 's(O) Pulitzer(B-award) Prize(I-award) -nominated(O) play(O) ,(O) Fool(B-book) for(I-book) Love(I-book) ((O) 1985(O) )(O) featured(O) the(O) playwright-actor(O) alongside(O) Kim(B-person) Basinger(I-person) ,(O) Harry(B-person) Dean(I-person) Stanton(I-person) and(O) Randy(B-person) Quaid(I-person) ;(O) it(O) fared(O) better(O) than(O) most(O) of(O) his(O) films(O) from(O) the(O) era(O) ,(O) earning(O) $(O) 900,000(O) domestically(O) on(O) a(O) $(O) 2(O) million(O) budget(O) and(O) positive(O) reviews(O) from(O) Roger(B-writer) Ebert(I-writer) and(O) Vincent(B-writer) Canby(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, literary genre, award, poem, country, book, person, magazine, event, organization, location and O.\nSentence: Adapted by Altman and Sam Shepard for The Cannon Group from the latter 's Pulitzer Prize -nominated play , Fool for Love ( 1985 ) featured the playwright-actor alongside Kim Basinger , Harry Dean Stanton and Randy Quaid ; it fared better than most of his films from the era , earning $ 900,000 domestically on a $ 2 million budget and positive reviews from Roger Ebert and Vincent Canby .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Adapted","by","Altman","and","Sam","Shepard","for","The","Cannon","Group","from","the","latter","'s","Pulitzer","Prize","-nominated","play",",","Fool","for","Love","(","1985",")","featured","the","playwright-actor","alongside","Kim","Basinger",",","Harry","Dean","Stanton","and","Randy","Quaid",";","it","fared","better","than","most","of","his","films","from","the","era",",","earning","$","900,000","domestically","on","a","$","2","million","budget","and","positive","reviews","from","Roger","Ebert","and","Vincent","Canby","."],"labels":["O","O","B-writer","O","B-writer","I-writer","O","B-organization","I-organization","I-organization","O","O","O","O","B-award","I-award","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","I-person","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["writer","literary_genre","award","poem","country","book","person","magazine","event","organization","location"]}
{"id":"349","dataset":"crossner_literature","split":"test","instance":{"id":"349","prompt_labels":"His(O) most(O) famous(O) poem(O) is(O) De(B-poem) bello(I-poem) Troiano(I-poem) ((O) On(B-poem) the(I-poem) Trojan(I-poem) War(I-poem) )(O) in(O) six(O) books(O) ,(O) most(O) of(O) which(O) was(O) written(O) before(O) 1183(O) ,(O) but(O) which(O) was(O) finished(O) after(O) 1184(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, magazine, literary genre, event, writer, award, person, book, organization, location, poem and O.\nSentence: His most famous poem is De bello Troiano ( On the Trojan War ) in six books , most of which was written before 1183 , but which was finished after 1184 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","most","famous","poem","is","De","bello","Troiano","(","On","the","Trojan","War",")","in","six","books",",","most","of","which","was","written","before","1183",",","but","which","was","finished","after","1184","."],"labels":["O","O","O","O","O","B-poem","I-poem","I-poem","O","B-poem","I-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","magazine","literary_genre","event","writer","award","person","book","organization","location","poem"]}
{"id":"351","dataset":"crossner_literature","split":"test","instance":{"id":"351","prompt_labels":"Being(O) educated(O) by(O) the(O) French(O) humanist(O) and(O) poet(O) ,(O) Nicholas(B-writer) Denisot(I-writer) ,(O) Anne(B-writer) Seymour(I-writer) with(O) her(O) sisters(O) Margaret(B-writer) and(O) Jane(B-writer) composed(O) 103(O) Latin(O) distich(O) s(O) for(O) the(O) tomb(O) of(O) Marguerite(B-writer) de(I-writer) Navarre(I-writer) ,(O) which(O) were(O) published(O) in(O) France(B-country) as(O) Hecatodistichon(B-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, writer, book, country, event, award, organization, location, magazine, poem, person and O.\nSentence: Being educated by the French humanist and poet , Nicholas Denisot , Anne Seymour with her sisters Margaret and Jane composed 103 Latin distich s for the tomb of Marguerite de Navarre , which were published in France as Hecatodistichon .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Being","educated","by","the","French","humanist","and","poet",",","Nicholas","Denisot",",","Anne","Seymour","with","her","sisters","Margaret","and","Jane","composed","103","Latin","distich","s","for","the","tomb","of","Marguerite","de","Navarre",",","which","were","published","in","France","as","Hecatodistichon","."],"labels":["O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","O","O","B-writer","O","B-writer","O","O","O","O","O","O","O","O","O","B-writer","I-writer","I-writer","O","O","O","O","O","B-country","O","B-poem","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","writer","book","country","event","award","organization","location","magazine","poem","person"]}
{"id":"352","dataset":"crossner_literature","split":"test","instance":{"id":"352","prompt_labels":"She(O) memorized(O) entire(O) books(O) ,(O) including(O) The(O) Shipwreck(B-book) ,(O) The(O) Lady(B-book) of(I-book) the(I-book) Lake(I-book) ,(O) Lalla-Rookh(B-poem) ,(O) The(B-poem) Bride(I-poem) of(I-poem) Abydos(I-poem) ,(O) and(O) The(B-poem) Corsair(I-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, poem, location, organization, event, book, literary genre, award, person, writer, country and O.\nSentence: She memorized entire books , including The Shipwreck , The Lady of the Lake , Lalla-Rookh , The Bride of Abydos , and The Corsair .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","memorized","entire","books",",","including","The","Shipwreck",",","The","Lady","of","the","Lake",",","Lalla-Rookh",",","The","Bride","of","Abydos",",","and","The","Corsair","."],"labels":["O","O","O","O","O","O","O","B-book","O","O","B-book","I-book","I-book","I-book","O","B-poem","O","B-poem","I-poem","I-poem","I-poem","O","O","B-poem","I-poem","O"],"target_index":null,"target_label":null},"label_list":["magazine","poem","location","organization","event","book","literary_genre","award","person","writer","country"]}
{"id":"353","dataset":"crossner_literature","split":"test","instance":{"id":"353","prompt_labels":"The(O) poem(B-literary genre) 's(O) major(O) source(O) is(O) Guido(B-writer) delle(I-writer) Colonne(I-writer) '(O) s(O) Historia(B-poem) destructionis(I-poem) Troiae(I-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, literary genre, country, writer, book, location, poem, magazine, organization, event, award and O.\nSentence: The poem 's major source is Guido delle Colonne ' s Historia destructionis Troiae .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","poem","'s","major","source","is","Guido","delle","Colonne","'","s","Historia","destructionis","Troiae","."],"labels":["O","B-literary genre","O","O","O","O","B-writer","I-writer","I-writer","O","O","B-poem","I-poem","I-poem","O"],"target_index":null,"target_label":null},"label_list":["person","literary_genre","country","writer","book","location","poem","magazine","organization","event","award"]}
{"id":"354","dataset":"crossner_literature","split":"test","instance":{"id":"354","prompt_labels":"The(O) Pahlavic(O) poetic(O) debate(O) Drakht-i(B-poem) Asurig(I-poem) indicates(O) the(O) history(O) of(O) this(O) form(O) of(O) debate(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, poem, literary genre, writer, magazine, country, organization, book, person, location, award and O.\nSentence: The Pahlavic poetic debate Drakht-i Asurig indicates the history of this form of debate .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Pahlavic","poetic","debate","Drakht-i","Asurig","indicates","the","history","of","this","form","of","debate","."],"labels":["O","O","O","O","B-poem","I-poem","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","poem","literary_genre","writer","magazine","country","organization","book","person","location","award"]}
{"id":"355","dataset":"crossner_literature","split":"test","instance":{"id":"355","prompt_labels":"She(O) wrote(O) seven(O) novels(B-literary genre) ,(O) Adam(B-book) Bede(I-book) ((O) 1859(O) )(O) ,(O) The(B-book) Mill(I-book) on(I-book) the(I-book) Floss(I-book) ((O) 1860(O) )(O) ,(O) Silas(B-book) Marner(I-book) ((O) 1861(O) )(O) ,(O) Romola(B-book) ((O) 1862-63(O) )(O) ,(O) Felix(B-book) Holt(I-book) ,(I-book) the(I-book) Radical(I-book) ((O) 1866(O) )(O) ,(O) Middlemarch(B-book) ((O) 1871-72(O) )(O) and(O) Daniel(B-book) Deronda(I-book) ((O) 1876(O) )(O) ,(O) most(O) of(O) which(O) are(O) set(O) in(O) provincial(O) England(B-country) and(O) known(O) for(O) their(O) realism(O) and(O) psychological(O) insight(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, organization, country, person, book, location, writer, award, literary genre, poem, event and O.\nSentence: She wrote seven novels , Adam Bede ( 1859 ) , The Mill on the Floss ( 1860 ) , Silas Marner ( 1861 ) , Romola ( 1862-63 ) , Felix Holt , the Radical ( 1866 ) , Middlemarch ( 1871-72 ) and Daniel Deronda ( 1876 ) , most of which are set in provincial England and known for their realism and psychological insight .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","wrote","seven","novels",",","Adam","Bede","(","1859",")",",","The","Mill","on","the","Floss","(","1860",")",",","Silas","Marner","(","1861",")",",","Romola","(","1862-63",")",",","Felix","Holt",",","the","Radical","(","1866",")",",","Middlemarch","(","1871-72",")","and","Daniel","Deronda","(","1876",")",",","most","of","which","are","set","in","provincial","England","and","known","for","their","realism","and","psychological","insight","."],"labels":["O","O","O","B-literary genre","O","B-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","O","O","O","O","B-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","O","O","O","O","B-book","I-book","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","organization","country","person","book","location","writer","award","literary_genre","poem","event"]}
{"id":"356","dataset":"crossner_literature","split":"test","instance":{"id":"356","prompt_labels":"He(O) received(O) honorary(B-award) doctorates(I-award) and(O) a(O) number(O) of(O) professional(O) honours(O) ,(O) such(O) the(O) Royal(B-organization) Society(I-organization) '(O) s(O) Royal(B-award) Medal(I-award) and(O) Darwin(B-award) Medal(I-award) in(O) 1868(O) and(O) 1890(O) respectively(O) ,(O) Above(O) all(O) ,(O) his(O) role(O) as(O) the(O) co-discoverer(O) of(O) natural(O) selection(O) and(O) his(O) work(O) on(O) zoogeography(O) marked(O) him(O) out(O) as(O) an(O) exceptional(O) figure(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, writer, magazine, country, event, poem, location, organization, book, award, person and O.\nSentence: He received honorary doctorates and a number of professional honours , such the Royal Society ' s Royal Medal and Darwin Medal in 1868 and 1890 respectively , Above all , his role as the co-discoverer of natural selection and his work on zoogeography marked him out as an exceptional figure .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","received","honorary","doctorates","and","a","number","of","professional","honours",",","such","the","Royal","Society","'","s","Royal","Medal","and","Darwin","Medal","in","1868","and","1890","respectively",",","Above","all",",","his","role","as","the","co-discoverer","of","natural","selection","and","his","work","on","zoogeography","marked","him","out","as","an","exceptional","figure","."],"labels":["O","O","B-award","I-award","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","B-award","I-award","O","B-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","writer","magazine","country","event","poem","location","organization","book","award","person"]}
{"id":"357","dataset":"crossner_literature","split":"test","instance":{"id":"357","prompt_labels":"For(O) example(O) ,(O) Holmes(B-person) falls(O) in(O) love(O) and(O) marries(O) in(O) Laurie(B-writer) R.(I-writer) King(I-writer) '(O) s(O) Mary(O) Russell(O) series(O) ,(O) is(O) re-animated(O) after(O) his(O) death(O) to(O) fight(O) future(O) crime(O) in(O) the(O) animated(O) series(O) Sherlock(O) Holmes(O) in(O) the(O) 22nd(O) Century(O) ,(O) and(O) is(O) meshed(O) with(O) the(O) setting(O) of(O) H.(B-writer) P.(I-writer) Lovecraft(I-writer) '(O) s(O) Cthulhu(B-book) Mythos(I-book) in(O) Neil(B-writer) Gaiman(I-writer) '(O) s(O) A(B-book) Study(I-book) in(I-book) Emerald(I-book) ((O) which(O) won(O) the(O) 2004(O) Hugo(B-award) Award(I-award) for(I-award) Best(I-award) Short(I-award) Story(I-award) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, award, poem, person, event, organization, country, location, book, writer, magazine and O.\nSentence: For example , Holmes falls in love and marries in Laurie R. King ' s Mary Russell series , is re-animated after his death to fight future crime in the animated series Sherlock Holmes in the 22nd Century , and is meshed with the setting of H. P. Lovecraft ' s Cthulhu Mythos in Neil Gaiman ' s A Study in Emerald ( which won the 2004 Hugo Award for Best Short Story ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","example",",","Holmes","falls","in","love","and","marries","in","Laurie","R.","King","'","s","Mary","Russell","series",",","is","re-animated","after","his","death","to","fight","future","crime","in","the","animated","series","Sherlock","Holmes","in","the","22nd","Century",",","and","is","meshed","with","the","setting","of","H.","P.","Lovecraft","'","s","Cthulhu","Mythos","in","Neil","Gaiman","'","s","A","Study","in","Emerald","(","which","won","the","2004","Hugo","Award","for","Best","Short","Story",")","."],"labels":["O","O","O","B-person","O","O","O","O","O","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","I-writer","O","O","B-book","I-book","O","B-writer","I-writer","O","O","B-book","I-book","I-book","I-book","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","award","poem","person","event","organization","country","location","book","writer","magazine"]}
{"id":"360","dataset":"crossner_literature","split":"test","instance":{"id":"360","prompt_labels":"His(O) 1968(O) novel(B-literary genre) Stand(B-book) on(I-book) Zanzibar(I-book) ,(O) about(O) an(O) overpopulated(O) world(O) ,(O) won(O) the(O) 1969(O) Hugo(B-award) Award(I-award) for(O) best(O) science(B-literary genre) fiction(I-literary genre) novel(I-literary genre) ,(O) and(O) the(O) BSFA(B-award) award(I-award) the(O) same(O) year(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, event, organization, magazine, person, location, award, country, book, writer, literary genre and O.\nSentence: His 1968 novel Stand on Zanzibar , about an overpopulated world , won the 1969 Hugo Award for best science fiction novel , and the BSFA award the same year .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","1968","novel","Stand","on","Zanzibar",",","about","an","overpopulated","world",",","won","the","1969","Hugo","Award","for","best","science","fiction","novel",",","and","the","BSFA","award","the","same","year","."],"labels":["O","O","B-literary genre","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","B-literary genre","I-literary genre","I-literary genre","O","O","O","B-award","I-award","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","event","organization","magazine","person","location","award","country","book","writer","literary_genre"]}
{"id":"361","dataset":"crossner_literature","split":"test","instance":{"id":"361","prompt_labels":"Although(O) Williams(B-writer) attracted(O) the(O) attention(O) and(O) admiration(O) of(O) some(O) of(O) the(O) most(O) notable(O) writers(O) of(O) his(O) day(O) ,(O) including(O) T.(B-writer) S.(I-writer) Eliot(I-writer) and(O) W.(B-writer) H.(I-writer) Auden(I-writer) ,(O) his(O) greatest(O) admirer(O) was(O) probably(O) C.(B-writer) S.(I-writer) Lewis(I-writer) ,(O) whose(O) novel(B-literary genre) That(B-book) Hideous(I-book) Strength(I-book) ((O) 1945(O) )(O) has(O) been(O) regarded(O) as(O) partially(O) inspired(O) by(O) his(O) acquaintance(O) with(O) both(O) the(O) man(O) and(O) his(O) novels(B-literary genre) and(O) poems(B-literary genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, country, writer, person, book, award, magazine, event, poem, location, organization and O.\nSentence: Although Williams attracted the attention and admiration of some of the most notable writers of his day , including T. S. Eliot and W. H. Auden , his greatest admirer was probably C. S. Lewis , whose novel That Hideous Strength ( 1945 ) has been regarded as partially inspired by his acquaintance with both the man and his novels and poems .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Although","Williams","attracted","the","attention","and","admiration","of","some","of","the","most","notable","writers","of","his","day",",","including","T.","S.","Eliot","and","W.","H.","Auden",",","his","greatest","admirer","was","probably","C.","S.","Lewis",",","whose","novel","That","Hideous","Strength","(","1945",")","has","been","regarded","as","partially","inspired","by","his","acquaintance","with","both","the","man","and","his","novels","and","poems","."],"labels":["O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","B-writer","I-writer","I-writer","O","O","B-literary genre","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-literary genre","O","B-literary genre","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","country","writer","person","book","award","magazine","event","poem","location","organization"]}
{"id":"362","dataset":"crossner_literature","split":"test","instance":{"id":"362","prompt_labels":"The(O) film(O) premiered(O) at(O) the(O) Venice(B-event) Film(I-event) Festival(I-event) where(O) it(O) won(O) the(O) festival(O) 's(O) Mimmo(B-award) Rotella(I-award) Foundation(I-award) Award(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, magazine, poem, organization, writer, event, literary genre, location, book, award, person and O.\nSentence: The film premiered at the Venice Film Festival where it won the festival 's Mimmo Rotella Foundation Award .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","film","premiered","at","the","Venice","Film","Festival","where","it","won","the","festival","'s","Mimmo","Rotella","Foundation","Award","."],"labels":["O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["country","magazine","poem","organization","writer","event","literary_genre","location","book","award","person"]}
{"id":"363","dataset":"crossner_literature","split":"test","instance":{"id":"363","prompt_labels":"Gathering(O) widespread(O) critical(O) acclaim(O) at(O) the(O) Toronto(B-event) and(I-event) New(I-event) York(I-event) Film(I-event) Festival(I-event) film(O) festivals(O) ,(O) the(O) film(O) also(O) became(O) a(O) favorite(O) when(O) Academy(B-award) Awards(I-award) nominations(O) were(O) announced(O) in(O) 2001(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, literary genre, poem, organization, event, magazine, person, writer, book, location and O.\nSentence: Gathering widespread critical acclaim at the Toronto and New York Film Festival film festivals , the film also became a favorite when Academy Awards nominations were announced in 2001 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Gathering","widespread","critical","acclaim","at","the","Toronto","and","New","York","Film","Festival","film","festivals",",","the","film","also","became","a","favorite","when","Academy","Awards","nominations","were","announced","in","2001","."],"labels":["O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","award","literary_genre","poem","organization","event","magazine","person","writer","book","location"]}
{"id":"364","dataset":"crossner_literature","split":"test","instance":{"id":"364","prompt_labels":"A(B-book) Fire(I-book) Upon(I-book) the(I-book) Deep(I-book) won(O) the(O) Hugo(B-award) Award(I-award) for(I-award) Best(I-award) Novel(I-award) in(O) 1993(O) ,(O) sharing(O) it(O) with(O) Doomsday(B-book) Book(I-book) by(O) Connie(B-writer) Willis(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, writer, book, magazine, literary genre, poem, event, person, award, organization, country and O.\nSentence: A Fire Upon the Deep won the Hugo Award for Best Novel in 1993 , sharing it with Doomsday Book by Connie Willis .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","Fire","Upon","the","Deep","won","the","Hugo","Award","for","Best","Novel","in","1993",",","sharing","it","with","Doomsday","Book","by","Connie","Willis","."],"labels":["B-book","I-book","I-book","I-book","I-book","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","B-book","I-book","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["location","writer","book","magazine","literary_genre","poem","event","person","award","organization","country"]}
{"id":"367","dataset":"crossner_literature","split":"test","instance":{"id":"367","prompt_labels":"Some(O) historians(O) such(O) as(O) John(B-writer) Julius(I-writer) Norwich(I-writer) ,(O) despite(O) their(O) admiration(O) for(O) his(O) furthering(O) of(O) historical(O) methodology(O) ,(O) consider(O) Gibbon(B-writer) 's(O) hostile(O) views(O) on(O) the(O) Byzantine(B-country) Empire(I-country) flawed(O) and(O) blame(O) him(O) somewhat(O) for(O) the(O) lack(O) of(O) interest(O) shown(O) in(O) the(O) subject(O) throughout(O) the(O) 19th(O) and(O) early(O) 20th(O) centuries.(O) John(B-writer) Julius(I-writer) Norwich(I-writer) ,(O) Byzantium(B-location) ((O) New(B-location) York(I-location) :(O) Knopf(O) ,(O) 1989(O) )(O) ;(O) Byzantium(B-location) :(O) the(O) apogee(O) ((O) London(B-location) and(O) New(B-location) York(I-location) :(O) Viking(B-organization) Press(I-organization) ,(O) 1991(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, location, organization, poem, magazine, writer, book, person, event, award, country and O.\nSentence: Some historians such as John Julius Norwich , despite their admiration for his furthering of historical methodology , consider Gibbon 's hostile views on the Byzantine Empire flawed and blame him somewhat for the lack of interest shown in the subject throughout the 19th and early 20th centuries. John Julius Norwich , Byzantium ( New York : Knopf , 1989 ) ; Byzantium : the apogee ( London and New York : Viking Press , 1991 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","historians","such","as","John","Julius","Norwich",",","despite","their","admiration","for","his","furthering","of","historical","methodology",",","consider","Gibbon","'s","hostile","views","on","the","Byzantine","Empire","flawed","and","blame","him","somewhat","for","the","lack","of","interest","shown","in","the","subject","throughout","the","19th","and","early","20th","centuries.","John","Julius","Norwich",",","Byzantium","(","New","York",":","Knopf",",","1989",")",";","Byzantium",":","the","apogee","(","London","and","New","York",":","Viking","Press",",","1991",")","."],"labels":["O","O","O","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","O","O","O","O","O","B-country","I-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","I-writer","O","B-location","O","B-location","I-location","O","O","O","O","O","O","B-location","O","O","O","O","B-location","O","B-location","I-location","O","B-organization","I-organization","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","location","organization","poem","magazine","writer","book","person","event","award","country"]}
{"id":"368","dataset":"crossner_literature","split":"test","instance":{"id":"368","prompt_labels":"Although(O) the(O) Mythos(B-book) was(O) not(O) formalized(O) or(O) acknowledged(O) between(O) them(O) ,(O) Lovecraft(B-writer) did(O) correspond(O) and(O) share(O) story(O) elements(O) with(O) other(O) contemporary(O) writers(O) including(O) Clark(B-writer) Ashton(I-writer) Smith(I-writer) ,(O) Robert(B-writer) E.(I-writer) Howard(I-writer) ,(O) Robert(B-writer) Bloch(I-writer) ,(O) Frank(B-writer) Belknap(I-writer) Long(I-writer) ,(O) Henry(B-writer) Kuttner(I-writer) ,(O) Henry(B-writer) S.(I-writer) Whitehead(I-writer) ,(O) and(O) Fritz(B-writer) Leiber(I-writer)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, writer, country, magazine, person, poem, event, literary genre, award, organization, location and O.\nSentence: Although the Mythos was not formalized or acknowledged between them , Lovecraft did correspond and share story elements with other contemporary writers including Clark Ashton Smith , Robert E. Howard , Robert Bloch , Frank Belknap Long , Henry Kuttner , Henry S. Whitehead , and Fritz Leiber","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Although","the","Mythos","was","not","formalized","or","acknowledged","between","them",",","Lovecraft","did","correspond","and","share","story","elements","with","other","contemporary","writers","including","Clark","Ashton","Smith",",","Robert","E.","Howard",",","Robert","Bloch",",","Frank","Belknap","Long",",","Henry","Kuttner",",","Henry","S.","Whitehead",",","and","Fritz","Leiber"],"labels":["O","O","B-book","O","O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","B-writer","I-writer"],"target_index":null,"target_label":null},"label_list":["book","writer","country","magazine","person","poem","event","literary_genre","award","organization","location"]}
{"id":"369","dataset":"crossner_literature","split":"test","instance":{"id":"369","prompt_labels":"Maria(B-person) died(O) of(O) cancer(O) on(O) 15(O) September(O) 1821(O) ,(O) leaving(O) five(O) daughters(O) ,(O) Maria(B-person) ,(O) Elizabeth(B-person) ,(O) Charlotte(B-writer) ,(O) Emily(B-writer) Bront(I-writer) and(O) Anne(B-writer) Bront(I-writer) ,(O) and(O) a(O) son(O) ,(O) Branwell(B-writer) Bront(I-writer) ,(O) to(O) be(O) taken(O) care(O) of(O) by(O) her(O) sister(O) ,(O) Elizabeth(B-person) Branwell(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, person, location, award, literary genre, magazine, writer, poem, book, country and O.\nSentence: Maria died of cancer on 15 September 1821 , leaving five daughters , Maria , Elizabeth , Charlotte , Emily Bront and Anne Bront , and a son , Branwell Bront , to be taken care of by her sister , Elizabeth Branwell .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Maria","died","of","cancer","on","15","September","1821",",","leaving","five","daughters",",","Maria",",","Elizabeth",",","Charlotte",",","Emily","Bront","and","Anne","Bront",",","and","a","son",",","Branwell","Bront",",","to","be","taken","care","of","by","her","sister",",","Elizabeth","Branwell","."],"labels":["B-person","O","O","O","O","O","O","O","O","O","O","O","O","B-person","O","B-person","O","B-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["event","organization","person","location","award","literary_genre","magazine","writer","poem","book","country"]}
{"id":"371","dataset":"crossner_literature","split":"test","instance":{"id":"371","prompt_labels":"Celebrated(O) Puerto(O) Rican(O) novelists(O) who(O) write(O) in(O) English(O) and(O) Spanish(O) include(O) Giannina(B-writer) Braschi(I-writer) ,(O) author(O) of(O) the(O) Spanglish(O) classic(O) Yo-Yo(B-book) Boing(I-book) !(I-book) and(O) Rosario(B-writer) Ferr(I-writer) ,(O) best(O) known(O) for(O) Eccentric(B-book) Neighborhoods(I-book) Puerto(B-location) Rico(I-location) has(O) also(O) produced(O) important(O) playwrights(O) such(O) as(O) Ren(B-writer) Marqus(I-writer) ,(O) Luis(B-writer) Rafael(I-writer) Snchez(I-writer) ,(O) and(O) Jos(B-writer) Rivera(I-writer) and(O) New(B-location) York(I-location) based(O) poets(O) such(O) as(O) Julia(B-writer) de(I-writer) Burgos(I-writer) ,(O) Giannina(B-writer) Braschi(I-writer) and(O) Pedro(B-writer) Pietri(I-writer) ,(O) as(O) well(O) as(O) various(O) members(O) of(O) the(O) Nuyorican(B-location) Poets(I-location) Caf(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, person, award, organization, writer, book, event, magazine, country, location, literary genre and O.\nSentence: Celebrated Puerto Rican novelists who write in English and Spanish include Giannina Braschi , author of the Spanglish classic Yo-Yo Boing ! and Rosario Ferr , best known for Eccentric Neighborhoods Puerto Rico has also produced important playwrights such as Ren Marqus , Luis Rafael Snchez , and Jos Rivera and New York based poets such as Julia de Burgos , Giannina Braschi and Pedro Pietri , as well as various members of the Nuyorican Poets Caf .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Celebrated","Puerto","Rican","novelists","who","write","in","English","and","Spanish","include","Giannina","Braschi",",","author","of","the","Spanglish","classic","Yo-Yo","Boing","!","and","Rosario","Ferr",",","best","known","for","Eccentric","Neighborhoods","Puerto","Rico","has","also","produced","important","playwrights","such","as","Ren","Marqus",",","Luis","Rafael","Snchez",",","and","Jos","Rivera","and","New","York","based","poets","such","as","Julia","de","Burgos",",","Giannina","Braschi","and","Pedro","Pietri",",","as","well","as","various","members","of","the","Nuyorican","Poets","Caf","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","B-book","I-book","I-book","O","B-writer","I-writer","O","O","O","O","B-book","I-book","B-location","I-location","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","B-writer","I-writer","O","B-location","I-location","O","O","O","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","O","O","O","O","O","O","O","B-location","I-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["poem","person","award","organization","writer","book","event","magazine","country","location","literary_genre"]}
{"id":"372","dataset":"crossner_literature","split":"test","instance":{"id":"372","prompt_labels":"He(O) is(O) best(O) known(O) for(O) his(O) works(O) of(O) fiction(B-literary genre) ,(O) especially(O) The(B-book) Screwtape(I-book) Letters(I-book) ,(O) The(B-book) Chronicles(I-book) of(I-book) Narnia(I-book) ,(O) and(O) The(B-book) Space(I-book) Trilogy(I-book) ,(O) and(O) for(O) his(O) non-fiction(B-literary genre) Christian(I-literary genre) apologetics(I-literary genre) ,(O) such(O) as(O) Mere(B-book) Christianity(I-book) ,(O) Miracles(B-book) ,(O) and(O) The(B-book) Problem(I-book) of(I-book) Pain(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, location, poem, organization, book, award, magazine, country, writer, event, person and O.\nSentence: He is best known for his works of fiction , especially The Screwtape Letters , The Chronicles of Narnia , and The Space Trilogy , and for his non-fiction Christian apologetics , such as Mere Christianity , Miracles , and The Problem of Pain .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","is","best","known","for","his","works","of","fiction",",","especially","The","Screwtape","Letters",",","The","Chronicles","of","Narnia",",","and","The","Space","Trilogy",",","and","for","his","non-fiction","Christian","apologetics",",","such","as","Mere","Christianity",",","Miracles",",","and","The","Problem","of","Pain","."],"labels":["O","O","O","O","O","O","O","O","B-literary genre","O","O","B-book","I-book","I-book","O","B-book","I-book","I-book","I-book","O","O","B-book","I-book","I-book","O","O","O","O","B-literary genre","I-literary genre","I-literary genre","O","O","O","B-book","I-book","O","B-book","O","O","B-book","I-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","location","poem","organization","book","award","magazine","country","writer","event","person"]}
{"id":"374","dataset":"crossner_literature","split":"test","instance":{"id":"374","prompt_labels":"Adaptations(O) of(O) the(O) novel(B-literary genre) include(O) Franois(B-writer) Truffaut(I-writer) '(O) s(O) 1966(O) film(O) adaptation(O) and(O) a(O) 1982(O) BBC(B-organization) Radio(I-organization) dramatization.Nov(O) 13(O) 1982(O) Fahrenheit(B-book) 451(I-book) ,(O) BBC(B-organization) Radio(I-organization) 4(O) Bradbury(B-organization) published(O) a(O) stage(O) play(O) version(O) in(O) 1979(O) and(O) helped(O) develop(O) a(O) 1984(O) interactive(B-literary genre) fiction(I-literary genre) computer(O) game(O) titled(O) Fahrenheit(O) 451(O) ,(O) as(O) well(O) as(O) a(O) collection(O) of(O) his(O) short(B-literary genre) stories(I-literary genre) titled(O) A(B-book) Pleasure(I-book) to(I-book) Burn(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, magazine, event, poem, award, writer, country, book, person, literary genre and O.\nSentence: Adaptations of the novel include Franois Truffaut ' s 1966 film adaptation and a 1982 BBC Radio dramatization.Nov 13 1982 Fahrenheit 451 , BBC Radio 4 Bradbury published a stage play version in 1979 and helped develop a 1984 interactive fiction computer game titled Fahrenheit 451 , as well as a collection of his short stories titled A Pleasure to Burn .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Adaptations","of","the","novel","include","Franois","Truffaut","'","s","1966","film","adaptation","and","a","1982","BBC","Radio","dramatization.Nov","13","1982","Fahrenheit","451",",","BBC","Radio","4","Bradbury","published","a","stage","play","version","in","1979","and","helped","develop","a","1984","interactive","fiction","computer","game","titled","Fahrenheit","451",",","as","well","as","a","collection","of","his","short","stories","titled","A","Pleasure","to","Burn","."],"labels":["O","O","O","B-literary genre","O","B-writer","I-writer","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","O","B-book","I-book","O","B-organization","I-organization","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","O","O","O","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","O","B-book","I-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["organization","location","magazine","event","poem","award","writer","country","book","person","literary_genre"]}
{"id":"375","dataset":"crossner_literature","split":"test","instance":{"id":"375","prompt_labels":"Scholars(O) S.T.(B-writer) Joshi(I-writer) and(O) David(B-writer) E.(I-writer) Schultz(I-writer) are(O) preparing(O) various(O) volumes(O) of(O) Smith(B-writer) 's(O) letters(O) to(O) such(O) of(O) his(O) individual(O) correspondents(O) as(O) Donald(B-writer) Wandrei(I-writer) ,(O) Robert(B-writer) H.(I-writer) Barlow(I-writer) ,(O) and(O) August(B-writer) Derleth(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, location, event, country, book, poem, writer, organization, person, magazine, award and O.\nSentence: Scholars S.T. Joshi and David E. Schultz are preparing various volumes of Smith 's letters to such of his individual correspondents as Donald Wandrei , Robert H. Barlow , and August Derleth .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Scholars","S.T.","Joshi","and","David","E.","Schultz","are","preparing","various","volumes","of","Smith","'s","letters","to","such","of","his","individual","correspondents","as","Donald","Wandrei",",","Robert","H.","Barlow",",","and","August","Derleth","."],"labels":["O","B-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","location","event","country","book","poem","writer","organization","person","magazine","award"]}
{"id":"377","dataset":"crossner_literature","split":"test","instance":{"id":"377","prompt_labels":";(O) few(O) writers(O) focus(O) on(O) this(O) idea(O) ,(O) although(O) it(O) has(O) been(O) explored(O) in(O) stories(O) such(O) as(O) Larry(B-writer) Niven(I-writer) '(O) s(O) story(O) All(B-book) the(I-book) Myriad(I-book) Ways(I-book) ,(O) where(O) the(O) reality(O) of(O) all(O) possible(O) universes(O) leads(O) to(O) an(O) epidemic(O) of(O) suicide(O) and(O) crime(O) because(O) people(O) conclude(O) their(O) choices(O) have(O) no(O) moral(O) import(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, writer, location, magazine, event, book, person, country, organization, literary genre, poem and O.\nSentence: ; few writers focus on this idea , although it has been explored in stories such as Larry Niven ' s story All the Myriad Ways , where the reality of all possible universes leads to an epidemic of suicide and crime because people conclude their choices have no moral import .","prediction_output":null,"prediction_outputs":null,"group":null,"words":[";","few","writers","focus","on","this","idea",",","although","it","has","been","explored","in","stories","such","as","Larry","Niven","'","s","story","All","the","Myriad","Ways",",","where","the","reality","of","all","possible","universes","leads","to","an","epidemic","of","suicide","and","crime","because","people","conclude","their","choices","have","no","moral","import","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","B-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","writer","location","magazine","event","book","person","country","organization","literary_genre","poem"]}
{"id":"378","dataset":"crossner_literature","split":"test","instance":{"id":"378","prompt_labels":"Philip(B-writer) Roth(I-writer) '(O) s(O) novel(B-literary genre) ,(O) The(B-book) Plot(I-book) Against(I-book) America(I-book) ((O) 2004(O) )(O) ,(O) looks(O) at(O) an(O) America(B-country) where(O) Franklin(B-person) D.(I-person) Roosevelt(I-person) is(O) defeated(O) in(O) 1940(O) in(O) his(O) bid(O) for(O) a(O) third(O) term(O) as(O) President(O) of(O) the(O) United(B-country) States(I-country) ,(O) and(O) Charles(B-person) Lindbergh(I-person) is(O) elected(O) ,(O) leading(O) to(O) a(O) US(B-country) that(O) features(O) increasing(O) fascism(O) and(O) anti-Semitism(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, poem, literary genre, location, magazine, person, organization, award, writer, event, book and O.\nSentence: Philip Roth ' s novel , The Plot Against America ( 2004 ) , looks at an America where Franklin D. Roosevelt is defeated in 1940 in his bid for a third term as President of the United States , and Charles Lindbergh is elected , leading to a US that features increasing fascism and anti-Semitism .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Philip","Roth","'","s","novel",",","The","Plot","Against","America","(","2004",")",",","looks","at","an","America","where","Franklin","D.","Roosevelt","is","defeated","in","1940","in","his","bid","for","a","third","term","as","President","of","the","United","States",",","and","Charles","Lindbergh","is","elected",",","leading","to","a","US","that","features","increasing","fascism","and","anti-Semitism","."],"labels":["B-writer","I-writer","O","O","B-literary genre","O","B-book","I-book","I-book","I-book","O","O","O","O","O","O","O","B-country","O","B-person","I-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O","O","B-person","I-person","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","poem","literary_genre","location","magazine","person","organization","award","writer","event","book"]}
{"id":"379","dataset":"crossner_literature","split":"test","instance":{"id":"379","prompt_labels":"Gustave(B-writer) Flaubert(I-writer) ,(O) recently(O) attacked(O) in(O) a(O) similar(O) fashion(O) for(O) Madame(B-book) Bovary(I-book) ((O) and(O) acquitted(O) )(O) ,(O) was(O) impressed(O) and(O) wrote(O) to(O) Baudelaire(B-writer) :(O) You(O) have(O) found(O) a(O) way(O) to(O) rejuvenate(O) Romanticism(O) ...(O) You(O) are(O) as(O) unyielding(O) as(O) marble(O) ,(O) and(O) as(O) penetrating(O) as(O) an(O) English(B-country) mist(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, literary genre, event, magazine, organization, country, location, award, poem, person, writer and O.\nSentence: Gustave Flaubert , recently attacked in a similar fashion for Madame Bovary ( and acquitted ) , was impressed and wrote to Baudelaire : You have found a way to rejuvenate Romanticism ... You are as unyielding as marble , and as penetrating as an English mist .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Gustave","Flaubert",",","recently","attacked","in","a","similar","fashion","for","Madame","Bovary","(","and","acquitted",")",",","was","impressed","and","wrote","to","Baudelaire",":","You","have","found","a","way","to","rejuvenate","Romanticism","...","You","are","as","unyielding","as","marble",",","and","as","penetrating","as","an","English","mist","."],"labels":["B-writer","I-writer","O","O","O","O","O","O","O","O","B-book","I-book","O","O","O","O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O"],"target_index":null,"target_label":null},"label_list":["book","literary_genre","event","magazine","organization","country","location","award","poem","person","writer"]}
{"id":"380","dataset":"crossner_literature","split":"test","instance":{"id":"380","prompt_labels":"The(O) most(O) celebrated(O) work(O) of(O) Ramapurathu(B-writer) Warrier(I-writer) is(O) Kuchelavritham(B-poem) Vanchippattu(I-poem) ,(O) which(O) depicts(O) the(O) story(O) of(O) Kuchela(B-person) ,(O) a(O) devotee(O) and(O) an(O) old(O) classmate(O) of(O) Krishna(B-person) ,(O) going(O) to(O) Dwaraka(B-location) to(O) meet(O) with(O) him(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, magazine, award, person, writer, location, event, literary genre, country, book, poem and O.\nSentence: The most celebrated work of Ramapurathu Warrier is Kuchelavritham Vanchippattu , which depicts the story of Kuchela , a devotee and an old classmate of Krishna , going to Dwaraka to meet with him .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","most","celebrated","work","of","Ramapurathu","Warrier","is","Kuchelavritham","Vanchippattu",",","which","depicts","the","story","of","Kuchela",",","a","devotee","and","an","old","classmate","of","Krishna",",","going","to","Dwaraka","to","meet","with","him","."],"labels":["O","O","O","O","O","B-writer","I-writer","O","B-poem","I-poem","O","O","O","O","O","O","B-person","O","O","O","O","O","O","O","O","B-person","O","O","O","B-location","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","magazine","award","person","writer","location","event","literary_genre","country","book","poem"]}
{"id":"381","dataset":"crossner_literature","split":"test","instance":{"id":"381","prompt_labels":"His(O) first(O) published(O) novels(B-literary genre) were(O) social(B-literary genre) satires(I-literary genre) ,(O) Crome(B-book) Yellow(I-book) ((O) 1921(O) )(O) ,(O) Antic(B-book) Hay(I-book) ((O) 1923(O) )(O) ,(O) Those(B-book) Barren(I-book) Leaves(I-book) ((O) 1925(O) )(O) ,(O) and(O) Point(B-book) Counter(I-book) Point(I-book) ((O) 1928(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, magazine, country, book, award, event, writer, literary genre, organization, location, poem and O.\nSentence: His first published novels were social satires , Crome Yellow ( 1921 ) , Antic Hay ( 1923 ) , Those Barren Leaves ( 1925 ) , and Point Counter Point ( 1928 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","first","published","novels","were","social","satires",",","Crome","Yellow","(","1921",")",",","Antic","Hay","(","1923",")",",","Those","Barren","Leaves","(","1925",")",",","and","Point","Counter","Point","(","1928",")","."],"labels":["O","O","O","B-literary genre","O","B-literary genre","I-literary genre","O","B-book","I-book","O","O","O","O","B-book","I-book","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","magazine","country","book","award","event","writer","literary_genre","organization","location","poem"]}
{"id":"384","dataset":"crossner_literature","split":"test","instance":{"id":"384","prompt_labels":"This(O) period(O) also(O) saw(O) alternate(B-literary genre) history(I-literary genre) works(O) by(O) S.(B-writer) M.(I-writer) Stirling(I-writer) ,(O) Kim(B-writer) Stanley(I-writer) Robinson(I-writer) ,(O) Harry(B-writer) Harrison(I-writer) ,(O) Howard(B-writer) Waldrop(I-writer) ,(O) and(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, book, poem, country, location, literary genre, event, person, magazine, award, organization and O.\nSentence: This period also saw alternate history works by S. M. Stirling , Kim Stanley Robinson , Harry Harrison , Howard Waldrop , and others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","period","also","saw","alternate","history","works","by","S.","M.","Stirling",",","Kim","Stanley","Robinson",",","Harry","Harrison",",","Howard","Waldrop",",","and","others","."],"labels":["O","O","O","O","B-literary genre","I-literary genre","O","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["writer","book","poem","country","location","literary_genre","event","person","magazine","award","organization"]}
{"id":"386","dataset":"crossner_literature","split":"test","instance":{"id":"386","prompt_labels":"Like(O) The(B-book) Black(I-book) Diamonds(I-book) ,(O) it(O) uses(O) a(O) medieval(O) ,(O) Arabian(B-book) Nights(I-book) -like(O) setting(O) ,(O) and(O) the(O) Arabian(B-book) Nights(I-book) ,(O) like(O) the(O) fairy(B-literary genre) tales(I-literary genre) of(O) the(B-writer) Brothers(I-writer) Grimm(I-writer) and(O) the(O) works(O) of(O) Edgar(B-writer) Allan(I-writer) Poe(I-writer) ,(O) are(O) known(O) to(O) have(O) strongly(O) influenced(O) Smith(B-writer) 's(O) early(O) writing(O) ,(O) as(O) did(O) William(B-writer) Beckford(I-writer) '(O) s(O) Vathek(B-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, location, literary genre, award, book, poem, organization, country, magazine, person, event and O.\nSentence: Like The Black Diamonds , it uses a medieval , Arabian Nights -like setting , and the Arabian Nights , like the fairy tales of the Brothers Grimm and the works of Edgar Allan Poe , are known to have strongly influenced Smith 's early writing , as did William Beckford ' s Vathek .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Like","The","Black","Diamonds",",","it","uses","a","medieval",",","Arabian","Nights","-like","setting",",","and","the","Arabian","Nights",",","like","the","fairy","tales","of","the","Brothers","Grimm","and","the","works","of","Edgar","Allan","Poe",",","are","known","to","have","strongly","influenced","Smith","'s","early","writing",",","as","did","William","Beckford","'","s","Vathek","."],"labels":["O","B-book","I-book","I-book","O","O","O","O","O","O","B-book","I-book","O","O","O","O","O","B-book","I-book","O","O","O","B-literary genre","I-literary genre","O","B-writer","I-writer","I-writer","O","O","O","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","B-writer","I-writer","O","O","B-book","O"],"target_index":null,"target_label":null},"label_list":["writer","location","literary_genre","award","book","poem","organization","country","magazine","person","event"]}
{"id":"387","dataset":"crossner_literature","split":"test","instance":{"id":"387","prompt_labels":"91(O) In(O) the(O) same(O) year(O) The(B-magazine) Saturday(I-magazine) Evening(I-magazine) Post(I-magazine) paid(O) $(O) 3,500(O) to(O) serialise(O) Something(B-book) Fresh(I-book) ,(O) the(O) first(O) of(O) what(O) became(O) a(O) series(O) of(O) novels(B-literary genre) set(O) at(O) Blandings(B-book) Castle(I-book) .(O) Wodehouse(B-writer) and(O) Ratcliffe(B-writer) ,(O) p(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, magazine, location, book, poem, writer, organization, award, person, literary genre, event and O.\nSentence: 91 In the same year The Saturday Evening Post paid $ 3,500 to serialise Something Fresh , the first of what became a series of novels set at Blandings Castle . Wodehouse and Ratcliffe , p .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["91","In","the","same","year","The","Saturday","Evening","Post","paid","$","3,500","to","serialise","Something","Fresh",",","the","first","of","what","became","a","series","of","novels","set","at","Blandings","Castle",".","Wodehouse","and","Ratcliffe",",","p","."],"labels":["O","O","O","O","O","B-magazine","I-magazine","I-magazine","I-magazine","O","O","O","O","O","B-book","I-book","O","O","O","O","O","O","O","O","O","B-literary genre","O","O","B-book","I-book","O","B-writer","O","B-writer","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","magazine","location","book","poem","writer","organization","award","person","literary_genre","event"]}
{"id":"388","dataset":"crossner_literature","split":"test","instance":{"id":"388","prompt_labels":"It(O) recounts(O) the(O) tale(O) of(O) a(O) man(O) who(O) is(O) considered(O) a(O) madman(O) due(O) to(O) his(O) perceptions(O) of(O) a(O) different(O) 1845(O) ,(O) a(O) reality(O) in(O) which(O) long-dead(O) famous(O) people(O) ,(O) such(O) as(O) the(O) poets(O) Robert(B-writer) Burns(I-writer) ,(O) Lord(B-writer) Byron(I-writer) ,(O) Percy(B-writer) Bysshe(I-writer) Shelley(I-writer) and(O) John(B-writer) Keats(I-writer) ,(O) the(O) actor(O) Edmund(B-person) Kean(I-person) ,(O) the(O) British(O) politician(O) George(B-person) Canning(I-person) ,(O) and(O) even(O) Napoleon(B-person) Bonaparte(I-person) ,(O) are(O) still(O) alive(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, award, poem, event, literary genre, writer, location, country, book, magazine and O.\nSentence: It recounts the tale of a man who is considered a madman due to his perceptions of a different 1845 , a reality in which long-dead famous people , such as the poets Robert Burns , Lord Byron , Percy Bysshe Shelley and John Keats , the actor Edmund Kean , the British politician George Canning , and even Napoleon Bonaparte , are still alive .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","recounts","the","tale","of","a","man","who","is","considered","a","madman","due","to","his","perceptions","of","a","different","1845",",","a","reality","in","which","long-dead","famous","people",",","such","as","the","poets","Robert","Burns",",","Lord","Byron",",","Percy","Bysshe","Shelley","and","John","Keats",",","the","actor","Edmund","Kean",",","the","British","politician","George","Canning",",","and","even","Napoleon","Bonaparte",",","are","still","alive","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","O","O","O","B-person","I-person","O","O","O","O","B-person","I-person","O","O","O","B-person","I-person","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","organization","award","poem","event","literary_genre","writer","location","country","book","magazine"]}
{"id":"389","dataset":"crossner_literature","split":"test","instance":{"id":"389","prompt_labels":"Likewise(O) ,(O) in(O) 1963(O) ,(O) J.(B-writer) M.(I-writer) G.(I-writer) Le(I-writer) Clzio(I-writer) ,(O) winner(O) of(O) the(O) 2008(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) ,(O) published(O) the(O) novel(B-literary genre) Le(B-book) Proces-Verbal(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, literary genre, book, writer, event, award, magazine, poem, country, location, organization and O.\nSentence: Likewise , in 1963 , J. M. G. Le Clzio , winner of the 2008 Nobel Prize in Literature , published the novel Le Proces-Verbal .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Likewise",",","in","1963",",","J.","M.","G.","Le","Clzio",",","winner","of","the","2008","Nobel","Prize","in","Literature",",","published","the","novel","Le","Proces-Verbal","."],"labels":["O","O","O","O","O","B-writer","I-writer","I-writer","I-writer","I-writer","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O","B-literary genre","B-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["person","literary_genre","book","writer","event","award","magazine","poem","country","location","organization"]}
{"id":"390","dataset":"crossner_literature","split":"test","instance":{"id":"390","prompt_labels":"Aung(B-writer) San(I-writer) Suu(I-writer) Kyi(I-writer) ((O) ;(O) born(O) 19(O) June(O) 1945(O) )(O) is(O) a(O) Myanmar(B-country) politician(O) ,(O) diplomat(O) ,(O) author(O) ,(O) and(O) a(O) 1991(O) Nobel(B-award) Peace(I-award) Prize(I-award) laureate(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, literary genre, writer, poem, country, award, event, person, magazine, organization, book and O.\nSentence: Aung San Suu Kyi ( ; born 19 June 1945 ) is a Myanmar politician , diplomat , author , and a 1991 Nobel Peace Prize laureate .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Aung","San","Suu","Kyi","(",";","born","19","June","1945",")","is","a","Myanmar","politician",",","diplomat",",","author",",","and","a","1991","Nobel","Peace","Prize","laureate","."],"labels":["B-writer","I-writer","I-writer","I-writer","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","O","O"],"target_index":null,"target_label":null},"label_list":["location","literary_genre","writer","poem","country","award","event","person","magazine","organization","book"]}
{"id":"391","dataset":"crossner_literature","split":"test","instance":{"id":"391","prompt_labels":"In(O) 1990(O) ,(O) after(O) an(O) ideological(O) disagreement(O) with(O) Peikoff(B-person) ,(O) philosopher(O) David(B-person) Kelley(I-person) founded(O) the(O) Institute(O) for(O) Objectivist(O) Studies(O) ,(O) now(O) known(O) as(O) The(B-organization) Atlas(I-organization) Society(I-organization) ..(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, country, writer, event, poem, organization, person, literary genre, award, location, book and O.\nSentence: In 1990 , after an ideological disagreement with Peikoff , philosopher David Kelley founded the Institute for Objectivist Studies , now known as The Atlas Society ..","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1990",",","after","an","ideological","disagreement","with","Peikoff",",","philosopher","David","Kelley","founded","the","Institute","for","Objectivist","Studies",",","now","known","as","The","Atlas","Society",".."],"labels":["O","O","O","O","O","O","O","O","B-person","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["magazine","country","writer","event","poem","organization","person","literary_genre","award","location","book"]}
{"id":"392","dataset":"crossner_literature","split":"test","instance":{"id":"392","prompt_labels":"Christie(B-writer) had(O) long(O) been(O) a(O) fan(O) of(O) detective(B-literary genre) novels(I-literary genre) ,(O) having(O) enjoyed(O) Wilkie(B-writer) Collins(I-writer) '(O) s(O) The(B-book) Woman(I-book) in(I-book) White(I-book) and(O) The(B-book) Moonstone(I-book) ,(O) as(O) well(O) as(O) Sir(B-writer) Arthur(I-writer) Conan(I-writer) Doyle(I-writer) '(O) s(O) early(B-book) Sherlock(I-book) Holmes(I-book) stories(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, literary genre, organization, location, event, person, writer, poem, book, award, country and O.\nSentence: Christie had long been a fan of detective novels , having enjoyed Wilkie Collins ' s The Woman in White and The Moonstone , as well as Sir Arthur Conan Doyle ' s early Sherlock Holmes stories .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Christie","had","long","been","a","fan","of","detective","novels",",","having","enjoyed","Wilkie","Collins","'","s","The","Woman","in","White","and","The","Moonstone",",","as","well","as","Sir","Arthur","Conan","Doyle","'","s","early","Sherlock","Holmes","stories","."],"labels":["B-writer","O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","O","B-writer","I-writer","O","O","B-book","I-book","I-book","I-book","O","B-book","I-book","O","O","O","O","B-writer","I-writer","I-writer","I-writer","O","O","B-book","I-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["magazine","literary_genre","organization","location","event","person","writer","poem","book","award","country"]}
{"id":"393","dataset":"crossner_literature","split":"test","instance":{"id":"393","prompt_labels":"The(O) Knighthood(O) ((O) i.e.(O) the(O) title(O) of(O) '(O) Sir(O) '(O) )(O) was(O) conferred(O) on(O) him(O) by(O) the(O) same(O) King(B-person) George(I-person) V(I-person) after(O) receiving(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) for(O) Gitanjali(B-book) from(O) the(O) government(O) of(O) Sweden(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, poem, person, location, country, literary genre, book, magazine, award, writer and O.\nSentence: The Knighthood ( i.e. the title of ' Sir ' ) was conferred on him by the same King George V after receiving the Nobel Prize in Literature for Gitanjali from the government of Sweden .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Knighthood","(","i.e.","the","title","of","'","Sir","'",")","was","conferred","on","him","by","the","same","King","George","V","after","receiving","the","Nobel","Prize","in","Literature","for","Gitanjali","from","the","government","of","Sweden","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","I-person","O","O","O","B-award","I-award","I-award","I-award","O","B-book","O","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["organization","event","poem","person","location","country","literary_genre","book","magazine","award","writer"]}
{"id":"394","dataset":"crossner_literature","split":"test","instance":{"id":"394","prompt_labels":"1969(O) ,(O) Crichton(B-writer) also(O) wrote(O) a(O) review(O) for(O) The(B-magazine) New(I-magazine) Republic(I-magazine) ((O) as(O) J.(B-writer) Michael(I-writer) Crichton(I-writer) )(O) ,(O) critiquing(O) Slaughterhouse(B-book) Five(I-book) by(O) Kurt(B-writer) Vonnegut(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, organization, book, poem, writer, event, magazine, literary genre, country, award and O.\nSentence: 1969 , Crichton also wrote a review for The New Republic ( as J. Michael Crichton ) , critiquing Slaughterhouse Five by Kurt Vonnegut .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["1969",",","Crichton","also","wrote","a","review","for","The","New","Republic","(","as","J.","Michael","Crichton",")",",","critiquing","Slaughterhouse","Five","by","Kurt","Vonnegut","."],"labels":["O","O","B-writer","O","O","O","O","O","B-magazine","I-magazine","I-magazine","O","O","B-writer","I-writer","I-writer","O","O","O","B-book","I-book","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["person","location","organization","book","poem","writer","event","magazine","literary_genre","country","award"]}
{"id":"396","dataset":"crossner_literature","split":"test","instance":{"id":"396","prompt_labels":"In(O) November(O) 1813(O) Johann(B-writer) Wolfgang(I-writer) von(I-writer) Goethe(I-writer) invited(O) Schopenhauer(B-writer) for(O) research(O) on(O) his(O) Theory(B-book) of(I-book) Colours(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, location, organization, award, country, event, person, poem, magazine, writer, literary genre and O.\nSentence: In November 1813 Johann Wolfgang von Goethe invited Schopenhauer for research on his Theory of Colours .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","November","1813","Johann","Wolfgang","von","Goethe","invited","Schopenhauer","for","research","on","his","Theory","of","Colours","."],"labels":["O","O","O","B-writer","I-writer","I-writer","I-writer","O","B-writer","O","O","O","O","B-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["book","location","organization","award","country","event","person","poem","magazine","writer","literary_genre"]}
{"id":"397","dataset":"crossner_literature","split":"test","instance":{"id":"397","prompt_labels":"She(O) also(O) read(O) the(O) work(O) of(O) Edith(B-writer) Nesbit(I-writer) ,(O) including(O) The(B-book) Story(I-book) of(I-book) the(I-book) Treasure(I-book) Seekers(I-book) ((O) 1899(O) )(O) ,(O) The(B-book) Phoenix(I-book) and(I-book) the(I-book) Carpet(I-book) ((O) 1903(O) )(O) ,(O) and(O) The(B-book) Railway(I-book) Children(I-book) ((O) 1906(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, organization, book, poem, country, magazine, event, award, literary genre, person, location and O.\nSentence: She also read the work of Edith Nesbit , including The Story of the Treasure Seekers ( 1899 ) , The Phoenix and the Carpet ( 1903 ) , and The Railway Children ( 1906 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","also","read","the","work","of","Edith","Nesbit",",","including","The","Story","of","the","Treasure","Seekers","(","1899",")",",","The","Phoenix","and","the","Carpet","(","1903",")",",","and","The","Railway","Children","(","1906",")","."],"labels":["O","O","O","O","O","O","B-writer","I-writer","O","O","B-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["writer","organization","book","poem","country","magazine","event","award","literary_genre","person","location"]}
{"id":"398","dataset":"crossner_literature","split":"test","instance":{"id":"398","prompt_labels":"Other(O) ancient(B-literary genre) epic(I-literary genre) poetry(I-literary genre) includes(O) the(O) Greek(B-literary genre) epics(I-literary genre) ,(O) the(O) Iliad(B-book) and(O) the(O) Odyssey(B-book) ;(O) the(O) Avestan(O) books(O) ,(O) the(O) Gathic(B-book) Avesta(I-book) and(O) the(O) Yasna(B-book) ;(O) the(O) Ancient(B-country) Rome(I-country) national(O) epic(O) ,(O) Virgil(B-writer) '(O) s(O) Aeneid(B-book) ((O) written(O) between(O) 29(O) and(O) 19(O) BCE(O) )(O) ;(O) and(O) the(O) Indian(B-literary genre) epics(I-literary genre) ,(O) the(O) Ramayana(B-book) and(O) the(O) Mahabharata(B-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, writer, literary genre, poem, location, magazine, book, organization, country, person and O.\nSentence: Other ancient epic poetry includes the Greek epics , the Iliad and the Odyssey ; the Avestan books , the Gathic Avesta and the Yasna ; the Ancient Rome national epic , Virgil ' s Aeneid ( written between 29 and 19 BCE ) ; and the Indian epics , the Ramayana and the Mahabharata .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","ancient","epic","poetry","includes","the","Greek","epics",",","the","Iliad","and","the","Odyssey",";","the","Avestan","books",",","the","Gathic","Avesta","and","the","Yasna",";","the","Ancient","Rome","national","epic",",","Virgil","'","s","Aeneid","(","written","between","29","and","19","BCE",")",";","and","the","Indian","epics",",","the","Ramayana","and","the","Mahabharata","."],"labels":["O","B-literary genre","I-literary genre","I-literary genre","O","O","B-literary genre","I-literary genre","O","O","B-book","O","O","B-book","O","O","O","O","O","O","B-book","I-book","O","O","B-book","O","O","B-country","I-country","O","O","O","B-writer","O","O","B-book","O","O","O","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","B-book","O","O","B-book","O"],"target_index":null,"target_label":null},"label_list":["event","award","writer","literary_genre","poem","location","magazine","book","organization","country","person"]}
{"id":"399","dataset":"crossner_literature","split":"test","instance":{"id":"399","prompt_labels":"Their(O) first(O) child(O) ,(O) a(O) daughter(O) ,(O) Maria(B-person) Teresa(I-person) ,(O) was(O) born(O) on(O) 9(O) March(O) 1938(O) and(O) a(O) son(O) ,(O) Auberon(B-writer) Waugh(I-writer) ,(O) on(O) 17(O) November(O) 1939(O) Hastings(B-writer) ,(O) pp.(O) 336(O) and(O) 392(O) Between(O) these(O) events(O) ,(O) Scoop(B-book) was(O) published(O) in(O) May(O) 1938(O) to(O) wide(O) critical(O) acclaim.(O) Stannard(B-person) ,(O) Vol.(O) i(O) pp.(O) 470-71(O) In(O) August(O) 1938(O) Waugh(B-writer) ,(O) with(O) Laura(B-person) ,(O) made(O) a(O) three-month(O) trip(O) to(O) Mexico(B-country) after(O) which(O) he(O) wrote(O) Robbery(B-book) Under(I-book) Law(I-book) ,(O) based(O) on(O) his(O) experiences(O) there(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, magazine, book, writer, award, poem, country, organization, event, location, literary genre and O.\nSentence: Their first child , a daughter , Maria Teresa , was born on 9 March 1938 and a son , Auberon Waugh , on 17 November 1939 Hastings , pp. 336 and 392 Between these events , Scoop was published in May 1938 to wide critical acclaim. Stannard , Vol. i pp. 470-71 In August 1938 Waugh , with Laura , made a three-month trip to Mexico after which he wrote Robbery Under Law , based on his experiences there .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Their","first","child",",","a","daughter",",","Maria","Teresa",",","was","born","on","9","March","1938","and","a","son",",","Auberon","Waugh",",","on","17","November","1939","Hastings",",","pp.","336","and","392","Between","these","events",",","Scoop","was","published","in","May","1938","to","wide","critical","acclaim.","Stannard",",","Vol.","i","pp.","470-71","In","August","1938","Waugh",",","with","Laura",",","made","a","three-month","trip","to","Mexico","after","which","he","wrote","Robbery","Under","Law",",","based","on","his","experiences","there","."],"labels":["O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","B-book","O","O","O","O","O","O","O","O","O","B-person","O","O","O","O","O","O","O","O","B-writer","O","O","B-person","O","O","O","O","O","O","B-country","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","magazine","book","writer","award","poem","country","organization","event","location","literary_genre"]}
{"id":"401","dataset":"crossner_literature","split":"test","instance":{"id":"401","prompt_labels":"He(O) served(O) as(O) President(O) of(O) the(O) Academy(B-organization) of(I-organization) Motion(I-organization) Picture(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) ,(O) worked(O) alongside(O) the(O) Writers(B-organization) Guild(I-organization) of(I-organization) America(I-organization) ,(O) and(O) was(O) head(O) of(O) the(O) Directors(B-organization) Guild(I-organization) of(I-organization) America(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, writer, book, organization, literary genre, person, location, magazine, poem, event, award and O.\nSentence: He served as President of the Academy of Motion Picture Arts and Sciences , worked alongside the Writers Guild of America , and was head of the Directors Guild of America .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","served","as","President","of","the","Academy","of","Motion","Picture","Arts","and","Sciences",",","worked","alongside","the","Writers","Guild","of","America",",","and","was","head","of","the","Directors","Guild","of","America","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["country","writer","book","organization","literary_genre","person","location","magazine","poem","event","award"]}
{"id":"403","dataset":"crossner_literature","split":"test","instance":{"id":"403","prompt_labels":"In(O) 1971(O) ,(O) as(O) a(O) response(O) to(O) the(O) popularity(O) of(O) sexual(O) guidebooks(O) such(O) as(O) The(B-book) Sensuous(I-book) Woman(I-book) ((O) by(O) J(B-writer) )(O) and(O) The(B-book) Sensuous(I-book) Man(I-book) ((O) by(O) M(B-writer) )(O) ,(O) Asimov(B-writer) published(O) The(B-book) Sensuous(I-book) Dirty(I-book) Old(I-book) Man(I-book) under(O) the(O) byline(O) Dr.(B-person) '(I-person) A(I-person)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, writer, literary genre, country, magazine, poem, organization, award, event, book, location and O.\nSentence: In 1971 , as a response to the popularity of sexual guidebooks such as The Sensuous Woman ( by J ) and The Sensuous Man ( by M ) , Asimov published The Sensuous Dirty Old Man under the byline Dr. ' A","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1971",",","as","a","response","to","the","popularity","of","sexual","guidebooks","such","as","The","Sensuous","Woman","(","by","J",")","and","The","Sensuous","Man","(","by","M",")",",","Asimov","published","The","Sensuous","Dirty","Old","Man","under","the","byline","Dr.","'","A"],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","O","O","B-writer","O","O","B-book","I-book","I-book","O","O","B-writer","O","O","B-writer","O","B-book","I-book","I-book","I-book","I-book","O","O","O","B-person","I-person","I-person"],"target_index":null,"target_label":null},"label_list":["person","writer","literary_genre","country","magazine","poem","organization","award","event","book","location"]}
{"id":"404","dataset":"crossner_literature","split":"test","instance":{"id":"404","prompt_labels":"Although(O) its(O) setting(O) is(O) Trojan(B-book) ,(O) Boccaccio(B-writer) 's(O) story(O) is(O) not(O) taken(O) from(O) Greek(O) myth(O) ,(O) but(O) from(O) the(O) Roman(B-book) de(I-book) Troie(I-book) ,(O) a(O) twelfth-century(O) French(O) medieval(O) re-elaboration(O) of(O) the(O) Trojan(B-book) legend(O) by(O) Benot(B-writer) de(I-writer) Sainte-Maure(I-writer) known(O) to(O) Boccaccio(B-writer) in(O) the(O) Latin(O) prose(B-literary genre) version(O) by(O) Guido(B-writer) delle(I-writer) Colonne(I-writer) ((O) Historia(B-poem) destructionis(I-poem) Troiae(I-poem) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, location, person, poem, country, magazine, organization, writer, book, literary genre and O.\nSentence: Although its setting is Trojan , Boccaccio 's story is not taken from Greek myth , but from the Roman de Troie , a twelfth-century French medieval re-elaboration of the Trojan legend by Benot de Sainte-Maure known to Boccaccio in the Latin prose version by Guido delle Colonne ( Historia destructionis Troiae ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Although","its","setting","is","Trojan",",","Boccaccio","'s","story","is","not","taken","from","Greek","myth",",","but","from","the","Roman","de","Troie",",","a","twelfth-century","French","medieval","re-elaboration","of","the","Trojan","legend","by","Benot","de","Sainte-Maure","known","to","Boccaccio","in","the","Latin","prose","version","by","Guido","delle","Colonne","(","Historia","destructionis","Troiae",")","."],"labels":["O","O","O","O","B-book","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","B-book","O","O","B-writer","I-writer","I-writer","O","O","B-writer","O","O","O","B-literary genre","O","O","B-writer","I-writer","I-writer","O","B-poem","I-poem","I-poem","O","O"],"target_index":null,"target_label":null},"label_list":["event","award","location","person","poem","country","magazine","organization","writer","book","literary_genre"]}
{"id":"405","dataset":"crossner_literature","split":"test","instance":{"id":"405","prompt_labels":"See(O) also(O) Two(B-poem) Figures(I-poem) In(I-poem) Dense(I-poem) Violet(I-poem) Night(I-poem) which(O) can(O) be(O) read(O) as(O) a(O) humorous(O) anecdote(O) about(O) the(O) gauche(O) male(O) ,(O) or(O) a(O) meditation(O) on(O) the(O) lover(O) 's(O) otherness(O) ,(O) or(O) the(O) poet(O) 's(O) challenge(O) to(O) the(O) imagination(O) of(O) the(O) reader(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, event, literary genre, person, country, magazine, writer, organization, award, location, book and O.\nSentence: See also Two Figures In Dense Violet Night which can be read as a humorous anecdote about the gauche male , or a meditation on the lover 's otherness , or the poet 's challenge to the imagination of the reader .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["See","also","Two","Figures","In","Dense","Violet","Night","which","can","be","read","as","a","humorous","anecdote","about","the","gauche","male",",","or","a","meditation","on","the","lover","'s","otherness",",","or","the","poet","'s","challenge","to","the","imagination","of","the","reader","."],"labels":["O","O","B-poem","I-poem","I-poem","I-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","event","literary_genre","person","country","magazine","writer","organization","award","location","book"]}
{"id":"407","dataset":"crossner_literature","split":"test","instance":{"id":"407","prompt_labels":"Germany(B-country) '(O) s(O) national(O) poet(O) ,(O) Johann(B-writer) Wolfgang(I-writer) von(I-writer) Goethe(I-writer) ,(O) also(O) wrote(O) many(O) sonnets(B-literary genre) ,(O) using(O) a(O) rhyme(O) scheme(O) derived(O) from(O) Italian(O) poetry(B-literary genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, writer, country, book, event, award, poem, organization, magazine, literary genre, person and O.\nSentence: Germany ' s national poet , Johann Wolfgang von Goethe , also wrote many sonnets , using a rhyme scheme derived from Italian poetry .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Germany","'","s","national","poet",",","Johann","Wolfgang","von","Goethe",",","also","wrote","many","sonnets",",","using","a","rhyme","scheme","derived","from","Italian","poetry","."],"labels":["B-country","O","O","O","O","O","B-writer","I-writer","I-writer","I-writer","O","O","O","O","B-literary genre","O","O","O","O","O","O","O","O","B-literary genre","O"],"target_index":null,"target_label":null},"label_list":["location","writer","country","book","event","award","poem","organization","magazine","literary_genre","person"]}
{"id":"408","dataset":"crossner_literature","split":"test","instance":{"id":"408","prompt_labels":"Ride(O) with(O) the(O) Devil(O) received(O) its(O) world(O) premiere(O) at(O) the(O) 25th(O) Deauville(B-event) American(I-event) Film(I-event) Festival(I-event) in(O) France(B-country) on(O) September(O) 9(O) ,(O) 1999(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, poem, person, location, country, writer, literary genre, award, organization, event, book and O.\nSentence: Ride with the Devil received its world premiere at the 25th Deauville American Film Festival in France on September 9 , 1999 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Ride","with","the","Devil","received","its","world","premiere","at","the","25th","Deauville","American","Film","Festival","in","France","on","September","9",",","1999","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","O","B-country","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","poem","person","location","country","writer","literary_genre","award","organization","event","book"]}
{"id":"409","dataset":"crossner_literature","split":"test","instance":{"id":"409","prompt_labels":"In(O) the(O) late(O) 1980s(O) ,(O) Coppola(B-writer) started(O) considering(O) concepts(O) for(O) a(O) motion(O) picture(O) based(O) upon(O) the(O) 19th-century(O) novel(B-literary genre) The(B-book) Adventures(I-book) of(I-book) Pinocchio(I-book) ,(O) and(O) in(O) 1991(O) Coppola(B-writer) and(O) Warner(B-organization) Bros.(I-organization) began(O) discussing(O) the(O) project(O) as(O) well(O) as(O) two(O) others(O) involving(O) the(O) life(O) of(O) J.(B-writer) Edgar(I-writer) Hoover(I-writer) and(O) the(O) children(O) 's(O) novel(B-literary genre) The(B-book) Secret(I-book) Garden(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, book, event, writer, award, person, literary genre, organization, location, magazine, country and O.\nSentence: In the late 1980s , Coppola started considering concepts for a motion picture based upon the 19th-century novel The Adventures of Pinocchio , and in 1991 Coppola and Warner Bros. began discussing the project as well as two others involving the life of J. Edgar Hoover and the children 's novel The Secret Garden .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","late","1980s",",","Coppola","started","considering","concepts","for","a","motion","picture","based","upon","the","19th-century","novel","The","Adventures","of","Pinocchio",",","and","in","1991","Coppola","and","Warner","Bros.","began","discussing","the","project","as","well","as","two","others","involving","the","life","of","J.","Edgar","Hoover","and","the","children","'s","novel","The","Secret","Garden","."],"labels":["O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","B-literary genre","B-book","I-book","I-book","I-book","O","O","O","O","B-writer","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","I-writer","O","O","O","O","B-literary genre","B-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["poem","book","event","writer","award","person","literary_genre","organization","location","magazine","country"]}
{"id":"410","dataset":"crossner_literature","split":"test","instance":{"id":"410","prompt_labels":"Among(O) his(O) earliest(O) influences(O) ,(O) Simon(B-writer) cited(O) Norman(B-writer) Angell(I-writer) for(O) his(O) book(O) The(B-book) Great(I-book) Illusion(I-book) and(O) Henry(B-writer) George(I-writer) for(O) his(O) book(O) Progress(B-book) and(I-book) Poverty(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, writer, location, magazine, book, poem, organization, literary genre, country, award and O.\nSentence: Among his earliest influences , Simon cited Norman Angell for his book The Great Illusion and Henry George for his book Progress and Poverty .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Among","his","earliest","influences",",","Simon","cited","Norman","Angell","for","his","book","The","Great","Illusion","and","Henry","George","for","his","book","Progress","and","Poverty","."],"labels":["O","O","O","O","O","B-writer","O","B-writer","I-writer","O","O","O","B-book","I-book","I-book","O","B-writer","I-writer","O","O","O","B-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["event","person","writer","location","magazine","book","poem","organization","literary_genre","country","award"]}
{"id":"411","dataset":"crossner_literature","split":"test","instance":{"id":"411","prompt_labels":"The(O) several(O) characters(O) live(O) within(O) a(O) divided(O) United(B-country) States(I-country) ,(O) in(O) which(O) the(O) Empire(B-country) of(I-country) Japan(I-country) takes(O) the(O) Pacific(B-location) states(I-location) ,(O) governing(O) them(O) as(O) a(O) puppet(O) ,(O) Nazi(B-country) Germany(I-country) takes(O) the(O) East(B-location) Coast(I-location) of(I-location) the(I-location) United(I-location) States(I-location) and(O) parts(O) of(O) the(O) Midwest(B-location) ,(O) with(O) the(O) remnants(O) of(O) the(O) old(O) United(B-country) States(I-country) '(O) government(O) as(O) the(O) Neutral(B-location) Zone(I-location) ,(O) a(O) buffer(O) state(O) between(O) the(O) two(O) superpowers(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, organization, literary genre, book, poem, award, person, location, event, magazine, country and O.\nSentence: The several characters live within a divided United States , in which the Empire of Japan takes the Pacific states , governing them as a puppet , Nazi Germany takes the East Coast of the United States and parts of the Midwest , with the remnants of the old United States ' government as the Neutral Zone , a buffer state between the two superpowers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","several","characters","live","within","a","divided","United","States",",","in","which","the","Empire","of","Japan","takes","the","Pacific","states",",","governing","them","as","a","puppet",",","Nazi","Germany","takes","the","East","Coast","of","the","United","States","and","parts","of","the","Midwest",",","with","the","remnants","of","the","old","United","States","'","government","as","the","Neutral","Zone",",","a","buffer","state","between","the","two","superpowers","."],"labels":["O","O","O","O","O","O","O","B-country","I-country","O","O","O","O","B-country","I-country","I-country","O","O","B-location","I-location","O","O","O","O","O","O","O","B-country","I-country","O","O","B-location","I-location","I-location","I-location","I-location","I-location","O","O","O","O","B-location","O","O","O","O","O","O","O","B-country","I-country","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["writer","organization","literary_genre","book","poem","award","person","location","event","magazine","country"]}
{"id":"413","dataset":"crossner_literature","split":"test","instance":{"id":"413","prompt_labels":"Eisenhower(B-person) 's(O) stint(O) as(O) the(O) president(O) of(O) Columbia(B-organization) University(I-organization) was(O) punctuated(O) by(O) his(O) activity(O) within(O) the(O) Council(B-organization) on(I-organization) Foreign(I-organization) Relations(I-organization) ,(O) a(O) study(O) group(O) he(O) led(O) as(O) president(O) concerning(O) the(O) political(O) and(O) military(O) implications(O) of(O) the(O) Marshall(O) Plan(O) ,(O) and(O) The(B-organization) American(I-organization) Assembly(I-organization) ,(O) Eisenhower(B-person) 's(O) vision(O) of(O) a(O) great(O) cultural(O) center(O) where(O) business(O) ,(O) professional(O) and(O) governmental(O) leaders(O) could(O) meet(O) from(O) time(O) to(O) time(O) to(O) discuss(O) and(O) reach(O) conclusions(O) concerning(O) problems(O) of(O) a(O) social(O) and(O) political(O) nature(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, literary genre, poem, event, location, magazine, writer, country, award, book and O.\nSentence: Eisenhower 's stint as the president of Columbia University was punctuated by his activity within the Council on Foreign Relations , a study group he led as president concerning the political and military implications of the Marshall Plan , and The American Assembly , Eisenhower 's vision of a great cultural center where business , professional and governmental leaders could meet from time to time to discuss and reach conclusions concerning problems of a social and political nature .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Eisenhower","'s","stint","as","the","president","of","Columbia","University","was","punctuated","by","his","activity","within","the","Council","on","Foreign","Relations",",","a","study","group","he","led","as","president","concerning","the","political","and","military","implications","of","the","Marshall","Plan",",","and","The","American","Assembly",",","Eisenhower","'s","vision","of","a","great","cultural","center","where","business",",","professional","and","governmental","leaders","could","meet","from","time","to","time","to","discuss","and","reach","conclusions","concerning","problems","of","a","social","and","political","nature","."],"labels":["B-person","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","person","literary_genre","poem","event","location","magazine","writer","country","award","book"]}
{"id":"414","dataset":"crossner_literature","split":"test","instance":{"id":"414","prompt_labels":"He(O) proposed(O) that(O) Germany(B-country) be(O) reunified(O) and(O) accept(O) the(O) Oder-Neisse(B-location) line(I-location) as(O) its(O) border(O) ,(O) and(O) that(O) a(O) neutral(O) zone(O) be(O) established(O) in(O) Central(B-location) Europe(I-location) ,(O) consisting(O) at(O) the(O) minimum(O) of(O) Germany(B-country) ,(O) Poland(B-country) ,(O) Hungary(B-country) ,(O) and(O) Czechoslovakia(B-country) ,(O) with(O) each(O) of(O) these(O) countries(O) being(O) free(O) of(O) foreign(O) troops(O) and(O) influence(O) ,(O) and(O) prohibited(O) from(O) forming(O) alliances(O) with(O) countries(O) outside(O) the(O) zone(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, writer, literary genre, poem, award, event, magazine, book, organization, location, country and O.\nSentence: He proposed that Germany be reunified and accept the Oder-Neisse line as its border , and that a neutral zone be established in Central Europe , consisting at the minimum of Germany , Poland , Hungary , and Czechoslovakia , with each of these countries being free of foreign troops and influence , and prohibited from forming alliances with countries outside the zone .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","proposed","that","Germany","be","reunified","and","accept","the","Oder-Neisse","line","as","its","border",",","and","that","a","neutral","zone","be","established","in","Central","Europe",",","consisting","at","the","minimum","of","Germany",",","Poland",",","Hungary",",","and","Czechoslovakia",",","with","each","of","these","countries","being","free","of","foreign","troops","and","influence",",","and","prohibited","from","forming","alliances","with","countries","outside","the","zone","."],"labels":["O","O","O","B-country","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","B-country","O","B-country","O","B-country","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","writer","literary_genre","poem","award","event","magazine","book","organization","location","country"]}
{"id":"415","dataset":"crossner_literature","split":"test","instance":{"id":"415","prompt_labels":"Shortly(O) thereafter(O) ,(O) Bloch(B-writer) created(O) the(O) Damon(B-writer) Runyon(I-writer) -esque(O) humorous(O) series(O) character(O) Lefty(B-person) Feep(I-person) in(O) the(O) story(O) Time(B-book) Wounds(I-book) All(I-book) Heels(I-book) Fantastic(B-magazine) Adventures(I-magazine) ((O) April(O) 1942(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, event, location, book, poem, country, award, person, writer, literary genre, organization and O.\nSentence: Shortly thereafter , Bloch created the Damon Runyon -esque humorous series character Lefty Feep in the story Time Wounds All Heels Fantastic Adventures ( April 1942 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Shortly","thereafter",",","Bloch","created","the","Damon","Runyon","-esque","humorous","series","character","Lefty","Feep","in","the","story","Time","Wounds","All","Heels","Fantastic","Adventures","(","April","1942",")","."],"labels":["O","O","O","B-writer","O","O","B-writer","I-writer","O","O","O","O","B-person","I-person","O","O","O","B-book","I-book","I-book","I-book","B-magazine","I-magazine","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","event","location","book","poem","country","award","person","writer","literary_genre","organization"]}
{"id":"0","dataset":"crossner_music","split":"test","instance":{"id":"0","prompt_labels":"It(O) is(O) based(O) on(O) the(O) classic(O) song(O) The(B-song) Guns(I-song) of(I-song) Brixton(I-song) on(O) The(B-band) Clash(I-band) '(O) s(O) London(B-album) Calling(I-album) and(O) has(O) proven(O) to(O) be(O) a(O) success(O) on(O) the(O) modern(O) rock(B-music genre) charts(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, song, award, location, event, organization, band, country, music genre, album, person, musical instrument and O.\nSentence: It is based on the classic song The Guns of Brixton on The Clash ' s London Calling and has proven to be a success on the modern rock charts .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","is","based","on","the","classic","song","The","Guns","of","Brixton","on","The","Clash","'","s","London","Calling","and","has","proven","to","be","a","success","on","the","modern","rock","charts","."],"labels":["O","O","O","O","O","O","O","B-song","I-song","I-song","I-song","O","B-band","I-band","O","O","B-album","I-album","O","O","O","O","O","O","O","O","O","O","B-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","song","award","location","event","organization","band","country","music_genre","album","person","musical_instrument"]}
{"id":"1","dataset":"crossner_music","split":"test","instance":{"id":"1","prompt_labels":"As(O) a(O) strong(O) advocate(O) of(O) animal(O) rights(O) ,(O) Linda(B-musical artist) lent(O) her(O) support(O) to(O) many(O) organizations(O) such(O) as(O) People(B-organization) for(I-organization) the(I-organization) Ethical(I-organization) Treatment(I-organization) of(I-organization) Animals(I-organization) ((O) PETA(B-organization) )(O) ,(O) the(O) Campaign(B-organization) to(I-organization) Protect(I-organization) Rural(I-organization) England(I-organization) ,(O) and(O) Friends(B-organization) of(I-organization) the(I-organization) Earth(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, musical instrument, event, song, band, album, musical artist, music genre, country, location, award, organization and O.\nSentence: As a strong advocate of animal rights , Linda lent her support to many organizations such as People for the Ethical Treatment of Animals ( PETA ) , the Campaign to Protect Rural England , and Friends of the Earth .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["As","a","strong","advocate","of","animal","rights",",","Linda","lent","her","support","to","many","organizations","such","as","People","for","the","Ethical","Treatment","of","Animals","(","PETA",")",",","the","Campaign","to","Protect","Rural","England",",","and","Friends","of","the","Earth","."],"labels":["O","O","O","O","O","O","O","O","B-musical artist","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["person","musical_instrument","event","song","band","album","musical_artist","music_genre","country","location","award","organization"]}
{"id":"3","dataset":"crossner_music","split":"test","instance":{"id":"3","prompt_labels":"Jazz(B-music genre) accordionists(O) from(O) the(O) United(B-country) States(I-country) include(O) Steve(B-musical artist) Bach(I-musical artist) ,(O) Milton(B-musical artist) DeLugg(I-musical artist) ,(O) Orlando(B-musical artist) DiGirolamo(I-musical artist) ,(O) Dominic(B-musical artist) Frontiere(I-musical artist) ,(O) Guy(B-musical artist) Klucevsek(I-musical artist) ,(O) Yuri(B-musical artist) Lemeshev(I-musical artist) ,(O) Frank(B-musical artist) Marocco(I-musical artist) ,(O) John(B-musical artist) Serry(I-musical artist) Sr.(I-musical artist) ,(O) Lee(B-musical artist) Tomboulian(I-musical artist) ,(O) and(O) Art(B-musical artist) Van(I-musical artist) Damme(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, person, band, album, musical artist, song, location, musical instrument, award, music genre, country and O.\nSentence: Jazz accordionists from the United States include Steve Bach , Milton DeLugg , Orlando DiGirolamo , Dominic Frontiere , Guy Klucevsek , Yuri Lemeshev , Frank Marocco , John Serry Sr. , Lee Tomboulian , and Art Van Damme .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Jazz","accordionists","from","the","United","States","include","Steve","Bach",",","Milton","DeLugg",",","Orlando","DiGirolamo",",","Dominic","Frontiere",",","Guy","Klucevsek",",","Yuri","Lemeshev",",","Frank","Marocco",",","John","Serry","Sr.",",","Lee","Tomboulian",",","and","Art","Van","Damme","."],"labels":["B-music genre","O","O","O","B-country","I-country","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["organization","event","person","band","album","musical_artist","song","location","musical_instrument","award","music_genre","country"]}
{"id":"4","dataset":"crossner_music","split":"test","instance":{"id":"4","prompt_labels":"None(O) of(O) the(O) singles(O) from(O) the(O) album(O) reached(O) number(O) one(O) on(O) the(O) UK(B-country) charts(O) ,(O) but(O) Chiquitita(B-song) ,(O) Does(B-song) Your(I-song) Mother(I-song) Know(I-song) ,(O) Angeleyes(B-song) ((O) with(O) Voulez-Vous(B-album) ,(O) released(O) as(O) a(O) double(O) A-side(O) )(O) and(O) I(B-song) Have(I-song) a(I-song) Dream(I-song) were(O) all(O) UK(B-country) Top(O) 5(O) hits(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, musical artist, country, song, band, organization, music genre, location, musical instrument, event, album and O.\nSentence: None of the singles from the album reached number one on the UK charts , but Chiquitita , Does Your Mother Know , Angeleyes ( with Voulez-Vous , released as a double A-side ) and I Have a Dream were all UK Top 5 hits .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["None","of","the","singles","from","the","album","reached","number","one","on","the","UK","charts",",","but","Chiquitita",",","Does","Your","Mother","Know",",","Angeleyes","(","with","Voulez-Vous",",","released","as","a","double","A-side",")","and","I","Have","a","Dream","were","all","UK","Top","5","hits","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","B-song","O","B-song","I-song","I-song","I-song","O","B-song","O","O","B-album","O","O","O","O","O","O","O","O","B-song","I-song","I-song","I-song","O","O","B-country","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","award","musical_artist","country","song","band","organization","music_genre","location","musical_instrument","event","album"]}
{"id":"5","dataset":"crossner_music","split":"test","instance":{"id":"5","prompt_labels":"Plaid(B-musical artist) went(O) on(O) to(O) release(O) several(O) more(O) albums(O) ,(O) including(O) Trainer(B-album) ((O) a(O) compilation(O) of(O) early(O) Plaid(B-musical artist) work(O) ,(O) including(O) the(O) complete(O) Mbuki(B-album) Mvuki(I-album) album(O) )(O) ,(O) Double(B-album) Figure(I-album) ,(O) Spokes(B-album) ,(O) Parts(B-album) in(I-album) the(I-album) Post(I-album) ((O) a(O) compilation(O) of(O) Plaid(B-musical artist) remixes(O) )(O) ,(O) Greedy(B-album) Baby(I-album) ((O) an(O) audio-visual(O) collaboration(O) with(O) Bob(B-musical artist) Jaroc(I-musical artist) )(O) ,(O) Scintilli(B-album) ,(O) Reachy(B-album) Prints(I-album) and(O) The(B-album) Digging(I-album) Remedy(I-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, album, band, award, musical instrument, event, person, song, musical artist, organization, music genre and O.\nSentence: Plaid went on to release several more albums , including Trainer ( a compilation of early Plaid work , including the complete Mbuki Mvuki album ) , Double Figure , Spokes , Parts in the Post ( a compilation of Plaid remixes ) , Greedy Baby ( an audio-visual collaboration with Bob Jaroc ) , Scintilli , Reachy Prints and The Digging Remedy .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Plaid","went","on","to","release","several","more","albums",",","including","Trainer","(","a","compilation","of","early","Plaid","work",",","including","the","complete","Mbuki","Mvuki","album",")",",","Double","Figure",",","Spokes",",","Parts","in","the","Post","(","a","compilation","of","Plaid","remixes",")",",","Greedy","Baby","(","an","audio-visual","collaboration","with","Bob","Jaroc",")",",","Scintilli",",","Reachy","Prints","and","The","Digging","Remedy","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","B-album","O","O","O","O","O","B-musical artist","O","O","O","O","O","B-album","I-album","O","O","O","B-album","I-album","O","B-album","O","B-album","I-album","I-album","I-album","O","O","O","O","B-musical artist","O","O","O","B-album","I-album","O","O","O","O","O","B-musical artist","I-musical artist","O","O","B-album","O","B-album","I-album","O","B-album","I-album","I-album","O"],"target_index":null,"target_label":null},"label_list":["country","location","album","band","award","musical_instrument","event","person","song","musical_artist","organization","music_genre"]}
{"id":"6","dataset":"crossner_music","split":"test","instance":{"id":"6","prompt_labels":"The(O) two(O) rappers(O) have(O) resolved(O) their(O) disagreement(O) and(O) have(O) collaborated(O) on(O) four(O) songs(O) :(O) Wish(B-song) You(I-song) Would(I-song) off(O) Ludacris(B-musical artist) 's(O) sixth(O) studio(O) album(O) ,(O) Theater(B-album) of(I-album) the(I-album) Mind(I-album) ,(O) On(B-song) Top(I-song) of(I-song) the(I-song) World(I-song) off(O) T.I.(B-musical artist) '(O) s(O) sixth(O) studio(O) album(O) ,(O) Paper(B-album) Trail(I-album) and(O) We(B-song) in(I-song) This(I-song) Bitch(I-song) off(O) DJ(B-musical artist) Drama(I-musical artist) 's(O) album(O) ,(O) Quality(B-album) Street(I-album) Music(I-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, location, event, musical instrument, person, album, award, band, musical artist, music genre, organization, country and O.\nSentence: The two rappers have resolved their disagreement and have collaborated on four songs : Wish You Would off Ludacris 's sixth studio album , Theater of the Mind , On Top of the World off T.I. ' s sixth studio album , Paper Trail and We in This Bitch off DJ Drama 's album , Quality Street Music .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","two","rappers","have","resolved","their","disagreement","and","have","collaborated","on","four","songs",":","Wish","You","Would","off","Ludacris","'s","sixth","studio","album",",","Theater","of","the","Mind",",","On","Top","of","the","World","off","T.I.","'","s","sixth","studio","album",",","Paper","Trail","and","We","in","This","Bitch","off","DJ","Drama","'s","album",",","Quality","Street","Music","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-song","I-song","I-song","O","B-musical artist","O","O","O","O","O","B-album","I-album","I-album","I-album","O","B-song","I-song","I-song","I-song","I-song","O","B-musical artist","O","O","O","O","O","O","B-album","I-album","O","B-song","I-song","I-song","I-song","O","B-musical artist","I-musical artist","O","O","O","B-album","I-album","I-album","O"],"target_index":null,"target_label":null},"label_list":["song","location","event","musical_instrument","person","album","award","band","musical_artist","music_genre","organization","country"]}
{"id":"7","dataset":"crossner_music","split":"test","instance":{"id":"7","prompt_labels":"Often(O) termed(O) soul(B-music genre) blues(I-music genre) or(O) Southern(B-music genre) soul(I-music genre) ,(O) the(O) music(O) at(O) the(O) heart(O) of(O) this(O) movement(O) was(O) given(O) new(O) life(O) by(O) the(O) unexpected(O) success(O) of(O) two(O) particular(O) recordings(O) on(O) the(O) Jackson-based(O) Malaco(B-organization) label(I-organization) :(O) Z.(B-musical artist) Z.(I-musical artist) Hill(I-musical artist) '(O) s(O) Down(B-song) Home(I-song) Blues(I-song) ((O) 1982(O) )(O) and(O) Little(B-musical artist) Milton(I-musical artist) '(O) s(O) The(B-song) Blues(I-song) is(I-song) Alright(I-song) ((O) 1984(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, person, song, album, event, organization, location, musical instrument, music genre, band, musical artist and O.\nSentence: Often termed soul blues or Southern soul , the music at the heart of this movement was given new life by the unexpected success of two particular recordings on the Jackson-based Malaco label : Z. Z. Hill ' s Down Home Blues ( 1982 ) and Little Milton ' s The Blues is Alright ( 1984 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Often","termed","soul","blues","or","Southern","soul",",","the","music","at","the","heart","of","this","movement","was","given","new","life","by","the","unexpected","success","of","two","particular","recordings","on","the","Jackson-based","Malaco","label",":","Z.","Z.","Hill","'","s","Down","Home","Blues","(","1982",")","and","Little","Milton","'","s","The","Blues","is","Alright","(","1984",")","."],"labels":["O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","B-musical artist","I-musical artist","I-musical artist","O","O","B-song","I-song","I-song","O","O","O","O","B-musical artist","I-musical artist","O","O","B-song","I-song","I-song","I-song","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","award","person","song","album","event","organization","location","musical_instrument","music_genre","band","musical_artist"]}
{"id":"8","dataset":"crossner_music","split":"test","instance":{"id":"8","prompt_labels":"The(O) Belgium(B-country) group(O) Technotronic(B-band) scored(O) an(O) international(O) hit(O) with(O) the(O) song(O) Pump(B-song) Up(I-song) the(I-song) Jam(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, music genre, award, event, country, album, musical instrument, person, song, band, location, organization and O.\nSentence: The Belgium group Technotronic scored an international hit with the song Pump Up the Jam .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Belgium","group","Technotronic","scored","an","international","hit","with","the","song","Pump","Up","the","Jam","."],"labels":["O","B-country","O","B-band","O","O","O","O","O","O","O","B-song","I-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","music_genre","award","event","country","album","musical_instrument","person","song","band","location","organization"]}
{"id":"9","dataset":"crossner_music","split":"test","instance":{"id":"9","prompt_labels":"Following(O) this(O) success(O) ,(O) Francis(B-musical artist) recorded(O) seven(O) more(O) albums(O) of(O) favorites(O) between(O) 1960(O) and(O) 1964(O) ,(O) including(O) Connie(B-album) Francis(I-album) Sings(I-album) Jewish(I-album) Favorites(I-album) ,(O) Connie(B-album) Francis(I-album) Sings(I-album) German(I-album) Favorites(I-album) ,(O) and(O) Connie(B-album) Francis(I-album) Sings(I-album) Irish(I-album) Favorites(I-album) ,(O) among(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, band, event, album, location, organization, music genre, country, song, musical instrument, musical artist and O.\nSentence: Following this success , Francis recorded seven more albums of favorites between 1960 and 1964 , including Connie Francis Sings Jewish Favorites , Connie Francis Sings German Favorites , and Connie Francis Sings Irish Favorites , among others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Following","this","success",",","Francis","recorded","seven","more","albums","of","favorites","between","1960","and","1964",",","including","Connie","Francis","Sings","Jewish","Favorites",",","Connie","Francis","Sings","German","Favorites",",","and","Connie","Francis","Sings","Irish","Favorites",",","among","others","."],"labels":["O","O","O","O","B-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","I-album","I-album","O","B-album","I-album","I-album","I-album","I-album","O","O","B-album","I-album","I-album","I-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","person","band","event","album","location","organization","music_genre","country","song","musical_instrument","musical_artist"]}
{"id":"10","dataset":"crossner_music","split":"test","instance":{"id":"10","prompt_labels":"With(O) Fredriksson(B-musical artist) 's(O) continued(O) treatment(O) and(O) recuperation(O) he(O) made(O) three(O) more(O) albums(O) ,(O) Son(B-album) of(I-album) a(I-album) Plumber(I-album) ((O) 2005(O) )(O) ,(O) En(B-album) hndig(I-album) man(I-album) ((O) A(B-album) handy(I-album) man(I-album) ,(O) 2007(O) )(O) and(O) Party(B-album) Crasher(I-album) ((O) 2008(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, award, song, band, musical instrument, person, location, music genre, country, organization, album, event and O.\nSentence: With Fredriksson 's continued treatment and recuperation he made three more albums , Son of a Plumber ( 2005 ) , En hndig man ( A handy man , 2007 ) and Party Crasher ( 2008 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["With","Fredriksson","'s","continued","treatment","and","recuperation","he","made","three","more","albums",",","Son","of","a","Plumber","(","2005",")",",","En","hndig","man","(","A","handy","man",",","2007",")","and","Party","Crasher","(","2008",")","."],"labels":["O","B-musical artist","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","O","B-album","I-album","I-album","O","O","O","O","B-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","award","song","band","musical_instrument","person","location","music_genre","country","organization","album","event"]}
{"id":"11","dataset":"crossner_music","split":"test","instance":{"id":"11","prompt_labels":"In(O) April(O) 2019(O) ,(O) Phair(B-musical artist) announced(O) via(O) her(O) Instagram(O) that(O) she(O) had(O) been(O) working(O) on(O) new(O) studio(O) material(O) with(O) Brad(B-musical artist) Wood(I-musical artist) ,(O) who(O) produced(O) Exile(B-album) in(I-album) Guyville(I-album) ,(O) Whip-Smart(B-album) ,(O) and(O) parts(O) of(O) Whitechocolatespaceegg(B-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, music genre, organization, band, event, musical artist, person, song, award, musical instrument, album and O.\nSentence: In April 2019 , Phair announced via her Instagram that she had been working on new studio material with Brad Wood , who produced Exile in Guyville , Whip-Smart , and parts of Whitechocolatespaceegg .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","April","2019",",","Phair","announced","via","her","Instagram","that","she","had","been","working","on","new","studio","material","with","Brad","Wood",",","who","produced","Exile","in","Guyville",",","Whip-Smart",",","and","parts","of","Whitechocolatespaceegg","."],"labels":["O","O","O","O","B-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","O","B-album","I-album","I-album","O","B-album","O","O","O","O","B-album","O"],"target_index":null,"target_label":null},"label_list":["country","location","music_genre","organization","band","event","musical_artist","person","song","award","musical_instrument","album"]}
{"id":"12","dataset":"crossner_music","split":"test","instance":{"id":"12","prompt_labels":"Annie(O) Hall(O) ((O) 1977(O) )(O) won(O) four(O) Academy(B-award) Awards(I-award) ,(O) including(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) for(O) Diane(B-person) Keaton(I-person) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Screenplay(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) for(O) Woody(B-person) Allen(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, person, event, song, musical instrument, music genre, location, musical artist, country, organization, band, award and O.\nSentence: Annie Hall ( 1977 ) won four Academy Awards , including Academy Award for Best Picture , Academy Award for Best Actress for Diane Keaton , Academy Award for Best Original Screenplay and Academy Award for Best Director for Woody Allen .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Annie","Hall","(","1977",")","won","four","Academy","Awards",",","including","Academy","Award","for","Best","Picture",",","Academy","Award","for","Best","Actress","for","Diane","Keaton",",","Academy","Award","for","Best","Original","Screenplay","and","Academy","Award","for","Best","Director","for","Woody","Allen","."],"labels":["O","O","O","O","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","B-person","I-person","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["album","person","event","song","musical_instrument","music_genre","location","musical_artist","country","organization","band","award"]}
{"id":"13","dataset":"crossner_music","split":"test","instance":{"id":"13","prompt_labels":"Also(O) in(O) 1993(O) ,(O) the(O) band(O) contributed(O) the(O) track(O) It(O) 's(O) the(B-song) New(I-song) Style(I-song) ((O) with(O) DJ(B-musical artist) Hurricane(I-musical artist) )(O) to(O) the(O) AIDS(O) benefit(O) album(O) No(B-album) Alternative(I-album) ,(O) produced(O) by(O) the(O) Red(B-organization) Hot(I-organization) Organization(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, song, musical artist, music genre, event, band, musical instrument, organization, album, location, award and O.\nSentence: Also in 1993 , the band contributed the track It 's the New Style ( with DJ Hurricane ) to the AIDS benefit album No Alternative , produced by the Red Hot Organization .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Also","in","1993",",","the","band","contributed","the","track","It","'s","the","New","Style","(","with","DJ","Hurricane",")","to","the","AIDS","benefit","album","No","Alternative",",","produced","by","the","Red","Hot","Organization","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-song","I-song","I-song","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","B-album","I-album","O","O","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["country","person","song","musical_artist","music_genre","event","band","musical_instrument","organization","album","location","award"]}
{"id":"14","dataset":"crossner_music","split":"test","instance":{"id":"14","prompt_labels":"Often(O) cited(O) as(O) a(O) pop(O) icon(O) ,(O) Timberlake(B-musical artist) is(O) the(O) recipient(O) of(O) numerous(O) awards(O) and(O) accolades(O) ,(O) including(O) ten(O) Grammy(B-award) Award(I-award) s(O) ,(O) four(O) Emmy(B-award) Award(I-award) s(O) ,(O) three(O) Brit(B-award) Awards(I-award) ,(O) nine(O) Billboard(B-award) Music(I-award) Awards(I-award) ,(O) the(O) Contemporary(B-award) Icon(I-award) Award(I-award) by(O) the(O) Songwriters(B-organization) Hall(I-organization) of(I-organization) Fame(I-organization) ,(O) an(O) honorary(B-award) Doctor(I-award) of(I-award) Music(I-award) degree(I-award) from(O) the(O) Berklee(B-organization) College(I-organization) of(I-organization) Music(I-organization) ,(O) and(O) the(O) Michael(B-award) Jackson(I-award) Video(I-award) Vanguard(I-award) Award(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, event, musical artist, album, music genre, location, country, musical instrument, band, organization, person, award and O.\nSentence: Often cited as a pop icon , Timberlake is the recipient of numerous awards and accolades , including ten Grammy Award s , four Emmy Award s , three Brit Awards , nine Billboard Music Awards , the Contemporary Icon Award by the Songwriters Hall of Fame , an honorary Doctor of Music degree from the Berklee College of Music , and the Michael Jackson Video Vanguard Award .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Often","cited","as","a","pop","icon",",","Timberlake","is","the","recipient","of","numerous","awards","and","accolades",",","including","ten","Grammy","Award","s",",","four","Emmy","Award","s",",","three","Brit","Awards",",","nine","Billboard","Music","Awards",",","the","Contemporary","Icon","Award","by","the","Songwriters","Hall","of","Fame",",","an","honorary","Doctor","of","Music","degree","from","the","Berklee","College","of","Music",",","and","the","Michael","Jackson","Video","Vanguard","Award","."],"labels":["O","O","O","O","O","O","O","B-musical artist","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","O","B-award","I-award","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","O","O","B-award","I-award","I-award","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-award","I-award","I-award","I-award","I-award","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","B-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["song","event","musical_artist","album","music_genre","location","country","musical_instrument","band","organization","person","award"]}
{"id":"16","dataset":"crossner_music","split":"test","instance":{"id":"16","prompt_labels":"The(O) film(O) won(O) Academy(B-award) Awards(I-award) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Costume(I-award) Design(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Makeup(I-award) and(I-award) Hairstyling(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Sound(I-award) Editing(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, music genre, person, musical instrument, award, organization, country, band, event, song, album, musical artist and O.\nSentence: The film won Academy Awards for Academy Award for Best Costume Design , Academy Award for Best Makeup and Hairstyling and Academy Award for Best Sound Editing .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","film","won","Academy","Awards","for","Academy","Award","for","Best","Costume","Design",",","Academy","Award","for","Best","Makeup","and","Hairstyling","and","Academy","Award","for","Best","Sound","Editing","."],"labels":["O","O","O","B-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["location","music_genre","person","musical_instrument","award","organization","country","band","event","song","album","musical_artist"]}
{"id":"17","dataset":"crossner_music","split":"test","instance":{"id":"17","prompt_labels":"In(O) the(O) early(O) 2010s(O) ,(O) there(O) was(O) somewhat(O) of(O) a(O) resurgence(O) of(O) boy(O) band(O) popularity(O) in(O) countries(O) where(O) the(O) trend(O) had(O) not(O) maintained(O) ,(O) with(O) the(O) emergence(O) of(O) new(O) boy(O) bands(O) like(O) Big(B-band) Time(I-band) Rush(I-band) ,(O) The(B-band) Wanted(I-band) ,(O) and(O) One(B-band) Direction(I-band) and(O) the(O) formation(O) of(O) supergroup(O) NKOTBSB(B-band) which(O) comprised(O) members(O) of(O) New(B-band) Kids(I-band) on(I-band) the(I-band) Block(I-band) and(O) Backstreet(B-band) Boys(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, location, musical instrument, album, country, event, organization, person, song, award, musical artist, band and O.\nSentence: In the early 2010s , there was somewhat of a resurgence of boy band popularity in countries where the trend had not maintained , with the emergence of new boy bands like Big Time Rush , The Wanted , and One Direction and the formation of supergroup NKOTBSB which comprised members of New Kids on the Block and Backstreet Boys .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","early","2010s",",","there","was","somewhat","of","a","resurgence","of","boy","band","popularity","in","countries","where","the","trend","had","not","maintained",",","with","the","emergence","of","new","boy","bands","like","Big","Time","Rush",",","The","Wanted",",","and","One","Direction","and","the","formation","of","supergroup","NKOTBSB","which","comprised","members","of","New","Kids","on","the","Block","and","Backstreet","Boys","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","I-band","O","B-band","I-band","O","O","B-band","I-band","O","O","O","O","O","B-band","O","O","O","O","B-band","I-band","I-band","I-band","I-band","O","B-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["music_genre","location","musical_instrument","album","country","event","organization","person","song","award","musical_artist","band"]}
{"id":"19","dataset":"crossner_music","split":"test","instance":{"id":"19","prompt_labels":"Beginning(O) in(O) 1999(O) ,(O) Righteous(B-organization) Babe(I-organization) Records(I-organization) began(O) releasing(O) albums(O) by(O) other(O) artists(O) including(O) Sara(B-musical artist) Lee(I-musical artist) ,(O) Sekou(B-musical artist) Sundiata(I-musical artist) ,(O) Michael(B-musical artist) Meldrum(I-musical artist) ,(O) Arto(B-musical artist) Lindsay(I-musical artist) ,(O) Bitch(B-band) and(I-band) Animal(I-band) ,(O) That(B-musical artist) One(I-musical artist) Guy(I-musical artist) ,(O) Utah(B-musical artist) Phillips(I-musical artist) ,(O) Hamell(B-band) on(I-band) Trial(I-band) ,(O) Andrew(B-musical artist) Bird(I-musical artist) ,(O) Kurt(B-musical artist) Swinghammer(I-musical artist) ,(O) Buddy(B-musical artist) Wakefield(I-musical artist) ,(O) Anas(B-musical artist) Mitchell(I-musical artist) and(O) Nona(B-musical artist) Hendryx(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, country, location, band, person, organization, musical instrument, event, music genre, award, song, musical artist and O.\nSentence: Beginning in 1999 , Righteous Babe Records began releasing albums by other artists including Sara Lee , Sekou Sundiata , Michael Meldrum , Arto Lindsay , Bitch and Animal , That One Guy , Utah Phillips , Hamell on Trial , Andrew Bird , Kurt Swinghammer , Buddy Wakefield , Anas Mitchell and Nona Hendryx .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Beginning","in","1999",",","Righteous","Babe","Records","began","releasing","albums","by","other","artists","including","Sara","Lee",",","Sekou","Sundiata",",","Michael","Meldrum",",","Arto","Lindsay",",","Bitch","and","Animal",",","That","One","Guy",",","Utah","Phillips",",","Hamell","on","Trial",",","Andrew","Bird",",","Kurt","Swinghammer",",","Buddy","Wakefield",",","Anas","Mitchell","and","Nona","Hendryx","."],"labels":["O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-band","I-band","I-band","O","B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-band","I-band","I-band","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["album","country","location","band","person","organization","musical_instrument","event","music_genre","award","song","musical_artist"]}
{"id":"20","dataset":"crossner_music","split":"test","instance":{"id":"20","prompt_labels":"Starting(O) in(O) the(O) early(O) 1950s(O) ,(O) and(O) during(O) the(O) mid-1960s(O) ,(O) Western(O) singer-songwriters(O) such(O) as(O) Michael(B-musical artist) Martin(I-musical artist) Murphey(I-musical artist) and(O) Marty(B-musical artist) Robbins(I-musical artist) rose(O) in(O) prominence(O) as(O) did(O) others(O) ,(O) throughout(O) Western(B-music genre) music(I-music genre) traditions(O) ,(O) like(O) New(B-music genre) Mexico(I-music genre) music(I-music genre) '(O) s(O) Al(B-musical artist) Hurricane(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, music genre, organization, musical artist, award, band, musical instrument, event, person, location, album, country and O.\nSentence: Starting in the early 1950s , and during the mid-1960s , Western singer-songwriters such as Michael Martin Murphey and Marty Robbins rose in prominence as did others , throughout Western music traditions , like New Mexico music ' s Al Hurricane .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Starting","in","the","early","1950s",",","and","during","the","mid-1960s",",","Western","singer-songwriters","such","as","Michael","Martin","Murphey","and","Marty","Robbins","rose","in","prominence","as","did","others",",","throughout","Western","music","traditions",",","like","New","Mexico","music","'","s","Al","Hurricane","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","B-music genre","I-music genre","I-music genre","O","O","B-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["song","music_genre","organization","musical_artist","award","band","musical_instrument","event","person","location","album","country"]}
{"id":"21","dataset":"crossner_music","split":"test","instance":{"id":"21","prompt_labels":"Under(O) the(O) direction(O) of(O) producers(O) such(O) as(O) Chet(B-musical artist) Atkins(I-musical artist) ,(O) Bill(B-musical artist) Porter(I-musical artist) ,(O) Paul(B-musical artist) Cohen(I-musical artist) ,(O) Owen(B-musical artist) Bradley(I-musical artist) ,(O) Bob(B-musical artist) Ferguson(I-musical artist) ,(O) and(O) later(O) Billy(B-musical artist) Sherrill(I-musical artist) ,(O) the(O) sound(O) brought(O) country(B-music genre) music(I-music genre) to(O) a(O) diverse(O) audience(O) and(O) helped(O) revive(O) country(B-music genre) as(O) it(O) emerged(O) from(O) a(O) commercially(O) fallow(O) period(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, band, organization, song, award, event, musical artist, music genre, musical instrument, album, location, country and O.\nSentence: Under the direction of producers such as Chet Atkins , Bill Porter , Paul Cohen , Owen Bradley , Bob Ferguson , and later Billy Sherrill , the sound brought country music to a diverse audience and helped revive country as it emerged from a commercially fallow period .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Under","the","direction","of","producers","such","as","Chet","Atkins",",","Bill","Porter",",","Paul","Cohen",",","Owen","Bradley",",","Bob","Ferguson",",","and","later","Billy","Sherrill",",","the","sound","brought","country","music","to","a","diverse","audience","and","helped","revive","country","as","it","emerged","from","a","commercially","fallow","period","."],"labels":["O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","B-musical artist","I-musical artist","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","B-music genre","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","band","organization","song","award","event","musical_artist","music_genre","musical_instrument","album","location","country"]}
{"id":"22","dataset":"crossner_music","split":"test","instance":{"id":"22","prompt_labels":"The(O) conductors(O) with(O) whom(O) she(O) has(O) worked(O) include(O) Pierre(B-musical artist) Boulez(I-musical artist) ,(O) Sir(O) Simon(B-musical artist) Rattle(I-musical artist) ,(O) Sir(O) Colin(B-musical artist) Davis(I-musical artist) ,(O) Michael(B-musical artist) Tilson(I-musical artist) Thomas(I-musical artist) ,(O) and(O) Valery(B-musical artist) Gergiev(I-musical artist) ,(O) and(O) she(O) has(O) appeared(O) in(O) many(O) of(O) the(O) world(O) 's(O) greatest(O) venues(O) ,(O) including(O) the(O) Wigmore(B-location) Hall(I-location) ,(O) Southbank(B-location) Centre(I-location) and(O) the(O) Barbican(B-location) Centre(I-location) in(O) London(B-location) ,(O) Sydney(B-location) Opera(I-location) House(I-location) ,(O) New(B-location) York(I-location) 's(O) Lincoln(B-location) Center(I-location) ,(O) Leipzig(B-location) Gewandhaus(I-location) ,(O) the(O) Concertgebouw(B-location) in(O) Amsterdam(B-location) and(O) the(O) Mozarteum(B-location) in(O) Salzburg(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, musical artist, location, band, award, album, music genre, musical instrument, song, event, person and O.\nSentence: The conductors with whom she has worked include Pierre Boulez , Sir Simon Rattle , Sir Colin Davis , Michael Tilson Thomas , and Valery Gergiev , and she has appeared in many of the world 's greatest venues , including the Wigmore Hall , Southbank Centre and the Barbican Centre in London , Sydney Opera House , New York 's Lincoln Center , Leipzig Gewandhaus , the Concertgebouw in Amsterdam and the Mozarteum in Salzburg .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","conductors","with","whom","she","has","worked","include","Pierre","Boulez",",","Sir","Simon","Rattle",",","Sir","Colin","Davis",",","Michael","Tilson","Thomas",",","and","Valery","Gergiev",",","and","she","has","appeared","in","many","of","the","world","'s","greatest","venues",",","including","the","Wigmore","Hall",",","Southbank","Centre","and","the","Barbican","Centre","in","London",",","Sydney","Opera","House",",","New","York","'s","Lincoln","Center",",","Leipzig","Gewandhaus",",","the","Concertgebouw","in","Amsterdam","and","the","Mozarteum","in","Salzburg","."],"labels":["O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","I-location","O","O","B-location","I-location","O","B-location","O","B-location","I-location","I-location","O","B-location","I-location","O","B-location","I-location","O","B-location","I-location","O","O","B-location","O","B-location","O","O","B-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["country","organization","musical_artist","location","band","award","album","music_genre","musical_instrument","song","event","person"]}
{"id":"24","dataset":"crossner_music","split":"test","instance":{"id":"24","prompt_labels":"The(O) venues(O) they(O) visited(O) were(O) ((O) in(O) chronological(O) order(O) )(O) ,(O) Southampton(B-location) Mayflower(I-location) Theatre(I-location) ,(O) Leeds(B-location) Grand(I-location) Theatre(I-location) ,(O) Southend(B-location) Cliffs(I-location) Pavilion(I-location) ,(O) Bristol(B-location) Hippodrome(I-location) ,(O) Bournemouth(B-location) International(I-location) Centre(I-location) ,(O) and(O) Milton(B-location) Keynes(I-location) Theatre(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, album, organization, musical artist, location, event, song, band, musical instrument, award, country, person and O.\nSentence: The venues they visited were ( in chronological order ) , Southampton Mayflower Theatre , Leeds Grand Theatre , Southend Cliffs Pavilion , Bristol Hippodrome , Bournemouth International Centre , and Milton Keynes Theatre .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","venues","they","visited","were","(","in","chronological","order",")",",","Southampton","Mayflower","Theatre",",","Leeds","Grand","Theatre",",","Southend","Cliffs","Pavilion",",","Bristol","Hippodrome",",","Bournemouth","International","Centre",",","and","Milton","Keynes","Theatre","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","O","B-location","I-location","I-location","O","B-location","I-location","I-location","O","B-location","I-location","O","B-location","I-location","I-location","O","O","B-location","I-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["music_genre","album","organization","musical_artist","location","event","song","band","musical_instrument","award","country","person"]}
{"id":"26","dataset":"crossner_music","split":"test","instance":{"id":"26","prompt_labels":"After(O) Gulf(B-album) Winds(I-album) ((O) 1976(O) )(O) ,(O) an(O) album(O) of(O) entirely(O) self-composed(O) songs(O) and(O) From(B-album) Every(I-album) Stage(I-album) ((O) 1976(O) )(O) ,(O) a(O) live(O) album(O) that(O) had(O) Baez(B-musical artist) performing(O) songs(O) from(O) every(O) stage(O) of(O) her(O) career(O) ,(O) Baez(B-musical artist) again(O) parted(O) ways(O) with(O) a(O) record(O) label(O) when(O) she(O) moved(O) to(O) CBS(B-organization) Records(I-organization) for(O) Blowin(B-album) '(I-album) Away(I-album) ((O) 1977(O) )(O) and(O) Honest(B-album) Lullaby(I-album) ((O) 1979(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, album, band, location, award, music genre, musical artist, song, country, organization, musical instrument, event and O.\nSentence: After Gulf Winds ( 1976 ) , an album of entirely self-composed songs and From Every Stage ( 1976 ) , a live album that had Baez performing songs from every stage of her career , Baez again parted ways with a record label when she moved to CBS Records for Blowin ' Away ( 1977 ) and Honest Lullaby ( 1979 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","Gulf","Winds","(","1976",")",",","an","album","of","entirely","self-composed","songs","and","From","Every","Stage","(","1976",")",",","a","live","album","that","had","Baez","performing","songs","from","every","stage","of","her","career",",","Baez","again","parted","ways","with","a","record","label","when","she","moved","to","CBS","Records","for","Blowin","'","Away","(","1977",")","and","Honest","Lullaby","(","1979",")","."],"labels":["O","B-album","I-album","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O","O","O","O","O","O","B-musical artist","O","O","O","O","O","O","O","O","O","B-musical artist","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","B-album","I-album","I-album","O","O","O","O","B-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","album","band","location","award","music_genre","musical_artist","song","country","organization","musical_instrument","event"]}
{"id":"27","dataset":"crossner_music","split":"test","instance":{"id":"27","prompt_labels":"He(O) played(O) on(O) the(O) live(O) albums(O) that(O) would(O) follow(O) the(O) release(O) of(O) Bitches(B-album) Brew(I-album) ,(O) taken(O) from(O) concerts(O) at(O) the(O) Fillmore(B-location) East(I-location) in(O) New(B-location) York(I-location) and(O) Fillmore(B-location) West(I-location) in(O) San(B-location) Francisco(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, musical artist, band, song, person, country, musical instrument, award, event, album, location, music genre and O.\nSentence: He played on the live albums that would follow the release of Bitches Brew , taken from concerts at the Fillmore East in New York and Fillmore West in San Francisco .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","played","on","the","live","albums","that","would","follow","the","release","of","Bitches","Brew",",","taken","from","concerts","at","the","Fillmore","East","in","New","York","and","Fillmore","West","in","San","Francisco","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","O","O","O","O","O","O","B-location","I-location","O","B-location","I-location","O","B-location","I-location","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["organization","musical_artist","band","song","person","country","musical_instrument","award","event","album","location","music_genre"]}
{"id":"28","dataset":"crossner_music","split":"test","instance":{"id":"28","prompt_labels":"Over(O) the(O) years(O) ,(O) the(O) festival(O) has(O) taken(O) place(O) in(O) numerous(O) venues(O) ,(O) including(O) the(O) Angel(B-location) Orensanz(I-location) Center(I-location) for(O) the(O) Arts(O) ,(O) the(O) St.(B-location) Nicholas(I-location) of(I-location) Myra(I-location) church(I-location) basement(I-location) ,(O) the(O) New(B-location) Age(I-location) Cabaret(I-location) ((O) formerly(O) known(O) as(O) the(O) Electric(B-location) Circus(I-location) )(O) ,(O) the(O) Knitting(B-location) Factory(I-location) ,(O) St.(B-location) Patrick(I-location) 's(I-location) youth(I-location) center(I-location) ,(O) CBGB(B-location) ,(O) Clemente(B-location) Soto(I-location) Vlez(I-location) Cultural(I-location) Center(I-location) ,(O) the(O) Abrons(B-location) Arts(I-location) Center(I-location) ,(O) Roulette(B-location) Intermedium(I-location) and(O) Judson(B-location) Memorial(I-location) Church(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, event, album, person, award, country, organization, band, song, music genre, musical artist, location and O.\nSentence: Over the years , the festival has taken place in numerous venues , including the Angel Orensanz Center for the Arts , the St. Nicholas of Myra church basement , the New Age Cabaret ( formerly known as the Electric Circus ) , the Knitting Factory , St. Patrick 's youth center , CBGB , Clemente Soto Vlez Cultural Center , the Abrons Arts Center , Roulette Intermedium and Judson Memorial Church .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Over","the","years",",","the","festival","has","taken","place","in","numerous","venues",",","including","the","Angel","Orensanz","Center","for","the","Arts",",","the","St.","Nicholas","of","Myra","church","basement",",","the","New","Age","Cabaret","(","formerly","known","as","the","Electric","Circus",")",",","the","Knitting","Factory",",","St.","Patrick","'s","youth","center",",","CBGB",",","Clemente","Soto","Vlez","Cultural","Center",",","the","Abrons","Arts","Center",",","Roulette","Intermedium","and","Judson","Memorial","Church","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","O","O","O","O","O","B-location","I-location","I-location","I-location","I-location","I-location","O","O","B-location","I-location","I-location","O","O","O","O","O","B-location","I-location","O","O","O","B-location","I-location","O","B-location","I-location","I-location","I-location","I-location","O","B-location","O","B-location","I-location","I-location","I-location","I-location","O","O","B-location","I-location","I-location","O","B-location","I-location","O","B-location","I-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","event","album","person","award","country","organization","band","song","music_genre","musical_artist","location"]}
{"id":"29","dataset":"crossner_music","split":"test","instance":{"id":"29","prompt_labels":"Her(O) charting(O) singles(O) include(O) Crucify(B-song) ,(O) Silent(B-song) All(I-song) These(I-song) Years(I-song) ,(O) God(B-song) ,(O) Cornflake(B-song) Girl(I-song) ,(O) Caught(B-song) a(I-song) Lite(I-song) Sneeze(I-song) ,(O) Professional(B-song) Widow(I-song) ,(O) Spark(B-song) ,(O) 1000(B-song) Oceans(I-song) ,(O) Flavor(B-song) and(O) A(B-song) Sorta(I-song) Fairytale(I-song) ,(O) her(O) most(O) commercially(O) successful(O) single(O) in(O) the(O) U.S.(B-country) to(O) date(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, album, musical instrument, person, event, location, band, award, song, musical artist, organization, music genre and O.\nSentence: Her charting singles include Crucify , Silent All These Years , God , Cornflake Girl , Caught a Lite Sneeze , Professional Widow , Spark , 1000 Oceans , Flavor and A Sorta Fairytale , her most commercially successful single in the U.S. to date .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Her","charting","singles","include","Crucify",",","Silent","All","These","Years",",","God",",","Cornflake","Girl",",","Caught","a","Lite","Sneeze",",","Professional","Widow",",","Spark",",","1000","Oceans",",","Flavor","and","A","Sorta","Fairytale",",","her","most","commercially","successful","single","in","the","U.S.","to","date","."],"labels":["O","O","O","O","B-song","O","B-song","I-song","I-song","I-song","O","B-song","O","B-song","I-song","O","B-song","I-song","I-song","I-song","O","B-song","I-song","O","B-song","O","B-song","I-song","O","B-song","O","B-song","I-song","I-song","O","O","O","O","O","O","O","O","B-country","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","album","musical_instrument","person","event","location","band","award","song","musical_artist","organization","music_genre"]}
{"id":"30","dataset":"crossner_music","split":"test","instance":{"id":"30","prompt_labels":"In(O) November(O) 2011(O) ,(O) three(O) of(O) the(O) band(O) 's(O) albums(O) ((O) The(B-album) Power(I-album) Cosmic(I-album) ,(O) Atlantis(B-album) Ascendant(I-album) and(O) The(B-album) Chthonic(I-album) Chronicles(I-album) )(O) were(O) reissued(O) as(O) limited(O) edition(O) digipaks(O) by(O) Nuclear(B-organization) Blast(I-organization) 's(O) affiliate(O) label(O) Metal(B-organization) Mind(I-organization) Productions(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, musical artist, musical instrument, award, event, organization, person, band, song, country, location, album and O.\nSentence: In November 2011 , three of the band 's albums ( The Power Cosmic , Atlantis Ascendant and The Chthonic Chronicles ) were reissued as limited edition digipaks by Nuclear Blast 's affiliate label Metal Mind Productions .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","November","2011",",","three","of","the","band","'s","albums","(","The","Power","Cosmic",",","Atlantis","Ascendant","and","The","Chthonic","Chronicles",")","were","reissued","as","limited","edition","digipaks","by","Nuclear","Blast","'s","affiliate","label","Metal","Mind","Productions","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O","B-album","I-album","O","B-album","I-album","I-album","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["music_genre","musical_artist","musical_instrument","award","event","organization","person","band","song","country","location","album"]}
{"id":"31","dataset":"crossner_music","split":"test","instance":{"id":"31","prompt_labels":"The(O) style(O) of(O) British(B-music genre) blues(I-music genre) developed(O) in(O) the(O) UK(B-country) ,(O) when(O) bands(O) such(O) as(O) the(O) The(B-band) Animals(I-band) ,(O) Fleetwood(B-band) Mac(I-band) ,(O) John(B-band) Mayall(I-band) &(I-band) the(I-band) Bluesbreakers(I-band) ,(O) the(O) The(B-band) Rolling(I-band) Stones(I-band) ,(O) the(O) The(B-band) Yardbirds(I-band) ,(O) the(O) supergroup(B-band) Cream(I-band) and(O) the(O) Irish(O) musician(O) Rory(B-musical artist) Gallagher(I-musical artist) performed(O) classic(O) blues(B-music genre) songs(O) from(O) the(O) Delta(B-music genre) blues(I-music genre) or(O) Chicago(B-music genre) blues(I-music genre) traditions(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, organization, song, music genre, award, musical instrument, musical artist, album, band, country, location and O.\nSentence: The style of British blues developed in the UK , when bands such as the The Animals , Fleetwood Mac , John Mayall & the Bluesbreakers , the The Rolling Stones , the The Yardbirds , the supergroup Cream and the Irish musician Rory Gallagher performed classic blues songs from the Delta blues or Chicago blues traditions .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","style","of","British","blues","developed","in","the","UK",",","when","bands","such","as","the","The","Animals",",","Fleetwood","Mac",",","John","Mayall","&","the","Bluesbreakers",",","the","The","Rolling","Stones",",","the","The","Yardbirds",",","the","supergroup","Cream","and","the","Irish","musician","Rory","Gallagher","performed","classic","blues","songs","from","the","Delta","blues","or","Chicago","blues","traditions","."],"labels":["O","O","O","B-music genre","I-music genre","O","O","O","B-country","O","O","O","O","O","O","B-band","I-band","O","B-band","I-band","O","B-band","I-band","I-band","I-band","I-band","O","O","B-band","I-band","I-band","O","O","B-band","I-band","O","O","B-band","I-band","O","O","O","O","B-musical artist","I-musical artist","O","O","B-music genre","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["person","event","organization","song","music_genre","award","musical_instrument","musical_artist","album","band","country","location"]}
{"id":"33","dataset":"crossner_music","split":"test","instance":{"id":"33","prompt_labels":"She(O) later(O) released(O) a(O) CD(O) on(O) Righteous(B-organization) Babe(I-organization) of(O) the(O) concert(O) Til(B-album) We(I-album) Outnumber(I-album) Em(I-album) featuring(O) artists(O) such(O) as(O) DiFranco(B-musical artist) ,(O) Billy(B-musical artist) Bragg(I-musical artist) ,(O) Ramblin(B-musical artist) '(I-musical artist) Jack(I-musical artist) Elliott(I-musical artist) ,(O) Arlo(B-musical artist) Guthrie(I-musical artist) ,(O) Indigo(B-band) Girls(I-band) ,(O) Dave(B-musical artist) Pirner(I-musical artist) ,(O) Tim(B-musical artist) Robbins(I-musical artist) ,(O) and(O) Bruce(B-musical artist) Springsteen(I-musical artist) with(O) 100(O) percent(O) of(O) proceeds(O) going(O) to(O) the(O) Woody(B-organization) Guthrie(I-organization) Foundation(I-organization) and(O) Archives(B-organization) and(O) the(O) Rock(B-organization) and(I-organization) Roll(I-organization) Hall(I-organization) of(I-organization) Fame(I-organization) Museum(I-organization) educational(O) department(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, song, music genre, person, band, musical artist, organization, musical instrument, album, event, award and O.\nSentence: She later released a CD on Righteous Babe of the concert Til We Outnumber Em featuring artists such as DiFranco , Billy Bragg , Ramblin ' Jack Elliott , Arlo Guthrie , Indigo Girls , Dave Pirner , Tim Robbins , and Bruce Springsteen with 100 percent of proceeds going to the Woody Guthrie Foundation and Archives and the Rock and Roll Hall of Fame Museum educational department .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","later","released","a","CD","on","Righteous","Babe","of","the","concert","Til","We","Outnumber","Em","featuring","artists","such","as","DiFranco",",","Billy","Bragg",",","Ramblin","'","Jack","Elliott",",","Arlo","Guthrie",",","Indigo","Girls",",","Dave","Pirner",",","Tim","Robbins",",","and","Bruce","Springsteen","with","100","percent","of","proceeds","going","to","the","Woody","Guthrie","Foundation","and","Archives","and","the","Rock","and","Roll","Hall","of","Fame","Museum","educational","department","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","B-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-band","I-band","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","country","song","music_genre","person","band","musical_artist","organization","musical_instrument","album","event","award"]}
{"id":"34","dataset":"crossner_music","split":"test","instance":{"id":"34","prompt_labels":"Other(O) artists(O) featured(O) on(O) the(O) show(O) include(O) Michael(B-musical artist) Jackson(I-musical artist) ,(O) Barry(B-musical artist) White(I-musical artist) ,(O) Al(B-musical artist) Green(I-musical artist) ,(O) Tina(B-musical artist) Turner(I-musical artist) ,(O) Macy(B-musical artist) Gray(I-musical artist) ,(O) Gloria(B-musical artist) Gaynor(I-musical artist) ,(O) Chayanne(B-musical artist) ,(O) Barry(B-musical artist) Manilow(I-musical artist) ,(O) Anastacia(B-musical artist) ,(O) Elton(B-musical artist) John(I-musical artist) ,(O) Sting(B-musical artist) and(O) Mariah(B-musical artist) Carey(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, album, award, location, organization, country, band, event, musical instrument, song, person, musical artist and O.\nSentence: Other artists featured on the show include Michael Jackson , Barry White , Al Green , Tina Turner , Macy Gray , Gloria Gaynor , Chayanne , Barry Manilow , Anastacia , Elton John , Sting and Mariah Carey .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","artists","featured","on","the","show","include","Michael","Jackson",",","Barry","White",",","Al","Green",",","Tina","Turner",",","Macy","Gray",",","Gloria","Gaynor",",","Chayanne",",","Barry","Manilow",",","Anastacia",",","Elton","John",",","Sting","and","Mariah","Carey","."],"labels":["O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","O","B-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["music_genre","album","award","location","organization","country","band","event","musical_instrument","song","person","musical_artist"]}
{"id":"35","dataset":"crossner_music","split":"test","instance":{"id":"35","prompt_labels":"The(O) festival(O) presents(O) concerts(O) in(O) Dixieland(B-music genre) ,(O) swing(B-music genre) ,(O) Jazz(B-music genre) fusion(I-music genre) ,(O) blues(B-music genre) ,(O) gospel(B-music genre) ,(O) funk(B-music genre) ,(O) soul(B-music genre) and(O) drum(B-music genre) and(I-music genre) bass(I-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, location, person, song, band, album, musical artist, music genre, musical instrument, award, organization and O.\nSentence: The festival presents concerts in Dixieland , swing , Jazz fusion , blues , gospel , funk , soul and drum and bass .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","festival","presents","concerts","in","Dixieland",",","swing",",","Jazz","fusion",",","blues",",","gospel",",","funk",",","soul","and","drum","and","bass","."],"labels":["O","O","O","O","O","B-music genre","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","O","B-music genre","O","B-music genre","O","B-music genre","I-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["country","event","location","person","song","band","album","musical_artist","music_genre","musical_instrument","award","organization"]}
{"id":"36","dataset":"crossner_music","split":"test","instance":{"id":"36","prompt_labels":"Duke(B-musical artist) turned(O) 65(O) in(O) the(O) spring(O) of(O) 1964(O) but(O) showed(O) no(O) signs(O) of(O) slowing(O) down(O) as(O) he(O) continued(O) to(O) make(O) vital(O) and(O) innovative(O) recordings(O) ,(O) including(O) The(B-album) Far(I-album) East(I-album) Suite(I-album) ((O) 1966(O) )(O) ,(O) New(B-album) Orleans(I-album) Suite(I-album) ((O) 1970(O) )(O) ,(O) Latin(B-album) American(I-album) Suite(I-album) ((O) 1972(O) )(O) and(O) The(B-album) Afro-Eurasian(I-album) Eclipse(I-album) ((O) 1971(O) )(O) ,(O) much(O) of(O) it(O) inspired(O) by(O) his(O) world(O) tours(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, band, song, musical instrument, country, music genre, organization, album, musical artist, person, award and O.\nSentence: Duke turned 65 in the spring of 1964 but showed no signs of slowing down as he continued to make vital and innovative recordings , including The Far East Suite ( 1966 ) , New Orleans Suite ( 1970 ) , Latin American Suite ( 1972 ) and The Afro-Eurasian Eclipse ( 1971 ) , much of it inspired by his world tours .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Duke","turned","65","in","the","spring","of","1964","but","showed","no","signs","of","slowing","down","as","he","continued","to","make","vital","and","innovative","recordings",",","including","The","Far","East","Suite","(","1966",")",",","New","Orleans","Suite","(","1970",")",",","Latin","American","Suite","(","1972",")","and","The","Afro-Eurasian","Eclipse","(","1971",")",",","much","of","it","inspired","by","his","world","tours","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","location","band","song","musical_instrument","country","music_genre","organization","album","musical_artist","person","award"]}
{"id":"37","dataset":"crossner_music","split":"test","instance":{"id":"37","prompt_labels":"Prince(B-musical artist) formed(O) the(B-band) Time(I-band) ,(O) originally(O) conceived(O) as(O) an(O) opening(O) act(O) for(O) him(O) and(O) based(O) on(O) his(O) Minneapolis(B-music genre) sound(I-music genre) ,(O) a(O) hybrid(O) mixture(O) of(O) funk(B-music genre) ,(O) R(B-music genre) &(I-music genre) B(I-music genre) ,(O) Rock(B-music genre) music(I-music genre) ,(O) Pop(B-music genre) music(I-music genre) &(O) amp(O) ;(O) New(B-music genre) wave(I-music genre) music(I-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, album, song, country, event, organization, musical instrument, award, band, location, person, music genre and O.\nSentence: Prince formed the Time , originally conceived as an opening act for him and based on his Minneapolis sound , a hybrid mixture of funk , R & B , Rock music , Pop music & amp ; New wave music .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Prince","formed","the","Time",",","originally","conceived","as","an","opening","act","for","him","and","based","on","his","Minneapolis","sound",",","a","hybrid","mixture","of","funk",",","R","&","B",",","Rock","music",",","Pop","music","&","amp",";","New","wave","music","."],"labels":["B-musical artist","O","B-band","I-band","O","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","B-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O","O","B-music genre","I-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","album","song","country","event","organization","musical_instrument","award","band","location","person","music_genre"]}
{"id":"38","dataset":"crossner_music","split":"test","instance":{"id":"38","prompt_labels":"Some(O) managers(O) in(O) Europe(B-location) soon(O) created(O) their(O) own(O) acts(O) after(O) being(O) inspired(O) by(O) New(B-band) Kids(I-band) on(I-band) the(I-band) Block(I-band) ,(O) beginning(O) with(O) Nigel(B-musical artist) Martin-Smith(I-musical artist) '(O) s(O) Take(B-band) That(I-band) in(O) the(O) UK(B-country) ((O) formed(O) in(O) 1990(O) )(O) and(O) followed(O) by(O) Tom(B-musical artist) Watkins(I-musical artist) ,(O) who(O) had(O) success(O) with(O) Bros(B-band) in(O) the(O) late(O) 1980s(O) and(O) formed(O) East(B-band) 17(I-band) in(O) 1991(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, musical artist, album, band, organization, country, event, award, song, musical instrument, location, person and O.\nSentence: Some managers in Europe soon created their own acts after being inspired by New Kids on the Block , beginning with Nigel Martin-Smith ' s Take That in the UK ( formed in 1990 ) and followed by Tom Watkins , who had success with Bros in the late 1980s and formed East 17 in 1991 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","managers","in","Europe","soon","created","their","own","acts","after","being","inspired","by","New","Kids","on","the","Block",",","beginning","with","Nigel","Martin-Smith","'","s","Take","That","in","the","UK","(","formed","in","1990",")","and","followed","by","Tom","Watkins",",","who","had","success","with","Bros","in","the","late","1980s","and","formed","East","17","in","1991","."],"labels":["O","O","O","B-location","O","O","O","O","O","O","O","O","O","B-band","I-band","I-band","I-band","I-band","O","O","O","B-musical artist","I-musical artist","O","O","B-band","I-band","O","O","B-country","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","B-band","O","O","O","O","O","O","B-band","I-band","O","O","O"],"target_index":null,"target_label":null},"label_list":["music_genre","musical_artist","album","band","organization","country","event","award","song","musical_instrument","location","person"]}
{"id":"39","dataset":"crossner_music","split":"test","instance":{"id":"39","prompt_labels":"It(O) contained(O) what(O) would(O) become(O) Brooks(B-musical artist) '(O) signature(O) song(O) ,(O) the(O) blue(B-song) collar(I-song) anthem(I-song) Friends(B-song) in(I-song) Low(I-song) Places(I-song) ,(O) as(O) well(O) as(O) other(O) popular(O) singles(O) ,(O) The(B-song) Thunder(I-song) Rolls(I-song) and(O) Unanswered(B-song) Prayers(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical instrument, organization, event, country, location, musical artist, award, song, person, music genre, album and O.\nSentence: It contained what would become Brooks ' signature song , the blue collar anthem Friends in Low Places , as well as other popular singles , The Thunder Rolls and Unanswered Prayers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","contained","what","would","become","Brooks","'","signature","song",",","the","blue","collar","anthem","Friends","in","Low","Places",",","as","well","as","other","popular","singles",",","The","Thunder","Rolls","and","Unanswered","Prayers","."],"labels":["O","O","O","O","O","B-musical artist","O","O","O","O","O","B-song","I-song","I-song","B-song","I-song","I-song","I-song","O","O","O","O","O","O","O","O","B-song","I-song","I-song","O","B-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["band","musical_instrument","organization","event","country","location","musical_artist","award","song","person","music_genre","album"]}
{"id":"40","dataset":"crossner_music","split":"test","instance":{"id":"40","prompt_labels":"On(O) 26(O) March(O) 2001(O) ,(O) their(O) first(O) full-length(O) album(O) ,(O) the(O) self-titled(O) Gorillaz(B-album) ,(O) was(O) released(O) ,(O) producing(O) four(O) singles(O) :(O) Clint(B-song) Eastwood(I-song) ,(O) 19-2000(B-song) ,(O) Rock(B-song) the(I-song) House(I-song) ,(O) and(O) Tomorrow(B-song) Comes(I-song) Today(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, musical artist, song, organization, location, album, musical instrument, country, band, music genre, event and O.\nSentence: On 26 March 2001 , their first full-length album , the self-titled Gorillaz , was released , producing four singles : Clint Eastwood , 19-2000 , Rock the House , and Tomorrow Comes Today .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","26","March","2001",",","their","first","full-length","album",",","the","self-titled","Gorillaz",",","was","released",",","producing","four","singles",":","Clint","Eastwood",",","19-2000",",","Rock","the","House",",","and","Tomorrow","Comes","Today","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-album","O","O","O","O","O","O","O","O","B-song","I-song","O","B-song","O","B-song","I-song","I-song","O","O","B-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["person","award","musical_artist","song","organization","location","album","musical_instrument","country","band","music_genre","event"]}
{"id":"42","dataset":"crossner_music","split":"test","instance":{"id":"42","prompt_labels":"By(O) the(O) end(O) of(O) the(O) decade(O) ,(O) Underwood(B-musical artist) had(O) amassed(O) eight(O) No.(O) 1(O) songs(O) on(O) the(O) Billboard(B-organization) Hot(O) Country(O) Songs(O) chart(O) ,(O) along(O) with(O) numerous(O) awards(O) from(O) the(O) Country(B-organization) Music(I-organization) Association(I-organization) ,(O) Academy(B-organization) of(I-organization) Country(I-organization) Music(I-organization) and(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, album, event, song, location, organization, band, music genre, award, musical instrument, musical artist and O.\nSentence: By the end of the decade , Underwood had amassed eight No. 1 songs on the Billboard Hot Country Songs chart , along with numerous awards from the Country Music Association , Academy of Country Music and others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["By","the","end","of","the","decade",",","Underwood","had","amassed","eight","No.","1","songs","on","the","Billboard","Hot","Country","Songs","chart",",","along","with","numerous","awards","from","the","Country","Music","Association",",","Academy","of","Country","Music","and","others","."],"labels":["O","O","O","O","O","O","O","B-musical artist","O","O","O","O","O","O","O","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","country","album","event","song","location","organization","band","music_genre","award","musical_instrument","musical_artist"]}
{"id":"43","dataset":"crossner_music","split":"test","instance":{"id":"43","prompt_labels":"It(O) was(O) not(O) until(O) 1997(O) and(O) the(O) change(O) to(O) pop-oriented(B-music genre) groups(O) such(O) as(O) Backstreet(B-band) Boys(I-band) ,(O) 98(B-band) Degrees(I-band) ,(O) NSYNC(B-band) ,(O) The(B-band) Moffatts(I-band) ,(O) and(O) Hanson(B-band) that(O) boy(O) bands(O) exploded(O) commercially(O) and(O) dominated(O) the(O) market(O) in(O) the(O) United(B-country) States(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, location, song, person, album, musical instrument, country, band, award, musical artist, music genre and O.\nSentence: It was not until 1997 and the change to pop-oriented groups such as Backstreet Boys , 98 Degrees , NSYNC , The Moffatts , and Hanson that boy bands exploded commercially and dominated the market in the United States .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","not","until","1997","and","the","change","to","pop-oriented","groups","such","as","Backstreet","Boys",",","98","Degrees",",","NSYNC",",","The","Moffatts",",","and","Hanson","that","boy","bands","exploded","commercially","and","dominated","the","market","in","the","United","States","."],"labels":["O","O","O","O","O","O","O","O","O","B-music genre","O","O","O","B-band","I-band","O","B-band","I-band","O","B-band","O","B-band","I-band","O","O","B-band","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["event","organization","location","song","person","album","musical_instrument","country","band","award","musical_artist","music_genre"]}
{"id":"46","dataset":"crossner_music","split":"test","instance":{"id":"46","prompt_labels":"In(O) January(O) 1970(O) Lucian(B-person) K.(I-person) Truscott(I-person) IV(I-person) reviewing(O) Led(B-album) Zeppelin(I-album) II(I-album) for(O) the(O) Village(O) Voice(O) described(O) the(O) sound(O) as(O) heavy(O) and(O) made(O) comparisons(O) with(O) Blue(B-band) Cheer(I-band) and(O) Vanilla(B-band) Fudge(I-band) ..(O) January(O) 22(O) ,(O) 1970(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, album, person, award, location, event, band, musical artist, country, musical instrument, organization, music genre and O.\nSentence: In January 1970 Lucian K. Truscott IV reviewing Led Zeppelin II for the Village Voice described the sound as heavy and made comparisons with Blue Cheer and Vanilla Fudge .. January 22 , 1970 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","January","1970","Lucian","K.","Truscott","IV","reviewing","Led","Zeppelin","II","for","the","Village","Voice","described","the","sound","as","heavy","and","made","comparisons","with","Blue","Cheer","and","Vanilla","Fudge","..","January","22",",","1970","."],"labels":["O","O","O","B-person","I-person","I-person","I-person","O","B-album","I-album","I-album","O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","O","B-band","I-band","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["song","album","person","award","location","event","band","musical_artist","country","musical_instrument","organization","music_genre"]}
{"id":"47","dataset":"crossner_music","split":"test","instance":{"id":"47","prompt_labels":"The(O) success(O) and(O) popularity(O) of(O) the(O) Commonwealth(B-event) Games(I-event) resulted(O) in(O) Edmonton(B-location) bidding(O) for(O) and(O) being(O) selected(O) to(O) host(O) the(O) 1983(B-event) Summer(I-event) Universiade(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, music genre, event, country, person, band, musical artist, organization, song, album, musical instrument, award and O.\nSentence: The success and popularity of the Commonwealth Games resulted in Edmonton bidding for and being selected to host the 1983 Summer Universiade .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","success","and","popularity","of","the","Commonwealth","Games","resulted","in","Edmonton","bidding","for","and","being","selected","to","host","the","1983","Summer","Universiade","."],"labels":["O","O","O","O","O","O","B-event","I-event","O","O","B-location","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["location","music_genre","event","country","person","band","musical_artist","organization","song","album","musical_instrument","award"]}
{"id":"49","dataset":"crossner_music","split":"test","instance":{"id":"49","prompt_labels":"In(O) 1985(O) ,(O) the(O) trio(B-band) joined(O) up(O) with(O) Marc(B-musical artist) Almond(I-musical artist) to(O) record(O) a(O) version(O) of(O) Donna(B-musical artist) Summer(I-musical artist) '(O) s(O) I(B-song) Feel(I-song) Love(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, music genre, organization, band, song, award, musical artist, album, musical instrument, country, event and O.\nSentence: In 1985 , the trio joined up with Marc Almond to record a version of Donna Summer ' s I Feel Love .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1985",",","the","trio","joined","up","with","Marc","Almond","to","record","a","version","of","Donna","Summer","'","s","I","Feel","Love","."],"labels":["O","O","O","O","B-band","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","B-musical artist","I-musical artist","O","O","B-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["location","person","music_genre","organization","band","song","award","musical_artist","album","musical_instrument","country","event"]}
{"id":"52","dataset":"crossner_music","split":"test","instance":{"id":"52","prompt_labels":"After(O) their(O) first(O) two(O) albums(O) ,(O) Fungus(B-album) Amongus(I-album) ((O) 1995(O) )(O) and(O) S.C.I.E.N.C.E.(B-album) ((O) 1997(O) )(O) ,(O) the(O) band(O) earned(O) mainstream(O) recognition(O) with(O) the(O) release(O) of(O) their(O) 1999(O) album(O) Make(B-album) Yourself(I-album) which(O) spawned(O) several(O) hits(O) ,(O) including(O) the(O) band(O) 's(O) highest(O) charting(O) song(O) Drive(B-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, musical instrument, song, musical artist, music genre, country, organization, event, band, award, album and O.\nSentence: After their first two albums , Fungus Amongus ( 1995 ) and S.C.I.E.N.C.E. ( 1997 ) , the band earned mainstream recognition with the release of their 1999 album Make Yourself which spawned several hits , including the band 's highest charting song Drive .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","their","first","two","albums",",","Fungus","Amongus","(","1995",")","and","S.C.I.E.N.C.E.","(","1997",")",",","the","band","earned","mainstream","recognition","with","the","release","of","their","1999","album","Make","Yourself","which","spawned","several","hits",",","including","the","band","'s","highest","charting","song","Drive","."],"labels":["O","O","O","O","O","O","B-album","I-album","O","O","O","O","B-album","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","O","O","O","O","O","O","O","O","O","O","O","O","B-song","O"],"target_index":null,"target_label":null},"label_list":["location","person","musical_instrument","song","musical_artist","music_genre","country","organization","event","band","award","album"]}
{"id":"53","dataset":"crossner_music","split":"test","instance":{"id":"53","prompt_labels":"Funk(O) drumming(O) creates(O) a(O) groove(O) by(O) emphasizing(O) the(O) drummer(O) 's(O) feel(O) and(O) emotion(O) ,(O) which(O) including(O) occasional(O) tempo(O) fluctuations(O) ,(O) the(O) use(O) of(O) Swing(B-music genre) music(I-music genre) feel(O) in(O) some(O) songs(O) ((O) e.g.(O) ,(O) Cissy(B-song) Strut(I-song) by(O) The(B-band) Meters(I-band) and(O) I(B-song) 'll(I-song) Take(I-song) You(I-song) There(I-song) by(O) The(B-band) Staple(I-band) Singers(I-band) ,(O) which(O) have(O) a(O) half-swung(O) feel(O) )(O) ,(O) and(O) less(O) use(O) of(O) fills(O) ((O) as(O) they(O) can(O) lessen(O) the(O) groove(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, country, organization, location, musical instrument, music genre, musical artist, person, song, band, event, award and O.\nSentence: Funk drumming creates a groove by emphasizing the drummer 's feel and emotion , which including occasional tempo fluctuations , the use of Swing music feel in some songs ( e.g. , Cissy Strut by The Meters and I 'll Take You There by The Staple Singers , which have a half-swung feel ) , and less use of fills ( as they can lessen the groove ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Funk","drumming","creates","a","groove","by","emphasizing","the","drummer","'s","feel","and","emotion",",","which","including","occasional","tempo","fluctuations",",","the","use","of","Swing","music","feel","in","some","songs","(","e.g.",",","Cissy","Strut","by","The","Meters","and","I","'ll","Take","You","There","by","The","Staple","Singers",",","which","have","a","half-swung","feel",")",",","and","less","use","of","fills","(","as","they","can","lessen","the","groove",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","B-song","I-song","O","B-band","I-band","O","B-song","I-song","I-song","I-song","I-song","O","B-band","I-band","I-band","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["album","country","organization","location","musical_instrument","music_genre","musical_artist","person","song","band","event","award"]}
{"id":"58","dataset":"crossner_music","split":"test","instance":{"id":"58","prompt_labels":"It(O) includes(O) collaborations(O) with(O) Pete(B-musical artist) Seeger(I-musical artist) ,(O) Ivan(B-musical artist) Neville(I-musical artist) ,(O) Cyril(B-musical artist) Neville(I-musical artist) ,(O) Skerik(B-musical artist) ,(O) Adam(B-musical artist) Levy(I-musical artist) ,(O) Righteous(B-organization) Babe(I-organization) recording(I-organization) artist(O) Anas(B-musical artist) Mitchell(I-musical artist) ,(O) CC(B-musical artist) Adcock(I-musical artist) ,(O) and(O) a(O) host(O) of(O) New(O) Orleans-based(O) horn(O) players(O) known(O) for(O) their(O) work(O) in(O) such(O) outfits(O) as(O) Galactic(B-band) ,(O) Bonerama(B-band) ,(O) and(O) Rebirth(B-band) Brass(I-band) Band(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, album, person, country, award, event, song, organization, location, musical instrument, band, musical artist and O.\nSentence: It includes collaborations with Pete Seeger , Ivan Neville , Cyril Neville , Skerik , Adam Levy , Righteous Babe recording artist Anas Mitchell , CC Adcock , and a host of New Orleans-based horn players known for their work in such outfits as Galactic , Bonerama , and Rebirth Brass Band .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","includes","collaborations","with","Pete","Seeger",",","Ivan","Neville",",","Cyril","Neville",",","Skerik",",","Adam","Levy",",","Righteous","Babe","recording","artist","Anas","Mitchell",",","CC","Adcock",",","and","a","host","of","New","Orleans-based","horn","players","known","for","their","work","in","such","outfits","as","Galactic",",","Bonerama",",","and","Rebirth","Brass","Band","."],"labels":["O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","O","B-musical artist","I-musical artist","O","B-organization","I-organization","I-organization","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","O","B-band","O","O","B-band","I-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["music_genre","album","person","country","award","event","song","organization","location","musical_instrument","band","musical_artist"]}
{"id":"60","dataset":"crossner_music","split":"test","instance":{"id":"60","prompt_labels":"The(O) New(B-music genre) Orleans(I-music genre) setting(O) of(O) the(O) film(O) played(O) to(O) Newman(B-musical artist) 's(O) musical(O) strengths(O) ,(O) and(O) his(O) songs(O) contained(O) elements(O) of(O) Cajun(B-music genre) music(I-music genre) ,(O) zydeco(B-music genre) ,(O) blues(B-music genre) and(O) Dixieland(B-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical instrument, country, song, person, album, event, musical artist, location, music genre, organization, award and O.\nSentence: The New Orleans setting of the film played to Newman 's musical strengths , and his songs contained elements of Cajun music , zydeco , blues and Dixieland .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","New","Orleans","setting","of","the","film","played","to","Newman","'s","musical","strengths",",","and","his","songs","contained","elements","of","Cajun","music",",","zydeco",",","blues","and","Dixieland","."],"labels":["O","B-music genre","I-music genre","O","O","O","O","O","O","B-musical artist","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","O","B-music genre","O"],"target_index":null,"target_label":null},"label_list":["band","musical_instrument","country","song","person","album","event","musical_artist","location","music_genre","organization","award"]}
{"id":"61","dataset":"crossner_music","split":"test","instance":{"id":"61","prompt_labels":"They(O) also(O) released(O) five(O) singles(O) to(O) promote(O) the(O) album(O) :(O) Enter(B-song) Sandman(I-song) ,(O) The(B-song) Unforgiven(I-song) ,(O) Nothing(B-song) Else(I-song) Matters(I-song) ,(O) Wherever(B-song) I(I-song) May(I-song) Roam(I-song) ,(O) and(O) Sad(B-song) but(I-song) TRUE(I-song) ,(O) all(O) of(O) which(O) have(O) been(O) considered(O) to(O) be(O) among(O) the(O) band(O) 's(O) best-known(O) songs(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, band, song, location, musical artist, music genre, person, event, album, country, award, musical instrument and O.\nSentence: They also released five singles to promote the album : Enter Sandman , The Unforgiven , Nothing Else Matters , Wherever I May Roam , and Sad but TRUE , all of which have been considered to be among the band 's best-known songs .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["They","also","released","five","singles","to","promote","the","album",":","Enter","Sandman",",","The","Unforgiven",",","Nothing","Else","Matters",",","Wherever","I","May","Roam",",","and","Sad","but","TRUE",",","all","of","which","have","been","considered","to","be","among","the","band","'s","best-known","songs","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-song","I-song","O","B-song","I-song","O","B-song","I-song","I-song","O","B-song","I-song","I-song","I-song","O","O","B-song","I-song","I-song","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","band","song","location","musical_artist","music_genre","person","event","album","country","award","musical_instrument"]}
{"id":"62","dataset":"crossner_music","split":"test","instance":{"id":"62","prompt_labels":"He(O) helped(O) found(O) the(O) American(B-location) Conservatory(I-location) Theater(I-location) in(O) San(B-location) Francisco(I-location) ,(O) the(O) Mark(B-location) Taper(I-location) Forum(I-location) in(O) Los(B-location) Angeles(I-location) ,(O) and(O) the(O) Brooklyn(B-location) Academy(I-location) of(I-location) Music(I-location) Repertory(B-location) Company(I-location) in(O) New(B-location) York(I-location) City(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, song, event, music genre, musical artist, country, person, organization, band, musical instrument, album, award and O.\nSentence: He helped found the American Conservatory Theater in San Francisco , the Mark Taper Forum in Los Angeles , and the Brooklyn Academy of Music Repertory Company in New York City .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","helped","found","the","American","Conservatory","Theater","in","San","Francisco",",","the","Mark","Taper","Forum","in","Los","Angeles",",","and","the","Brooklyn","Academy","of","Music","Repertory","Company","in","New","York","City","."],"labels":["O","O","O","O","B-location","I-location","I-location","O","B-location","I-location","O","O","B-location","I-location","I-location","O","B-location","I-location","O","O","O","B-location","I-location","I-location","I-location","B-location","I-location","O","B-location","I-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["location","song","event","music_genre","musical_artist","country","person","organization","band","musical_instrument","album","award"]}
{"id":"64","dataset":"crossner_music","split":"test","instance":{"id":"64","prompt_labels":"The(B-band) Police(I-band) won(O) a(O) number(O) of(O) music(O) awards(O) ,(O) including(O) six(O) Grammy(B-award) Award(I-award) s(O) ,(O) two(O) Brit(B-award) Awards(I-award) -(O) winning(O) Best(B-award) British(I-award) Group(I-award) once(O) ,(O) an(O) MTV(B-award) Video(I-award) Music(I-award) Award(I-award) ,(O) and(O) in(O) 2003(O) were(O) inducted(O) into(O) the(O) Rock(B-organization) and(I-organization) Roll(I-organization) Hall(I-organization) of(I-organization) Fame(I-organization) ..(O) Rolling(B-organization) Stone(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, musical artist, award, country, location, event, band, song, person, music genre, album, organization and O.\nSentence: The Police won a number of music awards , including six Grammy Award s , two Brit Awards - winning Best British Group once , an MTV Video Music Award , and in 2003 were inducted into the Rock and Roll Hall of Fame .. Rolling Stone .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Police","won","a","number","of","music","awards",",","including","six","Grammy","Award","s",",","two","Brit","Awards","-","winning","Best","British","Group","once",",","an","MTV","Video","Music","Award",",","and","in","2003","were","inducted","into","the","Rock","and","Roll","Hall","of","Fame","..","Rolling","Stone","."],"labels":["B-band","I-band","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","musical_artist","award","country","location","event","band","song","person","music_genre","album","organization"]}
{"id":"65","dataset":"crossner_music","split":"test","instance":{"id":"65","prompt_labels":"Alt-country(B-music genre) ,(O) in(O) various(O) iterations(O) overlapped(O) with(O) other(O) genres(O) ,(O) including(O) Red(B-music genre) Dirt(I-music genre) country(I-music genre) music(I-music genre) ((O) Cross(B-band) Canadian(I-band) Ragweed(I-band) )(O) ,(O) jam(O) band(O) s(O) ((O) My(B-band) Morning(I-band) Jacket(I-band) and(O) The(B-band) String(I-band) Cheese(I-band) Incident(I-band) )(O) ,(O) and(O) indie(B-music genre) folk(I-music genre) ((O) The(B-band) Avett(I-band) Brothers(I-band) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, band, song, location, person, event, country, organization, music genre, musical artist, musical instrument, award and O.\nSentence: Alt-country , in various iterations overlapped with other genres , including Red Dirt country music ( Cross Canadian Ragweed ) , jam band s ( My Morning Jacket and The String Cheese Incident ) , and indie folk ( The Avett Brothers ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Alt-country",",","in","various","iterations","overlapped","with","other","genres",",","including","Red","Dirt","country","music","(","Cross","Canadian","Ragweed",")",",","jam","band","s","(","My","Morning","Jacket","and","The","String","Cheese","Incident",")",",","and","indie","folk","(","The","Avett","Brothers",")","."],"labels":["B-music genre","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","I-music genre","I-music genre","O","B-band","I-band","I-band","O","O","O","O","O","O","B-band","I-band","I-band","O","B-band","I-band","I-band","I-band","O","O","O","B-music genre","I-music genre","O","B-band","I-band","I-band","O","O"],"target_index":null,"target_label":null},"label_list":["album","band","song","location","person","event","country","organization","music_genre","musical_artist","musical_instrument","award"]}
{"id":"69","dataset":"crossner_music","split":"test","instance":{"id":"69","prompt_labels":"Today(O) ,(O) zydeco(O) integrates(O) genres(O) such(O) as(O) Rhythm(B-music genre) and(I-music genre) blues(I-music genre) ,(O) Soul(B-music genre) music(I-music genre) ,(O) brass(B-music genre) band(I-music genre) ,(O) reggae(B-music genre) ,(O) Hip(B-music genre) hop(I-music genre) music(I-music genre) ,(O) ska(B-music genre) ,(O) Rock(B-music genre) music(I-music genre) ,(O) Afro-Caribbean(B-music genre) music(I-music genre) and(O) other(O) styles(O) ,(O) in(O) addition(O) to(O) the(O) traditional(O) forms(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, location, band, music genre, country, album, event, song, musical artist, organization, person, award and O.\nSentence: Today , zydeco integrates genres such as Rhythm and blues , Soul music , brass band , reggae , Hip hop music , ska , Rock music , Afro-Caribbean music and other styles , in addition to the traditional forms .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Today",",","zydeco","integrates","genres","such","as","Rhythm","and","blues",",","Soul","music",",","brass","band",",","reggae",",","Hip","hop","music",",","ska",",","Rock","music",",","Afro-Caribbean","music","and","other","styles",",","in","addition","to","the","traditional","forms","."],"labels":["O","O","O","O","O","O","O","B-music genre","I-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","location","band","music_genre","country","album","event","song","musical_artist","organization","person","award"]}
{"id":"71","dataset":"crossner_music","split":"test","instance":{"id":"71","prompt_labels":"There(O) have(O) also(O) been(O) several(O) spin-offs(O) featuring(O) one(O) or(O) more(O) core(O) members(O) ,(O) such(O) as(O) Dead(B-band) &(I-band) amp(I-band) ;(I-band) Company(I-band) ,(O) Furthur(B-band) ,(O) the(O) Rhythm(B-band) Devils(I-band) ,(O) Phil(B-band) Lesh(I-band) and(I-band) Friends(I-band) ,(O) RatDog(B-band) ,(O) and(O) Billy(B-band) &(I-band) amp(I-band) ;(I-band) the(I-band) Kids(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, song, music genre, award, band, country, location, event, album, person, musical artist, musical instrument and O.\nSentence: There have also been several spin-offs featuring one or more core members , such as Dead & amp ; Company , Furthur , the Rhythm Devils , Phil Lesh and Friends , RatDog , and Billy & amp ; the Kids .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["There","have","also","been","several","spin-offs","featuring","one","or","more","core","members",",","such","as","Dead","&","amp",";","Company",",","Furthur",",","the","Rhythm","Devils",",","Phil","Lesh","and","Friends",",","RatDog",",","and","Billy","&","amp",";","the","Kids","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","I-band","I-band","I-band","O","B-band","O","O","B-band","I-band","O","B-band","I-band","I-band","I-band","O","B-band","O","O","B-band","I-band","I-band","I-band","I-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["organization","song","music_genre","award","band","country","location","event","album","person","musical_artist","musical_instrument"]}
{"id":"75","dataset":"crossner_music","split":"test","instance":{"id":"75","prompt_labels":"Troy(B-musical artist) Cassar-Daley(I-musical artist) is(O) among(O) Australia(B-country) 's(O) successful(O) contemporary(O) indigenous(O) performers(O) ,(O) and(O) Kev(B-musical artist) Carmody(I-musical artist) and(O) Archie(B-musical artist) Roach(I-musical artist) employ(O) a(O) combination(O) of(O) folk-rock(B-music genre) and(O) country(B-music genre) music(I-music genre) to(O) sing(O) about(O) Aboriginal(O) rights(O) issues(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, album, award, music genre, song, event, band, musical artist, organization, musical instrument, person and O.\nSentence: Troy Cassar-Daley is among Australia 's successful contemporary indigenous performers , and Kev Carmody and Archie Roach employ a combination of folk-rock and country music to sing about Aboriginal rights issues .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Troy","Cassar-Daley","is","among","Australia","'s","successful","contemporary","indigenous","performers",",","and","Kev","Carmody","and","Archie","Roach","employ","a","combination","of","folk-rock","and","country","music","to","sing","about","Aboriginal","rights","issues","."],"labels":["B-musical artist","I-musical artist","O","O","B-country","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O","B-music genre","O","B-music genre","I-music genre","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","country","album","award","music_genre","song","event","band","musical_artist","organization","musical_instrument","person"]}
{"id":"76","dataset":"crossner_music","split":"test","instance":{"id":"76","prompt_labels":"She(O) won(O) Top(B-award) New(I-award) Female(I-award) Vocalist(I-award) from(O) the(O) Academy(B-organization) of(I-organization) Country(I-organization) Music(I-organization) and(O) the(O) Horizon(B-award) Award(I-award) from(O) the(O) Country(B-organization) Music(I-organization) Association(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, musical artist, location, country, organization, song, album, band, award, person, event, music genre and O.\nSentence: She won Top New Female Vocalist from the Academy of Country Music and the Horizon Award from the Country Music Association .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","won","Top","New","Female","Vocalist","from","the","Academy","of","Country","Music","and","the","Horizon","Award","from","the","Country","Music","Association","."],"labels":["O","O","B-award","I-award","I-award","I-award","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-award","I-award","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","musical_artist","location","country","organization","song","album","band","award","person","event","music_genre"]}
{"id":"77","dataset":"crossner_music","split":"test","instance":{"id":"77","prompt_labels":"The(O) band(O) headlined(O) the(O) Soul(O) Assassins(O) tour(O) with(O) House(B-band) of(I-band) Pain(I-band) and(O) Funkdoobiest(B-band) as(O) support(O) ,(O) then(O) performed(O) on(O) a(O) college(O) tour(O) with(O) Rage(B-band) Against(I-band) the(I-band) Machine(I-band) and(O) Seven(B-band) Year(I-band) Bitch(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, band, musical artist, music genre, award, event, song, person, album, musical instrument, location, organization and O.\nSentence: The band headlined the Soul Assassins tour with House of Pain and Funkdoobiest as support , then performed on a college tour with Rage Against the Machine and Seven Year Bitch .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","band","headlined","the","Soul","Assassins","tour","with","House","of","Pain","and","Funkdoobiest","as","support",",","then","performed","on","a","college","tour","with","Rage","Against","the","Machine","and","Seven","Year","Bitch","."],"labels":["O","O","O","O","O","O","O","O","B-band","I-band","I-band","O","B-band","O","O","O","O","O","O","O","O","O","O","B-band","I-band","I-band","I-band","O","B-band","I-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["country","band","musical_artist","music_genre","award","event","song","person","album","musical_instrument","location","organization"]}
{"id":"79","dataset":"crossner_music","split":"test","instance":{"id":"79","prompt_labels":"They(O) play(O) Avant-garde(B-music genre) jazz(I-music genre) versions(O) of(O) tradition(O) American(B-music genre) Folk(I-music genre) music(I-music genre) &(O) amp(O) ;(O) Blues(B-music genre) songs(O) with(O) Ritchie(B-musical artist) 's(O) shakuhachi(B-musical instrument) playing(O) as(O) the(O) focal(O) point(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, award, musical instrument, event, song, band, musical artist, organization, music genre, album, country and O.\nSentence: They play Avant-garde jazz versions of tradition American Folk music & amp ; Blues songs with Ritchie 's shakuhachi playing as the focal point .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["They","play","Avant-garde","jazz","versions","of","tradition","American","Folk","music","&","amp",";","Blues","songs","with","Ritchie","'s","shakuhachi","playing","as","the","focal","point","."],"labels":["O","O","B-music genre","I-music genre","O","O","O","B-music genre","I-music genre","I-music genre","O","O","O","B-music genre","O","O","B-musical artist","O","B-musical instrument","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","person","award","musical_instrument","event","song","band","musical_artist","organization","music_genre","album","country"]}
{"id":"81","dataset":"crossner_music","split":"test","instance":{"id":"81","prompt_labels":"Shulman(B-person) ,(I-person) Art(I-person) Dynamo(O) -(O) Country(O) Style(O) ((O) 1956(O) )(O) ,(O) TV(O) Guide(O) ,(O) p(O) ,(O) 28(O) The(O) late(O) 1950s(O) saw(O) the(O) emergence(O) of(O) Buddy(B-album) Holly(I-album) ,(O) but(O) by(O) the(O) end(O) of(O) the(O) decade(O) ,(O) backlash(O) as(O) well(O) as(O) traditional(O) artists(O) such(O) as(O) Ray(B-musical artist) Price(I-musical artist) ,(O) Marty(B-musical artist) Robbins(I-musical artist) ,(O) and(O) Johnny(B-musical artist) Horton(I-musical artist) began(O) to(O) shift(O) the(O) industry(O) away(O) from(O) the(O) rock(B-music genre) n(I-music genre) '(I-music genre) roll(I-music genre) influences(O) of(O) the(O) mid-1950s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, event, musical artist, country, award, musical instrument, location, person, music genre, organization, song, album and O.\nSentence: Shulman , Art Dynamo - Country Style ( 1956 ) , TV Guide , p , 28 The late 1950s saw the emergence of Buddy Holly , but by the end of the decade , backlash as well as traditional artists such as Ray Price , Marty Robbins , and Johnny Horton began to shift the industry away from the rock n ' roll influences of the mid-1950s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Shulman",",","Art","Dynamo","-","Country","Style","(","1956",")",",","TV","Guide",",","p",",","28","The","late","1950s","saw","the","emergence","of","Buddy","Holly",",","but","by","the","end","of","the","decade",",","backlash","as","well","as","traditional","artists","such","as","Ray","Price",",","Marty","Robbins",",","and","Johnny","Horton","began","to","shift","the","industry","away","from","the","rock","n","'","roll","influences","of","the","mid-1950s","."],"labels":["B-person","I-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","B-music genre","I-music genre","I-music genre","I-music genre","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["band","event","musical_artist","country","award","musical_instrument","location","person","music_genre","organization","song","album"]}
{"id":"82","dataset":"crossner_music","split":"test","instance":{"id":"82","prompt_labels":"By(O) the(O) time(O) she(O) released(O) her(O) fourth(O) album(O) in(O) the(O) early(O) 1990s(O) ,(O) she(O) had(O) amassed(O) several(O) top(O) ten(O) singles(O) in(O) the(O) UK(B-country) and(O) Australia(B-country) ,(O) including(O) I(B-song) Should(I-song) Be(I-song) So(I-song) Lucky(I-song) ,(O) The(B-song) Loco-Motion(I-song) ,(O) Hand(B-song) on(I-song) Your(I-song) Heart(I-song) ,(O) Better(B-song) the(I-song) Devil(I-song) You(I-song) Know(I-song) and(O) Step(B-song) Back(I-song) in(I-song) Time(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, album, person, song, country, band, location, musical artist, musical instrument, event, music genre, organization and O.\nSentence: By the time she released her fourth album in the early 1990s , she had amassed several top ten singles in the UK and Australia , including I Should Be So Lucky , The Loco-Motion , Hand on Your Heart , Better the Devil You Know and Step Back in Time .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["By","the","time","she","released","her","fourth","album","in","the","early","1990s",",","she","had","amassed","several","top","ten","singles","in","the","UK","and","Australia",",","including","I","Should","Be","So","Lucky",",","The","Loco-Motion",",","Hand","on","Your","Heart",",","Better","the","Devil","You","Know","and","Step","Back","in","Time","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","B-country","O","O","B-song","I-song","I-song","I-song","I-song","O","B-song","I-song","O","B-song","I-song","I-song","I-song","O","B-song","I-song","I-song","I-song","I-song","O","B-song","I-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["award","album","person","song","country","band","location","musical_artist","musical_instrument","event","music_genre","organization"]}
{"id":"84","dataset":"crossner_music","split":"test","instance":{"id":"84","prompt_labels":"She(O) had(O) her(O) first(O) experience(O) in(O) show(O) business(O) when(O) she(O) was(O) crowned(O) Miss(B-award) Teenager(I-award) Universal(I-award) in(O) 1971(O) ,(O) and(O) was(O) Miss(B-award) World(I-award) /(O) Venezuela(B-country) in(O) 1975(O) where(O) she(O) became(O) sixth(O) runner-up(O) in(O) the(O) Miss(B-event) World(I-event) pageant(I-event) won(O) by(O) Puerto(B-location) Rico(I-location) 's(O) Wilnelia(B-person) Merced(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, album, location, event, song, organization, musical artist, person, music genre, musical instrument, band, award and O.\nSentence: She had her first experience in show business when she was crowned Miss Teenager Universal in 1971 , and was Miss World / Venezuela in 1975 where she became sixth runner-up in the Miss World pageant won by Puerto Rico 's Wilnelia Merced .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","had","her","first","experience","in","show","business","when","she","was","crowned","Miss","Teenager","Universal","in","1971",",","and","was","Miss","World","/","Venezuela","in","1975","where","she","became","sixth","runner-up","in","the","Miss","World","pageant","won","by","Puerto","Rico","'s","Wilnelia","Merced","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","O","O","O","O","O","B-award","I-award","O","B-country","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","B-location","I-location","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["country","album","location","event","song","organization","musical_artist","person","music_genre","musical_instrument","band","award"]}
{"id":"85","dataset":"crossner_music","split":"test","instance":{"id":"85","prompt_labels":"Nathan(B-location) Phillips(I-location) Square(I-location) was(O) also(O) a(O) venue(O) for(O) Wheelchair(O) tennis(O) for(O) the(O) 2017(B-event) Invictus(I-event) Games(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, music genre, organization, album, event, song, award, musical artist, person, musical instrument, location, band and O.\nSentence: Nathan Phillips Square was also a venue for Wheelchair tennis for the 2017 Invictus Games .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Nathan","Phillips","Square","was","also","a","venue","for","Wheelchair","tennis","for","the","2017","Invictus","Games","."],"labels":["B-location","I-location","I-location","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["country","music_genre","organization","album","event","song","award","musical_artist","person","musical_instrument","location","band"]}
{"id":"86","dataset":"crossner_music","split":"test","instance":{"id":"86","prompt_labels":"Based(O) on(O) the(O) songs(O) of(O) the(O) Spice(B-band) Girls(I-band) ,(O) the(O) show(O) began(O) previews(O) at(O) the(O) Piccadilly(B-location) Theatre(I-location) in(O) the(O) West(B-event) End(I-event) on(O) 27(O) November(O) 2012(O) and(O) had(O) its(O) Press(B-event) Night(I-event) on(O) 11(O) December(O) 2012(O) and(O) features(O) some(O) of(O) the(O) group(O) 's(O) biggest(O) hit(O) songs(O) including(O) Wannabe(B-song) ,(O) Spice(B-song) Up(I-song) Your(I-song) Life(I-song) and(O) the(O) eponymous(O) Viva(B-song) Forever(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, country, musical instrument, song, musical artist, award, album, band, person, event, music genre and O.\nSentence: Based on the songs of the Spice Girls , the show began previews at the Piccadilly Theatre in the West End on 27 November 2012 and had its Press Night on 11 December 2012 and features some of the group 's biggest hit songs including Wannabe , Spice Up Your Life and the eponymous Viva Forever .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Based","on","the","songs","of","the","Spice","Girls",",","the","show","began","previews","at","the","Piccadilly","Theatre","in","the","West","End","on","27","November","2012","and","had","its","Press","Night","on","11","December","2012","and","features","some","of","the","group","'s","biggest","hit","songs","including","Wannabe",",","Spice","Up","Your","Life","and","the","eponymous","Viva","Forever","."],"labels":["O","O","O","O","O","O","B-band","I-band","O","O","O","O","O","O","O","B-location","I-location","O","O","B-event","I-event","O","O","O","O","O","O","O","B-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-song","O","B-song","I-song","I-song","I-song","O","O","O","B-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["location","organization","country","musical_instrument","song","musical_artist","award","album","band","person","event","music_genre"]}
{"id":"87","dataset":"crossner_music","split":"test","instance":{"id":"87","prompt_labels":"The(O) third(O) generation(O) ((O) 1950s-1960s(O) )(O) started(O) at(O) the(O) end(O) of(O) World(B-event) War(I-event) II(I-event) with(O) mountaineer(O) string(O) band(O) music(O) known(O) as(O) Bluegrass(B-music genre) music(I-music genre) ,(O) which(O) emerged(O) when(O) Bill(B-musical artist) Monroe(I-musical artist) ,(O) along(O) with(O) Lester(B-musical artist) Flatt(I-musical artist) and(O) Earl(B-musical artist) Scruggs(I-musical artist) were(O) introduced(O) by(O) Roy(B-musical artist) Acuff(I-musical artist) at(O) the(O) Grand(B-location) Ole(I-location) Opry(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, location, award, musical instrument, music genre, event, country, band, person, album, song, organization and O.\nSentence: The third generation ( 1950s-1960s ) started at the end of World War II with mountaineer string band music known as Bluegrass music , which emerged when Bill Monroe , along with Lester Flatt and Earl Scruggs were introduced by Roy Acuff at the Grand Ole Opry .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","third","generation","(","1950s-1960s",")","started","at","the","end","of","World","War","II","with","mountaineer","string","band","music","known","as","Bluegrass","music",",","which","emerged","when","Bill","Monroe",",","along","with","Lester","Flatt","and","Earl","Scruggs","were","introduced","by","Roy","Acuff","at","the","Grand","Ole","Opry","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","B-musical artist","I-musical artist","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","B-musical artist","I-musical artist","O","O","B-location","I-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","location","award","musical_instrument","music_genre","event","country","band","person","album","song","organization"]}
{"id":"90","dataset":"crossner_music","split":"test","instance":{"id":"90","prompt_labels":"He(O) also(O) co-wrote(O) Posible(B-song) ,(O) which(O) has(O) been(O) used(O) as(O) a(O) theme(O) song(O) for(O) the(O) 2005(B-event) Southeast(I-event) Asian(I-event) Games(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, award, person, country, event, musical artist, musical instrument, song, album, band, organization, location and O.\nSentence: He also co-wrote Posible , which has been used as a theme song for the 2005 Southeast Asian Games .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","also","co-wrote","Posible",",","which","has","been","used","as","a","theme","song","for","the","2005","Southeast","Asian","Games","."],"labels":["O","O","O","B-song","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["music_genre","award","person","country","event","musical_artist","musical_instrument","song","album","band","organization","location"]}
{"id":"92","dataset":"crossner_music","split":"test","instance":{"id":"92","prompt_labels":"Their(O) farewell(O) concerts(O) at(O) Wembley(B-location) Arena(I-location) were(O) multiple(O) sell-outs(O) ;(O) their(O) final(O) concert(O) took(O) place(O) at(O) the(O) Brighton(B-location) Centre(I-location) on(O) 11(O) December(O) 1982(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, music genre, organization, album, band, location, person, country, musical artist, musical instrument, event, award and O.\nSentence: Their farewell concerts at Wembley Arena were multiple sell-outs ; their final concert took place at the Brighton Centre on 11 December 1982 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Their","farewell","concerts","at","Wembley","Arena","were","multiple","sell-outs",";","their","final","concert","took","place","at","the","Brighton","Centre","on","11","December","1982","."],"labels":["O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["song","music_genre","organization","album","band","location","person","country","musical_artist","musical_instrument","event","award"]}
{"id":"95","dataset":"crossner_music","split":"test","instance":{"id":"95","prompt_labels":"The(O) West(B-location) African(I-location) community(O) is(O) also(O) very(O) large(O) ,(O) integrated(O) by(O) people(O) from(O) Senegal(B-country) ,(O) Mali(B-country) ,(O) Ivory(B-country) Coast(I-country) ,(O) and(O) Guinea(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, musical instrument, musical artist, award, song, event, organization, music genre, person, location, band, country and O.\nSentence: The West African community is also very large , integrated by people from Senegal , Mali , Ivory Coast , and Guinea .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","West","African","community","is","also","very","large",",","integrated","by","people","from","Senegal",",","Mali",",","Ivory","Coast",",","and","Guinea","."],"labels":["O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","B-country","O","B-country","O","B-country","I-country","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["album","musical_instrument","musical_artist","award","song","event","organization","music_genre","person","location","band","country"]}
{"id":"98","dataset":"crossner_music","split":"test","instance":{"id":"98","prompt_labels":"Their(O) musical(O) style(O) shifted(O) in(O) the(O) direction(O) of(O) progressive(B-music genre) rock(I-music genre) with(O) the(O) albums(O) Aqualung(B-album) ((O) 1971(O) )(O) ,(O) Thick(B-album) as(I-album) a(I-album) Brick(I-album) ((O) 1972(O) )(O) and(O) A(B-album) Passion(I-album) Play(I-album) ((O) 1973(O) )(O) ,(O) and(O) shifted(O) again(O) to(O) hard(B-music genre) rock(I-music genre) mixed(O) with(O) folk(B-music genre) rock(I-music genre) with(O) Songs(B-album) from(I-album) the(I-album) Wood(I-album) ((O) 1977(O) )(O) and(O) Heavy(B-album) Horses(I-album) ((O) 1978(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, country, organization, location, musical instrument, event, musical artist, band, album, song, music genre and O.\nSentence: Their musical style shifted in the direction of progressive rock with the albums Aqualung ( 1971 ) , Thick as a Brick ( 1972 ) and A Passion Play ( 1973 ) , and shifted again to hard rock mixed with folk rock with Songs from the Wood ( 1977 ) and Heavy Horses ( 1978 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Their","musical","style","shifted","in","the","direction","of","progressive","rock","with","the","albums","Aqualung","(","1971",")",",","Thick","as","a","Brick","(","1972",")","and","A","Passion","Play","(","1973",")",",","and","shifted","again","to","hard","rock","mixed","with","folk","rock","with","Songs","from","the","Wood","(","1977",")","and","Heavy","Horses","(","1978",")","."],"labels":["O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","B-album","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","B-music genre","I-music genre","O","B-album","I-album","I-album","I-album","O","O","O","O","B-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","award","country","organization","location","musical_instrument","event","musical_artist","band","album","song","music_genre"]}
{"id":"101","dataset":"crossner_music","split":"test","instance":{"id":"101","prompt_labels":"The(B-band) Godfather(I-band) won(O) three(O) Academy(B-award) Awards(I-award) :(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) ,(O) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Adapted(I-award) Screenplay(I-award) ((O) shared(O) with(O) Mario(B-person) Puzo(I-person) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, organization, location, band, country, musical instrument, musical artist, album, event, song, music genre and O.\nSentence: The Godfather won three Academy Awards : Academy Award for Best Picture , Academy Award for Best Actor , and Academy Award for Best Adapted Screenplay ( shared with Mario Puzo ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Godfather","won","three","Academy","Awards",":","Academy","Award","for","Best","Picture",",","Academy","Award","for","Best","Actor",",","and","Academy","Award","for","Best","Adapted","Screenplay","(","shared","with","Mario","Puzo",")","."],"labels":["B-band","I-band","O","O","B-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","B-person","I-person","O","O"],"target_index":null,"target_label":null},"label_list":["person","award","organization","location","band","country","musical_instrument","musical_artist","album","event","song","music_genre"]}
{"id":"104","dataset":"crossner_music","split":"test","instance":{"id":"104","prompt_labels":"Local(O) varieties(O) of(O) this(O) dance(O) are(O) also(O) found(O) in(O) the(O) Nordic(O) countries(O) ,(O) United(B-country) Kingdom(I-country) ,(O) Republic(B-country) of(I-country) Ireland(I-country) ,(O) Latin(B-location) America(I-location) ((O) especially(O) Mexico(B-country) )(O) ,(O) and(O) in(O) the(O) United(B-country) States(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, musical instrument, musical artist, person, organization, event, award, music genre, album, song, band and O.\nSentence: Local varieties of this dance are also found in the Nordic countries , United Kingdom , Republic of Ireland , Latin America ( especially Mexico ) , and in the United States .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Local","varieties","of","this","dance","are","also","found","in","the","Nordic","countries",",","United","Kingdom",",","Republic","of","Ireland",",","Latin","America","(","especially","Mexico",")",",","and","in","the","United","States","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O","B-country","I-country","I-country","O","B-location","I-location","O","O","B-country","O","O","O","O","O","B-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["country","location","musical_instrument","musical_artist","person","organization","event","award","music_genre","album","song","band"]}
{"id":"106","dataset":"crossner_music","split":"test","instance":{"id":"106","prompt_labels":"McTell(B-musical artist) 's(O) influence(O) extended(O) over(O) a(O) wide(O) variety(O) of(O) artists(O) ,(O) including(O) the(O) The(B-band) Allman(I-band) Brothers(I-band) Band(I-band) ,(O) who(O) covered(O) his(O) Statesboro(B-song) Blues(I-song) ,(O) and(O) Bob(B-person) Dylan(I-person) ,(O) who(O) paid(O) tribute(O) to(O) him(O) in(O) his(O) 1983(O) song(O) Blind(B-musical artist) Willie(I-musical artist) McTell(I-musical artist) ,(O) the(O) refrain(O) of(O) which(O) is(O) And(O) I(O) know(O) no(O) one(O) can(O) sing(O) the(O) blues(O) like(O) Blind(B-musical artist) Willie(I-musical artist) McTell(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, organization, country, person, music genre, event, musical artist, musical instrument, band, location, award, song and O.\nSentence: McTell 's influence extended over a wide variety of artists , including the The Allman Brothers Band , who covered his Statesboro Blues , and Bob Dylan , who paid tribute to him in his 1983 song Blind Willie McTell , the refrain of which is And I know no one can sing the blues like Blind Willie McTell .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["McTell","'s","influence","extended","over","a","wide","variety","of","artists",",","including","the","The","Allman","Brothers","Band",",","who","covered","his","Statesboro","Blues",",","and","Bob","Dylan",",","who","paid","tribute","to","him","in","his","1983","song","Blind","Willie","McTell",",","the","refrain","of","which","is","And","I","know","no","one","can","sing","the","blues","like","Blind","Willie","McTell","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","I-band","I-band","O","O","O","O","B-song","I-song","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["album","organization","country","person","music_genre","event","musical_artist","musical_instrument","band","location","award","song"]}
{"id":"107","dataset":"crossner_music","split":"test","instance":{"id":"107","prompt_labels":"He(O) also(O) performed(O) in(O) The(O) Rocky(O) Horror(O) Show(O) ,(O) as(O) the(O) narrator(O) ,(O) at(O) the(O) Churchill(B-location) Theatre(I-location) in(O) Bromley(B-location) and(O) the(O) New(B-location) Wimbledon(I-location) Theatre(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, award, person, organization, musical instrument, location, country, event, song, musical artist, band, music genre and O.\nSentence: He also performed in The Rocky Horror Show , as the narrator , at the Churchill Theatre in Bromley and the New Wimbledon Theatre .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","also","performed","in","The","Rocky","Horror","Show",",","as","the","narrator",",","at","the","Churchill","Theatre","in","Bromley","and","the","New","Wimbledon","Theatre","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","O","O","B-location","I-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["album","award","person","organization","musical_instrument","location","country","event","song","musical_artist","band","music_genre"]}
{"id":"109","dataset":"crossner_music","split":"test","instance":{"id":"109","prompt_labels":"Paul(B-musical artist) Gilbert(I-musical artist) composes(O) music(O) in(O) a(O) wide(O) variety(O) of(O) styles(O) ,(O) including(O) Pop(B-music genre) music(I-music genre) ,(O) Rock(B-music genre) and(I-music genre) roll(I-music genre) ,(O) metal(B-music genre) ,(O) blues(B-music genre) ,(O) and(O) funk(B-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, event, band, person, musical artist, musical instrument, music genre, award, song, album, location and O.\nSentence: Paul Gilbert composes music in a wide variety of styles , including Pop music , Rock and roll , metal , blues , and funk .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Paul","Gilbert","composes","music","in","a","wide","variety","of","styles",",","including","Pop","music",",","Rock","and","roll",",","metal",",","blues",",","and","funk","."],"labels":["B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","O","B-music genre","O","O","B-music genre","O"],"target_index":null,"target_label":null},"label_list":["country","organization","event","band","person","musical_artist","musical_instrument","music_genre","award","song","album","location"]}
{"id":"110","dataset":"crossner_music","split":"test","instance":{"id":"110","prompt_labels":"The(O) follow-up(O) release(O) ,(O) Road(B-song) Rage(I-song) ,(O) reached(O) number(O) 5(O) in(O) May(O) ,(O) and(O) was(O) nominated(O) for(O) best(O) song(O) at(O) the(O) Brit(B-award) Awards(I-award) and(O) the(O) Ivor(B-award) Novello(I-award) Awards(I-award) ,(O) winning(O) at(O) the(O) Q(B-award) Awards(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, organization, musical artist, song, band, music genre, person, country, album, award, location, event and O.\nSentence: The follow-up release , Road Rage , reached number 5 in May , and was nominated for best song at the Brit Awards and the Ivor Novello Awards , winning at the Q Awards .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","follow-up","release",",","Road","Rage",",","reached","number","5","in","May",",","and","was","nominated","for","best","song","at","the","Brit","Awards","and","the","Ivor","Novello","Awards",",","winning","at","the","Q","Awards","."],"labels":["O","O","O","O","B-song","I-song","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","O","O","O","O","B-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","organization","musical_artist","song","band","music_genre","person","country","album","award","location","event"]}
{"id":"111","dataset":"crossner_music","split":"test","instance":{"id":"111","prompt_labels":"With(O) the(O) song(O) 's(O) success(O) ,(O) the(O) age(O) of(O) rock(B-music genre) music(I-music genre) began(O) overnight(O) and(O) ended(O) the(O) dominance(O) of(O) the(O) jazz(B-music genre) and(O) pop(B-music genre) standards(O) performed(O) by(O) Frank(B-musical artist) Sinatra(I-musical artist) ,(O) Jo(B-musical artist) Stafford(I-musical artist) ,(O) Perry(B-musical artist) Como(I-musical artist) ,(O) Bing(B-musical artist) Crosby(I-musical artist) ,(O) Eddie(B-musical artist) Fisher(I-musical artist) ,(O) Patti(B-musical artist) Page(I-musical artist) and(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, location, person, band, organization, musical instrument, event, album, musical artist, music genre, country, award and O.\nSentence: With the song 's success , the age of rock music began overnight and ended the dominance of the jazz and pop standards performed by Frank Sinatra , Jo Stafford , Perry Como , Bing Crosby , Eddie Fisher , Patti Page and others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["With","the","song","'s","success",",","the","age","of","rock","music","began","overnight","and","ended","the","dominance","of","the","jazz","and","pop","standards","performed","by","Frank","Sinatra",",","Jo","Stafford",",","Perry","Como",",","Bing","Crosby",",","Eddie","Fisher",",","Patti","Page","and","others","."],"labels":["O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","B-music genre","O","B-music genre","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O"],"target_index":null,"target_label":null},"label_list":["song","location","person","band","organization","musical_instrument","event","album","musical_artist","music_genre","country","award"]}
{"id":"112","dataset":"crossner_music","split":"test","instance":{"id":"112","prompt_labels":"Govere(B-person) first(O) attracted(O) media(O) attention(O) by(O) being(O) the(O) first(O) black(O) female(O) gymnast(O) to(O) represent(O) Zimbabwe(B-country) in(O) artistic(O) gymnastics(O) at(O) the(O) 1999(B-event) All-Africa(I-event) Games(I-event) ,(O) and(O) later(O) by(O) co-founding(O) the(O) Kijana(B-organization) Project(I-organization) ,(O) which(O) provides(O) relief(O) for(O) AIDS(O) orphans(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, country, organization, band, song, event, music genre, musical artist, location, person, musical instrument, award and O.\nSentence: Govere first attracted media attention by being the first black female gymnast to represent Zimbabwe in artistic gymnastics at the 1999 All-Africa Games , and later by co-founding the Kijana Project , which provides relief for AIDS orphans .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Govere","first","attracted","media","attention","by","being","the","first","black","female","gymnast","to","represent","Zimbabwe","in","artistic","gymnastics","at","the","1999","All-Africa","Games",",","and","later","by","co-founding","the","Kijana","Project",",","which","provides","relief","for","AIDS","orphans","."],"labels":["B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["album","country","organization","band","song","event","music_genre","musical_artist","location","person","musical_instrument","award"]}
{"id":"113","dataset":"crossner_music","split":"test","instance":{"id":"113","prompt_labels":"Examples(O) can(O) be(O) found(O) in(O) the(O) music(O) of(O) the(B-band) Chambers(I-band) Brothers(I-band) ,(O) George(B-musical artist) Clinton(I-musical artist) with(O) his(O) Parliament-Funkadelic(B-band) collective(O) ,(O) Sly(B-band) and(I-band) the(I-band) Family(I-band) Stone(I-band) and(O) the(O) productions(O) of(O) Norman(B-musical artist) Whitfield(I-musical artist) with(O) The(B-band) Temptations(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, musical instrument, song, album, event, organization, band, award, musical artist, location, music genre, person and O.\nSentence: Examples can be found in the music of the Chambers Brothers , George Clinton with his Parliament-Funkadelic collective , Sly and the Family Stone and the productions of Norman Whitfield with The Temptations .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Examples","can","be","found","in","the","music","of","the","Chambers","Brothers",",","George","Clinton","with","his","Parliament-Funkadelic","collective",",","Sly","and","the","Family","Stone","and","the","productions","of","Norman","Whitfield","with","The","Temptations","."],"labels":["O","O","O","O","O","O","O","O","B-band","I-band","I-band","O","B-musical artist","I-musical artist","O","O","B-band","O","O","B-band","I-band","I-band","I-band","I-band","O","O","O","O","B-musical artist","I-musical artist","O","B-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["country","musical_instrument","song","album","event","organization","band","award","musical_artist","location","music_genre","person"]}
{"id":"115","dataset":"crossner_music","split":"test","instance":{"id":"115","prompt_labels":"Looking(O) to(O) depart(O) from(O) the(O) distorted(O) production(O) of(O) their(O) previous(O) record(O) ,(O) The(B-album) Downward(I-album) Spiral(I-album) ((O) 1994(O) )(O) ,(O) the(O) album(O) features(O) elements(O) of(O) Ambient(B-music genre) music(I-music genre) and(O) Electronic(B-music genre) music(I-music genre) music(O) ,(O) alongside(O) the(O) band(O) 's(O) traditional(O) industrial(B-music genre) rock(I-music genre) sound(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, event, music genre, musical artist, song, organization, album, person, band, location, country, award and O.\nSentence: Looking to depart from the distorted production of their previous record , The Downward Spiral ( 1994 ) , the album features elements of Ambient music and Electronic music music , alongside the band 's traditional industrial rock sound .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Looking","to","depart","from","the","distorted","production","of","their","previous","record",",","The","Downward","Spiral","(","1994",")",",","the","album","features","elements","of","Ambient","music","and","Electronic","music","music",",","alongside","the","band","'s","traditional","industrial","rock","sound","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","event","music_genre","musical_artist","song","organization","album","person","band","location","country","award"]}
{"id":"116","dataset":"crossner_music","split":"test","instance":{"id":"116","prompt_labels":"G-Mex(B-location) was(O) also(O) the(O) 2002(B-event) Commonwealth(I-event) Games(I-event) venue(O) for(O) gymnastics(O) ,(O) weightlifting(O) ,(O) judo(O) and(O) wrestling(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, musical artist, album, location, band, organization, song, musical instrument, person, award, music genre, event and O.\nSentence: G-Mex was also the 2002 Commonwealth Games venue for gymnastics , weightlifting , judo and wrestling .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["G-Mex","was","also","the","2002","Commonwealth","Games","venue","for","gymnastics",",","weightlifting",",","judo","and","wrestling","."],"labels":["B-location","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","musical_artist","album","location","band","organization","song","musical_instrument","person","award","music_genre","event"]}
{"id":"117","dataset":"crossner_music","split":"test","instance":{"id":"117","prompt_labels":"Buckingham(B-musical artist) 's(O) Second(B-song) Hand(I-song) News(I-song) ,(O) Nicks(B-musical artist) 's(O) Gold(B-song) Dust(I-song) Woman(I-song) and(O) The(B-song) Chain(I-song) ((O) the(O) only(O) song(O) written(O) by(O) all(O) five(O) band(O) members(O) )(O) also(O) received(O) significant(O) radio(O) airplay(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, musical artist, music genre, song, organization, country, location, album, event, person, band, award and O.\nSentence: Buckingham 's Second Hand News , Nicks 's Gold Dust Woman and The Chain ( the only song written by all five band members ) also received significant radio airplay .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Buckingham","'s","Second","Hand","News",",","Nicks","'s","Gold","Dust","Woman","and","The","Chain","(","the","only","song","written","by","all","five","band","members",")","also","received","significant","radio","airplay","."],"labels":["B-musical artist","O","B-song","I-song","I-song","O","B-musical artist","O","B-song","I-song","I-song","O","B-song","I-song","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","musical_artist","music_genre","song","organization","country","location","album","event","person","band","award"]}
{"id":"118","dataset":"crossner_music","split":"test","instance":{"id":"118","prompt_labels":"Johnston(B-person) was(O) involved(O) in(O) the(O) 1999(B-event) All-Africa(I-event) Games(I-event) ,(O) and(O) in(O) South(B-country) Africa(I-country) 's(O) bid(O) for(O) the(O) 2004(B-event) Summer(I-event) Olympics(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, musical artist, song, organization, award, album, country, musical instrument, event, band, person, music genre and O.\nSentence: Johnston was involved in the 1999 All-Africa Games , and in South Africa 's bid for the 2004 Summer Olympics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Johnston","was","involved","in","the","1999","All-Africa","Games",",","and","in","South","Africa","'s","bid","for","the","2004","Summer","Olympics","."],"labels":["B-person","O","O","O","O","B-event","I-event","I-event","O","O","O","B-country","I-country","O","O","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["location","musical_artist","song","organization","award","album","country","musical_instrument","event","band","person","music_genre"]}
{"id":"120","dataset":"crossner_music","split":"test","instance":{"id":"120","prompt_labels":"His(O) songs(O) for(O) the(O) group(O) included(O) Taxman(B-song) ,(O) Within(B-song) You(I-song) Without(I-song) You(I-song) ,(O) While(B-song) My(I-song) Guitar(I-song) Gently(I-song) Weeps(I-song) ,(O) Here(B-song) Comes(I-song) the(I-song) Sun(I-song) and(O) Something(B-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, musical artist, song, country, musical instrument, award, organization, event, music genre, band, album, location and O.\nSentence: His songs for the group included Taxman , Within You Without You , While My Guitar Gently Weeps , Here Comes the Sun and Something .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","songs","for","the","group","included","Taxman",",","Within","You","Without","You",",","While","My","Guitar","Gently","Weeps",",","Here","Comes","the","Sun","and","Something","."],"labels":["O","O","O","O","O","O","B-song","O","B-song","I-song","I-song","I-song","O","B-song","I-song","I-song","I-song","I-song","O","B-song","I-song","I-song","I-song","O","B-song","O"],"target_index":null,"target_label":null},"label_list":["person","musical_artist","song","country","musical_instrument","award","organization","event","music_genre","band","album","location"]}
{"id":"121","dataset":"crossner_music","split":"test","instance":{"id":"121","prompt_labels":"New(O) songwriting(O) and(O) recording(O) sessions(O) took(O) place(O) ,(O) and(O) during(O) October(O) and(O) December(O) ,(O) they(O) released(O) the(O) singles(O) The(B-song) Day(I-song) Before(I-song) You(I-song) Came(I-song) /(O) Cassandra(B-song) and(O) Under(B-song) Attack(I-song) /(O) You(B-song) Owe(I-song) Me(I-song) One(I-song) ,(O) the(O) A-sides(O) of(O) which(O) were(O) included(O) on(O) the(O) compilation(O) album(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, country, music genre, album, person, location, song, musical instrument, musical artist, organization, award, event and O.\nSentence: New songwriting and recording sessions took place , and during October and December , they released the singles The Day Before You Came / Cassandra and Under Attack / You Owe Me One , the A-sides of which were included on the compilation album .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["New","songwriting","and","recording","sessions","took","place",",","and","during","October","and","December",",","they","released","the","singles","The","Day","Before","You","Came","/","Cassandra","and","Under","Attack","/","You","Owe","Me","One",",","the","A-sides","of","which","were","included","on","the","compilation","album","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-song","I-song","I-song","I-song","I-song","O","B-song","O","B-song","I-song","O","B-song","I-song","I-song","I-song","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["band","country","music_genre","album","person","location","song","musical_instrument","musical_artist","organization","award","event"]}
{"id":"122","dataset":"crossner_music","split":"test","instance":{"id":"122","prompt_labels":"John(B-musical artist) Zorn(I-musical artist) ((O) born(O) September(O) 2(O) ,(O) 1953(O) )(O) is(O) an(O) American(O) composer(O) ,(O) arranger(O) ,(O) record(O) producer(O) ,(O) saxophonist(O) ,(O) and(O) multi-instrumentalist(O) with(O) hundreds(O) of(O) album(O) credits(O) as(O) performer(O) ,(O) composer(O) ,(O) and(O) producer(O) across(O) a(O) variety(O) of(O) genres(O) including(O) jazz(B-music genre) ,(O) rock(B-music genre) ,(O) Hardcore(B-music genre) punk(I-music genre) ,(O) classical(B-music genre) ,(O) Surf(B-music genre) music(I-music genre) ,(O) Heavy(B-music genre) metal(I-music genre) music(I-music genre) ,(O) soundtrack(B-music genre) ,(O) Ambient(B-music genre) music(I-music genre) ,(O) and(O) improvised(B-music genre) music(I-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, musical instrument, song, band, award, album, country, event, organization, person, music genre, location and O.\nSentence: John Zorn ( born September 2 , 1953 ) is an American composer , arranger , record producer , saxophonist , and multi-instrumentalist with hundreds of album credits as performer , composer , and producer across a variety of genres including jazz , rock , Hardcore punk , classical , Surf music , Heavy metal music , soundtrack , Ambient music , and improvised music .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["John","Zorn","(","born","September","2",",","1953",")","is","an","American","composer",",","arranger",",","record","producer",",","saxophonist",",","and","multi-instrumentalist","with","hundreds","of","album","credits","as","performer",",","composer",",","and","producer","across","a","variety","of","genres","including","jazz",",","rock",",","Hardcore","punk",",","classical",",","Surf","music",",","Heavy","metal","music",",","soundtrack",",","Ambient","music",",","and","improvised","music","."],"labels":["B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","O","B-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","musical_instrument","song","band","award","album","country","event","organization","person","music_genre","location"]}
{"id":"123","dataset":"crossner_music","split":"test","instance":{"id":"123","prompt_labels":"For(O) the(O) 2000(B-event) World(I-event) Junior(I-event) Championships(I-event) in(I-event) Athletics(I-event) ,(O) the(O) installation(O) of(O) individual(O) seats(O) was(O) required(O) ,(O) which(O) reduced(O) capacity(O) to(O) 66,000(O) spectators(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, award, band, event, person, music genre, album, musical instrument, location, organization, musical artist, country and O.\nSentence: For the 2000 World Junior Championships in Athletics , the installation of individual seats was required , which reduced capacity to 66,000 spectators .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","the","2000","World","Junior","Championships","in","Athletics",",","the","installation","of","individual","seats","was","required",",","which","reduced","capacity","to","66,000","spectators","."],"labels":["O","O","B-event","I-event","I-event","I-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["song","award","band","event","person","music_genre","album","musical_instrument","location","organization","musical_artist","country"]}
{"id":"124","dataset":"crossner_music","split":"test","instance":{"id":"124","prompt_labels":"Coolio(B-musical artist) was(O) dropped(O) from(O) Tommy(B-organization) Boy(I-organization) Records(I-organization) and(O) his(O) albums(O) since(O) then(O) ,(O) 2001(O) 's(O) Coolio.com(B-album) ,(O) 2003(O) 's(O) El(B-album) Cool(I-album) Magnifico(I-album) ,(O) 2006(O) 's(O) The(B-album) Return(I-album) of(I-album) the(I-album) Gangsta(I-album) ,(O) and(O) 2008(O) 's(O) Steal(B-album) Hear(I-album) ,(O) have(O) not(O) charted(O) on(O) any(O) Billboard(O) chart(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, album, location, musical artist, person, music genre, song, award, musical instrument, event, organization, country and O.\nSentence: Coolio was dropped from Tommy Boy Records and his albums since then , 2001 's Coolio.com , 2003 's El Cool Magnifico , 2006 's The Return of the Gangsta , and 2008 's Steal Hear , have not charted on any Billboard chart .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Coolio","was","dropped","from","Tommy","Boy","Records","and","his","albums","since","then",",","2001","'s","Coolio.com",",","2003","'s","El","Cool","Magnifico",",","2006","'s","The","Return","of","the","Gangsta",",","and","2008","'s","Steal","Hear",",","have","not","charted","on","any","Billboard","chart","."],"labels":["B-musical artist","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","B-album","O","O","O","B-album","I-album","I-album","O","O","O","B-album","I-album","I-album","I-album","I-album","O","O","O","O","B-album","I-album","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["band","album","location","musical_artist","person","music_genre","song","award","musical_instrument","event","organization","country"]}
{"id":"126","dataset":"crossner_music","split":"test","instance":{"id":"126","prompt_labels":"Due(O) to(O) shared(O) cultural(O) heritage(O) and(O) language(O) ,(O) Indian(O) music(O) and(O) Bollywood(O) films(O) are(O) also(O) popular(O) in(O) Afghanistan(B-country) ,(O) Pakistan(B-country) ,(O) Bangladesh(B-country) ,(O) and(O) Nepal(B-country) ,(O) where(O) Hindustani(O) is(O) widely(O) understood(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, album, musical instrument, music genre, song, band, musical artist, location, award, person, country and O.\nSentence: Due to shared cultural heritage and language , Indian music and Bollywood films are also popular in Afghanistan , Pakistan , Bangladesh , and Nepal , where Hindustani is widely understood .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Due","to","shared","cultural","heritage","and","language",",","Indian","music","and","Bollywood","films","are","also","popular","in","Afghanistan",",","Pakistan",",","Bangladesh",",","and","Nepal",",","where","Hindustani","is","widely","understood","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","B-country","O","B-country","O","O","B-country","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","event","album","musical_instrument","music_genre","song","band","musical_artist","location","award","person","country"]}
{"id":"127","dataset":"crossner_music","split":"test","instance":{"id":"127","prompt_labels":"Several(O) countries(O) geographically(O) outside(O) the(O) boundaries(O) of(O) Europe(B-location) have(O) competed(O) :(O) Israel(B-country) ,(O) Cyprus(B-country) and(O) Armenia(B-country) in(O) Western(B-location) Asia(I-location) ((O) Cyprus(B-country) is(O) a(O) member(O) of(O) the(O) Council(B-organization) of(I-organization) Europe(I-organization) and(O) a(O) member(O) state(O) of(O) the(O) European(B-organization) Union(I-organization) )(O) ,(O) since(O) 1973(O) ,(O) 1981(O) and(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2006(I-event) respectively(O) ;(O) Australia(B-country) since(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2015(I-event)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, award, location, song, band, musical instrument, album, musical artist, event, country, person, organization and O.\nSentence: Several countries geographically outside the boundaries of Europe have competed : Israel , Cyprus and Armenia in Western Asia ( Cyprus is a member of the Council of Europe and a member state of the European Union ) , since 1973 , 1981 and Eurovision Song Contest 2006 respectively ; Australia since Eurovision Song Contest 2015","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Several","countries","geographically","outside","the","boundaries","of","Europe","have","competed",":","Israel",",","Cyprus","and","Armenia","in","Western","Asia","(","Cyprus","is","a","member","of","the","Council","of","Europe","and","a","member","state","of","the","European","Union",")",",","since","1973",",","1981","and","Eurovision","Song","Contest","2006","respectively",";","Australia","since","Eurovision","Song","Contest","2015"],"labels":["O","O","O","O","O","O","O","B-location","O","O","O","B-country","O","B-country","O","B-country","O","B-location","I-location","O","B-country","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","O","O","B-country","O","B-event","I-event","I-event","I-event"],"target_index":null,"target_label":null},"label_list":["music_genre","award","location","song","band","musical_instrument","album","musical_artist","event","country","person","organization"]}
{"id":"128","dataset":"crossner_music","split":"test","instance":{"id":"128","prompt_labels":"The(O) 2017(O) production(O) was(O) nominated(O) for(O) 10(O) Laurence(B-award) Olivier(I-award) Award(I-award) s(O) and(O) won(O) 2(O) for(O) Laurence(B-award) Olivier(I-award) Award(I-award) for(I-award) Best(I-award) Musical(I-award) Revival(I-award) and(O) Laurence(B-award) Olivier(I-award) Award(I-award) for(I-award) Best(I-award) Costume(I-award) Design(I-award) ((O) by(O) Vicki(B-person) Mortimer(I-person) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, album, band, location, musical artist, award, organization, musical instrument, song, event, music genre, country and O.\nSentence: The 2017 production was nominated for 10 Laurence Olivier Award s and won 2 for Laurence Olivier Award for Best Musical Revival and Laurence Olivier Award for Best Costume Design ( by Vicki Mortimer ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","2017","production","was","nominated","for","10","Laurence","Olivier","Award","s","and","won","2","for","Laurence","Olivier","Award","for","Best","Musical","Revival","and","Laurence","Olivier","Award","for","Best","Costume","Design","(","by","Vicki","Mortimer",")","."],"labels":["O","O","O","O","O","O","O","B-award","I-award","I-award","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","B-person","I-person","O","O"],"target_index":null,"target_label":null},"label_list":["person","album","band","location","musical_artist","award","organization","musical_instrument","song","event","music_genre","country"]}
{"id":"131","dataset":"crossner_music","split":"test","instance":{"id":"131","prompt_labels":"Artists(O) who(O) typified(O) this(O) sound(O) included(O) Travis(B-musical artist) Tritt(I-musical artist) ,(O) Reba(B-musical artist) McEntire(I-musical artist) ,(O) George(B-musical artist) Strait(I-musical artist) ,(O) Keith(B-musical artist) Whitley(I-musical artist) ,(O) Alan(B-musical artist) Jackson(I-musical artist) ,(O) Ricky(B-musical artist) Skaggs(I-musical artist) ,(O) Patty(B-musical artist) Loveless(I-musical artist) ,(O) Kathy(B-musical artist) Mattea(I-musical artist) ,(O) Randy(B-musical artist) Travis(I-musical artist) ,(O) Dwight(B-musical artist) Yoakam(I-musical artist) ,(O) and(O) The(B-band) Judds(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, song, organization, event, band, location, award, music genre, musical instrument, album, person, country and O.\nSentence: Artists who typified this sound included Travis Tritt , Reba McEntire , George Strait , Keith Whitley , Alan Jackson , Ricky Skaggs , Patty Loveless , Kathy Mattea , Randy Travis , Dwight Yoakam , and The Judds .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Artists","who","typified","this","sound","included","Travis","Tritt",",","Reba","McEntire",",","George","Strait",",","Keith","Whitley",",","Alan","Jackson",",","Ricky","Skaggs",",","Patty","Loveless",",","Kathy","Mattea",",","Randy","Travis",",","Dwight","Yoakam",",","and","The","Judds","."],"labels":["O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","B-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","song","organization","event","band","location","award","music_genre","musical_instrument","album","person","country"]}
{"id":"132","dataset":"crossner_music","split":"test","instance":{"id":"132","prompt_labels":"However(O) ,(O) several(O) 1960s(O) pop(O) music(O) hits(O) feature(O) the(O) bassoon(B-musical instrument) ,(O) including(O) The(B-song) Tears(I-song) of(I-song) a(I-song) Clown(I-song) by(O) The(B-band) Miracles(I-band) ((O) the(O) bassoonist(O) was(O) Charles(B-musical artist) R.(I-musical artist) Sirard(I-musical artist) )(O) ,(O) Jennifer(B-song) Juniper(I-song) by(O) Donovan(B-musical artist) ,(O) 59th(B-song) Street(I-song) Bridge(I-song) Song(I-song) by(O) Harpers(B-band) Bizarre(I-band) ,(O) and(O) the(O) oompah(B-music genre) bassoon(B-musical instrument) underlying(O) The(B-band) New(I-band) Vaudeville(I-band) Band(I-band) '(O) s(O) Winchester(B-location) Cathedral(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, person, music genre, location, country, band, album, musical instrument, musical artist, award, organization, event and O.\nSentence: However , several 1960s pop music hits feature the bassoon , including The Tears of a Clown by The Miracles ( the bassoonist was Charles R. Sirard ) , Jennifer Juniper by Donovan , 59th Street Bridge Song by Harpers Bizarre , and the oompah bassoon underlying The New Vaudeville Band ' s Winchester Cathedral .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["However",",","several","1960s","pop","music","hits","feature","the","bassoon",",","including","The","Tears","of","a","Clown","by","The","Miracles","(","the","bassoonist","was","Charles","R.","Sirard",")",",","Jennifer","Juniper","by","Donovan",",","59th","Street","Bridge","Song","by","Harpers","Bizarre",",","and","the","oompah","bassoon","underlying","The","New","Vaudeville","Band","'","s","Winchester","Cathedral","."],"labels":["O","O","O","O","O","O","O","O","O","B-musical instrument","O","O","B-song","I-song","I-song","I-song","I-song","O","B-band","I-band","O","O","O","O","B-musical artist","I-musical artist","I-musical artist","O","O","B-song","I-song","O","B-musical artist","O","B-song","I-song","I-song","I-song","O","B-band","I-band","O","O","O","B-music genre","B-musical instrument","O","B-band","I-band","I-band","I-band","O","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["song","person","music_genre","location","country","band","album","musical_instrument","musical_artist","award","organization","event"]}
{"id":"133","dataset":"crossner_music","split":"test","instance":{"id":"133","prompt_labels":"The(O) group(O) have(O) released(O) six(O) studio(O) albums(O) :(O) both(O) The(B-album) Official(I-album) Fiction(I-album) ((O) 2003(O) )(O) and(O) Desert(B-album) Lights(I-album) ((O) 2006(O) )(O) topped(O) the(O) ARIA(O) Albums(O) Chart(O) ;(O) while(O) Beautiful(B-album) Sharks(I-album) ((O) 1999(O) )(O) ,(O) Echolalia(B-album) ((O) 2001(O) )(O) and(O) Leave(B-album) Your(I-album) Soul(I-album) to(I-album) Science(I-album) ((O) 2012(O) )(O) reached(O) the(O) top(O) 10(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, event, location, musical instrument, country, award, musical artist, song, band, album, organization, person and O.\nSentence: The group have released six studio albums : both The Official Fiction ( 2003 ) and Desert Lights ( 2006 ) topped the ARIA Albums Chart ; while Beautiful Sharks ( 1999 ) , Echolalia ( 2001 ) and Leave Your Soul to Science ( 2012 ) reached the top 10 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","group","have","released","six","studio","albums",":","both","The","Official","Fiction","(","2003",")","and","Desert","Lights","(","2006",")","topped","the","ARIA","Albums","Chart",";","while","Beautiful","Sharks","(","1999",")",",","Echolalia","(","2001",")","and","Leave","Your","Soul","to","Science","(","2012",")","reached","the","top","10","."],"labels":["O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O","B-album","I-album","O","O","O","O","O","O","O","O","O","O","B-album","I-album","O","O","O","O","B-album","O","O","O","O","B-album","I-album","I-album","I-album","I-album","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["music_genre","event","location","musical_instrument","country","award","musical_artist","song","band","album","organization","person"]}
{"id":"134","dataset":"crossner_music","split":"test","instance":{"id":"134","prompt_labels":"Their(O) fifth(O) album(O) ,(O) Rosenrot(B-album) ,(O) was(O) released(O) in(O) 2005(O) ,(O) and(O) the(O) lead(O) single(O) ,(O) Benzin(B-song) ,(O) reached(O) No.(O) 1(O) in(O) Finland(B-country) ,(O) becoming(O) their(O) first(O) ever(O) No.(O) 1(O) single(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, band, album, musical artist, song, event, organization, musical instrument, music genre, award, country, person and O.\nSentence: Their fifth album , Rosenrot , was released in 2005 , and the lead single , Benzin , reached No. 1 in Finland , becoming their first ever No. 1 single .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Their","fifth","album",",","Rosenrot",",","was","released","in","2005",",","and","the","lead","single",",","Benzin",",","reached","No.","1","in","Finland",",","becoming","their","first","ever","No.","1","single","."],"labels":["O","O","O","O","B-album","O","O","O","O","O","O","O","O","O","O","O","B-song","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","band","album","musical_artist","song","event","organization","musical_instrument","music_genre","award","country","person"]}
{"id":"136","dataset":"crossner_music","split":"test","instance":{"id":"136","prompt_labels":"|(O) Invisible(O) Children(O) Invisible(B-organization) Children(I-organization) ,(I-organization) Inc(I-organization) ,(O) To(B-organization) Write(I-organization) Love(I-organization) On(I-organization) Her(I-organization) Arms(I-organization) ,(O) Shirts(B-organization) for(I-organization) a(I-organization) Cure(I-organization) ,(O) Keep(B-organization) A(I-organization) Breast(I-organization) Foundation(I-organization) ,(O) and(O) Hope(B-organization) For(I-organization) The(I-organization) Day(I-organization) to(O) advocate(O) about(O) their(O) cause(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, song, band, location, award, country, musical artist, person, music genre, album, organization, musical instrument and O.\nSentence: | Invisible Children Invisible Children , Inc , To Write Love On Her Arms , Shirts for a Cure , Keep A Breast Foundation , and Hope For The Day to advocate about their cause .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["|","Invisible","Children","Invisible","Children",",","Inc",",","To","Write","Love","On","Her","Arms",",","Shirts","for","a","Cure",",","Keep","A","Breast","Foundation",",","and","Hope","For","The","Day","to","advocate","about","their","cause","."],"labels":["O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","song","band","location","award","country","musical_artist","person","music_genre","album","organization","musical_instrument"]}
{"id":"137","dataset":"crossner_music","split":"test","instance":{"id":"137","prompt_labels":"House(B-band) of(I-band) Pain(I-band) abruptly(O) broke(O) up(O) in(O) 1996(O) after(O) the(O) release(O) of(O) their(O) third(O) album(O) ,(O) Truth(B-album) Crushed(I-album) to(I-album) Earth(I-album) Shall(I-album) Rise(I-album) Again(I-album) ,(O) which(O) featured(O) guest(O) appearances(O) by(O) rappers(O) Sadat(B-musical artist) X(I-musical artist) of(O) Brand(B-band) Nubian(I-band) ,(O) Guru(B-musical artist) of(O) Gang(B-band) Starr(I-band) ,(O) Divine(B-musical artist) Styler(I-musical artist) and(O) Cokni(B-musical artist) O(I-musical artist) 'Dire(I-musical artist) of(O) the(O) Scheme(B-band) Team(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, award, band, location, musical artist, album, event, organization, song, musical instrument, music genre and O.\nSentence: House of Pain abruptly broke up in 1996 after the release of their third album , Truth Crushed to Earth Shall Rise Again , which featured guest appearances by rappers Sadat X of Brand Nubian , Guru of Gang Starr , Divine Styler and Cokni O 'Dire of the Scheme Team .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["House","of","Pain","abruptly","broke","up","in","1996","after","the","release","of","their","third","album",",","Truth","Crushed","to","Earth","Shall","Rise","Again",",","which","featured","guest","appearances","by","rappers","Sadat","X","of","Brand","Nubian",",","Guru","of","Gang","Starr",",","Divine","Styler","and","Cokni","O","'Dire","of","the","Scheme","Team","."],"labels":["B-band","I-band","I-band","O","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","I-album","I-album","I-album","I-album","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-band","I-band","O","B-musical artist","O","B-band","I-band","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","O","B-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["country","person","award","band","location","musical_artist","album","event","organization","song","musical_instrument","music_genre"]}
{"id":"138","dataset":"crossner_music","split":"test","instance":{"id":"138","prompt_labels":"Kid(B-album) A(I-album) received(O) a(O) Grammy(B-award) Award(I-award) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Alternative(I-award) Music(I-award) Album(I-award) and(O) a(O) nomination(O) for(O) Grammy(B-award) Award(I-award) for(I-award) Album(I-award) of(I-award) the(I-award) Year(I-award) in(O) early(O) 2001(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, country, musical instrument, album, event, organization, person, song, band, music genre, location, musical artist and O.\nSentence: Kid A received a Grammy Award for Grammy Award for Best Alternative Music Album and a nomination for Grammy Award for Album of the Year in early 2001 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Kid","A","received","a","Grammy","Award","for","Grammy","Award","for","Best","Alternative","Music","Album","and","a","nomination","for","Grammy","Award","for","Album","of","the","Year","in","early","2001","."],"labels":["B-album","I-album","O","O","B-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","country","musical_instrument","album","event","organization","person","song","band","music_genre","location","musical_artist"]}
{"id":"139","dataset":"crossner_music","split":"test","instance":{"id":"139","prompt_labels":"Alan(B-musical artist) Lomax(I-musical artist) '(O) s(O) recordings(O) of(O) Mississippi(B-musical artist) Fred(I-musical artist) McDowell(I-musical artist) would(O) eventually(O) bring(O) him(O) wider(O) attention(O) on(O) both(O) the(O) blues(B-music genre) and(O) Folk(B-music genre) music(I-music genre) circuit(O) ,(O) with(O) McDowell(B-musical artist) 's(O) droning(B-music genre) style(I-music genre) influencing(O) North(B-location) Mississippi(I-location) hill(B-music genre) country(I-music genre) blues(I-music genre) musicians(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, event, album, musical artist, person, band, musical instrument, award, organization, location, music genre, country and O.\nSentence: Alan Lomax ' s recordings of Mississippi Fred McDowell would eventually bring him wider attention on both the blues and Folk music circuit , with McDowell 's droning style influencing North Mississippi hill country blues musicians .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Alan","Lomax","'","s","recordings","of","Mississippi","Fred","McDowell","would","eventually","bring","him","wider","attention","on","both","the","blues","and","Folk","music","circuit",",","with","McDowell","'s","droning","style","influencing","North","Mississippi","hill","country","blues","musicians","."],"labels":["B-musical artist","I-musical artist","O","O","O","O","B-musical artist","I-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","B-music genre","O","B-music genre","I-music genre","O","O","O","B-musical artist","O","B-music genre","I-music genre","O","B-location","I-location","B-music genre","I-music genre","I-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["song","event","album","musical_artist","person","band","musical_instrument","award","organization","location","music_genre","country"]}
{"id":"141","dataset":"crossner_music","split":"test","instance":{"id":"141","prompt_labels":"In(O) 1998(O) Warp(B-musical artist) signed(O) Boards(B-band) Of(I-band) Canada(I-band) ,(O) a(O) duo(O) that(O) would(O) go(O) on(O) to(O) release(O) some(O) of(O) the(O) most(O) highly(O) revered(O) electronic(B-music genre) music(I-music genre) albums(O) of(O) their(O) time(O) :(O) Music(B-album) Has(I-album) the(I-album) Right(I-album) to(I-album) Children(I-album) ((O) 1998(O) )(O) ,(O) Geogaddi(B-album) ((O) 2002(O) )(O) ,(O) The(B-album) Campfire(I-album) Headphase(I-album) ((O) 2005(O) )(O) and(O) Tomorrow(B-album) 's(I-album) Harvest(I-album) ((O) 2013(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, award, music genre, location, event, country, album, band, organization, person, musical instrument, musical artist and O.\nSentence: In 1998 Warp signed Boards Of Canada , a duo that would go on to release some of the most highly revered electronic music albums of their time : Music Has the Right to Children ( 1998 ) , Geogaddi ( 2002 ) , The Campfire Headphase ( 2005 ) and Tomorrow 's Harvest ( 2013 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1998","Warp","signed","Boards","Of","Canada",",","a","duo","that","would","go","on","to","release","some","of","the","most","highly","revered","electronic","music","albums","of","their","time",":","Music","Has","the","Right","to","Children","(","1998",")",",","Geogaddi","(","2002",")",",","The","Campfire","Headphase","(","2005",")","and","Tomorrow","'s","Harvest","(","2013",")","."],"labels":["O","O","B-musical artist","O","B-band","I-band","I-band","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","B-album","I-album","I-album","I-album","I-album","I-album","O","O","O","O","B-album","O","O","O","O","B-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["song","award","music_genre","location","event","country","album","band","organization","person","musical_instrument","musical_artist"]}
{"id":"142","dataset":"crossner_music","split":"test","instance":{"id":"142","prompt_labels":"Three(O) of(O) them(O) ,(O) Emergency(B-album) on(I-album) Planet(I-album) Earth(I-album) ((O) 1993(O) )(O) ,(O) Synkronized(B-album) ((O) 1999(O) )(O) ,(O) A(B-album) Funk(I-album) Odyssey(I-album) ((O) 2001(O) )(O) ,(O) along(O) with(O) their(B-album) greatest(I-album) hits(I-album) compilation(O) ,(O) charted(O) at(O) number(O) 1(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, person, song, country, music genre, event, band, musical instrument, album, location, award, organization and O.\nSentence: Three of them , Emergency on Planet Earth ( 1993 ) , Synkronized ( 1999 ) , A Funk Odyssey ( 2001 ) , along with their greatest hits compilation , charted at number 1 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Three","of","them",",","Emergency","on","Planet","Earth","(","1993",")",",","Synkronized","(","1999",")",",","A","Funk","Odyssey","(","2001",")",",","along","with","their","greatest","hits","compilation",",","charted","at","number","1","."],"labels":["O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","B-album","O","O","O","O","B-album","I-album","I-album","O","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","person","song","country","music_genre","event","band","musical_instrument","album","location","award","organization"]}
{"id":"144","dataset":"crossner_music","split":"test","instance":{"id":"144","prompt_labels":"In(O) the(O) Southwestern(B-location) United(I-location) States(I-location) ,(O) it(O) was(O) the(O) Rocky(B-location) Mountains(I-location) ,(O) American(B-location) frontier(I-location) ,(O) and(O) Rio(B-location) Grande(I-location) that(O) acted(O) as(O) a(O) similar(O) backdrop(O) for(O) Indigenous(B-music genre) music(I-music genre) of(I-music genre) North(I-music genre) America(I-music genre) ,(O) Mexican(O) ,(O) and(O) cowboy(B-music genre) ballads(I-music genre) ,(O) which(O) resulted(O) in(O) New(B-music genre) Mexico(I-music genre) music(I-music genre) and(O) the(O) development(O) of(O) Western(B-music genre) music(I-music genre) ,(O) and(O) its(O) directly(O) related(O) Red(B-music genre) Dirt(I-music genre) ,(O) Texas(B-music genre) country(I-music genre) music(I-music genre) ,(O) and(O) Tejano(B-music genre) music(I-music genre) styles(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, location, band, award, event, organization, person, album, music genre, country, musical artist, musical instrument and O.\nSentence: In the Southwestern United States , it was the Rocky Mountains , American frontier , and Rio Grande that acted as a similar backdrop for Indigenous music of North America , Mexican , and cowboy ballads , which resulted in New Mexico music and the development of Western music , and its directly related Red Dirt , Texas country music , and Tejano music styles .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","Southwestern","United","States",",","it","was","the","Rocky","Mountains",",","American","frontier",",","and","Rio","Grande","that","acted","as","a","similar","backdrop","for","Indigenous","music","of","North","America",",","Mexican",",","and","cowboy","ballads",",","which","resulted","in","New","Mexico","music","and","the","development","of","Western","music",",","and","its","directly","related","Red","Dirt",",","Texas","country","music",",","and","Tejano","music","styles","."],"labels":["O","O","B-location","I-location","I-location","O","O","O","O","B-location","I-location","O","B-location","I-location","O","O","B-location","I-location","O","O","O","O","O","O","O","B-music genre","I-music genre","I-music genre","I-music genre","I-music genre","O","O","O","O","B-music genre","I-music genre","O","O","O","O","B-music genre","I-music genre","I-music genre","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","O","B-music genre","I-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["song","location","band","award","event","organization","person","album","music_genre","country","musical_artist","musical_instrument"]}
{"id":"148","dataset":"crossner_music","split":"test","instance":{"id":"148","prompt_labels":"Wills(B-musical artist) favored(O) jazz-like(B-music genre) arrangements(O) and(O) the(O) band(O) found(O) national(O) popularity(O) into(O) the(O) 1940s(O) with(O) such(O) hits(O) as(O) Steel(B-song) Guitar(I-song) Rag(I-song) ,(O) New(B-song) San(I-song) Antonio(I-song) Rose(I-song) ,(O) Smoke(B-song) On(I-song) The(I-song) Water(I-song) ,(O) Stars(B-song) And(I-song) Stripes(I-song) On(I-song) Iwo(I-song) Jima(I-song) ,(O) and(O) New(B-song) Spanish(I-song) Two(I-song) Step(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, location, song, music genre, album, musical instrument, event, band, award, person, organization, country and O.\nSentence: Wills favored jazz-like arrangements and the band found national popularity into the 1940s with such hits as Steel Guitar Rag , New San Antonio Rose , Smoke On The Water , Stars And Stripes On Iwo Jima , and New Spanish Two Step .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Wills","favored","jazz-like","arrangements","and","the","band","found","national","popularity","into","the","1940s","with","such","hits","as","Steel","Guitar","Rag",",","New","San","Antonio","Rose",",","Smoke","On","The","Water",",","Stars","And","Stripes","On","Iwo","Jima",",","and","New","Spanish","Two","Step","."],"labels":["B-musical artist","O","B-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-song","I-song","I-song","O","B-song","I-song","I-song","I-song","O","B-song","I-song","I-song","I-song","O","B-song","I-song","I-song","I-song","I-song","I-song","O","O","B-song","I-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","location","song","music_genre","album","musical_instrument","event","band","award","person","organization","country"]}
{"id":"149","dataset":"crossner_music","split":"test","instance":{"id":"149","prompt_labels":"((O) e.g.(O) Red(B-song) Sector(I-song) A(I-song) ,(O) Closer(B-song) to(I-song) the(I-song) Heart(I-song) on(O) A(B-album) Show(I-album) of(I-album) Hands(I-album) and(O) Mystic(B-song) Rhythms(I-song) on(O) R30(B-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, song, album, band, organization, musical artist, award, country, musical instrument, music genre, location, person and O.\nSentence: ( e.g. Red Sector A , Closer to the Heart on A Show of Hands and Mystic Rhythms on R30 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["(","e.g.","Red","Sector","A",",","Closer","to","the","Heart","on","A","Show","of","Hands","and","Mystic","Rhythms","on","R30","."],"labels":["O","O","B-song","I-song","I-song","O","B-song","I-song","I-song","I-song","O","B-album","I-album","I-album","I-album","O","B-song","I-song","O","B-event","O"],"target_index":null,"target_label":null},"label_list":["event","song","album","band","organization","musical_artist","award","country","musical_instrument","music_genre","location","person"]}
{"id":"150","dataset":"crossner_music","split":"test","instance":{"id":"150","prompt_labels":"The(O) new(O) millennium(O) brought(O) about(O) a(O) drastic(O) change(O) in(O) Autechre(B-band) 's(O) style(O) ,(O) demonstrated(O) by(O) Confield(B-album) ((O) 2001(O) )(O) and(O) Draft(B-album) 7.30(I-album) ((O) 2003(O) )(O) ,(O) as(O) well(O) as(O) the(O) Gantz(B-album) Graf(I-album) EP(O) ((O) 2002(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, band, country, person, location, event, song, award, music genre, organization, musical instrument, album and O.\nSentence: The new millennium brought about a drastic change in Autechre 's style , demonstrated by Confield ( 2001 ) and Draft 7.30 ( 2003 ) , as well as the Gantz Graf EP ( 2002 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","new","millennium","brought","about","a","drastic","change","in","Autechre","'s","style",",","demonstrated","by","Confield","(","2001",")","and","Draft","7.30","(","2003",")",",","as","well","as","the","Gantz","Graf","EP","(","2002",")","."],"labels":["O","O","O","O","O","O","O","O","O","B-band","O","O","O","O","O","B-album","O","O","O","O","B-album","I-album","O","O","O","O","O","O","O","O","B-album","I-album","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","band","country","person","location","event","song","award","music_genre","organization","musical_instrument","album"]}
{"id":"151","dataset":"crossner_music","split":"test","instance":{"id":"151","prompt_labels":"The(O) agreement(O) sees(O) an(O) annual(O) fixture(O) at(O) the(O) MCG(B-location) ,(O) beginning(O) with(O) a(O) clash(O) between(O) Australia(B-country) and(O) European(O) champions(O) Greece(B-country) on(O) 25(O) May(O) 2006(O) in(O) front(O) of(O) a(O) sell-out(O) crowd(O) of(O) 95,103(O) ,(O) before(O) Australia(B-country) left(O) to(O) contest(O) in(O) the(O) World(B-event) Cup(I-event) finals(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, band, musical artist, person, award, event, location, musical instrument, organization, country, album, song and O.\nSentence: The agreement sees an annual fixture at the MCG , beginning with a clash between Australia and European champions Greece on 25 May 2006 in front of a sell-out crowd of 95,103 , before Australia left to contest in the World Cup finals .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","agreement","sees","an","annual","fixture","at","the","MCG",",","beginning","with","a","clash","between","Australia","and","European","champions","Greece","on","25","May","2006","in","front","of","a","sell-out","crowd","of","95,103",",","before","Australia","left","to","contest","in","the","World","Cup","finals","."],"labels":["O","O","O","O","O","O","O","O","B-location","O","O","O","O","O","O","B-country","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","B-event","I-event","O","O"],"target_index":null,"target_label":null},"label_list":["music_genre","band","musical_artist","person","award","event","location","musical_instrument","organization","country","album","song"]}
{"id":"152","dataset":"crossner_music","split":"test","instance":{"id":"152","prompt_labels":"As(O) the(O) recording(O) industry(O) grew(O) ,(O) country(B-music genre) blues(I-music genre) performers(O) like(O) Bo(B-musical artist) Carter(I-musical artist) ,(O) Jimmie(B-musical artist) Rodgers(I-musical artist) ((O) country(O) singer(O) )(O) ,(O) Blind(B-musical artist) Lemon(I-musical artist) Jefferson(I-musical artist) ,(O) Lonnie(B-musical artist) Johnson(I-musical artist) ,(O) Tampa(B-musical artist) Red(I-musical artist) and(O) Blind(B-musical artist) Blake(I-musical artist) became(O) more(O) popular(O) in(O) the(O) African(O) American(O) community(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, location, musical artist, person, song, music genre, event, country, band, organization, award, musical instrument and O.\nSentence: As the recording industry grew , country blues performers like Bo Carter , Jimmie Rodgers ( country singer ) , Blind Lemon Jefferson , Lonnie Johnson , Tampa Red and Blind Blake became more popular in the African American community .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["As","the","recording","industry","grew",",","country","blues","performers","like","Bo","Carter",",","Jimmie","Rodgers","(","country","singer",")",",","Blind","Lemon","Jefferson",",","Lonnie","Johnson",",","Tampa","Red","and","Blind","Blake","became","more","popular","in","the","African","American","community","."],"labels":["O","O","O","O","O","O","B-music genre","I-music genre","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O","O","B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["album","location","musical_artist","person","song","music_genre","event","country","band","organization","award","musical_instrument"]}
{"id":"153","dataset":"crossner_music","split":"test","instance":{"id":"153","prompt_labels":"Some(O) of(O) Mattacks(B-musical artist) '(O) most(O) notable(O) participation(O) in(O) studio(O) recordings(O) in(O) the(O) late(O) 1970s(O) are(O) the(O) work(O) on(O) art(O) rock(O) studio(O) albums(O) by(O) Brian(B-musical artist) Eno(I-musical artist) ((O) Before(B-album) and(I-album) After(I-album) Science(I-album) )(O) and(O) 801(B-band) '(O) s(O) Listen(B-album) Now(I-album) ,(O) as(O) well(O) as(O) several(O) Ashley(B-musical artist) Hutchings(I-musical artist) -related(O) folk(B-music genre) rock(I-music genre) projects(O) ((O) The(B-album) Compleat(I-album) Dancing(I-album) Master(I-album) ,(O) Son(B-album) of(I-album) Morris(I-album) On(I-album) etc(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, event, album, award, song, organization, music genre, country, location, band, person, musical instrument and O.\nSentence: Some of Mattacks ' most notable participation in studio recordings in the late 1970s are the work on art rock studio albums by Brian Eno ( Before and After Science ) and 801 ' s Listen Now , as well as several Ashley Hutchings -related folk rock projects ( The Compleat Dancing Master , Son of Morris On etc .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","of","Mattacks","'","most","notable","participation","in","studio","recordings","in","the","late","1970s","are","the","work","on","art","rock","studio","albums","by","Brian","Eno","(","Before","and","After","Science",")","and","801","'","s","Listen","Now",",","as","well","as","several","Ashley","Hutchings","-related","folk","rock","projects","(","The","Compleat","Dancing","Master",",","Son","of","Morris","On","etc","."],"labels":["O","O","B-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-album","I-album","I-album","I-album","O","O","B-band","O","O","B-album","I-album","O","O","O","O","O","B-musical artist","I-musical artist","O","B-music genre","I-music genre","O","O","B-album","I-album","I-album","I-album","O","B-album","I-album","I-album","I-album","O","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","event","album","award","song","organization","music_genre","country","location","band","person","musical_instrument"]}
{"id":"155","dataset":"crossner_music","split":"test","instance":{"id":"155","prompt_labels":"Wilson(B-musical artist) 's(O) charity(O) work(O) includes(O) the(O) Susan(B-organization) G.(I-organization) Komen(I-organization) for(I-organization) the(I-organization) Cure(I-organization) Race(B-event) for(I-event) the(I-event) Cure(I-event) ,(O) the(O) American(B-organization) Cancer(I-organization) Society(I-organization) ,(O) St.(B-organization) Jude(I-organization) 's(I-organization) Children(I-organization) 's(I-organization) Research(I-organization) Hospital(I-organization) ,(O) the(O) Easter(B-organization) Seals(I-organization) Foundation(I-organization) ,(O) UNICEF(B-organization) ,(O) The(O) NAACP(B-organization) ,(O) the(O) Cystic(B-organization) Fibrosis(I-organization) Foundation(I-organization) ,(O) the(O) All-Star(B-organization) Network(I-organization) ,(O) and(O) Figure(B-organization) Skaters(I-organization) of(I-organization) Harlem(I-organization) ,(O) a(O) youth(O) organization(O) devoted(O) to(O) helping(O) children(O) towards(O) entering(O) the(O) Olympics(B-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, song, music genre, award, musical instrument, location, musical artist, album, person, event, country, organization and O.\nSentence: Wilson 's charity work includes the Susan G. Komen for the Cure Race for the Cure , the American Cancer Society , St. Jude 's Children 's Research Hospital , the Easter Seals Foundation , UNICEF , The NAACP , the Cystic Fibrosis Foundation , the All-Star Network , and Figure Skaters of Harlem , a youth organization devoted to helping children towards entering the Olympics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Wilson","'s","charity","work","includes","the","Susan","G.","Komen","for","the","Cure","Race","for","the","Cure",",","the","American","Cancer","Society",",","St.","Jude","'s","Children","'s","Research","Hospital",",","the","Easter","Seals","Foundation",",","UNICEF",",","The","NAACP",",","the","Cystic","Fibrosis","Foundation",",","the","All-Star","Network",",","and","Figure","Skaters","of","Harlem",",","a","youth","organization","devoted","to","helping","children","towards","entering","the","Olympics","."],"labels":["B-musical artist","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","B-event","I-event","I-event","I-event","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O","B-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","B-event","O"],"target_index":null,"target_label":null},"label_list":["band","song","music_genre","award","musical_instrument","location","musical_artist","album","person","event","country","organization"]}
{"id":"156","dataset":"crossner_music","split":"test","instance":{"id":"156","prompt_labels":"In(O) 1990(O) ,(O) he(O) appeared(O) on(O) Kool(B-song) Thing(I-song) ,(O) a(O) song(O) by(O) the(O) alternative(B-music genre) rock(I-music genre) band(O) Sonic(B-band) Youth(I-band) ,(O) and(O) along(O) with(O) Flavor(B-musical artist) Flav(I-musical artist) ,(O) he(O) sang(O) on(O) George(B-musical artist) Clinton(I-musical artist) '(O) s(O) song(O) Tweakin(B-song) '(O) ,(O) which(O) appears(O) on(O) his(O) 1989(O) album(O) The(B-album) Cinderella(I-album) Theory(I-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, album, country, musical artist, band, music genre, award, musical instrument, event, location, organization, song and O.\nSentence: In 1990 , he appeared on Kool Thing , a song by the alternative rock band Sonic Youth , and along with Flavor Flav , he sang on George Clinton ' s song Tweakin ' , which appears on his 1989 album The Cinderella Theory .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1990",",","he","appeared","on","Kool","Thing",",","a","song","by","the","alternative","rock","band","Sonic","Youth",",","and","along","with","Flavor","Flav",",","he","sang","on","George","Clinton","'","s","song","Tweakin","'",",","which","appears","on","his","1989","album","The","Cinderella","Theory","."],"labels":["O","O","O","O","O","O","B-song","I-song","O","O","O","O","O","B-music genre","I-music genre","O","B-band","I-band","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","B-musical artist","I-musical artist","O","O","O","B-song","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O"],"target_index":null,"target_label":null},"label_list":["person","album","country","musical_artist","band","music_genre","award","musical_instrument","event","location","organization","song"]}
{"id":"157","dataset":"crossner_music","split":"test","instance":{"id":"157","prompt_labels":"After(O) Deftones(B-band) '(O) third(O) album(O) White(B-album) Pony(I-album) ,(O) subsequent(O) releases(O) would(O) be(O) written(O) with(O) seven-strings(B-musical instrument) ,(O) until(O) 2010(O) 's(O) Diamond(B-album) Eyes(I-album) and(O) 2012(O) 's(O) Koi(B-album) No(I-album) Yokan(I-album) ,(O) which(O) were(O) written(O) with(O) an(O) eight-string(B-musical instrument) guitar(I-musical instrument) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical artist, country, location, musical instrument, music genre, album, song, award, person, event, organization and O.\nSentence: After Deftones ' third album White Pony , subsequent releases would be written with seven-strings , until 2010 's Diamond Eyes and 2012 's Koi No Yokan , which were written with an eight-string guitar .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","Deftones","'","third","album","White","Pony",",","subsequent","releases","would","be","written","with","seven-strings",",","until","2010","'s","Diamond","Eyes","and","2012","'s","Koi","No","Yokan",",","which","were","written","with","an","eight-string","guitar","."],"labels":["O","B-band","O","O","O","B-album","I-album","O","O","O","O","O","O","O","B-musical instrument","O","O","O","O","B-album","I-album","O","O","O","B-album","I-album","I-album","O","O","O","O","O","O","B-musical instrument","I-musical instrument","O"],"target_index":null,"target_label":null},"label_list":["band","musical_artist","country","location","musical_instrument","music_genre","album","song","award","person","event","organization"]}
{"id":"158","dataset":"crossner_music","split":"test","instance":{"id":"158","prompt_labels":"The(O) four-piece(O) Indie(B-music genre) rock(I-music genre) band(O) played(O) gigs(O) at(O) pubs(O) and(O) festivals(O) from(O) 2005(O) to(O) 2007(O) such(O) as(O) Knitting(B-location) Factory(I-location) ,(O) Bamboozle(B-location) Left(I-location) ,(O) The(B-location) Roxy(I-location) ,(O) Spaceland(B-location) ,(O) and(O) The(B-location) Viper(I-location) Room(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, person, award, band, country, song, organization, music genre, event, album, musical artist, location and O.\nSentence: The four-piece Indie rock band played gigs at pubs and festivals from 2005 to 2007 such as Knitting Factory , Bamboozle Left , The Roxy , Spaceland , and The Viper Room .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","four-piece","Indie","rock","band","played","gigs","at","pubs","and","festivals","from","2005","to","2007","such","as","Knitting","Factory",",","Bamboozle","Left",",","The","Roxy",",","Spaceland",",","and","The","Viper","Room","."],"labels":["O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","I-location","O","B-location","I-location","O","B-location","O","O","B-location","I-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","person","award","band","country","song","organization","music_genre","event","album","musical_artist","location"]}
{"id":"159","dataset":"crossner_music","split":"test","instance":{"id":"159","prompt_labels":"His(O) mix(O) of(O) traditional(O) Senegalese(O) mbalax(B-music genre) with(O) eclectic(B-music genre) influences(O) ranging(O) from(O) Cuban(B-music genre) rumba(I-music genre) to(O) Hip(B-music genre) hop(I-music genre) music(I-music genre) ,(O) jazz(B-music genre) ,(O) and(O) Soul(B-music genre) music(I-music genre) won(O) him(O) an(O) international(O) fan(O) base(O) of(O) millions(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, location, music genre, band, organization, album, musical instrument, event, award, country, song, person and O.\nSentence: His mix of traditional Senegalese mbalax with eclectic influences ranging from Cuban rumba to Hip hop music , jazz , and Soul music won him an international fan base of millions .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","mix","of","traditional","Senegalese","mbalax","with","eclectic","influences","ranging","from","Cuban","rumba","to","Hip","hop","music",",","jazz",",","and","Soul","music","won","him","an","international","fan","base","of","millions","."],"labels":["O","O","O","O","O","B-music genre","O","B-music genre","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","location","music_genre","band","organization","album","musical_instrument","event","award","country","song","person"]}
{"id":"160","dataset":"crossner_music","split":"test","instance":{"id":"160","prompt_labels":"This(O) list(O) includes(O) artists(O) that(O) perform(O) in(O) traditional(O) gospel(B-music genre) music(I-music genre) genres(O) such(O) as(O) Southern(B-music genre) gospel(I-music genre) ,(O) traditional(B-music genre) black(I-music genre) gospel(I-music genre) ,(O) progressive(B-music genre) Southern(I-music genre) gospel(I-music genre) ,(O) urban(B-music genre) contemporary(I-music genre) gospel(I-music genre) ,(O) gospel(B-music genre) blues(I-music genre) ,(O) Christian(B-music genre) country(I-music genre) music(I-music genre) ,(O) Celtic(B-music genre) gospel(I-music genre) and(O) British(B-music genre) black(I-music genre) gospel(I-music genre) as(O) well(O) as(O) artists(O) in(O) the(O) general(O) market(O) who(O) have(O) recorded(O) music(O) in(O) these(O) genres(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, music genre, album, organization, person, location, musical instrument, band, country, song, event, award and O.\nSentence: This list includes artists that perform in traditional gospel music genres such as Southern gospel , traditional black gospel , progressive Southern gospel , urban contemporary gospel , gospel blues , Christian country music , Celtic gospel and British black gospel as well as artists in the general market who have recorded music in these genres .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","list","includes","artists","that","perform","in","traditional","gospel","music","genres","such","as","Southern","gospel",",","traditional","black","gospel",",","progressive","Southern","gospel",",","urban","contemporary","gospel",",","gospel","blues",",","Christian","country","music",",","Celtic","gospel","and","British","black","gospel","as","well","as","artists","in","the","general","market","who","have","recorded","music","in","these","genres","."],"labels":["O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","music_genre","album","organization","person","location","musical_instrument","band","country","song","event","award"]}
{"id":"161","dataset":"crossner_music","split":"test","instance":{"id":"161","prompt_labels":"Dave(B-musical artist) Bush(I-musical artist) joined(O) on(O) keyboards(O) for(O) 1992(O) 's(O) Code(B-album) :(I-album) Selfish(I-album) ,(O) followed(O) by(O) the(O) band(O) 's(O) return(O) to(O) an(O) independent(O) record(O) label(O) for(O) The(B-album) Infotainment(I-album) Scan(I-album) ((O) 1993(O) )(O) ,(O) Middle(B-album) Class(I-album) Revolt(I-album) ((O) 1994(O) )(O) ,(O) and(O) Cerebral(B-album) Caustic(I-album) ((O) 1995(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, musical instrument, organization, person, album, country, song, band, music genre, musical artist, award and O.\nSentence: Dave Bush joined on keyboards for 1992 's Code : Selfish , followed by the band 's return to an independent record label for The Infotainment Scan ( 1993 ) , Middle Class Revolt ( 1994 ) , and Cerebral Caustic ( 1995 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Dave","Bush","joined","on","keyboards","for","1992","'s","Code",":","Selfish",",","followed","by","the","band","'s","return","to","an","independent","record","label","for","The","Infotainment","Scan","(","1993",")",",","Middle","Class","Revolt","(","1994",")",",","and","Cerebral","Caustic","(","1995",")","."],"labels":["B-musical artist","I-musical artist","O","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","O","O","O","O","O","B-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","location","musical_instrument","organization","person","album","country","song","band","music_genre","musical_artist","award"]}
{"id":"162","dataset":"crossner_music","split":"test","instance":{"id":"162","prompt_labels":"The(O) Nordic(B-band) Choir(I-band) has(O) also(O) appeared(O) throughout(O) the(O) United(B-country) States(I-country) ,(O) performing(O) in(O) well-known(O) concert(O) halls(O) as(O) Lincoln(B-location) Center(I-location) in(O) New(B-location) York(I-location) and(O) the(O) John(B-location) F.(I-location) Kennedy(I-location) Center(I-location) for(I-location) the(I-location) Performing(I-location) Arts(I-location) in(O) Washington(B-location) ,(I-location) D.C.(I-location) Additionally(O) ,(O) Luther(B-location) College(I-location) has(O) the(O) largest(O) collegiate(O) choral(O) program(O) in(O) the(O) United(B-country) States(I-country) with(O) almost(O) 600(O) student(O) singers(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, song, organization, location, award, band, musical instrument, person, country, event, music genre, album and O.\nSentence: The Nordic Choir has also appeared throughout the United States , performing in well-known concert halls as Lincoln Center in New York and the John F. Kennedy Center for the Performing Arts in Washington , D.C. Additionally , Luther College has the largest collegiate choral program in the United States with almost 600 student singers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Nordic","Choir","has","also","appeared","throughout","the","United","States",",","performing","in","well-known","concert","halls","as","Lincoln","Center","in","New","York","and","the","John","F.","Kennedy","Center","for","the","Performing","Arts","in","Washington",",","D.C.","Additionally",",","Luther","College","has","the","largest","collegiate","choral","program","in","the","United","States","with","almost","600","student","singers","."],"labels":["O","B-band","I-band","O","O","O","O","O","B-country","I-country","O","O","O","O","O","O","O","B-location","I-location","O","B-location","I-location","O","O","B-location","I-location","I-location","I-location","I-location","I-location","I-location","I-location","O","B-location","I-location","I-location","O","O","B-location","I-location","O","O","O","O","O","O","O","O","B-country","I-country","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","song","organization","location","award","band","musical_instrument","person","country","event","music_genre","album"]}
{"id":"164","dataset":"crossner_music","split":"test","instance":{"id":"164","prompt_labels":"Australian(B-organization) Film(I-organization) Institute(I-organization) ,(O) Blockbuster(B-award) Entertainment(I-award) Awards(I-award) ,(O) Empire(B-award) Awards(I-award) ,(O) Hollywood(B-event) Film(I-event) Festival(I-event) ,(O) London(B-organization) Film(I-organization) Critics(I-organization) '(I-organization) Circle(I-organization) ,(O) Russian(B-organization) Guild(I-organization) of(I-organization) Film(I-organization) Critics(I-organization) ,(O) Satellite(B-award) Awards(I-award) ,(O) and(O) Southeastern(B-organization) Film(I-organization) Critics(I-organization) Association(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, award, location, song, musical instrument, musical artist, country, event, person, album, music genre, organization and O.\nSentence: Australian Film Institute , Blockbuster Entertainment Awards , Empire Awards , Hollywood Film Festival , London Film Critics ' Circle , Russian Guild of Film Critics , Satellite Awards , and Southeastern Film Critics Association .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Australian","Film","Institute",",","Blockbuster","Entertainment","Awards",",","Empire","Awards",",","Hollywood","Film","Festival",",","London","Film","Critics","'","Circle",",","Russian","Guild","of","Film","Critics",",","Satellite","Awards",",","and","Southeastern","Film","Critics","Association","."],"labels":["B-organization","I-organization","I-organization","O","B-award","I-award","I-award","O","B-award","I-award","O","B-event","I-event","I-event","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-award","I-award","O","O","B-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["band","award","location","song","musical_instrument","musical_artist","country","event","person","album","music_genre","organization"]}
{"id":"165","dataset":"crossner_music","split":"test","instance":{"id":"165","prompt_labels":"This(O) was(O) the(O) instrument(O) of(O) the(O) early(O) jazz(B-music genre) great(O) Johnny(B-musical artist) St.(I-musical artist) Cyr(I-musical artist) ,(O) jazzmen(O) Django(B-musical artist) Reinhardt(I-musical artist) ,(O) Danny(B-musical artist) Barker(I-musical artist) ,(O) Papa(B-musical artist) Charlie(I-musical artist) Jackson(I-musical artist) and(O) Clancy(B-musical artist) Hayes(I-musical artist) ,(O) as(O) well(O) as(O) the(O) blues(B-music genre) and(O) gospel(B-music genre) singer(O) The(O) Reverend(B-musical artist) Gary(I-musical artist) Davis(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, band, song, country, event, location, person, organization, award, musical instrument, album, music genre and O.\nSentence: This was the instrument of the early jazz great Johnny St. Cyr , jazzmen Django Reinhardt , Danny Barker , Papa Charlie Jackson and Clancy Hayes , as well as the blues and gospel singer The Reverend Gary Davis .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","was","the","instrument","of","the","early","jazz","great","Johnny","St.","Cyr",",","jazzmen","Django","Reinhardt",",","Danny","Barker",",","Papa","Charlie","Jackson","and","Clancy","Hayes",",","as","well","as","the","blues","and","gospel","singer","The","Reverend","Gary","Davis","."],"labels":["O","O","O","O","O","O","O","B-music genre","O","B-musical artist","I-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O","O","B-music genre","O","B-music genre","O","O","B-musical artist","I-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","band","song","country","event","location","person","organization","award","musical_instrument","album","music_genre"]}
{"id":"166","dataset":"crossner_music","split":"test","instance":{"id":"166","prompt_labels":"For(O) portraying(O) both(O) gunfighter(O) Kid(B-person) Shelleen(I-person) and(O) criminal(O) Tim(B-person) Strawn(I-person) ,(O) he(O) won(O) the(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) ,(O) along(O) with(O) a(O) BAFTA(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) in(I-award) a(I-award) Leading(I-award) Role(I-award) ,(O) a(O) Golden(B-award) Globe(I-award) Award(I-award) ,(O) an(O) National(B-award) Board(I-award) of(I-award) Review(I-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) ,(O) and(O) the(O) Silver(B-award) Bear(I-award) for(I-award) Best(I-award) Actor(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, music genre, musical artist, album, person, event, band, organization, location, musical instrument, song and O.\nSentence: For portraying both gunfighter Kid Shelleen and criminal Tim Strawn , he won the Academy Award for Best Actor , along with a BAFTA Award for Best Actor in a Leading Role , a Golden Globe Award , an National Board of Review Award for Best Actor , and the Silver Bear for Best Actor .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","portraying","both","gunfighter","Kid","Shelleen","and","criminal","Tim","Strawn",",","he","won","the","Academy","Award","for","Best","Actor",",","along","with","a","BAFTA","Award","for","Best","Actor","in","a","Leading","Role",",","a","Golden","Globe","Award",",","an","National","Board","of","Review","Award","for","Best","Actor",",","and","the","Silver","Bear","for","Best","Actor","."],"labels":["O","O","O","O","B-person","I-person","O","O","B-person","I-person","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["country","award","music_genre","musical_artist","album","person","event","band","organization","location","musical_instrument","song"]}
{"id":"171","dataset":"crossner_music","split":"test","instance":{"id":"171","prompt_labels":"Many(O) early(O) rock(B-music genre) and(I-music genre) roll(I-music genre) songs(O) are(O) based(O) on(O) blues(B-music genre) :(O) That(B-song) 's(I-song) All(I-song) Right(I-song) Mama(I-song) ,(O) Johnny(B-song) B.(I-song) Goode(I-song) ,(O) Blue(B-song) Suede(I-song) Shoes(I-song) ,(O) Whole(B-song) Lotta(I-song) Shakin(I-song) '(I-song) Goin(I-song) On(I-song) ,(O) Shake(B-song) ,(I-song) Rattle(I-song) ,(I-song) and(I-song) Roll(I-song) ,(O) and(O) Long(B-song) Tall(I-song) Sally(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, country, organization, event, album, band, music genre, musical instrument, person, musical artist, location, award and O.\nSentence: Many early rock and roll songs are based on blues : That 's All Right Mama , Johnny B. Goode , Blue Suede Shoes , Whole Lotta Shakin ' Goin On , Shake , Rattle , and Roll , and Long Tall Sally .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Many","early","rock","and","roll","songs","are","based","on","blues",":","That","'s","All","Right","Mama",",","Johnny","B.","Goode",",","Blue","Suede","Shoes",",","Whole","Lotta","Shakin","'","Goin","On",",","Shake",",","Rattle",",","and","Roll",",","and","Long","Tall","Sally","."],"labels":["O","O","B-music genre","I-music genre","I-music genre","O","O","O","O","B-music genre","O","B-song","I-song","I-song","I-song","I-song","O","B-song","I-song","I-song","O","B-song","I-song","I-song","O","B-song","I-song","I-song","I-song","I-song","I-song","O","B-song","I-song","I-song","I-song","I-song","I-song","O","O","B-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["song","country","organization","event","album","band","music_genre","musical_instrument","person","musical_artist","location","award"]}
{"id":"174","dataset":"crossner_music","split":"test","instance":{"id":"174","prompt_labels":"Hee(O) Haw(O) featured(O) a(O) premiere(O) showcase(O) on(O) commercial(O) television(O) throughout(O) its(O) run(O) for(O) Country(B-music genre) music(I-music genre) ,(O) Bluegrass(B-music genre) music(I-music genre) ,(O) Gospel(B-music genre) music(I-music genre) ,(O) and(O) other(O) styles(O) of(O) American(B-music genre) traditional(I-music genre) music(I-music genre) ,(O) featuring(O) hundreds(O) of(O) elite(O) musical(O) performances(O) that(O) were(O) paramount(O) to(O) the(O) success(O) ,(O) popularity(O) and(O) legacy(O) of(O) the(O) series(O) for(O) a(O) broad(O) audience(O) of(O) Southern(B-music genre) ,(O) rural(B-music genre) and(O) purely(B-music genre) music(I-music genre) fans(O) alike(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, location, organization, musical instrument, person, song, album, country, musical artist, band, event, award and O.\nSentence: Hee Haw featured a premiere showcase on commercial television throughout its run for Country music , Bluegrass music , Gospel music , and other styles of American traditional music , featuring hundreds of elite musical performances that were paramount to the success , popularity and legacy of the series for a broad audience of Southern , rural and purely music fans alike .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Hee","Haw","featured","a","premiere","showcase","on","commercial","television","throughout","its","run","for","Country","music",",","Bluegrass","music",",","Gospel","music",",","and","other","styles","of","American","traditional","music",",","featuring","hundreds","of","elite","musical","performances","that","were","paramount","to","the","success",",","popularity","and","legacy","of","the","series","for","a","broad","audience","of","Southern",",","rural","and","purely","music","fans","alike","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O","O","O","O","B-music genre","I-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","O","B-music genre","O","B-music genre","I-music genre","O","O","O"],"target_index":null,"target_label":null},"label_list":["music_genre","location","organization","musical_instrument","person","song","album","country","musical_artist","band","event","award"]}
{"id":"176","dataset":"crossner_music","split":"test","instance":{"id":"176","prompt_labels":"The(O) Pixies(B-band) ,(O) minus(O) original(O) bassist(O) Kim(B-musical artist) Deal(I-musical artist) ,(O) reunited(O) for(O) a(O) United(B-country) States(I-country) and(O) world(O) tour(O) in(O) 2014(O) and(O) have(O) subsequently(O) released(O) three(O) additional(O) studio(O) albums(O) :(O) Indie(B-album) Cindy(I-album) ((O) 2014(O) )(O) ,(O) Head(B-album) Carrier(I-album) ((O) 2016(O) )(O) and(O) Beneath(B-album) the(I-album) Eyrie(I-album) ((O) 2019(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, musical instrument, country, song, location, music genre, band, musical artist, organization, event, award, album and O.\nSentence: The Pixies , minus original bassist Kim Deal , reunited for a United States and world tour in 2014 and have subsequently released three additional studio albums : Indie Cindy ( 2014 ) , Head Carrier ( 2016 ) and Beneath the Eyrie ( 2019 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Pixies",",","minus","original","bassist","Kim","Deal",",","reunited","for","a","United","States","and","world","tour","in","2014","and","have","subsequently","released","three","additional","studio","albums",":","Indie","Cindy","(","2014",")",",","Head","Carrier","(","2016",")","and","Beneath","the","Eyrie","(","2019",")","."],"labels":["O","B-band","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","B-country","I-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","O","O","O","O","B-album","I-album","O","O","O","O","B-album","I-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","musical_instrument","country","song","location","music_genre","band","musical_artist","organization","event","award","album"]}
{"id":"178","dataset":"crossner_music","split":"test","instance":{"id":"178","prompt_labels":"He(O) traveled(O) to(O) Africa(B-location) in(O) 1973(O) ,(O) where(O) he(O) visited(O) Ethiopia(B-country) ,(O) Kenya(B-country) ,(O) Tanzania(B-country) ,(O) Malawi(B-country) ,(O) and(O) South(B-country) Africa(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, event, organization, band, location, musical instrument, award, person, country, song, music genre, album and O.\nSentence: He traveled to Africa in 1973 , where he visited Ethiopia , Kenya , Tanzania , Malawi , and South Africa .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","traveled","to","Africa","in","1973",",","where","he","visited","Ethiopia",",","Kenya",",","Tanzania",",","Malawi",",","and","South","Africa","."],"labels":["O","O","O","B-location","O","O","O","O","O","O","B-country","O","B-country","O","B-country","O","B-country","O","O","B-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","event","organization","band","location","musical_instrument","award","person","country","song","music_genre","album"]}
{"id":"179","dataset":"crossner_music","split":"test","instance":{"id":"179","prompt_labels":"Particularly(O) noted(O) in(O) the(O) UK(B-country) are(O) the(O) Music(B-location) of(I-location) Newport(I-location) ,(O) once(O) labelled(O) '(O) the(O) new(B-location) Seattle(I-location) '(O) ,(O) and(O) the(O) Music(B-location) of(I-location) Cardiff(I-location) ,(O) for(O) which(O) the(O) city(O) has(O) recently(O) been(O) labelled(O) '(O) Music(B-location) City(I-location) '(O) ,(O) for(O) having(O) the(O) second(O) highest(O) number(O) of(O) independent(O) music(O) venues(O) in(O) the(O) UK(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, event, song, music genre, country, musical artist, organization, award, person, location, band, album and O.\nSentence: Particularly noted in the UK are the Music of Newport , once labelled ' the new Seattle ' , and the Music of Cardiff , for which the city has recently been labelled ' Music City ' , for having the second highest number of independent music venues in the UK .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Particularly","noted","in","the","UK","are","the","Music","of","Newport",",","once","labelled","'","the","new","Seattle","'",",","and","the","Music","of","Cardiff",",","for","which","the","city","has","recently","been","labelled","'","Music","City","'",",","for","having","the","second","highest","number","of","independent","music","venues","in","the","UK","."],"labels":["O","O","O","O","B-country","O","O","B-location","I-location","I-location","O","O","O","O","O","B-location","I-location","O","O","O","O","B-location","I-location","I-location","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","event","song","music_genre","country","musical_artist","organization","award","person","location","band","album"]}
{"id":"181","dataset":"crossner_music","split":"test","instance":{"id":"181","prompt_labels":"In(O) 2001(O) ,(O) having(O) previously(O) won(O) an(O) Emmy(B-award) Award(I-award) ,(O) a(O) Grammy(B-award) Award(I-award) and(O) an(O) Academy(B-award) Awards(I-award) ,(O) he(O) joined(O) a(O) small(O) list(O) of(O) EGOT(B-award) winners(O) with(O) his(O) Tony(B-award) Award(I-award) wins(O) for(O) The(O) Producers(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, musical instrument, album, organization, award, band, country, location, music genre, song, event, musical artist and O.\nSentence: In 2001 , having previously won an Emmy Award , a Grammy Award and an Academy Awards , he joined a small list of EGOT winners with his Tony Award wins for The Producers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2001",",","having","previously","won","an","Emmy","Award",",","a","Grammy","Award","and","an","Academy","Awards",",","he","joined","a","small","list","of","EGOT","winners","with","his","Tony","Award","wins","for","The","Producers","."],"labels":["O","O","O","O","O","O","O","B-award","I-award","O","O","B-award","I-award","O","O","B-award","I-award","O","O","O","O","O","O","O","B-award","O","O","O","B-award","I-award","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","musical_instrument","album","organization","award","band","country","location","music_genre","song","event","musical_artist"]}
{"id":"183","dataset":"crossner_music","split":"test","instance":{"id":"183","prompt_labels":"In(O) addition(O) to(O) Lady(B-band) Antebellum(I-band) ,(O) groups(O) such(O) as(O) Herrick(B-band) ,(O) The(B-band) Quebe(I-band) Sisters(I-band) Band(I-band) ,(O) Little(B-band) Big(I-band) Town(I-band) ,(O) The(B-band) Band(I-band) Perry(I-band) ,(O) Gloriana(B-band) ,(O) Thompson(B-band) Square(I-band) ,(O) Eli(B-band) Young(I-band) Band(I-band) ,(O) Zac(B-band) Brown(I-band) Band(I-band) and(O) British(O) duo(O) The(B-band) Shires(I-band) have(O) emerged(O) to(O) occupy(O) a(O) large(O) portion(O) of(O) the(O) new(O) country(O) artists(O) in(O) the(O) popular(O) scene(O) along(O) with(O) solo(O) singers(O) Kacey(B-musical artist) Musgraves(I-musical artist) and(O) Miranda(B-musical artist) Lambert(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, location, album, country, song, musical artist, event, person, award, music genre, organization, musical instrument and O.\nSentence: In addition to Lady Antebellum , groups such as Herrick , The Quebe Sisters Band , Little Big Town , The Band Perry , Gloriana , Thompson Square , Eli Young Band , Zac Brown Band and British duo The Shires have emerged to occupy a large portion of the new country artists in the popular scene along with solo singers Kacey Musgraves and Miranda Lambert .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","addition","to","Lady","Antebellum",",","groups","such","as","Herrick",",","The","Quebe","Sisters","Band",",","Little","Big","Town",",","The","Band","Perry",",","Gloriana",",","Thompson","Square",",","Eli","Young","Band",",","Zac","Brown","Band","and","British","duo","The","Shires","have","emerged","to","occupy","a","large","portion","of","the","new","country","artists","in","the","popular","scene","along","with","solo","singers","Kacey","Musgraves","and","Miranda","Lambert","."],"labels":["O","O","O","B-band","I-band","O","O","O","O","B-band","O","B-band","I-band","I-band","I-band","O","B-band","I-band","I-band","O","B-band","I-band","I-band","O","B-band","O","B-band","I-band","O","B-band","I-band","I-band","O","B-band","I-band","I-band","O","O","O","B-band","I-band","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["band","location","album","country","song","musical_artist","event","person","award","music_genre","organization","musical_instrument"]}
{"id":"184","dataset":"crossner_music","split":"test","instance":{"id":"184","prompt_labels":"Organised(O) by(O) the(O) European(B-organization) Broadcasting(I-organization) Union(I-organization) ((O) EBU(B-organization) )(O) and(O) host(O) broadcaster(O) Latvijas(B-organization) Televzija(I-organization) ((O) LTV(B-organization) )(O) ,(O) the(O) contest(O) was(O) held(O) at(O) the(O) Skonto(B-location) Hall(I-location) ,(O) with(O) the(O) final(O) on(O) 24(O) May(O) 2003(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, musical artist, album, country, award, song, organization, band, musical instrument, event, person, music genre and O.\nSentence: Organised by the European Broadcasting Union ( EBU ) and host broadcaster Latvijas Televzija ( LTV ) , the contest was held at the Skonto Hall , with the final on 24 May 2003 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Organised","by","the","European","Broadcasting","Union","(","EBU",")","and","host","broadcaster","Latvijas","Televzija","(","LTV",")",",","the","contest","was","held","at","the","Skonto","Hall",",","with","the","final","on","24","May","2003","."],"labels":["O","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","O","B-organization","I-organization","O","B-organization","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","musical_artist","album","country","award","song","organization","band","musical_instrument","event","person","music_genre"]}
{"id":"185","dataset":"crossner_music","split":"test","instance":{"id":"185","prompt_labels":"In(O) February(O) 1969(O) ,(O) RCA(B-organization) released(O) the(O) live(O) album(O) Bless(B-album) Its(I-album) Pointed(I-album) Little(I-album) Head(I-album) ,(O) which(O) was(O) culled(O) from(O) 1968(O) performances(O) at(O) the(O) Fillmore(B-location) West(I-location) on(O) October(O) 24-26(O) and(O) the(O) Fillmore(B-location) East(I-location) on(O) November(O) 28-30(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, award, music genre, country, album, musical artist, organization, band, event, song, musical instrument and O.\nSentence: In February 1969 , RCA released the live album Bless Its Pointed Little Head , which was culled from 1968 performances at the Fillmore West on October 24-26 and the Fillmore East on November 28-30 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","February","1969",",","RCA","released","the","live","album","Bless","Its","Pointed","Little","Head",",","which","was","culled","from","1968","performances","at","the","Fillmore","West","on","October","24-26","and","the","Fillmore","East","on","November","28-30","."],"labels":["O","O","O","O","B-organization","O","O","O","O","B-album","I-album","I-album","I-album","I-album","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","B-location","I-location","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","person","award","music_genre","country","album","musical_artist","organization","band","event","song","musical_instrument"]}
{"id":"186","dataset":"crossner_music","split":"test","instance":{"id":"186","prompt_labels":"Seal(B-musical artist) has(O) won(O) multiple(O) awards(O) throughout(O) his(O) career(O) ,(O) including(O) three(O) Brit(B-award) Awards(I-award) ;(O) he(O) won(O) Brit(B-award) Award(I-award) for(I-award) British(I-award) Male(I-award) Solo(I-award) Artist(I-award) in(O) 1992(O) ,(O) as(O) well(O) as(O) four(O) Grammy(B-award) Award(I-award) s(O) and(O) an(O) MTV(B-award) Video(I-award) Music(I-award) Award(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, location, country, music genre, musical instrument, organization, musical artist, song, person, award, album, event and O.\nSentence: Seal has won multiple awards throughout his career , including three Brit Awards ; he won Brit Award for British Male Solo Artist in 1992 , as well as four Grammy Award s and an MTV Video Music Award .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Seal","has","won","multiple","awards","throughout","his","career",",","including","three","Brit","Awards",";","he","won","Brit","Award","for","British","Male","Solo","Artist","in","1992",",","as","well","as","four","Grammy","Award","s","and","an","MTV","Video","Music","Award","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","B-award","I-award","O","O","O","B-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["band","location","country","music_genre","musical_instrument","organization","musical_artist","song","person","award","album","event"]}
{"id":"187","dataset":"crossner_music","split":"test","instance":{"id":"187","prompt_labels":"Four(O) demos(O) for(O) the(O) album(O) were(O) recorded(O) on(O) August(O) 13(O) ,(O) 1990(O) ;(O) Enter(B-song) Sandman(I-song) ,(O) The(B-song) Unforgiven(I-song) ,(O) Nothing(B-song) Else(I-song) Matters(I-song) and(O) Wherever(B-song) I(I-song) May(I-song) Roam(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, musical instrument, person, music genre, band, country, event, location, album, musical artist, song, organization and O.\nSentence: Four demos for the album were recorded on August 13 , 1990 ; Enter Sandman , The Unforgiven , Nothing Else Matters and Wherever I May Roam .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Four","demos","for","the","album","were","recorded","on","August","13",",","1990",";","Enter","Sandman",",","The","Unforgiven",",","Nothing","Else","Matters","and","Wherever","I","May","Roam","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-song","I-song","O","B-song","I-song","O","B-song","I-song","I-song","O","B-song","I-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["award","musical_instrument","person","music_genre","band","country","event","location","album","musical_artist","song","organization"]}
{"id":"188","dataset":"crossner_music","split":"test","instance":{"id":"188","prompt_labels":"In(O) 2006(O) ,(O) along(O) with(O) Sonu(B-musical artist) Nigam(I-musical artist) ,(O) Sunidhi(B-musical artist) Chauhan(I-musical artist) and(O) Shiamak(B-musical artist) Davar(I-musical artist) ,(O) Ghoshal(B-musical artist) performed(O) the(O) theme(O) song(O) of(O) 2010(B-event) Commonwealth(I-event) Games(I-event) at(O) its(O) closing(O) ceremony(O) ,(O) as(O) an(O) invitation(O) to(O) everyone(O) to(O) the(O) following(O) Commonwealth(B-event) Games(I-event) in(O) Delhi(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, album, location, award, event, musical instrument, country, organization, music genre, person, band, song and O.\nSentence: In 2006 , along with Sonu Nigam , Sunidhi Chauhan and Shiamak Davar , Ghoshal performed the theme song of 2010 Commonwealth Games at its closing ceremony , as an invitation to everyone to the following Commonwealth Games in Delhi .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2006",",","along","with","Sonu","Nigam",",","Sunidhi","Chauhan","and","Shiamak","Davar",",","Ghoshal","performed","the","theme","song","of","2010","Commonwealth","Games","at","its","closing","ceremony",",","as","an","invitation","to","everyone","to","the","following","Commonwealth","Games","in","Delhi","."],"labels":["O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","album","location","award","event","musical_instrument","country","organization","music_genre","person","band","song"]}
{"id":"191","dataset":"crossner_music","split":"test","instance":{"id":"191","prompt_labels":"He(O) is(O) also(O) a(O) recipient(O) of(O) the(O) AFI(B-award) Life(I-award) Achievement(I-award) Award(I-award) for(O) his(O) contributions(O) to(O) the(O) cinema(O) ,(O) and(O) has(O) won(O) an(O) Academy(B-award) Awards(I-award) ,(O) a(O) Palme(B-award) d(I-award) 'Or(I-award) ,(O) Cannes(B-award) Film(I-award) Festival(I-award) Best(I-award) Director(I-award) Award(I-award) ,(O) Silver(B-award) Lion(I-award) ,(O) Grammy(B-award) Award(I-award) ,(O) Emmy(B-award) Award(I-award) ,(O) Golden(B-award) Globes(I-award) ,(O) British(B-award) Academy(I-award) Film(I-award) Awards(I-award) ,(O) and(O) Directors(B-award) Guild(I-award) of(I-award) America(I-award) Award(I-award) s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, album, event, organization, musical instrument, song, music genre, musical artist, band, country, person and O.\nSentence: He is also a recipient of the AFI Life Achievement Award for his contributions to the cinema , and has won an Academy Awards , a Palme d 'Or , Cannes Film Festival Best Director Award , Silver Lion , Grammy Award , Emmy Award , Golden Globes , British Academy Film Awards , and Directors Guild of America Award s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","is","also","a","recipient","of","the","AFI","Life","Achievement","Award","for","his","contributions","to","the","cinema",",","and","has","won","an","Academy","Awards",",","a","Palme","d","'Or",",","Cannes","Film","Festival","Best","Director","Award",",","Silver","Lion",",","Grammy","Award",",","Emmy","Award",",","Golden","Globes",",","British","Academy","Film","Awards",",","and","Directors","Guild","of","America","Award","s","."],"labels":["O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","O","B-award","I-award","O","B-award","I-award","O","B-award","I-award","O","B-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O","O"],"target_index":null,"target_label":null},"label_list":["location","award","album","event","organization","musical_instrument","song","music_genre","musical_artist","band","country","person"]}
{"id":"194","dataset":"crossner_music","split":"test","instance":{"id":"194","prompt_labels":"Bubbles(B-person) created(O) sleeves(O) for(O) bands(O) including(O) the(B-band) Damned(I-band) ,(O) Elvis(B-musical artist) Costello(I-musical artist) ,(O) Ian(B-musical artist) Dury(I-musical artist) and(O) Wreckless(B-musical artist) Eric(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical instrument, music genre, location, musical artist, album, song, band, country, award, person, organization and O.\nSentence: Bubbles created sleeves for bands including the Damned , Elvis Costello , Ian Dury and Wreckless Eric .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Bubbles","created","sleeves","for","bands","including","the","Damned",",","Elvis","Costello",",","Ian","Dury","and","Wreckless","Eric","."],"labels":["B-person","O","O","O","O","O","B-band","I-band","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["event","musical_instrument","music_genre","location","musical_artist","album","song","band","country","award","person","organization"]}
{"id":"196","dataset":"crossner_music","split":"test","instance":{"id":"196","prompt_labels":"Jagger(B-musical artist) ,(O) in(O) the(O) role(O) of(O) Turner(B-person) in(O) the(O) 1970(O) film(O) Performance(O) ,(O) performed(O) excerpts(O) from(O) Come(B-song) On(I-song) in(I-song) My(I-song) Kitchen(I-song) and(O) Me(B-song) and(I-song) the(I-song) Devil(I-song) Blues(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, organization, location, album, musical instrument, person, song, musical artist, event, country, music genre, award and O.\nSentence: Jagger , in the role of Turner in the 1970 film Performance , performed excerpts from Come On in My Kitchen and Me and the Devil Blues .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Jagger",",","in","the","role","of","Turner","in","the","1970","film","Performance",",","performed","excerpts","from","Come","On","in","My","Kitchen","and","Me","and","the","Devil","Blues","."],"labels":["B-musical artist","O","O","O","O","O","B-person","O","O","O","O","O","O","O","O","O","B-song","I-song","I-song","I-song","I-song","O","B-song","I-song","I-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["band","organization","location","album","musical_instrument","person","song","musical_artist","event","country","music_genre","award"]}
{"id":"200","dataset":"crossner_music","split":"test","instance":{"id":"200","prompt_labels":"Additionally(O) ,(O) Grant(B-person) won(O) the(O) Golden(B-award) Globe(I-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) -(I-award) Motion(I-award) Picture(I-award) Musical(I-award) or(I-award) Comedy(I-award) and(O) the(O) BAFTA(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) in(I-award) a(I-award) Leading(I-award) Role(I-award) ,(O) and(O) the(O) film(O) won(O) the(O) British(B-award) Academy(I-award) Film(I-award) Awards(I-award) BAFTA(B-award) Award(I-award) for(I-award) Best(I-award) Film(I-award) ,(O) BAFTA(B-award) Award(I-award) for(I-award) Best(I-award) Direction(I-award) ,(O) and(O) BAFTA(B-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) in(I-award) a(I-award) Supporting(I-award) Role(I-award) for(O) Scott(B-person) Thomas(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, event, band, person, musical artist, country, musical instrument, song, organization, album, award, location and O.\nSentence: Additionally , Grant won the Golden Globe Award for Best Actor - Motion Picture Musical or Comedy and the BAFTA Award for Best Actor in a Leading Role , and the film won the British Academy Film Awards BAFTA Award for Best Film , BAFTA Award for Best Direction , and BAFTA Award for Best Actress in a Supporting Role for Scott Thomas .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Additionally",",","Grant","won","the","Golden","Globe","Award","for","Best","Actor","-","Motion","Picture","Musical","or","Comedy","and","the","BAFTA","Award","for","Best","Actor","in","a","Leading","Role",",","and","the","film","won","the","British","Academy","Film","Awards","BAFTA","Award","for","Best","Film",",","BAFTA","Award","for","Best","Direction",",","and","BAFTA","Award","for","Best","Actress","in","a","Supporting","Role","for","Scott","Thomas","."],"labels":["O","O","B-person","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","B-award","I-award","I-award","I-award","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["music_genre","event","band","person","musical_artist","country","musical_instrument","song","organization","album","award","location"]}
{"id":"201","dataset":"crossner_music","split":"test","instance":{"id":"201","prompt_labels":"XXXVII(O) No.(O) 32(O) ,(O) pp.(O) 69(O) ,(O) 82(O) )(O) Spaces(O) in(O) Manhattan(B-location) that(O) supported(O) Downtown(B-music genre) music(I-music genre) from(O) the(O) 1960s(O) on(O) included(O) the(O) Judson(B-location) Memorial(I-location) Church(I-location) ,(O) The(B-location) Kitchen(I-location) ,(O) Experimental(B-location) Intermedia(I-location) ,(O) Roulette(B-location) Intermedium(I-location) ,(O) the(O) Knitting(B-location) Factory(I-location) ,(O) Dance(B-location) Theater(I-location) Workshop(I-location) ,(O) Tonic(B-location) ,(O) the(O) Gas(B-location) Station(I-location) ,(O) the(O) Paula(B-location) Cooper(I-location) Gallery(I-location) ,(O) and(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, musical artist, location, album, musical instrument, organization, music genre, band, person, event, country, award and O.\nSentence: XXXVII No. 32 , pp. 69 , 82 ) Spaces in Manhattan that supported Downtown music from the 1960s on included the Judson Memorial Church , The Kitchen , Experimental Intermedia , Roulette Intermedium , the Knitting Factory , Dance Theater Workshop , Tonic , the Gas Station , the Paula Cooper Gallery , and others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["XXXVII","No.","32",",","pp.","69",",","82",")","Spaces","in","Manhattan","that","supported","Downtown","music","from","the","1960s","on","included","the","Judson","Memorial","Church",",","The","Kitchen",",","Experimental","Intermedia",",","Roulette","Intermedium",",","the","Knitting","Factory",",","Dance","Theater","Workshop",",","Tonic",",","the","Gas","Station",",","the","Paula","Cooper","Gallery",",","and","others","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-location","O","O","B-music genre","I-music genre","O","O","O","O","O","O","B-location","I-location","I-location","O","B-location","I-location","O","B-location","I-location","O","B-location","I-location","O","O","B-location","I-location","O","B-location","I-location","I-location","O","B-location","O","O","B-location","I-location","O","O","B-location","I-location","I-location","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["song","musical_artist","location","album","musical_instrument","organization","music_genre","band","person","event","country","award"]}
{"id":"202","dataset":"crossner_music","split":"test","instance":{"id":"202","prompt_labels":"The(O) singles(O) One(B-song) Night(I-song) in(I-song) Bangkok(I-song) ((O) with(O) vocals(O) by(O) Murray(B-musical artist) Head(I-musical artist) and(O) Anders(B-musical artist) Glenmark(I-musical artist) )(O) and(O) I(B-song) Know(I-song) Him(I-song) So(I-song) Well(I-song) ((O) a(O) duet(O) by(O) Barbara(B-musical artist) Dickson(I-musical artist) and(O) Elaine(B-musical artist) Paige(I-musical artist) ,(O) and(O) later(O) also(O) recorded(O) by(O) Barbra(B-musical artist) Streisand(I-musical artist) and(O) Whitney(B-musical artist) Houston(I-musical artist) )(O) were(O) both(O) hugely(O) successful(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, musical instrument, event, album, music genre, song, award, band, person, organization, country, musical artist and O.\nSentence: The singles One Night in Bangkok ( with vocals by Murray Head and Anders Glenmark ) and I Know Him So Well ( a duet by Barbara Dickson and Elaine Paige , and later also recorded by Barbra Streisand and Whitney Houston ) were both hugely successful .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","singles","One","Night","in","Bangkok","(","with","vocals","by","Murray","Head","and","Anders","Glenmark",")","and","I","Know","Him","So","Well","(","a","duet","by","Barbara","Dickson","and","Elaine","Paige",",","and","later","also","recorded","by","Barbra","Streisand","and","Whitney","Houston",")","were","both","hugely","successful","."],"labels":["O","O","B-song","I-song","I-song","I-song","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","B-song","I-song","I-song","I-song","I-song","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","musical_instrument","event","album","music_genre","song","award","band","person","organization","country","musical_artist"]}
{"id":"203","dataset":"crossner_music","split":"test","instance":{"id":"203","prompt_labels":"They(O) took(O) up(O) a(O) long(O) residency(O) at(O) San(B-location) Francisco(I-location) 's(O) Black(B-location) Hawk(I-location) nightclub(O) and(O) gained(O) great(O) popularity(O) touring(O) college(O) campuses(O) ,(O) recording(O) a(O) series(O) of(O) albums(O) with(O) such(O) titles(O) as(O) Jazz(B-album) at(I-album) Oberlin(I-album) ((O) 1953(O) )(O) ,(O) Jazz(B-album) at(I-album) the(I-album) College(I-album) of(I-album) the(I-album) Pacific(I-album) ((O) 1953(O) )(O) ,(O) and(O) Brubeck(B-musical artist) 's(O) debut(O) on(O) Columbia(B-organization) Records(I-organization) ,(O) Jazz(B-album) Goes(I-album) to(I-album) College(I-album) ((O) 1954(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, award, song, country, musical instrument, album, event, organization, music genre, band, location, person and O.\nSentence: They took up a long residency at San Francisco 's Black Hawk nightclub and gained great popularity touring college campuses , recording a series of albums with such titles as Jazz at Oberlin ( 1953 ) , Jazz at the College of the Pacific ( 1953 ) , and Brubeck 's debut on Columbia Records , Jazz Goes to College ( 1954 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["They","took","up","a","long","residency","at","San","Francisco","'s","Black","Hawk","nightclub","and","gained","great","popularity","touring","college","campuses",",","recording","a","series","of","albums","with","such","titles","as","Jazz","at","Oberlin","(","1953",")",",","Jazz","at","the","College","of","the","Pacific","(","1953",")",",","and","Brubeck","'s","debut","on","Columbia","Records",",","Jazz","Goes","to","College","(","1954",")","."],"labels":["O","O","O","O","O","O","O","B-location","I-location","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","I-album","I-album","I-album","I-album","O","O","O","O","O","B-musical artist","O","O","O","B-organization","I-organization","O","B-album","I-album","I-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","award","song","country","musical_instrument","album","event","organization","music_genre","band","location","person"]}
{"id":"204","dataset":"crossner_music","split":"test","instance":{"id":"204","prompt_labels":"Rollins(B-musical artist) left(O) to(O) join(O) them(O) soon(O) after(O) ,(O) leaving(O) Davis(B-musical artist) to(O) pay(O) over(O) $(O) 25,000(O) ((O) US(B-country) $(O) With(O) this(O) group(O) ,(O) Davis(B-musical artist) completed(O) the(O) rest(O) of(O) what(O) became(O) Seven(B-album) Steps(I-album) to(I-album) Heaven(I-album) ((O) 1963(O) )(O) and(O) recorded(O) the(O) live(O) albums(O) Miles(B-album) Davis(I-album) in(I-album) Europe(I-album) ((O) 1964(O) )(O) ,(O) My(B-song) Funny(I-song) Valentine(I-song) ((O) 1965(O) )(O) ,(O) and(O) Four(B-album) &(I-album) More(I-album) ((O) 1966(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, country, event, album, organization, music genre, song, musical instrument, band, location, award, person and O.\nSentence: Rollins left to join them soon after , leaving Davis to pay over $ 25,000 ( US $ With this group , Davis completed the rest of what became Seven Steps to Heaven ( 1963 ) and recorded the live albums Miles Davis in Europe ( 1964 ) , My Funny Valentine ( 1965 ) , and Four & More ( 1966 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Rollins","left","to","join","them","soon","after",",","leaving","Davis","to","pay","over","$","25,000","(","US","$","With","this","group",",","Davis","completed","the","rest","of","what","became","Seven","Steps","to","Heaven","(","1963",")","and","recorded","the","live","albums","Miles","Davis","in","Europe","(","1964",")",",","My","Funny","Valentine","(","1965",")",",","and","Four","&","More","(","1966",")","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","B-musical artist","O","O","O","O","O","O","B-country","O","O","O","O","O","B-musical artist","O","O","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","B-song","I-song","I-song","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","country","event","album","organization","music_genre","song","musical_instrument","band","location","award","person"]}
{"id":"207","dataset":"crossner_music","split":"test","instance":{"id":"207","prompt_labels":"For(O) Revolver(B-band) he(O) selected(O) five(O) non-(O) Heavy(B-music genre) metal(I-music genre) music(I-music genre) records(O) that(O) influenced(O) him(O) :(O) The(O) Cure(B-band) 's(O) Pornography(B-album) ,(O) Helium(B-band) 's(O) No(B-album) Guitars(I-album) ,(O) Mogwai(B-band) EP(B-album) +(I-album) 2(I-album) ,(O) My(B-band) Bloody(I-band) Valentine(I-band) 's(O) Loveless(B-album) and(O) The(B-band) Smashing(I-band) Pumpkins(I-band) '(O) Siamese(B-album) Dream(I-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, person, organization, album, musical artist, music genre, musical instrument, location, band, country, song and O.\nSentence: For Revolver he selected five non- Heavy metal music records that influenced him : The Cure 's Pornography , Helium 's No Guitars , Mogwai EP + 2 , My Bloody Valentine 's Loveless and The Smashing Pumpkins ' Siamese Dream .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","Revolver","he","selected","five","non-","Heavy","metal","music","records","that","influenced","him",":","The","Cure","'s","Pornography",",","Helium","'s","No","Guitars",",","Mogwai","EP","+","2",",","My","Bloody","Valentine","'s","Loveless","and","The","Smashing","Pumpkins","'","Siamese","Dream","."],"labels":["O","B-band","O","O","O","O","B-music genre","I-music genre","I-music genre","O","O","O","O","O","O","B-band","O","B-album","O","B-band","O","B-album","I-album","O","B-band","B-album","I-album","I-album","O","B-band","I-band","I-band","O","B-album","O","B-band","I-band","I-band","O","B-album","I-album","O"],"target_index":null,"target_label":null},"label_list":["award","event","person","organization","album","musical_artist","music_genre","musical_instrument","location","band","country","song"]}
{"id":"208","dataset":"crossner_music","split":"test","instance":{"id":"208","prompt_labels":"He(O) has(O) also(O) released(O) three(O) solo(O) albums(O) :(O) Stranger(B-album) in(I-album) This(I-album) Town(I-album) in(O) 1991(O) ,(O) Undiscovered(B-album) Soul(I-album) in(O) 1998(O) ,(O) and(O) Aftermath(B-album) of(I-album) the(I-album) Lowdown(I-album) released(O) in(O) September(O) 2012(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, album, event, band, music genre, award, person, organization, musical artist, musical instrument, song and O.\nSentence: He has also released three solo albums : Stranger in This Town in 1991 , Undiscovered Soul in 1998 , and Aftermath of the Lowdown released in September 2012 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","has","also","released","three","solo","albums",":","Stranger","in","This","Town","in","1991",",","Undiscovered","Soul","in","1998",",","and","Aftermath","of","the","Lowdown","released","in","September","2012","."],"labels":["O","O","O","O","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","B-album","I-album","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","country","album","event","band","music_genre","award","person","organization","musical_artist","musical_instrument","song"]}
{"id":"211","dataset":"crossner_music","split":"test","instance":{"id":"211","prompt_labels":"In(O) 2002(O) ,(O) Grohl(B-musical artist) helped(O) Chan(B-musical artist) Marshall(I-musical artist) of(O) Cat(B-musical artist) Power(I-musical artist) on(O) the(O) album(O) You(B-album) Are(I-album) Free(I-album) and(O) played(O) with(O) Queens(B-band) of(I-band) the(I-band) Stone(I-band) Age(I-band) on(O) their(O) album(O) Songs(B-album) for(I-album) the(I-album) Deaf(I-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, music genre, band, song, album, person, award, country, musical artist, musical instrument, location and O.\nSentence: In 2002 , Grohl helped Chan Marshall of Cat Power on the album You Are Free and played with Queens of the Stone Age on their album Songs for the Deaf .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2002",",","Grohl","helped","Chan","Marshall","of","Cat","Power","on","the","album","You","Are","Free","and","played","with","Queens","of","the","Stone","Age","on","their","album","Songs","for","the","Deaf","."],"labels":["O","O","O","B-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","B-album","I-album","I-album","O","O","O","B-band","I-band","I-band","I-band","I-band","O","O","O","B-album","I-album","I-album","I-album","O"],"target_index":null,"target_label":null},"label_list":["organization","event","music_genre","band","song","album","person","award","country","musical_artist","musical_instrument","location"]}
{"id":"212","dataset":"crossner_music","split":"test","instance":{"id":"212","prompt_labels":"He(O) recorded(O) four(O) studio(O) albums(O) for(O) A(B-organization) &(I-organization) amp(I-organization) ;(I-organization) M(I-organization) :(O) Pleasures(B-album) of(I-album) the(I-album) Harbor(I-album) ((O) 1967(O) )(O) ,(O) Tape(B-album) from(I-album) California(I-album) ((O) 1968(O) )(O) ,(O) Rehearsals(B-album) for(I-album) Retirement(I-album) ((O) 1969(O) )(O) ,(O) and(O) the(O) ironically(O) titled(O) Greatest(B-album) Hits(I-album) ((O) 1970(O) )(O) ((O) which(O) actually(O) consisted(O) of(O) all(O) new(O) material(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, musical artist, country, event, album, music genre, musical instrument, location, award, song, band, organization and O.\nSentence: He recorded four studio albums for A & amp ; M : Pleasures of the Harbor ( 1967 ) , Tape from California ( 1968 ) , Rehearsals for Retirement ( 1969 ) , and the ironically titled Greatest Hits ( 1970 ) ( which actually consisted of all new material ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","recorded","four","studio","albums","for","A","&","amp",";","M",":","Pleasures","of","the","Harbor","(","1967",")",",","Tape","from","California","(","1968",")",",","Rehearsals","for","Retirement","(","1969",")",",","and","the","ironically","titled","Greatest","Hits","(","1970",")","(","which","actually","consisted","of","all","new","material",")","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-album","I-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","O","O","O","O","O","O","O","O","B-album","I-album","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","musical_artist","country","event","album","music_genre","musical_instrument","location","award","song","band","organization"]}
{"id":"213","dataset":"crossner_music","split":"test","instance":{"id":"213","prompt_labels":"Howlin(B-musical artist) '(I-musical artist) Wolf(I-musical artist) ,(O) Muddy(B-musical artist) Waters(I-musical artist) ,(O) Willie(B-musical artist) Dixon(I-musical artist) and(O) Jimmy(B-musical artist) Reed(I-musical artist) were(O) all(O) born(O) in(O) Mississippi(B-location) and(O) moved(O) to(O) Chicago(B-location) during(O) the(O) Great(B-event) Migration(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical artist, musical instrument, band, country, award, album, song, person, music genre, location, organization and O.\nSentence: Howlin ' Wolf , Muddy Waters , Willie Dixon and Jimmy Reed were all born in Mississippi and moved to Chicago during the Great Migration .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Howlin","'","Wolf",",","Muddy","Waters",",","Willie","Dixon","and","Jimmy","Reed","were","all","born","in","Mississippi","and","moved","to","Chicago","during","the","Great","Migration","."],"labels":["B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O","B-location","O","O","O","B-location","O","O","B-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["event","musical_artist","musical_instrument","band","country","award","album","song","person","music_genre","location","organization"]}
{"id":"214","dataset":"crossner_music","split":"test","instance":{"id":"214","prompt_labels":"It(O) received(O) a(O) major(O) expansion(O) ahead(O) of(O) the(O) 1983(B-event) Summer(I-event) Universiade(I-event) ,(O) when(O) it(O) reached(O) a(O) capacity(O) of(O) 60,081(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, song, award, event, musical artist, album, country, musical instrument, music genre, band, location, person and O.\nSentence: It received a major expansion ahead of the 1983 Summer Universiade , when it reached a capacity of 60,081 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","received","a","major","expansion","ahead","of","the","1983","Summer","Universiade",",","when","it","reached","a","capacity","of","60,081","."],"labels":["O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","song","award","event","musical_artist","album","country","musical_instrument","music_genre","band","location","person"]}
{"id":"216","dataset":"crossner_music","split":"test","instance":{"id":"216","prompt_labels":"Reggae(B-music genre) en(I-music genre) Espaol(I-music genre) spread(O) from(O) the(O) Spanish-speaking(O) Central(B-location) American(I-location) country(O) of(O) Panama(B-country) to(O) the(O) mainland(O) South(B-location) American(I-location) countries(O) of(O) Venezuela(B-country) and(O) Guyana(B-country) then(O) to(O) the(O) rest(O) of(O) South(B-location) America(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical instrument, music genre, location, person, award, organization, album, band, song, musical artist, country and O.\nSentence: Reggae en Espaol spread from the Spanish-speaking Central American country of Panama to the mainland South American countries of Venezuela and Guyana then to the rest of South America .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Reggae","en","Espaol","spread","from","the","Spanish-speaking","Central","American","country","of","Panama","to","the","mainland","South","American","countries","of","Venezuela","and","Guyana","then","to","the","rest","of","South","America","."],"labels":["B-music genre","I-music genre","I-music genre","O","O","O","O","B-location","I-location","O","O","B-country","O","O","O","B-location","I-location","O","O","B-country","O","B-country","O","O","O","O","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["event","musical_instrument","music_genre","location","person","award","organization","album","band","song","musical_artist","country"]}
{"id":"217","dataset":"crossner_music","split":"test","instance":{"id":"217","prompt_labels":"Cliff(B-musical artist) Richard(I-musical artist) covered(O) Lay(B-song) All(I-song) Your(I-song) Love(I-song) on(I-song) Me(I-song) ,(O) while(O) Dionne(B-musical artist) Warwick(I-musical artist) ,(O) Peter(B-musical artist) Cetera(I-musical artist) ,(O) and(O) Celebrity(B-album) Skin(I-album) recorded(O) their(O) versions(O) of(O) SOS(B-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, event, music genre, person, organization, country, musical instrument, award, location, album, band, song and O.\nSentence: Cliff Richard covered Lay All Your Love on Me , while Dionne Warwick , Peter Cetera , and Celebrity Skin recorded their versions of SOS .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Cliff","Richard","covered","Lay","All","Your","Love","on","Me",",","while","Dionne","Warwick",",","Peter","Cetera",",","and","Celebrity","Skin","recorded","their","versions","of","SOS","."],"labels":["B-musical artist","I-musical artist","O","B-song","I-song","I-song","I-song","I-song","I-song","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","B-album","I-album","O","O","O","O","B-song","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","event","music_genre","person","organization","country","musical_instrument","award","location","album","band","song"]}
{"id":"218","dataset":"crossner_music","split":"test","instance":{"id":"218","prompt_labels":"In(O) 2003(O) ,(O) Afshin-Jam(B-person) became(O) Miss(B-organization) World(I-organization) Canada(I-organization) and(O) joined(O) in(O) the(O) Miss(B-organization) World(I-organization) contest(O) in(O) Sanya(B-location) ,(O) China(B-country) ,(O) where(O) she(O) placed(O) second(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, musical instrument, award, music genre, musical artist, event, organization, person, band, location, song, country and O.\nSentence: In 2003 , Afshin-Jam became Miss World Canada and joined in the Miss World contest in Sanya , China , where she placed second .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2003",",","Afshin-Jam","became","Miss","World","Canada","and","joined","in","the","Miss","World","contest","in","Sanya",",","China",",","where","she","placed","second","."],"labels":["O","O","O","B-person","O","B-organization","I-organization","I-organization","O","O","O","O","B-organization","I-organization","O","O","B-location","O","B-country","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["album","musical_instrument","award","music_genre","musical_artist","event","organization","person","band","location","song","country"]}
{"id":"219","dataset":"crossner_music","split":"test","instance":{"id":"219","prompt_labels":"On(O) June(O) 11-12(O) ,(O) 2008(O) ,(O) they(O) played(O) in(O) Philadelphia(B-location) and(O) New(B-location) Jersey(I-location) at(O) the(O) Trocadero(B-location) Theatre(I-location) and(O) Starland(B-location) Ballroom(I-location) ,(O) respectively(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, event, country, organization, song, award, band, musical instrument, music genre, album, location, person and O.\nSentence: On June 11-12 , 2008 , they played in Philadelphia and New Jersey at the Trocadero Theatre and Starland Ballroom , respectively .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","June","11-12",",","2008",",","they","played","in","Philadelphia","and","New","Jersey","at","the","Trocadero","Theatre","and","Starland","Ballroom",",","respectively","."],"labels":["O","O","O","O","O","O","O","O","O","B-location","O","B-location","I-location","O","O","B-location","I-location","O","B-location","I-location","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","event","country","organization","song","award","band","musical_instrument","music_genre","album","location","person"]}
{"id":"220","dataset":"crossner_music","split":"test","instance":{"id":"220","prompt_labels":"Brad(B-musical artist) Wilk(I-musical artist) of(O) Rage(B-band) Against(I-band) the(I-band) Machine(I-band) was(O) chosen(O) as(O) the(O) drummer(O) ,(O) and(O) Rick(B-musical artist) Rubin(I-musical artist) was(O) chosen(O) as(O) the(O) producer(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, song, country, music genre, band, musical instrument, musical artist, location, award, album, event and O.\nSentence: Brad Wilk of Rage Against the Machine was chosen as the drummer , and Rick Rubin was chosen as the producer .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Brad","Wilk","of","Rage","Against","the","Machine","was","chosen","as","the","drummer",",","and","Rick","Rubin","was","chosen","as","the","producer","."],"labels":["B-musical artist","I-musical artist","O","B-band","I-band","I-band","I-band","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","person","song","country","music_genre","band","musical_instrument","musical_artist","location","award","album","event"]}
{"id":"221","dataset":"crossner_music","split":"test","instance":{"id":"221","prompt_labels":"Many(O) stations(O) play(O) primarily(O) gospel(B-music genre) music(I-music genre) ,(O) including(O) Urban(B-music genre) contemporary(I-music genre) gospel(I-music genre) and(O) Southern(B-music genre) Gospel(I-music genre) ,(O) or(O) contemporary(B-music genre) worship(I-music genre) music(I-music genre) ,(O) while(O) others(O) play(O) all(O) formats(O) of(O) contemporary(B-music genre) Christian(I-music genre) music(I-music genre) ,(O) including(O) Christian(O) pop(O) ,(O) Christian(B-music genre) rock(I-music genre) ,(O) Christian(B-music genre) rap(I-music genre) ,(O) Christian(B-music genre) country(I-music genre) music(I-music genre) ,(O) and(O) Christian(B-music genre) alternative(I-music genre) rock(I-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, music genre, award, band, musical instrument, organization, song, country, person, location, album, musical artist and O.\nSentence: Many stations play primarily gospel music , including Urban contemporary gospel and Southern Gospel , or contemporary worship music , while others play all formats of contemporary Christian music , including Christian pop , Christian rock , Christian rap , Christian country music , and Christian alternative rock .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Many","stations","play","primarily","gospel","music",",","including","Urban","contemporary","gospel","and","Southern","Gospel",",","or","contemporary","worship","music",",","while","others","play","all","formats","of","contemporary","Christian","music",",","including","Christian","pop",",","Christian","rock",",","Christian","rap",",","Christian","country","music",",","and","Christian","alternative","rock","."],"labels":["O","O","O","O","B-music genre","I-music genre","O","O","B-music genre","I-music genre","I-music genre","O","B-music genre","I-music genre","O","O","B-music genre","I-music genre","I-music genre","O","O","O","O","O","O","O","B-music genre","I-music genre","I-music genre","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","O","B-music genre","I-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["event","music_genre","award","band","musical_instrument","organization","song","country","person","location","album","musical_artist"]}
{"id":"222","dataset":"crossner_music","split":"test","instance":{"id":"222","prompt_labels":"Williams(B-musical artist) has(O) received(O) a(O) record(O) eighteen(O) Brit(B-award) Awards(I-award) -(O) winning(O) Brit(B-award) Award(I-award) for(I-award) British(I-award) Male(I-award) Solo(I-award) Artist(I-award) four(O) times(O) ,(O) two(O) awards(O) for(O) Outstanding(B-award) Contribution(I-award) to(I-award) Music(I-award) and(O) the(O) 2017(B-award) Brits(I-award) Icon(I-award) for(O) his(O) lasting(O) impact(O) on(O) British(O) culture(O) ,(O) eight(O) German(O) Echo(B-award) Music(I-award) Prize(I-award) ,(O) and(O) three(O) MTV(B-award) European(I-award) Music(I-award) Awards(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, person, musical instrument, location, award, album, organization, musical artist, music genre, song, band and O.\nSentence: Williams has received a record eighteen Brit Awards - winning Brit Award for British Male Solo Artist four times , two awards for Outstanding Contribution to Music and the 2017 Brits Icon for his lasting impact on British culture , eight German Echo Music Prize , and three MTV European Music Awards .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Williams","has","received","a","record","eighteen","Brit","Awards","-","winning","Brit","Award","for","British","Male","Solo","Artist","four","times",",","two","awards","for","Outstanding","Contribution","to","Music","and","the","2017","Brits","Icon","for","his","lasting","impact","on","British","culture",",","eight","German","Echo","Music","Prize",",","and","three","MTV","European","Music","Awards","."],"labels":["B-musical artist","O","O","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["country","event","person","musical_instrument","location","award","album","organization","musical_artist","music_genre","song","band"]}
{"id":"223","dataset":"crossner_music","split":"test","instance":{"id":"223","prompt_labels":"The(O) album(O) included(O) three(O) songs(O) written(O) or(O) co-written(O) by(O) Dylan(B-musical artist) ((O) This(B-song) Wheel(I-song) 's(I-song) on(I-song) Fire(I-song) ,(O) Tears(B-song) of(I-song) Rage(I-song) ,(O) and(O) I(B-song) Shall(I-song) Be(I-song) Released(I-song) )(O) as(O) well(O) as(O) The(B-song) Weight(I-song) ,(O) the(O) use(O) of(O) which(O) in(O) the(O) film(O) Easy(O) Rider(O) would(O) make(O) it(O) one(O) of(O) their(O) best-known(O) songs(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, band, music genre, album, musical instrument, musical artist, award, location, person, song, event, country and O.\nSentence: The album included three songs written or co-written by Dylan ( This Wheel 's on Fire , Tears of Rage , and I Shall Be Released ) as well as The Weight , the use of which in the film Easy Rider would make it one of their best-known songs .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","album","included","three","songs","written","or","co-written","by","Dylan","(","This","Wheel","'s","on","Fire",",","Tears","of","Rage",",","and","I","Shall","Be","Released",")","as","well","as","The","Weight",",","the","use","of","which","in","the","film","Easy","Rider","would","make","it","one","of","their","best-known","songs","."],"labels":["O","O","O","O","O","O","O","O","O","B-musical artist","O","B-song","I-song","I-song","I-song","I-song","O","B-song","I-song","I-song","O","O","B-song","I-song","I-song","I-song","O","O","O","O","B-song","I-song","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","band","music_genre","album","musical_instrument","musical_artist","award","location","person","song","event","country"]}
{"id":"224","dataset":"crossner_music","split":"test","instance":{"id":"224","prompt_labels":"Phoenix(B-location) music(O) venues(O) have(O) included(O) Comerica(B-location) Theatre(I-location) ,(O) Ashley(B-location) Furniture(I-location) HomeStore(I-location) Pavilion(I-location) ,(O) Long(B-location) Wong(I-location) 's(I-location) ,(O) The(B-location) Rebel(I-location) Lounge(I-location) ((O) formerly(O) The(B-location) Mason(I-location) Jar(I-location) )(O) ,(O) Modified(B-location) Arts(I-location) ,(O) Trunk(B-location) Space(I-location) ,(O) Paper(B-location) Heart(I-location) Gallery(I-location) ,(O) Club(B-location) Red(I-location) ,(O) Marquee(B-location) Theatre(I-location) ,(O) The(B-location) Van(I-location) Buren(I-location) ,(O) Crescent(B-location) Ball(I-location) Room(I-location) ,(O) Valley(B-location) Bar(I-location) ,(O) the(O) Nile(B-location) Theater(I-location) ,(O) The(B-location) Rhythm(I-location) Room(I-location) ,(O) and(O) Compton(B-location) Terrace(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, location, musical artist, organization, person, event, album, country, band, song, award, musical instrument and O.\nSentence: Phoenix music venues have included Comerica Theatre , Ashley Furniture HomeStore Pavilion , Long Wong 's , The Rebel Lounge ( formerly The Mason Jar ) , Modified Arts , Trunk Space , Paper Heart Gallery , Club Red , Marquee Theatre , The Van Buren , Crescent Ball Room , Valley Bar , the Nile Theater , The Rhythm Room , and Compton Terrace .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Phoenix","music","venues","have","included","Comerica","Theatre",",","Ashley","Furniture","HomeStore","Pavilion",",","Long","Wong","'s",",","The","Rebel","Lounge","(","formerly","The","Mason","Jar",")",",","Modified","Arts",",","Trunk","Space",",","Paper","Heart","Gallery",",","Club","Red",",","Marquee","Theatre",",","The","Van","Buren",",","Crescent","Ball","Room",",","Valley","Bar",",","the","Nile","Theater",",","The","Rhythm","Room",",","and","Compton","Terrace","."],"labels":["B-location","O","O","O","O","B-location","I-location","O","B-location","I-location","I-location","I-location","O","B-location","I-location","I-location","O","B-location","I-location","I-location","O","O","B-location","I-location","I-location","O","O","B-location","I-location","O","B-location","I-location","O","B-location","I-location","I-location","O","B-location","I-location","O","B-location","I-location","O","B-location","I-location","I-location","O","B-location","I-location","I-location","O","B-location","I-location","O","O","B-location","I-location","O","B-location","I-location","I-location","O","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["music_genre","location","musical_artist","organization","person","event","album","country","band","song","award","musical_instrument"]}
{"id":"227","dataset":"crossner_music","split":"test","instance":{"id":"227","prompt_labels":"The(O) latter(O) earned(O) Kidman(B-person) the(O) Primetime(B-award) Emmy(I-award) Award(I-award) for(O) Primetime(B-award) Emmy(I-award) Award(I-award) for(I-award) Outstanding(I-award) Lead(I-award) Actress(I-award) in(I-award) a(I-award) Limited(I-award) Series(I-award) or(I-award) Movie(I-award) and(O) Primetime(B-award) Emmy(I-award) Award(I-award) for(I-award) Outstanding(I-award) Limited(I-award) Series(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, musical instrument, song, band, album, event, location, organization, music genre, person, country, musical artist and O.\nSentence: The latter earned Kidman the Primetime Emmy Award for Primetime Emmy Award for Outstanding Lead Actress in a Limited Series or Movie and Primetime Emmy Award for Outstanding Limited Series .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","latter","earned","Kidman","the","Primetime","Emmy","Award","for","Primetime","Emmy","Award","for","Outstanding","Lead","Actress","in","a","Limited","Series","or","Movie","and","Primetime","Emmy","Award","for","Outstanding","Limited","Series","."],"labels":["O","O","O","B-person","O","B-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["award","musical_instrument","song","band","album","event","location","organization","music_genre","person","country","musical_artist"]}
{"id":"229","dataset":"crossner_music","split":"test","instance":{"id":"229","prompt_labels":"Martin(B-musical artist) Fry(I-musical artist) and(O) band(O) were(O) once(O) more(O) accompanied(O) by(O) the(O) Southbank(B-band) Sinfonia(I-band) Orchestra(I-band) for(O) dates(O) at(O) Liverpool(B-location) 's(O) Philharmonic(B-location) Hall(I-location) ,(O) Glasgow(B-location) Royal(I-location) Concert(I-location) Hall(I-location) ,(O) Sheffield(B-location) City(I-location) Hall(I-location) ,(O) the(O) Theatre(B-location) Royal(I-location) ,(O) Drury(B-location) Lane(I-location) in(O) London(B-location) and(O) Symphony(B-location) Hall(I-location) ,(O) Birmingham(B-location) ,(O) between(O) November(O) 4th(O) and(O) 9th(O) ,(O) 2015(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, musical instrument, event, music genre, location, band, album, song, organization, country, musical artist and O.\nSentence: Martin Fry and band were once more accompanied by the Southbank Sinfonia Orchestra for dates at Liverpool 's Philharmonic Hall , Glasgow Royal Concert Hall , Sheffield City Hall , the Theatre Royal , Drury Lane in London and Symphony Hall , Birmingham , between November 4th and 9th , 2015 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Martin","Fry","and","band","were","once","more","accompanied","by","the","Southbank","Sinfonia","Orchestra","for","dates","at","Liverpool","'s","Philharmonic","Hall",",","Glasgow","Royal","Concert","Hall",",","Sheffield","City","Hall",",","the","Theatre","Royal",",","Drury","Lane","in","London","and","Symphony","Hall",",","Birmingham",",","between","November","4th","and","9th",",","2015","."],"labels":["B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","B-band","I-band","I-band","O","O","O","B-location","O","B-location","I-location","O","B-location","I-location","I-location","I-location","O","B-location","I-location","I-location","O","O","B-location","I-location","O","B-location","I-location","O","B-location","O","B-location","I-location","O","B-location","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","person","musical_instrument","event","music_genre","location","band","album","song","organization","country","musical_artist"]}
{"id":"231","dataset":"crossner_music","split":"test","instance":{"id":"231","prompt_labels":"The(O) near(O) 3-hour-long(O) epic(O) ,(O) which(O) chronicled(O) the(O) saga(O) of(O) the(O) Corleone(O) family(O) ,(O) received(O) overwhelmingly(O) positive(O) reviews(O) from(O) critics(O) and(O) fetched(O) Coppola(B-person) the(O) Academy(B-award) Awards(I-award) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Adapted(I-award) Screenplay(I-award) ,(O) which(O) he(O) shared(O) with(O) Mario(B-person) Puzo(I-person) and(O) two(O) Golden(B-award) Globe(I-award) Award(I-award) s(O) :(O) for(O) Golden(B-award) Globe(I-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) and(O) Golden(B-award) Globe(I-award) Award(I-award) for(I-award) Best(I-award) Screenplay(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, music genre, country, event, band, award, location, song, organization, musical artist, album, person and O.\nSentence: The near 3-hour-long epic , which chronicled the saga of the Corleone family , received overwhelmingly positive reviews from critics and fetched Coppola the Academy Awards for Academy Award for Best Adapted Screenplay , which he shared with Mario Puzo and two Golden Globe Award s : for Golden Globe Award for Best Director and Golden Globe Award for Best Screenplay .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","near","3-hour-long","epic",",","which","chronicled","the","saga","of","the","Corleone","family",",","received","overwhelmingly","positive","reviews","from","critics","and","fetched","Coppola","the","Academy","Awards","for","Academy","Award","for","Best","Adapted","Screenplay",",","which","he","shared","with","Mario","Puzo","and","two","Golden","Globe","Award","s",":","for","Golden","Globe","Award","for","Best","Director","and","Golden","Globe","Award","for","Best","Screenplay","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","O","B-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","B-person","I-person","O","O","B-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","music_genre","country","event","band","award","location","song","organization","musical_artist","album","person"]}
{"id":"232","dataset":"crossner_music","split":"test","instance":{"id":"232","prompt_labels":"He(O) has(O) been(O) nominated(O) in(O) more(O) categories(O) than(O) any(O) other(O) musician(O) ,(O) namely(O) Country(B-music genre) music(I-music genre) ,(O) pop(B-music genre) ,(O) jazz(B-music genre) ,(O) Bluegrass(B-music genre) music(I-music genre) ,(O) classical(B-music genre) ,(O) Folk(B-music genre) music(I-music genre) ,(O) spoken(O) word(O) ,(O) composition(O) ,(O) and(O) arranging(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, band, music genre, song, event, location, album, musical instrument, person, organization, musical artist, award and O.\nSentence: He has been nominated in more categories than any other musician , namely Country music , pop , jazz , Bluegrass music , classical , Folk music , spoken word , composition , and arranging .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","has","been","nominated","in","more","categories","than","any","other","musician",",","namely","Country","music",",","pop",",","jazz",",","Bluegrass","music",",","classical",",","Folk","music",",","spoken","word",",","composition",",","and","arranging","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","band","music_genre","song","event","location","album","musical_instrument","person","organization","musical_artist","award"]}
{"id":"233","dataset":"crossner_music","split":"test","instance":{"id":"233","prompt_labels":"That(O) January(O) ,(O) the(O) San(B-location) Francisco(I-location) band(O) Blue(B-band) Cheer(I-band) released(O) a(O) cover(O) of(O) Eddie(B-musical artist) Cochran(I-musical artist) '(O) s(O) classic(O) Summertime(B-song) Blues(I-song) ,(O) from(O) their(O) debut(O) album(O) Vincebus(B-album) Eruptum(I-album) ,(O) that(O) many(O) consider(O) the(O) first(O) TRUE(O) heavy(B-music genre) metal(I-music genre) recording.McCleary(O) ((O) 2004(O) )(O) ,(O) pp.(O) 240(O) ,(O) 506(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, award, song, band, organization, person, album, musical instrument, country, music genre, musical artist and O.\nSentence: That January , the San Francisco band Blue Cheer released a cover of Eddie Cochran ' s classic Summertime Blues , from their debut album Vincebus Eruptum , that many consider the first TRUE heavy metal recording.McCleary ( 2004 ) , pp. 240 , 506 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["That","January",",","the","San","Francisco","band","Blue","Cheer","released","a","cover","of","Eddie","Cochran","'","s","classic","Summertime","Blues",",","from","their","debut","album","Vincebus","Eruptum",",","that","many","consider","the","first","TRUE","heavy","metal","recording.McCleary","(","2004",")",",","pp.","240",",","506","."],"labels":["O","O","O","O","B-location","I-location","O","B-band","I-band","O","O","O","O","B-musical artist","I-musical artist","O","O","O","B-song","I-song","O","O","O","O","O","B-album","I-album","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","event","award","song","band","organization","person","album","musical_instrument","country","music_genre","musical_artist"]}
{"id":"234","dataset":"crossner_music","split":"test","instance":{"id":"234","prompt_labels":"It(O) was(O) not(O) long(O) before(O) Pea(B-band) was(O) touring(O) the(O) world(O) ,(O) both(O) as(O) a(O) soloist(O) and(O) an(O) accompanist(O) with(O) performances(O) at(O) Carnegie(B-location) Hall(I-location) in(O) New(B-location) York(I-location) City(I-location) ,(O) the(O) Royal(B-location) Albert(I-location) Hall(I-location) in(O) London(B-location) and(O) the(O) Concertgebouw(B-location) in(O) Amsterdam(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, person, song, album, event, music genre, musical artist, organization, musical instrument, country, award, location and O.\nSentence: It was not long before Pea was touring the world , both as a soloist and an accompanist with performances at Carnegie Hall in New York City , the Royal Albert Hall in London and the Concertgebouw in Amsterdam .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","not","long","before","Pea","was","touring","the","world",",","both","as","a","soloist","and","an","accompanist","with","performances","at","Carnegie","Hall","in","New","York","City",",","the","Royal","Albert","Hall","in","London","and","the","Concertgebouw","in","Amsterdam","."],"labels":["O","O","O","O","O","B-band","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","I-location","I-location","O","O","B-location","I-location","I-location","O","B-location","O","O","B-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["band","person","song","album","event","music_genre","musical_artist","organization","musical_instrument","country","award","location"]}
{"id":"236","dataset":"crossner_music","split":"test","instance":{"id":"236","prompt_labels":"Collette(B-musical artist) supports(O) various(O) charities(O) including(O) Mdecins(B-organization) Sans(I-organization) Frontires(I-organization) ,(O) Amnesty(B-organization) International(I-organization) and(O) Feeding(B-organization) America(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, musical artist, music genre, country, album, award, song, musical instrument, organization, band, location and O.\nSentence: Collette supports various charities including Mdecins Sans Frontires , Amnesty International and Feeding America .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Collette","supports","various","charities","including","Mdecins","Sans","Frontires",",","Amnesty","International","and","Feeding","America","."],"labels":["B-musical artist","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["person","event","musical_artist","music_genre","country","album","award","song","musical_instrument","organization","band","location"]}
{"id":"238","dataset":"crossner_music","split":"test","instance":{"id":"238","prompt_labels":"In(O) 2014(O) ,(O) the(O) Robert(B-person) David(I-person) MacDonald(I-person) and(O) Jeremy(B-person) Sams(I-person) translation(O) ((O) previously(O) used(O) in(O) 1994(O) at(O) the(O) Donmar(B-location) Warehouse(I-location) )(O) toured(O) the(O) UK(B-country) ,(O) presented(O) by(O) the(O) Graeae(B-organization) Theatre(I-organization) Company(I-organization) with(O) Nottingham(B-organization) Playhouse(I-organization) ,(O) New(B-organization) Wolsey(I-organization) Theatre(I-organization) Ipswich(O) ,(O) Birmingham(B-organization) Repertory(I-organization) Theatre(I-organization) and(O) West(B-organization) Yorkshire(I-organization) Playhouse(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, music genre, event, band, organization, country, award, album, song, location, person, musical instrument and O.\nSentence: In 2014 , the Robert David MacDonald and Jeremy Sams translation ( previously used in 1994 at the Donmar Warehouse ) toured the UK , presented by the Graeae Theatre Company with Nottingham Playhouse , New Wolsey Theatre Ipswich , Birmingham Repertory Theatre and West Yorkshire Playhouse .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2014",",","the","Robert","David","MacDonald","and","Jeremy","Sams","translation","(","previously","used","in","1994","at","the","Donmar","Warehouse",")","toured","the","UK",",","presented","by","the","Graeae","Theatre","Company","with","Nottingham","Playhouse",",","New","Wolsey","Theatre","Ipswich",",","Birmingham","Repertory","Theatre","and","West","Yorkshire","Playhouse","."],"labels":["O","O","O","O","B-person","I-person","I-person","O","B-person","I-person","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","B-country","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","music_genre","event","band","organization","country","award","album","song","location","person","musical_instrument"]}
{"id":"239","dataset":"crossner_music","split":"test","instance":{"id":"239","prompt_labels":"Rob(B-musical artist) Sheffield(I-musical artist) in(O) an(O) AllMusic(B-organization) review(O) feels(O) that(O) ,(O) like(O) Mellow(B-album) Gold(I-album) ,(O) Odelay(B-album) incorporates(O) elements(O) from(O) various(O) genres(O) ,(O) including(O) Folk(B-music genre) music(I-music genre) and(O) Country(B-music genre) music(I-music genre) ,(O) grungy(B-music genre) garage(I-music genre) rock(I-music genre) ,(O) stiff-boned(O) electro(B-music genre) ,(O) louche(O) exotica(B-music genre) ,(O) Old-school(B-music genre) hip(I-music genre) hop(I-music genre) and(O) noise(B-music genre) rock(I-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, musical instrument, band, event, organization, country, song, album, award, music genre, musical artist, location and O.\nSentence: Rob Sheffield in an AllMusic review feels that , like Mellow Gold , Odelay incorporates elements from various genres , including Folk music and Country music , grungy garage rock , stiff-boned electro , louche exotica , Old-school hip hop and noise rock .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Rob","Sheffield","in","an","AllMusic","review","feels","that",",","like","Mellow","Gold",",","Odelay","incorporates","elements","from","various","genres",",","including","Folk","music","and","Country","music",",","grungy","garage","rock",",","stiff-boned","electro",",","louche","exotica",",","Old-school","hip","hop","and","noise","rock","."],"labels":["B-musical artist","I-musical artist","O","O","B-organization","O","O","O","O","O","B-album","I-album","O","B-album","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","O","B-music genre","O","O","B-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["person","musical_instrument","band","event","organization","country","song","album","award","music_genre","musical_artist","location"]}
{"id":"240","dataset":"crossner_music","split":"test","instance":{"id":"240","prompt_labels":"Apple(B-musical artist) 's(O) debut(O) album(O) earned(O) her(O) a(O) Grammy(B-award) Award(I-award) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Female(I-award) Rock(I-award) Vocal(I-award) Performance(I-award) for(O) Criminal(B-song) and(O) the(O) MTV(B-award) Video(I-award) Music(I-award) Award(I-award) for(O) MTV(B-award) Video(I-award) Music(I-award) Award(I-award) for(I-award) Best(I-award) New(I-award) Artist(I-award) for(O) Sleep(B-song) to(I-song) Dream(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, musical instrument, location, award, band, organization, music genre, country, musical artist, event, song, album and O.\nSentence: Apple 's debut album earned her a Grammy Award for Grammy Award for Best Female Rock Vocal Performance for Criminal and the MTV Video Music Award for MTV Video Music Award for Best New Artist for Sleep to Dream .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Apple","'s","debut","album","earned","her","a","Grammy","Award","for","Grammy","Award","for","Best","Female","Rock","Vocal","Performance","for","Criminal","and","the","MTV","Video","Music","Award","for","MTV","Video","Music","Award","for","Best","New","Artist","for","Sleep","to","Dream","."],"labels":["B-musical artist","O","O","O","O","O","O","B-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","B-song","O","O","B-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","B-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["person","musical_instrument","location","award","band","organization","music_genre","country","musical_artist","event","song","album"]}
{"id":"241","dataset":"crossner_music","split":"test","instance":{"id":"241","prompt_labels":"The(O) focus(O) of(O) their(O) activities(O) in(O) Europe(B-location) became(O) annual(O) concert(O) cycles(O) at(O) the(O) Wiener(B-location) Konzerthaus(I-location) ,(O) at(O) London(B-location) 's(O) Queen(B-location) Elizabeth(I-location) Hall(I-location) ,(O) the(O) Frankfurt(B-location) Alte(I-location) Oper(I-location) ,(O) the(O) Thtre(B-location) des(I-location) Champs(I-location) Elyses(I-location) in(O) Paris(B-location) ,(O) the(O) Philharmonic(B-location) Hall(I-location) in(O) Cologne(B-location) ,(O) the(O) Zurich(B-location) Opera(I-location) ,(O) as(O) well(O) as(O) regular(O) concerts(O) at(O) most(O) major(O) halls(O) and(O) venues(O) around(O) the(O) world(O) ((O) among(O) them(O) La(B-location) Scala(I-location) ,(O) Concertgebouw(B-location) ,(O) Berliner(B-location) Philharmonie(I-location) ,(O) Carnegie(B-location) Hall(I-location) ,(O) Teatro(B-location) Coln(I-location) ,(O) Suntory(B-location) Hall(I-location) ,(O) etc(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, location, musical artist, organization, person, song, music genre, country, award, album, band, event and O.\nSentence: The focus of their activities in Europe became annual concert cycles at the Wiener Konzerthaus , at London 's Queen Elizabeth Hall , the Frankfurt Alte Oper , the Thtre des Champs Elyses in Paris , the Philharmonic Hall in Cologne , the Zurich Opera , as well as regular concerts at most major halls and venues around the world ( among them La Scala , Concertgebouw , Berliner Philharmonie , Carnegie Hall , Teatro Coln , Suntory Hall , etc .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","focus","of","their","activities","in","Europe","became","annual","concert","cycles","at","the","Wiener","Konzerthaus",",","at","London","'s","Queen","Elizabeth","Hall",",","the","Frankfurt","Alte","Oper",",","the","Thtre","des","Champs","Elyses","in","Paris",",","the","Philharmonic","Hall","in","Cologne",",","the","Zurich","Opera",",","as","well","as","regular","concerts","at","most","major","halls","and","venues","around","the","world","(","among","them","La","Scala",",","Concertgebouw",",","Berliner","Philharmonie",",","Carnegie","Hall",",","Teatro","Coln",",","Suntory","Hall",",","etc","."],"labels":["O","O","O","O","O","O","B-location","O","O","O","O","O","O","B-location","I-location","O","O","B-location","O","B-location","I-location","I-location","O","O","B-location","I-location","I-location","O","O","B-location","I-location","I-location","I-location","O","B-location","O","O","B-location","I-location","O","B-location","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","O","B-location","I-location","O","B-location","I-location","O","B-location","I-location","O","B-location","I-location","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","location","musical_artist","organization","person","song","music_genre","country","award","album","band","event"]}
{"id":"244","dataset":"crossner_music","split":"test","instance":{"id":"244","prompt_labels":"He(O) also(O) produced(O) tracks(O) for(O) a(O) number(O) of(O) other(O) acts(O) on(O) Ruthless(B-organization) Records(I-organization) ,(O) including(O) Eazy-E(B-musical artist) 's(O) 1988(O) solo(O) debut(O) Eazy-Duz-It(B-album) ,(O) Above(B-band) the(I-band) Law(I-band) '(O) s(O) 1990(O) debut(O) Livin(B-album) '(I-album) Like(I-album) Hustlers(I-album) ,(O) Michel(B-musical artist) 'le(I-musical artist) '(O) s(O) 1989(O) self-titled(O) debut(O) ,(O) The(B-musical artist) D.O.C.(I-musical artist) '(O) s(O) 1989(O) debut(O) No(B-album) One(I-album) Can(I-album) Do(I-album) It(I-album) Better(I-album) ,(O) J.J.(B-band) Fad(I-band) '(O) s(O) 1988(O) debut(O) Supersonic(B-album) and(O) funk(B-music genre) rock(I-music genre) musician(O) Jimmy(B-musical artist) Z(I-musical artist) '(O) s(O) 1991(O) album(O) Muzical(B-album) Madness(I-album)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, band, musical artist, award, music genre, country, album, organization, song, location, musical instrument and O.\nSentence: He also produced tracks for a number of other acts on Ruthless Records , including Eazy-E 's 1988 solo debut Eazy-Duz-It , Above the Law ' s 1990 debut Livin ' Like Hustlers , Michel 'le ' s 1989 self-titled debut , The D.O.C. ' s 1989 debut No One Can Do It Better , J.J. Fad ' s 1988 debut Supersonic and funk rock musician Jimmy Z ' s 1991 album Muzical Madness","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","also","produced","tracks","for","a","number","of","other","acts","on","Ruthless","Records",",","including","Eazy-E","'s","1988","solo","debut","Eazy-Duz-It",",","Above","the","Law","'","s","1990","debut","Livin","'","Like","Hustlers",",","Michel","'le","'","s","1989","self-titled","debut",",","The","D.O.C.","'","s","1989","debut","No","One","Can","Do","It","Better",",","J.J.","Fad","'","s","1988","debut","Supersonic","and","funk","rock","musician","Jimmy","Z","'","s","1991","album","Muzical","Madness"],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","B-musical artist","O","O","O","O","B-album","O","B-band","I-band","I-band","O","O","O","O","B-album","I-album","I-album","I-album","O","B-musical artist","I-musical artist","O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","B-album","I-album","I-album","I-album","I-album","I-album","O","B-band","I-band","O","O","O","O","B-album","O","B-music genre","I-music genre","O","B-musical artist","I-musical artist","O","O","O","O","B-album","I-album"],"target_index":null,"target_label":null},"label_list":["person","event","band","musical_artist","award","music_genre","country","album","organization","song","location","musical_instrument"]}
{"id":"245","dataset":"crossner_music","split":"test","instance":{"id":"245","prompt_labels":"In(O) 2004(O) ,(O) Grohl(B-musical artist) drummed(O) on(O) several(O) tracks(O) for(O) Nine(B-band) Inch(I-band) Nails(I-band) '(O) 2005(O) album(O) With(B-album) Teeth(I-album) ,(O) later(O) returning(O) to(O) play(O) drums(B-musical instrument) on(O) '(O) The(B-song) Idea(I-song) of(I-song) You(I-song) '(O) from(O) their(O) 2016(O) EP(O) Not(B-album) the(I-album) Actual(I-album) Events(I-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, band, organization, song, album, award, music genre, musical instrument, person, musical artist, country, event and O.\nSentence: In 2004 , Grohl drummed on several tracks for Nine Inch Nails ' 2005 album With Teeth , later returning to play drums on ' The Idea of You ' from their 2016 EP Not the Actual Events .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2004",",","Grohl","drummed","on","several","tracks","for","Nine","Inch","Nails","'","2005","album","With","Teeth",",","later","returning","to","play","drums","on","'","The","Idea","of","You","'","from","their","2016","EP","Not","the","Actual","Events","."],"labels":["O","O","O","B-musical artist","O","O","O","O","O","B-band","I-band","I-band","O","O","O","B-album","I-album","O","O","O","O","O","B-musical instrument","O","O","B-song","I-song","I-song","I-song","O","O","O","O","O","B-album","I-album","I-album","I-album","O"],"target_index":null,"target_label":null},"label_list":["location","band","organization","song","album","award","music_genre","musical_instrument","person","musical_artist","country","event"]}
{"id":"246","dataset":"crossner_music","split":"test","instance":{"id":"246","prompt_labels":"Love(B-musical artist) has(O) been(O) candid(O) about(O) her(O) diverse(O) musical(O) influences(O) ,(O) the(O) earliest(O) being(O) Patti(B-musical artist) Smith(I-musical artist) ,(O) The(B-band) Runaways(I-band) ,(O) and(O) The(B-band) Pretenders(I-band) ,(O) artists(O) she(O) discovered(O) while(O) in(O) juvenile(O) hall(O) at(O) age(O) fifteen(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, song, person, album, music genre, band, event, country, organization, location, musical artist, award and O.\nSentence: Love has been candid about her diverse musical influences , the earliest being Patti Smith , The Runaways , and The Pretenders , artists she discovered while in juvenile hall at age fifteen .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Love","has","been","candid","about","her","diverse","musical","influences",",","the","earliest","being","Patti","Smith",",","The","Runaways",",","and","The","Pretenders",",","artists","she","discovered","while","in","juvenile","hall","at","age","fifteen","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-band","I-band","O","O","B-band","I-band","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","song","person","album","music_genre","band","event","country","organization","location","musical_artist","award"]}
{"id":"247","dataset":"crossner_music","split":"test","instance":{"id":"247","prompt_labels":"In(O) 1999(O) ,(O) it(O) was(O) certified(O) 3(O) (O) platinum(O) by(O) the(O) British(B-organization) Phonographic(I-organization) Industry(I-organization) ((O) BPI(B-organization) )(O) ,(O) 3(O) (O) Platinum(O) by(O) the(O) Australian(B-organization) Record(I-organization) Industry(I-organization) Association(I-organization) and(O) platinum(O) by(O) the(O) Recording(B-organization) Industry(I-organization) Association(I-organization) of(I-organization) America(I-organization) ((O) RIAA(B-organization) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, musical instrument, song, award, band, event, music genre, organization, musical artist, person, location, album and O.\nSentence: In 1999 , it was certified 3  platinum by the British Phonographic Industry ( BPI ) , 3  Platinum by the Australian Record Industry Association and platinum by the Recording Industry Association of America ( RIAA ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1999",",","it","was","certified","3","","platinum","by","the","British","Phonographic","Industry","(","BPI",")",",","3","","Platinum","by","the","Australian","Record","Industry","Association","and","platinum","by","the","Recording","Industry","Association","of","America","(","RIAA",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O"],"target_index":null,"target_label":null},"label_list":["country","musical_instrument","song","award","band","event","music_genre","organization","musical_artist","person","location","album"]}
{"id":"248","dataset":"crossner_music","split":"test","instance":{"id":"248","prompt_labels":"Every(O) style(O) of(O) music(O) was(O) offered(O) ,(O) from(O) Rock(B-music genre) music(I-music genre) and(O) Pop(B-music genre) music(I-music genre) to(O) Spanish-language(O) programming(O) ((O) for(O) Mexican(O) restaurants(O) )(O) ,(O) jazz(B-music genre) ,(O) blues(B-music genre) ,(O) classical(B-music genre) and(O) even(O) easy(B-music genre) listening(I-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, music genre, award, album, country, musical artist, location, organization, musical instrument, song, person, band and O.\nSentence: Every style of music was offered , from Rock music and Pop music to Spanish-language programming ( for Mexican restaurants ) , jazz , blues , classical and even easy listening .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Every","style","of","music","was","offered",",","from","Rock","music","and","Pop","music","to","Spanish-language","programming","(","for","Mexican","restaurants",")",",","jazz",",","blues",",","classical","and","even","easy","listening","."],"labels":["O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","B-music genre","O","B-music genre","O","B-music genre","O","O","B-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["event","music_genre","award","album","country","musical_artist","location","organization","musical_instrument","song","person","band"]}
{"id":"249","dataset":"crossner_music","split":"test","instance":{"id":"249","prompt_labels":"Singers(O) such(O) as(O) Blind(B-musical artist) Willie(I-musical artist) McTell(I-musical artist) and(O) Blind(B-musical artist) Boy(I-musical artist) Fuller(I-musical artist) performed(O) in(O) the(O) southeastern(O) delicate(O) and(O) lyrical(O) Piedmont(B-music genre) blues(I-music genre) tradition(O) ,(O) which(O) used(O) an(O) elaborate(O) ragtime-based(O) fingerpicking(O) guitar(O) technique(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, album, person, band, musical instrument, location, music genre, event, organization, musical artist, song and O.\nSentence: Singers such as Blind Willie McTell and Blind Boy Fuller performed in the southeastern delicate and lyrical Piedmont blues tradition , which used an elaborate ragtime-based fingerpicking guitar technique .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Singers","such","as","Blind","Willie","McTell","and","Blind","Boy","Fuller","performed","in","the","southeastern","delicate","and","lyrical","Piedmont","blues","tradition",",","which","used","an","elaborate","ragtime-based","fingerpicking","guitar","technique","."],"labels":["O","O","O","B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","award","album","person","band","musical_instrument","location","music_genre","event","organization","musical_artist","song"]}
{"id":"250","dataset":"crossner_music","split":"test","instance":{"id":"250","prompt_labels":"The(O) band(O) also(O) saw(O) success(O) with(O) the(O) rest(O) of(O) the(O) album(O) 's(O) singles(O) ,(O) Bleed(B-song) It(I-song) Out(I-song) ,(O) Shadow(B-song) of(I-song) the(I-song) Day(I-song) ,(O) Given(B-song) Up(I-song) ,(O) and(O) Leave(B-song) Out(I-song) All(I-song) the(I-song) Rest(I-song) ,(O) which(O) were(O) released(O) throughout(O) 2007(O) and(O) early(O) 2008(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical artist, organization, music genre, song, person, country, award, musical instrument, location, album, band and O.\nSentence: The band also saw success with the rest of the album 's singles , Bleed It Out , Shadow of the Day , Given Up , and Leave Out All the Rest , which were released throughout 2007 and early 2008 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","band","also","saw","success","with","the","rest","of","the","album","'s","singles",",","Bleed","It","Out",",","Shadow","of","the","Day",",","Given","Up",",","and","Leave","Out","All","the","Rest",",","which","were","released","throughout","2007","and","early","2008","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-song","I-song","I-song","O","B-song","I-song","I-song","I-song","O","B-song","I-song","O","O","B-song","I-song","I-song","I-song","I-song","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","musical_artist","organization","music_genre","song","person","country","award","musical_instrument","location","album","band"]}
{"id":"253","dataset":"crossner_music","split":"test","instance":{"id":"253","prompt_labels":"Franklin(B-musical artist) continued(O) to(O) record(O) acclaimed(O) albums(O) such(O) as(O) I(B-album) Never(I-album) Loved(I-album) a(I-album) Man(I-album) the(I-album) Way(I-album) I(I-album) Love(I-album) You(I-album) ((O) 1967(O) )(O) ,(O) Lady(B-album) Soul(I-album) ((O) 1968(O) )(O) ,(O) Spirit(B-album) in(I-album) the(I-album) Dark(I-album) ((O) 1970(O) )(O) ,(O) Young(B-album) ,(I-album) Gifted(I-album) and(I-album) Black(I-album) ((O) 1972(O) )(O) ,(O) Amazing(B-album) Grace(I-album) ((O) 1972(O) )(O) ,(O) and(O) Sparkle(B-album) ((O) 1976(O) )(O) before(O) experiencing(O) problems(O) with(O) her(O) record(O) company(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, music genre, album, song, musical instrument, person, country, musical artist, event, location, organization, award and O.\nSentence: Franklin continued to record acclaimed albums such as I Never Loved a Man the Way I Love You ( 1967 ) , Lady Soul ( 1968 ) , Spirit in the Dark ( 1970 ) , Young , Gifted and Black ( 1972 ) , Amazing Grace ( 1972 ) , and Sparkle ( 1976 ) before experiencing problems with her record company .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Franklin","continued","to","record","acclaimed","albums","such","as","I","Never","Loved","a","Man","the","Way","I","Love","You","(","1967",")",",","Lady","Soul","(","1968",")",",","Spirit","in","the","Dark","(","1970",")",",","Young",",","Gifted","and","Black","(","1972",")",",","Amazing","Grace","(","1972",")",",","and","Sparkle","(","1976",")","before","experiencing","problems","with","her","record","company","."],"labels":["B-musical artist","O","O","O","O","O","O","O","B-album","I-album","I-album","I-album","I-album","I-album","I-album","I-album","I-album","I-album","O","O","O","O","B-album","I-album","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","I-album","I-album","O","O","O","O","B-album","I-album","O","O","O","O","O","B-album","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["band","music_genre","album","song","musical_instrument","person","country","musical_artist","event","location","organization","award"]}
{"id":"255","dataset":"crossner_music","split":"test","instance":{"id":"255","prompt_labels":"Benson(B-person) also(O) did(O) a(O) version(O) of(O) The(B-band) Beatles(I-band) '(O) s(O) 1969(O) album(O) Abbey(B-album) Road(I-album) called(O) The(B-album) Other(I-album) Side(I-album) of(I-album) Abbey(I-album) Road(I-album) ,(O) also(O) released(O) in(O) 1969(O) ,(O) and(O) a(O) version(O) of(O) White(B-song) Rabbit(I-song) ,(O) originally(O) written(O) and(O) recorded(O) by(O) San(B-location) Francisco(I-location) rock(B-music genre) group(O) Great(B-band) Society(I-band) ,(O) and(O) made(O) famous(O) by(O) Jefferson(B-band) Airplane(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, award, person, band, event, organization, location, musical instrument, country, song, album, music genre and O.\nSentence: Benson also did a version of The Beatles ' s 1969 album Abbey Road called The Other Side of Abbey Road , also released in 1969 , and a version of White Rabbit , originally written and recorded by San Francisco rock group Great Society , and made famous by Jefferson Airplane .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Benson","also","did","a","version","of","The","Beatles","'","s","1969","album","Abbey","Road","called","The","Other","Side","of","Abbey","Road",",","also","released","in","1969",",","and","a","version","of","White","Rabbit",",","originally","written","and","recorded","by","San","Francisco","rock","group","Great","Society",",","and","made","famous","by","Jefferson","Airplane","."],"labels":["B-person","O","O","O","O","O","B-band","I-band","O","O","O","O","B-album","I-album","O","B-album","I-album","I-album","I-album","I-album","I-album","O","O","O","O","O","O","O","O","O","O","B-song","I-song","O","O","O","O","O","O","B-location","I-location","B-music genre","O","B-band","I-band","O","O","O","O","O","B-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","award","person","band","event","organization","location","musical_instrument","country","song","album","music_genre"]}
{"id":"256","dataset":"crossner_music","split":"test","instance":{"id":"256","prompt_labels":"During(O) the(O) course(O) of(O) her(O) career(O) ,(O) Saariaho(B-musical artist) has(O) received(O) commissions(O) from(O) the(O) Lincoln(B-organization) Center(I-organization) for(O) the(O) Kronos(B-band) Quartet(I-band) and(O) from(O) IRCAM(B-organization) for(O) the(O) Ensemble(B-band) Intercontemporain(I-band) ,(O) the(O) BBC(B-organization) ,(O) the(O) New(B-band) York(I-band) Philharmonic(I-band) ,(O) the(O) Salzburg(B-event) Music(I-event) Festival(I-event) ,(O) the(O) Thtre(B-location) du(I-location) Chtelet(I-location) in(O) Paris(B-location) ,(O) and(O) the(O) Finnish(B-location) National(I-location) Opera(I-location) ,(O) among(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, band, event, person, song, musical artist, album, award, musical instrument, music genre, country and O.\nSentence: During the course of her career , Saariaho has received commissions from the Lincoln Center for the Kronos Quartet and from IRCAM for the Ensemble Intercontemporain , the BBC , the New York Philharmonic , the Salzburg Music Festival , the Thtre du Chtelet in Paris , and the Finnish National Opera , among others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","the","course","of","her","career",",","Saariaho","has","received","commissions","from","the","Lincoln","Center","for","the","Kronos","Quartet","and","from","IRCAM","for","the","Ensemble","Intercontemporain",",","the","BBC",",","the","New","York","Philharmonic",",","the","Salzburg","Music","Festival",",","the","Thtre","du","Chtelet","in","Paris",",","and","the","Finnish","National","Opera",",","among","others","."],"labels":["O","O","O","O","O","O","O","B-musical artist","O","O","O","O","O","B-organization","I-organization","O","O","B-band","I-band","O","O","B-organization","O","O","B-band","I-band","O","O","B-organization","O","O","B-band","I-band","I-band","O","O","B-event","I-event","I-event","O","O","B-location","I-location","I-location","O","B-location","O","O","O","B-location","I-location","I-location","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","location","band","event","person","song","musical_artist","album","award","musical_instrument","music_genre","country"]}
{"id":"257","dataset":"crossner_music","split":"test","instance":{"id":"257","prompt_labels":"The(O) first(O) known(O) opera(O) from(O) Turkey(B-country) ((O) the(O) Ottoman(B-country) Empire(I-country) )(O) was(O) Arshak(O) II(O) ,(O) which(O) was(O) an(O) Armenia(B-country) n(O) opera(O) composed(O) by(O) an(O) ethnic(O) Armenian(O) composer(O) Tigran(B-person) Chukhajian(I-person) in(O) 1868(O) and(O) partially(O) performed(O) in(O) 1873(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, musical artist, award, person, location, event, album, country, music genre, musical instrument, band, organization and O.\nSentence: The first known opera from Turkey ( the Ottoman Empire ) was Arshak II , which was an Armenia n opera composed by an ethnic Armenian composer Tigran Chukhajian in 1868 and partially performed in 1873 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","first","known","opera","from","Turkey","(","the","Ottoman","Empire",")","was","Arshak","II",",","which","was","an","Armenia","n","opera","composed","by","an","ethnic","Armenian","composer","Tigran","Chukhajian","in","1868","and","partially","performed","in","1873","."],"labels":["O","O","O","O","O","B-country","O","O","B-country","I-country","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["song","musical_artist","award","person","location","event","album","country","music_genre","musical_instrument","band","organization"]}
{"id":"258","dataset":"crossner_music","split":"test","instance":{"id":"258","prompt_labels":"Hank(B-musical artist) Williams(I-musical artist) ,(I-musical artist) Jr(I-musical artist) ((O) and(O) ,(O) to(O) an(O) even(O) greater(O) extent(O) ,(O) Hank(B-musical artist) Williams(I-musical artist) III(I-musical artist) )(O) ,(O) Gary(B-musical artist) Allan(I-musical artist) ,(O) Shania(B-musical artist) Twain(I-musical artist) ,(O) Brooks(B-band) &(I-band) amp(I-band) ;(I-band) Dunn(I-band) ,(O) Faith(B-musical artist) Hill(I-musical artist) ,(O) Garth(B-musical artist) Brooks(I-musical artist) ,(O) Alan(B-musical artist) Jackson(I-musical artist) ,(O) Dwight(B-musical artist) Yoakam(I-musical artist) ,(O) Steve(B-musical artist) Earle(I-musical artist) ,(O) Dolly(B-musical artist) Parton(I-musical artist) ,(O) Rosanne(B-musical artist) Cash(I-musical artist) and(O) Linda(B-musical artist) Ronstadt(I-musical artist) moved(O) country(O) further(O) towards(O) rock(B-music genre) influence(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, person, country, musical artist, location, song, album, musical instrument, award, music genre, event, organization and O.\nSentence: Hank Williams , Jr ( and , to an even greater extent , Hank Williams III ) , Gary Allan , Shania Twain , Brooks & amp ; Dunn , Faith Hill , Garth Brooks , Alan Jackson , Dwight Yoakam , Steve Earle , Dolly Parton , Rosanne Cash and Linda Ronstadt moved country further towards rock influence .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Hank","Williams",",","Jr","(","and",",","to","an","even","greater","extent",",","Hank","Williams","III",")",",","Gary","Allan",",","Shania","Twain",",","Brooks","&","amp",";","Dunn",",","Faith","Hill",",","Garth","Brooks",",","Alan","Jackson",",","Dwight","Yoakam",",","Steve","Earle",",","Dolly","Parton",",","Rosanne","Cash","and","Linda","Ronstadt","moved","country","further","towards","rock","influence","."],"labels":["B-musical artist","I-musical artist","I-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-band","I-band","I-band","I-band","I-band","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O","B-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["band","person","country","musical_artist","location","song","album","musical_instrument","award","music_genre","event","organization"]}
{"id":"260","dataset":"crossner_music","split":"test","instance":{"id":"260","prompt_labels":"The(O) stadium(O) hosted(O) the(O) 2003(B-event) World(I-event) Championships(I-event) in(I-event) Athletics(I-event) and(O) from(O) 1999(O) to(O) 2016(O) it(O) hosted(O) the(O) annual(O) Meeting(B-event) Areva(I-event) athletics(O) meet(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, location, musical artist, musical instrument, award, band, person, organization, event, album, music genre, country and O.\nSentence: The stadium hosted the 2003 World Championships in Athletics and from 1999 to 2016 it hosted the annual Meeting Areva athletics meet .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","stadium","hosted","the","2003","World","Championships","in","Athletics","and","from","1999","to","2016","it","hosted","the","annual","Meeting","Areva","athletics","meet","."],"labels":["O","O","O","O","B-event","I-event","I-event","I-event","I-event","O","O","O","O","O","O","O","O","O","B-event","I-event","O","O","O"],"target_index":null,"target_label":null},"label_list":["song","location","musical_artist","musical_instrument","award","band","person","organization","event","album","music_genre","country"]}
{"id":"261","dataset":"crossner_music","split":"test","instance":{"id":"261","prompt_labels":"Since(O) the(O) band(O) 's(O) 1960s(O) heyday(O) ,(O) the(O) influence(O) of(O) the(O) Byrds(B-band) on(O) successive(O) generations(O) of(O) Rock(B-music genre) music(I-music genre) and(O) Pop(O) music(O) musicians(O) has(O) grown(O) steadily(O) ,(O) with(O) acts(O) such(O) as(O) the(O) Eagles(B-band) ,(O) Big(B-band) Star(I-band) ,(O) Tom(B-band) Petty(I-band) &(I-band) amp(I-band) ;(I-band) the(I-band) Heartbreakers(I-band) ,(O) R.E.M.(B-band) ,(O) the(B-band) Bangles(I-band) ,(O) the(B-band) Smiths(I-band) ,(O) and(O) innumerable(O) alternative(O) rock(O) bands(O) of(O) the(O) post-(O) Punk(B-music genre) rock(I-music genre) era(O) all(O) exhibiting(O) signs(O) of(O) their(O) influence(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, award, song, band, person, location, album, organization, music genre, musical instrument, event, country and O.\nSentence: Since the band 's 1960s heyday , the influence of the Byrds on successive generations of Rock music and Pop music musicians has grown steadily , with acts such as the Eagles , Big Star , Tom Petty & amp ; the Heartbreakers , R.E.M. , the Bangles , the Smiths , and innumerable alternative rock bands of the post- Punk rock era all exhibiting signs of their influence .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Since","the","band","'s","1960s","heyday",",","the","influence","of","the","Byrds","on","successive","generations","of","Rock","music","and","Pop","music","musicians","has","grown","steadily",",","with","acts","such","as","the","Eagles",",","Big","Star",",","Tom","Petty","&","amp",";","the","Heartbreakers",",","R.E.M.",",","the","Bangles",",","the","Smiths",",","and","innumerable","alternative","rock","bands","of","the","post-","Punk","rock","era","all","exhibiting","signs","of","their","influence","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-band","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","O","B-band","I-band","O","B-band","I-band","I-band","I-band","I-band","I-band","I-band","O","B-band","O","B-band","I-band","O","B-band","I-band","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","award","song","band","person","location","album","organization","music_genre","musical_instrument","event","country"]}
{"id":"262","dataset":"crossner_music","split":"test","instance":{"id":"262","prompt_labels":"The(O) band(O) headlined(O) the(O) Soul(O) Assassins(O) tour(O) with(O) House(B-band) of(I-band) Pain(I-band) and(O) Funkdoobiest(B-band) as(O) support(O) ,(O) then(O) performed(O) on(O) a(O) college(O) tour(O) with(O) Rage(B-band) Against(I-band) the(I-band) Machine(I-band) and(O) Seven(B-band) Year(I-band) Bitch(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical artist, song, band, award, location, music genre, album, organization, musical instrument, country, person and O.\nSentence: The band headlined the Soul Assassins tour with House of Pain and Funkdoobiest as support , then performed on a college tour with Rage Against the Machine and Seven Year Bitch .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","band","headlined","the","Soul","Assassins","tour","with","House","of","Pain","and","Funkdoobiest","as","support",",","then","performed","on","a","college","tour","with","Rage","Against","the","Machine","and","Seven","Year","Bitch","."],"labels":["O","O","O","O","O","O","O","O","B-band","I-band","I-band","O","B-band","O","O","O","O","O","O","O","O","O","O","B-band","I-band","I-band","I-band","O","B-band","I-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["event","musical_artist","song","band","award","location","music_genre","album","organization","musical_instrument","country","person"]}
{"id":"263","dataset":"crossner_music","split":"test","instance":{"id":"263","prompt_labels":"The(O) Elephant(O) Man(O) was(O) a(O) huge(O) critical(O) and(O) commercial(O) success(O) ,(O) and(O) earned(O) eight(O) Academy(B-award) Awards(I-award) nominations(O) ,(O) including(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Adapted(I-award) Screenplay(I-award) for(O) Lynch(B-musical artist) personally(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, album, person, musical artist, award, song, location, country, music genre, band, musical instrument, event and O.\nSentence: The Elephant Man was a huge critical and commercial success , and earned eight Academy Awards nominations , including Academy Award for Best Director and Academy Award for Best Adapted Screenplay for Lynch personally .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Elephant","Man","was","a","huge","critical","and","commercial","success",",","and","earned","eight","Academy","Awards","nominations",",","including","Academy","Award","for","Best","Director","and","Academy","Award","for","Best","Adapted","Screenplay","for","Lynch","personally","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-musical artist","O","O"],"target_index":null,"target_label":null},"label_list":["organization","album","person","musical_artist","award","song","location","country","music_genre","band","musical_instrument","event"]}
{"id":"265","dataset":"crossner_music","split":"test","instance":{"id":"265","prompt_labels":"The(O) set(O) includes(O) the(O) first(O) round(O) of(O) the(O) remastered(O) series(O) plus(O) the(O) long-awaited(O) remastered(O) versions(O) of(O) On(B-album) Your(I-album) Feet(I-album) or(I-album) on(I-album) Your(I-album) Knees(I-album) ((O) 1975(O) )(O) ,(O) Mirrors(B-album) ,(O) Cultsaurus(B-album) Erectus(I-album) ,(O) Fire(B-album) Of(I-album) Unknown(I-album) Origin(I-album) ,(O) Extraterrestrial(B-album) Live(I-album) ,(O) The(B-album) Revlution(I-album) by(I-album) Night(I-album) ,(O) Club(B-album) Ninja(I-album) and(O) Imaginos(B-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical artist, country, award, musical instrument, event, person, location, music genre, song, album, organization and O.\nSentence: The set includes the first round of the remastered series plus the long-awaited remastered versions of On Your Feet or on Your Knees ( 1975 ) , Mirrors , Cultsaurus Erectus , Fire Of Unknown Origin , Extraterrestrial Live , The Revlution by Night , Club Ninja and Imaginos .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","set","includes","the","first","round","of","the","remastered","series","plus","the","long-awaited","remastered","versions","of","On","Your","Feet","or","on","Your","Knees","(","1975",")",",","Mirrors",",","Cultsaurus","Erectus",",","Fire","Of","Unknown","Origin",",","Extraterrestrial","Live",",","The","Revlution","by","Night",",","Club","Ninja","and","Imaginos","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","I-album","I-album","I-album","I-album","O","O","O","O","B-album","O","B-album","I-album","O","B-album","I-album","I-album","I-album","O","B-album","I-album","O","B-album","I-album","I-album","I-album","O","B-album","I-album","O","B-album","O"],"target_index":null,"target_label":null},"label_list":["band","musical_artist","country","award","musical_instrument","event","person","location","music_genre","song","album","organization"]}
{"id":"266","dataset":"crossner_music","split":"test","instance":{"id":"266","prompt_labels":"The(O) musical(O) debuted(O) in(O) San(B-location) Diego(I-location) at(O) the(O) Old(B-organization) Globe(I-organization) Theatre(I-organization) in(O) 1986(O) and(O) premiered(O) on(O) Broadway(B-organization) on(O) November(O) 5(O) ,(O) 1987(O) ,(O) where(O) it(O) won(O) several(O) Tony(B-award) Award(I-award) s(O) ,(O) including(O) Tony(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Score(I-award) ,(O) Tony(B-award) Award(I-award) for(I-award) Best(I-award) Book(I-award) of(I-award) a(I-award) Musical(I-award) ,(O) and(O) Best(B-award) Actress(I-award) in(I-award) a(I-award) Musical(I-award) ((O) Joanna(B-musical artist) Gleason(I-musical artist) )(O) ,(O) in(O) a(O) year(O) dominated(O) by(O) The(O) Phantom(O) of(O) the(O) Opera(O) ((O) 1988(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, event, organization, location, musical artist, country, song, band, music genre, award, musical instrument, person and O.\nSentence: The musical debuted in San Diego at the Old Globe Theatre in 1986 and premiered on Broadway on November 5 , 1987 , where it won several Tony Award s , including Tony Award for Best Original Score , Tony Award for Best Book of a Musical , and Best Actress in a Musical ( Joanna Gleason ) , in a year dominated by The Phantom of the Opera ( 1988 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","musical","debuted","in","San","Diego","at","the","Old","Globe","Theatre","in","1986","and","premiered","on","Broadway","on","November","5",",","1987",",","where","it","won","several","Tony","Award","s",",","including","Tony","Award","for","Best","Original","Score",",","Tony","Award","for","Best","Book","of","a","Musical",",","and","Best","Actress","in","a","Musical","(","Joanna","Gleason",")",",","in","a","year","dominated","by","The","Phantom","of","the","Opera","(","1988",")","."],"labels":["O","O","O","O","B-location","I-location","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","B-organization","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["album","event","organization","location","musical_artist","country","song","band","music_genre","award","musical_instrument","person"]}
{"id":"267","dataset":"crossner_music","split":"test","instance":{"id":"267","prompt_labels":"He(O) has(O) won(O) five(O) Primetime(B-award) Emmy(I-award) Award(I-award) s(O) ,(O) four(O) Golden(B-award) Globe(I-award) Awards(I-award) ,(O) a(O) Grammy(B-award) Award(I-award) ,(O) and(O) two(O) Screen(B-award) Actors(I-award) Guild(I-award) Awards(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, location, person, musical artist, song, country, award, organization, event, album, band, music genre and O.\nSentence: He has won five Primetime Emmy Award s , four Golden Globe Awards , a Grammy Award , and two Screen Actors Guild Awards .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","has","won","five","Primetime","Emmy","Award","s",",","four","Golden","Globe","Awards",",","a","Grammy","Award",",","and","two","Screen","Actors","Guild","Awards","."],"labels":["O","O","O","O","B-award","I-award","I-award","O","O","O","B-award","I-award","I-award","O","O","B-award","I-award","O","O","O","B-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","location","person","musical_artist","song","country","award","organization","event","album","band","music_genre"]}
{"id":"268","dataset":"crossner_music","split":"test","instance":{"id":"268","prompt_labels":"Simon(B-person) Fuller(I-person) has(O) championed(O) many(O) good(O) causes(O) through(O) his(O) working(O) life(O) including(O) The(B-organization) Prince(I-organization) 's(I-organization) Trust(I-organization) ,(O) Amnesty(B-organization) International(I-organization) ,(O) Save(B-organization) the(I-organization) Children(I-organization) ,(O) United(B-organization) Nations(I-organization) Foundation(I-organization) ,(O) UNICEF(B-organization) ,(O) Greenpeace(B-organization) ,(O) World(B-organization) Wide(I-organization) Fund(I-organization) for(I-organization) Nature(I-organization) and(O) Comic(B-organization) Relief(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, music genre, song, album, musical instrument, country, award, band, location, musical artist, person, event and O.\nSentence: Simon Fuller has championed many good causes through his working life including The Prince 's Trust , Amnesty International , Save the Children , United Nations Foundation , UNICEF , Greenpeace , World Wide Fund for Nature and Comic Relief .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Simon","Fuller","has","championed","many","good","causes","through","his","working","life","including","The","Prince","'s","Trust",",","Amnesty","International",",","Save","the","Children",",","United","Nations","Foundation",",","UNICEF",",","Greenpeace",",","World","Wide","Fund","for","Nature","and","Comic","Relief","."],"labels":["B-person","I-person","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","O","B-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["organization","music_genre","song","album","musical_instrument","country","award","band","location","musical_artist","person","event"]}
{"id":"269","dataset":"crossner_music","split":"test","instance":{"id":"269","prompt_labels":"Early(O) in(O) their(O) recording(O) careers(O) ,(O) Hall(B-band) and(I-band) Oates(I-band) had(O) trouble(O) clearly(O) defining(O) their(O) sound(O) ,(O) alternating(O) among(O) Folk(B-music genre) music(I-music genre) ,(O) Soul(B-music genre) music(I-music genre) ,(O) rock(B-music genre) and(O) pop(B-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, musical instrument, music genre, album, award, musical artist, organization, song, band, event, country and O.\nSentence: Early in their recording careers , Hall and Oates had trouble clearly defining their sound , alternating among Folk music , Soul music , rock and pop .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Early","in","their","recording","careers",",","Hall","and","Oates","had","trouble","clearly","defining","their","sound",",","alternating","among","Folk","music",",","Soul","music",",","rock","and","pop","."],"labels":["O","O","O","O","O","O","B-band","I-band","I-band","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","O"],"target_index":null,"target_label":null},"label_list":["person","location","musical_instrument","music_genre","album","award","musical_artist","organization","song","band","event","country"]}
{"id":"270","dataset":"crossner_music","split":"test","instance":{"id":"270","prompt_labels":"The(O) mid(O) 2000s(O) ,(O) especially(O) the(O) United(B-country) Kingdom(I-country) and(O) the(O) rest(O) of(O) Europe(B-location) ,(O) saw(O) the(O) continued(O) longevity(O) of(O) nineties(O) boy(O) bands(O) such(O) as(O) Backstreet(B-band) Boys(I-band) and(O) Westlife(B-band) ((O) before(O) they(O) disbanded(O) in(O) 2012(O) )(O) ,(O) and(O) the(O) successful(O) comeback(O) of(O) Take(B-band) That(I-band) in(O) 2005(O) ,(O) Boyzone(B-band) in(O) 2007(O) ,(O) and(O) New(B-band) Kids(I-band) on(I-band) the(I-band) Block(I-band) in(O) 2008(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, country, organization, music genre, band, location, song, musical instrument, award, album, person, event and O.\nSentence: The mid 2000s , especially the United Kingdom and the rest of Europe , saw the continued longevity of nineties boy bands such as Backstreet Boys and Westlife ( before they disbanded in 2012 ) , and the successful comeback of Take That in 2005 , Boyzone in 2007 , and New Kids on the Block in 2008 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","mid","2000s",",","especially","the","United","Kingdom","and","the","rest","of","Europe",",","saw","the","continued","longevity","of","nineties","boy","bands","such","as","Backstreet","Boys","and","Westlife","(","before","they","disbanded","in","2012",")",",","and","the","successful","comeback","of","Take","That","in","2005",",","Boyzone","in","2007",",","and","New","Kids","on","the","Block","in","2008","."],"labels":["O","O","O","O","O","O","B-country","I-country","O","O","O","O","B-location","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","O","B-band","O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","O","O","O","B-band","O","O","O","O","B-band","I-band","I-band","I-band","I-band","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","country","organization","music_genre","band","location","song","musical_instrument","award","album","person","event"]}
{"id":"271","dataset":"crossner_music","split":"test","instance":{"id":"271","prompt_labels":"The(O) film(O) received(O) Academy(B-award) Awards(I-award) nominations(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) ((O) producer(O) Lawrence(B-person) Turman(I-person) )(O) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) ((O) Dustin(B-person) Hoffman(I-person) )(O) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) ((O) Anne(B-person) Bancroft(I-person) )(O) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Supporting(I-award) Actress(I-award) ((O) Katharine(B-person) Ross(I-person) )(O) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Adapted(I-award) Screenplay(I-award) ((O) Buck(B-person) Henry(I-person) and(O) Calder(B-person) Willingham(I-person) )(O) ,(O) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Cinematography(I-award) ((O) Robert(B-person) L.(I-person) Surtees(I-person) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical artist, award, person, organization, musical instrument, album, music genre, country, location, band, song and O.\nSentence: The film received Academy Awards nominations for Academy Award for Best Picture ( producer Lawrence Turman ) , Academy Award for Best Actor ( Dustin Hoffman ) , Academy Award for Best Actress ( Anne Bancroft ) , Academy Award for Best Supporting Actress ( Katharine Ross ) , Academy Award for Best Adapted Screenplay ( Buck Henry and Calder Willingham ) , and Academy Award for Best Cinematography ( Robert L. Surtees ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","film","received","Academy","Awards","nominations","for","Academy","Award","for","Best","Picture","(","producer","Lawrence","Turman",")",",","Academy","Award","for","Best","Actor","(","Dustin","Hoffman",")",",","Academy","Award","for","Best","Actress","(","Anne","Bancroft",")",",","Academy","Award","for","Best","Supporting","Actress","(","Katharine","Ross",")",",","Academy","Award","for","Best","Adapted","Screenplay","(","Buck","Henry","and","Calder","Willingham",")",",","and","Academy","Award","for","Best","Cinematography","(","Robert","L.","Surtees",")","."],"labels":["O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O","O","B-person","I-person","O","O","B-award","I-award","I-award","I-award","I-award","O","B-person","I-person","O","O","B-award","I-award","I-award","I-award","I-award","O","B-person","I-person","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-person","I-person","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-person","I-person","O","B-person","I-person","O","O","O","B-award","I-award","I-award","I-award","I-award","O","B-person","I-person","I-person","O","O"],"target_index":null,"target_label":null},"label_list":["event","musical_artist","award","person","organization","musical_instrument","album","music_genre","country","location","band","song"]}
{"id":"273","dataset":"crossner_music","split":"test","instance":{"id":"273","prompt_labels":"A(O) few(O) of(O) those(O) include(O) bagad(B-band) ((O) Breton(O) pipe(O) bands(O) )(O) ,(O) Fairport(B-band) Convention(I-band) ,(O) Pentangle(B-band) ,(O) Steeleye(B-band) Span(I-band) and(O) Horslips(B-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, musical instrument, album, location, event, band, song, musical artist, music genre, country, person, award and O.\nSentence: A few of those include bagad ( Breton pipe bands ) , Fairport Convention , Pentangle , Steeleye Span and Horslips .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","few","of","those","include","bagad","(","Breton","pipe","bands",")",",","Fairport","Convention",",","Pentangle",",","Steeleye","Span","and","Horslips","."],"labels":["O","O","O","O","O","B-band","O","O","O","O","O","O","B-band","I-band","O","B-band","O","B-band","I-band","O","B-band","O"],"target_index":null,"target_label":null},"label_list":["organization","musical_instrument","album","location","event","band","song","musical_artist","music_genre","country","person","award"]}
{"id":"275","dataset":"crossner_music","split":"test","instance":{"id":"275","prompt_labels":"The(O) British(O) and(O) blues(B-music genre) musicians(O) of(O) the(O) early(O) 1960s(O) inspired(O) a(O) number(O) of(O) American(O) blues(B-music genre) rock(I-music genre) fusion(I-music genre) performers(O) ,(O) including(O) the(O) The(B-band) Doors(I-band) ,(O) Canned(B-band) Heat(I-band) ,(O) the(O) early(O) Jefferson(B-band) Airplane(I-band) ,(O) Janis(B-person) Joplin(I-person) ,(O) Johnny(B-musical artist) Winter(I-musical artist) ,(O) The(B-band) J.(I-band) Geils(I-band) Band(I-band) ,(O) Ry(B-musical artist) Cooder(I-musical artist) ,(O) and(O) the(O) Allman(B-band) Brothers(I-band) Band(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical instrument, location, music genre, musical artist, album, organization, person, event, country, song, award and O.\nSentence: The British and blues musicians of the early 1960s inspired a number of American blues rock fusion performers , including the The Doors , Canned Heat , the early Jefferson Airplane , Janis Joplin , Johnny Winter , The J. Geils Band , Ry Cooder , and the Allman Brothers Band .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","British","and","blues","musicians","of","the","early","1960s","inspired","a","number","of","American","blues","rock","fusion","performers",",","including","the","The","Doors",",","Canned","Heat",",","the","early","Jefferson","Airplane",",","Janis","Joplin",",","Johnny","Winter",",","The","J.","Geils","Band",",","Ry","Cooder",",","and","the","Allman","Brothers","Band","."],"labels":["O","O","O","B-music genre","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","I-music genre","O","O","O","O","B-band","I-band","O","B-band","I-band","O","O","O","B-band","I-band","O","B-person","I-person","O","B-musical artist","I-musical artist","O","B-band","I-band","I-band","I-band","O","B-musical artist","I-musical artist","O","O","O","B-band","I-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["band","musical_instrument","location","music_genre","musical_artist","album","organization","person","event","country","song","award"]}
{"id":"277","dataset":"crossner_music","split":"test","instance":{"id":"277","prompt_labels":"Respected(O) for(O) her(O) versatility(O) ,(O) she(O) received(O) an(O) Academy(B-award) Juvenile(I-award) Award(I-award) ,(O) a(O) Golden(B-award) Globe(I-award) Award(I-award) ,(O) a(O) Special(B-award) Tony(I-award) Award(I-award) ,(O) and(O) was(O) the(O) first(O) woman(O) to(O) win(O) the(O) Grammy(B-award) Award(I-award) for(I-award) Album(I-award) of(I-award) the(I-award) Year(I-award) for(O) her(O) 1961(O) live(O) recording(O) Judy(B-album) at(I-album) Carnegie(I-album) Hall(I-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, location, album, musical instrument, music genre, musical artist, event, award, song, person, band and O.\nSentence: Respected for her versatility , she received an Academy Juvenile Award , a Golden Globe Award , a Special Tony Award , and was the first woman to win the Grammy Award for Album of the Year for her 1961 live recording Judy at Carnegie Hall .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Respected","for","her","versatility",",","she","received","an","Academy","Juvenile","Award",",","a","Golden","Globe","Award",",","a","Special","Tony","Award",",","and","was","the","first","woman","to","win","the","Grammy","Award","for","Album","of","the","Year","for","her","1961","live","recording","Judy","at","Carnegie","Hall","."],"labels":["O","O","O","O","O","O","O","O","B-award","I-award","I-award","O","O","B-award","I-award","I-award","O","O","B-award","I-award","I-award","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","B-album","I-album","I-album","I-album","O"],"target_index":null,"target_label":null},"label_list":["country","organization","location","album","musical_instrument","music_genre","musical_artist","event","award","song","person","band"]}
{"id":"278","dataset":"crossner_music","split":"test","instance":{"id":"278","prompt_labels":"However(O) ,(O) the(O) 2001(O) release(O) Destroyer(B-album) of(I-album) Worlds(I-album) was(O) a(O) transitional(O) release(O) that(O) led(O) to(O) a(O) full(O) return(O) to(O) the(O) Viking(B-music genre) metal(I-music genre) style(O) with(O) the(O) releases(O) of(O) Nordland(B-album) I(I-album) ((O) 2002(O) )(O) and(O) Nordland(B-album) II(I-album) ((O) 2003(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, musical artist, event, album, award, musical instrument, location, organization, music genre, band, song and O.\nSentence: However , the 2001 release Destroyer of Worlds was a transitional release that led to a full return to the Viking metal style with the releases of Nordland I ( 2002 ) and Nordland II ( 2003 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["However",",","the","2001","release","Destroyer","of","Worlds","was","a","transitional","release","that","led","to","a","full","return","to","the","Viking","metal","style","with","the","releases","of","Nordland","I","(","2002",")","and","Nordland","II","(","2003",")","."],"labels":["O","O","O","O","O","B-album","I-album","I-album","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","B-album","I-album","O","O","O","O","B-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","country","musical_artist","event","album","award","musical_instrument","location","organization","music_genre","band","song"]}
{"id":"279","dataset":"crossner_music","split":"test","instance":{"id":"279","prompt_labels":"Black(B-album) Messiah(I-album) ,(O) Beyonc(B-musical artist) '(O) s(O) self-titled(O) album(O) ((O) 2013(O) )(O) ,(O) Run(B-band) the(I-band) Jewels(I-band) '(O) Run(B-album) the(I-album) Jewels(I-album) 2(I-album) ((O) 2014(O) )(O) ,(O) and(O) Kendrick(B-musical artist) Lamar(I-musical artist) '(O) s(O) To(B-album) Pimp(I-album) a(I-album) Butterfly(I-album) ((O) 2015(O) )(O) were(O) noted(O) as(O) laying(O) the(O) groundwork(O) down(O) for(O) the(O) politically(O) charged(O) releases(O) that(O) happened(O) in(O) 2016(O) ,(O) which(O) included(O) Rihanna(B-musical artist) '(O) s(O) Anti(B-album) ,(O) Kanye(B-musical artist) West(I-musical artist) 's(O) The(B-album) Life(I-album) of(I-album) Pablo(I-album) ,(O) and(O) Beyonce(B-musical artist) 's(O) Formation(B-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, organization, country, album, band, song, musical artist, music genre, musical instrument, event, location, person and O.\nSentence: Black Messiah , Beyonc ' s self-titled album ( 2013 ) , Run the Jewels ' Run the Jewels 2 ( 2014 ) , and Kendrick Lamar ' s To Pimp a Butterfly ( 2015 ) were noted as laying the groundwork down for the politically charged releases that happened in 2016 , which included Rihanna ' s Anti , Kanye West 's The Life of Pablo , and Beyonce 's Formation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Black","Messiah",",","Beyonc","'","s","self-titled","album","(","2013",")",",","Run","the","Jewels","'","Run","the","Jewels","2","(","2014",")",",","and","Kendrick","Lamar","'","s","To","Pimp","a","Butterfly","(","2015",")","were","noted","as","laying","the","groundwork","down","for","the","politically","charged","releases","that","happened","in","2016",",","which","included","Rihanna","'","s","Anti",",","Kanye","West","'s","The","Life","of","Pablo",",","and","Beyonce","'s","Formation","."],"labels":["B-album","I-album","O","B-musical artist","O","O","O","O","O","O","O","O","B-band","I-band","I-band","O","B-album","I-album","I-album","I-album","O","O","O","O","O","B-musical artist","I-musical artist","O","O","B-album","I-album","I-album","I-album","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","O","O","B-album","O","B-musical artist","I-musical artist","O","B-album","I-album","I-album","I-album","O","O","B-musical artist","O","B-album","O"],"target_index":null,"target_label":null},"label_list":["award","organization","country","album","band","song","musical_artist","music_genre","musical_instrument","event","location","person"]}
{"id":"282","dataset":"crossner_music","split":"test","instance":{"id":"282","prompt_labels":"Since(O) 2001(O) ,(O) the(O) dominance(O) of(O) traditional(O) boy(O) bands(O) on(O) pop(B-music genre) charts(O) began(O) to(O) fade(O) in(O) the(O) western(O) hemisphere(O) ,(O) although(O) Gil(B-person) Kaufman(I-person) of(O) MTV(B-organization) has(O) described(O) new(O) boy(O) bands(O) that(O) are(O) more(O) likely(O) to(O) resemble(O) My(B-band) Chemical(I-band) Romance(I-band) ,(O) Sum(B-band) 41(I-band) ,(O) and(O) Simple(B-band) Plan(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical artist, country, organization, award, musical instrument, song, music genre, location, person, event, album and O.\nSentence: Since 2001 , the dominance of traditional boy bands on pop charts began to fade in the western hemisphere , although Gil Kaufman of MTV has described new boy bands that are more likely to resemble My Chemical Romance , Sum 41 , and Simple Plan .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Since","2001",",","the","dominance","of","traditional","boy","bands","on","pop","charts","began","to","fade","in","the","western","hemisphere",",","although","Gil","Kaufman","of","MTV","has","described","new","boy","bands","that","are","more","likely","to","resemble","My","Chemical","Romance",",","Sum","41",",","and","Simple","Plan","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-music genre","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","I-band","O","B-band","I-band","O","O","B-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["band","musical_artist","country","organization","award","musical_instrument","song","music_genre","location","person","event","album"]}
{"id":"284","dataset":"crossner_music","split":"test","instance":{"id":"284","prompt_labels":"Taiko(B-musical instrument) have(O) a(O) mythological(O) origin(O) in(O) Japanese(O) folklore(O) ,(O) but(O) historical(O) records(O) suggest(O) that(O) taiko(B-musical instrument) were(O) introduced(O) to(O) Japan(B-country) through(O) Korea(B-country) n(O) and(O) China(B-country) cultural(O) influence(O) as(O) early(O) as(O) the(O) 6th(O) century(O) CE(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, person, musical instrument, musical artist, event, music genre, song, country, location, award, band, organization and O.\nSentence: Taiko have a mythological origin in Japanese folklore , but historical records suggest that taiko were introduced to Japan through Korea n and China cultural influence as early as the 6th century CE .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Taiko","have","a","mythological","origin","in","Japanese","folklore",",","but","historical","records","suggest","that","taiko","were","introduced","to","Japan","through","Korea","n","and","China","cultural","influence","as","early","as","the","6th","century","CE","."],"labels":["B-musical instrument","O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical instrument","O","O","O","B-country","O","B-country","O","O","B-country","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["album","person","musical_instrument","musical_artist","event","music_genre","song","country","location","award","band","organization"]}
{"id":"285","dataset":"crossner_music","split":"test","instance":{"id":"285","prompt_labels":"Focusing(O) on(O) live(O) performance(O) in(O) Scandinavia(B-location) ,(O) Rednex(B-band) appeared(O) at(O) the(O) 2005(B-event) World(I-event) Championships(I-event) in(I-event) Athletics(I-event) IAAF(B-event) World(I-event) Championships(I-event) in(I-event) Athletics(I-event) ,(O) in(O) Helsinki(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, band, event, musical instrument, person, country, music genre, organization, song, award, musical artist, location and O.\nSentence: Focusing on live performance in Scandinavia , Rednex appeared at the 2005 World Championships in Athletics IAAF World Championships in Athletics , in Helsinki .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Focusing","on","live","performance","in","Scandinavia",",","Rednex","appeared","at","the","2005","World","Championships","in","Athletics","IAAF","World","Championships","in","Athletics",",","in","Helsinki","."],"labels":["O","O","O","O","O","B-location","O","B-band","O","O","O","B-event","I-event","I-event","I-event","I-event","B-event","I-event","I-event","I-event","I-event","O","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["album","band","event","musical_instrument","person","country","music_genre","organization","song","award","musical_artist","location"]}
{"id":"289","dataset":"crossner_music","split":"test","instance":{"id":"289","prompt_labels":"It(O) won(O) two(O) Academy(B-award) Awards(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Score(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Song(I-award) for(O) A(B-song) Whole(I-song) New(I-song) World(I-song) and(O) receiving(O) nominations(O) for(O) Best(B-award) Song(I-award) ((O) Friend(B-song) Like(I-song) Me(I-song) )(O) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Sound(I-award) Editing(I-award) ((O) Mark(B-musical artist) A.(I-musical artist) Mangini(I-musical artist) )(O) ,(O) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Sound(I-award) Mixing(I-award) ((O) Terry(B-musical artist) Porter(I-musical artist) ,(O) Mel(B-musical artist) Metcalfe(I-musical artist) ,(O) David(B-musical artist) J.(I-musical artist) Hudson(I-musical artist) and(O) Doc(B-musical artist) Kane(I-musical artist) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, band, person, musical artist, organization, award, song, album, event, musical instrument, music genre and O.\nSentence: It won two Academy Awards , Academy Award for Best Original Score and Academy Award for Best Original Song for A Whole New World and receiving nominations for Best Song ( Friend Like Me ) , Academy Award for Best Sound Editing ( Mark A. Mangini ) , and Academy Award for Best Sound Mixing ( Terry Porter , Mel Metcalfe , David J. Hudson and Doc Kane ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","won","two","Academy","Awards",",","Academy","Award","for","Best","Original","Score","and","Academy","Award","for","Best","Original","Song","for","A","Whole","New","World","and","receiving","nominations","for","Best","Song","(","Friend","Like","Me",")",",","Academy","Award","for","Best","Sound","Editing","(","Mark","A.","Mangini",")",",","and","Academy","Award","for","Best","Sound","Mixing","(","Terry","Porter",",","Mel","Metcalfe",",","David","J.","Hudson","and","Doc","Kane",")","."],"labels":["O","O","O","B-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-song","I-song","I-song","I-song","O","O","O","O","B-award","I-award","O","B-song","I-song","I-song","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-musical artist","I-musical artist","I-musical artist","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O"],"target_index":null,"target_label":null},"label_list":["location","country","band","person","musical_artist","organization","award","song","album","event","musical_instrument","music_genre"]}
{"id":"291","dataset":"crossner_music","split":"test","instance":{"id":"291","prompt_labels":"Highlights(O) in(O) 2002(O) included(O) the(O) premieres(O) of(O) several(O) major(O) works(O) ,(O) including(O) View(B-album) From(I-album) Olympus(I-album) a(O) double(O) concerto(O) for(O) piano(B-musical instrument) ,(O) percussion(O) and(O) orchestra(O) performed(O) by(O) Evelyn(B-musical artist) Glennie(I-musical artist) ,(O) Philip(B-musical artist) Smith(I-musical artist) and(O) the(O) Halle(B-band) Orchestra(I-band) conducted(O) by(O) Mark(B-person) Elder(I-person) at(O) the(O) Royal(B-event) Gala(I-event) finale(I-event) of(O) the(O) 2002(B-event) Commonwealth(I-event) Games(I-event) '(O) Pulse(B-event) '(I-event) music(I-event) festival(I-event) in(O) Manchester(B-location) ,(O) UK(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, location, song, country, album, musical instrument, event, band, person, musical artist, music genre and O.\nSentence: Highlights in 2002 included the premieres of several major works , including View From Olympus a double concerto for piano , percussion and orchestra performed by Evelyn Glennie , Philip Smith and the Halle Orchestra conducted by Mark Elder at the Royal Gala finale of the 2002 Commonwealth Games ' Pulse ' music festival in Manchester , UK .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Highlights","in","2002","included","the","premieres","of","several","major","works",",","including","View","From","Olympus","a","double","concerto","for","piano",",","percussion","and","orchestra","performed","by","Evelyn","Glennie",",","Philip","Smith","and","the","Halle","Orchestra","conducted","by","Mark","Elder","at","the","Royal","Gala","finale","of","the","2002","Commonwealth","Games","'","Pulse","'","music","festival","in","Manchester",",","UK","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O","B-musical instrument","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","B-band","I-band","O","O","B-person","I-person","O","O","B-event","I-event","I-event","O","O","B-event","I-event","I-event","O","B-event","I-event","I-event","I-event","O","B-location","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["organization","award","location","song","country","album","musical_instrument","event","band","person","musical_artist","music_genre"]}
{"id":"293","dataset":"crossner_music","split":"test","instance":{"id":"293","prompt_labels":"These(O) have(O) been(O) used(O) in(O) clay(O) shooting(O) and(O) were(O) suggested(O) for(O) use(O) in(O) the(O) Modern(B-event) pentathlon(I-event) at(I-event) the(I-event) 2012(I-event) Summer(I-event) Olympics(I-event) after(O) a(O) successful(O) trial(O) at(O) the(O) 2010(B-event) Summer(I-event) Youth(I-event) Olympics(I-event) in(O) Singapore(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, album, location, country, music genre, event, band, organization, musical instrument, person, award, musical artist and O.\nSentence: These have been used in clay shooting and were suggested for use in the Modern pentathlon at the 2012 Summer Olympics after a successful trial at the 2010 Summer Youth Olympics in Singapore .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["These","have","been","used","in","clay","shooting","and","were","suggested","for","use","in","the","Modern","pentathlon","at","the","2012","Summer","Olympics","after","a","successful","trial","at","the","2010","Summer","Youth","Olympics","in","Singapore","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","I-event","I-event","O","O","O","O","O","O","B-event","I-event","I-event","I-event","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["song","album","location","country","music_genre","event","band","organization","musical_instrument","person","award","musical_artist"]}
{"id":"294","dataset":"crossner_music","split":"test","instance":{"id":"294","prompt_labels":"On(O) September(O) 12(O) ,(O) 2010(O) ,(O) during(O) the(O) Chilean(B-event) bicentennial(I-event) festivities(I-event) ,(O) President(O) Sebastin(B-person) Piera(I-person) announced(O) that(O) the(O) capacity(O) of(O) the(O) stadium(O) will(O) be(O) increased(O) so(O) as(O) to(O) reach(O) 70,000(O) seats(O) for(O) the(O) 2014(B-event) South(I-event) American(I-event) Games(I-event) that(O) will(O) take(O) place(O) in(O) Santiago(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, award, band, location, person, musical instrument, song, musical artist, event, music genre, organization, country and O.\nSentence: On September 12 , 2010 , during the Chilean bicentennial festivities , President Sebastin Piera announced that the capacity of the stadium will be increased so as to reach 70,000 seats for the 2014 South American Games that will take place in Santiago .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","September","12",",","2010",",","during","the","Chilean","bicentennial","festivities",",","President","Sebastin","Piera","announced","that","the","capacity","of","the","stadium","will","be","increased","so","as","to","reach","70,000","seats","for","the","2014","South","American","Games","that","will","take","place","in","Santiago","."],"labels":["O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","O","O","O","O","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["album","award","band","location","person","musical_instrument","song","musical_artist","event","music_genre","organization","country"]}
{"id":"295","dataset":"crossner_music","split":"test","instance":{"id":"295","prompt_labels":"The(O) subsequent(O) albums(O) The(B-album) Clones(I-album) of(I-album) Dr.(I-album) Funkenstein(I-album) ((O) 1976(O) )(O) ,(O) Funkentelechy(B-album) vs.(I-album) the(I-album) Placebo(I-album) Syndrome(I-album) ((O) 1977(O) )(O) ,(O) and(O) Motor(B-album) Booty(I-album) Affair(I-album) ((O) 1978(O) )(O) all(O) reached(O) high(O) on(O) both(O) the(O) R(B-music genre) &(I-music genre) amp(I-music genre) ;(I-music genre) B(I-music genre) and(O) Pop(B-music genre) charts(O) ,(O) while(O) Funkadelic(B-band) was(O) also(O) experiencing(O) significant(O) mainstream(O) success(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, musical instrument, musical artist, country, band, event, song, music genre, award, album, person, organization and O.\nSentence: The subsequent albums The Clones of Dr. Funkenstein ( 1976 ) , Funkentelechy vs. the Placebo Syndrome ( 1977 ) , and Motor Booty Affair ( 1978 ) all reached high on both the R & amp ; B and Pop charts , while Funkadelic was also experiencing significant mainstream success .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","subsequent","albums","The","Clones","of","Dr.","Funkenstein","(","1976",")",",","Funkentelechy","vs.","the","Placebo","Syndrome","(","1977",")",",","and","Motor","Booty","Affair","(","1978",")","all","reached","high","on","both","the","R","&","amp",";","B","and","Pop","charts",",","while","Funkadelic","was","also","experiencing","significant","mainstream","success","."],"labels":["O","O","O","B-album","I-album","I-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","I-album","I-album","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","I-music genre","I-music genre","I-music genre","O","B-music genre","O","O","O","B-band","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","musical_instrument","musical_artist","country","band","event","song","music_genre","award","album","person","organization"]}
{"id":"296","dataset":"crossner_music","split":"test","instance":{"id":"296","prompt_labels":"Deep(B-band) Purple(I-band) and(O) Whitesnake(B-band) '(O) s(O) David(B-musical artist) Coverdale(I-musical artist) ,(O) Samson(B-band) '(O) s(O) Nicky(B-musical artist) Moore(I-musical artist) and(O) Lone(B-band) Star(I-band) '(O) s(O) John(B-musical artist) Sloman(I-musical artist) were(O) all(O) considered(O) and(O) Iommi(B-musical artist) states(O) in(O) his(O) autobiography(O) that(O) Michael(B-musical artist) Bolton(I-musical artist) auditioned(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, event, country, organization, award, song, album, location, musical artist, musical instrument, person, music genre and O.\nSentence: Deep Purple and Whitesnake ' s David Coverdale , Samson ' s Nicky Moore and Lone Star ' s John Sloman were all considered and Iommi states in his autobiography that Michael Bolton auditioned .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Deep","Purple","and","Whitesnake","'","s","David","Coverdale",",","Samson","'","s","Nicky","Moore","and","Lone","Star","'","s","John","Sloman","were","all","considered","and","Iommi","states","in","his","autobiography","that","Michael","Bolton","auditioned","."],"labels":["B-band","I-band","O","B-band","O","O","B-musical artist","I-musical artist","O","B-band","O","O","B-musical artist","I-musical artist","O","B-band","I-band","O","O","B-musical artist","I-musical artist","O","O","O","O","B-musical artist","O","O","O","O","O","B-musical artist","I-musical artist","O","O"],"target_index":null,"target_label":null},"label_list":["band","event","country","organization","award","song","album","location","musical_artist","musical_instrument","person","music_genre"]}
{"id":"297","dataset":"crossner_music","split":"test","instance":{"id":"297","prompt_labels":"Indie(O) band(O) Rilo(B-band) Kiley(I-band) ,(O) in(O) keeping(O) with(O) their(O) tendency(O) to(O) explore(O) a(O) variety(O) of(O) rockish(O) styles(O) ,(O) incorporated(O) funk(B-music genre) into(O) their(O) song(O) The(B-song) Moneymaker(I-song) on(O) the(O) album(O) Under(B-album) the(I-album) Blacklight(I-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, band, album, music genre, musical instrument, award, person, organization, location, country, musical artist, song and O.\nSentence: Indie band Rilo Kiley , in keeping with their tendency to explore a variety of rockish styles , incorporated funk into their song The Moneymaker on the album Under the Blacklight .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Indie","band","Rilo","Kiley",",","in","keeping","with","their","tendency","to","explore","a","variety","of","rockish","styles",",","incorporated","funk","into","their","song","The","Moneymaker","on","the","album","Under","the","Blacklight","."],"labels":["O","O","B-band","I-band","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","O","O","O","B-song","I-song","O","O","O","B-album","I-album","I-album","O"],"target_index":null,"target_label":null},"label_list":["event","band","album","music_genre","musical_instrument","award","person","organization","location","country","musical_artist","song"]}
{"id":"298","dataset":"crossner_music","split":"test","instance":{"id":"298","prompt_labels":"The(O) box(O) set(O) was(O) accompanied(O) by(O) 5(B-album) Album(I-album) Studio(I-album) Set(I-album) ,(O) which(O) contains(O) only(O) the(O) first(O) five(O) studio(O) albums(O) ((O) excluding(O) Cut(B-album) the(I-album) Crap(I-album) )(O) ,(O) and(O) The(B-album) Clash(I-album) Hits(I-album) Back(I-album) ,(O) a(O) 33-track(O) ,(O) two-CD(O) best(O) of(O) collection(O) sequenced(O) to(O) copy(O) the(O) set(O) played(O) by(O) the(O) band(O) at(O) the(O) Brixton(B-organization) Fair(I-organization) Deal(I-organization) ((O) now(O) the(O) Academy(O) )(O) on(O) 19(O) July(O) 1982(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, musical instrument, album, band, event, award, song, person, organization, location, music genre, musical artist and O.\nSentence: The box set was accompanied by 5 Album Studio Set , which contains only the first five studio albums ( excluding Cut the Crap ) , and The Clash Hits Back , a 33-track , two-CD best of collection sequenced to copy the set played by the band at the Brixton Fair Deal ( now the Academy ) on 19 July 1982 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","box","set","was","accompanied","by","5","Album","Studio","Set",",","which","contains","only","the","first","five","studio","albums","(","excluding","Cut","the","Crap",")",",","and","The","Clash","Hits","Back",",","a","33-track",",","two-CD","best","of","collection","sequenced","to","copy","the","set","played","by","the","band","at","the","Brixton","Fair","Deal","(","now","the","Academy",")","on","19","July","1982","."],"labels":["O","O","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","musical_instrument","album","band","event","award","song","person","organization","location","music_genre","musical_artist"]}
{"id":"299","dataset":"crossner_music","split":"test","instance":{"id":"299","prompt_labels":"Six(O) albums(O) released(O) by(O) the(O) label(O) have(O) been(O) certified(O) gold(O) for(O) sales(O) of(O) 500,000(O) copies(O) :(O) Oh(B-album) ,(I-album) Inverted(I-album) World(I-album) ,(O) Chutes(B-album) Too(I-album) Narrow(I-album) and(O) Wincing(B-album) the(I-album) Night(I-album) Away(I-album) ,(O) all(O) by(O) The(B-band) Shins(I-band) ,(O) Fleet(B-band) Foxes(I-band) by(O) Fleet(B-band) Foxes(I-band) ,(O) The(B-band) Head(I-band) and(I-band) the(I-band) Heart(I-band) by(O) The(B-band) Head(I-band) and(I-band) the(I-band) Heart(I-band) ,(O) and(O) Everything(B-album) All(I-album) the(I-album) Time(I-album) by(O) Band(B-band) of(I-band) Horses(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, song, music genre, event, band, organization, musical instrument, person, location, country, album, award and O.\nSentence: Six albums released by the label have been certified gold for sales of 500,000 copies : Oh , Inverted World , Chutes Too Narrow and Wincing the Night Away , all by The Shins , Fleet Foxes by Fleet Foxes , The Head and the Heart by The Head and the Heart , and Everything All the Time by Band of Horses .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Six","albums","released","by","the","label","have","been","certified","gold","for","sales","of","500,000","copies",":","Oh",",","Inverted","World",",","Chutes","Too","Narrow","and","Wincing","the","Night","Away",",","all","by","The","Shins",",","Fleet","Foxes","by","Fleet","Foxes",",","The","Head","and","the","Heart","by","The","Head","and","the","Heart",",","and","Everything","All","the","Time","by","Band","of","Horses","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","I-album","O","B-album","I-album","I-album","O","B-album","I-album","I-album","I-album","O","O","O","B-band","I-band","O","B-band","I-band","O","B-band","I-band","O","B-band","I-band","I-band","I-band","I-band","O","B-band","I-band","I-band","I-band","I-band","O","O","B-album","I-album","I-album","I-album","O","B-band","I-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","song","music_genre","event","band","organization","musical_instrument","person","location","country","album","award"]}
{"id":"300","dataset":"crossner_music","split":"test","instance":{"id":"300","prompt_labels":"In(O) the(O) Southwestern(B-location) United(I-location) States(I-location) ,(O) it(O) was(O) the(O) Rocky(B-location) Mountains(I-location) ,(O) American(B-location) frontier(I-location) ,(O) and(O) Rio(B-location) Grande(I-location) that(O) acted(O) as(O) a(O) similar(O) backdrop(O) for(O) Indigenous(B-music genre) music(I-music genre) of(I-music genre) North(I-music genre) America(I-music genre) ,(O) Mexican(O) ,(O) and(O) cowboy(O) ballads(O) ,(O) which(O) resulted(O) in(O) New(B-music genre) Mexico(I-music genre) music(I-music genre) and(O) the(O) development(O) of(O) Western(O) music(O) ,(O) and(O) its(O) directly(O) related(O) Red(B-music genre) Dirt(I-music genre) ,(O) Texas(B-music genre) country(I-music genre) music(I-music genre) ,(O) and(O) Tejano(B-music genre) music(I-music genre) styles(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, musical artist, event, musical instrument, person, music genre, band, song, organization, country, album, location and O.\nSentence: In the Southwestern United States , it was the Rocky Mountains , American frontier , and Rio Grande that acted as a similar backdrop for Indigenous music of North America , Mexican , and cowboy ballads , which resulted in New Mexico music and the development of Western music , and its directly related Red Dirt , Texas country music , and Tejano music styles .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","Southwestern","United","States",",","it","was","the","Rocky","Mountains",",","American","frontier",",","and","Rio","Grande","that","acted","as","a","similar","backdrop","for","Indigenous","music","of","North","America",",","Mexican",",","and","cowboy","ballads",",","which","resulted","in","New","Mexico","music","and","the","development","of","Western","music",",","and","its","directly","related","Red","Dirt",",","Texas","country","music",",","and","Tejano","music","styles","."],"labels":["O","O","B-location","I-location","I-location","O","O","O","O","B-location","I-location","O","B-location","I-location","O","O","B-location","I-location","O","O","O","O","O","O","O","B-music genre","I-music genre","I-music genre","I-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","O","B-music genre","I-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["award","musical_artist","event","musical_instrument","person","music_genre","band","song","organization","country","album","location"]}
{"id":"301","dataset":"crossner_music","split":"test","instance":{"id":"301","prompt_labels":"The(O) picture(O) received(O) Academy(B-award) Awards(I-award) nominations(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Sound(I-award) Mixing(I-award) ((O) Loren(B-musical artist) L.(I-musical artist) Ryder(I-musical artist) )(O) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Screenplay(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, event, location, music genre, musical instrument, song, band, person, album, award, country, organization and O.\nSentence: The picture received Academy Awards nominations for Academy Award for Best Sound Mixing ( Loren L. Ryder ) and Academy Award for Best Original Screenplay .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","picture","received","Academy","Awards","nominations","for","Academy","Award","for","Best","Sound","Mixing","(","Loren","L.","Ryder",")","and","Academy","Award","for","Best","Original","Screenplay","."],"labels":["O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-musical artist","I-musical artist","I-musical artist","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","event","location","music_genre","musical_instrument","song","band","person","album","award","country","organization"]}
{"id":"302","dataset":"crossner_music","split":"test","instance":{"id":"302","prompt_labels":"By(O) the(O) mid-1980s(O) ,(O) bands(O) began(O) proliferating(O) and(O) became(O) increasingly(O) popular(O) ,(O) including(O) the(B-band) Sisters(I-band) of(I-band) Mercy(I-band) ,(O) the(B-band) Mission(I-band) ,(O) Alien(B-band) Sex(I-band) Fiend(I-band) ,(O) the(B-band) March(I-band) Violets(I-band) ,(O) Xmal(B-band) Deutschland(I-band) ,(O) the(B-band) Membranes(I-band) ,(O) and(O) Fields(B-band) of(I-band) the(I-band) Nephilim(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, musical instrument, event, country, music genre, album, person, song, musical artist, award, band, organization and O.\nSentence: By the mid-1980s , bands began proliferating and became increasingly popular , including the Sisters of Mercy , the Mission , Alien Sex Fiend , the March Violets , Xmal Deutschland , the Membranes , and Fields of the Nephilim .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["By","the","mid-1980s",",","bands","began","proliferating","and","became","increasingly","popular",",","including","the","Sisters","of","Mercy",",","the","Mission",",","Alien","Sex","Fiend",",","the","March","Violets",",","Xmal","Deutschland",",","the","Membranes",",","and","Fields","of","the","Nephilim","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","I-band","I-band","O","B-band","I-band","O","B-band","I-band","I-band","O","B-band","I-band","I-band","O","B-band","I-band","O","B-band","I-band","O","O","B-band","I-band","I-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["location","musical_instrument","event","country","music_genre","album","person","song","musical_artist","award","band","organization"]}
{"id":"303","dataset":"crossner_music","split":"test","instance":{"id":"303","prompt_labels":"Breakbeats(O) have(O) been(O) used(O) in(O) styles(O) such(O) as(O) hip(B-music genre) hop(I-music genre) ,(O) Jungle(B-music genre) music(I-music genre) ,(O) drum(B-music genre) and(I-music genre) bass(I-music genre) ,(O) big(B-music genre) beat(I-music genre) ,(O) hardcore(B-music genre) ,(O) and(O) UK(B-music genre) garage(I-music genre) styles(O) ((O) including(O) 2-step(B-music genre) garage(I-music genre) ,(O) breakstep(B-music genre) and(O) dubstep(B-music genre) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, song, music genre, organization, musical artist, album, musical instrument, country, location, band, person and O.\nSentence: Breakbeats have been used in styles such as hip hop , Jungle music , drum and bass , big beat , hardcore , and UK garage styles ( including 2-step garage , breakstep and dubstep ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Breakbeats","have","been","used","in","styles","such","as","hip","hop",",","Jungle","music",",","drum","and","bass",",","big","beat",",","hardcore",",","and","UK","garage","styles","(","including","2-step","garage",",","breakstep","and","dubstep",")","."],"labels":["O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","O","O","B-music genre","I-music genre","O","O","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["award","event","song","music_genre","organization","musical_artist","album","musical_instrument","country","location","band","person"]}
{"id":"304","dataset":"crossner_music","split":"test","instance":{"id":"304","prompt_labels":"In(O) 2016(O) Chuck(B-musical artist) D(I-musical artist) joined(O) the(O) band(O) Prophets(B-band) of(I-band) Rage(I-band) along(O) with(O) B-Real(B-musical artist) and(O) former(O) members(O) of(O) Rage(B-band) Against(I-band) the(I-band) Machine(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, organization, award, music genre, band, musical instrument, location, musical artist, person, country, event, album and O.\nSentence: In 2016 Chuck D joined the band Prophets of Rage along with B-Real and former members of Rage Against the Machine .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2016","Chuck","D","joined","the","band","Prophets","of","Rage","along","with","B-Real","and","former","members","of","Rage","Against","the","Machine","."],"labels":["O","O","B-musical artist","I-musical artist","O","O","O","B-band","I-band","I-band","O","O","B-musical artist","O","O","O","O","B-band","I-band","I-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["song","organization","award","music_genre","band","musical_instrument","location","musical_artist","person","country","event","album"]}
{"id":"305","dataset":"crossner_music","split":"test","instance":{"id":"305","prompt_labels":"The(O) viola(B-musical instrument) is(O) also(O) an(O) important(O) accompaniment(O) instrument(O) in(O) Slovakia(B-country) n(O) ,(O) Hungary(B-country) and(O) Romania(B-country) n(O) folk(O) string(O) band(O) music(O) ,(O) especially(O) in(O) Transylvania(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, song, music genre, band, award, event, album, musical artist, location, organization, country, musical instrument and O.\nSentence: The viola is also an important accompaniment instrument in Slovakia n , Hungary and Romania n folk string band music , especially in Transylvania .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","viola","is","also","an","important","accompaniment","instrument","in","Slovakia","n",",","Hungary","and","Romania","n","folk","string","band","music",",","especially","in","Transylvania","."],"labels":["O","B-musical instrument","O","O","O","O","O","O","O","B-country","O","O","B-country","O","B-country","O","O","O","O","O","O","O","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["person","song","music_genre","band","award","event","album","musical_artist","location","organization","country","musical_instrument"]}
{"id":"306","dataset":"crossner_music","split":"test","instance":{"id":"306","prompt_labels":"Folsom(B-location) Field(I-location) is(O) also(O) used(O) as(O) the(O) finish(O) line(O) for(O) the(O) Bolder(B-event) Boulder(I-event) ,(O) a(O) popular(O) 10K(O) run(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, location, song, band, award, musical instrument, event, country, organization, music genre, musical artist, person and O.\nSentence: Folsom Field is also used as the finish line for the Bolder Boulder , a popular 10K run .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Folsom","Field","is","also","used","as","the","finish","line","for","the","Bolder","Boulder",",","a","popular","10K","run","."],"labels":["B-location","I-location","O","O","O","O","O","O","O","O","O","B-event","I-event","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["album","location","song","band","award","musical_instrument","event","country","organization","music_genre","musical_artist","person"]}
{"id":"308","dataset":"crossner_music","split":"test","instance":{"id":"308","prompt_labels":"During(O) their(O) three(O) years(O) as(O) a(O) mainstream(O) act(O) ,(O) Nirvana(B-band) was(O) awarded(O) an(O) American(B-award) Music(I-award) Awards(I-award) ,(O) Brit(B-award) Awards(I-award) ,(O) Grammy(B-award) Award(I-award) ,(O) seven(O) MTV(B-award) Video(I-award) Music(I-award) Award(I-award) s(O) and(O) two(O) NME(B-award) Awards(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, location, musical instrument, country, event, award, person, music genre, organization, album, song, band and O.\nSentence: During their three years as a mainstream act , Nirvana was awarded an American Music Awards , Brit Awards , Grammy Award , seven MTV Video Music Award s and two NME Awards .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","their","three","years","as","a","mainstream","act",",","Nirvana","was","awarded","an","American","Music","Awards",",","Brit","Awards",",","Grammy","Award",",","seven","MTV","Video","Music","Award","s","and","two","NME","Awards","."],"labels":["O","O","O","O","O","O","O","O","O","B-band","O","O","O","B-award","I-award","I-award","O","B-award","I-award","O","B-award","I-award","O","O","B-award","I-award","I-award","I-award","O","O","O","B-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","location","musical_instrument","country","event","award","person","music_genre","organization","album","song","band"]}
{"id":"309","dataset":"crossner_music","split":"test","instance":{"id":"309","prompt_labels":"This(O) was(O) reflected(O) in(O) a(O) series(O) of(O) albums(O) released(O) by(O) Island(B-organization) Records(I-organization) ,(O) including(O) Swordfishtrombones(B-album) ((O) 1983(O) )(O) ,(O) Rain(B-album) Dogs(I-album) ((O) 1985(O) )(O) ,(O) and(O) Franks(B-album) Wild(I-album) Years(I-album) ((O) 1987(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, organization, location, music genre, song, event, album, person, musical artist, band, award, country and O.\nSentence: This was reflected in a series of albums released by Island Records , including Swordfishtrombones ( 1983 ) , Rain Dogs ( 1985 ) , and Franks Wild Years ( 1987 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","was","reflected","in","a","series","of","albums","released","by","Island","Records",",","including","Swordfishtrombones","(","1983",")",",","Rain","Dogs","(","1985",")",",","and","Franks","Wild","Years","(","1987",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","B-album","O","O","O","O","B-album","I-album","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","organization","location","music_genre","song","event","album","person","musical_artist","band","award","country"]}
{"id":"314","dataset":"crossner_music","split":"test","instance":{"id":"314","prompt_labels":"The(O) group(O) 's(O) evolution(O) can(O) be(O) traced(O) through(O) the(O) recordings(O) The(B-album) John(I-album) Coltrane(I-album) Quartet(I-album) Plays(I-album) ,(O) Living(B-album) Space(I-album) and(O) Transition(B-album) ((O) both(O) June(O) 1965(O) )(O) ,(O) New(B-album) Thing(I-album) at(I-album) Newport(I-album) ((O) July(O) 1965(O) )(O) ,(O) Sun(B-album) Ship(I-album) ((O) August(O) 1965(O) )(O) ,(O) and(O) First(B-album) Meditations(I-album) ((O) September(O) 1965(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, award, music genre, event, location, organization, musical instrument, album, person, song, country, musical artist and O.\nSentence: The group 's evolution can be traced through the recordings The John Coltrane Quartet Plays , Living Space and Transition ( both June 1965 ) , New Thing at Newport ( July 1965 ) , Sun Ship ( August 1965 ) , and First Meditations ( September 1965 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","group","'s","evolution","can","be","traced","through","the","recordings","The","John","Coltrane","Quartet","Plays",",","Living","Space","and","Transition","(","both","June","1965",")",",","New","Thing","at","Newport","(","July","1965",")",",","Sun","Ship","(","August","1965",")",",","and","First","Meditations","(","September","1965",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","I-album","I-album","O","B-album","I-album","O","B-album","O","O","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","O","B-album","I-album","O","O","O","O","O","O","B-album","I-album","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["band","award","music_genre","event","location","organization","musical_instrument","album","person","song","country","musical_artist"]}
{"id":"315","dataset":"crossner_music","split":"test","instance":{"id":"315","prompt_labels":"Female(O) artists(O) such(O) as(O) Reba(B-person) McEntire(I-person) ,(O) Patty(B-musical artist) Loveless(I-musical artist) ,(O) Faith(B-person) Hill(I-person) ,(O) Martina(B-musical artist) McBride(I-musical artist) ,(O) Deana(B-musical artist) Carter(I-musical artist) ,(O) LeAnn(B-musical artist) Rimes(I-musical artist) ,(O) Mindy(B-musical artist) McCready(I-musical artist) ,(O) Lorrie(B-musical artist) Morgan(I-musical artist) ,(O) Shania(B-person) Twain(I-person) ,(O) and(O) Mary(B-musical artist) Chapin(I-musical artist) Carpenter(I-musical artist) all(O) released(O) platinum-selling(O) albums(O) in(O) the(O) 1990s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, band, location, organization, award, country, music genre, person, musical instrument, album, song, musical artist and O.\nSentence: Female artists such as Reba McEntire , Patty Loveless , Faith Hill , Martina McBride , Deana Carter , LeAnn Rimes , Mindy McCready , Lorrie Morgan , Shania Twain , and Mary Chapin Carpenter all released platinum-selling albums in the 1990s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Female","artists","such","as","Reba","McEntire",",","Patty","Loveless",",","Faith","Hill",",","Martina","McBride",",","Deana","Carter",",","LeAnn","Rimes",",","Mindy","McCready",",","Lorrie","Morgan",",","Shania","Twain",",","and","Mary","Chapin","Carpenter","all","released","platinum-selling","albums","in","the","1990s","."],"labels":["O","O","O","O","B-person","I-person","O","B-musical artist","I-musical artist","O","B-person","I-person","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-person","I-person","O","O","B-musical artist","I-musical artist","I-musical artist","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","band","location","organization","award","country","music_genre","person","musical_instrument","album","song","musical_artist"]}
{"id":"316","dataset":"crossner_music","split":"test","instance":{"id":"316","prompt_labels":"On(O) 19(O) September(O) ,(O) he(O) played(O) with(O) Korean(O) artist(O) PSY(B-musical artist) for(O) opening(O) ceremony(O) of(O) 2014(B-event) Asian(I-event) Games(I-event) in(O) Incheon(B-location) ,(O) South(B-country) Korea(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical artist, music genre, location, country, person, musical instrument, album, event, award, song, organization and O.\nSentence: On 19 September , he played with Korean artist PSY for opening ceremony of 2014 Asian Games in Incheon , South Korea .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","19","September",",","he","played","with","Korean","artist","PSY","for","opening","ceremony","of","2014","Asian","Games","in","Incheon",",","South","Korea","."],"labels":["O","O","O","O","O","O","O","O","O","B-musical artist","O","O","O","O","B-event","I-event","I-event","O","B-location","O","B-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["band","musical_artist","music_genre","location","country","person","musical_instrument","album","event","award","song","organization"]}
{"id":"318","dataset":"crossner_music","split":"test","instance":{"id":"318","prompt_labels":"Shore(B-musical artist) has(O) also(O) been(O) honored(O) with(O) awards(O) from(O) National(B-organization) Board(I-organization) of(I-organization) Review(I-organization) ,(O) Recording(B-award) Academy(I-award) Honors(I-award) ,(O) Broadcast(B-organization) Film(I-organization) Critics(I-organization) Association(I-organization) ,(O) Chicago(B-organization) Film(I-organization) Critics(I-organization) ,(O) Genie(B-award) Award(I-award) ,(O) World(B-award) Soundtrack(I-award) Award(I-award) ,(O) New(B-location) York(I-location) 's(O) Gotham(B-award) Award(I-award) ,(O) and(O) The(O) Saturn(B-award) Award(I-award) for(I-award) Science(I-award) Fiction(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical artist, music genre, song, musical instrument, country, album, person, event, organization, location, award and O.\nSentence: Shore has also been honored with awards from National Board of Review , Recording Academy Honors , Broadcast Film Critics Association , Chicago Film Critics , Genie Award , World Soundtrack Award , New York 's Gotham Award , and The Saturn Award for Science Fiction .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Shore","has","also","been","honored","with","awards","from","National","Board","of","Review",",","Recording","Academy","Honors",",","Broadcast","Film","Critics","Association",",","Chicago","Film","Critics",",","Genie","Award",",","World","Soundtrack","Award",",","New","York","'s","Gotham","Award",",","and","The","Saturn","Award","for","Science","Fiction","."],"labels":["B-musical artist","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-award","I-award","I-award","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-award","I-award","O","B-award","I-award","I-award","O","B-location","I-location","O","B-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["band","musical_artist","music_genre","song","musical_instrument","country","album","person","event","organization","location","award"]}
{"id":"320","dataset":"crossner_music","split":"test","instance":{"id":"320","prompt_labels":"In(O) Scotland(B-country) ,(O) The(B-band) Corries(I-band) ,(O) Silly(B-band) Wizard(I-band) ,(O) Capercaillie(B-band) ,(O) Runrig(B-band) ,(O) Jackie(B-musical artist) Leven(I-musical artist) ,(O) Julie(B-musical artist) Fowlis(I-musical artist) ,(O) Karine(B-musical artist) Polwart(I-musical artist) ,(O) Alasdair(B-musical artist) Roberts(I-musical artist) ,(O) Dick(B-musical artist) Gaughan(I-musical artist) ,(O) Wolfstone(B-band) ,(O) Boys(B-band) of(I-band) the(I-band) Lough(I-band) ,(O) and(O) The(B-band) Silencers(I-band) have(O) kept(O) Scottish(B-music genre) folk(I-music genre) vibrant(O) and(O) fresh(O) by(O) mixing(O) traditional(B-music genre) Scottish(I-music genre) and(O) Gaelic(B-music genre) folk(I-music genre) songs(O) with(O) more(O) contemporary(O) genres(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, band, award, organization, musical artist, music genre, location, musical instrument, event, album, person, song and O.\nSentence: In Scotland , The Corries , Silly Wizard , Capercaillie , Runrig , Jackie Leven , Julie Fowlis , Karine Polwart , Alasdair Roberts , Dick Gaughan , Wolfstone , Boys of the Lough , and The Silencers have kept Scottish folk vibrant and fresh by mixing traditional Scottish and Gaelic folk songs with more contemporary genres .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","Scotland",",","The","Corries",",","Silly","Wizard",",","Capercaillie",",","Runrig",",","Jackie","Leven",",","Julie","Fowlis",",","Karine","Polwart",",","Alasdair","Roberts",",","Dick","Gaughan",",","Wolfstone",",","Boys","of","the","Lough",",","and","The","Silencers","have","kept","Scottish","folk","vibrant","and","fresh","by","mixing","traditional","Scottish","and","Gaelic","folk","songs","with","more","contemporary","genres","."],"labels":["O","B-country","O","B-band","I-band","O","B-band","I-band","O","B-band","O","B-band","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-band","O","B-band","I-band","I-band","I-band","O","O","B-band","I-band","O","O","B-music genre","I-music genre","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","band","award","organization","musical_artist","music_genre","location","musical_instrument","event","album","person","song"]}
{"id":"321","dataset":"crossner_music","split":"test","instance":{"id":"321","prompt_labels":"In(O) conjunction(O) with(O) the(O) book(O) and(O) the(O) play(O) that(O) also(O) paid(O) tribute(O) to(O) his(O) uncle(O) ,(O) Milt(B-person) Gabler(I-person) ,(O) Crystal(B-musical artist) produced(O) two(O) CD(O) compilations(O) :(O) Billy(B-musical artist) Crystal(I-musical artist) Presents(O) :(O) The(B-album) Milt(I-album) Gabler(I-album) Story(I-album) ,(O) which(O) featured(O) his(O) uncle(O) 's(O) most(O) influential(O) recordings(O) from(O) Billie(B-musical artist) Holiday(I-musical artist) '(O) s(O) Strange(B-song) Fruit(I-song) to(O) Rock(B-song) Around(I-song) the(I-song) Clock(I-song) by(O) Bill(B-band) Haley(I-band) &(I-band) His(I-band) Comets(I-band) ;(O) and(O) Billy(B-album) Remembers(I-album) Billie(I-album) featuring(O) Crystal(B-musical artist) 's(O) favorite(O) Holiday(B-musical artist) recordings(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, album, musical artist, music genre, location, organization, country, person, award, band, song, event and O.\nSentence: In conjunction with the book and the play that also paid tribute to his uncle , Milt Gabler , Crystal produced two CD compilations : Billy Crystal Presents : The Milt Gabler Story , which featured his uncle 's most influential recordings from Billie Holiday ' s Strange Fruit to Rock Around the Clock by Bill Haley & His Comets ; and Billy Remembers Billie featuring Crystal 's favorite Holiday recordings .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","conjunction","with","the","book","and","the","play","that","also","paid","tribute","to","his","uncle",",","Milt","Gabler",",","Crystal","produced","two","CD","compilations",":","Billy","Crystal","Presents",":","The","Milt","Gabler","Story",",","which","featured","his","uncle","'s","most","influential","recordings","from","Billie","Holiday","'","s","Strange","Fruit","to","Rock","Around","the","Clock","by","Bill","Haley","&","His","Comets",";","and","Billy","Remembers","Billie","featuring","Crystal","'s","favorite","Holiday","recordings","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","B-musical artist","O","O","O","O","O","B-musical artist","I-musical artist","O","O","B-album","I-album","I-album","I-album","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","B-song","I-song","O","B-song","I-song","I-song","I-song","O","B-band","I-band","I-band","I-band","I-band","O","O","B-album","I-album","I-album","O","B-musical artist","O","O","B-musical artist","O","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","album","musical_artist","music_genre","location","organization","country","person","award","band","song","event"]}
{"id":"323","dataset":"crossner_music","split":"test","instance":{"id":"323","prompt_labels":"The(O) awards(O) were(O) established(O) in(O) 1969(O) ,(O) and(O) represent(O) a(O) variety(O) of(O) musical(O) styles(O) ,(O) including(O) Christian(B-music genre) rock(I-music genre) ,(O) Contemporary(B-music genre) Christian(I-music genre) music(I-music genre) ,(O) Christian(B-music genre) hip(I-music genre) hop(I-music genre) ,(O) Christian(B-music genre) country(I-music genre) music(I-music genre) ,(O) and(O) Urban(B-music genre) contemporary(I-music genre) gospel(I-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, music genre, location, song, organization, event, band, person, album, country, award, musical instrument and O.\nSentence: The awards were established in 1969 , and represent a variety of musical styles , including Christian rock , Contemporary Christian music , Christian hip hop , Christian country music , and Urban contemporary gospel .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","awards","were","established","in","1969",",","and","represent","a","variety","of","musical","styles",",","including","Christian","rock",",","Contemporary","Christian","music",",","Christian","hip","hop",",","Christian","country","music",",","and","Urban","contemporary","gospel","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","O","B-music genre","I-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","music_genre","location","song","organization","event","band","person","album","country","award","musical_instrument"]}
{"id":"324","dataset":"crossner_music","split":"test","instance":{"id":"324","prompt_labels":"He(O) provided(O) musical(O) direction(O) for(O) the(O) ceremonies(O) of(O) the(O) 1978(B-event) Commonwealth(I-event) Games(I-event) ,(O) EXPO(B-event) 86(I-event) ,(O) The(B-event) World(I-event) University(I-event) Games(I-event) ,(O) the(O) XV(B-event) Olympic(I-event) Winter(I-event) Games(I-event) ,(O) and(O) for(O) countless(O) television(O) shows(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, music genre, song, event, organization, award, country, musical artist, location, musical instrument, band, album and O.\nSentence: He provided musical direction for the ceremonies of the 1978 Commonwealth Games , EXPO 86 , The World University Games , the XV Olympic Winter Games , and for countless television shows .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","provided","musical","direction","for","the","ceremonies","of","the","1978","Commonwealth","Games",",","EXPO","86",",","The","World","University","Games",",","the","XV","Olympic","Winter","Games",",","and","for","countless","television","shows","."],"labels":["O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","B-event","I-event","O","B-event","I-event","I-event","I-event","O","O","B-event","I-event","I-event","I-event","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","music_genre","song","event","organization","award","country","musical_artist","location","musical_instrument","band","album"]}
{"id":"326","dataset":"crossner_music","split":"test","instance":{"id":"326","prompt_labels":"Performers(O) such(O) as(O) Frank(B-musical artist) Stokes(I-musical artist) ,(O) Sleepy(B-musical artist) John(I-musical artist) Estes(I-musical artist) ,(O) Robert(B-musical artist) Wilkins(I-musical artist) ,(O) Joe(B-musical artist) McCoy(I-musical artist) ,(O) Casey(B-musical artist) Bill(I-musical artist) Weldon(I-musical artist) and(O) Memphis(B-musical artist) Minnie(I-musical artist) used(O) a(O) variety(O) of(O) unusual(O) instruments(O) such(O) as(O) washboard(B-musical instrument) ,(O) fiddle(B-musical instrument) ,(O) kazoo(B-musical instrument) or(O) mandolin(B-musical instrument) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, location, musical artist, album, country, award, musical instrument, event, band, organization, song, person and O.\nSentence: Performers such as Frank Stokes , Sleepy John Estes , Robert Wilkins , Joe McCoy , Casey Bill Weldon and Memphis Minnie used a variety of unusual instruments such as washboard , fiddle , kazoo or mandolin .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Performers","such","as","Frank","Stokes",",","Sleepy","John","Estes",",","Robert","Wilkins",",","Joe","McCoy",",","Casey","Bill","Weldon","and","Memphis","Minnie","used","a","variety","of","unusual","instruments","such","as","washboard",",","fiddle",",","kazoo","or","mandolin","."],"labels":["O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","B-musical instrument","O","B-musical instrument","O","B-musical instrument","O","B-musical instrument","O"],"target_index":null,"target_label":null},"label_list":["music_genre","location","musical_artist","album","country","award","musical_instrument","event","band","organization","song","person"]}
{"id":"327","dataset":"crossner_music","split":"test","instance":{"id":"327","prompt_labels":"The(O) World(B-organization) DanceSport(I-organization) Federation(I-organization) ((O) WDSF(B-organization) )(O) ,(O) formerly(O) the(O) International(B-organization) DanceSport(I-organization) Federation(I-organization) ((O) IDSF(B-organization) )(O) ,(O) is(O) the(O) international(O) governing(O) body(O) of(O) DanceSport(O) and(O) Wheelchair(O) DanceSport(O) ,(O) as(O) recognised(O) by(O) the(O) International(B-organization) Olympic(I-organization) Committee(I-organization) ((O) IOC(B-organization) )(O) and(O) the(O) International(B-organization) Paralympic(I-organization) Committee(I-organization) ((O) IPC(B-organization) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, award, band, album, musical instrument, music genre, organization, country, song, location, musical artist and O.\nSentence: The World DanceSport Federation ( WDSF ) , formerly the International DanceSport Federation ( IDSF ) , is the international governing body of DanceSport and Wheelchair DanceSport , as recognised by the International Olympic Committee ( IOC ) and the International Paralympic Committee ( IPC ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","World","DanceSport","Federation","(","WDSF",")",",","formerly","the","International","DanceSport","Federation","(","IDSF",")",",","is","the","international","governing","body","of","DanceSport","and","Wheelchair","DanceSport",",","as","recognised","by","the","International","Olympic","Committee","(","IOC",")","and","the","International","Paralympic","Committee","(","IPC",")","."],"labels":["O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O"],"target_index":null,"target_label":null},"label_list":["person","event","award","band","album","musical_instrument","music_genre","organization","country","song","location","musical_artist"]}
{"id":"329","dataset":"crossner_music","split":"test","instance":{"id":"329","prompt_labels":"In(O) the(O) 1990s(O) ,(O) artists(O) like(O) Me(B-musical artist) 'shell(I-musical artist) Ndegeocello(I-musical artist) ,(O) Brooklyn(B-band) Funk(I-band) Essentials(I-band) and(O) the(O) ((O) predominantly(O) UK-based(O) )(O) acid(B-music genre) jazz(I-music genre) movement(O) including(O) artists(O) and(O) bands(O) such(O) as(O) Jamiroquai(B-band) ,(O) Incognito(B-band) ,(O) Galliano(B-band) ,(O) Omar(B-musical artist) Lye-Fook(I-musical artist) ,(O) Los(B-band) Tetas(I-band) and(O) the(O) Brand(B-band) New(I-band) Heavies(I-band) carried(O) on(O) with(O) strong(O) elements(O) of(O) funk(B-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, music genre, song, band, location, award, musical artist, album, event, country, musical instrument and O.\nSentence: In the 1990s , artists like Me 'shell Ndegeocello , Brooklyn Funk Essentials and the ( predominantly UK-based ) acid jazz movement including artists and bands such as Jamiroquai , Incognito , Galliano , Omar Lye-Fook , Los Tetas and the Brand New Heavies carried on with strong elements of funk .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","1990s",",","artists","like","Me","'shell","Ndegeocello",",","Brooklyn","Funk","Essentials","and","the","(","predominantly","UK-based",")","acid","jazz","movement","including","artists","and","bands","such","as","Jamiroquai",",","Incognito",",","Galliano",",","Omar","Lye-Fook",",","Los","Tetas","and","the","Brand","New","Heavies","carried","on","with","strong","elements","of","funk","."],"labels":["O","O","O","O","O","O","B-musical artist","I-musical artist","I-musical artist","O","B-band","I-band","I-band","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","B-band","O","B-band","O","B-band","O","B-musical artist","I-musical artist","O","B-band","I-band","O","O","B-band","I-band","I-band","O","O","O","O","O","O","B-music genre","O"],"target_index":null,"target_label":null},"label_list":["organization","person","music_genre","song","band","location","award","musical_artist","album","event","country","musical_instrument"]}
{"id":"331","dataset":"crossner_music","split":"test","instance":{"id":"331","prompt_labels":"His(O) film(O) Annie(O) Hall(O) ((O) 1977(O) )(O) ,(O) a(O) romantic(O) comedy(O) featuring(O) Allen(B-person) and(O) his(O) frequent(O) collaborator(O) Diane(B-person) Keaton(I-person) ,(O) won(O) four(O) Academy(B-award) Awards(I-award) ,(O) including(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Screenplay(I-award) ,(O) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) for(O) Keaton(B-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, organization, album, person, band, musical artist, location, award, song, event, music genre, country and O.\nSentence: His film Annie Hall ( 1977 ) , a romantic comedy featuring Allen and his frequent collaborator Diane Keaton , won four Academy Awards , including Academy Award for Best Picture , Academy Award for Best Director , Academy Award for Best Original Screenplay , and Academy Award for Best Actress for Keaton .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","film","Annie","Hall","(","1977",")",",","a","romantic","comedy","featuring","Allen","and","his","frequent","collaborator","Diane","Keaton",",","won","four","Academy","Awards",",","including","Academy","Award","for","Best","Picture",",","Academy","Award","for","Best","Director",",","Academy","Award","for","Best","Original","Screenplay",",","and","Academy","Award","for","Best","Actress","for","Keaton","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-person","O","O","O","O","B-person","I-person","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O","B-person","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","organization","album","person","band","musical_artist","location","award","song","event","music_genre","country"]}
{"id":"333","dataset":"crossner_music","split":"test","instance":{"id":"333","prompt_labels":"It(O) won(O) five(O) Grammys(O) :(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Rock(I-award) Album(I-award) ,(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Rock(I-award) Song(I-award) ((O) Dani(B-song) California(I-song) )(O) ,(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Rock(I-award) Performance(I-award) by(I-award) a(I-award) Duo(I-award) or(I-award) Group(I-award) with(I-award) Vocal(I-award) ((O) Dani(B-song) California(I-song) )(O) ,(O) Best(B-award) Boxed(I-award) Or(I-award) Special(I-award) Limited(I-award) Edition(I-award) Package(I-award) ,(O) and(O) Best(B-award) Producer(I-award) ((O) Rick(B-song) Rubin(I-song) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, music genre, country, event, song, musical instrument, band, location, album, organization, person, musical artist and O.\nSentence: It won five Grammys : Grammy Award for Best Rock Album , Grammy Award for Best Rock Song ( Dani California ) , Grammy Award for Best Rock Performance by a Duo or Group with Vocal ( Dani California ) , Best Boxed Or Special Limited Edition Package , and Best Producer ( Rick Rubin ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","won","five","Grammys",":","Grammy","Award","for","Best","Rock","Album",",","Grammy","Award","for","Best","Rock","Song","(","Dani","California",")",",","Grammy","Award","for","Best","Rock","Performance","by","a","Duo","or","Group","with","Vocal","(","Dani","California",")",",","Best","Boxed","Or","Special","Limited","Edition","Package",",","and","Best","Producer","(","Rick","Rubin",")","."],"labels":["O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-song","I-song","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","B-song","I-song","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","O","B-song","I-song","O","O"],"target_index":null,"target_label":null},"label_list":["award","music_genre","country","event","song","musical_instrument","band","location","album","organization","person","musical_artist"]}
{"id":"335","dataset":"crossner_music","split":"test","instance":{"id":"335","prompt_labels":"The(O) band(O) released(O) four(O) albums(O) :(O) Styx(B-album) I(I-album) ((O) 1972(O) )(O) ,(O) Styx(B-album) II(I-album) ((O) 1973(O) )(O) ,(O) The(B-album) Serpent(I-album) Is(I-album) Rising(I-album) ((O) 1973(O) )(O) ,(O) and(O) Man(B-album) of(I-album) Miracles(I-album) ((O) 1974(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, music genre, location, person, album, musical instrument, event, song, country, musical artist, band, award and O.\nSentence: The band released four albums : Styx I ( 1972 ) , Styx II ( 1973 ) , The Serpent Is Rising ( 1973 ) , and Man of Miracles ( 1974 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","band","released","four","albums",":","Styx","I","(","1972",")",",","Styx","II","(","1973",")",",","The","Serpent","Is","Rising","(","1973",")",",","and","Man","of","Miracles","(","1974",")","."],"labels":["O","O","O","O","O","O","B-album","I-album","O","O","O","O","B-album","I-album","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","music_genre","location","person","album","musical_instrument","event","song","country","musical_artist","band","award"]}
{"id":"336","dataset":"crossner_music","split":"test","instance":{"id":"336","prompt_labels":"Nirvana(B-musical artist) occasionally(O) played(O) cover(O) songs(O) by(O) these(O) bands(O) ,(O) including(O) Led(B-band) Zeppelin(I-band) 's(O) Heartbreaker(B-song) ,(O) Moby(B-song) Dick(I-song) and(O) Immigrant(B-song) Song(I-song) ,(O) Black(B-band) Sabbath(I-band) 's(O) Hand(B-song) of(I-song) Doom(I-song) ,(O) and(O) Kiss(B-song) '(I-song) Do(I-song) You(I-song) Love(I-song) Me(I-song) ?(I-song) and(O) wrote(O) the(O) Incesticide(B-album) song(O) Aero(B-song) Zeppelin(I-song) as(O) a(O) tribute(O) to(O) Led(B-band) Zeppelin(I-band) and(O) Aerosmith(B-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, location, organization, country, award, album, band, song, musical instrument, music genre, person, event and O.\nSentence: Nirvana occasionally played cover songs by these bands , including Led Zeppelin 's Heartbreaker , Moby Dick and Immigrant Song , Black Sabbath 's Hand of Doom , and Kiss ' Do You Love Me ? and wrote the Incesticide song Aero Zeppelin as a tribute to Led Zeppelin and Aerosmith .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Nirvana","occasionally","played","cover","songs","by","these","bands",",","including","Led","Zeppelin","'s","Heartbreaker",",","Moby","Dick","and","Immigrant","Song",",","Black","Sabbath","'s","Hand","of","Doom",",","and","Kiss","'","Do","You","Love","Me","?","and","wrote","the","Incesticide","song","Aero","Zeppelin","as","a","tribute","to","Led","Zeppelin","and","Aerosmith","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","B-band","I-band","O","B-song","O","B-song","I-song","O","B-song","I-song","O","B-band","I-band","O","B-song","I-song","I-song","O","O","B-song","I-song","I-song","I-song","I-song","I-song","I-song","O","O","O","B-album","O","B-song","I-song","O","O","O","O","B-band","I-band","O","B-band","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","location","organization","country","award","album","band","song","musical_instrument","music_genre","person","event"]}
{"id":"338","dataset":"crossner_music","split":"test","instance":{"id":"338","prompt_labels":"Os(B-band) Mutantes(I-band) ((O) meaning(O) the(O) mutants(O) ;(O) )(O) are(O) an(O) influential(O) Brazilian(O) Rock(B-music genre) music(I-music genre) band(O) that(O) were(O) linked(O) with(O) the(O) Tropiclia(B-music genre) movement(O) ,(O) a(O) dissident(O) musical(O) movement(O) during(O) the(O) Brazilian(O) dictatorship(O) of(O) the(O) late(O) 1960s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, person, event, musical artist, location, organization, award, album, band, music genre, musical instrument, country and O.\nSentence: Os Mutantes ( meaning the mutants ; ) are an influential Brazilian Rock music band that were linked with the Tropiclia movement , a dissident musical movement during the Brazilian dictatorship of the late 1960s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Os","Mutantes","(","meaning","the","mutants",";",")","are","an","influential","Brazilian","Rock","music","band","that","were","linked","with","the","Tropiclia","movement",",","a","dissident","musical","movement","during","the","Brazilian","dictatorship","of","the","late","1960s","."],"labels":["B-band","I-band","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","B-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["song","person","event","musical_artist","location","organization","award","album","band","music_genre","musical_instrument","country"]}
{"id":"339","dataset":"crossner_music","split":"test","instance":{"id":"339","prompt_labels":"Distinct(O) from(O) Afrobeat(B-music genre) is(O) Afrobeats(B-music genre) -(O) a(O) sound(O) originating(O) in(O) West(B-location) Africa(I-location) in(O) the(O) 21st(O) century(O) ,(O) one(O) which(O) takes(O) in(O) diverse(O) influences(O) and(O) is(O) an(O) eclectic(O) combination(O) of(O) genres(O) such(O) as(O) British(O) house(B-music genre) music(I-music genre) ,(O) hiplife(B-music genre) ,(O) hip(B-music genre) hop(I-music genre) ,(O) dancehall(B-music genre) ,(O) Soca(B-music genre) music(I-music genre) ,(O) Jj(B-music genre) music(I-music genre) ,(O) highlife(B-music genre) ,(O) Rhythm(B-music genre) and(I-music genre) blues(I-music genre) ,(O) Ndombolo(B-music genre) ,(O) Naija(B-music genre) beats(I-music genre) ,(O) Azonto(B-music genre) ,(O) and(O) Palm-wine(B-music genre) music(I-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, award, musical instrument, music genre, location, album, event, song, organization, musical artist, person, country and O.\nSentence: Distinct from Afrobeat is Afrobeats - a sound originating in West Africa in the 21st century , one which takes in diverse influences and is an eclectic combination of genres such as British house music , hiplife , hip hop , dancehall , Soca music , Jj music , highlife , Rhythm and blues , Ndombolo , Naija beats , Azonto , and Palm-wine music .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Distinct","from","Afrobeat","is","Afrobeats","-","a","sound","originating","in","West","Africa","in","the","21st","century",",","one","which","takes","in","diverse","influences","and","is","an","eclectic","combination","of","genres","such","as","British","house","music",",","hiplife",",","hip","hop",",","dancehall",",","Soca","music",",","Jj","music",",","highlife",",","Rhythm","and","blues",",","Ndombolo",",","Naija","beats",",","Azonto",",","and","Palm-wine","music","."],"labels":["O","O","B-music genre","O","B-music genre","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","O","O","B-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["band","award","musical_instrument","music_genre","location","album","event","song","organization","musical_artist","person","country"]}
{"id":"340","dataset":"crossner_music","split":"test","instance":{"id":"340","prompt_labels":"Priories(O) may(O) be(O) houses(O) of(O) mendicant(O) friar(O) s(O) or(O) nun(O) s(O) ((O) such(O) as(O) the(O) Dominican(B-organization) Order(I-organization) ,(O) Augustinians(B-organization) ,(O) Franciscans(B-organization) ,(O) and(O) Carmelites(B-organization) ,(O) for(O) instance(O) )(O) ,(O) or(O) monasteries(O) of(O) monk(O) s(O) or(O) nun(O) s(O) ((O) as(O) with(O) the(O) Benedictines(B-organization) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, album, event, song, person, music genre, musical artist, organization, location, band, country, award and O.\nSentence: Priories may be houses of mendicant friar s or nun s ( such as the Dominican Order , Augustinians , Franciscans , and Carmelites , for instance ) , or monasteries of monk s or nun s ( as with the Benedictines ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Priories","may","be","houses","of","mendicant","friar","s","or","nun","s","(","such","as","the","Dominican","Order",",","Augustinians",",","Franciscans",",","and","Carmelites",",","for","instance",")",",","or","monasteries","of","monk","s","or","nun","s","(","as","with","the","Benedictines",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","B-organization","O","B-organization","O","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","album","event","song","person","music_genre","musical_artist","organization","location","band","country","award"]}
{"id":"341","dataset":"crossner_music","split":"test","instance":{"id":"341","prompt_labels":"Along(O) with(O) one(O) other(O) Original(O) Six(O) indoor(O) ice(O) hockey(O) arena(O) ,(O) the(O) Boston(B-location) Garden(I-location) ,(O) the(O) Montreal(B-location) Forum(I-location) used(O) a(O) high-pitched(O) siren(O) to(O) signal(O) the(O) end(O) of(O) an(O) NHL(O) game(O) 's(O) period(O) -(O) the(O) siren(O) would(O) later(O) be(O) re-installed(O) in(O) the(O) Forum(B-location) 's(O) successor(O) facility(O) ,(O) the(O) Bell(B-location) Centre(I-location) ((O) and(O) still(O) in(O) use(O) there(O) )(O) ,(O) much(O) as(O) the(O) TD(B-location) Garden(I-location) in(O) Boston(B-location) inherited(O) the(O) lower-pitched(O) Garden(B-location) 's(O) siren(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, musical instrument, award, song, organization, country, band, album, person, event, musical artist, location and O.\nSentence: Along with one other Original Six indoor ice hockey arena , the Boston Garden , the Montreal Forum used a high-pitched siren to signal the end of an NHL game 's period - the siren would later be re-installed in the Forum 's successor facility , the Bell Centre ( and still in use there ) , much as the TD Garden in Boston inherited the lower-pitched Garden 's siren .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Along","with","one","other","Original","Six","indoor","ice","hockey","arena",",","the","Boston","Garden",",","the","Montreal","Forum","used","a","high-pitched","siren","to","signal","the","end","of","an","NHL","game","'s","period","-","the","siren","would","later","be","re-installed","in","the","Forum","'s","successor","facility",",","the","Bell","Centre","(","and","still","in","use","there",")",",","much","as","the","TD","Garden","in","Boston","inherited","the","lower-pitched","Garden","'s","siren","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","O","O","O","B-location","O","O","O"],"target_index":null,"target_label":null},"label_list":["music_genre","musical_instrument","award","song","organization","country","band","album","person","event","musical_artist","location"]}
{"id":"345","dataset":"crossner_music","split":"test","instance":{"id":"345","prompt_labels":"Other(O) artists(O) such(O) as(O) Brooks(B-band) and(I-band) Dunn(I-band) ((O) Boot(B-song) Scootin(I-song) '(I-song) Boogie(I-song) )(O) also(O) combined(O) conventional(O) country(B-music genre) with(O) slick(O) ,(O) rock(B-music genre) elements(O) ,(O) while(O) Lorrie(B-musical artist) Morgan(I-musical artist) ,(O) Mary(B-musical artist) Chapin(I-musical artist) Carpenter(I-musical artist) ,(O) and(O) Kathy(B-musical artist) Mattea(I-musical artist) updated(O) neotraditionalist(O) styles(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, album, country, event, band, music genre, location, song, person, musical instrument, organization, musical artist and O.\nSentence: Other artists such as Brooks and Dunn ( Boot Scootin ' Boogie ) also combined conventional country with slick , rock elements , while Lorrie Morgan , Mary Chapin Carpenter , and Kathy Mattea updated neotraditionalist styles .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","artists","such","as","Brooks","and","Dunn","(","Boot","Scootin","'","Boogie",")","also","combined","conventional","country","with","slick",",","rock","elements",",","while","Lorrie","Morgan",",","Mary","Chapin","Carpenter",",","and","Kathy","Mattea","updated","neotraditionalist","styles","."],"labels":["O","O","O","O","B-band","I-band","I-band","O","B-song","I-song","I-song","I-song","O","O","O","O","B-music genre","O","O","O","B-music genre","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","album","country","event","band","music_genre","location","song","person","musical_instrument","organization","musical_artist"]}
{"id":"349","dataset":"crossner_music","split":"test","instance":{"id":"349","prompt_labels":"A(O) 1984(O) New(B-organization) York(I-organization) Times(I-organization) article(O) on(O) the(O) emerging(O) aesthetic(O) acknowledged(O) cowpunk(B-music genre) as(O) one(O) of(O) several(O) catch-all(O) terms(O) critics(O) were(O) using(O) to(O) categorize(O) the(O) Country(B-music genre) music(I-music genre) -influenced(O) music(O) of(O) otherwise(O) unrelated(O) Punk(B-music genre) rock(I-music genre) and(O) New(B-music genre) wave(I-music genre) music(I-music genre) bands(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, country, music genre, musical artist, band, organization, song, location, musical instrument, album, person and O.\nSentence: A 1984 New York Times article on the emerging aesthetic acknowledged cowpunk as one of several catch-all terms critics were using to categorize the Country music -influenced music of otherwise unrelated Punk rock and New wave music bands .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","1984","New","York","Times","article","on","the","emerging","aesthetic","acknowledged","cowpunk","as","one","of","several","catch-all","terms","critics","were","using","to","categorize","the","Country","music","-influenced","music","of","otherwise","unrelated","Punk","rock","and","New","wave","music","bands","."],"labels":["O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","B-music genre","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["event","award","country","music_genre","musical_artist","band","organization","song","location","musical_instrument","album","person"]}
{"id":"350","dataset":"crossner_music","split":"test","instance":{"id":"350","prompt_labels":"Brooks(B-band) and(I-band) Dunn(I-band) have(O) more(O) Country(B-award) Music(I-award) Association(I-award) awards(I-award) and(O) Academy(B-award) of(I-award) Country(I-award) Music(I-award) awards(I-award) than(O) any(O) act(O) in(O) the(O) history(O) of(O) country(B-music genre) music(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, music genre, musical artist, organization, country, band, event, award, person, song, location, album and O.\nSentence: Brooks and Dunn have more Country Music Association awards and Academy of Country Music awards than any act in the history of country music .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Brooks","and","Dunn","have","more","Country","Music","Association","awards","and","Academy","of","Country","Music","awards","than","any","act","in","the","history","of","country","music","."],"labels":["B-band","I-band","I-band","O","O","B-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","B-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","music_genre","musical_artist","organization","country","band","event","award","person","song","location","album"]}
{"id":"351","dataset":"crossner_music","split":"test","instance":{"id":"351","prompt_labels":"Byrne(B-musical artist) has(O) contributed(O) songs(O) to(O) five(O) AIDS(O) benefit(O) compilation(O) albums(O) produced(O) by(O) the(O) Red(B-organization) Hot(I-organization) Organization(I-organization) :(O) Red(B-album) Hot(I-album) +(I-album) Blue(I-album) :(I-album) A(I-album) Tribute(I-album) to(I-album) Cole(I-album) Porter(I-album) ,(O) Red(B-album) Hot(I-album) +(I-album) Rio(I-album) ,(O) Silencio(B-album) =(I-album) Muerte(I-album) :(I-album) Red(I-album) Hot(I-album) +(I-album) Latin(I-album) ,(O) Onda(B-album) Sonora(I-album) :(I-album) Red(I-album) Hot(I-album) +(I-album) Lisbon(I-album) ,(O) and(O) Offbeat(B-album) :(I-album) A(I-album) Red(I-album) Hot(I-album) Soundtrip(I-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, song, musical instrument, award, musical artist, country, album, person, event, music genre, band, location and O.\nSentence: Byrne has contributed songs to five AIDS benefit compilation albums produced by the Red Hot Organization : Red Hot + Blue : A Tribute to Cole Porter , Red Hot + Rio , Silencio = Muerte : Red Hot + Latin , Onda Sonora : Red Hot + Lisbon , and Offbeat : A Red Hot Soundtrip .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Byrne","has","contributed","songs","to","five","AIDS","benefit","compilation","albums","produced","by","the","Red","Hot","Organization",":","Red","Hot","+","Blue",":","A","Tribute","to","Cole","Porter",",","Red","Hot","+","Rio",",","Silencio","=","Muerte",":","Red","Hot","+","Latin",",","Onda","Sonora",":","Red","Hot","+","Lisbon",",","and","Offbeat",":","A","Red","Hot","Soundtrip","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-album","I-album","I-album","I-album","I-album","I-album","I-album","I-album","I-album","I-album","O","B-album","I-album","I-album","I-album","O","B-album","I-album","I-album","I-album","I-album","I-album","I-album","I-album","O","B-album","I-album","I-album","I-album","I-album","I-album","I-album","O","O","B-album","I-album","I-album","I-album","I-album","I-album","O"],"target_index":null,"target_label":null},"label_list":["organization","song","musical_instrument","award","musical_artist","country","album","person","event","music_genre","band","location"]}
{"id":"352","dataset":"crossner_music","split":"test","instance":{"id":"352","prompt_labels":"Three(O) UEFA(B-event) Euro(I-event) 2012(I-event) Group(I-event) D(I-event) matches(O) ,(O) a(O) quarter-final(O) and(O) the(O) final(O) were(O) scheduled(O) for(O) here(O) ((O) with(O) the(O) other(O) matches(O) in(O) Group(O) D(O) being(O) played(O) at(O) the(O) Donbass(B-location) Arena(I-location) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, album, song, musical instrument, country, band, event, musical artist, person, music genre, organization and O.\nSentence: Three UEFA Euro 2012 Group D matches , a quarter-final and the final were scheduled for here ( with the other matches in Group D being played at the Donbass Arena ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Three","UEFA","Euro","2012","Group","D","matches",",","a","quarter-final","and","the","final","were","scheduled","for","here","(","with","the","other","matches","in","Group","D","being","played","at","the","Donbass","Arena",")","."],"labels":["O","B-event","I-event","I-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O"],"target_index":null,"target_label":null},"label_list":["award","location","album","song","musical_instrument","country","band","event","musical_artist","person","music_genre","organization"]}
{"id":"356","dataset":"crossner_music","split":"test","instance":{"id":"356","prompt_labels":"Traditional(O) power(B-music genre) metal(I-music genre) bands(O) like(O) Sweden(B-country) 's(O) HammerFall(B-band) ,(O) England(B-country) 's(O) DragonForce(B-band) ,(O) and(O) America(B-country) 's(O) Iced(B-band) Earth(I-band) have(O) a(O) sound(O) clearly(O) indebted(O) to(O) the(O) classic(O) NWOBHM(B-music genre) style.See(O) ,(O) e.g.(O) ,(O) Reesman(B-person) Bryan(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, song, musical instrument, country, organization, person, album, music genre, award, band, location, musical artist and O.\nSentence: Traditional power metal bands like Sweden 's HammerFall , England 's DragonForce , and America 's Iced Earth have a sound clearly indebted to the classic NWOBHM style.See , e.g. , Reesman Bryan .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Traditional","power","metal","bands","like","Sweden","'s","HammerFall",",","England","'s","DragonForce",",","and","America","'s","Iced","Earth","have","a","sound","clearly","indebted","to","the","classic","NWOBHM","style.See",",","e.g.",",","Reesman","Bryan","."],"labels":["O","B-music genre","I-music genre","O","O","B-country","O","B-band","O","B-country","O","B-band","O","O","B-country","O","B-band","I-band","O","O","O","O","O","O","O","O","B-music genre","O","O","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["event","song","musical_instrument","country","organization","person","album","music_genre","award","band","location","musical_artist"]}
{"id":"357","dataset":"crossner_music","split":"test","instance":{"id":"357","prompt_labels":"Other(O) songs(O) from(O) 2015(O) like(O) I(B-song) Don(I-song) 't(I-song) Like(I-song) It(I-song) ,(I-song) I(I-song) Love(I-song) It(I-song) by(O) Flo(B-musical artist) Rida(I-musical artist) ,(O) Adventure(B-song) of(I-song) a(I-song) Lifetime(I-song) by(O) Coldplay(B-band) ,(O) Back(B-song) Together(I-song) by(O) Robin(B-musical artist) Thicke(I-musical artist) and(O) Levels(B-song) by(O) Nick(B-musical artist) Jonas(I-musical artist) feature(O) disco(B-music genre) elements(O) as(O) well(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, musical instrument, song, musical artist, award, location, person, band, country, music genre, organization, event and O.\nSentence: Other songs from 2015 like I Don 't Like It , I Love It by Flo Rida , Adventure of a Lifetime by Coldplay , Back Together by Robin Thicke and Levels by Nick Jonas feature disco elements as well .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","songs","from","2015","like","I","Don","'t","Like","It",",","I","Love","It","by","Flo","Rida",",","Adventure","of","a","Lifetime","by","Coldplay",",","Back","Together","by","Robin","Thicke","and","Levels","by","Nick","Jonas","feature","disco","elements","as","well","."],"labels":["O","O","O","O","O","B-song","I-song","I-song","I-song","I-song","I-song","I-song","I-song","I-song","O","B-musical artist","I-musical artist","O","B-song","I-song","I-song","I-song","O","B-band","O","B-song","I-song","O","B-musical artist","I-musical artist","O","B-song","O","B-musical artist","I-musical artist","O","B-music genre","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["album","musical_instrument","song","musical_artist","award","location","person","band","country","music_genre","organization","event"]}
{"id":"359","dataset":"crossner_music","split":"test","instance":{"id":"359","prompt_labels":"More(O) recently(O) ,(O) Illinois(B-musical artist) Jacquet(I-musical artist) ,(O) Ray(B-musical artist) Pizzi(I-musical artist) ,(O) Frank(B-musical artist) Tiberi(I-musical artist) ,(O) and(O) Marshall(B-musical artist) Allen(I-musical artist) have(O) both(O) doubled(O) on(O) bassoon(B-musical instrument) in(O) addition(O) to(O) their(O) saxophone(B-musical instrument) performances(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, musical instrument, location, band, musical artist, country, event, song, person, award, album, organization and O.\nSentence: More recently , Illinois Jacquet , Ray Pizzi , Frank Tiberi , and Marshall Allen have both doubled on bassoon in addition to their saxophone performances .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["More","recently",",","Illinois","Jacquet",",","Ray","Pizzi",",","Frank","Tiberi",",","and","Marshall","Allen","have","both","doubled","on","bassoon","in","addition","to","their","saxophone","performances","."],"labels":["O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","O","O","O","O","B-musical instrument","O","O","O","O","B-musical instrument","O","O"],"target_index":null,"target_label":null},"label_list":["music_genre","musical_instrument","location","band","musical_artist","country","event","song","person","award","album","organization"]}
{"id":"364","dataset":"crossner_music","split":"test","instance":{"id":"364","prompt_labels":"The(O) group(O) 's(O) original(O) lineup(O) consisted(O) of(O) brothers(O) Brian(B-musical artist) Wilson(I-musical artist) ,(O) Dennis(B-musical artist) Wilson(I-musical artist) ,(O) and(O) Carl(B-musical artist) Wilson(I-musical artist) ,(O) their(O) cousin(O) Mike(B-musical artist) Love(I-musical artist) ,(O) and(O) their(O) friend(O) Al(B-musical artist) Jardine(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, musical instrument, country, event, person, organization, song, award, band, musical artist, album, music genre and O.\nSentence: The group 's original lineup consisted of brothers Brian Wilson , Dennis Wilson , and Carl Wilson , their cousin Mike Love , and their friend Al Jardine .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","group","'s","original","lineup","consisted","of","brothers","Brian","Wilson",",","Dennis","Wilson",",","and","Carl","Wilson",",","their","cousin","Mike","Love",",","and","their","friend","Al","Jardine","."],"labels":["O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","O","O","O","B-musical artist","I-musical artist","O","O","O","O","B-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["location","musical_instrument","country","event","person","organization","song","award","band","musical_artist","album","music_genre"]}
{"id":"366","dataset":"crossner_music","split":"test","instance":{"id":"366","prompt_labels":"Clooney(B-person) lists(O) 9(O) hotels(O) including(O) The(B-location) Dorchester(I-location) ,(O) 45(B-location) Park(I-location) Lane(I-location) ,(O) Coworth(B-location) Park(I-location) ,(O) The(B-location) Beverly(I-location) Hills(I-location) Hotel(I-location) ,(O) Hotel(B-location) Bel-Air(I-location) ,(O) Le(B-location) Meurice(I-location) ,(O) Hotel(B-location) Plaza(I-location) Athenee(I-location) ,(O) Hotel(B-location) Eden(I-location) and(O) Hotel(B-location) Principe(I-location) di(I-location) Savoia(I-location) and(O) asks(O) readers(O) to(O) consider(O) how(O) we(O) are(O) putting(O) money(O) directly(O) into(O) the(O) pockets(O) of(O) men(O) who(O) choose(O) to(O) stone(O) and(O) whip(O) to(O) death(O) their(O) own(O) citizens(O) for(O) being(O) gay(O) or(O) accused(O) of(O) adultery(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, musical instrument, country, album, award, event, location, band, organization, musical artist, music genre, person and O.\nSentence: Clooney lists 9 hotels including The Dorchester , 45 Park Lane , Coworth Park , The Beverly Hills Hotel , Hotel Bel-Air , Le Meurice , Hotel Plaza Athenee , Hotel Eden and Hotel Principe di Savoia and asks readers to consider how we are putting money directly into the pockets of men who choose to stone and whip to death their own citizens for being gay or accused of adultery .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Clooney","lists","9","hotels","including","The","Dorchester",",","45","Park","Lane",",","Coworth","Park",",","The","Beverly","Hills","Hotel",",","Hotel","Bel-Air",",","Le","Meurice",",","Hotel","Plaza","Athenee",",","Hotel","Eden","and","Hotel","Principe","di","Savoia","and","asks","readers","to","consider","how","we","are","putting","money","directly","into","the","pockets","of","men","who","choose","to","stone","and","whip","to","death","their","own","citizens","for","being","gay","or","accused","of","adultery","."],"labels":["B-person","O","O","O","O","B-location","I-location","O","B-location","I-location","I-location","O","B-location","I-location","O","B-location","I-location","I-location","I-location","O","B-location","I-location","O","B-location","I-location","O","B-location","I-location","I-location","O","B-location","I-location","O","B-location","I-location","I-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["song","musical_instrument","country","album","award","event","location","band","organization","musical_artist","music_genre","person"]}
{"id":"367","dataset":"crossner_music","split":"test","instance":{"id":"367","prompt_labels":"In(O) 2019(O) Ulvaeus(B-musical artist) worked(O) with(O) Swedish(O) songwriter(O) Andreas(B-musical artist) Carlsson(I-musical artist) to(O) arrange(O) an(O) English(O) dub(O) of(O) Tomas(B-musical artist) Ledin(I-musical artist) jukebox(O) musical(O) film(O) En(B-song) del(I-song) av(I-song) mitt(I-song) hjrta(I-song) ((O) English(O) :(O) A(B-song) Piece(I-song) of(I-song) My(I-song) Heart(I-song) )(O) directed(O) by(O) Edward(B-person) af(I-person) Silln(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, location, person, album, award, music genre, musical instrument, country, event, musical artist, organization, band and O.\nSentence: In 2019 Ulvaeus worked with Swedish songwriter Andreas Carlsson to arrange an English dub of Tomas Ledin jukebox musical film En del av mitt hjrta ( English : A Piece of My Heart ) directed by Edward af Silln .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2019","Ulvaeus","worked","with","Swedish","songwriter","Andreas","Carlsson","to","arrange","an","English","dub","of","Tomas","Ledin","jukebox","musical","film","En","del","av","mitt","hjrta","(","English",":","A","Piece","of","My","Heart",")","directed","by","Edward","af","Silln","."],"labels":["O","O","B-musical artist","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","O","B-song","I-song","I-song","I-song","I-song","O","O","O","B-song","I-song","I-song","I-song","I-song","O","O","O","B-person","I-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["song","location","person","album","award","music_genre","musical_instrument","country","event","musical_artist","organization","band"]}
{"id":"368","dataset":"crossner_music","split":"test","instance":{"id":"368","prompt_labels":"Prior(O) to(O) recording(O) ,(O) Dylan(B-musical artist) previewed(O) the(O) songs(O) that(O) would(O) constitute(O) Blood(B-album) on(I-album) the(I-album) Tracks(I-album) for(O) a(O) number(O) of(O) friends(O) in(O) the(O) music(O) world(O) ,(O) including(O) David(B-musical artist) Crosby(I-musical artist) ,(O) Graham(B-musical artist) Nash(I-musical artist) ,(O) Stephen(B-musical artist) Stills(I-musical artist) ,(O) Tim(B-musical artist) Drummond(I-musical artist) and(O) Peter(B-musical artist) Rowan(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, album, musical artist, band, organization, country, musical instrument, music genre, song, location, event, person and O.\nSentence: Prior to recording , Dylan previewed the songs that would constitute Blood on the Tracks for a number of friends in the music world , including David Crosby , Graham Nash , Stephen Stills , Tim Drummond and Peter Rowan .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Prior","to","recording",",","Dylan","previewed","the","songs","that","would","constitute","Blood","on","the","Tracks","for","a","number","of","friends","in","the","music","world",",","including","David","Crosby",",","Graham","Nash",",","Stephen","Stills",",","Tim","Drummond","and","Peter","Rowan","."],"labels":["O","O","O","O","B-musical artist","O","O","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["award","album","musical_artist","band","organization","country","musical_instrument","music_genre","song","location","event","person"]}
{"id":"369","dataset":"crossner_music","split":"test","instance":{"id":"369","prompt_labels":"Their(O) third(O) album(O) ,(O) Inhuman(B-album) Rampage(I-album) ((O) 2006(O) )(O) ,(O) was(O) certified(O) gold(O) by(O) the(O) Recording(B-organization) Industry(I-organization) Association(I-organization) of(I-organization) America(I-organization) ((O) RIAA(B-organization) )(O) and(O) the(O) British(B-organization) Phonographic(I-organization) Industry(I-organization) ((O) BPI(B-organization) )(O) ;(O) its(O) lead(O) single(O) ,(O) Through(B-song) the(I-song) Fire(I-song) and(I-song) Flames(I-song) ,(O) is(O) their(O) best-known(O) song(O) ,(O) and(O) was(O) featured(O) in(O) several(O) video(O) games(O) ,(O) including(O) Guitar(O) Hero(O) III(O) :(O) Legends(O) of(O) Rock(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, award, event, organization, musical instrument, location, musical artist, music genre, album, country, person, song and O.\nSentence: Their third album , Inhuman Rampage ( 2006 ) , was certified gold by the Recording Industry Association of America ( RIAA ) and the British Phonographic Industry ( BPI ) ; its lead single , Through the Fire and Flames , is their best-known song , and was featured in several video games , including Guitar Hero III : Legends of Rock .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Their","third","album",",","Inhuman","Rampage","(","2006",")",",","was","certified","gold","by","the","Recording","Industry","Association","of","America","(","RIAA",")","and","the","British","Phonographic","Industry","(","BPI",")",";","its","lead","single",",","Through","the","Fire","and","Flames",",","is","their","best-known","song",",","and","was","featured","in","several","video","games",",","including","Guitar","Hero","III",":","Legends","of","Rock","."],"labels":["O","O","O","O","B-album","I-album","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","B-song","I-song","I-song","I-song","I-song","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["band","award","event","organization","musical_instrument","location","musical_artist","music_genre","album","country","person","song"]}
{"id":"370","dataset":"crossner_music","split":"test","instance":{"id":"370","prompt_labels":"Medeski(B-band) Martin(I-band) &(I-band) amp(I-band) ;(I-band) Wood(I-band) ,(O) Robert(B-band) Randolph(I-band) &(I-band) amp(I-band) ;(I-band) the(I-band) Family(I-band) Band(I-band) ,(O) Galactic(B-band) ,(O) Widespread(B-band) Panic(I-band) ,(O) Jam(B-band) Underground(I-band) ,(O) Diazpora(B-band) ,(O) Soulive(B-band) ,(O) and(O) Karl(B-band) Denson(I-band) 's(I-band) Tiny(I-band) Universe(I-band) all(O) drew(O) heavily(O) from(O) the(O) funk(B-music genre) tradition(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, song, band, award, musical artist, location, musical instrument, album, event, music genre, person and O.\nSentence: Medeski Martin & amp ; Wood , Robert Randolph & amp ; the Family Band , Galactic , Widespread Panic , Jam Underground , Diazpora , Soulive , and Karl Denson 's Tiny Universe all drew heavily from the funk tradition .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Medeski","Martin","&","amp",";","Wood",",","Robert","Randolph","&","amp",";","the","Family","Band",",","Galactic",",","Widespread","Panic",",","Jam","Underground",",","Diazpora",",","Soulive",",","and","Karl","Denson","'s","Tiny","Universe","all","drew","heavily","from","the","funk","tradition","."],"labels":["B-band","I-band","I-band","I-band","I-band","I-band","O","B-band","I-band","I-band","I-band","I-band","I-band","I-band","I-band","O","B-band","O","B-band","I-band","O","B-band","I-band","O","B-band","O","B-band","O","O","B-band","I-band","I-band","I-band","I-band","O","O","O","O","O","B-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["country","organization","song","band","award","musical_artist","location","musical_instrument","album","event","music_genre","person"]}
{"id":"372","dataset":"crossner_music","split":"test","instance":{"id":"372","prompt_labels":"Steve(B-musical artist) Huey(I-musical artist) of(O) AllMusic(B-organization) said(O) that(O) the(O) band(O) 's(O) earlier(O) Punk(B-music genre) rock(I-music genre) influences(O) are(O) rarely(O) detectable(O) ,(O) replaced(O) by(O) surprisingly(O) effective(O) appropriations(O) of(O) Pop(B-music genre) music(I-music genre) and(O) psychedelia(B-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, album, music genre, organization, country, award, event, musical instrument, musical artist, person, band, song and O.\nSentence: Steve Huey of AllMusic said that the band 's earlier Punk rock influences are rarely detectable , replaced by surprisingly effective appropriations of Pop music and psychedelia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Steve","Huey","of","AllMusic","said","that","the","band","'s","earlier","Punk","rock","influences","are","rarely","detectable",",","replaced","by","surprisingly","effective","appropriations","of","Pop","music","and","psychedelia","."],"labels":["B-musical artist","I-musical artist","O","B-organization","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","O"],"target_index":null,"target_label":null},"label_list":["location","album","music_genre","organization","country","award","event","musical_instrument","musical_artist","person","band","song"]}
{"id":"373","dataset":"crossner_music","split":"test","instance":{"id":"373","prompt_labels":"Delta(B-music genre) blues(I-music genre) was(O) also(O) an(O) inspiration(O) for(O) the(O) creation(O) of(O) British(O) skiffle(B-music genre) music(I-music genre) ,(O) from(O) which(O) eventually(O) came(O) the(O) British(O) invasion(O) bands(O) ,(O) while(O) simultaneously(O) influencing(O) British(B-music genre) blues(I-music genre) ,(O) which(O) led(O) to(O) the(O) birth(O) of(O) early(O) hard(B-music genre) rock(I-music genre) and(O) Heavy(B-music genre) metal(I-music genre) music(I-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, location, song, person, award, music genre, event, album, organization, musical artist, country, band and O.\nSentence: Delta blues was also an inspiration for the creation of British skiffle music , from which eventually came the British invasion bands , while simultaneously influencing British blues , which led to the birth of early hard rock and Heavy metal music .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Delta","blues","was","also","an","inspiration","for","the","creation","of","British","skiffle","music",",","from","which","eventually","came","the","British","invasion","bands",",","while","simultaneously","influencing","British","blues",",","which","led","to","the","birth","of","early","hard","rock","and","Heavy","metal","music","."],"labels":["B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","location","song","person","award","music_genre","event","album","organization","musical_artist","country","band"]}
{"id":"374","dataset":"crossner_music","split":"test","instance":{"id":"374","prompt_labels":"Finn(B-musical artist) has(O) recorded(O) four(O) solo(O) albums(O) ,(O) Try(B-album) Whistling(I-album) This(I-album) ((O) 1998(O) )(O) ,(O) One(B-album) Nil(I-album) ((O) 2001(O) )(O) ,(O) Dizzy(B-album) Heights(I-album) ((O) 2014(O) )(O) ,(O) and(O) Out(B-album) of(I-album) Silence(I-album) ((O) 2017(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, song, location, album, music genre, musical instrument, award, musical artist, person, event, organization, country and O.\nSentence: Finn has recorded four solo albums , Try Whistling This ( 1998 ) , One Nil ( 2001 ) , Dizzy Heights ( 2014 ) , and Out of Silence ( 2017 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Finn","has","recorded","four","solo","albums",",","Try","Whistling","This","(","1998",")",",","One","Nil","(","2001",")",",","Dizzy","Heights","(","2014",")",",","and","Out","of","Silence","(","2017",")","."],"labels":["B-musical artist","O","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O","B-album","I-album","O","O","O","O","B-album","I-album","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["band","song","location","album","music_genre","musical_instrument","award","musical_artist","person","event","organization","country"]}
{"id":"376","dataset":"crossner_music","split":"test","instance":{"id":"376","prompt_labels":"A(O) breakthrough(O) came(O) when(O) Van(B-person) Damm(I-person) began(O) to(O) incorporate(O) glamorous(O) nude(O) females(O) on(O) stage(O) ,(O) inspired(O) by(O) the(O) Folies(B-location) Bergre(I-location) and(O) Moulin(B-location) Rouge(I-location) in(O) Paris(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, country, location, musical artist, music genre, album, musical instrument, song, event, award, band and O.\nSentence: A breakthrough came when Van Damm began to incorporate glamorous nude females on stage , inspired by the Folies Bergre and Moulin Rouge in Paris .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","breakthrough","came","when","Van","Damm","began","to","incorporate","glamorous","nude","females","on","stage",",","inspired","by","the","Folies","Bergre","and","Moulin","Rouge","in","Paris","."],"labels":["O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","I-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["person","organization","country","location","musical_artist","music_genre","album","musical_instrument","song","event","award","band"]}
{"id":"377","dataset":"crossner_music","split":"test","instance":{"id":"377","prompt_labels":"Since(O) 1993(O) ,(O) Jimmy(B-musical artist) Webb(I-musical artist) has(O) produced(O) five(O) critically(O) acclaimed(O) solo(O) albums(O) :(O) Suspending(B-album) Disbelief(I-album) ((O) 1993(O) )(O) ,(O) Ten(B-album) Easy(I-album) Pieces(I-album) ((O) 1996(O) )(O) ,(O) Twilight(B-album) of(I-album) the(I-album) Renegades(I-album) ((O) 2005(O) )(O) ,(O) Just(B-album) Across(I-album) the(I-album) River(I-album) ((O) 2010(O) )(O) ,(O) and(O) Still(B-album) Within(I-album) the(I-album) Sound(I-album) of(I-album) My(I-album) Voice(I-album) ((O) 2013(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, award, musical instrument, location, event, country, person, band, album, music genre, song, organization and O.\nSentence: Since 1993 , Jimmy Webb has produced five critically acclaimed solo albums : Suspending Disbelief ( 1993 ) , Ten Easy Pieces ( 1996 ) , Twilight of the Renegades ( 2005 ) , Just Across the River ( 2010 ) , and Still Within the Sound of My Voice ( 2013 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Since","1993",",","Jimmy","Webb","has","produced","five","critically","acclaimed","solo","albums",":","Suspending","Disbelief","(","1993",")",",","Ten","Easy","Pieces","(","1996",")",",","Twilight","of","the","Renegades","(","2005",")",",","Just","Across","the","River","(","2010",")",",","and","Still","Within","the","Sound","of","My","Voice","(","2013",")","."],"labels":["O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","B-album","I-album","O","O","O","O","B-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","O","B-album","I-album","I-album","I-album","I-album","I-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","award","musical_instrument","location","event","country","person","band","album","music_genre","song","organization"]}
{"id":"380","dataset":"crossner_music","split":"test","instance":{"id":"380","prompt_labels":"Brad(B-person) Shoup(I-person) of(O) Stereogum(O) surmised(O) that(O) ,(O) thanks(O) to(O) the(O) Ramones(B-band) '(O) praise(O) for(O) the(O) group(O) ,(O) many(O) punk(B-music genre) ,(O) pop(B-music genre) punk(I-music genre) ,(O) or(O) punk-adjacent(B-music genre) artists(O) showed(O) influence(O) from(O) the(B-band) Beach(I-band) Boys(I-band) ,(O) noting(O) cover(O) versions(O) of(O) the(O) band(O) 's(O) songs(O) recorded(O) by(O) Slickee(B-band) Boys(I-band) ,(O) Agent(B-band) Orange(I-band) ,(O) Bad(B-band) Religion(I-band) ,(O) Shonen(B-band) Knife(I-band) ,(O) the(B-band) Queers(I-band) ,(O) Hi-Standard(B-band) ,(O) the(B-band) Descendents(I-band) ,(O) the(B-band) Donnas(I-band) ,(O) M.O.D.(B-band) ,(O) and(O) the(B-band) Vandals(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, album, event, person, musical instrument, band, musical artist, location, award, country, organization, music genre and O.\nSentence: Brad Shoup of Stereogum surmised that , thanks to the Ramones ' praise for the group , many punk , pop punk , or punk-adjacent artists showed influence from the Beach Boys , noting cover versions of the band 's songs recorded by Slickee Boys , Agent Orange , Bad Religion , Shonen Knife , the Queers , Hi-Standard , the Descendents , the Donnas , M.O.D. , and the Vandals .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Brad","Shoup","of","Stereogum","surmised","that",",","thanks","to","the","Ramones","'","praise","for","the","group",",","many","punk",",","pop","punk",",","or","punk-adjacent","artists","showed","influence","from","the","Beach","Boys",",","noting","cover","versions","of","the","band","'s","songs","recorded","by","Slickee","Boys",",","Agent","Orange",",","Bad","Religion",",","Shonen","Knife",",","the","Queers",",","Hi-Standard",",","the","Descendents",",","the","Donnas",",","M.O.D.",",","and","the","Vandals","."],"labels":["B-person","I-person","O","O","O","O","O","O","O","O","B-band","O","O","O","O","O","O","O","B-music genre","O","B-music genre","I-music genre","O","O","B-music genre","O","O","O","O","B-band","I-band","I-band","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","O","B-band","I-band","O","B-band","I-band","O","B-band","I-band","O","B-band","I-band","O","B-band","O","B-band","I-band","O","B-band","I-band","O","B-band","O","O","B-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["song","album","event","person","musical_instrument","band","musical_artist","location","award","country","organization","music_genre"]}
{"id":"383","dataset":"crossner_music","split":"test","instance":{"id":"383","prompt_labels":"A(O) third(O) UK(B-country) tour(O) for(O) 2017(O) /(O) 2018(O) opened(O) at(O) the(O) Curve(B-organization) in(O) Leicester(B-location) ,(O) and(O) also(O) toured(O) to(O) the(O) Birmingham(B-location) Hippodrome(I-location) ,(O) the(O) Bord(B-location) Gis(I-location) Energy(I-location) Theatre(I-location) in(O) Dublin(B-location) ,(O) the(O) Wales(B-location) Millennium(I-location) Centre(I-location) in(O) Cardiff(B-location) ,(O) the(O) Edinburgh(B-location) Festival(I-location) Theatre(I-location) ,(O) the(O) Mayflower(B-location) Theatre(I-location) in(O) Southampton(B-location) and(O) the(O) Palace(B-location) Theatre(I-location) in(O) Manchester(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, musical instrument, organization, song, location, album, band, event, music genre, country, musical artist and O.\nSentence: A third UK tour for 2017 / 2018 opened at the Curve in Leicester , and also toured to the Birmingham Hippodrome , the Bord Gis Energy Theatre in Dublin , the Wales Millennium Centre in Cardiff , the Edinburgh Festival Theatre , the Mayflower Theatre in Southampton and the Palace Theatre in Manchester .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","third","UK","tour","for","2017","/","2018","opened","at","the","Curve","in","Leicester",",","and","also","toured","to","the","Birmingham","Hippodrome",",","the","Bord","Gis","Energy","Theatre","in","Dublin",",","the","Wales","Millennium","Centre","in","Cardiff",",","the","Edinburgh","Festival","Theatre",",","the","Mayflower","Theatre","in","Southampton","and","the","Palace","Theatre","in","Manchester","."],"labels":["O","O","B-country","O","O","O","O","O","O","O","O","B-organization","O","B-location","O","O","O","O","O","O","B-location","I-location","O","O","B-location","I-location","I-location","I-location","O","B-location","O","O","B-location","I-location","I-location","O","B-location","O","O","B-location","I-location","I-location","O","O","B-location","I-location","O","B-location","O","O","B-location","I-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["award","person","musical_instrument","organization","song","location","album","band","event","music_genre","country","musical_artist"]}
{"id":"384","dataset":"crossner_music","split":"test","instance":{"id":"384","prompt_labels":"Arthur(B-musical artist) Schwartz(I-musical artist) and(O) Howard(B-musical artist) Dietz(I-musical artist) first(O) tried(O) ,(O) and(O) then(O) Richard(B-musical artist) Rodgers(I-musical artist) and(O) Oscar(B-musical artist) Hammerstein(I-musical artist) II(I-musical artist) attempted(O) ,(O) but(O) gave(O) up(O) and(O) Hammerstein(B-musical artist) told(O) Lerner(B-musical artist) ,(O) Pygmalion(O) had(O) no(O) subplot(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, country, album, event, song, music genre, band, organization, musical artist, musical instrument, award and O.\nSentence: Arthur Schwartz and Howard Dietz first tried , and then Richard Rodgers and Oscar Hammerstein II attempted , but gave up and Hammerstein told Lerner , Pygmalion had no subplot .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Arthur","Schwartz","and","Howard","Dietz","first","tried",",","and","then","Richard","Rodgers","and","Oscar","Hammerstein","II","attempted",",","but","gave","up","and","Hammerstein","told","Lerner",",","Pygmalion","had","no","subplot","."],"labels":["B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","O","O","O","O","O","B-musical artist","O","B-musical artist","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","person","country","album","event","song","music_genre","band","organization","musical_artist","musical_instrument","award"]}
{"id":"386","dataset":"crossner_music","split":"test","instance":{"id":"386","prompt_labels":"Morissette(B-musical artist) assumed(O) creative(O) control(O) and(O) producing(O) duties(O) for(O) her(O) subsequent(O) studio(O) albums(O) ,(O) including(O) Under(B-album) Rug(I-album) Swept(I-album) ((O) 2002(O) )(O) ,(O) So-Called(B-album) Chaos(I-album) ((O) 2004(O) )(O) ,(O) and(O) Flavors(B-album) of(I-album) Entanglement(I-album) ((O) 2008(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, album, song, award, country, musical instrument, person, music genre, location, musical artist, event, organization and O.\nSentence: Morissette assumed creative control and producing duties for her subsequent studio albums , including Under Rug Swept ( 2002 ) , So-Called Chaos ( 2004 ) , and Flavors of Entanglement ( 2008 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Morissette","assumed","creative","control","and","producing","duties","for","her","subsequent","studio","albums",",","including","Under","Rug","Swept","(","2002",")",",","So-Called","Chaos","(","2004",")",",","and","Flavors","of","Entanglement","(","2008",")","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O","B-album","I-album","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["band","album","song","award","country","musical_instrument","person","music_genre","location","musical_artist","event","organization"]}
{"id":"387","dataset":"crossner_music","split":"test","instance":{"id":"387","prompt_labels":"He(O) also(O) recorded(O) jazz(B-music genre) ,(O) such(O) as(O) The(B-album) Great(I-album) Ray(I-album) Charles(I-album) ((O) 1957(O) )(O) and(O) worked(O) with(O) vibraphonist(O) Milt(B-musical artist) Jackson(I-musical artist) ,(O) releasing(O) Soul(B-album) Brothers(I-album) in(O) 1958(O) and(O) Soul(B-album) Meeting(I-album) in(O) 1961(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, song, country, event, location, band, music genre, organization, award, album, musical artist, musical instrument and O.\nSentence: He also recorded jazz , such as The Great Ray Charles ( 1957 ) and worked with vibraphonist Milt Jackson , releasing Soul Brothers in 1958 and Soul Meeting in 1961 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","also","recorded","jazz",",","such","as","The","Great","Ray","Charles","(","1957",")","and","worked","with","vibraphonist","Milt","Jackson",",","releasing","Soul","Brothers","in","1958","and","Soul","Meeting","in","1961","."],"labels":["O","O","O","B-music genre","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","B-album","I-album","O","O","O","B-album","I-album","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","song","country","event","location","band","music_genre","organization","award","album","musical_artist","musical_instrument"]}
{"id":"388","dataset":"crossner_music","split":"test","instance":{"id":"388","prompt_labels":"The(O) groups(O) which(O) made(O) up(O) the(O) Native(O) Tongues(O) Posse(O) tended(O) toward(O) jazzy(O) releases(O) :(O) these(O) include(O) the(O) Jungle(B-band) Brothers(I-band) '(O) debut(O) Straight(B-album) Out(I-album) the(I-album) Jungle(I-album) ((O) 1988(O) )(O) ,(O) and(O) A(B-band) Tribe(I-band) Called(I-band) Quest(I-band) '(O) s(O) People(B-album) 's(I-album) Instinctive(I-album) Travels(I-album) and(I-album) the(I-album) Paths(I-album) of(I-album) Rhythm(I-album) ((O) 1990(O) )(O) and(O) The(B-album) Low(I-album) End(I-album) Theory(I-album) ((O) 1991(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, band, song, musical instrument, album, music genre, person, country, musical artist, location, award and O.\nSentence: The groups which made up the Native Tongues Posse tended toward jazzy releases : these include the Jungle Brothers ' debut Straight Out the Jungle ( 1988 ) , and A Tribe Called Quest ' s People 's Instinctive Travels and the Paths of Rhythm ( 1990 ) and The Low End Theory ( 1991 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","groups","which","made","up","the","Native","Tongues","Posse","tended","toward","jazzy","releases",":","these","include","the","Jungle","Brothers","'","debut","Straight","Out","the","Jungle","(","1988",")",",","and","A","Tribe","Called","Quest","'","s","People","'s","Instinctive","Travels","and","the","Paths","of","Rhythm","(","1990",")","and","The","Low","End","Theory","(","1991",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","O","O","B-album","I-album","I-album","I-album","O","O","O","O","O","B-band","I-band","I-band","I-band","O","O","B-album","I-album","I-album","I-album","I-album","I-album","I-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","organization","band","song","musical_instrument","album","music_genre","person","country","musical_artist","location","award"]}
{"id":"389","dataset":"crossner_music","split":"test","instance":{"id":"389","prompt_labels":"The(O) music(O) arose(O) as(O) a(O) synthesis(O) of(O) traditional(O) Creole(B-music genre) music(I-music genre) ,(O) some(O) Cajun(B-music genre) music(I-music genre) influences(O) ,(O) and(O) African-American(O) traditions(O) ,(O) including(O) Rhythm(B-music genre) and(I-music genre) blues(I-music genre) ,(O) blues(B-music genre) ,(O) jazz(B-music genre) ,(O) and(O) Gospel(B-music genre) music(I-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, organization, song, musical instrument, award, musical artist, music genre, album, country, location, band and O.\nSentence: The music arose as a synthesis of traditional Creole music , some Cajun music influences , and African-American traditions , including Rhythm and blues , blues , jazz , and Gospel music .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","music","arose","as","a","synthesis","of","traditional","Creole","music",",","some","Cajun","music","influences",",","and","African-American","traditions",",","including","Rhythm","and","blues",",","blues",",","jazz",",","and","Gospel","music","."],"labels":["O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","B-music genre","I-music genre","I-music genre","O","B-music genre","O","B-music genre","O","O","B-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["event","person","organization","song","musical_instrument","award","musical_artist","music_genre","album","country","location","band"]}
{"id":"390","dataset":"crossner_music","split":"test","instance":{"id":"390","prompt_labels":"Lyngstad(B-musical artist) returned(O) to(O) the(O) recording(O) studio(O) in(O) 2010(O) to(O) record(O) vocals(O) for(O) the(O) Cat(B-musical artist) Stevens(I-musical artist) song(O) Morning(B-song) Has(I-song) Broken(I-song) ,(O) for(O) Swedish(O) guitarist(O) Georg(B-musical artist) Wadenius(I-musical artist) '(O) s(O) album(O) Reconnection(B-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, country, musical artist, organization, award, person, band, music genre, musical instrument, event, location, album and O.\nSentence: Lyngstad returned to the recording studio in 2010 to record vocals for the Cat Stevens song Morning Has Broken , for Swedish guitarist Georg Wadenius ' s album Reconnection .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Lyngstad","returned","to","the","recording","studio","in","2010","to","record","vocals","for","the","Cat","Stevens","song","Morning","Has","Broken",",","for","Swedish","guitarist","Georg","Wadenius","'","s","album","Reconnection","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-song","I-song","I-song","O","O","O","O","B-musical artist","I-musical artist","O","O","O","B-album","O"],"target_index":null,"target_label":null},"label_list":["song","country","musical_artist","organization","award","person","band","music_genre","musical_instrument","event","location","album"]}
{"id":"391","dataset":"crossner_music","split":"test","instance":{"id":"391","prompt_labels":"The(O) album(O) was(O) a(O) commercial(O) disappointment(O) compared(O) to(O) the(O) band(O) 's(O) previous(O) effort(O) ,(O) though(O) it(O) charted(O) generally(O) high(O) at(O) #(O) 5(O) in(O) the(O) United(B-country) Kingdom(I-country) and(O) Germany(B-country) ,(O) #(O) 7(O) on(O) the(O) Austria(B-country) n(O) and(O) Switzerland(B-country) music(O) charts(O) and(O) #(O) 8(O) in(O) Norway(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, band, award, location, organization, music genre, event, musical instrument, song, album, musical artist and O.\nSentence: The album was a commercial disappointment compared to the band 's previous effort , though it charted generally high at # 5 in the United Kingdom and Germany , # 7 on the Austria n and Switzerland music charts and # 8 in Norway .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","album","was","a","commercial","disappointment","compared","to","the","band","'s","previous","effort",",","though","it","charted","generally","high","at","#","5","in","the","United","Kingdom","and","Germany",",","#","7","on","the","Austria","n","and","Switzerland","music","charts","and","#","8","in","Norway","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O","B-country","O","O","O","O","O","B-country","O","O","B-country","O","O","O","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["person","country","band","award","location","organization","music_genre","event","musical_instrument","song","album","musical_artist"]}
{"id":"392","dataset":"crossner_music","split":"test","instance":{"id":"392","prompt_labels":"Returning(O) to(O) his(O) solo(O) work(O) ,(O) Iommi(B-musical artist) enlisted(O) bassist(O) Dave(B-musical artist) Spitz(I-musical artist) ((O) ex-(O) Great(B-band) White(I-band) )(O) ,(O) drummer(O) Eric(B-musical artist) Singer(I-musical artist) and(O) initially(O) intended(O) to(O) use(O) multiple(O) singers(O) ,(O) including(O) Rob(B-musical artist) Halford(I-musical artist) of(O) Judas(B-band) Priest(I-band) ,(O) former(O) Deep(B-band) Purple(I-band) and(O) Trapeze(B-band) vocalist(O) Glenn(B-musical artist) Hughes(I-musical artist) ,(O) and(O) former(O) Sabbath(B-band) vocalist(O) Ronnie(B-musical artist) James(I-musical artist) Dio(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, music genre, band, album, country, musical instrument, musical artist, organization, person, award, location, song and O.\nSentence: Returning to his solo work , Iommi enlisted bassist Dave Spitz ( ex- Great White ) , drummer Eric Singer and initially intended to use multiple singers , including Rob Halford of Judas Priest , former Deep Purple and Trapeze vocalist Glenn Hughes , and former Sabbath vocalist Ronnie James Dio .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Returning","to","his","solo","work",",","Iommi","enlisted","bassist","Dave","Spitz","(","ex-","Great","White",")",",","drummer","Eric","Singer","and","initially","intended","to","use","multiple","singers",",","including","Rob","Halford","of","Judas","Priest",",","former","Deep","Purple","and","Trapeze","vocalist","Glenn","Hughes",",","and","former","Sabbath","vocalist","Ronnie","James","Dio","."],"labels":["O","O","O","O","O","O","B-musical artist","O","O","B-musical artist","I-musical artist","O","O","B-band","I-band","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-band","I-band","O","O","B-band","I-band","O","B-band","O","B-musical artist","I-musical artist","O","O","O","B-band","O","B-musical artist","I-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["event","music_genre","band","album","country","musical_instrument","musical_artist","organization","person","award","location","song"]}
{"id":"394","dataset":"crossner_music","split":"test","instance":{"id":"394","prompt_labels":"The(O) exceptions(O) to(O) this(O) scheme(O) were(O) the(O) band(O) 's(O) fourth(O) album(O) ,(O) a(O) live(O) boxed(O) set(O) entitled(O) Chicago(B-album) at(I-album) Carnegie(I-album) Hall(I-album) ,(O) their(O) twelfth(O) album(O) Hot(B-album) Streets(I-album) ,(O) and(O) the(O) Arabic-numbered(O) Chicago(B-album) 13(I-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, music genre, award, song, musical instrument, organization, country, musical artist, event, band, album, location and O.\nSentence: The exceptions to this scheme were the band 's fourth album , a live boxed set entitled Chicago at Carnegie Hall , their twelfth album Hot Streets , and the Arabic-numbered Chicago 13 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","exceptions","to","this","scheme","were","the","band","'s","fourth","album",",","a","live","boxed","set","entitled","Chicago","at","Carnegie","Hall",",","their","twelfth","album","Hot","Streets",",","and","the","Arabic-numbered","Chicago","13","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","B-album","I-album","O","O","O","O","B-album","I-album","O"],"target_index":null,"target_label":null},"label_list":["person","music_genre","award","song","musical_instrument","organization","country","musical_artist","event","band","album","location"]}
{"id":"395","dataset":"crossner_music","split":"test","instance":{"id":"395","prompt_labels":"Deacon(B-band) Blue(I-band) also(O) performed(O) at(O) the(O) Glasgow(B-location) 2014(B-event) Commonwealth(I-event) Games(I-event) closing(O) ceremony(O) on(O) 3(O) August(O) 2014(O) ,(O) performing(O) their(O) hit(O) ,(O) Dignity(B-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, song, person, award, event, location, band, musical artist, musical instrument, music genre, album and O.\nSentence: Deacon Blue also performed at the Glasgow 2014 Commonwealth Games closing ceremony on 3 August 2014 , performing their hit , Dignity .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Deacon","Blue","also","performed","at","the","Glasgow","2014","Commonwealth","Games","closing","ceremony","on","3","August","2014",",","performing","their","hit",",","Dignity","."],"labels":["B-band","I-band","O","O","O","O","B-location","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","B-song","O"],"target_index":null,"target_label":null},"label_list":["organization","country","song","person","award","event","location","band","musical_artist","musical_instrument","music_genre","album"]}
{"id":"397","dataset":"crossner_music","split":"test","instance":{"id":"397","prompt_labels":"The(O) film(O) was(O) nominated(O) for(O) Academy(B-award) Awards(I-award) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Costume(I-award) Design(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Score(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Screenplay(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, award, event, album, music genre, location, musical artist, band, country, song, person, organization and O.\nSentence: The film was nominated for Academy Awards for Academy Award for Best Costume Design , Academy Award for Best Original Score and Academy Award for Best Original Screenplay .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","film","was","nominated","for","Academy","Awards","for","Academy","Award","for","Best","Costume","Design",",","Academy","Award","for","Best","Original","Score","and","Academy","Award","for","Best","Original","Screenplay","."],"labels":["O","O","O","O","O","B-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","award","event","album","music_genre","location","musical_artist","band","country","song","person","organization"]}
{"id":"398","dataset":"crossner_music","split":"test","instance":{"id":"398","prompt_labels":"He(O) hosted(O) the(O) television(O) show(O) Gastank(O) ,(O) and(O) recorded(O) his(O) first(O) of(O) several(O) New-age(B-music genre) music(I-music genre) ,(O) ambient(B-music genre) ,(O) and(O) Christian(B-music genre) music(I-music genre) albums(O) with(O) Country(B-album) Airs(I-album) ((O) 1986(O) )(O) and(O) The(B-album) Gospels(I-album) ((O) 1987(O) )(O) ,(O) respectively(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, music genre, event, song, musical instrument, organization, award, country, person, musical artist, band, album and O.\nSentence: He hosted the television show Gastank , and recorded his first of several New-age music , ambient , and Christian music albums with Country Airs ( 1986 ) and The Gospels ( 1987 ) , respectively .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","hosted","the","television","show","Gastank",",","and","recorded","his","first","of","several","New-age","music",",","ambient",",","and","Christian","music","albums","with","Country","Airs","(","1986",")","and","The","Gospels","(","1987",")",",","respectively","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","O","O","B-music genre","I-music genre","O","O","B-album","I-album","O","O","O","O","B-album","I-album","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","music_genre","event","song","musical_instrument","organization","award","country","person","musical_artist","band","album"]}
{"id":"399","dataset":"crossner_music","split":"test","instance":{"id":"399","prompt_labels":"Heavily(O) based(O) in(O) soul(B-music genre) music(I-music genre) ,(O) neo(B-music genre) soul(I-music genre) is(O) distinguished(O) by(O) a(O) less(O) conventional(O) sound(O) than(O) its(O) contemporary(O) R(B-music genre) &(I-music genre) B(I-music genre) counterpart(O) ,(O) with(O) incorporated(O) elements(O) ranging(O) from(O) jazz(B-music genre) ,(O) funk(B-music genre) ,(O) Hip(B-music genre) hop(I-music genre) music(I-music genre) and(O) Electronic(B-music genre) music(I-music genre) to(O) Pop(B-music genre) music(I-music genre) ,(O) Jazz(B-music genre) fusion(I-music genre) ,(O) and(O) African(B-music genre) music(I-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, album, musical artist, music genre, organization, country, musical instrument, location, band, award, person, song and O.\nSentence: Heavily based in soul music , neo soul is distinguished by a less conventional sound than its contemporary R & B counterpart , with incorporated elements ranging from jazz , funk , Hip hop music and Electronic music to Pop music , Jazz fusion , and African music .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Heavily","based","in","soul","music",",","neo","soul","is","distinguished","by","a","less","conventional","sound","than","its","contemporary","R","&","B","counterpart",",","with","incorporated","elements","ranging","from","jazz",",","funk",",","Hip","hop","music","and","Electronic","music","to","Pop","music",",","Jazz","fusion",",","and","African","music","."],"labels":["O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","I-music genre","O","O","O","O","O","O","O","B-music genre","O","B-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O","B-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["event","album","musical_artist","music_genre","organization","country","musical_instrument","location","band","award","person","song"]}
{"id":"402","dataset":"crossner_music","split":"test","instance":{"id":"402","prompt_labels":"For(O) example(O) ,(O) in(O) Major(B-song) Minus(I-song) and(O) Hurts(B-song) Like(I-song) Heaven(I-song) ,(O) both(O) from(O) the(O) album(O) Mylo(B-album) Xyloto(I-album) ((O) 2011(O) )(O) ,(O) Chris(B-musical artist) Martin(I-musical artist) '(O) s(O) vocals(O) are(O) mostly(O) vocoder-processed(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, album, country, person, band, location, event, award, musical instrument, song, music genre, musical artist and O.\nSentence: For example , in Major Minus and Hurts Like Heaven , both from the album Mylo Xyloto ( 2011 ) , Chris Martin ' s vocals are mostly vocoder-processed .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","example",",","in","Major","Minus","and","Hurts","Like","Heaven",",","both","from","the","album","Mylo","Xyloto","(","2011",")",",","Chris","Martin","'","s","vocals","are","mostly","vocoder-processed","."],"labels":["O","O","O","O","B-song","I-song","O","B-song","I-song","I-song","O","O","O","O","O","B-album","I-album","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","album","country","person","band","location","event","award","musical_instrument","song","music_genre","musical_artist"]}
{"id":"403","dataset":"crossner_music","split":"test","instance":{"id":"403","prompt_labels":"Major(O) artists(O) of(O) the(O) Texas(O) style(O) are(O) Johnny(B-musical artist) Winter(I-musical artist) ,(O) Stevie(B-musical artist) Ray(I-musical artist) Vaughan(I-musical artist) ,(O) the(O) The(B-band) Fabulous(I-band) Thunderbirds(I-band) ((O) led(O) by(O) harmonica(B-musical instrument) player(O) and(O) singer-songwriter(O) Kim(B-musical artist) Wilson(I-musical artist) )(O) ,(O) and(O) ZZ(B-band) Top(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, music genre, country, location, organization, event, musical instrument, award, song, musical artist, band, person and O.\nSentence: Major artists of the Texas style are Johnny Winter , Stevie Ray Vaughan , the The Fabulous Thunderbirds ( led by harmonica player and singer-songwriter Kim Wilson ) , and ZZ Top .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Major","artists","of","the","Texas","style","are","Johnny","Winter",",","Stevie","Ray","Vaughan",",","the","The","Fabulous","Thunderbirds","(","led","by","harmonica","player","and","singer-songwriter","Kim","Wilson",")",",","and","ZZ","Top","."],"labels":["O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","O","B-band","I-band","I-band","O","O","O","B-musical instrument","O","O","O","B-musical artist","I-musical artist","O","O","O","B-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["album","music_genre","country","location","organization","event","musical_instrument","award","song","musical_artist","band","person"]}
{"id":"405","dataset":"crossner_music","split":"test","instance":{"id":"405","prompt_labels":"He(O) worked(O) with(O) Berlin(B-organization) State(I-organization) Opera(I-organization) ;(O) La(B-location) Scala(I-location) ,(O) Milan(B-location) ;(O) Royal(B-location) Opera(I-location) Stockholm(I-location) ;(O) the(O) Royal(B-location) Opera(I-location) House(I-location) at(O) Covent(B-location) Garden(I-location) ,(O) Chorgies(B-location) d(I-location) 'Orange(I-location) and(O) Houston(B-location) Grand(I-location) Opera(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, musical artist, organization, location, musical instrument, person, country, music genre, album, band, song, event and O.\nSentence: He worked with Berlin State Opera ; La Scala , Milan ; Royal Opera Stockholm ; the Royal Opera House at Covent Garden , Chorgies d 'Orange and Houston Grand Opera .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","worked","with","Berlin","State","Opera",";","La","Scala",",","Milan",";","Royal","Opera","Stockholm",";","the","Royal","Opera","House","at","Covent","Garden",",","Chorgies","d","'Orange","and","Houston","Grand","Opera","."],"labels":["O","O","O","B-organization","I-organization","I-organization","O","B-location","I-location","O","B-location","O","B-location","I-location","I-location","O","O","B-location","I-location","I-location","O","B-location","I-location","O","B-location","I-location","I-location","O","B-location","I-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["award","musical_artist","organization","location","musical_instrument","person","country","music_genre","album","band","song","event"]}
{"id":"406","dataset":"crossner_music","split":"test","instance":{"id":"406","prompt_labels":"Notable(O) recital(O) engagements(O) have(O) included(O) her(O) Carnegie(B-location) Hall(I-location) debut(O) and(O) performances(O) at(O) the(O) Kennedy(B-location) Center(I-location) ,(O) Symphony(B-location) Center(I-location) ,(O) Symphony(B-location) Hall(I-location) ,(O) Barbican(B-location) Centre(I-location) ,(O) Philharmonie(B-location) ,(O) and(O) Concertgebouw(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, musical artist, organization, musical instrument, country, album, band, award, music genre, song, event and O.\nSentence: Notable recital engagements have included her Carnegie Hall debut and performances at the Kennedy Center , Symphony Center , Symphony Hall , Barbican Centre , Philharmonie , and Concertgebouw .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Notable","recital","engagements","have","included","her","Carnegie","Hall","debut","and","performances","at","the","Kennedy","Center",",","Symphony","Center",",","Symphony","Hall",",","Barbican","Centre",",","Philharmonie",",","and","Concertgebouw","."],"labels":["O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","B-location","I-location","O","B-location","I-location","O","B-location","I-location","O","B-location","I-location","O","B-location","O","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["person","location","musical_artist","organization","musical_instrument","country","album","band","award","music_genre","song","event"]}
{"id":"407","dataset":"crossner_music","split":"test","instance":{"id":"407","prompt_labels":"He(O) is(O) the(O) recipient(O) of(O) numerous(O) accolades(O) ,(O) including(O) a(O) Cannes(B-award) Film(I-award) Festival(I-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) and(O) nominations(O) for(O) a(O) Tony(B-award) Award(I-award) ,(O) an(O) Academy(B-award) Awards(I-award) ,(O) two(O) Primetime(B-award) Emmy(I-award) Award(I-award) s(O) and(O) five(O) Golden(B-award) Globe(I-award) Awards(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, event, person, musical artist, country, location, award, album, band, song, music genre, organization and O.\nSentence: He is the recipient of numerous accolades , including a Cannes Film Festival Award for Best Actor and nominations for a Tony Award , an Academy Awards , two Primetime Emmy Award s and five Golden Globe Awards .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","is","the","recipient","of","numerous","accolades",",","including","a","Cannes","Film","Festival","Award","for","Best","Actor","and","nominations","for","a","Tony","Award",",","an","Academy","Awards",",","two","Primetime","Emmy","Award","s","and","five","Golden","Globe","Awards","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","B-award","I-award","O","O","B-award","I-award","O","O","B-award","I-award","I-award","O","O","O","B-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","event","person","musical_artist","country","location","award","album","band","song","music_genre","organization"]}
{"id":"408","dataset":"crossner_music","split":"test","instance":{"id":"408","prompt_labels":"In(O) this(O) era(O) ,(O) Chicago(B-location) radio(O) jocks(O) The(O) Hot(B-band) Mix(I-band) 5(I-band) ,(O) and(O) club(O) DJs(O) Ron(B-musical artist) Hardy(I-musical artist) and(O) Frankie(B-musical artist) Knuckles(I-musical artist) played(O) various(O) styles(O) of(O) dance(B-music genre) music(I-music genre) ,(O) including(O) older(O) disco(B-music genre) records(O) ,(O) newer(O) Italo(B-music genre) disco(I-music genre) and(O) electro(B-music genre) funk(I-music genre) tracks(O) ,(O) B-boy(O) hip(B-music genre) hop(I-music genre) music(I-music genre) by(O) Man(B-musical artist) Parrish(I-musical artist) ,(O) Jellybean(B-musical artist) Benitez(I-musical artist) ,(O) Arthur(B-musical artist) Baker(I-musical artist) and(O) John(B-musical artist) Robie(I-musical artist) as(O) well(O) as(O) Electropop(B-music genre) music(B-music genre) by(O) Kraftwerk(B-band) and(O) Yellow(B-band) Magic(I-band) Orchestra(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, band, musical instrument, location, album, country, event, musical artist, music genre, award, person, song and O.\nSentence: In this era , Chicago radio jocks The Hot Mix 5 , and club DJs Ron Hardy and Frankie Knuckles played various styles of dance music , including older disco records , newer Italo disco and electro funk tracks , B-boy hip hop music by Man Parrish , Jellybean Benitez , Arthur Baker and John Robie as well as Electropop music by Kraftwerk and Yellow Magic Orchestra .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","this","era",",","Chicago","radio","jocks","The","Hot","Mix","5",",","and","club","DJs","Ron","Hardy","and","Frankie","Knuckles","played","various","styles","of","dance","music",",","including","older","disco","records",",","newer","Italo","disco","and","electro","funk","tracks",",","B-boy","hip","hop","music","by","Man","Parrish",",","Jellybean","Benitez",",","Arthur","Baker","and","John","Robie","as","well","as","Electropop","music","by","Kraftwerk","and","Yellow","Magic","Orchestra","."],"labels":["O","O","O","O","B-location","O","O","O","B-band","I-band","I-band","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O","B-music genre","I-music genre","O","O","O","B-music genre","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O","O","B-music genre","I-music genre","I-music genre","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","B-music genre","B-music genre","O","B-band","O","B-band","I-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["organization","band","musical_instrument","location","album","country","event","musical_artist","music_genre","award","person","song"]}
{"id":"409","dataset":"crossner_music","split":"test","instance":{"id":"409","prompt_labels":"Nine(B-band) Inch(I-band) Nails(I-band) covered(O) the(O) song(O) Metal(B-song) on(O) The(B-album) Fragile(I-album) remix(O) album(O) Things(B-album) Falling(I-album) Apart(I-album) as(O) did(O) Afrika(B-musical artist) Bambaataa(I-musical artist) ((O) with(O) Numan(B-musical artist) himself(O) )(O) on(O) the(O) album(O) Dark(B-album) Matter(I-album) Moving(I-album) at(I-album) the(I-album) Speed(I-album) of(I-album) Light(I-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, country, award, person, location, band, album, song, music genre, event, organization, musical instrument and O.\nSentence: Nine Inch Nails covered the song Metal on The Fragile remix album Things Falling Apart as did Afrika Bambaataa ( with Numan himself ) on the album Dark Matter Moving at the Speed of Light .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Nine","Inch","Nails","covered","the","song","Metal","on","The","Fragile","remix","album","Things","Falling","Apart","as","did","Afrika","Bambaataa","(","with","Numan","himself",")","on","the","album","Dark","Matter","Moving","at","the","Speed","of","Light","."],"labels":["B-band","I-band","I-band","O","O","O","B-song","O","B-album","I-album","O","O","B-album","I-album","I-album","O","O","B-musical artist","I-musical artist","O","O","B-musical artist","O","O","O","O","O","B-album","I-album","I-album","I-album","I-album","I-album","I-album","I-album","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","country","award","person","location","band","album","song","music_genre","event","organization","musical_instrument"]}
{"id":"413","dataset":"crossner_music","split":"test","instance":{"id":"413","prompt_labels":"He(O) lent(O) active(O) support(O) to(O) several(O) philanthropic(O) endeavours(O) such(O) as(O) the(O) library(O) at(O) St.(B-location) Mary(I-location) 's(I-location) School(I-location) ,(O) Mumbai(B-location) ,(O) Bombay(B-location) Hospital(I-location) ,(O) Child(B-organization) Rights(I-organization) and(I-organization) You(I-organization) ,(O) Save(B-organization) the(I-organization) Children(I-organization) and(O) ALMA(B-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, band, musical artist, country, organization, musical instrument, event, song, person, location, award, music genre and O.\nSentence: He lent active support to several philanthropic endeavours such as the library at St. Mary 's School , Mumbai , Bombay Hospital , Child Rights and You , Save the Children and ALMA .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","lent","active","support","to","several","philanthropic","endeavours","such","as","the","library","at","St.","Mary","'s","School",",","Mumbai",",","Bombay","Hospital",",","Child","Rights","and","You",",","Save","the","Children","and","ALMA","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","I-location","O","B-location","O","B-location","I-location","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","O"],"target_index":null,"target_label":null},"label_list":["album","band","musical_artist","country","organization","musical_instrument","event","song","person","location","award","music_genre"]}
{"id":"414","dataset":"crossner_music","split":"test","instance":{"id":"414","prompt_labels":"She(O) was(O) a(O) fellow(O) ,(O) examiner(O) ,(O) lecturer(O) ,(O) committee(O) and(O) council(O) member(O) of(O) the(O) Imperial(B-organization) Society(I-organization) of(I-organization) Teachers(I-organization) of(I-organization) Dancing(I-organization) ,(O) life(O) member(O) of(O) the(O) Royal(B-organization) Academy(I-organization) of(I-organization) Dance(I-organization) ,(O) and(O) Honorary(B-award) MA(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, musical instrument, location, album, organization, band, person, country, music genre, song, musical artist, event and O.\nSentence: She was a fellow , examiner , lecturer , committee and council member of the Imperial Society of Teachers of Dancing , life member of the Royal Academy of Dance , and Honorary MA .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","was","a","fellow",",","examiner",",","lecturer",",","committee","and","council","member","of","the","Imperial","Society","of","Teachers","of","Dancing",",","life","member","of","the","Royal","Academy","of","Dance",",","and","Honorary","MA","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["award","musical_instrument","location","album","organization","band","person","country","music_genre","song","musical_artist","event"]}
{"id":"416","dataset":"crossner_music","split":"test","instance":{"id":"416","prompt_labels":"Electropop(B-music genre) is(O) a(O) music(O) genre(O) combining(O) elements(O) of(O) Electronic(B-music genre) music(I-music genre) and(O) Pop(B-music genre) music(I-music genre) genres(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, album, music genre, award, country, musical artist, event, song, person, organization, location, musical instrument and O.\nSentence: Electropop is a music genre combining elements of Electronic music and Pop music genres .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Electropop","is","a","music","genre","combining","elements","of","Electronic","music","and","Pop","music","genres","."],"labels":["B-music genre","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["band","album","music_genre","award","country","musical_artist","event","song","person","organization","location","musical_instrument"]}
{"id":"418","dataset":"crossner_music","split":"test","instance":{"id":"418","prompt_labels":"After(O) attaining(O) mainstream(O) success(O) ,(O) Cobain(B-musical artist) became(O) a(O) devoted(O) champion(O) of(O) lesser(O) known(O) indie(O) bands(O) ,(O) covering(O) songs(O) by(O) The(B-band) Vaselines(I-band) ,(O) Meat(B-band) Puppets(I-band) ,(O) Wipers(B-band) and(O) Fang(B-band) onstage(O) and(O) /(O) or(O) in(O) the(O) studio(O) ,(O) wearing(O) Daniel(B-musical artist) Johnston(I-musical artist) T-shirts(O) during(O) photo(O) shoots(O) ,(O) having(O) the(O) K(O) Records(O) logo(O) tattooed(O) on(O) his(O) forearm(O) ,(O) and(O) enlisting(O) bands(O) like(O) Butthole(B-band) Surfers(I-band) ,(O) Shonen(B-band) Knife(I-band) ,(O) Chokebore(B-band) and(O) Half(B-band) Japanese(I-band) along(O) for(O) the(O) In(B-album) Utero(I-album) tour(O) in(O) late(O) 1993(O) and(O) early(O) 1994(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, award, music genre, organization, country, person, event, musical instrument, album, band, song, location and O.\nSentence: After attaining mainstream success , Cobain became a devoted champion of lesser known indie bands , covering songs by The Vaselines , Meat Puppets , Wipers and Fang onstage and / or in the studio , wearing Daniel Johnston T-shirts during photo shoots , having the K Records logo tattooed on his forearm , and enlisting bands like Butthole Surfers , Shonen Knife , Chokebore and Half Japanese along for the In Utero tour in late 1993 and early 1994 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","attaining","mainstream","success",",","Cobain","became","a","devoted","champion","of","lesser","known","indie","bands",",","covering","songs","by","The","Vaselines",",","Meat","Puppets",",","Wipers","and","Fang","onstage","and","/","or","in","the","studio",",","wearing","Daniel","Johnston","T-shirts","during","photo","shoots",",","having","the","K","Records","logo","tattooed","on","his","forearm",",","and","enlisting","bands","like","Butthole","Surfers",",","Shonen","Knife",",","Chokebore","and","Half","Japanese","along","for","the","In","Utero","tour","in","late","1993","and","early","1994","."],"labels":["O","O","O","O","O","B-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","O","B-band","I-band","O","B-band","O","B-band","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","O","B-band","I-band","O","B-band","O","B-band","I-band","O","O","O","B-album","I-album","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","award","music_genre","organization","country","person","event","musical_instrument","album","band","song","location"]}
{"id":"420","dataset":"crossner_music","split":"test","instance":{"id":"420","prompt_labels":"He(O) has(O) won(O) four(O) Academy(B-award) Awards(I-award) ,(O) one(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) ,(O) and(O) three(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Screenplay(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, person, location, music genre, song, event, award, album, band, country, organization, musical instrument and O.\nSentence: He has won four Academy Awards , one for Academy Award for Best Director , and three for Academy Award for Best Original Screenplay .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","has","won","four","Academy","Awards",",","one","for","Academy","Award","for","Best","Director",",","and","three","for","Academy","Award","for","Best","Original","Screenplay","."],"labels":["O","O","O","O","B-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","person","location","music_genre","song","event","award","album","band","country","organization","musical_instrument"]}
{"id":"423","dataset":"crossner_music","split":"test","instance":{"id":"423","prompt_labels":"The(O) musical(O) was(O) nominated(O) for(O) three(O) Tony(B-award) Awards(I-award) :(O) Tony(B-award) Award(I-award) for(I-award) Best(I-award) Musical(I-award) ,(O) Tony(B-award) Award(I-award) for(I-award) Best(I-award) Scenic(I-award) Design(I-award) ,(O) and(O) Tony(B-award) Award(I-award) for(I-award) Best(I-award) Lighting(I-award) Design(I-award) ,(O) winning(O) the(O) latter(O) two(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, album, music genre, event, song, person, award, musical artist, country, musical instrument, band and O.\nSentence: The musical was nominated for three Tony Awards : Tony Award for Best Musical , Tony Award for Best Scenic Design , and Tony Award for Best Lighting Design , winning the latter two .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","musical","was","nominated","for","three","Tony","Awards",":","Tony","Award","for","Best","Musical",",","Tony","Award","for","Best","Scenic","Design",",","and","Tony","Award","for","Best","Lighting","Design",",","winning","the","latter","two","."],"labels":["O","O","O","O","O","O","B-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","location","album","music_genre","event","song","person","award","musical_artist","country","musical_instrument","band"]}
{"id":"424","dataset":"crossner_music","split":"test","instance":{"id":"424","prompt_labels":"Rain(O) Man(O) won(O) four(O) Academy(B-award) Awards(I-award) ,(O) including(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) for(O) Hoffman(B-person) ,(O) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) for(O) Barry(B-person) Levinson(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, music genre, song, band, organization, award, musical artist, location, person, event, musical instrument, album and O.\nSentence: Rain Man won four Academy Awards , including Academy Award for Best Picture , Academy Award for Best Actor for Hoffman , and Academy Award for Best Director for Barry Levinson .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Rain","Man","won","four","Academy","Awards",",","including","Academy","Award","for","Best","Picture",",","Academy","Award","for","Best","Actor","for","Hoffman",",","and","Academy","Award","for","Best","Director","for","Barry","Levinson","."],"labels":["O","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","B-person","O","O","B-award","I-award","I-award","I-award","I-award","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["country","music_genre","song","band","organization","award","musical_artist","location","person","event","musical_instrument","album"]}
{"id":"425","dataset":"crossner_music","split":"test","instance":{"id":"425","prompt_labels":"For(O) his(O) work(O) in(O) the(O) Western(O) film(O) Unforgiven(O) ((O) 1992(O) )(O) and(O) the(O) sports(O) drama(O) Million(O) Dollar(O) Baby(O) ((O) 2004(O) )(O) ,(O) Eastwood(B-person) won(O) Academy(B-award) Awards(I-award) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) ,(O) as(O) well(O) as(O) receiving(O) nominations(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, musical instrument, event, person, organization, song, location, country, musical artist, music genre, band, award and O.\nSentence: For his work in the Western film Unforgiven ( 1992 ) and the sports drama Million Dollar Baby ( 2004 ) , Eastwood won Academy Awards for Academy Award for Best Director and Academy Award for Best Picture , as well as receiving nominations for Academy Award for Best Actor .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","his","work","in","the","Western","film","Unforgiven","(","1992",")","and","the","sports","drama","Million","Dollar","Baby","(","2004",")",",","Eastwood","won","Academy","Awards","for","Academy","Award","for","Best","Director","and","Academy","Award","for","Best","Picture",",","as","well","as","receiving","nominations","for","Academy","Award","for","Best","Actor","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","O","B-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["album","musical_instrument","event","person","organization","song","location","country","musical_artist","music_genre","band","award"]}
{"id":"428","dataset":"crossner_music","split":"test","instance":{"id":"428","prompt_labels":"Others(O) ,(O) including(O) San(B-location) Francisco(I-location) Bay(I-location) Area(I-location) 's(O) Testament(B-band) and(O) Exodus(B-band) ,(O) New(B-location) Jersey(I-location) 's(O) Overkill(B-band) ,(O) and(O) Brazil(B-location) 's(O) Sepultura(B-band) and(O) Sarcfago(B-band) ,(O) also(O) had(O) a(O) significant(O) impact(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, location, person, organization, song, music genre, musical artist, band, musical instrument, award, event, country and O.\nSentence: Others , including San Francisco Bay Area 's Testament and Exodus , New Jersey 's Overkill , and Brazil 's Sepultura and Sarcfago , also had a significant impact .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Others",",","including","San","Francisco","Bay","Area","'s","Testament","and","Exodus",",","New","Jersey","'s","Overkill",",","and","Brazil","'s","Sepultura","and","Sarcfago",",","also","had","a","significant","impact","."],"labels":["O","O","O","B-location","I-location","I-location","I-location","O","B-band","O","B-band","O","B-location","I-location","O","B-band","O","O","B-location","O","B-band","O","B-band","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["album","location","person","organization","song","music_genre","musical_artist","band","musical_instrument","award","event","country"]}
{"id":"429","dataset":"crossner_music","split":"test","instance":{"id":"429","prompt_labels":"The(O) top(O) 20(O) also(O) included(O) LPs(O) by(O) The(B-band) Velvet(I-band) Underground(I-band) ,(O) The(B-band) Ramones(I-band) ,(O) Pulp(B-band) ,(O) Misty(B-band) in(I-band) Roots(I-band) ,(O) Nirvana(B-band) ,(O) Neil(B-musical artist) Young(I-musical artist) ,(O) Pink(B-band) Floyd(I-band) ,(O) The(B-band) Four(I-band) Brothers(I-band) ,(O) Dave(B-musical artist) Clarke(I-musical artist) ,(O) Richard(B-band) and(I-band) Linda(I-band) Thompson(I-band) and(O) The(B-band) Rolling(I-band) Stones(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, band, person, event, album, musical artist, country, musical instrument, song, organization, music genre, location and O.\nSentence: The top 20 also included LPs by The Velvet Underground , The Ramones , Pulp , Misty in Roots , Nirvana , Neil Young , Pink Floyd , The Four Brothers , Dave Clarke , Richard and Linda Thompson and The Rolling Stones .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","top","20","also","included","LPs","by","The","Velvet","Underground",",","The","Ramones",",","Pulp",",","Misty","in","Roots",",","Nirvana",",","Neil","Young",",","Pink","Floyd",",","The","Four","Brothers",",","Dave","Clarke",",","Richard","and","Linda","Thompson","and","The","Rolling","Stones","."],"labels":["O","O","O","O","O","O","O","B-band","I-band","I-band","O","B-band","I-band","O","B-band","O","B-band","I-band","I-band","O","B-band","O","B-musical artist","I-musical artist","O","B-band","I-band","O","B-band","I-band","I-band","O","B-musical artist","I-musical artist","O","B-band","I-band","I-band","I-band","O","B-band","I-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["award","band","person","event","album","musical_artist","country","musical_instrument","song","organization","music_genre","location"]}
{"id":"430","dataset":"crossner_music","split":"test","instance":{"id":"430","prompt_labels":"Nitin(B-musical artist) Mishra(I-musical artist) came(O) up(O) with(O) contemporary(O) storylines(O) like(O) '(O) Hum(B-song) Honge(I-song) Kamyaab(I-song) '(O) in(O) 2010(O) ,(O) which(O) was(O) his(O) take(O) on(O) the(O) controversies(O) attached(O) with(O) the(O) 2010(B-event) Commonwealth(I-event) Games(I-event) conducted(O) in(O) India(B-country) in(O) 2010(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, musical artist, album, event, band, musical instrument, award, person, country, music genre, song and O.\nSentence: Nitin Mishra came up with contemporary storylines like ' Hum Honge Kamyaab ' in 2010 , which was his take on the controversies attached with the 2010 Commonwealth Games conducted in India in 2010 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Nitin","Mishra","came","up","with","contemporary","storylines","like","'","Hum","Honge","Kamyaab","'","in","2010",",","which","was","his","take","on","the","controversies","attached","with","the","2010","Commonwealth","Games","conducted","in","India","in","2010","."],"labels":["B-musical artist","I-musical artist","O","O","O","O","O","O","O","B-song","I-song","I-song","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","B-country","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","organization","musical_artist","album","event","band","musical_instrument","award","person","country","music_genre","song"]}
{"id":"432","dataset":"crossner_music","split":"test","instance":{"id":"432","prompt_labels":"The(O) current(O) general(O) music(O) director(O) of(O) the(O) Berlin(B-location) State(I-location) Opera(I-location) and(O) the(O) Staatskapelle(B-band) Berlin(I-band) ,(O) Barenboim(B-musical artist) previously(O) served(O) as(O) Music(O) Director(O) of(O) the(O) Chicago(B-band) Symphony(I-band) Orchestra(I-band) ,(O) the(O) Orchestre(B-band) de(I-band) Paris(I-band) and(O) La(B-location) Scala(I-location) in(O) Milan(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, musical artist, album, band, person, musical instrument, location, country, award, organization, event, song and O.\nSentence: The current general music director of the Berlin State Opera and the Staatskapelle Berlin , Barenboim previously served as Music Director of the Chicago Symphony Orchestra , the Orchestre de Paris and La Scala in Milan .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","current","general","music","director","of","the","Berlin","State","Opera","and","the","Staatskapelle","Berlin",",","Barenboim","previously","served","as","Music","Director","of","the","Chicago","Symphony","Orchestra",",","the","Orchestre","de","Paris","and","La","Scala","in","Milan","."],"labels":["O","O","O","O","O","O","O","B-location","I-location","I-location","O","O","B-band","I-band","O","B-musical artist","O","O","O","O","O","O","O","B-band","I-band","I-band","O","O","B-band","I-band","I-band","O","B-location","I-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["music_genre","musical_artist","album","band","person","musical_instrument","location","country","award","organization","event","song"]}
{"id":"433","dataset":"crossner_music","split":"test","instance":{"id":"433","prompt_labels":"Building(O) on(O) the(O) distorted(O) electric(B-musical instrument) guitar(I-musical instrument) sound(O) of(O) early(O) records(O) ,(O) his(O) 1958(O) instrumental(O) hit(O) Rumble(B-album) by(O) Link(B-band) Wray(I-band) &(I-band) His(I-band) Ray(I-band) Men(I-band) popularized(O) the(O) power(O) chord(O) ,(O) the(O) major(O) modus(O) operandi(O) of(O) modern(O) Rock(B-music genre) music(I-music genre) guitarists(O) ,(O) facilitating(O) the(O) emergence(O) of(O) Punk(B-music genre) rock(I-music genre) and(O) Hard(B-music genre) rock(I-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, album, award, song, band, musical artist, organization, person, location, event, music genre, country and O.\nSentence: Building on the distorted electric guitar sound of early records , his 1958 instrumental hit Rumble by Link Wray & His Ray Men popularized the power chord , the major modus operandi of modern Rock music guitarists , facilitating the emergence of Punk rock and Hard rock .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Building","on","the","distorted","electric","guitar","sound","of","early","records",",","his","1958","instrumental","hit","Rumble","by","Link","Wray","&","His","Ray","Men","popularized","the","power","chord",",","the","major","modus","operandi","of","modern","Rock","music","guitarists",",","facilitating","the","emergence","of","Punk","rock","and","Hard","rock","."],"labels":["O","O","O","O","B-musical instrument","I-musical instrument","O","O","O","O","O","O","O","O","O","B-album","O","B-band","I-band","I-band","I-band","I-band","I-band","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","album","award","song","band","musical_artist","organization","person","location","event","music_genre","country"]}
{"id":"434","dataset":"crossner_music","split":"test","instance":{"id":"434","prompt_labels":"Furthermore(O) ,(O) he(O) wrote(O) the(O) music(O) for(O) the(O) Sestriere(B-location) FIS(B-event) Alpine(I-event) World(I-event) Ski(I-event) Championships(I-event) 1997(I-event) opening(O) ceremony(O) of(O) 1997(O) including(O) the(O) official(O) anthem(O) ,(O) incidental(O) music(O) for(O) stage(O) productions(O) ,(O) and(O) a(O) film(O) score(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, band, album, song, music genre, award, person, location, country, organization, musical artist, musical instrument and O.\nSentence: Furthermore , he wrote the music for the Sestriere FIS Alpine World Ski Championships 1997 opening ceremony of 1997 including the official anthem , incidental music for stage productions , and a film score .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Furthermore",",","he","wrote","the","music","for","the","Sestriere","FIS","Alpine","World","Ski","Championships","1997","opening","ceremony","of","1997","including","the","official","anthem",",","incidental","music","for","stage","productions",",","and","a","film","score","."],"labels":["O","O","O","O","O","O","O","O","B-location","B-event","I-event","I-event","I-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","band","album","song","music_genre","award","person","location","country","organization","musical_artist","musical_instrument"]}
{"id":"435","dataset":"crossner_music","split":"test","instance":{"id":"435","prompt_labels":"format(O) =(O) Country(B-music genre) music(I-music genre) ,(O) Bluegrass(B-music genre) music(I-music genre) ,(O) Gospel(B-music genre) music(I-music genre) |(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, song, event, music genre, organization, band, musical artist, award, country, album, musical instrument and O.\nSentence: format = Country music , Bluegrass music , Gospel music |","prediction_output":null,"prediction_outputs":null,"group":null,"words":["format","=","Country","music",",","Bluegrass","music",",","Gospel","music","|"],"labels":["O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["person","location","song","event","music_genre","organization","band","musical_artist","award","country","album","musical_instrument"]}
{"id":"436","dataset":"crossner_music","split":"test","instance":{"id":"436","prompt_labels":"Foreman(O) wrote(O) Melbourne(B-song) Girl(I-song) for(O) the(O) Closing(O) Ceremony(O) of(O) the(O) 2002(B-event) Commonwealth(I-event) Games(I-event) in(O) Manchester(B-location) ,(O) UK(B-country) which(O) was(O) performed(O) by(O) Vanessa(B-musical artist) Amorosi(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, band, organization, location, musical artist, musical instrument, music genre, event, person, award, album, song and O.\nSentence: Foreman wrote Melbourne Girl for the Closing Ceremony of the 2002 Commonwealth Games in Manchester , UK which was performed by Vanessa Amorosi .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Foreman","wrote","Melbourne","Girl","for","the","Closing","Ceremony","of","the","2002","Commonwealth","Games","in","Manchester",",","UK","which","was","performed","by","Vanessa","Amorosi","."],"labels":["O","O","B-song","I-song","O","O","O","O","O","O","B-event","I-event","I-event","O","B-location","O","B-country","O","O","O","O","B-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["country","band","organization","location","musical_artist","musical_instrument","music_genre","event","person","award","album","song"]}
{"id":"437","dataset":"crossner_music","split":"test","instance":{"id":"437","prompt_labels":"In(O) addition(O) ,(O) McBride(B-musical artist) has(O) the(O) Country(B-organization) Music(I-organization) Association(I-organization) '(O) s(O) Female(B-award) Vocalist(I-award) of(I-award) the(I-award) Year(I-award) award(I-award) four(O) times(O) ((O) tied(O) with(O) Reba(B-musical artist) McEntire(I-musical artist) for(O) the(O) second-most(O) wins(O) )(O) and(O) the(O) Academy(B-organization) of(I-organization) Country(I-organization) Music(I-organization) '(O) s(O) Top(B-award) Female(I-award) Vocalist(I-award) award(I-award) three(O) times(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, person, location, award, musical instrument, musical artist, album, music genre, organization, country, event, song and O.\nSentence: In addition , McBride has the Country Music Association ' s Female Vocalist of the Year award four times ( tied with Reba McEntire for the second-most wins ) and the Academy of Country Music ' s Top Female Vocalist award three times .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","addition",",","McBride","has","the","Country","Music","Association","'","s","Female","Vocalist","of","the","Year","award","four","times","(","tied","with","Reba","McEntire","for","the","second-most","wins",")","and","the","Academy","of","Country","Music","'","s","Top","Female","Vocalist","award","three","times","."],"labels":["O","O","O","B-musical artist","O","O","B-organization","I-organization","I-organization","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-award","I-award","I-award","I-award","O","O","O"],"target_index":null,"target_label":null},"label_list":["band","person","location","award","musical_instrument","musical_artist","album","music_genre","organization","country","event","song"]}
{"id":"438","dataset":"crossner_music","split":"test","instance":{"id":"438","prompt_labels":"Boogie(B-band) Down(I-band) Productions(I-band) was(O) a(O) Hip(B-music genre) hop(I-music genre) music(I-music genre) group(O) ,(O) originally(O) composed(O) of(O) KRS-One(B-musical artist) ,(O) D-Nice(B-musical artist) ,(O) and(O) DJ(O) Scott(B-musical artist) La(I-musical artist) Rock(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, event, person, album, location, organization, band, music genre, musical instrument, award, country, musical artist and O.\nSentence: Boogie Down Productions was a Hip hop music group , originally composed of KRS-One , D-Nice , and DJ Scott La Rock .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Boogie","Down","Productions","was","a","Hip","hop","music","group",",","originally","composed","of","KRS-One",",","D-Nice",",","and","DJ","Scott","La","Rock","."],"labels":["B-band","I-band","I-band","O","O","B-music genre","I-music genre","I-music genre","O","O","O","O","O","B-musical artist","O","B-musical artist","O","O","O","B-musical artist","I-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["song","event","person","album","location","organization","band","music_genre","musical_instrument","award","country","musical_artist"]}
{"id":"439","dataset":"crossner_music","split":"test","instance":{"id":"439","prompt_labels":"The(O) track(O) was(O) used(O) for(O) the(O) entrance(O) of(O) Team(B-organization) Scotland(I-organization) during(O) the(O) opening(O) ceremony(O) of(O) the(O) 2014(B-event) Commonwealth(I-event) Games(I-event) at(O) Celtic(B-location) Park(I-location) in(O) Glasgow(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, award, person, music genre, musical instrument, song, location, country, organization, event, musical artist, band and O.\nSentence: The track was used for the entrance of Team Scotland during the opening ceremony of the 2014 Commonwealth Games at Celtic Park in Glasgow .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","track","was","used","for","the","entrance","of","Team","Scotland","during","the","opening","ceremony","of","the","2014","Commonwealth","Games","at","Celtic","Park","in","Glasgow","."],"labels":["O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","B-event","I-event","I-event","O","B-location","I-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["album","award","person","music_genre","musical_instrument","song","location","country","organization","event","musical_artist","band"]}
{"id":"440","dataset":"crossner_music","split":"test","instance":{"id":"440","prompt_labels":"Parton(B-musical artist) recorded(O) a(O) series(O) of(O) bluegrass(B-music genre) -inspired(O) albums(O) ,(O) beginning(O) with(O) The(B-album) Grass(I-album) Is(I-album) Blue(I-album) ((O) 1999(O) )(O) ,(O) winning(O) a(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Bluegrass(I-award) Album(I-award) ;(O) and(O) Little(B-album) Sparrow(I-album) ((O) 2001(O) )(O) ,(O) with(O) its(O) cover(O) of(O) Collective(B-band) Soul(I-band) '(O) s(O) Shine(B-song) winning(O) a(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Female(I-award) Country(I-award) Vocal(I-award) Performance(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, country, song, band, organization, event, album, music genre, location, award, person, musical artist and O.\nSentence: Parton recorded a series of bluegrass -inspired albums , beginning with The Grass Is Blue ( 1999 ) , winning a Grammy Award for Best Bluegrass Album ; and Little Sparrow ( 2001 ) , with its cover of Collective Soul ' s Shine winning a Grammy Award for Best Female Country Vocal Performance .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Parton","recorded","a","series","of","bluegrass","-inspired","albums",",","beginning","with","The","Grass","Is","Blue","(","1999",")",",","winning","a","Grammy","Award","for","Best","Bluegrass","Album",";","and","Little","Sparrow","(","2001",")",",","with","its","cover","of","Collective","Soul","'","s","Shine","winning","a","Grammy","Award","for","Best","Female","Country","Vocal","Performance","."],"labels":["B-musical artist","O","O","O","O","B-music genre","O","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","B-album","I-album","O","O","O","O","O","O","O","O","B-band","I-band","O","O","B-song","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","country","song","band","organization","event","album","music_genre","location","award","person","musical_artist"]}
{"id":"444","dataset":"crossner_music","split":"test","instance":{"id":"444","prompt_labels":"Hathaway(B-person) currently(O) serves(O) on(O) the(O) board(O) of(O) the(O) Lollipop(B-organization) Theatre(I-organization) Network(I-organization) and(O) is(O) involved(O) with(O) charities(O) Creative(B-organization) Coalition(I-organization) ,(O) St.(B-organization) Jude(I-organization) Children(I-organization) 's(I-organization) Research(I-organization) Hospital(I-organization) and(O) the(O) Human(B-organization) Rights(I-organization) Campaign(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, musical instrument, music genre, event, location, band, organization, musical artist, album, song, person, country and O.\nSentence: Hathaway currently serves on the board of the Lollipop Theatre Network and is involved with charities Creative Coalition , St. Jude Children 's Research Hospital and the Human Rights Campaign .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Hathaway","currently","serves","on","the","board","of","the","Lollipop","Theatre","Network","and","is","involved","with","charities","Creative","Coalition",",","St.","Jude","Children","'s","Research","Hospital","and","the","Human","Rights","Campaign","."],"labels":["B-person","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["award","musical_instrument","music_genre","event","location","band","organization","musical_artist","album","song","person","country"]}
{"id":"445","dataset":"crossner_music","split":"test","instance":{"id":"445","prompt_labels":"O(B-musical artist) 'Connell(I-musical artist) co-hosted(O) the(O) Miss(B-organization) USA(I-organization) and(O) Miss(B-organization) Universe(I-organization) pageants(O) with(O) Bob(B-person) Barker(I-person) from(O) 1972(O) to(O) 1980(O) and(O) was(O) nominated(O) for(O) an(O) Emmy(B-award) award(I-award) in(O) 1976(O) for(O) her(O) coverage(O) of(O) the(O) Miss(B-organization) Universe(I-organization) pageant(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, location, musical artist, song, event, country, music genre, person, band, award, album, organization and O.\nSentence: O 'Connell co-hosted the Miss USA and Miss Universe pageants with Bob Barker from 1972 to 1980 and was nominated for an Emmy award in 1976 for her coverage of the Miss Universe pageant .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["O","'Connell","co-hosted","the","Miss","USA","and","Miss","Universe","pageants","with","Bob","Barker","from","1972","to","1980","and","was","nominated","for","an","Emmy","award","in","1976","for","her","coverage","of","the","Miss","Universe","pageant","."],"labels":["B-musical artist","I-musical artist","O","O","B-organization","I-organization","O","B-organization","I-organization","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","O","O","O","O","O","B-organization","I-organization","O","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","location","musical_artist","song","event","country","music_genre","person","band","award","album","organization"]}
{"id":"446","dataset":"crossner_music","split":"test","instance":{"id":"446","prompt_labels":"Sosa(B-musical artist) performed(O) in(O) venues(O) such(O) as(O) the(O) Lincoln(B-location) Center(I-location) in(O) New(B-location) York(I-location) City(I-location) ,(O) the(O) Thtre(B-location) Mogador(I-location) in(O) Paris(B-location) and(O) the(O) Sistine(B-location) Chapel(I-location) in(O) Vatican(B-location) City(I-location) ,(O) as(O) well(O) as(O) sell-out(O) shows(O) in(O) New(B-location) York(I-location) 's(O) Carnegie(B-location) Hall(I-location) and(O) the(O) Roman(B-location) Colosseum(I-location) during(O) her(O) final(O) decade(O) of(O) life(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, musical instrument, person, album, location, event, musical artist, band, organization, music genre, award, song and O.\nSentence: Sosa performed in venues such as the Lincoln Center in New York City , the Thtre Mogador in Paris and the Sistine Chapel in Vatican City , as well as sell-out shows in New York 's Carnegie Hall and the Roman Colosseum during her final decade of life .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Sosa","performed","in","venues","such","as","the","Lincoln","Center","in","New","York","City",",","the","Thtre","Mogador","in","Paris","and","the","Sistine","Chapel","in","Vatican","City",",","as","well","as","sell-out","shows","in","New","York","'s","Carnegie","Hall","and","the","Roman","Colosseum","during","her","final","decade","of","life","."],"labels":["B-musical artist","O","O","O","O","O","O","B-location","I-location","O","B-location","I-location","I-location","O","O","B-location","I-location","O","B-location","O","O","B-location","I-location","O","B-location","I-location","O","O","O","O","O","O","O","B-location","I-location","O","B-location","I-location","O","O","B-location","I-location","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","musical_instrument","person","album","location","event","musical_artist","band","organization","music_genre","award","song"]}
{"id":"447","dataset":"crossner_music","split":"test","instance":{"id":"447","prompt_labels":"From(O) the(O) 1970s(O) through(O) the(O) 1990s(O) ,(O) Terry(B-musical artist) performed(O) at(O) Carnegie(B-location) Hall(I-location) ,(O) Town(B-location) Hall(I-location) ,(O) and(O) Lincoln(B-location) Center(I-location) ,(O) toured(O) with(O) the(O) Newport(B-event) Jazz(I-event) All(I-event) Stars(I-event) and(O) Jazz(B-event) at(I-event) the(I-event) Philharmonic(I-event) ,(O) and(O) was(O) featured(O) with(O) Skitch(B-musical artist) Henderson(I-musical artist) '(O) s(O) New(B-location) York(I-location) Pops(I-location) Orchestra(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, music genre, band, album, musical instrument, event, award, song, organization, musical artist, location and O.\nSentence: From the 1970s through the 1990s , Terry performed at Carnegie Hall , Town Hall , and Lincoln Center , toured with the Newport Jazz All Stars and Jazz at the Philharmonic , and was featured with Skitch Henderson ' s New York Pops Orchestra .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["From","the","1970s","through","the","1990s",",","Terry","performed","at","Carnegie","Hall",",","Town","Hall",",","and","Lincoln","Center",",","toured","with","the","Newport","Jazz","All","Stars","and","Jazz","at","the","Philharmonic",",","and","was","featured","with","Skitch","Henderson","'","s","New","York","Pops","Orchestra","."],"labels":["O","O","O","O","O","O","O","B-musical artist","O","O","B-location","I-location","O","B-location","I-location","O","O","B-location","I-location","O","O","O","O","B-event","I-event","I-event","I-event","O","B-event","I-event","I-event","I-event","O","O","O","O","O","B-musical artist","I-musical artist","O","O","B-location","I-location","I-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["person","country","music_genre","band","album","musical_instrument","event","award","song","organization","musical_artist","location"]}
{"id":"448","dataset":"crossner_music","split":"test","instance":{"id":"448","prompt_labels":"By(O) the(O) early(O) 20th(O) century(O) ,(O) the(O) United(B-country) States(I-country) had(O) become(O) a(O) major(O) center(O) for(O) folk(B-music genre) music(I-music genre) from(O) around(O) the(O) world(O) ,(O) including(O) polka(B-music genre) ,(O) Ukrainian(B-music genre) and(O) Polish(B-music genre) fiddling(I-music genre) ,(O) Ashkenazi(B-music genre) ,(O) Klezmer(B-music genre) ,(O) and(O) several(O) kinds(O) of(O) Latin(B-music genre) music(I-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, album, location, song, country, event, award, band, music genre, musical artist, person, organization and O.\nSentence: By the early 20th century , the United States had become a major center for folk music from around the world , including polka , Ukrainian and Polish fiddling , Ashkenazi , Klezmer , and several kinds of Latin music .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["By","the","early","20th","century",",","the","United","States","had","become","a","major","center","for","folk","music","from","around","the","world",",","including","polka",",","Ukrainian","and","Polish","fiddling",",","Ashkenazi",",","Klezmer",",","and","several","kinds","of","Latin","music","."],"labels":["O","O","O","O","O","O","O","B-country","I-country","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","B-music genre","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","O","O","O","O","O","B-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","album","location","song","country","event","award","band","music_genre","musical_artist","person","organization"]}
{"id":"450","dataset":"crossner_music","split":"test","instance":{"id":"450","prompt_labels":"Since(O) 2000(O) ,(O) France(B-country) ,(O) Germany(B-country) ,(O) Spain(B-country) and(O) United(B-country) Kingdom(I-country) have(O) automatically(O) qualified(O) for(O) the(O) final(O) ,(O) regardless(O) of(O) their(O) positions(O) on(O) the(O) scoreboard(O) in(O) previous(O) contests(O) ,(O) as(O) they(O) are(O) the(O) four(O) biggest(O) financial(O) contributors(O) to(O) the(O) European(B-organization) Broadcasting(I-organization) Union(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, band, organization, musical instrument, country, song, music genre, person, musical artist, event, location, award and O.\nSentence: Since 2000 , France , Germany , Spain and United Kingdom have automatically qualified for the final , regardless of their positions on the scoreboard in previous contests , as they are the four biggest financial contributors to the European Broadcasting Union .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Since","2000",",","France",",","Germany",",","Spain","and","United","Kingdom","have","automatically","qualified","for","the","final",",","regardless","of","their","positions","on","the","scoreboard","in","previous","contests",",","as","they","are","the","four","biggest","financial","contributors","to","the","European","Broadcasting","Union","."],"labels":["O","O","O","B-country","O","B-country","O","B-country","O","B-country","I-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["album","band","organization","musical_instrument","country","song","music_genre","person","musical_artist","event","location","award"]}
{"id":"453","dataset":"crossner_music","split":"test","instance":{"id":"453","prompt_labels":"Independent(O) from(O) the(O) United(B-country) Kingdom(I-country) scene(O) ,(O) in(O) the(O) late(O) 1970s(O) and(O) early(O) 1980s(O) in(O) California(B-location) ,(O) deathrock(B-music genre) developed(O) as(O) a(O) distinct(O) branch(O) of(O) American(O) punk(B-music genre) rock(I-music genre) ,(O) with(O) acts(O) such(O) as(O) Christian(B-band) Death(I-band) and(O) 45(B-band) Grave(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, music genre, musical artist, song, award, person, organization, album, country, band, location, event and O.\nSentence: Independent from the United Kingdom scene , in the late 1970s and early 1980s in California , deathrock developed as a distinct branch of American punk rock , with acts such as Christian Death and 45 Grave .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Independent","from","the","United","Kingdom","scene",",","in","the","late","1970s","and","early","1980s","in","California",",","deathrock","developed","as","a","distinct","branch","of","American","punk","rock",",","with","acts","such","as","Christian","Death","and","45","Grave","."],"labels":["O","O","O","B-country","I-country","O","O","O","O","O","O","O","O","O","O","B-location","O","B-music genre","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","B-band","I-band","O","B-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","music_genre","musical_artist","song","award","person","organization","album","country","band","location","event"]}
{"id":"455","dataset":"crossner_music","split":"test","instance":{"id":"455","prompt_labels":"Autechre(B-musical artist) remain(O) going(O) strong(O) to(O) this(O) day(O) ,(O) where(O) they(O) continue(O) to(O) call(O) Warp(B-organization) Records(I-organization) their(O) home(O) ,(O) having(O) released(O) numerous(O) albums(O) to(O) critical(O) acclaim(O) in(O) the(O) years(O) to(O) follow(O) ,(O) such(O) as(O) Confield(B-album) ,(O) Draft(B-album) 7.30(I-album) ,(O) Untilted(B-album) ,(O) Quaristice(B-album) ,(O) Oversteps(B-album) ,(O) Exai(B-album) and(O) elseq(B-album) 1-5(I-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, band, country, album, musical artist, event, organization, award, location, musical instrument, song, music genre and O.\nSentence: Autechre remain going strong to this day , where they continue to call Warp Records their home , having released numerous albums to critical acclaim in the years to follow , such as Confield , Draft 7.30 , Untilted , Quaristice , Oversteps , Exai and elseq 1-5 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Autechre","remain","going","strong","to","this","day",",","where","they","continue","to","call","Warp","Records","their","home",",","having","released","numerous","albums","to","critical","acclaim","in","the","years","to","follow",",","such","as","Confield",",","Draft","7.30",",","Untilted",",","Quaristice",",","Oversteps",",","Exai","and","elseq","1-5","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-album","O","B-album","I-album","O","B-album","O","B-album","O","B-album","O","B-album","O","B-album","I-album","O"],"target_index":null,"target_label":null},"label_list":["person","band","country","album","musical_artist","event","organization","award","location","musical_instrument","song","music_genre"]}
{"id":"456","dataset":"crossner_music","split":"test","instance":{"id":"456","prompt_labels":"Crust(B-music genre) punk(I-music genre) groups(O) ,(O) such(O) as(O) Antisect(B-band) ,(O) Sacrilege(B-band) and(O) Anti(B-band) System(I-band) took(O) some(O) influence(O) from(O) early(O) black(B-music genre) metal(I-music genre) bands(O) like(O) Venom(B-band) ,(O) Hellhammer(B-band) ,(O) and(O) Celtic(B-band) Frost(I-band) ,(O) In(O) addition(O) ,(O) Norwegian(O) band(O) Darkthrone(B-band) have(O) incorporated(O) crust(B-music genre) punk(I-music genre) traits(O) in(O) their(O) more(O) recent(O) material(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, country, event, location, award, album, musical artist, band, person, music genre, song, organization and O.\nSentence: Crust punk groups , such as Antisect , Sacrilege and Anti System took some influence from early black metal bands like Venom , Hellhammer , and Celtic Frost , In addition , Norwegian band Darkthrone have incorporated crust punk traits in their more recent material .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Crust","punk","groups",",","such","as","Antisect",",","Sacrilege","and","Anti","System","took","some","influence","from","early","black","metal","bands","like","Venom",",","Hellhammer",",","and","Celtic","Frost",",","In","addition",",","Norwegian","band","Darkthrone","have","incorporated","crust","punk","traits","in","their","more","recent","material","."],"labels":["B-music genre","I-music genre","O","O","O","O","B-band","O","B-band","O","B-band","I-band","O","O","O","O","O","B-music genre","I-music genre","O","O","B-band","O","B-band","O","O","B-band","I-band","O","O","O","O","O","O","B-band","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","country","event","location","award","album","musical_artist","band","person","music_genre","song","organization"]}
{"id":"457","dataset":"crossner_music","split":"test","instance":{"id":"457","prompt_labels":"Madlib(B-musical artist) samples(O) Gong(B-band) 's(O) music(O) frequently(O) ,(O) including(O) Eat(B-song) That(I-song) Phone(I-song) Book(I-song) Coda(I-song) on(O) his(O) song(O) The(B-album) Further(I-album) Adventures(I-album) of(I-album) Lord(I-album) Quas(I-album) ,(O) You(B-song) Never(I-song) Blow(I-song) Yr(I-song) Trip(I-song) Forever(I-song) on(O) The(B-album) Further(I-album) Adventures(I-album) of(I-album) Lord(I-album) Quas(I-album) ,(O) and(O) Shamal(B-song) on(O) The(B-album) Further(I-album) Adventures(I-album) of(I-album) Lord(I-album) Quas(I-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, award, location, song, music genre, band, event, musical instrument, country, organization, album, person and O.\nSentence: Madlib samples Gong 's music frequently , including Eat That Phone Book Coda on his song The Further Adventures of Lord Quas , You Never Blow Yr Trip Forever on The Further Adventures of Lord Quas , and Shamal on The Further Adventures of Lord Quas .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Madlib","samples","Gong","'s","music","frequently",",","including","Eat","That","Phone","Book","Coda","on","his","song","The","Further","Adventures","of","Lord","Quas",",","You","Never","Blow","Yr","Trip","Forever","on","The","Further","Adventures","of","Lord","Quas",",","and","Shamal","on","The","Further","Adventures","of","Lord","Quas","."],"labels":["B-musical artist","O","B-band","O","O","O","O","O","B-song","I-song","I-song","I-song","I-song","O","O","O","B-album","I-album","I-album","I-album","I-album","I-album","O","B-song","I-song","I-song","I-song","I-song","I-song","O","B-album","I-album","I-album","I-album","I-album","I-album","O","O","B-song","O","B-album","I-album","I-album","I-album","I-album","I-album","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","award","location","song","music_genre","band","event","musical_instrument","country","organization","album","person"]}
{"id":"462","dataset":"crossner_music","split":"test","instance":{"id":"462","prompt_labels":"Crosby(B-musical artist) gained(O) valuable(O) experience(O) on(O) tour(O) for(O) a(O) year(O) with(O) Whiteman(B-musical artist) and(O) performing(O) and(O) recording(O) with(O) Bix(B-musical artist) Beiderbecke(I-musical artist) ,(O) Jack(B-musical artist) Teagarden(I-musical artist) ,(O) Tommy(B-musical artist) Dorsey(I-musical artist) ,(O) Jimmy(B-musical artist) Dorsey(I-musical artist) ,(O) Eddie(B-musical artist) Lang(I-musical artist) ,(O) and(O) Hoagy(B-musical artist) Carmichael(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, event, organization, music genre, song, person, musical instrument, musical artist, album, award, country, location and O.\nSentence: Crosby gained valuable experience on tour for a year with Whiteman and performing and recording with Bix Beiderbecke , Jack Teagarden , Tommy Dorsey , Jimmy Dorsey , Eddie Lang , and Hoagy Carmichael .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Crosby","gained","valuable","experience","on","tour","for","a","year","with","Whiteman","and","performing","and","recording","with","Bix","Beiderbecke",",","Jack","Teagarden",",","Tommy","Dorsey",",","Jimmy","Dorsey",",","Eddie","Lang",",","and","Hoagy","Carmichael","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","B-musical artist","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["band","event","organization","music_genre","song","person","musical_instrument","musical_artist","album","award","country","location"]}
{"id":"463","dataset":"crossner_music","split":"test","instance":{"id":"463","prompt_labels":"Today(O) there(O) are(O) Celtic-influenced(O) subgenres(O) of(O) virtually(O) every(O) type(O) of(O) popular(B-music genre) music(I-music genre) including(O) electronica(B-music genre) ,(O) Celtic(B-music genre) rock(I-music genre) ,(O) Celtic(B-music genre) metal(I-music genre) ,(O) Celtic(B-music genre) punk(I-music genre) ,(O) Hip(B-music genre) hop(I-music genre) music(I-music genre) ,(O) reggae(B-music genre) ,(O) New-age(B-music genre) music(I-music genre) ,(O) Latin(B-music genre) ,(O) Andean(B-music genre) and(O) Pop(B-music genre) music(I-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, musical artist, person, band, musical instrument, song, album, event, organization, country, music genre and O.\nSentence: Today there are Celtic-influenced subgenres of virtually every type of popular music including electronica , Celtic rock , Celtic metal , Celtic punk , Hip hop music , reggae , New-age music , Latin , Andean and Pop music .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Today","there","are","Celtic-influenced","subgenres","of","virtually","every","type","of","popular","music","including","electronica",",","Celtic","rock",",","Celtic","metal",",","Celtic","punk",",","Hip","hop","music",",","reggae",",","New-age","music",",","Latin",",","Andean","and","Pop","music","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","O","B-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["location","award","musical_artist","person","band","musical_instrument","song","album","event","organization","country","music_genre"]}
{"id":"1","dataset":"crossner_politics","split":"test","instance":{"id":"1","prompt_labels":"Lincoln(B-politician) replaced(O) Buell(B-politician) with(O) William(B-politician) Rosecrans(I-politician) ;(O) and(O) after(O) the(O) 1862(B-election) and(I-election) 1863(I-election) United(I-election) States(I-election) House(I-election) of(I-election) Representatives(I-election) elections(I-election) he(O) replaced(O) McClellan(B-politician) with(O) Ambrose(B-politician) Burnside(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, politician, organization, event, location, political party, election and O.\nSentence: Lincoln replaced Buell with William Rosecrans ; and after the 1862 and 1863 United States House of Representatives elections he replaced McClellan with Ambrose Burnside .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Lincoln","replaced","Buell","with","William","Rosecrans",";","and","after","the","1862","and","1863","United","States","House","of","Representatives","elections","he","replaced","McClellan","with","Ambrose","Burnside","."],"labels":["B-politician","O","B-politician","O","B-politician","I-politician","O","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","B-politician","O","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["country","person","politician","organization","event","location","political_party","election"]}
{"id":"2","dataset":"crossner_politics","split":"test","instance":{"id":"2","prompt_labels":"Tamsin(B-person) Greig(I-person) narrated(O) ,(O) and(O) the(O) cast(O) included(O) Nicky(B-person) Henson(I-person) as(O) Napoleon(O) ,(O) Toby(B-person) Jones(I-person) as(O) the(O) propagandist(O) Squealer(O) ,(O) and(O) Ralph(B-person) Ineson(I-person) as(O) Boxer(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, person, event, location, country, politician, political party, organization and O.\nSentence: Tamsin Greig narrated , and the cast included Nicky Henson as Napoleon , Toby Jones as the propagandist Squealer , and Ralph Ineson as Boxer .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Tamsin","Greig","narrated",",","and","the","cast","included","Nicky","Henson","as","Napoleon",",","Toby","Jones","as","the","propagandist","Squealer",",","and","Ralph","Ineson","as","Boxer","."],"labels":["B-person","I-person","O","O","O","O","O","O","B-person","I-person","O","O","O","B-person","I-person","O","O","O","O","O","O","B-person","I-person","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","person","event","location","country","politician","political_party","organization"]}
{"id":"3","dataset":"crossner_politics","split":"test","instance":{"id":"3","prompt_labels":"Many(O) of(O) the(O) most(O) prominent(O) national(O) leaders(O) ,(O) such(O) as(O) Washington(B-politician) ,(O) John(B-politician) Adams(I-politician) ,(O) John(B-politician) Hancock(I-politician) ,(O) and(O) Benjamin(B-politician) Franklin(I-politician) ,(O) retired(O) from(O) public(O) life(O) ,(O) served(O) as(O) foreign(O) delegates(O) ,(O) or(O) held(O) office(O) in(O) state(O) governments(O) ;(O) and(O) for(O) the(O) general(O) public(O) ,(O) local(O) government(O) and(O) self-rule(O) seemed(O) quite(O) satisfactory(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, organization, political party, event, country, location, person and O.\nSentence: Many of the most prominent national leaders , such as Washington , John Adams , John Hancock , and Benjamin Franklin , retired from public life , served as foreign delegates , or held office in state governments ; and for the general public , local government and self-rule seemed quite satisfactory .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Many","of","the","most","prominent","national","leaders",",","such","as","Washington",",","John","Adams",",","John","Hancock",",","and","Benjamin","Franklin",",","retired","from","public","life",",","served","as","foreign","delegates",",","or","held","office","in","state","governments",";","and","for","the","general","public",",","local","government","and","self-rule","seemed","quite","satisfactory","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["politician","election","organization","political_party","event","country","location","person"]}
{"id":"5","dataset":"crossner_politics","split":"test","instance":{"id":"5","prompt_labels":"Locker-Lampson(B-politician) took(O) Einstein(B-person) to(O) meet(O) Winston(B-politician) Churchill(I-politician) at(O) his(O) home(O) ,(O) and(O) later(O) ,(O) Austen(B-politician) Chamberlain(I-politician) and(O) former(O) Prime(O) Minister(O) David(B-politician) Lloyd(I-politician) George(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, location, organization, election, political party, event, politician and O.\nSentence: Locker-Lampson took Einstein to meet Winston Churchill at his home , and later , Austen Chamberlain and former Prime Minister David Lloyd George .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Locker-Lampson","took","Einstein","to","meet","Winston","Churchill","at","his","home",",","and","later",",","Austen","Chamberlain","and","former","Prime","Minister","David","Lloyd","George","."],"labels":["B-politician","O","B-person","O","O","B-politician","I-politician","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","B-politician","I-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["country","person","location","organization","election","political_party","event","politician"]}
{"id":"8","dataset":"crossner_politics","split":"test","instance":{"id":"8","prompt_labels":"After(O) the(O) assassination(O) of(O) Perdiccas(B-person) in(O) 321(O) BC(O) ,(O) Macedonian(B-country) unity(I-country) collapsed(O) ,(O) and(O) 40(O) years(O) of(O) war(O) between(O) The(O) Successors(O) ((O) Diadochi(O) )(O) ensued(O) before(O) the(O) Hellenistic(O) world(O) settled(O) into(O) four(O) stable(O) power(O) blocs(O) :(O) Ptolemaic(B-country) Kingdom(I-country) Egypt(B-country) ,(O) Seleucid(B-country) Empire(I-country) Mesopotamia(B-location) and(O) Central(B-location) Asia(I-location) ,(O) Attalid(O) dynasty(O) Anatolia(B-location) ,(O) and(O) Antigonid(O) dynasty(O) Macedon(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, political party, election, event, country, politician, location and O.\nSentence: After the assassination of Perdiccas in 321 BC , Macedonian unity collapsed , and 40 years of war between The Successors ( Diadochi ) ensued before the Hellenistic world settled into four stable power blocs : Ptolemaic Kingdom Egypt , Seleucid Empire Mesopotamia and Central Asia , Attalid dynasty Anatolia , and Antigonid dynasty Macedon .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","the","assassination","of","Perdiccas","in","321","BC",",","Macedonian","unity","collapsed",",","and","40","years","of","war","between","The","Successors","(","Diadochi",")","ensued","before","the","Hellenistic","world","settled","into","four","stable","power","blocs",":","Ptolemaic","Kingdom","Egypt",",","Seleucid","Empire","Mesopotamia","and","Central","Asia",",","Attalid","dynasty","Anatolia",",","and","Antigonid","dynasty","Macedon","."],"labels":["O","O","O","O","B-person","O","O","O","O","B-country","I-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","B-country","O","B-country","I-country","B-location","O","B-location","I-location","O","O","O","B-location","O","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["person","organization","political_party","election","event","country","politician","location"]}
{"id":"13","dataset":"crossner_politics","split":"test","instance":{"id":"13","prompt_labels":"The(O) empire(O) 's(O) capital(O) was(O) Pataliputra(B-location) ((O) in(O) Magadha(B-country) ,(O) present-day(O) Patna(B-location) )(O) ,(O) with(O) provincial(O) capitals(O) at(O) Taxila(B-location) and(O) Ujjain(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, organization, politician, person, location, political party, election and O.\nSentence: The empire 's capital was Pataliputra ( in Magadha , present-day Patna ) , with provincial capitals at Taxila and Ujjain .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","empire","'s","capital","was","Pataliputra","(","in","Magadha",",","present-day","Patna",")",",","with","provincial","capitals","at","Taxila","and","Ujjain","."],"labels":["O","O","O","O","O","B-location","O","O","B-country","O","O","B-location","O","O","O","O","O","O","B-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["country","event","organization","politician","person","location","political_party","election"]}
{"id":"14","dataset":"crossner_politics","split":"test","instance":{"id":"14","prompt_labels":"The(O) provinces(O) ceded(O) to(O) Augustus(B-person) for(O) that(O) ten-year(O) period(O) comprised(O) much(O) of(O) the(O) conquered(O) Roman(O) world(O) ,(O) including(O) all(O) of(O) Hispania(B-location) and(O) Gaul(B-location) ,(O) Syria(B-country) ,(O) Cilicia(B-location) ,(O) Cyprus(B-country) ,(O) and(O) Egypt(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, election, event, location, person, country, politician, political party and O.\nSentence: The provinces ceded to Augustus for that ten-year period comprised much of the conquered Roman world , including all of Hispania and Gaul , Syria , Cilicia , Cyprus , and Egypt .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","provinces","ceded","to","Augustus","for","that","ten-year","period","comprised","much","of","the","conquered","Roman","world",",","including","all","of","Hispania","and","Gaul",",","Syria",",","Cilicia",",","Cyprus",",","and","Egypt","."],"labels":["O","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-country","O","B-location","O","B-country","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["organization","election","event","location","person","country","politician","political_party"]}
{"id":"15","dataset":"crossner_politics","split":"test","instance":{"id":"15","prompt_labels":"Four(O) governors(O) have(O) served(O) multiple(O) non-consecutive(O) terms(O) :(O) Bibb(B-politician) Graves(I-politician) ,(O) Jim(B-politician) Folsom(I-politician) ,(O) and(O) Fob(B-politician) James(I-politician) each(O) served(O) two(O) ,(O) and(O) George(B-politician) Wallace(I-politician) served(O) three(O) non-consecutive(O) periods(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, political party, location, event, organization, country, person and O.\nSentence: Four governors have served multiple non-consecutive terms : Bibb Graves , Jim Folsom , and Fob James each served two , and George Wallace served three non-consecutive periods .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Four","governors","have","served","multiple","non-consecutive","terms",":","Bibb","Graves",",","Jim","Folsom",",","and","Fob","James","each","served","two",",","and","George","Wallace","served","three","non-consecutive","periods","."],"labels":["O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O","O","B-politician","I-politician","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["politician","election","political_party","location","event","organization","country","person"]}
{"id":"16","dataset":"crossner_politics","split":"test","instance":{"id":"16","prompt_labels":"The(O) twelve(O) countries(O) that(O) had(O) significant(O) interests(O) in(O) Antarctica(B-location) at(O) the(O) time(O) were(O) :(O) Argentina(B-country) ,(O) Australia(B-country) ,(O) Belgium(B-country) ,(O) Chile(B-country) ,(O) France(B-country) ,(O) Japan(B-country) ,(O) New(B-country) Zealand(I-country) ,(O) Norway(B-country) ,(O) South(B-country) Africa(I-country) ,(O) the(O) Soviet(B-country) Union(I-country) ,(O) the(O) United(B-country) Kingdom(I-country) ,(O) and(O) the(O) United(B-country) States(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, person, location, organization, country, politician, event, election and O.\nSentence: The twelve countries that had significant interests in Antarctica at the time were : Argentina , Australia , Belgium , Chile , France , Japan , New Zealand , Norway , South Africa , the Soviet Union , the United Kingdom , and the United States .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","twelve","countries","that","had","significant","interests","in","Antarctica","at","the","time","were",":","Argentina",",","Australia",",","Belgium",",","Chile",",","France",",","Japan",",","New","Zealand",",","Norway",",","South","Africa",",","the","Soviet","Union",",","the","United","Kingdom",",","and","the","United","States","."],"labels":["O","O","O","O","O","O","O","O","B-location","O","O","O","O","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","I-country","O","B-country","O","B-country","I-country","O","O","B-country","I-country","O","O","B-country","I-country","O","O","O","B-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["political_party","person","location","organization","country","politician","event","election"]}
{"id":"21","dataset":"crossner_politics","split":"test","instance":{"id":"21","prompt_labels":"Rallying(O) his(O) Afghan(O) tribes(O) and(O) allies(O) ,(O) he(O) pushed(O) east(O) towards(O) the(O) Mughal(B-country) Empire(I-country) and(O) the(O) Maratha(B-country) Empire(I-country) empires(O) of(O) India(B-country) ,(O) west(O) towards(O) the(O) disintegrating(O) Afsharid(B-country) Empire(I-country) of(I-country) Persia(I-country) ,(O) and(O) north(O) toward(O) the(O) Khanate(B-country) of(I-country) Bukhara(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, political party, event, organization, politician, election, location and O.\nSentence: Rallying his Afghan tribes and allies , he pushed east towards the Mughal Empire and the Maratha Empire empires of India , west towards the disintegrating Afsharid Empire of Persia , and north toward the Khanate of Bukhara .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Rallying","his","Afghan","tribes","and","allies",",","he","pushed","east","towards","the","Mughal","Empire","and","the","Maratha","Empire","empires","of","India",",","west","towards","the","disintegrating","Afsharid","Empire","of","Persia",",","and","north","toward","the","Khanate","of","Bukhara","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O","O","B-country","I-country","O","O","B-country","O","O","O","O","O","B-country","I-country","I-country","I-country","O","O","O","O","O","B-country","I-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["country","person","political_party","event","organization","politician","election","location"]}
{"id":"22","dataset":"crossner_politics","split":"test","instance":{"id":"22","prompt_labels":"He(O) then(O) moved(O) southward(O) into(O) Greece(B-country) ,(O) where(O) he(O) sacked(O) Piraeus(B-location) ((O) the(O) port(O) of(O) Athens(B-location) )(O) and(O) destroyed(O) Corinth(B-location) ,(O) Megara(B-location) ,(O) Argos(B-location) and(O) Sparta(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, organization, election, person, country, politician, political party and O.\nSentence: He then moved southward into Greece , where he sacked Piraeus ( the port of Athens ) and destroyed Corinth , Megara , Argos and Sparta .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","then","moved","southward","into","Greece",",","where","he","sacked","Piraeus","(","the","port","of","Athens",")","and","destroyed","Corinth",",","Megara",",","Argos","and","Sparta","."],"labels":["O","O","O","O","O","B-country","O","O","O","O","B-location","O","O","O","O","B-location","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["location","event","organization","election","person","country","politician","political_party"]}
{"id":"25","dataset":"crossner_politics","split":"test","instance":{"id":"25","prompt_labels":"Jackson(B-politician) 's(O) state(O) department(O) was(O) active(O) and(O) successful(O) at(O) making(O) trade(O) agreements(O) with(O) Russia(B-country) ,(O) Spain(B-country) ,(O) Turkey(B-country) ,(O) United(B-country) Kingdom(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, person, location, organization, political party, country, politician and O.\nSentence: Jackson 's state department was active and successful at making trade agreements with Russia , Spain , Turkey , United Kingdom .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Jackson","'s","state","department","was","active","and","successful","at","making","trade","agreements","with","Russia",",","Spain",",","Turkey",",","United","Kingdom","."],"labels":["B-politician","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","B-country","O","B-country","O","B-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["event","election","person","location","organization","political_party","country","politician"]}
{"id":"30","dataset":"crossner_politics","split":"test","instance":{"id":"30","prompt_labels":"His(O) reign(O) saw(O) the(O) incorporation(O) of(O) the(O) County(B-country) of(I-country) Urgell(I-country) ,(O) Duchy(B-country) of(I-country) Athens(I-country) ,(O) and(O) Duchy(B-country) of(I-country) Neopatria(I-country) into(O) the(O) Crown(B-country) of(I-country) Aragon(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, location, person, event, organization, country, political party and O.\nSentence: His reign saw the incorporation of the County of Urgell , Duchy of Athens , and Duchy of Neopatria into the Crown of Aragon .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","reign","saw","the","incorporation","of","the","County","of","Urgell",",","Duchy","of","Athens",",","and","Duchy","of","Neopatria","into","the","Crown","of","Aragon","."],"labels":["O","O","O","O","O","O","O","B-country","I-country","I-country","O","B-country","I-country","I-country","O","O","B-country","I-country","I-country","O","O","B-country","I-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["election","politician","location","person","event","organization","country","political_party"]}
{"id":"32","dataset":"crossner_politics","split":"test","instance":{"id":"32","prompt_labels":"In(O) 1996(O) ,(O) bin(B-politician) Laden(I-politician) personally(O) engineered(O) a(O) plot(O) to(O) assassinate(O) United(B-country) States(I-country) President(O) Bill(B-politician) Clinton(I-politician) while(O) the(O) president(O) was(O) in(O) Manila(B-location) for(O) the(O) Asia-Pacific(O) Economic(O) Cooperation(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, political party, election, politician, country, event, location and O.\nSentence: In 1996 , bin Laden personally engineered a plot to assassinate United States President Bill Clinton while the president was in Manila for the Asia-Pacific Economic Cooperation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1996",",","bin","Laden","personally","engineered","a","plot","to","assassinate","United","States","President","Bill","Clinton","while","the","president","was","in","Manila","for","the","Asia-Pacific","Economic","Cooperation","."],"labels":["O","O","O","B-politician","I-politician","O","O","O","O","O","O","B-country","I-country","O","B-politician","I-politician","O","O","O","O","O","B-location","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","organization","political_party","election","politician","country","event","location"]}
{"id":"34","dataset":"crossner_politics","split":"test","instance":{"id":"34","prompt_labels":"He(O) also(O) recalled(O) a(O) past(O) instance(O) in(O) which(O) Osama(B-politician) bin(I-politician) Laden(I-politician) called(O) on(O) al-Qaeda(B-organization) members(O) and(O) supporters(O) to(O) give(O) allegiance(O) to(O) Abu(B-politician) Omar(I-politician) al-Baghdadi(I-politician) when(O) the(O) group(O) was(O) still(O) solely(O) operating(O) in(O) Iraq(B-country) ,(O) as(O) the(O) Islamic(B-organization) State(I-organization) of(I-organization) Iraq(I-organization) ,(O) and(O) condemned(O) Ayman(B-politician) al-Zawahiri(I-politician) for(O) not(O) making(O) this(O) same(O) claim(O) for(O) Abu(B-politician) Bakr(I-politician) al-Baghdadi(I-politician) ,(O) and(O) that(O) Zawahiri(B-politician) was(O) encouraging(O) factionalism(O) and(O) division(O) between(O) former(O) allies(O) of(O) ISIL(B-organization) such(O) as(O) the(O) al-Nusra(B-organization) Front(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, politician, election, event, country, political party, person and O.\nSentence: He also recalled a past instance in which Osama bin Laden called on al-Qaeda members and supporters to give allegiance to Abu Omar al-Baghdadi when the group was still solely operating in Iraq , as the Islamic State of Iraq , and condemned Ayman al-Zawahiri for not making this same claim for Abu Bakr al-Baghdadi , and that Zawahiri was encouraging factionalism and division between former allies of ISIL such as the al-Nusra Front .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","also","recalled","a","past","instance","in","which","Osama","bin","Laden","called","on","al-Qaeda","members","and","supporters","to","give","allegiance","to","Abu","Omar","al-Baghdadi","when","the","group","was","still","solely","operating","in","Iraq",",","as","the","Islamic","State","of","Iraq",",","and","condemned","Ayman","al-Zawahiri","for","not","making","this","same","claim","for","Abu","Bakr","al-Baghdadi",",","and","that","Zawahiri","was","encouraging","factionalism","and","division","between","former","allies","of","ISIL","such","as","the","al-Nusra","Front","."],"labels":["O","O","O","O","O","O","O","O","B-politician","I-politician","I-politician","O","O","B-organization","O","O","O","O","O","O","O","B-politician","I-politician","I-politician","O","O","O","O","O","O","O","O","B-country","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","B-politician","I-politician","I-politician","O","O","O","B-politician","O","O","O","O","O","O","O","O","O","B-organization","O","O","O","B-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["organization","location","politician","election","event","country","political_party","person"]}
{"id":"38","dataset":"crossner_politics","split":"test","instance":{"id":"38","prompt_labels":"Norman(B-person) Thomas(I-person) ,(O) John(B-person) Haynes(I-person) Holmes(I-person) ,(O) and(O) Morris(B-person) Ernst(I-person) were(O) anti-Communists(O) who(O) wanted(O) to(O) distance(O) the(O) ACLU(B-organization) from(O) Communism(O) ;(O) opposing(O) them(O) were(O) Harry(B-person) F.(I-person) Ward(I-person) ,(O) Corliss(B-person) Lamont(I-person) ,(O) and(O) Elizabeth(B-person) Gurley(I-person) Flynn(I-person) ,(O) who(O) rejected(O) any(O) political(O) test(O) for(O) ACLU(B-organization) leadership(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, political party, person, organization, location, event, country and O.\nSentence: Norman Thomas , John Haynes Holmes , and Morris Ernst were anti-Communists who wanted to distance the ACLU from Communism ; opposing them were Harry F. Ward , Corliss Lamont , and Elizabeth Gurley Flynn , who rejected any political test for ACLU leadership .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Norman","Thomas",",","John","Haynes","Holmes",",","and","Morris","Ernst","were","anti-Communists","who","wanted","to","distance","the","ACLU","from","Communism",";","opposing","them","were","Harry","F.","Ward",",","Corliss","Lamont",",","and","Elizabeth","Gurley","Flynn",",","who","rejected","any","political","test","for","ACLU","leadership","."],"labels":["B-person","I-person","O","B-person","I-person","I-person","O","O","B-person","I-person","O","O","O","O","O","O","O","B-organization","O","O","O","O","O","O","B-person","I-person","I-person","O","B-person","I-person","O","O","B-person","I-person","I-person","O","O","O","O","O","O","O","B-organization","O","O"],"target_index":null,"target_label":null},"label_list":["election","politician","political_party","person","organization","location","event","country"]}
{"id":"41","dataset":"crossner_politics","split":"test","instance":{"id":"41","prompt_labels":"Assyria(B-country) ((O) This(O) vast(O) span(O) of(O) time(O) is(O) divided(O) in(O) Early(O) Period(O) ((O) 2500(O) BCE-2025(O) BCE(O) )(O) ,(O) Old(B-country) Assyrian(I-country) Empire(I-country) ((O) 2025(O) BCE(O) -(O) 1378(O) BCE(O) )(O) ,(O) Middle(B-country) Assyrian(I-country) Empire(I-country) ((O) 1392(O) BCE(O) -(O) 934(O) BCE(O) )(O) and(O) Neo-Assyrian(B-country) Empire(I-country) ((O) 911(O) BCE(O) -(O) 609(O) BCE(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, politician, election, event, location, political party, organization, person and O.\nSentence: Assyria ( This vast span of time is divided in Early Period ( 2500 BCE-2025 BCE ) , Old Assyrian Empire ( 2025 BCE - 1378 BCE ) , Middle Assyrian Empire ( 1392 BCE - 934 BCE ) and Neo-Assyrian Empire ( 911 BCE - 609 BCE ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Assyria","(","This","vast","span","of","time","is","divided","in","Early","Period","(","2500","BCE-2025","BCE",")",",","Old","Assyrian","Empire","(","2025","BCE","-","1378","BCE",")",",","Middle","Assyrian","Empire","(","1392","BCE","-","934","BCE",")","and","Neo-Assyrian","Empire","(","911","BCE","-","609","BCE",")","."],"labels":["B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","I-country","O","O","O","O","O","O","O","O","B-country","I-country","I-country","O","O","O","O","O","O","O","O","B-country","I-country","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","politician","election","event","location","political_party","organization","person"]}
{"id":"44","dataset":"crossner_politics","split":"test","instance":{"id":"44","prompt_labels":"The(O) Old(B-country) Assyrian(I-country) Empire(I-country) is(O) one(O) of(O) four(O) periods(O) into(O) which(O) the(O) history(O) of(O) Assyria(B-country) is(O) divided(O) ,(O) the(O) other(O) three(O) being(O) :(O) the(O) Early(O) Assyrian(O) Period(O) ,(O) the(O) Middle(B-country) Assyrian(I-country) Empire(I-country) and(O) the(O) Neo-Assyrian(B-country) Empire(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, organization, politician, country, person, location, political party and O.\nSentence: The Old Assyrian Empire is one of four periods into which the history of Assyria is divided , the other three being : the Early Assyrian Period , the Middle Assyrian Empire and the Neo-Assyrian Empire .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Old","Assyrian","Empire","is","one","of","four","periods","into","which","the","history","of","Assyria","is","divided",",","the","other","three","being",":","the","Early","Assyrian","Period",",","the","Middle","Assyrian","Empire","and","the","Neo-Assyrian","Empire","."],"labels":["O","B-country","I-country","I-country","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","I-country","O","O","B-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["event","election","organization","politician","country","person","location","political_party"]}
{"id":"45","dataset":"crossner_politics","split":"test","instance":{"id":"45","prompt_labels":"Tiglath-Pileser(B-politician) III(I-politician) conquered(O) as(O) far(O) as(O) the(O) East(B-location) Mediterranean(I-location) ,(O) bringing(O) the(O) Greeks(B-country) of(I-country) Cyprus(I-country) ,(O) Phoenicia(B-country) ,(O) Kingdom(B-country) of(I-country) Judah(I-country) ,(O) Philistia(B-country) ,(O) Samaria(B-location) and(O) the(O) whole(O) of(O) Aramea(B-country) under(O) Assyrian(B-country) control(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, country, organization, person, political party, location, event, politician and O.\nSentence: Tiglath-Pileser III conquered as far as the East Mediterranean , bringing the Greeks of Cyprus , Phoenicia , Kingdom of Judah , Philistia , Samaria and the whole of Aramea under Assyrian control .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Tiglath-Pileser","III","conquered","as","far","as","the","East","Mediterranean",",","bringing","the","Greeks","of","Cyprus",",","Phoenicia",",","Kingdom","of","Judah",",","Philistia",",","Samaria","and","the","whole","of","Aramea","under","Assyrian","control","."],"labels":["B-politician","I-politician","O","O","O","O","O","B-location","I-location","O","O","O","B-country","I-country","I-country","O","B-country","O","B-country","I-country","I-country","O","B-country","O","B-location","O","O","O","O","B-country","O","B-country","O","O"],"target_index":null,"target_label":null},"label_list":["election","country","organization","person","political_party","location","event","politician"]}
{"id":"46","dataset":"crossner_politics","split":"test","instance":{"id":"46","prompt_labels":"Ultimately(O) ,(O) Assyria(B-country) conquered(O) Babylonia(B-country) ,(O) Chaldea(B-country) ,(O) Elam(B-country) ,(O) Media(B-country) ,(O) Persia(B-country) ,(O) Urartu(B-country) ((O) Armenia(B-country) )(O) ,(O) Phoenicia(B-country) ,(O) Aramea(B-country) /(O) Syria(B-country) ,(O) Phrygia(B-country) ,(O) the(O) Neo-Hittite(B-country) States(I-country) ,(O) the(O) Hurrian(B-location) lands(I-location) ,(O) Arabia(B-country) ,(O) Gutium(B-country) ,(O) Israel(B-country) ,(O) Kingdom(B-country) of(I-country) Judah(I-country) ,(O) and(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, political party, location, country, politician, event, person and O.\nSentence: Ultimately , Assyria conquered Babylonia , Chaldea , Elam , Media , Persia , Urartu ( Armenia ) , Phoenicia , Aramea / Syria , Phrygia , the Neo-Hittite States , the Hurrian lands , Arabia , Gutium , Israel , Kingdom of Judah , and others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Ultimately",",","Assyria","conquered","Babylonia",",","Chaldea",",","Elam",",","Media",",","Persia",",","Urartu","(","Armenia",")",",","Phoenicia",",","Aramea","/","Syria",",","Phrygia",",","the","Neo-Hittite","States",",","the","Hurrian","lands",",","Arabia",",","Gutium",",","Israel",",","Kingdom","of","Judah",",","and","others","."],"labels":["O","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","O","O","B-country","O","B-country","O","B-country","O","B-country","O","O","B-country","I-country","O","O","B-location","I-location","O","B-country","O","B-country","O","B-country","O","B-country","I-country","I-country","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","organization","political_party","location","country","politician","event","person"]}
{"id":"48","dataset":"crossner_politics","split":"test","instance":{"id":"48","prompt_labels":"They(O) also(O) raided(O) the(O) Levant(B-location) ,(O) Israel(B-country) and(O) Kingdom(B-country) of(I-country) Judah(I-country) ((O) where(O) Ashkelon(B-location) was(O) sacked(O) by(O) the(O) Scythians(O) )(O) and(O) all(O) the(O) way(O) into(O) Egypt(B-country) whose(O) coasts(O) were(O) ravaged(O) and(O) looted(O) with(O) impunity(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, election, organization, political party, event, person, location and O.\nSentence: They also raided the Levant , Israel and Kingdom of Judah ( where Ashkelon was sacked by the Scythians ) and all the way into Egypt whose coasts were ravaged and looted with impunity .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["They","also","raided","the","Levant",",","Israel","and","Kingdom","of","Judah","(","where","Ashkelon","was","sacked","by","the","Scythians",")","and","all","the","way","into","Egypt","whose","coasts","were","ravaged","and","looted","with","impunity","."],"labels":["O","O","O","O","B-location","O","B-country","O","B-country","I-country","I-country","O","O","B-location","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["politician","country","election","organization","political_party","event","person","location"]}
{"id":"49","dataset":"crossner_politics","split":"test","instance":{"id":"49","prompt_labels":"The(O) Iranic(O) peoples(O) under(O) the(O) Medes(B-country) ,(O) aided(O) by(O) the(O) previous(O) Neo-Assyrian(B-country) Empire(I-country) destruction(O) of(O) the(O) hitherto(O) dominant(O) Elamites(O) of(O) Ancient(B-country) Iran(I-country) ,(O) also(O) took(O) advantage(O) of(O) the(O) upheavals(O) in(O) Neo-Assyrian(B-country) Empire(I-country) to(O) coalesce(O) into(O) a(O) powerful(O) Median-dominated(O) force(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, person, election, politician, country, event, location and O.\nSentence: The Iranic peoples under the Medes , aided by the previous Neo-Assyrian Empire destruction of the hitherto dominant Elamites of Ancient Iran , also took advantage of the upheavals in Neo-Assyrian Empire to coalesce into a powerful Median-dominated force .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Iranic","peoples","under","the","Medes",",","aided","by","the","previous","Neo-Assyrian","Empire","destruction","of","the","hitherto","dominant","Elamites","of","Ancient","Iran",",","also","took","advantage","of","the","upheavals","in","Neo-Assyrian","Empire","to","coalesce","into","a","powerful","Median-dominated","force","."],"labels":["O","O","O","O","O","B-country","O","O","O","O","O","B-country","I-country","O","O","O","O","O","O","O","B-country","I-country","O","O","O","O","O","O","O","O","B-country","I-country","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","political_party","person","election","politician","country","event","location"]}
{"id":"53","dataset":"crossner_politics","split":"test","instance":{"id":"53","prompt_labels":"He(O) subsequently(O) became(O) the(O) de(O) facto(O) leader(O) of(O) Afghanistan(B-country) 's(O) Uzbek(O) community(O) ,(O) controlling(O) the(O) country(O) 's(O) northern(O) provinces(O) and(O) Mazar-i-Sharif(B-location) ,(O) effectively(O) creating(O) his(O) own(O) proto-state(O) with(O) an(O) army(O) of(O) up(O) to(O) 40,000(O) men(O) with(O) tanks(O) supplied(O) by(O) Uzbekistan(B-country) and(O) Russia(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, person, location, election, country, organization, politician and O.\nSentence: He subsequently became the de facto leader of Afghanistan 's Uzbek community , controlling the country 's northern provinces and Mazar-i-Sharif , effectively creating his own proto-state with an army of up to 40,000 men with tanks supplied by Uzbekistan and Russia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","subsequently","became","the","de","facto","leader","of","Afghanistan","'s","Uzbek","community",",","controlling","the","country","'s","northern","provinces","and","Mazar-i-Sharif",",","effectively","creating","his","own","proto-state","with","an","army","of","up","to","40,000","men","with","tanks","supplied","by","Uzbekistan","and","Russia","."],"labels":["O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","B-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["event","political_party","person","location","election","country","organization","politician"]}
{"id":"54","dataset":"crossner_politics","split":"test","instance":{"id":"54","prompt_labels":"He(O) initially(O) supported(O) the(O) new(O) government(O) of(O) Burhanuddin(B-politician) Rabbani(I-politician) in(O) Kabul(B-location) but(O) in(O) 1994(O) switched(O) sides(O) and(O) allied(O) with(O) Gulbuddin(B-politician) Hekmatyar(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, event, location, organization, person, election, politician and O.\nSentence: He initially supported the new government of Burhanuddin Rabbani in Kabul but in 1994 switched sides and allied with Gulbuddin Hekmatyar .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","initially","supported","the","new","government","of","Burhanuddin","Rabbani","in","Kabul","but","in","1994","switched","sides","and","allied","with","Gulbuddin","Hekmatyar","."],"labels":["O","O","O","O","O","O","O","B-politician","I-politician","O","B-location","O","O","O","O","O","O","O","O","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["country","political_party","event","location","organization","person","election","politician"]}
{"id":"55","dataset":"crossner_politics","split":"test","instance":{"id":"55","prompt_labels":"Dostum(B-politician) was(O) directly(O) chosen(O) as(O) First(O) Vice(O) President(O) of(O) Afghanistan(B-country) in(O) the(O) 2014(B-election) Afghan(I-election) presidential(I-election) election(I-election) ,(O) next(O) to(O) Ashraf(B-politician) Ghani(I-politician) as(O) President(O) and(O) Sarwar(B-politician) Danish(I-politician) as(O) second(O) Vice(O) President(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, election, location, politician, country, political party, organization and O.\nSentence: Dostum was directly chosen as First Vice President of Afghanistan in the 2014 Afghan presidential election , next to Ashraf Ghani as President and Sarwar Danish as second Vice President .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Dostum","was","directly","chosen","as","First","Vice","President","of","Afghanistan","in","the","2014","Afghan","presidential","election",",","next","to","Ashraf","Ghani","as","President","and","Sarwar","Danish","as","second","Vice","President","."],"labels":["B-politician","O","O","O","O","O","O","O","O","B-country","O","O","B-election","I-election","I-election","I-election","O","O","O","B-politician","I-politician","O","O","O","B-politician","I-politician","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","event","election","location","politician","country","political_party","organization"]}
{"id":"56","dataset":"crossner_politics","split":"test","instance":{"id":"56","prompt_labels":"She(O) contested(O) the(O) seat(O) of(O) Burnley(B-location) in(O) Lancashire(B-location) in(O) the(O) 1979(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) then(O) ,(O) against(O) David(B-politician) Owen(I-politician) ,(O) the(O) Plymouth(B-location) Devonport(I-location) seat(O) in(O) the(O) 1983(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, organization, location, election, politician, event, political party and O.\nSentence: She contested the seat of Burnley in Lancashire in the 1979 United Kingdom general election and then , against David Owen , the Plymouth Devonport seat in the 1983 United Kingdom general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","contested","the","seat","of","Burnley","in","Lancashire","in","the","1979","United","Kingdom","general","election","and","then",",","against","David","Owen",",","the","Plymouth","Devonport","seat","in","the","1983","United","Kingdom","general","election","."],"labels":["O","O","O","O","O","B-location","O","B-location","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","B-politician","I-politician","O","O","B-location","I-location","O","O","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["person","country","organization","location","election","politician","event","political_party"]}
{"id":"59","dataset":"crossner_politics","split":"test","instance":{"id":"59","prompt_labels":"Early(O) loans(O) went(O) largely(O) to(O) Indonesia(B-country) ,(O) Thailand(B-country) ,(O) Malaysia(B-country) ,(O) South(B-country) Korea(I-country) and(O) the(O) Philippines(B-country) ;(O) these(O) nations(O) accounted(O) for(O) 78.48(O) %(O) of(O) the(O) total(O) ADB(B-organization) loans(O) between(O) 1967(O) and(O) 1972(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, person, politician, country, location, event, organization, political party and O.\nSentence: Early loans went largely to Indonesia , Thailand , Malaysia , South Korea and the Philippines ; these nations accounted for 78.48 % of the total ADB loans between 1967 and 1972 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Early","loans","went","largely","to","Indonesia",",","Thailand",",","Malaysia",",","South","Korea","and","the","Philippines",";","these","nations","accounted","for","78.48","%","of","the","total","ADB","loans","between","1967","and","1972","."],"labels":["O","O","O","O","O","B-country","O","B-country","O","B-country","O","B-country","I-country","O","O","B-country","O","O","O","O","O","O","O","O","O","O","B-organization","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","person","politician","country","location","event","organization","political_party"]}
{"id":"60","dataset":"crossner_politics","split":"test","instance":{"id":"60","prompt_labels":"Abbas(B-person) II(I-person) Helmy(I-person) Bey(I-person) ((O) also(O) known(O) as(O) Abbs(B-person) ilm(I-person) Pasha(I-person) ,(O) In(O) 1914(O) ,(O) after(O) the(O) Ottoman(B-country) Empire(I-country) joined(O) the(O) Central(B-country) Powers(I-country) in(O) World(B-event) War(I-event) I(I-event) ,(O) the(O) nationalist(O) Khedive(O) was(O) removed(O) by(O) the(O) British(O) ,(O) then(O) ruling(O) Egypt(B-country) ,(O) in(O) favor(O) of(O) his(O) more(O) pro-British(O) uncle(O) ,(O) Hussein(B-politician) Kamel(I-politician) ,(O) marking(O) the(O) de(O) jure(O) end(O) of(O) Egypt(B-country) 's(O) four-century(O) era(O) as(O) a(O) province(O) of(O) the(O) Ottoman(B-country) Empire(I-country) ,(O) which(O) had(O) begun(O) in(O) 1517(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, location, organization, person, event, politician, political party, country and O.\nSentence: Abbas II Helmy Bey ( also known as Abbs ilm Pasha , In 1914 , after the Ottoman Empire joined the Central Powers in World War I , the nationalist Khedive was removed by the British , then ruling Egypt , in favor of his more pro-British uncle , Hussein Kamel , marking the de jure end of Egypt 's four-century era as a province of the Ottoman Empire , which had begun in 1517 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Abbas","II","Helmy","Bey","(","also","known","as","Abbs","ilm","Pasha",",","In","1914",",","after","the","Ottoman","Empire","joined","the","Central","Powers","in","World","War","I",",","the","nationalist","Khedive","was","removed","by","the","British",",","then","ruling","Egypt",",","in","favor","of","his","more","pro-British","uncle",",","Hussein","Kamel",",","marking","the","de","jure","end","of","Egypt","'s","four-century","era","as","a","province","of","the","Ottoman","Empire",",","which","had","begun","in","1517","."],"labels":["B-person","I-person","I-person","I-person","O","O","O","O","B-person","I-person","I-person","O","O","O","O","O","O","B-country","I-country","O","O","B-country","I-country","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","B-country","I-country","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","location","organization","person","event","politician","political_party","country"]}
{"id":"61","dataset":"crossner_politics","split":"test","instance":{"id":"61","prompt_labels":"Abbas(B-person) II(I-person) ((O) full(O) name(O) :(O) Abbas(B-person) Hilmy(I-person) )(O) ,(O) the(O) great-great-grandson(O) of(O) Muhammad(B-person) Ali(I-person) ,(O) was(O) born(O) in(O) Alexandria(B-location) ,(O) Egypt(B-country) on(O) 14(O) July(O) 1874(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, person, politician, election, organization, political party, location and O.\nSentence: Abbas II ( full name : Abbas Hilmy ) , the great-great-grandson of Muhammad Ali , was born in Alexandria , Egypt on 14 July 1874 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Abbas","II","(","full","name",":","Abbas","Hilmy",")",",","the","great-great-grandson","of","Muhammad","Ali",",","was","born","in","Alexandria",",","Egypt","on","14","July","1874","."],"labels":["B-person","I-person","O","O","O","O","B-person","I-person","O","O","O","O","O","B-person","I-person","O","O","O","O","B-location","O","B-country","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","event","person","politician","election","organization","political_party","location"]}
{"id":"62","dataset":"crossner_politics","split":"test","instance":{"id":"62","prompt_labels":"His(O) second(O) marriage(O) in(O) ubuklu(B-location) ,(O) Turkey(B-country) on(O) 1(O) March(O) 1910(O) was(O) to(O) Hungary(B-country) noblewoman(O) Marianna(B-person) Trk(I-person) de(I-person) Szendr(I-person) ,(O) who(O) took(O) the(O) name(O) Zbeyde(B-person) Cavidan(I-person) Hanm(I-person) ((O) Philadelphia(B-location) ,(O) Pennsylvania(B-location) ,(O) U.S.(B-country) ,(O) 8(O) January(O) 1874(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, politician, organization, political party, location, election, person and O.\nSentence: His second marriage in ubuklu , Turkey on 1 March 1910 was to Hungary noblewoman Marianna Trk de Szendr , who took the name Zbeyde Cavidan Hanm ( Philadelphia , Pennsylvania , U.S. , 8 January 1874","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","second","marriage","in","ubuklu",",","Turkey","on","1","March","1910","was","to","Hungary","noblewoman","Marianna","Trk","de","Szendr",",","who","took","the","name","Zbeyde","Cavidan","Hanm","(","Philadelphia",",","Pennsylvania",",","U.S.",",","8","January","1874"],"labels":["O","O","O","O","B-location","O","B-country","O","O","O","O","O","O","B-country","O","B-person","I-person","I-person","I-person","O","O","O","O","O","B-person","I-person","I-person","O","B-location","O","B-location","O","B-country","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","event","politician","organization","political_party","location","election","person"]}
{"id":"63","dataset":"crossner_politics","split":"test","instance":{"id":"63","prompt_labels":"Today(O) ,(O) his(O) descendants(O) can(O) be(O) found(O) in(O) many(O) places(O) outside(O) Afghanistan(B-country) ,(O) such(O) as(O) in(O) America(B-country) ,(O) France(B-country) ,(O) Germany(B-country) ,(O) and(O) even(O) in(O) Scandinavian(O) countries(O) such(O) as(O) Denmark(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, politician, event, political party, election, person, country and O.\nSentence: Today , his descendants can be found in many places outside Afghanistan , such as in America , France , Germany , and even in Scandinavian countries such as Denmark .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Today",",","his","descendants","can","be","found","in","many","places","outside","Afghanistan",",","such","as","in","America",",","France",",","Germany",",","and","even","in","Scandinavian","countries","such","as","Denmark","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","B-country","O","B-country","O","B-country","O","O","O","O","O","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["organization","location","politician","event","political_party","election","person","country"]}
{"id":"64","dataset":"crossner_politics","split":"test","instance":{"id":"64","prompt_labels":"Adam(B-person) confirmed(O) through(O) a(O) press(O) conference(O) that(O) the(O) film(O) would(O) co-star(O) David(B-person) Alan(I-person) Grier(I-person) ,(O) Illeana(B-person) Douglas(I-person) ,(O) Diane(B-person) Farr(I-person) ,(O) and(O) Larry(B-person) Miller(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, election, location, country, person, political party, event, politician and O.\nSentence: Adam confirmed through a press conference that the film would co-star David Alan Grier , Illeana Douglas , Diane Farr , and Larry Miller .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Adam","confirmed","through","a","press","conference","that","the","film","would","co-star","David","Alan","Grier",",","Illeana","Douglas",",","Diane","Farr",",","and","Larry","Miller","."],"labels":["B-person","O","O","O","O","O","O","O","O","O","O","B-person","I-person","I-person","O","B-person","I-person","O","B-person","I-person","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["organization","election","location","country","person","political_party","event","politician"]}
{"id":"65","dataset":"crossner_politics","split":"test","instance":{"id":"65","prompt_labels":"The(O) youngest(O) daughter(O) of(O) Aung(B-politician) San(I-politician) ,(O) Father(O) of(O) the(O) Nation(O) of(O) modern-day(O) Myanmar(B-country) ,(O) and(O) Khin(B-politician) Kyi(I-politician) ,(O) Aung(B-politician) San(I-politician) Suu(I-politician) Kyi(I-politician) was(O) born(O) in(O) Yangon(B-location) ,(O) British(O) rule(O) in(O) Burma(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, location, organization, politician, event, person, election and O.\nSentence: The youngest daughter of Aung San , Father of the Nation of modern-day Myanmar , and Khin Kyi , Aung San Suu Kyi was born in Yangon , British rule in Burma .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","youngest","daughter","of","Aung","San",",","Father","of","the","Nation","of","modern-day","Myanmar",",","and","Khin","Kyi",",","Aung","San","Suu","Kyi","was","born","in","Yangon",",","British","rule","in","Burma","."],"labels":["O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","B-country","O","O","B-politician","I-politician","O","B-politician","I-politician","I-politician","I-politician","O","O","O","B-location","O","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["country","political_party","location","organization","politician","event","person","election"]}
{"id":"66","dataset":"crossner_politics","split":"test","instance":{"id":"66","prompt_labels":"On(O) 9(O) November(O) 1996(O) ,(O) the(O) motorcade(O) that(O) Aung(B-politician) San(I-politician) Suu(I-politician) Kyi(I-politician) was(O) traveling(O) in(O) with(O) other(O) National(B-political party) League(I-political party) for(I-political party) Democracy(I-political party) leaders(O) Tin(B-politician) Oo(I-politician) and(O) Kyi(B-politician) Maung(I-politician) ,(O) was(O) attacked(O) in(O) Yangon(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, politician, location, person, organization, political party, country and O.\nSentence: On 9 November 1996 , the motorcade that Aung San Suu Kyi was traveling in with other National League for Democracy leaders Tin Oo and Kyi Maung , was attacked in Yangon .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","9","November","1996",",","the","motorcade","that","Aung","San","Suu","Kyi","was","traveling","in","with","other","National","League","for","Democracy","leaders","Tin","Oo","and","Kyi","Maung",",","was","attacked","in","Yangon","."],"labels":["O","O","O","O","O","O","O","O","B-politician","I-politician","I-politician","I-politician","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-politician","I-politician","O","B-politician","I-politician","O","O","O","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["event","election","politician","location","person","organization","political_party","country"]}
{"id":"67","dataset":"crossner_politics","split":"test","instance":{"id":"67","prompt_labels":"Asked(O) what(O) democratic(O) models(O) Myanmar(B-country) could(O) look(O) to(O) ,(O) she(O) said(O) :(O) We(O) have(O) many(O) ,(O) many(O) lessons(O) to(O) learn(O) from(O) various(O) places(O) ,(O) not(O) just(O) the(O) Asian(O) countries(O) like(O) South(B-country) Korea(I-country) ,(O) Mongolia(B-country) ,(O) and(O) Indonesia(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, political party, organization, politician, election, country, location and O.\nSentence: Asked what democratic models Myanmar could look to , she said : We have many , many lessons to learn from various places , not just the Asian countries like South Korea , Mongolia , and Indonesia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Asked","what","democratic","models","Myanmar","could","look","to",",","she","said",":","We","have","many",",","many","lessons","to","learn","from","various","places",",","not","just","the","Asian","countries","like","South","Korea",",","Mongolia",",","and","Indonesia","."],"labels":["O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O","B-country","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["person","event","political_party","organization","politician","election","country","location"]}
{"id":"68","dataset":"crossner_politics","split":"test","instance":{"id":"68","prompt_labels":"In(O) a(O) nod(O) to(O) the(O) deep(O) US(B-country) political(O) divide(O) between(O) Republicans(O) led(O) by(O) Mitt(B-politician) Romney(I-politician) and(O) the(O) Democrats(O) of(O) Barack(B-politician) Obama(I-politician) -(O) then(O) battling(O) to(O) win(O) the(O) 2012(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) -(O) she(O) stressed(O) ,(O) Those(O) of(O) you(O) who(O) are(O) familiar(O) with(O) American(O) politics(O) I(O) 'm(O) sure(O) understand(O) the(O) need(O) for(O) negotiated(O) compromise(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, event, organization, person, politician, political party, election and O.\nSentence: In a nod to the deep US political divide between Republicans led by Mitt Romney and the Democrats of Barack Obama - then battling to win the 2012 United States presidential election - she stressed , Those of you who are familiar with American politics I 'm sure understand the need for negotiated compromise .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","a","nod","to","the","deep","US","political","divide","between","Republicans","led","by","Mitt","Romney","and","the","Democrats","of","Barack","Obama","-","then","battling","to","win","the","2012","United","States","presidential","election","-","she","stressed",",","Those","of","you","who","are","familiar","with","American","politics","I","'m","sure","understand","the","need","for","negotiated","compromise","."],"labels":["O","O","O","O","O","O","B-country","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","country","event","organization","person","politician","political_party","election"]}
{"id":"69","dataset":"crossner_politics","split":"test","instance":{"id":"69","prompt_labels":"The(O) life(O) of(O) Aung(B-politician) San(I-politician) Suu(I-politician) Kyi(I-politician) and(O) her(O) husband(O) Michael(B-person) Aris(I-person) is(O) portrayed(O) in(O) Luc(B-person) Besson(I-person) '(O) s(O) 2011(O) film(O) The(O) Lady(O) ,(O) in(O) which(O) they(O) are(O) played(O) by(O) Michelle(B-person) Yeoh(I-person) and(O) David(B-person) Thewlis(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, election, location, country, event, political party, politician and O.\nSentence: The life of Aung San Suu Kyi and her husband Michael Aris is portrayed in Luc Besson ' s 2011 film The Lady , in which they are played by Michelle Yeoh and David Thewlis .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","life","of","Aung","San","Suu","Kyi","and","her","husband","Michael","Aris","is","portrayed","in","Luc","Besson","'","s","2011","film","The","Lady",",","in","which","they","are","played","by","Michelle","Yeoh","and","David","Thewlis","."],"labels":["O","O","O","B-politician","I-politician","I-politician","I-politician","O","O","O","B-person","I-person","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["organization","person","election","location","country","event","political_party","politician"]}
{"id":"78","dataset":"crossner_politics","split":"test","instance":{"id":"78","prompt_labels":"A(O) group(O) of(O) six(O) assassins(O) ((O) Cvjetko(B-person) Popovi(I-person) ,(O) Gavrilo(B-person) Princip(I-person) ,(O) Muhamed(B-person) Mehmedbai(I-person) ,(O) Nedeljko(B-person) abrinovi(I-person) ,(O) Trifko(B-person) Grabe(I-person) ,(O) Vaso(B-person) ubrilovi(I-person) )(O) from(O) the(O) nationalist(O) group(O) Young(B-organization) Bosnia(I-organization) ,(O) supplied(O) by(O) the(O) Black(B-organization) Hand(I-organization) ,(O) had(O) gathered(O) on(O) the(O) street(O) where(O) the(O) Archduke(O) 's(O) motorcade(O) would(O) pass(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, political party, election, politician, person, event, country and O.\nSentence: A group of six assassins ( Cvjetko Popovi , Gavrilo Princip , Muhamed Mehmedbai , Nedeljko abrinovi , Trifko Grabe , Vaso ubrilovi ) from the nationalist group Young Bosnia , supplied by the Black Hand , had gathered on the street where the Archduke 's motorcade would pass .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","group","of","six","assassins","(","Cvjetko","Popovi",",","Gavrilo","Princip",",","Muhamed","Mehmedbai",",","Nedeljko","abrinovi",",","Trifko","Grabe",",","Vaso","ubrilovi",")","from","the","nationalist","group","Young","Bosnia",",","supplied","by","the","Black","Hand",",","had","gathered","on","the","street","where","the","Archduke","'s","motorcade","would","pass","."],"labels":["O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","O","O","O","O","B-organization","I-organization","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","organization","political_party","election","politician","person","event","country"]}
{"id":"83","dataset":"crossner_politics","split":"test","instance":{"id":"83","prompt_labels":"On(O) the(O) Coalition(O) side(O) of(O) politics(O) ,(O) Pru(B-politician) Goward(I-politician) has(O) served(O) as(O) a(O) Minister(O) in(O) the(O) NSW(B-political party) state(I-political party) Liberal(I-political party) Government(O) ,(O) Scott(B-politician) Emerson(I-politician) and(O) Sarah(B-politician) Henderson(I-politician) all(O) held(O) ,(O) or(O) hold(O) ,(O) positions(O) at(O) the(O) ABC(B-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, event, organization, person, election, political party, location and O.\nSentence: On the Coalition side of politics , Pru Goward has served as a Minister in the NSW state Liberal Government , Scott Emerson and Sarah Henderson all held , or hold , positions at the ABC .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","the","Coalition","side","of","politics",",","Pru","Goward","has","served","as","a","Minister","in","the","NSW","state","Liberal","Government",",","Scott","Emerson","and","Sarah","Henderson","all","held",",","or","hold",",","positions","at","the","ABC","."],"labels":["O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-politician","I-politician","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","B-organization","O"],"target_index":null,"target_label":null},"label_list":["politician","country","event","organization","person","election","political_party","location"]}
{"id":"84","dataset":"crossner_politics","split":"test","instance":{"id":"84","prompt_labels":"This(O) school(O) initially(O) consisted(O) of(O) nearly(O) 200(O) faculty(O) members(O) and(O) Ph.D.(O) students(O) from(O) the(O) Vrije(B-organization) Universiteit(I-organization) ,(O) University(B-organization) of(I-organization) Amsterdam(I-organization) ,(O) Delft(B-organization) University(I-organization) of(I-organization) Technology(I-organization) ,(O) and(O) Leiden(B-organization) University(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, election, event, politician, person, organization, political party and O.\nSentence: This school initially consisted of nearly 200 faculty members and Ph.D. students from the Vrije Universiteit , University of Amsterdam , Delft University of Technology , and Leiden University .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","school","initially","consisted","of","nearly","200","faculty","members","and","Ph.D.","students","from","the","Vrije","Universiteit",",","University","of","Amsterdam",",","Delft","University","of","Technology",",","and","Leiden","University","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["country","location","election","event","politician","person","organization","political_party"]}
{"id":"91","dataset":"crossner_politics","split":"test","instance":{"id":"91","prompt_labels":"Belgium(B-country) became(O) one(O) of(O) the(O) six(O) founding(O) members(O) of(O) the(O) European(B-organization) Coal(I-organization) and(I-organization) Steel(I-organization) Community(I-organization) in(O) 1951(O) and(O) of(O) the(O) European(B-organization) Atomic(I-organization) Energy(I-organization) Community(I-organization) and(O) European(B-organization) Economic(I-organization) Community(I-organization) ,(O) established(O) in(O) 1957(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, country, politician, election, person, organization, location and O.\nSentence: Belgium became one of the six founding members of the European Coal and Steel Community in 1951 and of the European Atomic Energy Community and European Economic Community , established in 1957 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Belgium","became","one","of","the","six","founding","members","of","the","European","Coal","and","Steel","Community","in","1951","and","of","the","European","Atomic","Energy","Community","and","European","Economic","Community",",","established","in","1957","."],"labels":["B-country","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","event","country","politician","election","person","organization","location"]}
{"id":"94","dataset":"crossner_politics","split":"test","instance":{"id":"94","prompt_labels":"In(O) the(O) 1996(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) Clinton(B-politician) was(O) re-elected(O) ,(O) receiving(O) 49.2(O) percent(O) of(O) the(O) popular(O) vote(O) over(O) Republican(O) Bob(B-politician) Dole(I-politician) ((O) 40.7(O) percent(O) of(O) the(O) popular(O) vote(O) )(O) and(O) Reform(B-political party) Party(I-political party) of(I-political party) the(I-political party) United(I-political party) States(I-political party) of(I-political party) America(I-political party) candidate(O) Ross(B-politician) Perot(I-politician) ((O) 8.4(O) percent(O) of(O) the(O) popular(O) vote(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, country, event, organization, location, election, person and O.\nSentence: In the 1996 United States presidential election , Clinton was re-elected , receiving 49.2 percent of the popular vote over Republican Bob Dole ( 40.7 percent of the popular vote ) and Reform Party of the United States of America candidate Ross Perot ( 8.4 percent of the popular vote ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","1996","United","States","presidential","election",",","Clinton","was","re-elected",",","receiving","49.2","percent","of","the","popular","vote","over","Republican","Bob","Dole","(","40.7","percent","of","the","popular","vote",")","and","Reform","Party","of","the","United","States","of","America","candidate","Ross","Perot","(","8.4","percent","of","the","popular","vote",")","."],"labels":["O","O","B-election","I-election","I-election","I-election","I-election","O","B-politician","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","politician","country","event","organization","location","election","person"]}
{"id":"97","dataset":"crossner_politics","split":"test","instance":{"id":"97","prompt_labels":"The(O) main(O) Eurosceptic(O) parties(O) in(O) Portugal(B-country) are(O) National(B-political party) Renovator(I-political party) Party(I-political party) ((O) PNR(B-political party) )(O) ,(O) Portuguese(B-political party) Communist(I-political party) Party(I-political party) ((O) PCP(B-political party) )(O) ,(O) and(O) Left(B-political party) Bloc(I-political party) ((O) BE(B-political party) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, organization, political party, event, election, location, politician and O.\nSentence: The main Eurosceptic parties in Portugal are National Renovator Party ( PNR ) , Portuguese Communist Party ( PCP ) , and Left Bloc ( BE ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","main","Eurosceptic","parties","in","Portugal","are","National","Renovator","Party","(","PNR",")",",","Portuguese","Communist","Party","(","PCP",")",",","and","Left","Bloc","(","BE",")","."],"labels":["O","O","O","O","O","B-country","O","B-political party","I-political party","I-political party","O","B-political party","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","O","B-political party","O","O"],"target_index":null,"target_label":null},"label_list":["country","person","organization","political_party","event","election","location","politician"]}
{"id":"98","dataset":"crossner_politics","split":"test","instance":{"id":"98","prompt_labels":"In(O) the(O) last(O) 2014(B-election) European(I-election) Parliament(I-election) election(I-election) ,(O) the(O) Portuguese(B-political party) Communist(I-political party) Party(I-political party) won(O) three(O) seats(O) and(O) the(O) Left(B-political party) Bloc(I-political party) won(O) one(O) seat(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, election, organization, country, event, person, location and O.\nSentence: In the last 2014 European Parliament election , the Portuguese Communist Party won three seats and the Left Bloc won one seat .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","last","2014","European","Parliament","election",",","the","Portuguese","Communist","Party","won","three","seats","and","the","Left","Bloc","won","one","seat","."],"labels":["O","O","O","B-election","I-election","I-election","I-election","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","B-political party","I-political party","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","politician","election","organization","country","event","person","location"]}
{"id":"100","dataset":"crossner_politics","split":"test","instance":{"id":"100","prompt_labels":"He(O) was(O) a(O) founder(O) and(O) the(O) only(O) leader(O) of(O) the(O) Reform(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) ,(O) a(O) Canadian(O) federal(O) political(O) party(O) that(O) evolved(O) into(O) the(O) Canadian(B-political party) Alliance(I-political party) which(O) in(O) turn(O) merged(O) with(O) the(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) to(O) form(O) today(O) 's(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, country, person, election, location, event, politician and O.\nSentence: He was a founder and the only leader of the Reform Party of Canada , a Canadian federal political party that evolved into the Canadian Alliance which in turn merged with the Progressive Conservative Party of Canada to form today 's Conservative Party of Canada .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","a","founder","and","the","only","leader","of","the","Reform","Party","of","Canada",",","a","Canadian","federal","political","party","that","evolved","into","the","Canadian","Alliance","which","in","turn","merged","with","the","Progressive","Conservative","Party","of","Canada","to","form","today","'s","Conservative","Party","of","Canada","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["political_party","organization","country","person","election","location","event","politician"]}
{"id":"103","dataset":"crossner_politics","split":"test","instance":{"id":"103","prompt_labels":"In(O) 1959(O) ,(O) he(O) resigned(O) from(O) the(O) Indian(B-organization) National(I-organization) Congress(I-organization) and(O) founded(O) the(O) Swatantra(B-political party) Party(I-political party) ,(O) which(O) fought(O) against(O) the(O) Congress(O) in(O) the(O) 1962(B-election) Indian(I-election) general(I-election) election(I-election) ,(O) 1967(B-election) Indian(I-election) general(I-election) election(I-election) and(O) 1971(B-election) Indian(I-election) general(I-election) election(I-election) elections(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, organization, election, country, location, event, person and O.\nSentence: In 1959 , he resigned from the Indian National Congress and founded the Swatantra Party , which fought against the Congress in the 1962 Indian general election , 1967 Indian general election and 1971 Indian general election elections .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1959",",","he","resigned","from","the","Indian","National","Congress","and","founded","the","Swatantra","Party",",","which","fought","against","the","Congress","in","the","1962","Indian","general","election",",","1967","Indian","general","election","and","1971","Indian","general","election","elections","."],"labels":["O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","politician","organization","election","country","location","event","person"]}
{"id":"104","dataset":"crossner_politics","split":"test","instance":{"id":"104","prompt_labels":"Like(O) its(O) competitor(O) The(B-organization) Globe(I-organization) and(I-organization) Mail(I-organization) ,(O) the(O) Post(O) publishes(O) a(O) separate(O) edition(O) in(O) Toronto(B-location) ,(O) Ontario(B-location) ,(O) Canada(B-country) 's(O) largest(O) city(O) and(O) the(O) fourth(O) largest(O) English-language(O) media(O) centre(O) in(O) North(B-location) America(I-location) after(O) New(B-location) York(I-location) City(I-location) ,(O) Los(B-location) Angeles(I-location) and(O) Chicago(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, election, organization, location, political party, event, politician, country and O.\nSentence: Like its competitor The Globe and Mail , the Post publishes a separate edition in Toronto , Ontario , Canada 's largest city and the fourth largest English-language media centre in North America after New York City , Los Angeles and Chicago .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Like","its","competitor","The","Globe","and","Mail",",","the","Post","publishes","a","separate","edition","in","Toronto",",","Ontario",",","Canada","'s","largest","city","and","the","fourth","largest","English-language","media","centre","in","North","America","after","New","York","City",",","Los","Angeles","and","Chicago","."],"labels":["O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-country","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","I-location","I-location","O","B-location","I-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["person","election","organization","location","political_party","event","politician","country"]}
{"id":"106","dataset":"crossner_politics","split":"test","instance":{"id":"106","prompt_labels":"This(O) was(O) followed(O) by(O) the(O) Somaschi(B-organization) Fathers(I-organization) in(O) 1528(O) ,(O) the(O) Barnabites(B-organization) in(O) 1530(O) ,(O) the(O) Ursulines(B-organization) in(O) 1535(O) ,(O) the(O) Jesuits(B-organization) ,(O) canonically(O) recognised(O) in(O) 1540(O) ,(O) the(O) Clerics(B-organization) Regular(I-organization) of(I-organization) the(I-organization) Mother(I-organization) of(I-organization) God(I-organization) of(I-organization) Lucca(I-organization) in(O) 1583(O) ,(O) the(O) Camillians(B-organization) in(O) 1584(O) ,(O) the(O) Adorno(B-organization) Fathers(I-organization) in(O) 1588(O) ,(O) and(O) finally(O) the(O) Piarists(B-organization) in(O) 1621(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, location, country, person, politician, political party, election and O.\nSentence: This was followed by the Somaschi Fathers in 1528 , the Barnabites in 1530 , the Ursulines in 1535 , the Jesuits , canonically recognised in 1540 , the Clerics Regular of the Mother of God of Lucca in 1583 , the Camillians in 1584 , the Adorno Fathers in 1588 , and finally the Piarists in 1621 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","was","followed","by","the","Somaschi","Fathers","in","1528",",","the","Barnabites","in","1530",",","the","Ursulines","in","1535",",","the","Jesuits",",","canonically","recognised","in","1540",",","the","Clerics","Regular","of","the","Mother","of","God","of","Lucca","in","1583",",","the","Camillians","in","1584",",","the","Adorno","Fathers","in","1588",",","and","finally","the","Piarists","in","1621","."],"labels":["O","O","O","O","O","B-organization","I-organization","O","O","O","O","B-organization","O","O","O","O","B-organization","O","O","O","O","B-organization","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","B-organization","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","B-organization","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","organization","location","country","person","politician","political_party","election"]}
{"id":"107","dataset":"crossner_politics","split":"test","instance":{"id":"107","prompt_labels":"Orders(O) such(O) as(O) the(O) Order(B-organization) of(I-organization) Friars(I-organization) Minor(I-organization) Capuchin(I-organization) ,(O) Discalced(B-organization) Carmelites(I-organization) ,(O) Discalced(B-organization) Augustinians(I-organization) ,(O) Augustinian(B-organization) Recollects(I-organization) ,(O) Cistercian(B-organization) Feuillants(I-organization) ,(O) Ursulines(B-organization) ,(O) Theatines(B-organization) ,(O) Barnabites(B-organization) ,(O) Congregation(B-organization) of(I-organization) the(I-organization) Oratory(I-organization) of(I-organization) Saint(I-organization) Philip(I-organization) Neri(I-organization) ,(O) and(O) especially(O) Society(B-organization) of(I-organization) Jesus(I-organization) worked(O) in(O) rural(O) parishes(O) and(O) set(O) examples(O) of(O) Catholic(O) renewal(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, event, country, organization, political party, location, election, person and O.\nSentence: Orders such as the Order of Friars Minor Capuchin , Discalced Carmelites , Discalced Augustinians , Augustinian Recollects , Cistercian Feuillants , Ursulines , Theatines , Barnabites , Congregation of the Oratory of Saint Philip Neri , and especially Society of Jesus worked in rural parishes and set examples of Catholic renewal .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Orders","such","as","the","Order","of","Friars","Minor","Capuchin",",","Discalced","Carmelites",",","Discalced","Augustinians",",","Augustinian","Recollects",",","Cistercian","Feuillants",",","Ursulines",",","Theatines",",","Barnabites",",","Congregation","of","the","Oratory","of","Saint","Philip","Neri",",","and","especially","Society","of","Jesus","worked","in","rural","parishes","and","set","examples","of","Catholic","renewal","."],"labels":["O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","O","B-organization","O","B-organization","O","B-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["politician","event","country","organization","political_party","location","election","person"]}
{"id":"110","dataset":"crossner_politics","split":"test","instance":{"id":"110","prompt_labels":"MacKay(B-politician) was(O) the(O) final(O) leader(O) of(O) the(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) ((O) PC(B-political party) Party(I-political party) )(O) ,(O) and(O) he(O) agreed(O) to(O) merge(O) the(O) party(O) with(O) Stephen(B-politician) Harper(I-politician) 's(O) Canadian(B-political party) Alliance(I-political party) in(O) 2003(O) ,(O) forming(O) the(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, location, organization, person, politician, political party, country and O.\nSentence: MacKay was the final leader of the Progressive Conservative Party of Canada ( PC Party ) , and he agreed to merge the party with Stephen Harper 's Canadian Alliance in 2003 , forming the Conservative Party of Canada .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["MacKay","was","the","final","leader","of","the","Progressive","Conservative","Party","of","Canada","(","PC","Party",")",",","and","he","agreed","to","merge","the","party","with","Stephen","Harper","'s","Canadian","Alliance","in","2003",",","forming","the","Conservative","Party","of","Canada","."],"labels":["B-politician","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-political party","I-political party","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["event","election","location","organization","person","politician","political_party","country"]}
{"id":"111","dataset":"crossner_politics","split":"test","instance":{"id":"111","prompt_labels":"In(O) New(B-country) Zealand(I-country) ,(O) the(O) Mori(B-political party) Party(I-political party) won(O) one(O) overhang(O) seat(O) in(O) 2005(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) and(O) 2011(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ,(O) and(O) two(O) overhang(O) seats(O) in(O) 2008(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, organization, election, person, event, location, country and O.\nSentence: In New Zealand , the Mori Party won one overhang seat in 2005 New Zealand general election and 2011 New Zealand general election , and two overhang seats in 2008 New Zealand general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","New","Zealand",",","the","Mori","Party","won","one","overhang","seat","in","2005","New","Zealand","general","election","and","2011","New","Zealand","general","election",",","and","two","overhang","seats","in","2008","New","Zealand","general","election","."],"labels":["O","B-country","I-country","O","O","B-political party","I-political party","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["political_party","politician","organization","election","person","event","location","country"]}
{"id":"113","dataset":"crossner_politics","split":"test","instance":{"id":"113","prompt_labels":"Reid(B-politician) adopted(O) a(O) strategy(O) of(O) trying(O) to(O) reorient(O) the(O) party(O) system(O) along(O) Australian(B-political party) Labor(I-political party) Party(I-political party) vs(O) non-Labor(O) lines(O) -(O) prior(O) to(O) the(O) 1906(B-election) Australian(I-election) federal(I-election) election(I-election) ,(O) he(O) renamed(O) his(O) Free(B-political party) Trade(I-political party) Party(I-political party) to(O) the(O) Anti-Socialist(B-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, election, political party, location, event, politician, organization, country and O.\nSentence: Reid adopted a strategy of trying to reorient the party system along Australian Labor Party vs non-Labor lines - prior to the 1906 Australian federal election , he renamed his Free Trade Party to the Anti-Socialist Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Reid","adopted","a","strategy","of","trying","to","reorient","the","party","system","along","Australian","Labor","Party","vs","non-Labor","lines","-","prior","to","the","1906","Australian","federal","election",",","he","renamed","his","Free","Trade","Party","to","the","Anti-Socialist","Party","."],"labels":["B-politician","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["person","election","political_party","location","event","politician","organization","country"]}
{"id":"114","dataset":"crossner_politics","split":"test","instance":{"id":"114","prompt_labels":"Born(O) in(O) Hanover(B-location) to(O) Carl(B-person) Hugenberg(I-person) ,(O) a(O) royal(O) Kingdom(B-country) of(I-country) Hanover(I-country) official(O) who(O) in(O) 1867(O) entered(O) the(O) Prussian(B-organization) Landtag(I-organization) as(O) a(O) member(O) of(O) the(O) National(B-political party) Liberal(I-political party) Party(I-political party) ,(O) he(O) studied(O) law(O) in(O) University(B-organization) of(I-organization) Gttingen(I-organization) ,(O) Heidelberg(B-location) ,(O) and(O) Humboldt(B-organization) University(I-organization) of(I-organization) Berlin(I-organization) ,(O) as(O) well(O) as(O) economics(O) in(O) University(B-organization) of(I-organization) Strasbourg(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, political party, location, person, country, election, event and O.\nSentence: Born in Hanover to Carl Hugenberg , a royal Kingdom of Hanover official who in 1867 entered the Prussian Landtag as a member of the National Liberal Party , he studied law in University of Gttingen , Heidelberg , and Humboldt University of Berlin , as well as economics in University of Strasbourg .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Born","in","Hanover","to","Carl","Hugenberg",",","a","royal","Kingdom","of","Hanover","official","who","in","1867","entered","the","Prussian","Landtag","as","a","member","of","the","National","Liberal","Party",",","he","studied","law","in","University","of","Gttingen",",","Heidelberg",",","and","Humboldt","University","of","Berlin",",","as","well","as","economics","in","University","of","Strasbourg","."],"labels":["O","O","B-location","O","B-person","I-person","O","O","O","B-country","I-country","I-country","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-location","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["politician","organization","political_party","location","person","country","election","event"]}
{"id":"115","dataset":"crossner_politics","split":"test","instance":{"id":"115","prompt_labels":"He(O) was(O) re-elected(O) in(O) the(O) general(O) elections(O) of(O) 1971(B-election) Saskatchewan(I-election) general(I-election) election(I-election) ,(O) 1975(B-election) Saskatchewan(I-election) general(I-election) election(I-election) and(O) 1978(B-election) Saskatchewan(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, election, political party, person, country, location, politician and O.\nSentence: He was re-elected in the general elections of 1971 Saskatchewan general election , 1975 Saskatchewan general election and 1978 Saskatchewan general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","re-elected","in","the","general","elections","of","1971","Saskatchewan","general","election",",","1975","Saskatchewan","general","election","and","1978","Saskatchewan","general","election","."],"labels":["O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["organization","event","election","political_party","person","country","location","politician"]}
{"id":"116","dataset":"crossner_politics","split":"test","instance":{"id":"116","prompt_labels":"The(O) Liberal(B-political party) Democrats(I-political party) were(O) the(O) third(O) largest(O) party(O) until(O) the(O) 2015(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) when(O) they(O) were(O) overtaken(O) by(O) the(O) Scottish(B-political party) National(I-political party) Party(I-political party) in(O) terms(O) of(O) seats(O) and(O) UK(O) political(O) party(O) membership(O) ,(O) and(O) by(O) the(O) UK(B-political party) Independence(I-political party) Party(I-political party) in(O) terms(O) of(O) votes(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, political party, person, election, politician, organization, event and O.\nSentence: The Liberal Democrats were the third largest party until the 2015 United Kingdom general election when they were overtaken by the Scottish National Party in terms of seats and UK political party membership , and by the UK Independence Party in terms of votes .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Liberal","Democrats","were","the","third","largest","party","until","the","2015","United","Kingdom","general","election","when","they","were","overtaken","by","the","Scottish","National","Party","in","terms","of","seats","and","UK","political","party","membership",",","and","by","the","UK","Independence","Party","in","terms","of","votes","."],"labels":["O","B-political party","I-political party","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","location","political_party","person","election","politician","organization","event"]}
{"id":"127","dataset":"crossner_politics","split":"test","instance":{"id":"127","prompt_labels":"The(O) Evolution(O) of(O) American(O) Electoral(O) Systems(O) ((O) 1983(O) )(O) ,(O) The(O) first(O) two-party(O) system(O) consisted(O) of(O) the(O) Federalist(B-political party) Party(I-political party) ,(O) who(O) supported(O) the(O) ratification(O) of(O) the(O) Constitution(O) ,(O) and(O) the(O) Democratic-Republican(B-political party) Party(I-political party) or(O) the(O) Anti-Administration(B-political party) party(I-political party) ((O) Anti-Federalists(B-political party) )(O) ,(O) who(O) opposed(O) the(O) powerful(O) central(O) government(O) ,(O) among(O) others(O) ,(O) that(O) the(O) Constitution(O) established(O) when(O) it(O) took(O) effect(O) in(O) 1789(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, politician, political party, organization, person, country, location and O.\nSentence: The Evolution of American Electoral Systems ( 1983 ) , The first two-party system consisted of the Federalist Party , who supported the ratification of the Constitution , and the Democratic-Republican Party or the Anti-Administration party ( Anti-Federalists ) , who opposed the powerful central government , among others , that the Constitution established when it took effect in 1789 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Evolution","of","American","Electoral","Systems","(","1983",")",",","The","first","two-party","system","consisted","of","the","Federalist","Party",",","who","supported","the","ratification","of","the","Constitution",",","and","the","Democratic-Republican","Party","or","the","Anti-Administration","party","(","Anti-Federalists",")",",","who","opposed","the","powerful","central","government",",","among","others",",","that","the","Constitution","established","when","it","took","effect","in","1789","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","B-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","event","politician","political_party","organization","person","country","location"]}
{"id":"133","dataset":"crossner_politics","split":"test","instance":{"id":"133","prompt_labels":"Gold(O) coins(O) bearing(O) portraits(O) of(O) Chandragupta(B-person) and(O) Kumaradevi(B-person) have(O) been(O) discovered(O) at(O) Mathura(B-location) ,(O) Ayodhya(B-location) ,(O) Lucknow(B-location) ,(O) Sitapur(B-location) ,(O) Tanda(B-location) ,(O) Ghazipur(B-location) ,(O) and(O) Varanasi(B-location) in(O) Uttar(B-location) Pradesh(B-location) ;(O) Bayana(B-location) in(O) Rajasthan(B-location) ;(O) and(O) Hajipur(B-location) in(O) Bihar(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, election, person, event, politician, location, country and O.\nSentence: Gold coins bearing portraits of Chandragupta and Kumaradevi have been discovered at Mathura , Ayodhya , Lucknow , Sitapur , Tanda , Ghazipur , and Varanasi in Uttar Pradesh ; Bayana in Rajasthan ; and Hajipur in Bihar .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Gold","coins","bearing","portraits","of","Chandragupta","and","Kumaradevi","have","been","discovered","at","Mathura",",","Ayodhya",",","Lucknow",",","Sitapur",",","Tanda",",","Ghazipur",",","and","Varanasi","in","Uttar","Pradesh",";","Bayana","in","Rajasthan",";","and","Hajipur","in","Bihar","."],"labels":["O","O","O","O","O","B-person","O","B-person","O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","O","B-location","O","B-location","B-location","O","B-location","O","B-location","O","O","B-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["organization","political_party","election","person","event","politician","location","country"]}
{"id":"134","dataset":"crossner_politics","split":"test","instance":{"id":"134","prompt_labels":"A(O) February(B-election) 1974(I-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) at(O) the(O) end(O) of(O) February(O) was(O) a(O) triumph(O) for(O) the(O) United(B-political party) Ulster(I-political party) Unionist(I-political party) Coalition(I-political party) ,(O) in(O) which(O) the(O) bulk(O) of(O) his(O) old(O) party(O) stood(O) as(O) Official(O) Unionists(O) with(O) William(B-politician) Craig(I-politician) 's(O) Vanguard(B-political party) Unionist(I-political party) Progressive(I-political party) Party(I-political party) and(O) Paisley(B-politician) 's(O) new(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, person, country, political party, election, event, location and O.\nSentence: A February 1974 United Kingdom general election at the end of February was a triumph for the United Ulster Unionist Coalition , in which the bulk of his old party stood as Official Unionists with William Craig 's Vanguard Unionist Progressive Party and Paisley 's new Democratic Unionist Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","February","1974","United","Kingdom","general","election","at","the","end","of","February","was","a","triumph","for","the","United","Ulster","Unionist","Coalition",",","in","which","the","bulk","of","his","old","party","stood","as","Official","Unionists","with","William","Craig","'s","Vanguard","Unionist","Progressive","Party","and","Paisley","'s","new","Democratic","Unionist","Party","."],"labels":["O","B-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-political party","I-political party","I-political party","I-political party","O","B-politician","O","O","B-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["politician","organization","person","country","political_party","election","event","location"]}
{"id":"138","dataset":"crossner_politics","split":"test","instance":{"id":"138","prompt_labels":"In(O) 1927(O) ,(O) protests(O) on(O) their(O) behalf(O) were(O) held(O) in(O) every(O) major(O) city(O) in(O) North(B-location) America(I-location) and(O) Europe(B-location) ,(O) as(O) well(O) as(O) in(O) Tokyo(B-location) ,(O) Sydney(B-location) ,(O) Melbourne(B-location) ,(O) So(B-location) Paulo(I-location) ,(O) Rio(B-location) de(I-location) Janeiro(I-location) ,(O) Buenos(B-location) Aires(I-location) ,(O) Dubai(B-location) ,(O) Montevideo(B-location) ,(O) Johannesburg(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, person, politician, election, political party, event, country and O.\nSentence: In 1927 , protests on their behalf were held in every major city in North America and Europe , as well as in Tokyo , Sydney , Melbourne , So Paulo , Rio de Janeiro , Buenos Aires , Dubai , Montevideo , Johannesburg .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1927",",","protests","on","their","behalf","were","held","in","every","major","city","in","North","America","and","Europe",",","as","well","as","in","Tokyo",",","Sydney",",","Melbourne",",","So","Paulo",",","Rio","de","Janeiro",",","Buenos","Aires",",","Dubai",",","Montevideo",",","Johannesburg","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","O","O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","I-location","O","B-location","I-location","I-location","O","B-location","I-location","O","B-location","O","B-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["location","organization","person","politician","election","political_party","event","country"]}
{"id":"142","dataset":"crossner_politics","split":"test","instance":{"id":"142","prompt_labels":"The(O) four(O) main(O) cities(O) of(O) Scotland(B-location) ,(O) Glasgow(B-location) ,(O) Edinburgh(B-location) ,(O) Aberdeen(B-location) and(O) Dundee(B-location) have(O) a(O) Lord(O) Provost(O) who(O) is(O) also(O) ,(O) ex(O) officio(O) ,(O) Lord(O) Lieutenant(O) for(O) that(O) city(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, event, election, country, location, organization, political party and O.\nSentence: The four main cities of Scotland , Glasgow , Edinburgh , Aberdeen and Dundee have a Lord Provost who is also , ex officio , Lord Lieutenant for that city .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","four","main","cities","of","Scotland",",","Glasgow",",","Edinburgh",",","Aberdeen","and","Dundee","have","a","Lord","Provost","who","is","also",",","ex","officio",",","Lord","Lieutenant","for","that","city","."],"labels":["O","O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["politician","person","event","election","country","location","organization","political_party"]}
{"id":"143","dataset":"crossner_politics","split":"test","instance":{"id":"143","prompt_labels":"It(O) merged(O) with(O) the(O) centre-right(O) Scottish(B-political party) Party(I-political party) in(O) 1934(O) to(O) form(O) the(O) Scottish(B-political party) National(I-political party) Party(I-political party) ,(O) but(O) the(O) SNP(B-political party) remained(O) a(O) peripheral(O) force(O) until(O) the(O) watershed(O) 1967(B-election) Hamilton(I-election) by-election(I-election) of(O) 1967(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, person, location, country, election, politician, organization and O.\nSentence: It merged with the centre-right Scottish Party in 1934 to form the Scottish National Party , but the SNP remained a peripheral force until the watershed 1967 Hamilton by-election of 1967 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","merged","with","the","centre-right","Scottish","Party","in","1934","to","form","the","Scottish","National","Party",",","but","the","SNP","remained","a","peripheral","force","until","the","watershed","1967","Hamilton","by-election","of","1967","."],"labels":["O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","B-political party","O","O","O","O","O","O","O","B-election","I-election","I-election","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","event","person","location","country","election","politician","organization"]}
{"id":"144","dataset":"crossner_politics","split":"test","instance":{"id":"144","prompt_labels":"The(O) Communists(O) won(O) West(B-location) Fife(I-location) in(O) 1935(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) again(O) in(O) 1945(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ((O) Willie(B-politician) Gallacher(I-politician) )(O) and(O) several(O) Glasgow(B-location) Labour(B-political party) MPs(O) joined(O) the(O) Independent(B-political party) Labour(I-political party) Party(I-political party) in(O) the(O) 1930s(O) ,(O) often(O) heavily(O) defeating(O) the(O) official(O) Labour(B-political party) candidates(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, politician, location, event, election, organization, political party and O.\nSentence: The Communists won West Fife in 1935 United Kingdom general election and again in 1945 United Kingdom general election ( Willie Gallacher ) and several Glasgow Labour MPs joined the Independent Labour Party in the 1930s , often heavily defeating the official Labour candidates .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Communists","won","West","Fife","in","1935","United","Kingdom","general","election","and","again","in","1945","United","Kingdom","general","election","(","Willie","Gallacher",")","and","several","Glasgow","Labour","MPs","joined","the","Independent","Labour","Party","in","the","1930s",",","often","heavily","defeating","the","official","Labour","candidates","."],"labels":["O","O","O","B-location","I-location","O","B-election","I-election","I-election","I-election","I-election","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-politician","I-politician","O","O","O","B-location","B-political party","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","B-political party","O","O"],"target_index":null,"target_label":null},"label_list":["person","country","politician","location","event","election","organization","political_party"]}
{"id":"145","dataset":"crossner_politics","split":"test","instance":{"id":"145","prompt_labels":"The(O) 2017(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) produced(O) a(O) mixed(O) result(O) for(O) the(O) party(O) as(O) it(O) gained(O) six(O) seat(O) and(O) increased(O) its(O) vote(O) by(O) 2.8(O) %(O) but(O) the(O) party(O) came(O) in(O) third(O) behind(O) the(O) Scottish(B-political party) National(I-political party) Party(I-political party) and(O) Scottish(B-political party) Conservatives(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, country, person, location, politician, political party, event and O.\nSentence: The 2017 United Kingdom general election produced a mixed result for the party as it gained six seat and increased its vote by 2.8 % but the party came in third behind the Scottish National Party and Scottish Conservatives .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","2017","United","Kingdom","general","election","produced","a","mixed","result","for","the","party","as","it","gained","six","seat","and","increased","its","vote","by","2.8","%","but","the","party","came","in","third","behind","the","Scottish","National","Party","and","Scottish","Conservatives","."],"labels":["O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["election","organization","country","person","location","politician","political_party","event"]}
{"id":"148","dataset":"crossner_politics","split":"test","instance":{"id":"148","prompt_labels":"He(O) was(O) put(O) in(O) charge(O) of(O) governing(O) Lingling(O) ((O) present(O) day(O) Yongzhou(B-location) ,(O) Hunan(B-location) )(O) ,(O) Guiyang(B-location) and(O) Changsha(B-location) commanderies(O) and(O) collecting(O) taxes(O) to(O) fund(O) the(O) military(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, election, event, politician, country, person, location, organization and O.\nSentence: He was put in charge of governing Lingling ( present day Yongzhou , Hunan ) , Guiyang and Changsha commanderies and collecting taxes to fund the military .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","put","in","charge","of","governing","Lingling","(","present","day","Yongzhou",",","Hunan",")",",","Guiyang","and","Changsha","commanderies","and","collecting","taxes","to","fund","the","military","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","O","B-location","O","B-location","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","election","event","politician","country","person","location","organization"]}
{"id":"150","dataset":"crossner_politics","split":"test","instance":{"id":"150","prompt_labels":"The(O) Anti-Defamation(B-organization) League(I-organization) has(O) accused(O) ANSWER(B-organization) of(O) supporting(O) terrorist(O) organizations(O) ,(O) such(O) as(O) Hezbollah(B-political party) and(O) Hamas(B-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, election, organization, person, political party, politician, country and O.\nSentence: The Anti-Defamation League has accused ANSWER of supporting terrorist organizations , such as Hezbollah and Hamas .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Anti-Defamation","League","has","accused","ANSWER","of","supporting","terrorist","organizations",",","such","as","Hezbollah","and","Hamas","."],"labels":["O","B-organization","I-organization","O","O","B-organization","O","O","O","O","O","O","O","B-political party","O","B-political party","O"],"target_index":null,"target_label":null},"label_list":["location","event","election","organization","person","political_party","politician","country"]}
{"id":"153","dataset":"crossner_politics","split":"test","instance":{"id":"153","prompt_labels":"During(O) the(O) Eurozone(B-event) crisis(I-event) ,(O) the(O) two(O) main(O) parties(O) ,(O) The(B-political party) People(I-political party) of(I-political party) Freedom(I-political party) and(O) the(O) Democratic(B-political party) Party(I-political party) ,(O) along(O) with(O) other(O) minor(O) political(O) forces(O) ,(O) supported(O) the(O) Monti(B-organization) cabinet(I-organization) ,(O) and(O) eventually(O) ,(O) after(O) the(O) 2013(B-election) Italian(I-election) general(I-election) election(I-election) ,(O) formed(O) a(O) grand(O) coalition(O) in(O) support(O) of(O) the(O) Letta(B-organization) Cabinet(I-organization) ,(O) which(O) ,(O) however(O) ,(O) was(O) opposed(O) by(O) a(O) new(O) major(O) political(O) force(O) in(O) parliament(O) ,(O) the(O) anti-establishment(O) Five(B-political party) Star(I-political party) Movement(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, election, event, politician, political party, country, organization, person and O.\nSentence: During the Eurozone crisis , the two main parties , The People of Freedom and the Democratic Party , along with other minor political forces , supported the Monti cabinet , and eventually , after the 2013 Italian general election , formed a grand coalition in support of the Letta Cabinet , which , however , was opposed by a new major political force in parliament , the anti-establishment Five Star Movement .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","the","Eurozone","crisis",",","the","two","main","parties",",","The","People","of","Freedom","and","the","Democratic","Party",",","along","with","other","minor","political","forces",",","supported","the","Monti","cabinet",",","and","eventually",",","after","the","2013","Italian","general","election",",","formed","a","grand","coalition","in","support","of","the","Letta","Cabinet",",","which",",","however",",","was","opposed","by","a","new","major","political","force","in","parliament",",","the","anti-establishment","Five","Star","Movement","."],"labels":["O","O","B-event","I-event","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["location","election","event","politician","political_party","country","organization","person"]}
{"id":"154","dataset":"crossner_politics","split":"test","instance":{"id":"154","prompt_labels":"After(O) a(O) period(O) of(O) negotiations(O) ,(O) the(O) party(O) formed(O) a(O) pact(O) with(O) Republican(B-political party) Left(I-political party) of(I-political party) Catalonia(I-political party) ,(O) Initiative(B-political party) for(I-political party) Catalonia(I-political party) Greens(I-political party) and(O) the(O) United(B-political party) and(I-political party) Alternative(I-political party) Left(I-political party) ,(O) and(O) governed(O) in(O) Catalonia(B-location) until(O) 2010(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, organization, country, event, politician, political party, election and O.\nSentence: After a period of negotiations , the party formed a pact with Republican Left of Catalonia , Initiative for Catalonia Greens and the United and Alternative Left , and governed in Catalonia until 2010 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","a","period","of","negotiations",",","the","party","formed","a","pact","with","Republican","Left","of","Catalonia",",","Initiative","for","Catalonia","Greens","and","the","United","and","Alternative","Left",",","and","governed","in","Catalonia","until","2010","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","B-location","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","location","organization","country","event","politician","political_party","election"]}
{"id":"155","dataset":"crossner_politics","split":"test","instance":{"id":"155","prompt_labels":"The(O) PSOE(B-political party) 's(O) motion(O) passed(O) with(O) the(O) support(O) of(O) Unidos(B-political party) Podemos(I-political party) ((O) UP(B-political party) )(O) ,(O) Republican(B-political party) Left(I-political party) of(I-political party) Catalonia(I-political party) ((O) ERC(B-political party) )(O) ,(O) Catalan(B-political party) European(I-political party) Democratic(I-political party) Party(I-political party) ((O) PDeCAT(B-political party) )(O) ,(O) Basque(B-political party) Nationalist(I-political party) Party(I-political party) ((O) PNV(B-political party) )(O) ,(O) Coalici(B-political party) Comproms(I-political party) ,(O) EH(B-political party) Bildu(I-political party) and(O) New(B-political party) Canaries(I-political party) ((O) NCa(B-political party) )(O) ,(O) bringing(O) down(O) the(O) Rajoy(B-politician) government(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, politician, location, organization, person, country, political party and O.\nSentence: The PSOE 's motion passed with the support of Unidos Podemos ( UP ) , Republican Left of Catalonia ( ERC ) , Catalan European Democratic Party ( PDeCAT ) , Basque Nationalist Party ( PNV ) , Coalici Comproms , EH Bildu and New Canaries ( NCa ) , bringing down the Rajoy government .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","PSOE","'s","motion","passed","with","the","support","of","Unidos","Podemos","(","UP",")",",","Republican","Left","of","Catalonia","(","ERC",")",",","Catalan","European","Democratic","Party","(","PDeCAT",")",",","Basque","Nationalist","Party","(","PNV",")",",","Coalici","Comproms",",","EH","Bildu","and","New","Canaries","(","NCa",")",",","bringing","down","the","Rajoy","government","."],"labels":["O","B-political party","O","O","O","O","O","O","O","B-political party","I-political party","O","B-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","B-political party","I-political party","O","B-political party","I-political party","O","B-political party","I-political party","O","B-political party","O","O","O","O","O","B-politician","O","O"],"target_index":null,"target_label":null},"label_list":["event","election","politician","location","organization","person","country","political_party"]}
{"id":"156","dataset":"crossner_politics","split":"test","instance":{"id":"156","prompt_labels":"On(O) the(O) same(O) day(O) as(O) the(O) 2019(O) general(O) election(O) ,(O) 2019(B-election) Valencian(I-election) regional(I-election) election(I-election) ,(O) where(O) the(O) Socialist(B-political party) Party(I-political party) of(I-political party) the(I-political party) Valencian(I-political party) Country(I-political party) was(O) re-elected(O) ,(O) in(O) coalition(O) with(O) the(O) Valencianist(O) party(O) Coalici(B-political party) Comproms(I-political party) and(O) UP(B-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, location, person, organization, politician, election, event and O.\nSentence: On the same day as the 2019 general election , 2019 Valencian regional election , where the Socialist Party of the Valencian Country was re-elected , in coalition with the Valencianist party Coalici Comproms and UP .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","the","same","day","as","the","2019","general","election",",","2019","Valencian","regional","election",",","where","the","Socialist","Party","of","the","Valencian","Country","was","re-elected",",","in","coalition","with","the","Valencianist","party","Coalici","Comproms","and","UP","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","B-political party","O"],"target_index":null,"target_label":null},"label_list":["country","political_party","location","person","organization","politician","election","event"]}
{"id":"158","dataset":"crossner_politics","split":"test","instance":{"id":"158","prompt_labels":"The(O) party(O) took(O) over(O) the(O) Presidency(O) of(O) the(O) 2019(B-election) Canarian(I-election) regional(I-election) election(I-election) with(O) the(O) support(O) of(O) New(B-political party) Canaries(I-political party) and(O) Podemos(B-political party) ,(O) ending(O) 26(O) years(O) of(O) Canarian(B-political party) Coalition(I-political party) government(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, politician, organization, country, election, political party, person and O.\nSentence: The party took over the Presidency of the 2019 Canarian regional election with the support of New Canaries and Podemos , ending 26 years of Canarian Coalition government .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","party","took","over","the","Presidency","of","the","2019","Canarian","regional","election","with","the","support","of","New","Canaries","and","Podemos",",","ending","26","years","of","Canarian","Coalition","government","."],"labels":["O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","B-political party","I-political party","O","B-political party","O","O","O","O","O","B-political party","I-political party","O","O"],"target_index":null,"target_label":null},"label_list":["location","event","politician","organization","country","election","political_party","person"]}
{"id":"159","dataset":"crossner_politics","split":"test","instance":{"id":"159","prompt_labels":"The(O) PSOE(B-political party) ,(O) UP(B-political party) ,(O) En(B-political party) Com(I-political party) Podem(I-political party) ,(O) Grupo(B-political party) Comn(I-political party) da(I-political party) Esquerda(I-political party) ,(O) PNV(B-political party) ,(O) Ms(B-political party) Pas(I-political party) ,(O) Comproms(B-political party) ,(O) NCa(B-political party) ,(O) the(O) Galician(B-political party) Nationalist(I-political party) Bloc(I-political party) ((O) BNG(B-political party) )(O) and(O) Teruel(B-political party) Existe(I-political party) ((O) TE(B-political party) )(O) voting(O) in(O) favor(O) of(O) the(O) government(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, country, event, politician, political party, person, election and O.\nSentence: The PSOE , UP , En Com Podem , Grupo Comn da Esquerda , PNV , Ms Pas , Comproms , NCa , the Galician Nationalist Bloc ( BNG ) and Teruel Existe ( TE ) voting in favor of the government .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","PSOE",",","UP",",","En","Com","Podem",",","Grupo","Comn","da","Esquerda",",","PNV",",","Ms","Pas",",","Comproms",",","NCa",",","the","Galician","Nationalist","Bloc","(","BNG",")","and","Teruel","Existe","(","TE",")","voting","in","favor","of","the","government","."],"labels":["O","B-political party","O","B-political party","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","B-political party","I-political party","O","B-political party","O","B-political party","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","B-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","organization","country","event","politician","political_party","person","election"]}
{"id":"160","dataset":"crossner_politics","split":"test","instance":{"id":"160","prompt_labels":"The(O) current(O) prime(O) minister(O) of(O) Denmark(O) is(O) Mette(B-politician) Frederiksen(I-politician) who(O) ,(O) since(O) 27(O) June(O) 2019(O) ,(O) has(O) led(O) a(O) one-party(O) government(O) consisting(O) of(O) the(O) Social(O) Democrats(O) with(O) parliamentary(O) support(O) from(O) the(O) Danish(B-political party) Social(I-political party) Liberal(I-political party) Party(I-political party) ,(O) Socialist(B-political party) People(I-political party) 's(I-political party) Party(I-political party) ,(O) Red-Green(B-political party) Alliance(I-political party) ,(O) the(O) Faroese(O) Social(B-political party) Democratic(I-political party) Party(I-political party) and(O) Greenland(B-location) '(O) s(O) Inuit(B-political party) Ataqatigiit(I-political party) and(O) Siumut(B-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, event, election, country, political party, politician, organization and O.\nSentence: The current prime minister of Denmark is Mette Frederiksen who , since 27 June 2019 , has led a one-party government consisting of the Social Democrats with parliamentary support from the Danish Social Liberal Party , Socialist People 's Party , Red-Green Alliance , the Faroese Social Democratic Party and Greenland ' s Inuit Ataqatigiit and Siumut .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","current","prime","minister","of","Denmark","is","Mette","Frederiksen","who",",","since","27","June","2019",",","has","led","a","one-party","government","consisting","of","the","Social","Democrats","with","parliamentary","support","from","the","Danish","Social","Liberal","Party",",","Socialist","People","'s","Party",",","Red-Green","Alliance",",","the","Faroese","Social","Democratic","Party","and","Greenland","'","s","Inuit","Ataqatigiit","and","Siumut","."],"labels":["O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","O","B-location","O","O","B-political party","I-political party","O","B-political party","O"],"target_index":null,"target_label":null},"label_list":["location","person","event","election","country","political_party","politician","organization"]}
{"id":"162","dataset":"crossner_politics","split":"test","instance":{"id":"162","prompt_labels":"In(O) 1642(O) parliament(O) seized(O) control(O) of(O) the(O) Tower(O) mint(O) and(O) after(O) Charles(B-person) I(I-person) tried(O) to(O) arrest(O) the(O) Five(O) Members(O) he(O) was(O) forced(O) to(O) flee(O) London(B-location) ,(O) establishing(O) at(O) least(O) 16(O) emergency(O) mints(O) across(O) the(O) British(B-location) Isles(I-location) in(O) Colchester(B-location) ,(O) Chester(B-location) ,(O) Cork(B-location) ,(O) Edinburgh(B-location) ,(O) Dublin(B-location) ,(O) Exeter(B-location) ,(O) Salisbury(B-location) ,(O) parts(O) of(O) Cornwall(B-location) including(O) Truro(B-location) ,(O) Weymouth(B-location) ,(O) Worcester(B-location) ,(O) York(B-location) ,(O) Carlisle(B-location) ,(O) Newark(B-location) ,(O) Pontefract(B-location) and(O) Scarborough(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, person, politician, event, election, political party, location and O.\nSentence: In 1642 parliament seized control of the Tower mint and after Charles I tried to arrest the Five Members he was forced to flee London , establishing at least 16 emergency mints across the British Isles in Colchester , Chester , Cork , Edinburgh , Dublin , Exeter , Salisbury , parts of Cornwall including Truro , Weymouth , Worcester , York , Carlisle , Newark , Pontefract and Scarborough .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1642","parliament","seized","control","of","the","Tower","mint","and","after","Charles","I","tried","to","arrest","the","Five","Members","he","was","forced","to","flee","London",",","establishing","at","least","16","emergency","mints","across","the","British","Isles","in","Colchester",",","Chester",",","Cork",",","Edinburgh",",","Dublin",",","Exeter",",","Salisbury",",","parts","of","Cornwall","including","Truro",",","Weymouth",",","Worcester",",","York",",","Carlisle",",","Newark",",","Pontefract","and","Scarborough","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","B-location","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["country","organization","person","politician","event","election","political_party","location"]}
{"id":"167","dataset":"crossner_politics","split":"test","instance":{"id":"167","prompt_labels":"State(O) parties(O) which(O) exclusively(O) contest(O) Victorian(O) state(O) elections(O) include(O) the(O) Aussie(B-political party) Battler(I-political party) Party(I-political party) ,(O) Hudson(B-political party) for(I-political party) Northern(I-political party) Victoria(I-political party) ,(O) Victorian(B-political party) Socialists(I-political party) ,(O) and(O) Vote(B-political party) 1(I-political party) Local(I-political party) Jobs(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, country, election, event, location, politician, organization, person and O.\nSentence: State parties which exclusively contest Victorian state elections include the Aussie Battler Party , Hudson for Northern Victoria , Victorian Socialists , and Vote 1 Local Jobs .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["State","parties","which","exclusively","contest","Victorian","state","elections","include","the","Aussie","Battler","Party",",","Hudson","for","Northern","Victoria",",","Victorian","Socialists",",","and","Vote","1","Local","Jobs","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["political_party","country","election","event","location","politician","organization","person"]}
{"id":"170","dataset":"crossner_politics","split":"test","instance":{"id":"170","prompt_labels":"After(O) the(O) 2007(B-election) elections(I-election) ,(O) and(O) the(O) death(O) of(O) Anakpawis(B-political party) representative(O) Crispin(B-politician) Beltran(I-politician) ,(O) Bayan(B-politician) now(O) has(O) five(O) combined(O) representatives(O) in(O) the(O) 14th(B-organization) Congress(I-organization) of(I-organization) the(I-organization) Philippines(I-organization) ,(O) Satur(B-politician) Ocampo(I-politician) and(O) Teodoro(B-politician) Casio(I-politician) of(O) Bayan(B-political party) Muna(I-political party) ,(O) Rafael(B-politician) V.(I-politician) Mariano(I-politician) of(O) Anakpawis(B-political party) ,(O) and(O) Liza(B-politician) Maza(I-politician) and(O) Luzviminda(B-politician) Ilagan(I-politician) of(O) GABRIELA(B-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, country, event, election, organization, location, politician and O.\nSentence: After the 2007 elections , and the death of Anakpawis representative Crispin Beltran , Bayan now has five combined representatives in the 14th Congress of the Philippines , Satur Ocampo and Teodoro Casio of Bayan Muna , Rafael V. Mariano of Anakpawis , and Liza Maza and Luzviminda Ilagan of GABRIELA .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","the","2007","elections",",","and","the","death","of","Anakpawis","representative","Crispin","Beltran",",","Bayan","now","has","five","combined","representatives","in","the","14th","Congress","of","the","Philippines",",","Satur","Ocampo","and","Teodoro","Casio","of","Bayan","Muna",",","Rafael","V.","Mariano","of","Anakpawis",",","and","Liza","Maza","and","Luzviminda","Ilagan","of","GABRIELA","."],"labels":["O","O","B-election","I-election","O","O","O","O","O","B-political party","O","B-politician","I-politician","O","B-politician","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-politician","I-politician","O","B-politician","I-politician","O","B-political party","I-political party","O","B-politician","I-politician","I-politician","O","B-political party","O","O","B-politician","I-politician","O","B-politician","I-politician","O","B-political party","O"],"target_index":null,"target_label":null},"label_list":["person","political_party","country","event","election","organization","location","politician"]}
{"id":"177","dataset":"crossner_politics","split":"test","instance":{"id":"177","prompt_labels":"The(O) organization(O) has(O) local(O) chapters(O) in(O) Fredericton(B-location) ,(O) Montreal(B-location) ,(O) Ottawa(B-location) ,(O) Toronto(B-location) ,(O) Hamilton(B-location) ,(O) Edmonton(B-location) ,(O) Calgary(B-location) and(O) Vancouver(B-location) ,(O) plus(O) international(O) chapters(O) in(O) the(O) US(B-country) and(O) the(O) UK(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, political party, politician, person, organization, election, location and O.\nSentence: The organization has local chapters in Fredericton , Montreal , Ottawa , Toronto , Hamilton , Edmonton , Calgary and Vancouver , plus international chapters in the US and the UK .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","organization","has","local","chapters","in","Fredericton",",","Montreal",",","Ottawa",",","Toronto",",","Hamilton",",","Edmonton",",","Calgary","and","Vancouver",",","plus","international","chapters","in","the","US","and","the","UK","."],"labels":["O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","O","O","O","O","O","B-country","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["event","country","political_party","politician","person","organization","election","location"]}
{"id":"178","dataset":"crossner_politics","split":"test","instance":{"id":"178","prompt_labels":"Other(O) minor(O) parties(O) included(O) the(O) Green(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) which(O) ran(O) 79(O) candidates(O) ,(O) Libertarian(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) ,(O) the(O) Marxist-Leninist(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) and(O) the(O) Christian(B-political party) Heritage(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) ,(O) which(O) was(O) mainly(O) dedicated(O) to(O) opposing(O) abortion(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, election, organization, country, location, political party, politician and O.\nSentence: Other minor parties included the Green Party of Canada which ran 79 candidates , Libertarian Party of Canada , the Marxist-Leninist Party of Canada and the Christian Heritage Party of Canada , which was mainly dedicated to opposing abortion .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","minor","parties","included","the","Green","Party","of","Canada","which","ran","79","candidates",",","Libertarian","Party","of","Canada",",","the","Marxist-Leninist","Party","of","Canada","and","the","Christian","Heritage","Party","of","Canada",",","which","was","mainly","dedicated","to","opposing","abortion","."],"labels":["O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","person","election","organization","country","location","political_party","politician"]}
{"id":"182","dataset":"crossner_politics","split":"test","instance":{"id":"182","prompt_labels":"It(O) was(O) eclipsed(O) by(O) parties(O) like(O) the(O) ((O) relatively(O) moderate(O) )(O) National(B-political party) Peasants(I-political party) '(I-political party) Party(I-political party) and(O) its(O) more(O) radical(O) Romanian(B-political party) Front(I-political party) offshoot(O) ,(O) the(O) National-Christian(B-political party) Defense(I-political party) League(I-political party) ((O) LANC(B-political party) )(O) and(O) the(O) Iron(B-political party) Guard(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, politician, political party, country, location, election, event and O.\nSentence: It was eclipsed by parties like the ( relatively moderate ) National Peasants ' Party and its more radical Romanian Front offshoot , the National-Christian Defense League ( LANC ) and the Iron Guard .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","eclipsed","by","parties","like","the","(","relatively","moderate",")","National","Peasants","'","Party","and","its","more","radical","Romanian","Front","offshoot",",","the","National-Christian","Defense","League","(","LANC",")","and","the","Iron","Guard","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["person","organization","politician","political_party","country","location","election","event"]}
{"id":"186","dataset":"crossner_politics","split":"test","instance":{"id":"186","prompt_labels":"He(O) entered(O) the(O) British(B-organization) Diplomatic(I-organization) Service(I-organization) in(O) 1883(O) ,(O) and(O) served(O) in(O) minor(O) positions(O) at(O) embassies(O) in(O) Berlin(B-location) ,(O) Rome(B-location) ,(O) Athens(B-location) and(O) Paris(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, election, political party, event, politician, location, country, person and O.\nSentence: He entered the British Diplomatic Service in 1883 , and served in minor positions at embassies in Berlin , Rome , Athens and Paris .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","entered","the","British","Diplomatic","Service","in","1883",",","and","served","in","minor","positions","at","embassies","in","Berlin",",","Rome",",","Athens","and","Paris","."],"labels":["O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["organization","election","political_party","event","politician","location","country","person"]}
{"id":"187","dataset":"crossner_politics","split":"test","instance":{"id":"187","prompt_labels":"Some(O) parts(O) of(O) the(O) denomination(O) belong(O) to(O) the(O) National(B-organization) Association(I-organization) of(I-organization) Evangelicals(I-organization) ,(O) the(O) Canadian(B-organization) Council(I-organization) of(I-organization) Churches(I-organization) ,(O) and(O) the(O) Evangelical(B-organization) Fellowship(I-organization) of(I-organization) Canada(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, organization, politician, election, location, event, political party and O.\nSentence: Some parts of the denomination belong to the National Association of Evangelicals , the Canadian Council of Churches , and the Evangelical Fellowship of Canada .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","parts","of","the","denomination","belong","to","the","National","Association","of","Evangelicals",",","the","Canadian","Council","of","Churches",",","and","the","Evangelical","Fellowship","of","Canada","."],"labels":["O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["country","person","organization","politician","election","location","event","political_party"]}
{"id":"189","dataset":"crossner_politics","split":"test","instance":{"id":"189","prompt_labels":"In(O) Scotland(B-country) ,(O) Aberdeen(B-location) ,(O) Dundee(B-location) ,(O) Edinburgh(B-location) and(O) Glasgow(B-location) are(O) functionally(O) independent(O) cities(O) ,(O) though(O) the(O) term(O) is(O) not(O) used(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, organization, country, political party, person, event, location and O.\nSentence: In Scotland , Aberdeen , Dundee , Edinburgh and Glasgow are functionally independent cities , though the term is not used .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","Scotland",",","Aberdeen",",","Dundee",",","Edinburgh","and","Glasgow","are","functionally","independent","cities",",","though","the","term","is","not","used","."],"labels":["O","B-country","O","B-location","O","B-location","O","B-location","O","B-location","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["politician","election","organization","country","political_party","person","event","location"]}
{"id":"192","dataset":"crossner_politics","split":"test","instance":{"id":"192","prompt_labels":"ALP(B-political party) =(O) Australian(B-political party) Labor(I-political party) Party(I-political party) ,(O) L(O) +(O) NP(O) =(O) grouping(O) of(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) /(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) /(O) Liberal(B-political party) National(I-political party) Party(I-political party) of(I-political party) Queensland(I-political party) /(O) Country(B-political party) Liberal(I-political party) Party(I-political party) Coalition(O) parties(O) ((O) and(O) predecessors(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, country, election, person, politician, location, organization and O.\nSentence: ALP = Australian Labor Party , L + NP = grouping of Liberal Party of Australia / National Party of Australia / Liberal National Party of Queensland / Country Liberal Party Coalition parties ( and predecessors ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["ALP","=","Australian","Labor","Party",",","L","+","NP","=","grouping","of","Liberal","Party","of","Australia","/","National","Party","of","Australia","/","Liberal","National","Party","of","Queensland","/","Country","Liberal","Party","Coalition","parties","(","and","predecessors",")","."],"labels":["B-political party","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","event","country","election","person","politician","location","organization"]}
{"id":"194","dataset":"crossner_politics","split":"test","instance":{"id":"194","prompt_labels":"Throughout(O) the(O) 2000s(O) ((O) decade(O) )(O) ,(O) Thai(O) aggressively(O) continued(O) its(O) route(O) network(O) expansion(O) with(O) new(O) services(O) to(O) Chengdu(B-location) ,(O) Busan(B-location) ,(O) Chennai(B-location) ,(O) Xiamen(B-location) ,(O) Milan(B-location) ,(O) Moscow(B-location) ,(O) Islamabad(B-location) ,(O) Hyderabad(B-location) ,(O) Johannesburg(B-location) ((O) later(O) suspended(O) )(O) and(O) Oslo(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, election, politician, person, event, location, political party and O.\nSentence: Throughout the 2000s ( decade ) , Thai aggressively continued its route network expansion with new services to Chengdu , Busan , Chennai , Xiamen , Milan , Moscow , Islamabad , Hyderabad , Johannesburg ( later suspended ) and Oslo .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Throughout","the","2000s","(","decade",")",",","Thai","aggressively","continued","its","route","network","expansion","with","new","services","to","Chengdu",",","Busan",",","Chennai",",","Xiamen",",","Milan",",","Moscow",",","Islamabad",",","Hyderabad",",","Johannesburg","(","later","suspended",")","and","Oslo","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","O","O","O","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["country","organization","election","politician","person","event","location","political_party"]}
{"id":"196","dataset":"crossner_politics","split":"test","instance":{"id":"196","prompt_labels":"Previously(O) ,(O) in(O) Israel(B-country) ,(O) party(O) lists(O) had(O) been(O) decided(O) upon(O) by(O) the(O) parties(O) '(O) committees(O) ,(O) but(O) since(O) the(O) late(O) 1970s(O) ,(O) almost(O) every(O) party(O) in(O) Israel(B-country) ((O) with(O) the(O) exception(O) of(O) the(O) Ultra-orthodox(O) ones(O) ,(O) Shas(B-political party) and(O) United(B-political party) Torah(I-political party) Judaism(I-political party) ,(O) and(O) many(O) of(O) the(O) Centrist(B-political party) parties(I-political party) ;(O) Yesh(B-political party) Atid(I-political party) ,(O) Israel(B-political party) Resilience(I-political party) Party(I-political party) ,(O) Telem(B-political party) ,(O) and(O) Kulanu(B-political party) )(O) has(O) followed(O) Dash(B-politician) 's(O) lead(O) and(O) adopted(O) the(O) primaries(O) system(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, event, political party, country, location, organization, election and O.\nSentence: Previously , in Israel , party lists had been decided upon by the parties ' committees , but since the late 1970s , almost every party in Israel ( with the exception of the Ultra-orthodox ones , Shas and United Torah Judaism , and many of the Centrist parties ; Yesh Atid , Israel Resilience Party , Telem , and Kulanu ) has followed Dash 's lead and adopted the primaries system .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Previously",",","in","Israel",",","party","lists","had","been","decided","upon","by","the","parties","'","committees",",","but","since","the","late","1970s",",","almost","every","party","in","Israel","(","with","the","exception","of","the","Ultra-orthodox","ones",",","Shas","and","United","Torah","Judaism",",","and","many","of","the","Centrist","parties",";","Yesh","Atid",",","Israel","Resilience","Party",",","Telem",",","and","Kulanu",")","has","followed","Dash","'s","lead","and","adopted","the","primaries","system","."],"labels":["O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","B-political party","O","B-political party","I-political party","I-political party","O","O","O","O","O","B-political party","I-political party","O","B-political party","I-political party","O","B-political party","I-political party","I-political party","O","B-political party","O","O","B-political party","O","O","O","B-politician","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","politician","event","political_party","country","location","organization","election"]}
{"id":"198","dataset":"crossner_politics","split":"test","instance":{"id":"198","prompt_labels":"It(O) operates(O) four(O) channels(O) :(O) TVR1(O) ,(O) TVR2(O) ,(O) TVR3(O) and(O) TVRi(O) ,(O) along(O) with(O) six(O) regional(O) studios(O) in(O) Bucharest(B-location) ,(O) Cluj-Napoca(B-location) ,(O) Iai(B-location) ,(O) Timioara(B-location) ,(O) Craiova(B-location) and(O) Trgu(B-location) Mure(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, organization, event, election, political party, politician, country and O.\nSentence: It operates four channels : TVR1 , TVR2 , TVR3 and TVRi , along with six regional studios in Bucharest , Cluj-Napoca , Iai , Timioara , Craiova and Trgu Mure .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","operates","four","channels",":","TVR1",",","TVR2",",","TVR3","and","TVRi",",","along","with","six","regional","studios","in","Bucharest",",","Cluj-Napoca",",","Iai",",","Timioara",",","Craiova","and","Trgu","Mure","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["location","person","organization","event","election","political_party","politician","country"]}
{"id":"199","dataset":"crossner_politics","split":"test","instance":{"id":"199","prompt_labels":"Later(O) ,(O) the(O) Rainbow(B-event) Coalition(I-event) was(O) joined(O) nationwide(O) by(O) Students(B-organization) for(I-organization) a(I-organization) Democratic(I-organization) Society(I-organization) ((O) SDS(B-organization) )(O) ,(O) the(O) Brown(B-organization) Berets(I-organization) ,(O) American(B-political party) Indian(I-political party) Movement(I-political party) ,(O) and(O) the(O) Red(B-political party) Guard(I-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, politician, election, location, country, person, political party and O.\nSentence: Later , the Rainbow Coalition was joined nationwide by Students for a Democratic Society ( SDS ) , the Brown Berets , American Indian Movement , and the Red Guard Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Later",",","the","Rainbow","Coalition","was","joined","nationwide","by","Students","for","a","Democratic","Society","(","SDS",")",",","the","Brown","Berets",",","American","Indian","Movement",",","and","the","Red","Guard","Party","."],"labels":["O","O","O","B-event","I-event","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","B-organization","I-organization","O","B-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["organization","event","politician","election","location","country","person","political_party"]}
{"id":"200","dataset":"crossner_politics","split":"test","instance":{"id":"200","prompt_labels":"It(O) also(O) became(O) a(O) member(O) of(O) the(O) March(B-political party) 14(I-political party) Alliance(I-political party) ,(O) along(O) with(O) the(O) Future(B-political party) Movement(I-political party) ,(O) Progressive(B-political party) Socialist(I-political party) Party(I-political party) ,(O) Lebanese(B-political party) Forces(I-political party) and(O) other(O) minor(O) parties(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, politician, country, event, political party, election, organization and O.\nSentence: It also became a member of the March 14 Alliance , along with the Future Movement , Progressive Socialist Party , Lebanese Forces and other minor parties .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","also","became","a","member","of","the","March","14","Alliance",",","along","with","the","Future","Movement",",","Progressive","Socialist","Party",",","Lebanese","Forces","and","other","minor","parties","."],"labels":["O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","person","politician","country","event","political_party","election","organization"]}
{"id":"202","dataset":"crossner_politics","split":"test","instance":{"id":"202","prompt_labels":"Chandrika(B-politician) Kumaratunga(I-politician) briefly(O) took(O) over(O) the(O) leadership(O) of(O) her(O) husband(O) 's(O) party(O) ,(O) and(O) formed(O) the(O) United(B-political party) Socialist(I-political party) Alliance(I-political party) with(O) the(O) Communist(B-political party) Party(I-political party) of(I-political party) Sri(I-political party) Lanka(I-political party) ,(O) the(O) Lanka(B-political party) Sama(I-political party) Samaja(I-political party) Party(I-political party) ,(O) and(O) the(O) Nava(B-political party) Sama(I-political party) Samaja(I-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, organization, person, country, politician, political party, election and O.\nSentence: Chandrika Kumaratunga briefly took over the leadership of her husband 's party , and formed the United Socialist Alliance with the Communist Party of Sri Lanka , the Lanka Sama Samaja Party , and the Nava Sama Samaja Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Chandrika","Kumaratunga","briefly","took","over","the","leadership","of","her","husband","'s","party",",","and","formed","the","United","Socialist","Alliance","with","the","Communist","Party","of","Sri","Lanka",",","the","Lanka","Sama","Samaja","Party",",","and","the","Nava","Sama","Samaja","Party","."],"labels":["B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["event","location","organization","person","country","politician","political_party","election"]}
{"id":"205","dataset":"crossner_politics","split":"test","instance":{"id":"205","prompt_labels":"However(O) ,(O) there(O) are(O) other(O) small(O) conservative(O) parties(O) with(O) which(O) the(O) federal(O) Conservative(B-political party) Party(I-political party) has(O) close(O) ties(O) ,(O) such(O) as(O) the(O) Saskatchewan(B-political party) Party(I-political party) and(O) the(O) British(B-political party) Columbia(I-political party) Liberal(I-political party) Party(I-political party) ((O) not(O) associated(O) with(O) the(O) federal(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) despite(O) its(O) name(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, person, location, political party, election, country, event and O.\nSentence: However , there are other small conservative parties with which the federal Conservative Party has close ties , such as the Saskatchewan Party and the British Columbia Liberal Party ( not associated with the federal Liberal Party of Canada despite its name ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["However",",","there","are","other","small","conservative","parties","with","which","the","federal","Conservative","Party","has","close","ties",",","such","as","the","Saskatchewan","Party","and","the","British","Columbia","Liberal","Party","(","not","associated","with","the","federal","Liberal","Party","of","Canada","despite","its","name",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","O","O","B-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","politician","person","location","political_party","election","country","event"]}
{"id":"206","dataset":"crossner_politics","split":"test","instance":{"id":"206","prompt_labels":"In(O) Alberta(B-location) ,(O) relations(O) became(O) strained(O) between(O) the(O) federal(O) Conservative(O) Party(O) and(O) the(O) Progressive(B-political party) Conservative(I-political party) Association(I-political party) of(I-political party) Alberta(I-political party) prior(O) to(O) its(O) provincial(O) loss(O) in(O) 2015(B-election) Alberta(I-election) general(I-election) election(I-election) and(O) eventually(O) emergence(O) with(O) the(O) Wildrose(B-political party) Party(I-political party) into(O) the(O) United(B-political party) Conservative(I-political party) Party(I-political party) in(O) 2017(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, organization, person, location, political party, event, politician and O.\nSentence: In Alberta , relations became strained between the federal Conservative Party and the Progressive Conservative Association of Alberta prior to its provincial loss in 2015 Alberta general election and eventually emergence with the Wildrose Party into the United Conservative Party in 2017 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","Alberta",",","relations","became","strained","between","the","federal","Conservative","Party","and","the","Progressive","Conservative","Association","of","Alberta","prior","to","its","provincial","loss","in","2015","Alberta","general","election","and","eventually","emergence","with","the","Wildrose","Party","into","the","United","Conservative","Party","in","2017","."],"labels":["O","B-location","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","B-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","election","organization","person","location","political_party","event","politician"]}
{"id":"211","dataset":"crossner_politics","split":"test","instance":{"id":"211","prompt_labels":"For(O) four(O) decades(O) ,(O) the(O) party(O) dominated(O) the(O) British(B-location) Columbian(I-location) political(O) scene(O) ,(O) with(O) the(O) only(O) break(O) occurring(O) between(O) the(O) 1972(B-election) British(I-election) Columbia(I-election) general(I-election) election(I-election) and(O) 1975(B-election) British(I-election) Columbia(I-election) general(I-election) election(I-election) when(O) the(O) British(B-political party) Columbia(I-political party) New(I-political party) Democratic(I-political party) Party(I-political party) governed(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, political party, politician, country, organization, election, location and O.\nSentence: For four decades , the party dominated the British Columbian political scene , with the only break occurring between the 1972 British Columbia general election and 1975 British Columbia general election when the British Columbia New Democratic Party governed .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","four","decades",",","the","party","dominated","the","British","Columbian","political","scene",",","with","the","only","break","occurring","between","the","1972","British","Columbia","general","election","and","1975","British","Columbia","general","election","when","the","British","Columbia","New","Democratic","Party","governed","."],"labels":["O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O"],"target_index":null,"target_label":null},"label_list":["event","person","political_party","politician","country","organization","election","location"]}
{"id":"212","dataset":"crossner_politics","split":"test","instance":{"id":"212","prompt_labels":"For(O) the(O) 1952(B-election) British(I-election) Columbia(I-election) general(I-election) election(I-election) ,(O) the(O) coalition(O) government(O) between(O) the(O) British(B-political party) Columbia(I-political party) Liberal(I-political party) Party(I-political party) and(O) British(B-political party) Columbia(I-political party) Conservative(I-political party) Party(I-political party) reformed(O) the(O) electoral(O) system(O) from(O) first(O) past(O) the(O) post(O) to(O) the(O) alternative(O) vote(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, political party, election, country, person, politician, event and O.\nSentence: For the 1952 British Columbia general election , the coalition government between the British Columbia Liberal Party and British Columbia Conservative Party reformed the electoral system from first past the post to the alternative vote .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","the","1952","British","Columbia","general","election",",","the","coalition","government","between","the","British","Columbia","Liberal","Party","and","British","Columbia","Conservative","Party","reformed","the","electoral","system","from","first","past","the","post","to","the","alternative","vote","."],"labels":["O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","location","political_party","election","country","person","politician","event"]}
{"id":"213","dataset":"crossner_politics","split":"test","instance":{"id":"213","prompt_labels":"He(O) quickly(O) became(O) active(O) in(O) the(O) party(O) ,(O) making(O) two(O) sacrificial-lamb(O) bids(O) for(O) Parliament(O) against(O) entrenched(O) but(O) vulnerable(O) New(B-political party) Zealand(I-political party) Labour(I-political party) Party(I-political party) incumbents(O) in(O) 1954(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ((O) Mount(B-location) Albert(I-location) )(O) and(O) 1957(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ((O) Waitemata(B-location) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, country, election, organization, event, politician, political party and O.\nSentence: He quickly became active in the party , making two sacrificial-lamb bids for Parliament against entrenched but vulnerable New Zealand Labour Party incumbents in 1954 New Zealand general election ( Mount Albert ) and 1957 New Zealand general election ( Waitemata ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","quickly","became","active","in","the","party",",","making","two","sacrificial-lamb","bids","for","Parliament","against","entrenched","but","vulnerable","New","Zealand","Labour","Party","incumbents","in","1954","New","Zealand","general","election","(","Mount","Albert",")","and","1957","New","Zealand","general","election","(","Waitemata",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-election","I-election","I-election","I-election","I-election","O","B-location","I-location","O","O","B-election","I-election","I-election","I-election","I-election","O","B-location","O","O"],"target_index":null,"target_label":null},"label_list":["location","person","country","election","organization","event","politician","political_party"]}
{"id":"214","dataset":"crossner_politics","split":"test","instance":{"id":"214","prompt_labels":"Spivak(B-person) has(O) received(O) 11(O) honorary(O) doctorates(O) from(O) the(O) University(B-organization) of(I-organization) Toronto(I-organization) ,(O) University(B-organization) of(I-organization) London(I-organization) ,(O) Oberlin(B-organization) College(I-organization) ,(O) Rovira(B-organization) i(I-organization) Virgili(I-organization) University(I-organization) ,(O) Rabindra(B-organization) Bharati(I-organization) University(I-organization) ,(O) National(B-organization) University(I-organization) of(I-organization) General(I-organization) San(I-organization) Martn(I-organization) ,(O) University(B-organization) of(I-organization) St(I-organization) Andrews(I-organization) ,(O) Universit(B-organization) de(I-organization) Vincennes(I-organization) (I-organization) Saint-Denis(I-organization) ,(O) Presidency(B-organization) University(I-organization) ,(O) Yale(B-organization) University(I-organization) ,(O) and(O) University(B-organization) of(I-organization) Ghana(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, person, event, politician, location, country, political party and O.\nSentence: Spivak has received 11 honorary doctorates from the University of Toronto , University of London , Oberlin College , Rovira i Virgili University , Rabindra Bharati University , National University of General San Martn , University of St Andrews , Universit de Vincennes  Saint-Denis , Presidency University , Yale University , and University of Ghana .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Spivak","has","received","11","honorary","doctorates","from","the","University","of","Toronto",",","University","of","London",",","Oberlin","College",",","Rovira","i","Virgili","University",",","Rabindra","Bharati","University",",","National","University","of","General","San","Martn",",","University","of","St","Andrews",",","Universit","de","Vincennes","","Saint-Denis",",","Presidency","University",",","Yale","University",",","and","University","of","Ghana","."],"labels":["B-person","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["election","organization","person","event","politician","location","country","political_party"]}
{"id":"216","dataset":"crossner_politics","split":"test","instance":{"id":"216","prompt_labels":"The(O) commission(O) set(O) up(O) eight(O) regional(O) offices(O) in(O) Jalalabad(B-location) ,(O) Herat(B-location) ,(O) Kunduz(B-location) ,(O) Kabul(B-location) ,(O) Gardez(B-location) ,(O) Kandahar(B-location) ,(O) Mazar-i-Sharif(B-location) and(O) Bamyan(B-location) as(O) well(O) as(O) in(O) the(O) Pakistan(B-country) i(O) cities(O) of(O) Peshawar(B-location) and(O) Quetta(B-location) ,(O) and(O) in(O) the(O) Iran(B-country) ian(O) cities(O) of(O) Tehran(B-location) and(O) Mashhad(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, person, election, politician, political party, organization, location and O.\nSentence: The commission set up eight regional offices in Jalalabad , Herat , Kunduz , Kabul , Gardez , Kandahar , Mazar-i-Sharif and Bamyan as well as in the Pakistan i cities of Peshawar and Quetta , and in the Iran ian cities of Tehran and Mashhad .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","commission","set","up","eight","regional","offices","in","Jalalabad",",","Herat",",","Kunduz",",","Kabul",",","Gardez",",","Kandahar",",","Mazar-i-Sharif","and","Bamyan","as","well","as","in","the","Pakistan","i","cities","of","Peshawar","and","Quetta",",","and","in","the","Iran","ian","cities","of","Tehran","and","Mashhad","."],"labels":["O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","O","O","O","O","B-country","O","O","O","B-location","O","B-location","O","O","O","O","B-country","O","O","O","B-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["country","event","person","election","politician","political_party","organization","location"]}
{"id":"217","dataset":"crossner_politics","split":"test","instance":{"id":"217","prompt_labels":"The(O) NDP(B-political party) focused(O) the(O) campaign(O) on(O) winning(O) ridings(O) in(O) Canada(B-country) 's(O) urban(O) centres(O) ,(O) hoping(O) especially(O) to(O) win(O) seats(O) in(O) central(O) Toronto(B-location) ,(O) Hamilton(B-location) ,(O) Ottawa(B-location) and(O) Winnipeg(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, organization, election, person, location, event, country and O.\nSentence: The NDP focused the campaign on winning ridings in Canada 's urban centres , hoping especially to win seats in central Toronto , Hamilton , Ottawa and Winnipeg .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","NDP","focused","the","campaign","on","winning","ridings","in","Canada","'s","urban","centres",",","hoping","especially","to","win","seats","in","central","Toronto",",","Hamilton",",","Ottawa","and","Winnipeg","."],"labels":["O","B-political party","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["political_party","politician","organization","election","person","location","event","country"]}
{"id":"219","dataset":"crossner_politics","split":"test","instance":{"id":"219","prompt_labels":"In(O) addition(O) ,(O) students(O) from(O) Santa(B-organization) Ana(I-organization) College(I-organization) ,(O) Santiago(B-organization) Canyon(I-organization) College(I-organization) ,(O) Fullerton(B-organization) College(I-organization) ,(O) and(O) Golden(B-organization) West(I-organization) College(I-organization) can(O) local(O) fixed(O) route(O) buses(O) for(O) free(O) using(O) their(O) student(O) ID(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, event, political party, country, election, location, organization and O.\nSentence: In addition , students from Santa Ana College , Santiago Canyon College , Fullerton College , and Golden West College can local fixed route buses for free using their student ID .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","addition",",","students","from","Santa","Ana","College",",","Santiago","Canyon","College",",","Fullerton","College",",","and","Golden","West","College","can","local","fixed","route","buses","for","free","using","their","student","ID","."],"labels":["O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","politician","event","political_party","country","election","location","organization"]}
{"id":"220","dataset":"crossner_politics","split":"test","instance":{"id":"220","prompt_labels":"At(O) the(O) age(O) of(O) 16(O) ,(O) he(O) graduated(O) from(O) high(O) school(O) in(O) Husum(B-location) and(O) later(O) studied(O) at(O) the(O) universities(O) of(O) University(B-organization) of(I-organization) Jena(I-organization) ,(O) University(B-organization) of(I-organization) Bonn(I-organization) ,(O) Leipzig(B-location) ,(O) Berlin(B-location) ,(O) and(O) University(B-organization) of(I-organization) Tbingen(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, political party, politician, person, country, event, election and O.\nSentence: At the age of 16 , he graduated from high school in Husum and later studied at the universities of University of Jena , University of Bonn , Leipzig , Berlin , and University of Tbingen .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["At","the","age","of","16",",","he","graduated","from","high","school","in","Husum","and","later","studied","at","the","universities","of","University","of","Jena",",","University","of","Bonn",",","Leipzig",",","Berlin",",","and","University","of","Tbingen","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-location","O","B-location","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["organization","location","political_party","politician","person","country","event","election"]}
{"id":"221","dataset":"crossner_politics","split":"test","instance":{"id":"221","prompt_labels":"He(O) served(O) twice(O) as(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) ,(O) from(O) 2008(B-election) Liberal(I-election) Party(I-election) of(I-election) Australia(I-election) leadership(I-election) spill(I-election) to(O) 2009(B-election) Liberal(I-election) Party(I-election) of(I-election) Australia(I-election) leadership(I-election) spill(I-election) when(O) he(O) was(O) Leader(O) of(O) the(O) Opposition(B-political party) ,(O) and(O) from(O) 2015(O) to(O) 2018(O) when(O) he(O) was(O) Prime(O) Minister(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, political party, location, election, event, country, politician and O.\nSentence: He served twice as Liberal Party of Australia , from 2008 Liberal Party of Australia leadership spill to 2009 Liberal Party of Australia leadership spill when he was Leader of the Opposition , and from 2015 to 2018 when he was Prime Minister .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","served","twice","as","Liberal","Party","of","Australia",",","from","2008","Liberal","Party","of","Australia","leadership","spill","to","2009","Liberal","Party","of","Australia","leadership","spill","when","he","was","Leader","of","the","Opposition",",","and","from","2015","to","2018","when","he","was","Prime","Minister","."],"labels":["O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","organization","political_party","location","election","event","country","politician"]}
{"id":"222","dataset":"crossner_politics","split":"test","instance":{"id":"222","prompt_labels":"Other(O) controversial(O) municipal(O) amalgamations(O) took(O) place(O) during(O) Harris(B-location) '(O) second(O) term(O) ,(O) including(O) in(O) Ottawa(B-location) ,(O) Hamilton(B-location) ,(O) Greater(B-location) Sudbury(I-location) and(O) Kawartha(B-location) Lakes(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, event, person, political party, country, location, organization, election and O.\nSentence: Other controversial municipal amalgamations took place during Harris ' second term , including in Ottawa , Hamilton , Greater Sudbury and Kawartha Lakes .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","controversial","municipal","amalgamations","took","place","during","Harris","'","second","term",",","including","in","Ottawa",",","Hamilton",",","Greater","Sudbury","and","Kawartha","Lakes","."],"labels":["O","O","O","O","O","O","O","B-location","O","O","O","O","O","O","B-location","O","B-location","O","B-location","I-location","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["politician","event","person","political_party","country","location","organization","election"]}
{"id":"223","dataset":"crossner_politics","split":"test","instance":{"id":"223","prompt_labels":"He(O) was(O) re-elected(O) in(O) the(O) elections(O) of(O) 1966(B-election) Manitoba(I-election) general(I-election) election(I-election) ,(O) 1969(B-election) Manitoba(I-election) general(I-election) election(I-election) ,(O) 1973(B-election) Manitoba(I-election) general(I-election) election(I-election) and(O) 1977(B-election) Manitoba(I-election) general(I-election) election(I-election) ,(O) each(O) time(O) by(O) a(O) significant(O) margin(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, politician, political party, election, location, organization, event and O.\nSentence: He was re-elected in the elections of 1966 Manitoba general election , 1969 Manitoba general election , 1973 Manitoba general election and 1977 Manitoba general election , each time by a significant margin .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","re-elected","in","the","elections","of","1966","Manitoba","general","election",",","1969","Manitoba","general","election",",","1973","Manitoba","general","election","and","1977","Manitoba","general","election",",","each","time","by","a","significant","margin","."],"labels":["O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","country","politician","political_party","election","location","organization","event"]}
{"id":"224","dataset":"crossner_politics","split":"test","instance":{"id":"224","prompt_labels":"The(O) Vietnamese(O) immigration(O) pattern(O) has(O) shifted(O) to(O) other(O) states(O) ,(O) including(O) Denver(B-location) ,(O) Boston(B-location) ,(O) Chicago(B-location) ,(O) Oklahoma(B-location) ((O) Oklahoma(B-location) City(I-location) and(O) Tulsa(B-location) in(O) particular(O) )(O) and(O) Oregon(B-location) ((O) Portland(B-location) in(O) particular(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, event, politician, organization, election, political party, country and O.\nSentence: The Vietnamese immigration pattern has shifted to other states , including Denver , Boston , Chicago , Oklahoma ( Oklahoma City and Tulsa in particular ) and Oregon ( Portland in particular ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Vietnamese","immigration","pattern","has","shifted","to","other","states",",","including","Denver",",","Boston",",","Chicago",",","Oklahoma","(","Oklahoma","City","and","Tulsa","in","particular",")","and","Oregon","(","Portland","in","particular",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","I-location","O","B-location","O","O","O","O","B-location","O","B-location","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","location","event","politician","organization","election","political_party","country"]}
{"id":"226","dataset":"crossner_politics","split":"test","instance":{"id":"226","prompt_labels":"Shalala(B-person) has(O) been(O) elected(O) to(O) the(O) Council(B-organization) on(I-organization) Foreign(I-organization) Relations(I-organization) ;(O) National(B-organization) Academy(I-organization) of(I-organization) Education(I-organization) ;(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Public(I-organization) Administration(I-organization) ;(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) ;(O) the(O) American(B-organization) Philosophical(I-organization) Society(I-organization) ;(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Social(I-organization) Insurance(I-organization) ;(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Political(I-organization) and(I-organization) Social(I-organization) Science(I-organization) ;(O) and(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Medicine(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, organization, election, political party, location, politician, person and O.\nSentence: Shalala has been elected to the Council on Foreign Relations ; National Academy of Education ; the National Academy of Public Administration ; the American Academy of Arts and Sciences ; the American Philosophical Society ; the National Academy of Social Insurance ; the American Academy of Political and Social Science ; and the National Academy of Medicine .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Shalala","has","been","elected","to","the","Council","on","Foreign","Relations",";","National","Academy","of","Education",";","the","National","Academy","of","Public","Administration",";","the","American","Academy","of","Arts","and","Sciences",";","the","American","Philosophical","Society",";","the","National","Academy","of","Social","Insurance",";","the","American","Academy","of","Political","and","Social","Science",";","and","the","National","Academy","of","Medicine","."],"labels":["B-person","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["event","country","organization","election","political_party","location","politician","person"]}
{"id":"227","dataset":"crossner_politics","split":"test","instance":{"id":"227","prompt_labels":"He(O) resigned(O) from(O) the(O) governorship(O) to(O) briefly(O) co-lead(O) the(O) Sunrise(B-political party) Party(I-political party) ,(O) then(O) joined(O) the(O) Japan(B-political party) Restoration(I-political party) Party(I-political party) and(O) returned(O) to(O) the(O) House(B-organization) of(I-organization) Representatives(I-organization) in(O) the(O) 2012(B-election) Japanese(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, location, organization, political party, politician, event, election and O.\nSentence: He resigned from the governorship to briefly co-lead the Sunrise Party , then joined the Japan Restoration Party and returned to the House of Representatives in the 2012 Japanese general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","resigned","from","the","governorship","to","briefly","co-lead","the","Sunrise","Party",",","then","joined","the","Japan","Restoration","Party","and","returned","to","the","House","of","Representatives","in","the","2012","Japanese","general","election","."],"labels":["O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["person","country","location","organization","political_party","politician","event","election"]}
{"id":"236","dataset":"crossner_politics","split":"test","instance":{"id":"236","prompt_labels":"Gazprom(B-country) is(O) listed(O) on(O) the(O) stock(O) markets(O) of(O) Moscow(B-location) ,(O) London(B-location) ,(O) Karachi(B-location) ,(O) Berlin(B-location) ,(O) Frankfurt(B-location) and(O) Singapore(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, organization, politician, election, political party, location, event and O.\nSentence: Gazprom is listed on the stock markets of Moscow , London , Karachi , Berlin , Frankfurt and Singapore .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Gazprom","is","listed","on","the","stock","markets","of","Moscow",",","London",",","Karachi",",","Berlin",",","Frankfurt","and","Singapore","."],"labels":["B-country","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["country","person","organization","politician","election","political_party","location","event"]}
{"id":"237","dataset":"crossner_politics","split":"test","instance":{"id":"237","prompt_labels":"Davies(B-politician) was(O) first(O) elected(O) to(O) parliament(O) in(O) 1997(B-election) Canadian(I-election) federal(I-election) election(I-election) and(O) re-elected(O) in(O) 2000(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2004(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2006(B-election) Canadian(I-election) federal(I-election) election(I-election) and(O) 2008(B-election) Canadian(I-election) federal(I-election) election(I-election) and(O) 2011(B-election) Canadian(I-election) federal(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, politician, location, person, event, organization, political party and O.\nSentence: Davies was first elected to parliament in 1997 Canadian federal election and re-elected in 2000 Canadian federal election , 2004 Canadian federal election , 2006 Canadian federal election and 2008 Canadian federal election and 2011 Canadian federal election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Davies","was","first","elected","to","parliament","in","1997","Canadian","federal","election","and","re-elected","in","2000","Canadian","federal","election",",","2004","Canadian","federal","election",",","2006","Canadian","federal","election","and","2008","Canadian","federal","election","and","2011","Canadian","federal","election","."],"labels":["B-politician","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["country","election","politician","location","person","event","organization","political_party"]}
{"id":"238","dataset":"crossner_politics","split":"test","instance":{"id":"238","prompt_labels":"The(O) Workers(B-political party) '(I-political party) Party(I-political party) had(O) its(O) best(O) performance(O) at(O) the(O) polls(O) in(O) 1989(O) when(O) it(O) won(O) seven(O) seats(O) in(O) the(O) 1989(B-election) Irish(I-election) general(I-election) election(I-election) and(O) party(O) president(O) Proinsias(B-politician) De(I-politician) Rossa(I-politician) won(O) a(O) seat(O) in(O) Dublin(B-location) in(O) the(O) 1989(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) Ireland(I-election) held(O) on(O) the(O) same(O) day(O) ,(O) sitting(O) with(O) the(O) communist(O) Left(B-political party) Unity(I-political party) group(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, political party, location, election, organization, event, politician and O.\nSentence: The Workers ' Party had its best performance at the polls in 1989 when it won seven seats in the 1989 Irish general election and party president Proinsias De Rossa won a seat in Dublin in the 1989 European Parliament election in Ireland held on the same day , sitting with the communist Left Unity group .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Workers","'","Party","had","its","best","performance","at","the","polls","in","1989","when","it","won","seven","seats","in","the","1989","Irish","general","election","and","party","president","Proinsias","De","Rossa","won","a","seat","in","Dublin","in","the","1989","European","Parliament","election","in","Ireland","held","on","the","same","day",",","sitting","with","the","communist","Left","Unity","group","."],"labels":["O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","B-politician","I-politician","I-politician","O","O","O","O","B-location","O","O","B-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O"],"target_index":null,"target_label":null},"label_list":["country","person","political_party","location","election","organization","event","politician"]}
{"id":"239","dataset":"crossner_politics","split":"test","instance":{"id":"239","prompt_labels":"Attempts(O) to(O) secure(O) its(O) operation(O) on(O) a(O) permanent(O) basis(O) had(O) been(O) frustrated(O) by(O) disagreements(O) between(O) the(O) two(O) main(O) unionist(O) parties(O) ((O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) ((O) DUP(B-political party) )(O) and(O) the(O) Ulster(B-political party) Unionist(I-political party) Party(I-political party) )(O) and(O) Sinn(B-political party) Fin(I-political party) ,(O) the(O) largest(O) nationalist(O) party(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, political party, person, politician, event, election, organization and O.\nSentence: Attempts to secure its operation on a permanent basis had been frustrated by disagreements between the two main unionist parties ( the Democratic Unionist Party ( DUP ) and the Ulster Unionist Party ) and Sinn Fin , the largest nationalist party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Attempts","to","secure","its","operation","on","a","permanent","basis","had","been","frustrated","by","disagreements","between","the","two","main","unionist","parties","(","the","Democratic","Unionist","Party","(","DUP",")","and","the","Ulster","Unionist","Party",")","and","Sinn","Fin",",","the","largest","nationalist","party","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","country","political_party","person","politician","event","election","organization"]}
{"id":"240","dataset":"crossner_politics","split":"test","instance":{"id":"240","prompt_labels":"At(O) the(O) 2003(O) election(O) ,(O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) and(O) Sinn(B-political party) Fin(I-political party) displaced(O) the(O) more(O) moderate(O) Ulster(B-political party) Unionist(I-political party) Party(I-political party) and(O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) as(O) the(O) largest(O) parties(O) in(O) the(O) unionist(O) and(O) nationalist(O) blocks(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, organization, political party, election, politician, event, country and O.\nSentence: At the 2003 election , the Democratic Unionist Party and Sinn Fin displaced the more moderate Ulster Unionist Party and Social Democratic and Labour Party as the largest parties in the unionist and nationalist blocks .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["At","the","2003","election",",","the","Democratic","Unionist","Party","and","Sinn","Fin","displaced","the","more","moderate","Ulster","Unionist","Party","and","Social","Democratic","and","Labour","Party","as","the","largest","parties","in","the","unionist","and","nationalist","blocks","."],"labels":["O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","location","organization","political_party","election","politician","event","country"]}
{"id":"243","dataset":"crossner_politics","split":"test","instance":{"id":"243","prompt_labels":"During(O) campaigning(O) for(O) the(O) 1978(B-election) French(I-election) legislative(I-election) election(I-election) ,(O) in(O) his(O) Verdun-sur-le-Doubs(B-location) speech(O) ,(O) President(O) Giscard(B-politician) d(I-politician) 'Estaing(I-politician) noted(O) that(O) the(O) political(O) leanings(O) of(O) the(O) French(O) people(O) were(O) divided(O) among(O) four(O) groups(O) :(O) the(O) Communists(O) ((O) French(B-political party) Communist(I-political party) Party(I-political party) )(O) ,(O) the(O) Socialists(O) ((O) PS(O) )(O) ,(O) the(O) Neo-Gaullists(O) ((O) Rally(B-political party) for(I-political party) the(I-political party) Republic(I-political party) )(O) and(O) his(O) own(O) followers(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, politician, event, location, person, country, organization and O.\nSentence: During campaigning for the 1978 French legislative election , in his Verdun-sur-le-Doubs speech , President Giscard d 'Estaing noted that the political leanings of the French people were divided among four groups : the Communists ( French Communist Party ) , the Socialists ( PS ) , the Neo-Gaullists ( Rally for the Republic ) and his own followers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","campaigning","for","the","1978","French","legislative","election",",","in","his","Verdun-sur-le-Doubs","speech",",","President","Giscard","d","'Estaing","noted","that","the","political","leanings","of","the","French","people","were","divided","among","four","groups",":","the","Communists","(","French","Communist","Party",")",",","the","Socialists","(","PS",")",",","the","Neo-Gaullists","(","Rally","for","the","Republic",")","and","his","own","followers","."],"labels":["O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","B-location","O","O","O","B-politician","I-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","political_party","politician","event","location","person","country","organization"]}
{"id":"244","dataset":"crossner_politics","split":"test","instance":{"id":"244","prompt_labels":"She(O) has(O) been(O) a(O) frequent(O) candidate(O) for(O) the(O) French(O) presidency(O) ,(O) starting(O) with(O) 1974(B-election) French(I-election) presidential(I-election) election(I-election) ,(O) and(O) continuing(O) through(O) those(O) of(O) 1981(B-election) French(I-election) presidential(I-election) election(I-election) ,(O) 1988(B-election) French(I-election) presidential(I-election) election(I-election) ,(O) 1995(B-election) French(I-election) presidential(I-election) election(I-election) ,(O) 2002(B-election) French(I-election) presidential(I-election) election(I-election) ,(O) and(O) 2007(B-election) French(I-election) presidential(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, politician, country, location, election, event, political party and O.\nSentence: She has been a frequent candidate for the French presidency , starting with 1974 French presidential election , and continuing through those of 1981 French presidential election , 1988 French presidential election , 1995 French presidential election , 2002 French presidential election , and 2007 French presidential election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","has","been","a","frequent","candidate","for","the","French","presidency",",","starting","with","1974","French","presidential","election",",","and","continuing","through","those","of","1981","French","presidential","election",",","1988","French","presidential","election",",","1995","French","presidential","election",",","2002","French","presidential","election",",","and","2007","French","presidential","election","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["organization","person","politician","country","location","election","event","political_party"]}
{"id":"247","dataset":"crossner_politics","split":"test","instance":{"id":"247","prompt_labels":"Tyndall(B-politician) was(O) to(O) be(O) the(O) leader(O) of(O) this(O) new(O) party(O) ,(O) with(O) the(O) majority(O) of(O) its(O) members(O) coming(O) from(O) the(O) NNF(B-political party) ,(O) although(O) others(O) were(O) defectors(O) from(O) the(O) NF(B-political party) ,(O) British(B-political party) Movement(I-political party) ,(O) British(B-political party) Democratic(I-political party) Party(I-political party) ,(O) and(O) Constitutional(B-political party) Movement(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, country, person, location, politician, election, political party and O.\nSentence: Tyndall was to be the leader of this new party , with the majority of its members coming from the NNF , although others were defectors from the NF , British Movement , British Democratic Party , and Constitutional Movement .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Tyndall","was","to","be","the","leader","of","this","new","party",",","with","the","majority","of","its","members","coming","from","the","NNF",",","although","others","were","defectors","from","the","NF",",","British","Movement",",","British","Democratic","Party",",","and","Constitutional","Movement","."],"labels":["B-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","O","O","O","O","O","O","O","B-political party","O","B-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["event","organization","country","person","location","politician","election","political_party"]}
{"id":"249","dataset":"crossner_politics","split":"test","instance":{"id":"249","prompt_labels":"He(O) first(O) defeated(O) three-term(O) Republican(O) incumbent(O) Al(B-politician) D(I-politician) 'Amato(I-politician) before(O) being(O) reelected(O) in(O) 2004(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) New(I-election) York(I-election) with(O) 71(O) percent(O) of(O) the(O) vote(O) ,(O) in(O) 2010(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) New(I-election) York(I-election) with(O) 66(O) percent(O) of(O) the(O) vote(O) ,(O) and(O) in(O) 2016(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) New(I-election) York(I-election) with(O) 70(O) percent(O) of(O) the(O) vote(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, politician, country, event, election, location, person and O.\nSentence: He first defeated three-term Republican incumbent Al D 'Amato before being reelected in 2004 United States Senate election in New York with 71 percent of the vote , in 2010 United States Senate election in New York with 66 percent of the vote , and in 2016 United States Senate election in New York with 70 percent of the vote .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","first","defeated","three-term","Republican","incumbent","Al","D","'Amato","before","being","reelected","in","2004","United","States","Senate","election","in","New","York","with","71","percent","of","the","vote",",","in","2010","United","States","Senate","election","in","New","York","with","66","percent","of","the","vote",",","and","in","2016","United","States","Senate","election","in","New","York","with","70","percent","of","the","vote","."],"labels":["O","O","O","O","O","O","B-politician","I-politician","I-politician","O","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","organization","politician","country","event","election","location","person"]}
{"id":"250","dataset":"crossner_politics","split":"test","instance":{"id":"250","prompt_labels":"One(O) of(O) the(O) main(O) goals(O) of(O) the(O) Dal(O) is(O) to(O) build(O) the(O) Ramjanmabhoomi(B-location) temple(I-location) in(O) Ayodhya(B-location) ,(O) the(O) Krishnajanmabhoomi(B-location) temple(I-location) in(O) Mathura(B-location) and(O) the(O) Kashi(B-location) Vishwanath(I-location) temple(I-location) in(O) Varanasi(B-location) ,(O) which(O) are(O) currently(O) disputed(O) places(O) of(O) worship(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, event, organization, political party, person, election, politician and O.\nSentence: One of the main goals of the Dal is to build the Ramjanmabhoomi temple in Ayodhya , the Krishnajanmabhoomi temple in Mathura and the Kashi Vishwanath temple in Varanasi , which are currently disputed places of worship .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["One","of","the","main","goals","of","the","Dal","is","to","build","the","Ramjanmabhoomi","temple","in","Ayodhya",",","the","Krishnajanmabhoomi","temple","in","Mathura","and","the","Kashi","Vishwanath","temple","in","Varanasi",",","which","are","currently","disputed","places","of","worship","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","O","O","B-location","I-location","O","B-location","O","O","B-location","I-location","I-location","O","B-location","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","location","event","organization","political_party","person","election","politician"]}
{"id":"252","dataset":"crossner_politics","split":"test","instance":{"id":"252","prompt_labels":"The(O) agency(O) also(O) has(O) offices(O) in(O) Ottawa(B-location) ,(O) at(O) the(O) David(B-location) Florida(I-location) Laboratory(I-location) ,(O) and(O) small(O) liaison(O) offices(O) in(O) Houston(B-location) ;(O) Washington(B-location) ,(I-location) D.C.(I-location) ;(O) and(O) Paris(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, location, election, political party, politician, event, person and O.\nSentence: The agency also has offices in Ottawa , at the David Florida Laboratory , and small liaison offices in Houston ; Washington , D.C. ; and Paris .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","agency","also","has","offices","in","Ottawa",",","at","the","David","Florida","Laboratory",",","and","small","liaison","offices","in","Houston",";","Washington",",","D.C.",";","and","Paris","."],"labels":["O","O","O","O","O","O","B-location","O","O","O","B-location","I-location","I-location","O","O","O","O","O","O","B-location","O","B-location","I-location","I-location","O","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["organization","country","location","election","political_party","politician","event","person"]}
{"id":"254","dataset":"crossner_politics","split":"test","instance":{"id":"254","prompt_labels":"Examples(O) of(O) major(O) Christian(O) democratic(O) parties(O) include(O) the(O) Christian(B-political party) Democratic(I-political party) Union(I-political party) of(I-political party) Germany(I-political party) ,(O) the(O) Austrian(B-political party) People(I-political party) 's(I-political party) Party(I-political party) ,(O) Ireland(B-country) 's(O) Fine(B-political party) Gael(I-political party) ,(O) the(O) Christian(B-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Chile(I-political party) ,(O) the(O) Aruban(B-political party) People(I-political party) 's(I-political party) Party(I-political party) ,(O) the(O) Dutch(O) Christian(B-political party) Democratic(I-political party) Appeal(I-political party) ,(O) the(O) Christian(B-political party) Democratic(I-political party) People(I-political party) 's(I-political party) Party(I-political party) of(I-political party) Switzerland(I-political party) and(O) the(O) Spanish(O) People(B-political party) 's(I-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, organization, person, politician, event, election, political party and O.\nSentence: Examples of major Christian democratic parties include the Christian Democratic Union of Germany , the Austrian People 's Party , Ireland 's Fine Gael , the Christian Democratic Party of Chile , the Aruban People 's Party , the Dutch Christian Democratic Appeal , the Christian Democratic People 's Party of Switzerland and the Spanish People 's Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Examples","of","major","Christian","democratic","parties","include","the","Christian","Democratic","Union","of","Germany",",","the","Austrian","People","'s","Party",",","Ireland","'s","Fine","Gael",",","the","Christian","Democratic","Party","of","Chile",",","the","Aruban","People","'s","Party",",","the","Dutch","Christian","Democratic","Appeal",",","the","Christian","Democratic","People","'s","Party","of","Switzerland","and","the","Spanish","People","'s","Party","."],"labels":["O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","B-country","O","B-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["country","location","organization","person","politician","event","election","political_party"]}
{"id":"255","dataset":"crossner_politics","split":"test","instance":{"id":"255","prompt_labels":"This(O) is(O) because(O) Sendai(B-location) is(O) near(O) several(O) major(O) fishing(O) ports(O) ,(O) such(O) as(O) Kesennuma(B-location) ,(O) Ishinomaki(B-location) ,(O) and(O) Shiogama(B-location) ,(O) and(O) the(O) fact(O) that(O) Miyagi(B-location) Prefecture(I-location) is(O) a(O) major(O) producer(O) of(O) rice(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, election, location, organization, person, political party, politician and O.\nSentence: This is because Sendai is near several major fishing ports , such as Kesennuma , Ishinomaki , and Shiogama , and the fact that Miyagi Prefecture is a major producer of rice .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","is","because","Sendai","is","near","several","major","fishing","ports",",","such","as","Kesennuma",",","Ishinomaki",",","and","Shiogama",",","and","the","fact","that","Miyagi","Prefecture","is","a","major","producer","of","rice","."],"labels":["O","O","O","B-location","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","O","B-location","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","country","election","location","organization","person","political_party","politician"]}
{"id":"256","dataset":"crossner_politics","split":"test","instance":{"id":"256","prompt_labels":"Various(O) sporting(O) venues(O) can(O) be(O) found(O) in(O) Sendai(B-location) ,(O) such(O) as(O) Hitomebore(B-location) Stadium(I-location) Miyagi(I-location) ((O) venue(O) of(O) 2002(B-event) FIFA(I-event) World(I-event) Cup(I-event) )(O) ,(O) Yurtec(B-location) Stadium(I-location) Sendai(I-location) ,(O) Rakuten(B-location) Seimei(I-location) Park(I-location) Miyagi(I-location) ,(O) Kamei(B-location) Arena(I-location) Sendai(I-location) ,(O) Koshin(B-location) Gom(I-location) Athlete(I-location) Park(I-location) Sendai(I-location) ,(O) Shellcom(B-location) Sendai(I-location) and(O) Sendai(B-location) Hi-Land(I-location) Raceway(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, politician, location, political party, organization, election, country and O.\nSentence: Various sporting venues can be found in Sendai , such as Hitomebore Stadium Miyagi ( venue of 2002 FIFA World Cup ) , Yurtec Stadium Sendai , Rakuten Seimei Park Miyagi , Kamei Arena Sendai , Koshin Gom Athlete Park Sendai , Shellcom Sendai and Sendai Hi-Land Raceway .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Various","sporting","venues","can","be","found","in","Sendai",",","such","as","Hitomebore","Stadium","Miyagi","(","venue","of","2002","FIFA","World","Cup",")",",","Yurtec","Stadium","Sendai",",","Rakuten","Seimei","Park","Miyagi",",","Kamei","Arena","Sendai",",","Koshin","Gom","Athlete","Park","Sendai",",","Shellcom","Sendai","and","Sendai","Hi-Land","Raceway","."],"labels":["O","O","O","O","O","O","O","B-location","O","O","O","B-location","I-location","I-location","O","O","O","B-event","I-event","I-event","I-event","O","O","B-location","I-location","I-location","O","B-location","I-location","I-location","I-location","O","B-location","I-location","I-location","O","B-location","I-location","I-location","I-location","I-location","O","B-location","I-location","O","B-location","I-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["person","event","politician","location","political_party","organization","election","country"]}
{"id":"259","dataset":"crossner_politics","split":"test","instance":{"id":"259","prompt_labels":"The(O) National(B-political party) Party(I-political party) was(O) most(O) recently(O) in(O) government(O) from(O) 2008(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) to(O) 2017(O) under(O) John(B-politician) Key(I-politician) and(O) Bill(B-politician) English(I-politician) ;(O) it(O) governed(O) with(O) support(O) from(O) the(O) centrist(O) United(B-political party) Future(I-political party) ,(O) the(O) classical-liberal(O) ACT(B-political party) New(I-political party) Zealand(I-political party) and(O) the(O) indigenous-rights-based(O) Mori(B-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, politician, political party, country, election, organization, person and O.\nSentence: The National Party was most recently in government from 2008 New Zealand general election to 2017 under John Key and Bill English ; it governed with support from the centrist United Future , the classical-liberal ACT New Zealand and the indigenous-rights-based Mori Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","National","Party","was","most","recently","in","government","from","2008","New","Zealand","general","election","to","2017","under","John","Key","and","Bill","English",";","it","governed","with","support","from","the","centrist","United","Future",",","the","classical-liberal","ACT","New","Zealand","and","the","indigenous-rights-based","Mori","Party","."],"labels":["O","B-political party","I-political party","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["location","event","politician","political_party","country","election","organization","person"]}
{"id":"261","dataset":"crossner_politics","split":"test","instance":{"id":"261","prompt_labels":"National(O) formed(O) a(O) minority(O) government(O) under(O) John(B-politician) Key(I-politician) with(O) confidence-and-supply(O) support(O) from(O) the(O) ACT(B-political party) New(I-political party) Zealand(I-political party) ((O) 5(O) seats(O) )(O) ,(O) the(O) Mori(B-political party) Party(I-political party) ((O) 5(O) seats(O) )(O) and(O) United(B-political party) Future(I-political party) ((O) 1(O) seat(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, politician, person, election, location, organization, country and O.\nSentence: National formed a minority government under John Key with confidence-and-supply support from the ACT New Zealand ( 5 seats ) , the Mori Party ( 5 seats ) and United Future ( 1 seat ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["National","formed","a","minority","government","under","John","Key","with","confidence-and-supply","support","from","the","ACT","New","Zealand","(","5","seats",")",",","the","Mori","Party","(","5","seats",")","and","United","Future","(","1","seat",")","."],"labels":["O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","event","politician","person","election","location","organization","country"]}
{"id":"262","dataset":"crossner_politics","split":"test","instance":{"id":"262","prompt_labels":"The(O) NF(B-political party) was(O) founded(O) by(O) A.(B-politician) K.(I-politician) Chesterton(I-politician) ,(O) formerly(O) of(O) the(O) British(B-political party) Union(I-political party) of(I-political party) Fascists(I-political party) ,(O) as(O) a(O) merger(O) between(O) his(O) League(B-political party) of(I-political party) Empire(I-political party) Loyalists(I-political party) and(O) the(O) British(B-political party) National(I-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, person, location, political party, politician, country, organization, event and O.\nSentence: The NF was founded by A. K. Chesterton , formerly of the British Union of Fascists , as a merger between his League of Empire Loyalists and the British National Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","NF","was","founded","by","A.","K.","Chesterton",",","formerly","of","the","British","Union","of","Fascists",",","as","a","merger","between","his","League","of","Empire","Loyalists","and","the","British","National","Party","."],"labels":["O","B-political party","O","O","O","B-politician","I-politician","I-politician","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["election","person","location","political_party","politician","country","organization","event"]}
{"id":"263","dataset":"crossner_politics","split":"test","instance":{"id":"263","prompt_labels":"Born(O) and(O) raised(O) in(O) Indianapolis(B-location) ,(O) Indiana(B-location) ,(O) Cantwell(B-politician) graduated(O) from(O) Miami(B-organization) University(I-organization) before(O) moving(O) to(O) Seattle(B-location) to(O) work(O) on(O) Alan(B-politician) Cranston(I-politician) '(O) s(O) 1984(B-election) Democratic(I-election) Party(I-election) presidential(I-election) primaries(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, event, country, location, politician, organization, person and O.\nSentence: Born and raised in Indianapolis , Indiana , Cantwell graduated from Miami University before moving to Seattle to work on Alan Cranston ' s 1984 Democratic Party presidential primaries .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Born","and","raised","in","Indianapolis",",","Indiana",",","Cantwell","graduated","from","Miami","University","before","moving","to","Seattle","to","work","on","Alan","Cranston","'","s","1984","Democratic","Party","presidential","primaries","."],"labels":["O","O","O","O","B-location","O","B-location","O","B-politician","O","O","B-organization","I-organization","O","O","O","B-location","O","O","O","B-politician","I-politician","O","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["election","political_party","event","country","location","politician","organization","person"]}
{"id":"264","dataset":"crossner_politics","split":"test","instance":{"id":"264","prompt_labels":"Cantwell(B-politician) was(O) reelected(O) in(O) 2006(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Washington(I-election) ,(O) 2012(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Washington(I-election) ,(O) and(O) 2018(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Washington(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, political party, organization, event, person, country, election, politician and O.\nSentence: Cantwell was reelected in 2006 United States Senate election in Washington , 2012 United States Senate election in Washington , and 2018 United States Senate election in Washington .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Cantwell","was","reelected","in","2006","United","States","Senate","election","in","Washington",",","2012","United","States","Senate","election","in","Washington",",","and","2018","United","States","Senate","election","in","Washington","."],"labels":["B-politician","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["location","political_party","organization","event","person","country","election","politician"]}
{"id":"267","dataset":"crossner_politics","split":"test","instance":{"id":"267","prompt_labels":"He(O) went(O) from(O) place(O) to(O) place(O) ,(O) in(O) danger(O) of(O) his(O) life(O) ,(O) denouncing(O) the(O) errors(O) of(O) the(O) Papacy(O) and(O) the(O) abuses(O) in(O) the(O) churches(O) of(O) Montrose(B-location) ,(O) Dundee(B-location) ((O) where(O) he(O) escaped(O) an(O) attempt(O) on(O) his(O) life(O) )(O) ,(O) Ayr(B-location) ,(O) Perth(B-location) ,(O) Edinburgh(B-location) ,(O) Leith(B-location) ,(O) Haddington(B-location) ((O) where(O) Knox(B-person) accompanied(O) him(O) )(O) and(O) elsewhere(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, location, politician, country, person, organization, event and O.\nSentence: He went from place to place , in danger of his life , denouncing the errors of the Papacy and the abuses in the churches of Montrose , Dundee ( where he escaped an attempt on his life ) , Ayr , Perth , Edinburgh , Leith , Haddington ( where Knox accompanied him ) and elsewhere .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","went","from","place","to","place",",","in","danger","of","his","life",",","denouncing","the","errors","of","the","Papacy","and","the","abuses","in","the","churches","of","Montrose",",","Dundee","(","where","he","escaped","an","attempt","on","his","life",")",",","Ayr",",","Perth",",","Edinburgh",",","Leith",",","Haddington","(","where","Knox","accompanied","him",")","and","elsewhere","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","O","B-person","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","political_party","location","politician","country","person","organization","event"]}
{"id":"268","dataset":"crossner_politics","split":"test","instance":{"id":"268","prompt_labels":"Departing(O) from(O) Middlesbrough(B-location) railway(I-location) station(I-location) ,(O) Northern(O) operates(O) rail(O) services(O) throughout(O) the(O) north-east(O) region(O) including(O) to(O) Newcastle(B-location) upon(I-location) Tyne(I-location) ,(O) Sunderland(B-location) ,(O) Darlington(B-location) ,(O) Redcar(B-location) and(O) Whitby(B-location) ,(O) whilst(O) TransPennine(B-organization) Express(I-organization) provides(O) direct(O) rail(O) services(O) to(O) cities(O) such(O) as(O) Leeds(B-location) ,(O) York(B-location) ,(O) Liverpool(B-location) and(O) Manchester(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, person, politician, election, location, event, organization and O.\nSentence: Departing from Middlesbrough railway station , Northern operates rail services throughout the north-east region including to Newcastle upon Tyne , Sunderland , Darlington , Redcar and Whitby , whilst TransPennine Express provides direct rail services to cities such as Leeds , York , Liverpool and Manchester .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Departing","from","Middlesbrough","railway","station",",","Northern","operates","rail","services","throughout","the","north-east","region","including","to","Newcastle","upon","Tyne",",","Sunderland",",","Darlington",",","Redcar","and","Whitby",",","whilst","TransPennine","Express","provides","direct","rail","services","to","cities","such","as","Leeds",",","York",",","Liverpool","and","Manchester","."],"labels":["O","O","B-location","I-location","I-location","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","O","B-location","O","B-location","O","B-location","O","B-location","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["country","political_party","person","politician","election","location","event","organization"]}
{"id":"269","dataset":"crossner_politics","split":"test","instance":{"id":"269","prompt_labels":"He(O) has(O) since(O) been(O) re-elected(O) in(O) 2002(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Illinois(I-election) ,(O) 2008(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Illinois(I-election) and(O) 2014(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Illinois(I-election) ,(O) each(O) time(O) by(O) at(O) least(O) 10(O) %(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, politician, person, country, political party, event, location and O.\nSentence: He has since been re-elected in 2002 United States Senate election in Illinois , 2008 United States Senate election in Illinois and 2014 United States Senate election in Illinois , each time by at least 10 % .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","has","since","been","re-elected","in","2002","United","States","Senate","election","in","Illinois",",","2008","United","States","Senate","election","in","Illinois","and","2014","United","States","Senate","election","in","Illinois",",","each","time","by","at","least","10","%","."],"labels":["O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","organization","politician","person","country","political_party","event","location"]}
{"id":"272","dataset":"crossner_politics","split":"test","instance":{"id":"272","prompt_labels":"She(O) has(O) received(O) low(O) scores(O) from(O) low-spending(O) advocates(O) ((O) Club(B-organization) for(I-organization) Growth(I-organization) ,(O) 2016(O) ,(O) 8(O) %(O) ;(O) Citizens(B-organization) Against(I-organization) Government(I-organization) Waste(I-organization) ,(O) 2015(O) ,(O) 0(O) %(O) ;(O) National(B-organization) Taxpayers(I-organization) Union(I-organization) ,(O) 2015(O) ,(O) 9(O) %(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, election, politician, location, event, organization, country, person and O.\nSentence: She has received low scores from low-spending advocates ( Club for Growth , 2016 , 8 % ; Citizens Against Government Waste , 2015 , 0 % ; National Taxpayers Union , 2015 , 9 % ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","has","received","low","scores","from","low-spending","advocates","(","Club","for","Growth",",","2016",",","8","%",";","Citizens","Against","Government","Waste",",","2015",",","0","%",";","National","Taxpayers","Union",",","2015",",","9","%",")","."],"labels":["O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","election","politician","location","event","organization","country","person"]}
{"id":"273","dataset":"crossner_politics","split":"test","instance":{"id":"273","prompt_labels":"Levin(B-politician) was(O) re-elected(O) in(O) 1984(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Michigan(I-election) ,(O) 1990(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Michigan(I-election) ,(O) 1996(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Michigan(I-election) ,(O) 2002(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Michigan(I-election) and(O) 2008(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Michigan(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, location, political party, country, person, event, election and O.\nSentence: Levin was re-elected in 1984 United States Senate election in Michigan , 1990 United States Senate election in Michigan , 1996 United States Senate election in Michigan , 2002 United States Senate election in Michigan and 2008 United States Senate election in Michigan .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Levin","was","re-elected","in","1984","United","States","Senate","election","in","Michigan",",","1990","United","States","Senate","election","in","Michigan",",","1996","United","States","Senate","election","in","Michigan",",","2002","United","States","Senate","election","in","Michigan","and","2008","United","States","Senate","election","in","Michigan","."],"labels":["B-politician","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["politician","organization","location","political_party","country","person","event","election"]}
{"id":"275","dataset":"crossner_politics","split":"test","instance":{"id":"275","prompt_labels":"After(O) his(O) junior(O) year(O) at(O) Harvard(B-organization) College(I-organization) ,(O) he(O) spent(O) three(O) years(O) studying(O) Japanese(O) at(O) the(O) International(B-organization) Christian(I-organization) University(I-organization) in(O) Tokyo(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, country, person, politician, organization, location, political party and O.\nSentence: After his junior year at Harvard College , he spent three years studying Japanese at the International Christian University in Tokyo .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","his","junior","year","at","Harvard","College",",","he","spent","three","years","studying","Japanese","at","the","International","Christian","University","in","Tokyo","."],"labels":["O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["event","election","country","person","politician","organization","location","political_party"]}
{"id":"276","dataset":"crossner_politics","split":"test","instance":{"id":"276","prompt_labels":"Rockefeller(B-politician) was(O) re-elected(O) in(O) 1990(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) West(I-election) Virginia(I-election) ,(O) 1996(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) West(I-election) Virginia(I-election) ,(O) 2002(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) West(I-election) Virginia(I-election) and(O) 2008(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) West(I-election) Virginia(I-election) by(O) substantial(O) margins(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, location, person, political party, politician, organization, election and O.\nSentence: Rockefeller was re-elected in 1990 United States Senate election in West Virginia , 1996 United States Senate election in West Virginia , 2002 United States Senate election in West Virginia and 2008 United States Senate election in West Virginia by substantial margins .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Rockefeller","was","re-elected","in","1990","United","States","Senate","election","in","West","Virginia",",","1996","United","States","Senate","election","in","West","Virginia",",","2002","United","States","Senate","election","in","West","Virginia","and","2008","United","States","Senate","election","in","West","Virginia","by","substantial","margins","."],"labels":["B-politician","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","country","location","person","political_party","politician","organization","election"]}
{"id":"280","dataset":"crossner_politics","split":"test","instance":{"id":"280","prompt_labels":"For(O) example(O) ,(O) after(O) the(O) 2004(B-election) Australian(I-election) federal(I-election) election(I-election) ,(O) it(O) became(O) clear(O) that(O) the(O) governing(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) /(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) coalition(O) would(O) gain(O) a(O) majority(O) in(O) the(O) new(O) Senate(O) ,(O) which(O) was(O) due(O) to(O) sit(O) the(O) following(O) July(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, political party, organization, politician, event, country, election and O.\nSentence: For example , after the 2004 Australian federal election , it became clear that the governing Liberal Party of Australia / National Party of Australia coalition would gain a majority in the new Senate , which was due to sit the following July .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","example",",","after","the","2004","Australian","federal","election",",","it","became","clear","that","the","governing","Liberal","Party","of","Australia","/","National","Party","of","Australia","coalition","would","gain","a","majority","in","the","new","Senate",",","which","was","due","to","sit","the","following","July","."],"labels":["O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","person","political_party","organization","politician","event","country","election"]}
{"id":"281","dataset":"crossner_politics","split":"test","instance":{"id":"281","prompt_labels":"Liebknecht(B-politician) was(O) a(O) member(O) of(O) the(O) Independent(B-political party) Social(I-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Germany(I-political party) ((O) USPD(B-political party) )(O) ,(O) opposed(O) to(O) the(O) merger(O) with(O) the(O) Communist(B-political party) Party(I-political party) of(I-political party) Germany(I-political party) and(O) the(O) joining(O) of(O) the(O) Comintern(B-organization) but(O) also(O) to(O) the(O) reunification(O) of(O) the(O) party(O) with(O) the(O) Social(B-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Germany(I-political party) ,(O) he(O) continued(O) the(O) USPD(B-political party) as(O) an(O) independent(O) party(O) with(O) Georg(B-politician) Ledebour(I-politician) until(O) its(O) merger(O) into(O) the(O) Sozialistische(B-political party) Arbeiterpartei(I-political party) Deutschlands(I-political party) ((O) SAPD(B-political party) ,(O) Socialist(B-political party) Worker(I-political party) 's(I-political party) Party(I-political party) of(I-political party) Germany(I-political party) )(O) in(O) 1931(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, election, event, political party, country, person, politician, location and O.\nSentence: Liebknecht was a member of the Independent Social Democratic Party of Germany ( USPD ) , opposed to the merger with the Communist Party of Germany and the joining of the Comintern but also to the reunification of the party with the Social Democratic Party of Germany , he continued the USPD as an independent party with Georg Ledebour until its merger into the Sozialistische Arbeiterpartei Deutschlands ( SAPD , Socialist Worker 's Party of Germany ) in 1931 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Liebknecht","was","a","member","of","the","Independent","Social","Democratic","Party","of","Germany","(","USPD",")",",","opposed","to","the","merger","with","the","Communist","Party","of","Germany","and","the","joining","of","the","Comintern","but","also","to","the","reunification","of","the","party","with","the","Social","Democratic","Party","of","Germany",",","he","continued","the","USPD","as","an","independent","party","with","Georg","Ledebour","until","its","merger","into","the","Sozialistische","Arbeiterpartei","Deutschlands","(","SAPD",",","Socialist","Worker","'s","Party","of","Germany",")","in","1931","."],"labels":["B-politician","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","B-organization","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","B-political party","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","election","event","political_party","country","person","politician","location"]}
{"id":"282","dataset":"crossner_politics","split":"test","instance":{"id":"282","prompt_labels":"In(O) addition(O) ,(O) the(O) New(B-political party) Zealand(I-political party) National(I-political party) Party(I-political party) won(O) the(O) most(O) seats(O) overall(O) ,(O) forming(O) a(O) minority(O) government(O) ,(O) the(O) Fifth(B-organization) National(I-organization) Government(I-organization) of(I-organization) New(I-organization) Zealand(I-organization) ,(O) with(O) the(O) support(O) of(O) ACT(B-organization) as(O) well(O) as(O) the(O) Mori(B-political party) Party(I-political party) and(O) United(B-political party) Future(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, country, location, person, politician, organization, election, event and O.\nSentence: In addition , the New Zealand National Party won the most seats overall , forming a minority government , the Fifth National Government of New Zealand , with the support of ACT as well as the Mori Party and United Future .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","addition",",","the","New","Zealand","National","Party","won","the","most","seats","overall",",","forming","a","minority","government",",","the","Fifth","National","Government","of","New","Zealand",",","with","the","support","of","ACT","as","well","as","the","Mori","Party","and","United","Future","."],"labels":["O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","B-organization","O","O","O","O","B-political party","I-political party","O","B-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["political_party","country","location","person","politician","organization","election","event"]}
{"id":"286","dataset":"crossner_politics","split":"test","instance":{"id":"286","prompt_labels":"In(O) the(O) only(O) election(O) held(O) in(O) Zimbabwe(B-location) Rhodesia(I-location) ,(O) Bishop(O) Abel(B-politician) Muzorewa(I-politician) '(O) s(O) United(B-political party) African(I-political party) National(I-political party) Council(I-political party) ((O) United(B-political party) African(I-political party) National(I-political party) Council(I-political party) )(O) won(O) a(O) majority(O) in(O) the(O) common-roll(O) seats(O) ,(O) while(O) Ian(B-politician) Smith(I-politician) 's(O) Rhodesian(B-political party) Front(I-political party) ((O) RF(B-political party) )(O) won(O) all(O) of(O) the(O) old(O) voter(O) roll(O) seats(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, politician, country, election, person, political party, event, organization and O.\nSentence: In the only election held in Zimbabwe Rhodesia , Bishop Abel Muzorewa ' s United African National Council ( United African National Council ) won a majority in the common-roll seats , while Ian Smith 's Rhodesian Front ( RF ) won all of the old voter roll seats .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","only","election","held","in","Zimbabwe","Rhodesia",",","Bishop","Abel","Muzorewa","'","s","United","African","National","Council","(","United","African","National","Council",")","won","a","majority","in","the","common-roll","seats",",","while","Ian","Smith","'s","Rhodesian","Front","(","RF",")","won","all","of","the","old","voter","roll","seats","."],"labels":["O","O","O","O","O","O","B-location","I-location","O","O","B-politician","I-politician","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","politician","country","election","person","political_party","event","organization"]}
{"id":"289","dataset":"crossner_politics","split":"test","instance":{"id":"289","prompt_labels":"The(O) Imperial(B-location) Court(I-location) in(O) Kyoto(B-location) was(O) the(O) nominal(O) ruling(O) government(O) of(O) Japan(B-country) from(O) 794(O) AD(O) until(O) the(O) Meiji(O) period(O) ((O) 1868-1912(O) )(O) ,(O) after(O) which(O) the(O) court(O) was(O) moved(O) from(O) Kyoto(B-location) ((O) formerly(O) Heian-ky(B-location) )(O) to(O) Tokyo(B-location) ((O) formerly(O) Edo(B-location) )(O) and(O) integrated(O) into(O) the(O) Meiji(O) government(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, country, election, location, event, organization, politician and O.\nSentence: The Imperial Court in Kyoto was the nominal ruling government of Japan from 794 AD until the Meiji period ( 1868-1912 ) , after which the court was moved from Kyoto ( formerly Heian-ky ) to Tokyo ( formerly Edo ) and integrated into the Meiji government .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Imperial","Court","in","Kyoto","was","the","nominal","ruling","government","of","Japan","from","794","AD","until","the","Meiji","period","(","1868-1912",")",",","after","which","the","court","was","moved","from","Kyoto","(","formerly","Heian-ky",")","to","Tokyo","(","formerly","Edo",")","and","integrated","into","the","Meiji","government","."],"labels":["O","B-location","I-location","O","B-location","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","O","B-location","O","O","B-location","O","O","B-location","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","political_party","country","election","location","event","organization","politician"]}
{"id":"291","dataset":"crossner_politics","split":"test","instance":{"id":"291","prompt_labels":"Human(O) Rights(O) organizations(O) such(O) as(O) Amnesty(B-organization) International(I-organization) ,(O) Human(B-organization) Rights(I-organization) Watch(I-organization) and(O) Freedom(B-organization) House(I-organization) condemn(O) both(O) the(O) Saudi(B-country) criminal(O) justice(O) system(O) and(O) its(O) severe(O) punishments(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, election, political party, country, organization, event, politician, person and O.\nSentence: Human Rights organizations such as Amnesty International , Human Rights Watch and Freedom House condemn both the Saudi criminal justice system and its severe punishments .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Human","Rights","organizations","such","as","Amnesty","International",",","Human","Rights","Watch","and","Freedom","House","condemn","both","the","Saudi","criminal","justice","system","and","its","severe","punishments","."],"labels":["O","O","O","O","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","O","O","B-country","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","election","political_party","country","organization","event","politician","person"]}
{"id":"292","dataset":"crossner_politics","split":"test","instance":{"id":"292","prompt_labels":"As(O) late(O) as(O) 1970(O) ,(O) most(O) Saudis(O) lived(O) a(O) subsistence(O) life(O) in(O) the(O) rural(O) provinces(O) ,(O) but(O) in(O) the(O) last(O) half(O) of(O) the(O) 20th(O) century(O) the(O) kingdom(O) has(O) urbanized(O) rapidly.(O) about(O) 80(O) %(O) of(O) Saudis(O) live(O) in(O) urban(O) metropolitan(O) areas(O) -(O) specifically(O) Riyadh(B-location) ,(O) Jeddah(B-location) ,(O) or(O) Dammam(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, country, politician, organization, political party, election, person and O.\nSentence: As late as 1970 , most Saudis lived a subsistence life in the rural provinces , but in the last half of the 20th century the kingdom has urbanized rapidly. about 80 % of Saudis live in urban metropolitan areas - specifically Riyadh , Jeddah , or Dammam .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["As","late","as","1970",",","most","Saudis","lived","a","subsistence","life","in","the","rural","provinces",",","but","in","the","last","half","of","the","20th","century","the","kingdom","has","urbanized","rapidly.","about","80","%","of","Saudis","live","in","urban","metropolitan","areas","-","specifically","Riyadh",",","Jeddah",",","or","Dammam","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["location","event","country","politician","organization","political_party","election","person"]}
{"id":"294","dataset":"crossner_politics","split":"test","instance":{"id":"294","prompt_labels":"In(O) this(O) capacity(O) he(O) attended(O) numerous(O) international(O) conferences(O) ,(O) including(O) those(O) at(O) Tehran(B-location) ,(O) Yalta(B-location) ,(O) Dumbarton(B-location) Oaks(I-location) ,(O) and(O) Potsdam(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, political party, location, organization, politician, election, country and O.\nSentence: In this capacity he attended numerous international conferences , including those at Tehran , Yalta , Dumbarton Oaks , and Potsdam .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","this","capacity","he","attended","numerous","international","conferences",",","including","those","at","Tehran",",","Yalta",",","Dumbarton","Oaks",",","and","Potsdam","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","I-location","O","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["event","person","political_party","location","organization","politician","election","country"]}
{"id":"295","dataset":"crossner_politics","split":"test","instance":{"id":"295","prompt_labels":"The(O) fate(O) of(O) Poland(B-country) had(O) been(O) determined(O) in(O) a(O) series(O) of(O) negotiations(O) that(O) included(O) the(O) conferences(O) in(O) Tehran(B-location) ,(O) Yalta(B-location) ,(O) and(O) Potsdam(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, election, location, event, person, politician, country and O.\nSentence: The fate of Poland had been determined in a series of negotiations that included the conferences in Tehran , Yalta , and Potsdam .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","fate","of","Poland","had","been","determined","in","a","series","of","negotiations","that","included","the","conferences","in","Tehran",",","Yalta",",","and","Potsdam","."],"labels":["O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["political_party","organization","election","location","event","person","politician","country"]}
{"id":"297","dataset":"crossner_politics","split":"test","instance":{"id":"297","prompt_labels":"Karamanlis(B-politician) won(O) three(O) successive(O) elections(O) ((O) 1956(B-election) Greek(I-election) legislative(I-election) election(I-election) ,(O) 1958(B-election) Greek(I-election) legislative(I-election) election(I-election) and(O) 1961(B-election) Greek(I-election) legislative(I-election) election(I-election) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, country, political party, person, organization, politician, location and O.\nSentence: Karamanlis won three successive elections ( 1956 Greek legislative election , 1958 Greek legislative election and 1961 Greek legislative election ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Karamanlis","won","three","successive","elections","(","1956","Greek","legislative","election",",","1958","Greek","legislative","election","and","1961","Greek","legislative","election",")","."],"labels":["B-politician","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O"],"target_index":null,"target_label":null},"label_list":["election","event","country","political_party","person","organization","politician","location"]}
{"id":"302","dataset":"crossner_politics","split":"test","instance":{"id":"302","prompt_labels":"The(O) New(B-political party) Zealand(I-political party) Labour(I-political party) Party(I-political party) had(O) taken(O) office(O) after(O) defeating(O) the(O) New(B-political party) Zealand(I-political party) National(I-political party) Party(I-political party) under(O) Robert(B-politician) Muldoon(I-politician) in(O) the(O) 1984(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, person, politician, election, organization, location, event, country and O.\nSentence: The New Zealand Labour Party had taken office after defeating the New Zealand National Party under Robert Muldoon in the 1984 New Zealand general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","New","Zealand","Labour","Party","had","taken","office","after","defeating","the","New","Zealand","National","Party","under","Robert","Muldoon","in","the","1984","New","Zealand","general","election","."],"labels":["O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-politician","I-politician","O","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["political_party","person","politician","election","organization","location","event","country"]}
{"id":"303","dataset":"crossner_politics","split":"test","instance":{"id":"303","prompt_labels":"When(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) and(O) its(O) predecessors(O) ((O) the(O) Nationalist(B-political party) Party(I-political party) and(O) the(O) United(B-political party) Australia(I-political party) Party(I-political party) )(O) have(O) been(O) in(O) coalition(O) with(O) the(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) ((O) or(O) its(O) predecessor(O) the(O) Country(B-political party) Party(I-political party) )(O) ,(O) the(O) leader(O) of(O) the(O) junior(O) Coalition(O) party(O) has(O) had(O) the(O) right(O) to(O) nominate(O) his(O) party(O) 's(O) members(O) of(O) the(O) Coalition(O) ministry(O) ,(O) and(O) to(O) be(O) consulted(O) by(O) the(O) Prime(O) Minister(O) on(O) the(O) allocation(O) of(O) their(O) portfolios(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, political party, organization, election, location, country, politician and O.\nSentence: When the Liberal Party of Australia and its predecessors ( the Nationalist Party and the United Australia Party ) have been in coalition with the National Party of Australia ( or its predecessor the Country Party ) , the leader of the junior Coalition party has had the right to nominate his party 's members of the Coalition ministry , and to be consulted by the Prime Minister on the allocation of their portfolios .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["When","the","Liberal","Party","of","Australia","and","its","predecessors","(","the","Nationalist","Party","and","the","United","Australia","Party",")","have","been","in","coalition","with","the","National","Party","of","Australia","(","or","its","predecessor","the","Country","Party",")",",","the","leader","of","the","junior","Coalition","party","has","had","the","right","to","nominate","his","party","'s","members","of","the","Coalition","ministry",",","and","to","be","consulted","by","the","Prime","Minister","on","the","allocation","of","their","portfolios","."],"labels":["O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","B-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","event","political_party","organization","election","location","country","politician"]}
{"id":"304","dataset":"crossner_politics","split":"test","instance":{"id":"304","prompt_labels":"Since(O) 1942(O) ,(O) every(O) member(O) of(O) the(O) Cabinet(O) has(O) been(O) a(O) member(O) of(O) the(O) Australian(B-political party) Labor(I-political party) Party(I-political party) ,(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) ,(O) or(O) the(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, politician, location, organization, political party, election, country and O.\nSentence: Since 1942 , every member of the Cabinet has been a member of the Australian Labor Party , the Liberal Party of Australia , or the National Party of Australia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Since","1942",",","every","member","of","the","Cabinet","has","been","a","member","of","the","Australian","Labor","Party",",","the","Liberal","Party","of","Australia",",","or","the","National","Party","of","Australia","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["person","event","politician","location","organization","political_party","election","country"]}
{"id":"306","dataset":"crossner_politics","split":"test","instance":{"id":"306","prompt_labels":"Both(O) liberal(O) ((O) like(O) the(O) American(B-political party) Civil(I-political party) Liberties(I-political party) Union(I-political party) )(O) and(O) conservative(O) groups(O) ((O) like(O) the(O) Traditional(B-organization) Values(I-organization) Coalition(I-organization) )(O) as(O) well(O) as(O) other(O) groups(O) such(O) as(O) the(O) Christian(B-organization) Legal(I-organization) Society(I-organization) ,(O) the(O) American(B-organization) Jewish(I-organization) Congress(I-organization) ,(O) the(O) Baptist(B-organization) Joint(I-organization) Committee(I-organization) for(I-organization) Religious(I-organization) Liberty(I-organization) ,(O) and(O) the(O) National(B-organization) Association(I-organization) of(I-organization) Evangelicals(I-organization) joined(O) forces(O) to(O) support(O) RFRA(O) ,(O) which(O) would(O) reinstate(O) the(O) Sherbert(O) Test(O) ,(O) overturning(O) laws(O) if(O) they(O) burden(O) a(O) religion(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, election, politician, country, political party, event, location and O.\nSentence: Both liberal ( like the American Civil Liberties Union ) and conservative groups ( like the Traditional Values Coalition ) as well as other groups such as the Christian Legal Society , the American Jewish Congress , the Baptist Joint Committee for Religious Liberty , and the National Association of Evangelicals joined forces to support RFRA , which would reinstate the Sherbert Test , overturning laws if they burden a religion .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Both","liberal","(","like","the","American","Civil","Liberties","Union",")","and","conservative","groups","(","like","the","Traditional","Values","Coalition",")","as","well","as","other","groups","such","as","the","Christian","Legal","Society",",","the","American","Jewish","Congress",",","the","Baptist","Joint","Committee","for","Religious","Liberty",",","and","the","National","Association","of","Evangelicals","joined","forces","to","support","RFRA",",","which","would","reinstate","the","Sherbert","Test",",","overturning","laws","if","they","burden","a","religion","."],"labels":["O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","organization","election","politician","country","political_party","event","location"]}
{"id":"308","dataset":"crossner_politics","split":"test","instance":{"id":"308","prompt_labels":"He(O) soon(O) became(O) president(O) of(O) the(O) national(O) Social(B-political party) Credit(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) ,(O) doing(O) much(O) to(O) rebuild(O) the(O) party(O) after(O) it(O) was(O) shut(O) out(O) of(O) Parliament(O) in(O) the(O) massive(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) ((O) PC(B-political party) )(O) landslide(O) of(O) 1958(B-election) Canadian(I-election) federal(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, person, politician, political party, event, location, country and O.\nSentence: He soon became president of the national Social Credit Party of Canada , doing much to rebuild the party after it was shut out of Parliament in the massive Progressive Conservative Party of Canada ( PC ) landslide of 1958 Canadian federal election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","soon","became","president","of","the","national","Social","Credit","Party","of","Canada",",","doing","much","to","rebuild","the","party","after","it","was","shut","out","of","Parliament","in","the","massive","Progressive","Conservative","Party","of","Canada","(","PC",")","landslide","of","1958","Canadian","federal","election","."],"labels":["O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["election","organization","person","politician","political_party","event","location","country"]}
{"id":"309","dataset":"crossner_politics","split":"test","instance":{"id":"309","prompt_labels":"The(O) party(O) was(O) founded(O) on(O) 5(O) January(O) 1930(O) by(O) the(O) merger(O) of(O) the(O) Hapoel(B-political party) Hatzair(I-political party) founded(O) by(O) A.(B-politician) D.(I-politician) Gordon(I-politician) and(O) the(O) original(O) Ahdut(B-political party) HaAvoda(I-political party) ((O) founded(O) in(O) 1919(O) from(O) the(O) right(O) ,(O) more(O) moderate(O) ,(O) wing(O) of(O) the(O) Zionist(O) socialist(O) Poale(B-event) Zion(I-event) led(O) by(O) David(B-politician) Ben-Gurion(I-politician) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, person, organization, election, event, politician, country, location and O.\nSentence: The party was founded on 5 January 1930 by the merger of the Hapoel Hatzair founded by A. D. Gordon and the original Ahdut HaAvoda ( founded in 1919 from the right , more moderate , wing of the Zionist socialist Poale Zion led by David Ben-Gurion ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","party","was","founded","on","5","January","1930","by","the","merger","of","the","Hapoel","Hatzair","founded","by","A.","D.","Gordon","and","the","original","Ahdut","HaAvoda","(","founded","in","1919","from","the","right",",","more","moderate",",","wing","of","the","Zionist","socialist","Poale","Zion","led","by","David","Ben-Gurion",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","B-politician","I-politician","I-politician","O","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","O","O","B-politician","I-politician","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","person","organization","election","event","politician","country","location"]}
{"id":"311","dataset":"crossner_politics","split":"test","instance":{"id":"311","prompt_labels":"Ben-Gurion(B-politician) became(O) Prime(O) Minister(O) and(O) formed(O) a(O) coalition(O) with(O) the(O) United(B-political party) Religious(I-political party) Front(I-political party) ,(O) the(O) Progressive(B-political party) Party(I-political party) ,(O) the(O) Sephardim(B-political party) and(I-political party) Oriental(I-political party) Communities(I-political party) and(O) the(O) Democratic(B-political party) List(I-political party) of(I-political party) Nazareth(I-political party) ((O) an(O) Israeli(O) Arab(O) party(O) associated(O) with(O) Mapai(B-political party) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, location, politician, event, person, organization, political party, country and O.\nSentence: Ben-Gurion became Prime Minister and formed a coalition with the United Religious Front , the Progressive Party , the Sephardim and Oriental Communities and the Democratic List of Nazareth ( an Israeli Arab party associated with Mapai ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Ben-Gurion","became","Prime","Minister","and","formed","a","coalition","with","the","United","Religious","Front",",","the","Progressive","Party",",","the","Sephardim","and","Oriental","Communities","and","the","Democratic","List","of","Nazareth","(","an","Israeli","Arab","party","associated","with","Mapai",")","."],"labels":["B-politician","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","B-political party","O","O"],"target_index":null,"target_label":null},"label_list":["election","location","politician","event","person","organization","political_party","country"]}
{"id":"312","dataset":"crossner_politics","split":"test","instance":{"id":"312","prompt_labels":"Ben-Gurion(B-politician) again(O) formed(O) the(O) government(O) with(O) the(O) support(O) of(O) Mizrachi(B-organization) ,(O) Hapoel(B-political party) HaMizrachi(I-political party) ,(O) Agudat(B-political party) Yisrael(I-political party) ,(O) Poalei(B-political party) Agudat(I-political party) Yisrael(I-political party) and(O) the(O) three(O) Israeli(O) Arab(O) parties(O) associated(O) with(O) Mapai(B-political party) ,(O) the(O) Democratic(B-political party) List(I-political party) for(I-political party) Israeli(I-political party) Arabs(I-political party) ,(O) Progress(B-political party) and(I-political party) Work(I-political party) and(O) Agriculture(B-political party) and(I-political party) Development(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, country, politician, person, election, event, political party and O.\nSentence: Ben-Gurion again formed the government with the support of Mizrachi , Hapoel HaMizrachi , Agudat Yisrael , Poalei Agudat Yisrael and the three Israeli Arab parties associated with Mapai , the Democratic List for Israeli Arabs , Progress and Work and Agriculture and Development .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Ben-Gurion","again","formed","the","government","with","the","support","of","Mizrachi",",","Hapoel","HaMizrachi",",","Agudat","Yisrael",",","Poalei","Agudat","Yisrael","and","the","three","Israeli","Arab","parties","associated","with","Mapai",",","the","Democratic","List","for","Israeli","Arabs",",","Progress","and","Work","and","Agriculture","and","Development","."],"labels":["B-politician","O","O","O","O","O","O","O","O","B-organization","O","B-political party","I-political party","O","B-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","B-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["location","organization","country","politician","person","election","event","political_party"]}
{"id":"313","dataset":"crossner_politics","split":"test","instance":{"id":"313","prompt_labels":"Ben(B-politician) Gurion(I-politician) returned(O) as(O) Prime(O) Minister(O) ,(O) and(O) formed(O) a(O) coalition(O) with(O) the(O) National(O) Religious(O) Front(O) ((O) which(O) later(O) changed(O) its(O) name(O) to(O) the(O) National(B-political party) Religious(I-political party) Party(I-political party) )(O) ,(O) Mapam(B-political party) ,(O) Ahdut(B-political party) HaAvoda(I-political party) ,(O) and(O) the(O) three(O) Israeli(O) Arab(O) parties(O) ,(O) the(O) Democratic(O) List(O) for(O) Israeli(O) Arabs(O) ,(O) Progress(O) and(O) Work(O) and(O) Agriculture(O) and(O) Development(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, country, organization, election, location, person, event, politician and O.\nSentence: Ben Gurion returned as Prime Minister , and formed a coalition with the National Religious Front ( which later changed its name to the National Religious Party ) , Mapam , Ahdut HaAvoda , and the three Israeli Arab parties , the Democratic List for Israeli Arabs , Progress and Work and Agriculture and Development .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Ben","Gurion","returned","as","Prime","Minister",",","and","formed","a","coalition","with","the","National","Religious","Front","(","which","later","changed","its","name","to","the","National","Religious","Party",")",",","Mapam",",","Ahdut","HaAvoda",",","and","the","three","Israeli","Arab","parties",",","the","Democratic","List","for","Israeli","Arabs",",","Progress","and","Work","and","Agriculture","and","Development","."],"labels":["B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","country","organization","election","location","person","event","politician"]}
{"id":"315","dataset":"crossner_politics","split":"test","instance":{"id":"315","prompt_labels":"When(O) this(O) draft(O) was(O) discussed(O) in(O) 1929(O) by(O) the(O) judiciary(O) committee(O) of(O) the(O) Reichstag(B-location) ,(O) the(O) Social(B-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Germany(I-political party) ,(O) the(O) Communist(B-political party) Party(I-political party) of(I-political party) Germany(I-political party) ,(O) and(O) the(O) left-wing(O) liberal(O) German(B-political party) Democratic(I-political party) Party(I-political party) at(O) first(O) managed(O) to(O) mobilize(O) a(O) majority(O) of(O) 15(O) to(O) 13(O) votes(O) against(O) Paragraph(O) 296(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, location, event, politician, person, country, election, organization and O.\nSentence: When this draft was discussed in 1929 by the judiciary committee of the Reichstag , the Social Democratic Party of Germany , the Communist Party of Germany , and the left-wing liberal German Democratic Party at first managed to mobilize a majority of 15 to 13 votes against Paragraph 296 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["When","this","draft","was","discussed","in","1929","by","the","judiciary","committee","of","the","Reichstag",",","the","Social","Democratic","Party","of","Germany",",","the","Communist","Party","of","Germany",",","and","the","left-wing","liberal","German","Democratic","Party","at","first","managed","to","mobilize","a","majority","of","15","to","13","votes","against","Paragraph","296","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","location","event","politician","person","country","election","organization"]}
{"id":"321","dataset":"crossner_politics","split":"test","instance":{"id":"321","prompt_labels":"Other(O) fellowships(O) included(O) the(O) Guggenheim(B-organization) 1988-1989(O) ;(O) the(O) Woodrow(B-organization) Wilson(I-organization) International(I-organization) Center(I-organization) for(I-organization) Scholars(I-organization) 1977(O) and(O) 1979(O) ;(O) Fulbright(B-organization) 1964-1965(O) and(O) 1977(O) ;(O) SSRC-ACLS(B-organization) 1966-1968(O) ;(O) Ford(B-organization) Foundation(I-organization) ,(O) 1970(O) ;(O) German(B-organization) Marshall(I-organization) Fund(I-organization) ,(O) 1979(O) ;(O) Social(B-organization) Science(I-organization) Research(I-organization) Council(I-organization) ,(O) 1982(O) ;(O) SSRC-Foreign(B-organization) Policy(I-organization) Studies(I-organization) ,(O) 1988-1989(O) and(O) was(O) made(O) a(O) Harold(B-person) Lasswell(I-person) by(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Political(I-organization) and(I-organization) Social(I-organization) Science(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, location, politician, political party, organization, person, country and O.\nSentence: Other fellowships included the Guggenheim 1988-1989 ; the Woodrow Wilson International Center for Scholars 1977 and 1979 ; Fulbright 1964-1965 and 1977 ; SSRC-ACLS 1966-1968 ; Ford Foundation , 1970 ; German Marshall Fund , 1979 ; Social Science Research Council , 1982 ; SSRC-Foreign Policy Studies , 1988-1989 and was made a Harold Lasswell by the American Academy of Political and Social Science .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","fellowships","included","the","Guggenheim","1988-1989",";","the","Woodrow","Wilson","International","Center","for","Scholars","1977","and","1979",";","Fulbright","1964-1965","and","1977",";","SSRC-ACLS","1966-1968",";","Ford","Foundation",",","1970",";","German","Marshall","Fund",",","1979",";","Social","Science","Research","Council",",","1982",";","SSRC-Foreign","Policy","Studies",",","1988-1989","and","was","made","a","Harold","Lasswell","by","the","American","Academy","of","Political","and","Social","Science","."],"labels":["O","O","O","O","B-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","B-organization","O","O","O","O","B-organization","O","O","B-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","B-person","I-person","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["event","election","location","politician","political_party","organization","person","country"]}
{"id":"326","dataset":"crossner_politics","split":"test","instance":{"id":"326","prompt_labels":"Joining(O) forces(O) for(O) the(O) 2003(B-election) parliamentary(I-election) election(I-election) ,(O) they(O) achieved(O) limited(O) success(O) ,(O) but(O) more(O) importantly(O) managed(O) to(O) get(O) into(O) the(O) coalition(O) that(O) formed(O) the(O) minority(O) government(O) ((O) along(O) with(O) Democratic(B-political party) Party(I-political party) of(I-political party) Serbia(I-political party) ,(O) G17(B-political party) Plus(I-political party) )(O) ,(O) providing(O) it(O) with(O) critical(O) parliamentary(O) seats(O) to(O) keep(O) the(O) far-right(O) radicals(O) ((O) Serbian(B-political party) Radical(I-political party) Party(I-political party) )(O) at(O) bay(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, election, location, country, person, politician, organization, event and O.\nSentence: Joining forces for the 2003 parliamentary election , they achieved limited success , but more importantly managed to get into the coalition that formed the minority government ( along with Democratic Party of Serbia , G17 Plus ) , providing it with critical parliamentary seats to keep the far-right radicals ( Serbian Radical Party ) at bay .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Joining","forces","for","the","2003","parliamentary","election",",","they","achieved","limited","success",",","but","more","importantly","managed","to","get","into","the","coalition","that","formed","the","minority","government","(","along","with","Democratic","Party","of","Serbia",",","G17","Plus",")",",","providing","it","with","critical","parliamentary","seats","to","keep","the","far-right","radicals","(","Serbian","Radical","Party",")","at","bay","."],"labels":["O","O","O","O","B-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","election","location","country","person","politician","organization","event"]}
{"id":"327","dataset":"crossner_politics","split":"test","instance":{"id":"327","prompt_labels":"Goldstone(B-politician) serves(O) on(O) the(O) Board(O) of(O) Directors(O) of(O) several(O) nonprofit(O) organisations(O) that(O) promote(O) justice(O) ,(O) including(O) Physicians(B-organization) for(I-organization) Human(I-organization) Rights(I-organization) ,(O) the(O) International(B-organization) Center(I-organization) for(I-organization) Transitional(I-organization) Justice(I-organization) ,(O) the(O) Institute(B-organization) for(I-organization) Justice(I-organization) and(I-organization) Reconciliation(I-organization) ,(O) the(O) South(B-organization) African(I-organization) Legal(I-organization) Services(I-organization) Foundation(I-organization) ,(O) the(O) Brandeis(B-organization) University(I-organization) Center(I-organization) for(I-organization) Ethics(I-organization) ,(I-organization) Justice(I-organization) ,(I-organization) and(I-organization) Public(I-organization) Life(I-organization) ,(O) Human(B-organization) Rights(I-organization) Watch(I-organization) ,(O) and(O) the(O) Center(B-organization) for(I-organization) Economic(I-organization) and(I-organization) Social(I-organization) Rights(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, political party, politician, election, organization, person, location and O.\nSentence: Goldstone serves on the Board of Directors of several nonprofit organisations that promote justice , including Physicians for Human Rights , the International Center for Transitional Justice , the Institute for Justice and Reconciliation , the South African Legal Services Foundation , the Brandeis University Center for Ethics , Justice , and Public Life , Human Rights Watch , and the Center for Economic and Social Rights .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Goldstone","serves","on","the","Board","of","Directors","of","several","nonprofit","organisations","that","promote","justice",",","including","Physicians","for","Human","Rights",",","the","International","Center","for","Transitional","Justice",",","the","Institute","for","Justice","and","Reconciliation",",","the","South","African","Legal","Services","Foundation",",","the","Brandeis","University","Center","for","Ethics",",","Justice",",","and","Public","Life",",","Human","Rights","Watch",",","and","the","Center","for","Economic","and","Social","Rights","."],"labels":["B-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["event","country","political_party","politician","election","organization","person","location"]}
{"id":"329","dataset":"crossner_politics","split":"test","instance":{"id":"329","prompt_labels":"Two(O) members(O) being(O) appointed(O) by(O) each(O) of(O) the(O) National(B-political party) Governors(I-political party) Association(I-political party) ,(O) the(O) National(B-organization) Conference(I-organization) of(I-organization) State(I-organization) Legislatures(I-organization) ,(O) the(O) National(B-organization) Association(I-organization) of(I-organization) Secretaries(I-organization) of(I-organization) State(I-organization) ,(O) the(O) National(B-organization) Association(I-organization) of(I-organization) State(I-organization) Election(I-organization) Directors(I-organization) ,(O) the(O) National(B-organization) Association(I-organization) of(I-organization) Counties(I-organization) ,(O) the(O) )(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, politician, organization, election, event, country, political party and O.\nSentence: Two members being appointed by each of the National Governors Association , the National Conference of State Legislatures , the National Association of Secretaries of State , the National Association of State Election Directors , the National Association of Counties , the )","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Two","members","being","appointed","by","each","of","the","National","Governors","Association",",","the","National","Conference","of","State","Legislatures",",","the","National","Association","of","Secretaries","of","State",",","the","National","Association","of","State","Election","Directors",",","the","National","Association","of","Counties",",","the",")"],"labels":["O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","location","politician","organization","election","event","country","political_party"]}
{"id":"332","dataset":"crossner_politics","split":"test","instance":{"id":"332","prompt_labels":"In(O) 1980(O) the(O) party(O) merged(O) with(O) the(O) Anti-Revolutionary(B-political party) Party(I-political party) ((O) ARP(B-political party) )(O) and(O) the(O) Christian(B-political party) Historical(I-political party) Union(I-political party) ((O) CHU(B-political party) )(O) to(O) form(O) the(O) Christian(B-political party) Democratic(I-political party) Appeal(I-political party) ((O) CDA(B-political party) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, politician, political party, election, country, location, event and O.\nSentence: In 1980 the party merged with the Anti-Revolutionary Party ( ARP ) and the Christian Historical Union ( CHU ) to form the Christian Democratic Appeal ( CDA ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1980","the","party","merged","with","the","Anti-Revolutionary","Party","(","ARP",")","and","the","Christian","Historical","Union","(","CHU",")","to","form","the","Christian","Democratic","Appeal","(","CDA",")","."],"labels":["O","O","O","O","O","O","O","B-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O"],"target_index":null,"target_label":null},"label_list":["organization","person","politician","political_party","election","country","location","event"]}
{"id":"334","dataset":"crossner_politics","split":"test","instance":{"id":"334","prompt_labels":"This(O) tradition(O) was(O) broken(O) in(O) 1993(O) when(O) massive(O) vote-splitting(O) between(O) the(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) and(O) Reform(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) allowed(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) to(O) sneak(O) up(O) the(O) middle(O) and(O) take(O) the(O) riding(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, location, person, event, election, country, politician and O.\nSentence: This tradition was broken in 1993 when massive vote-splitting between the Progressive Conservative Party of Canada and Reform Party of Canada allowed the Liberal Party of Canada to sneak up the middle and take the riding .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","tradition","was","broken","in","1993","when","massive","vote-splitting","between","the","Progressive","Conservative","Party","of","Canada","and","Reform","Party","of","Canada","allowed","the","Liberal","Party","of","Canada","to","sneak","up","the","middle","and","take","the","riding","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","organization","location","person","event","election","country","politician"]}
{"id":"335","dataset":"crossner_politics","split":"test","instance":{"id":"335","prompt_labels":"Both(O) losses(O) ((O) 1974(B-election) Canadian(I-election) federal(I-election) election(I-election) and(O) 1993(B-election) Canadian(I-election) federal(I-election) election(I-election) )(O) have(O) come(O) at(O) the(O) hands(O) of(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) candidates(O) who(O) failed(O) to(O) retain(O) the(O) seat(O) at(O) the(O) next(O) election(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, person, country, election, politician, organization, location and O.\nSentence: Both losses ( 1974 Canadian federal election and 1993 Canadian federal election ) have come at the hands of Liberal Party of Canada candidates who failed to retain the seat at the next election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Both","losses","(","1974","Canadian","federal","election","and","1993","Canadian","federal","election",")","have","come","at","the","hands","of","Liberal","Party","of","Canada","candidates","who","failed","to","retain","the","seat","at","the","next","election","."],"labels":["O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","event","person","country","election","politician","organization","location"]}
{"id":"337","dataset":"crossner_politics","split":"test","instance":{"id":"337","prompt_labels":"The(O) UK(B-political party) Independence(I-political party) Party(I-political party) ,(O) Green(B-political party) Party(I-political party) of(I-political party) England(I-political party) and(I-political party) Wales(I-political party) and(O) Plaid(B-political party) Cymru(I-political party) won(O) their(O) first(O) ever(O) seats(O) in(O) the(O) European(B-organization) Parliament(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, event, political party, election, location, politician, country and O.\nSentence: The UK Independence Party , Green Party of England and Wales and Plaid Cymru won their first ever seats in the European Parliament .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","UK","Independence","Party",",","Green","Party","of","England","and","Wales","and","Plaid","Cymru","won","their","first","ever","seats","in","the","European","Parliament","."],"labels":["O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","O","O","O","O","O","O","O","B-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["person","organization","event","political_party","election","location","politician","country"]}
{"id":"341","dataset":"crossner_politics","split":"test","instance":{"id":"341","prompt_labels":"As(O) of(O) February(O) 2019(O) ,(O) the(O) Party(O) is(O) the(O) fourth-largest(O) political(O) party(O) in(O) the(O) State(O) of(O) New(B-location) York(I-location) ,(O) ranking(O) behind(O) the(O) Democratic(B-political party) Party(I-political party) ,(O) the(O) Republican(B-political party) Party(I-political party) and(O) the(O) Independence(B-political party) Party(I-political party) of(I-political party) New(I-political party) York(I-political party) and(O) ahead(O) of(O) the(O) Working(B-political party) Families(I-political party) Party(I-political party) and(O) the(O) Green(B-political party) Party(I-political party) of(I-political party) New(I-political party) York(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, event, location, political party, country, politician, person and O.\nSentence: As of February 2019 , the Party is the fourth-largest political party in the State of New York , ranking behind the Democratic Party , the Republican Party and the Independence Party of New York and ahead of the Working Families Party and the Green Party of New York .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["As","of","February","2019",",","the","Party","is","the","fourth-largest","political","party","in","the","State","of","New","York",",","ranking","behind","the","Democratic","Party",",","the","Republican","Party","and","the","Independence","Party","of","New","York","and","ahead","of","the","Working","Families","Party","and","the","Green","Party","of","New","York","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","O","B-political party","I-political party","O","O","B-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["election","organization","event","location","political_party","country","politician","person"]}
{"id":"345","dataset":"crossner_politics","split":"test","instance":{"id":"345","prompt_labels":"Throughout(O) his(O) political(O) career(O) he(O) has(O) been(O) variously(O) associated(O) with(O) conservative(O) groups(O) ,(O) including(O) the(O) Association(B-organization) of(I-organization) Christian(I-organization) Parent(I-organization) Controlled(I-organization) Schools(I-organization) ,(O) Salt(B-organization) Shakers(I-organization) ,(O) Focus(B-organization) on(I-organization) the(I-organization) Family(I-organization) ,(O) Lyons(B-organization) Forum(I-organization) ,(O) Endeavour(B-organization) Forum(I-organization) ,(O) Family(B-organization) Council(I-organization) of(I-organization) Victoria(I-organization) ,(O) Fatherhood(B-organization) Foundation(I-organization) ,(O) Australian(B-organization) Christian(I-organization) Lobby(I-organization) ,(O) Australian(B-organization) Family(I-organization) Association(I-organization) and(O) Right(B-organization) to(I-organization) Life(I-organization) Australia(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, organization, political party, election, location, country, politician and O.\nSentence: Throughout his political career he has been variously associated with conservative groups , including the Association of Christian Parent Controlled Schools , Salt Shakers , Focus on the Family , Lyons Forum , Endeavour Forum , Family Council of Victoria , Fatherhood Foundation , Australian Christian Lobby , Australian Family Association and Right to Life Australia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Throughout","his","political","career","he","has","been","variously","associated","with","conservative","groups",",","including","the","Association","of","Christian","Parent","Controlled","Schools",",","Salt","Shakers",",","Focus","on","the","Family",",","Lyons","Forum",",","Endeavour","Forum",",","Family","Council","of","Victoria",",","Fatherhood","Foundation",",","Australian","Christian","Lobby",",","Australian","Family","Association","and","Right","to","Life","Australia","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["person","event","organization","political_party","election","location","country","politician"]}
{"id":"347","dataset":"crossner_politics","split":"test","instance":{"id":"347","prompt_labels":"244(O) Among(O) the(O) most(O) important(O) are(O) World(B-organization) Vision(I-organization) International(I-organization) ((O) 1950(O) )(O) ,(O) Samaritan(B-organization) 's(I-organization) Purse(I-organization) ((O) 1970(O) )(O) ,(O) Mercy(B-organization) Ships(I-organization) ((O) 1978(O) )(O) ,(O) Prison(B-organization) Fellowship(I-organization) International(I-organization) ((O) 1979(O) )(O) ,(O) International(B-organization) Justice(I-organization) Mission(I-organization) ((O) 1997(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, country, person, event, politician, location, election and O.\nSentence: 244 Among the most important are World Vision International ( 1950 ) , Samaritan 's Purse ( 1970 ) , Mercy Ships ( 1978 ) , Prison Fellowship International ( 1979 ) , International Justice Mission ( 1997 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["244","Among","the","most","important","are","World","Vision","International","(","1950",")",",","Samaritan","'s","Purse","(","1970",")",",","Mercy","Ships","(","1978",")",",","Prison","Fellowship","International","(","1979",")",",","International","Justice","Mission","(","1997",")","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","B-organization","I-organization","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","political_party","country","person","event","politician","location","election"]}
{"id":"352","dataset":"crossner_politics","split":"test","instance":{"id":"352","prompt_labels":"The(O) Opposition(O) Australian(B-political party) Labor(I-political party) Party(I-political party) announced(O) they(O) would(O) not(O) support(O) the(O) bill(O) ;(O) nor(O) would(O) the(O) Australian(B-political party) Greens(I-political party) ,(O) Australian(B-political party) Democrats(I-political party) or(O) independent(O) Senator(O) Brian(B-politician) Harradine(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, location, politician, event, organization, country, election and O.\nSentence: The Opposition Australian Labor Party announced they would not support the bill ; nor would the Australian Greens , Australian Democrats or independent Senator Brian Harradine .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Opposition","Australian","Labor","Party","announced","they","would","not","support","the","bill",";","nor","would","the","Australian","Greens",",","Australian","Democrats","or","independent","Senator","Brian","Harradine","."],"labels":["O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","B-political party","I-political party","O","O","O","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["person","political_party","location","politician","event","organization","country","election"]}
{"id":"355","dataset":"crossner_politics","split":"test","instance":{"id":"355","prompt_labels":"In(O) neighbouring(O) Alberta(B-location) ,(O) The(O) United(B-political party) Conservative(I-political party) Party(I-political party) formed(O) from(O) a(O) merger(O) of(O) the(O) Progressive(B-political party) Conservative(I-political party) Association(I-political party) of(I-political party) Alberta(I-political party) and(O) Wildrose(B-political party) Party(I-political party) parties(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, person, event, political party, location, organization, country and O.\nSentence: In neighbouring Alberta , The United Conservative Party formed from a merger of the Progressive Conservative Association of Alberta and Wildrose Party parties .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","neighbouring","Alberta",",","The","United","Conservative","Party","formed","from","a","merger","of","the","Progressive","Conservative","Association","of","Alberta","and","Wildrose","Party","parties","."],"labels":["O","O","B-location","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","O","O"],"target_index":null,"target_label":null},"label_list":["election","politician","person","event","political_party","location","organization","country"]}
{"id":"357","dataset":"crossner_politics","split":"test","instance":{"id":"357","prompt_labels":"Australian(O) parliaments(O) are(O) modelled(O) on(O) the(O) Westminster(O) system(O) ,(O) with(O) a(O) hung(O) parliament(O) typically(O) defined(O) as(O) a(O) lack(O) of(O) a(O) lower(O) house(O) parliamentary(O) majority(O) from(O) either(O) the(O) Australian(B-political party) Labor(I-political party) Party(I-political party) or(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) /(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) Coalition(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, person, election, organization, event, politician, political party and O.\nSentence: Australian parliaments are modelled on the Westminster system , with a hung parliament typically defined as a lack of a lower house parliamentary majority from either the Australian Labor Party or Liberal Party of Australia / National Party of Australia Coalition .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Australian","parliaments","are","modelled","on","the","Westminster","system",",","with","a","hung","parliament","typically","defined","as","a","lack","of","a","lower","house","parliamentary","majority","from","either","the","Australian","Labor","Party","or","Liberal","Party","of","Australia","/","National","Party","of","Australia","Coalition","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","O"],"target_index":null,"target_label":null},"label_list":["location","country","person","election","organization","event","politician","political_party"]}
{"id":"359","dataset":"crossner_politics","split":"test","instance":{"id":"359","prompt_labels":"While(O) the(O) Conservative(B-political party) Party(I-political party) had(O) a(O) plurality(O) of(O) seats(O) ,(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) and(O) New(B-political party) Democratic(I-political party) Party(I-political party) ,(O) supported(O) by(O) The(O) Bloc(B-political party) Qubcois(I-political party) ,(O) agreed(O) to(O) defeat(O) the(O) Conservatives(B-political party) in(O) favour(O) of(O) a(O) Liberal(B-organization) /(I-organization) NDP(I-organization) coalition(I-organization) government(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, politician, election, political party, location, country, event and O.\nSentence: While the Conservative Party had a plurality of seats , the Liberal Party of Canada and New Democratic Party , supported by The Bloc Qubcois , agreed to defeat the Conservatives in favour of a Liberal / NDP coalition government .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["While","the","Conservative","Party","had","a","plurality","of","seats",",","the","Liberal","Party","of","Canada","and","New","Democratic","Party",",","supported","by","The","Bloc","Qubcois",",","agreed","to","defeat","the","Conservatives","in","favour","of","a","Liberal","/","NDP","coalition","government","."],"labels":["O","O","B-political party","I-political party","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","O","O","O","O","O","B-political party","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["person","organization","politician","election","political_party","location","country","event"]}
{"id":"360","dataset":"crossner_politics","split":"test","instance":{"id":"360","prompt_labels":"Only(O) on(O) five(O) occasions(O) since(O) the(O) beginnings(O) of(O) modern(O) party(O) politics(O) in(O) 1890(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) has(O) a(O) hung(O) parliament(O) occurred(O) ,(O) in(O) 1911(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ,(O) 1922(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ,(O) 1928(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ,(O) 1931(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) &(O) amp(O) ;(O) 1993(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) respectively(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, person, organization, election, politician, country, location and O.\nSentence: Only on five occasions since the beginnings of modern party politics in 1890 New Zealand general election has a hung parliament occurred , in 1911 New Zealand general election , 1922 New Zealand general election , 1928 New Zealand general election , 1931 New Zealand general election & amp ; 1993 New Zealand general election respectively .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Only","on","five","occasions","since","the","beginnings","of","modern","party","politics","in","1890","New","Zealand","general","election","has","a","hung","parliament","occurred",",","in","1911","New","Zealand","general","election",",","1922","New","Zealand","general","election",",","1928","New","Zealand","general","election",",","1931","New","Zealand","general","election","&","amp",";","1993","New","Zealand","general","election","respectively","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O"],"target_index":null,"target_label":null},"label_list":["event","political_party","person","organization","election","politician","country","location"]}
{"id":"361","dataset":"crossner_politics","split":"test","instance":{"id":"361","prompt_labels":"When(O) the(O) Assembly(O) was(O) 2003(B-election) National(I-election) Assembly(I-election) for(I-election) Wales(I-election) election(I-election) on(O) 1(O) May(O) 2003(O) ,(O) Welsh(B-political party) Labour(I-political party) won(O) 30(O) seats(O) ,(O) Plaid(B-political party) Cymru(I-political party) won(O) 12(O) ,(O) the(O) Conservatives(B-political party) won(O) 11(O) ,(O) Welsh(B-political party) Liberal(I-political party) Democrats(I-political party) won(O) 6(O) ,(O) and(O) the(O) John(B-political party) Marek(I-political party) Independent(I-political party) Party(I-political party) won(O) a(O) seat(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, election, person, location, politician, event, country, political party and O.\nSentence: When the Assembly was 2003 National Assembly for Wales election on 1 May 2003 , Welsh Labour won 30 seats , Plaid Cymru won 12 , the Conservatives won 11 , Welsh Liberal Democrats won 6 , and the John Marek Independent Party won a seat .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["When","the","Assembly","was","2003","National","Assembly","for","Wales","election","on","1","May","2003",",","Welsh","Labour","won","30","seats",",","Plaid","Cymru","won","12",",","the","Conservatives","won","11",",","Welsh","Liberal","Democrats","won","6",",","and","the","John","Marek","Independent","Party","won","a","seat","."],"labels":["O","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O","B-political party","I-political party","O","O","O","O","B-political party","I-political party","O","O","O","O","B-political party","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","election","person","location","politician","event","country","political_party"]}
{"id":"365","dataset":"crossner_politics","split":"test","instance":{"id":"365","prompt_labels":"These(O) policies(O) ((O) in(O) particular(O) ,(O) the(O) right(O) to(O) buy(O) scheme(O) )(O) are(O) thought(O) to(O) have(O) caused(O) many(O) people(O) who(O) had(O) traditionally(O) voted(O) Labour(B-political party) in(O) Essex(B-location) to(O) switch(O) their(O) allegiance(O) in(O) the(O) elections(O) of(O) 1979(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) 1983(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 1987(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, event, country, organization, person, location, political party, election and O.\nSentence: These policies ( in particular , the right to buy scheme ) are thought to have caused many people who had traditionally voted Labour in Essex to switch their allegiance in the elections of 1979 United Kingdom general election , 1983 United Kingdom general election and 1987 United Kingdom general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["These","policies","(","in","particular",",","the","right","to","buy","scheme",")","are","thought","to","have","caused","many","people","who","had","traditionally","voted","Labour","in","Essex","to","switch","their","allegiance","in","the","elections","of","1979","United","Kingdom","general","election",",","1983","United","Kingdom","general","election","and","1987","United","Kingdom","general","election","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","O","B-location","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["politician","event","country","organization","person","location","political_party","election"]}
{"id":"367","dataset":"crossner_politics","split":"test","instance":{"id":"367","prompt_labels":"The(O) Chinese(B-organization) reunification(I-organization) -leaning(I-organization) conservative(I-organization) Pan-Blue(I-organization) Coalition(I-organization) ((O) consisting(O) of(O) the(O) Kuomintang(B-political party) ,(O) People(B-political party) First(I-political party) Party(I-political party) ,(O) and(O) New(B-political party) Party(I-political party) )(O) retained(O) its(O) majority(O) in(O) the(O) legislature(O) ,(O) winning(O) 114(O) seats(O) ,(O) compared(O) to(O) 101(O) seats(O) won(O) by(O) the(O) Taiwan(B-organization) independence(I-organization) -leaning(I-organization) Pan-Green(I-organization) Coalition(I-organization) ((O) consisting(O) of(O) the(O) Democratic(B-political party) Progressive(I-political party) Party(I-political party) and(O) Taiwan(B-political party) Solidarity(I-political party) Union(I-political party) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, political party, politician, organization, election, location, person and O.\nSentence: The Chinese reunification -leaning conservative Pan-Blue Coalition ( consisting of the Kuomintang , People First Party , and New Party ) retained its majority in the legislature , winning 114 seats , compared to 101 seats won by the Taiwan independence -leaning Pan-Green Coalition ( consisting of the Democratic Progressive Party and Taiwan Solidarity Union ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Chinese","reunification","-leaning","conservative","Pan-Blue","Coalition","(","consisting","of","the","Kuomintang",",","People","First","Party",",","and","New","Party",")","retained","its","majority","in","the","legislature",",","winning","114","seats",",","compared","to","101","seats","won","by","the","Taiwan","independence","-leaning","Pan-Green","Coalition","(","consisting","of","the","Democratic","Progressive","Party","and","Taiwan","Solidarity","Union",")","."],"labels":["O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","B-political party","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O"],"target_index":null,"target_label":null},"label_list":["event","country","political_party","politician","organization","election","location","person"]}
{"id":"368","dataset":"crossner_politics","split":"test","instance":{"id":"368","prompt_labels":"In(O) the(O) wake(O) of(O) the(O) landslide(O) victory(O) of(O) the(O) liberal(O) forces(O) led(O) by(O) the(O) United(B-political party) Democrats(I-political party) of(I-political party) Hong(I-political party) Kong(I-political party) ((O) UDHK(B-political party) )(O) in(O) the(O) 1991(B-election) Hong(I-election) Kong(I-election) legislative(I-election) election(I-election) of(O) the(O) Legislative(B-organization) Council(I-organization) in(O) 1991(O) ,(O) Lee(O) formed(O) the(O) Co-operative(B-political party) Resources(I-political party) Centre(I-political party) ((O) CRC(B-political party) )(O) ,(O) a(O) conservative(O) parliamentary(O) group(O) with(O) other(O) appointed(O) and(O) indirectly(O) elected(O) members(O) from(O) the(O) business(O) sectors(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, event, country, political party, politician, organization, election and O.\nSentence: In the wake of the landslide victory of the liberal forces led by the United Democrats of Hong Kong ( UDHK ) in the 1991 Hong Kong legislative election of the Legislative Council in 1991 , Lee formed the Co-operative Resources Centre ( CRC ) , a conservative parliamentary group with other appointed and indirectly elected members from the business sectors .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","wake","of","the","landslide","victory","of","the","liberal","forces","led","by","the","United","Democrats","of","Hong","Kong","(","UDHK",")","in","the","1991","Hong","Kong","legislative","election","of","the","Legislative","Council","in","1991",",","Lee","formed","the","Co-operative","Resources","Centre","(","CRC",")",",","a","conservative","parliamentary","group","with","other","appointed","and","indirectly","elected","members","from","the","business","sectors","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","B-organization","I-organization","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","location","event","country","political_party","politician","organization","election"]}
{"id":"370","dataset":"crossner_politics","split":"test","instance":{"id":"370","prompt_labels":"Seven(O) parties(O) contested(O) the(O) elections(O) as(O) the(O) Mgwirizano(B-political party) Coalition(I-political party) ;(O) the(O) Republican(B-political party) Party(I-political party) ,(O) the(O) People(B-political party) 's(I-political party) Progressive(I-political party) Movement(I-political party) ,(O) the(O) Movement(B-political party) for(I-political party) Genuine(I-political party) Democratic(I-political party) Change(I-political party) ,(O) the(O) People(B-political party) 's(I-political party) Transformation(I-political party) Party(I-political party) ,(O) the(O) Malawi(B-political party) Forum(I-political party) for(I-political party) Unity(I-political party) and(I-political party) Development(I-political party) ,(O) the(O) National(B-political party) Unity(I-political party) Party(I-political party) and(O) the(O) Malawi(B-political party) Democratic(I-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, location, politician, event, organization, person, country and O.\nSentence: Seven parties contested the elections as the Mgwirizano Coalition ; the Republican Party , the People 's Progressive Movement , the Movement for Genuine Democratic Change , the People 's Transformation Party , the Malawi Forum for Unity and Development , the National Unity Party and the Malawi Democratic Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Seven","parties","contested","the","elections","as","the","Mgwirizano","Coalition",";","the","Republican","Party",",","the","People","'s","Progressive","Movement",",","the","Movement","for","Genuine","Democratic","Change",",","the","People","'s","Transformation","Party",",","the","Malawi","Forum","for","Unity","and","Development",",","the","National","Unity","Party","and","the","Malawi","Democratic","Party","."],"labels":["O","O","O","O","O","O","O","B-political party","I-political party","O","O","B-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["election","political_party","location","politician","event","organization","person","country"]}
{"id":"371","dataset":"crossner_politics","split":"test","instance":{"id":"371","prompt_labels":"He(O) was(O) re-elected(O) in(O) 1923(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) 1924(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 1929(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, political party, person, politician, election, location, event and O.\nSentence: He was re-elected in 1923 United Kingdom general election , 1924 United Kingdom general election and 1929 United Kingdom general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","re-elected","in","1923","United","Kingdom","general","election",",","1924","United","Kingdom","general","election","and","1929","United","Kingdom","general","election","."],"labels":["O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["organization","country","political_party","person","politician","election","location","event"]}
{"id":"372","dataset":"crossner_politics","split":"test","instance":{"id":"372","prompt_labels":"The(O) Eleventh(O) Amendment(O) was(O) introduced(O) by(O) a(O) Fianna(B-political party) Fil(I-political party) -(O) Progressive(B-political party) Democrats(I-political party) coalition(O) government(O) and(O) was(O) also(O) supported(O) by(O) opposition(O) parties(O) Fine(B-political party) Gael(I-political party) and(O) the(O) Labour(B-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, organization, politician, country, event, location, election and O.\nSentence: The Eleventh Amendment was introduced by a Fianna Fil - Progressive Democrats coalition government and was also supported by opposition parties Fine Gael and the Labour Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Eleventh","Amendment","was","introduced","by","a","Fianna","Fil","-","Progressive","Democrats","coalition","government","and","was","also","supported","by","opposition","parties","Fine","Gael","and","the","Labour","Party","."],"labels":["O","O","O","O","O","O","O","B-political party","I-political party","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","B-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["person","political_party","organization","politician","country","event","location","election"]}
{"id":"375","dataset":"crossner_politics","split":"test","instance":{"id":"375","prompt_labels":"Strauss(B-politician) writes(O) that(O) ,(O) as(O) a(O) result(O) of(O) far-right(O) involvement(O) ,(O) a(O) bizarre(O) ideological(O) turf(O) war(O) has(O) broken(O) out(O) ,(O) whereby(O) anti-globalization(O) activists(O) are(O) fighting(O) a(O) two-front(O) battle(O) ,(O) one(O) against(O) the(O) World(B-organization) Trade(I-organization) Organization(I-organization) ,(O) International(B-organization) Monetary(I-organization) Fund(I-organization) ((O) IMF(B-organization) )(O) ,(O) and(O) World(B-organization) Bank(I-organization) ,(O) the(O) other(O) against(O) the(O) extremists(O) who(O) turn(O) up(O) at(O) their(O) rallies(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, location, event, organization, election, country, political party and O.\nSentence: Strauss writes that , as a result of far-right involvement , a bizarre ideological turf war has broken out , whereby anti-globalization activists are fighting a two-front battle , one against the World Trade Organization , International Monetary Fund ( IMF ) , and World Bank , the other against the extremists who turn up at their rallies .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Strauss","writes","that",",","as","a","result","of","far-right","involvement",",","a","bizarre","ideological","turf","war","has","broken","out",",","whereby","anti-globalization","activists","are","fighting","a","two-front","battle",",","one","against","the","World","Trade","Organization",",","International","Monetary","Fund","(","IMF",")",",","and","World","Bank",",","the","other","against","the","extremists","who","turn","up","at","their","rallies","."],"labels":["B-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","politician","location","event","organization","election","country","political_party"]}
{"id":"379","dataset":"crossner_politics","split":"test","instance":{"id":"379","prompt_labels":"he(O) was(O) returned(O) as(O) MP(O) for(O) Reigate(B-location) where(O) he(O) was(O) returned(O) again(O) in(O) 1727(B-election) British(I-election) general(I-election) election(I-election) and(O) 1734(B-election) British(I-election) general(I-election) election(I-election) .(O) He(O) sponsored(O) the(O) Mortmain(O) Act(O) and(O) the(O) Gin(O) Act(O) 1736(O) ,(O) and(O) was(O) noted(O) for(O) his(O) opposition(O) to(O) intoxication(O) ,(O) which(O) annoyed(O) the(O) public(O) so(O) much(O) that(O) he(O) was(O) forced(O) to(O) have(O) a(O) guard(O) at(O) his(O) house(O) at(O) all(O) times(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, event, person, location, election, political party, country and O.\nSentence: he was returned as MP for Reigate where he was returned again in 1727 British general election and 1734 British general election . He sponsored the Mortmain Act and the Gin Act 1736 , and was noted for his opposition to intoxication , which annoyed the public so much that he was forced to have a guard at his house at all times .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["he","was","returned","as","MP","for","Reigate","where","he","was","returned","again","in","1727","British","general","election","and","1734","British","general","election",".","He","sponsored","the","Mortmain","Act","and","the","Gin","Act","1736",",","and","was","noted","for","his","opposition","to","intoxication",",","which","annoyed","the","public","so","much","that","he","was","forced","to","have","a","guard","at","his","house","at","all","times","."],"labels":["O","O","O","O","O","O","B-location","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","politician","event","person","location","election","political_party","country"]}
{"id":"380","dataset":"crossner_politics","split":"test","instance":{"id":"380","prompt_labels":"Presidential(O) elections(O) have(O) been(O) held(O) in(O) 1999(B-election) Slovak(I-election) presidential(I-election) election(I-election) ,(O) 2004(B-election) Slovak(I-election) presidential(I-election) election(I-election) ,(O) 2009(B-election) Slovak(I-election) presidential(I-election) election(I-election) ,(O) 2014(B-election) Slovak(I-election) presidential(I-election) election(I-election) and(O) 2019(B-election) Slovak(I-election) presidential(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, election, organization, event, country, location, politician and O.\nSentence: Presidential elections have been held in 1999 Slovak presidential election , 2004 Slovak presidential election , 2009 Slovak presidential election , 2014 Slovak presidential election and 2019 Slovak presidential election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Presidential","elections","have","been","held","in","1999","Slovak","presidential","election",",","2004","Slovak","presidential","election",",","2009","Slovak","presidential","election",",","2014","Slovak","presidential","election","and","2019","Slovak","presidential","election","."],"labels":["O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["person","political_party","election","organization","event","country","location","politician"]}
{"id":"381","dataset":"crossner_politics","split":"test","instance":{"id":"381","prompt_labels":"Dion(B-politician) would(O) hold(O) the(O) riding(O) in(O) 1997(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) and(O) was(O) reelected(O) again(O) in(O) the(O) 2000(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2004(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2006(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2008(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2011(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) and(O) 2015(B-election) Canadian(I-election) federal(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, person, organization, election, country, location, event, politician and O.\nSentence: Dion would hold the riding in 1997 Canadian federal election , and was reelected again in the 2000 Canadian federal election , 2004 Canadian federal election , 2006 Canadian federal election , 2008 Canadian federal election , 2011 Canadian federal election , and 2015 Canadian federal election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Dion","would","hold","the","riding","in","1997","Canadian","federal","election",",","and","was","reelected","again","in","the","2000","Canadian","federal","election",",","2004","Canadian","federal","election",",","2006","Canadian","federal","election",",","2008","Canadian","federal","election",",","2011","Canadian","federal","election",",","and","2015","Canadian","federal","election","."],"labels":["B-politician","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["political_party","person","organization","election","country","location","event","politician"]}
{"id":"382","dataset":"crossner_politics","split":"test","instance":{"id":"382","prompt_labels":"These(O) agreements(O) set(O) the(O) policies(O) for(O) the(O) Bank(B-organization) for(I-organization) International(I-organization) Settlements(I-organization) ((O) BIS(B-organization) )(O) ,(O) International(B-organization) Monetary(I-organization) Fund(I-organization) ((O) IMF(B-organization) )(O) ,(O) and(O) World(B-organization) Bank(I-organization) ,(O) the(O) so-called(O) Bretton(B-organization) Woods(B-organization) Institutions(B-organization) ,(O) launched(O) in(O) the(O) late(O) 1940s(O) for(O) the(O) last(O) two(O) ((O) the(O) BIS(B-organization) was(O) founded(O) in(O) 1930(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, location, country, election, politician, event, political party and O.\nSentence: These agreements set the policies for the Bank for International Settlements ( BIS ) , International Monetary Fund ( IMF ) , and World Bank , the so-called Bretton Woods Institutions , launched in the late 1940s for the last two ( the BIS was founded in 1930 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["These","agreements","set","the","policies","for","the","Bank","for","International","Settlements","(","BIS",")",",","International","Monetary","Fund","(","IMF",")",",","and","World","Bank",",","the","so-called","Bretton","Woods","Institutions",",","launched","in","the","late","1940s","for","the","last","two","(","the","BIS","was","founded","in","1930",")","."],"labels":["O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","B-organization","I-organization","O","O","O","B-organization","B-organization","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","organization","location","country","election","politician","event","political_party"]}
{"id":"386","dataset":"crossner_politics","split":"test","instance":{"id":"386","prompt_labels":"Future(B-political party) New(I-political party) Zealand(I-political party) joined(O) with(O) the(O) United(B-political party) New(I-political party) Zealand(I-political party) to(O) form(O) a(O) coalition(O) known(O) as(O) United(B-political party) Future(I-political party) New(I-political party) Zealand(I-political party) in(O) November(O) 2000(O) and(O) contested(O) the(O) 2002(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) as(O) such(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, organization, election, political party, politician, country, person and O.\nSentence: Future New Zealand joined with the United New Zealand to form a coalition known as United Future New Zealand in November 2000 and contested the 2002 New Zealand general election as such .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Future","New","Zealand","joined","with","the","United","New","Zealand","to","form","a","coalition","known","as","United","Future","New","Zealand","in","November","2000","and","contested","the","2002","New","Zealand","general","election","as","such","."],"labels":["B-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","location","organization","election","political_party","politician","country","person"]}
{"id":"388","dataset":"crossner_politics","split":"test","instance":{"id":"388","prompt_labels":"She(O) ran(O) again(O) in(O) 1979(O) ,(O) and(O) won(O) the(O) nomination(O) but(O) was(O) defeated(O) in(O) both(O) the(O) 1979(B-election) Canadian(I-election) federal(I-election) election(I-election) and(O) 1980(B-election) Canadian(I-election) federal(I-election) election(I-election) by(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) candidate(O) David(B-politician) Crombie(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, event, country, political party, election, location, person, organization and O.\nSentence: She ran again in 1979 , and won the nomination but was defeated in both the 1979 Canadian federal election and 1980 Canadian federal election by Progressive Conservative Party of Canada candidate David Crombie .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","ran","again","in","1979",",","and","won","the","nomination","but","was","defeated","in","both","the","1979","Canadian","federal","election","and","1980","Canadian","federal","election","by","Progressive","Conservative","Party","of","Canada","candidate","David","Crombie","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["politician","event","country","political_party","election","location","person","organization"]}
{"id":"389","dataset":"crossner_politics","split":"test","instance":{"id":"389","prompt_labels":"Condon(B-politician) ran(O) unsuccessfully(O) as(O) a(O) New(B-political party) Democratic(I-political party) Party(I-political party) candidate(O) for(O) the(O) House(B-organization) of(I-organization) Commons(I-organization) of(I-organization) Canada(I-organization) in(O) the(O) 1980(B-election) Canadian(I-election) federal(I-election) election(I-election) and(O) 1984(B-election) Canadian(I-election) federal(I-election) election(I-election) in(O) the(O) riding(O) of(O) Grand(B-location) Falls(I-location) -(O) White(B-location) Bay(I-location) -(O) Labrador(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, election, politician, location, country, event, person and O.\nSentence: Condon ran unsuccessfully as a New Democratic Party candidate for the House of Commons of Canada in the 1980 Canadian federal election and 1984 Canadian federal election in the riding of Grand Falls - White Bay - Labrador .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Condon","ran","unsuccessfully","as","a","New","Democratic","Party","candidate","for","the","House","of","Commons","of","Canada","in","the","1980","Canadian","federal","election","and","1984","Canadian","federal","election","in","the","riding","of","Grand","Falls","-","White","Bay","-","Labrador","."],"labels":["B-politician","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O","O","O","B-location","I-location","O","B-location","I-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["organization","political_party","election","politician","location","country","event","person"]}
{"id":"390","dataset":"crossner_politics","split":"test","instance":{"id":"390","prompt_labels":"After(O) the(O) 2008(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ,(O) 2011(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) and(O) 2014(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) s(O) ,(O) where(O) the(O) party(O) won(O) five(O) ,(O) three(O) and(O) two(O) Mori(O) seats(O) respectively(O) ,(O) it(O) supported(O) a(O) government(O) led(O) by(O) the(O) centre-right(O) New(B-political party) Zealand(I-political party) National(I-political party) Party(I-political party) ,(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, political party, event, election, organization, politician, person and O.\nSentence: After the 2008 New Zealand general election , 2011 New Zealand general election and 2014 New Zealand general election s , where the party won five , three and two Mori seats respectively , it supported a government led by the centre-right New Zealand National Party ,","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","the","2008","New","Zealand","general","election",",","2011","New","Zealand","general","election","and","2014","New","Zealand","general","election","s",",","where","the","party","won","five",",","three","and","two","Mori","seats","respectively",",","it","supported","a","government","led","by","the","centre-right","New","Zealand","National","Party",","],"labels":["O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["country","location","political_party","event","election","organization","politician","person"]}
{"id":"391","dataset":"crossner_politics","split":"test","instance":{"id":"391","prompt_labels":"The(O) New(B-political party) Zealand(I-political party) National(I-political party) Party(I-political party) won(O) the(O) most(O) seats(O) overall(O) and(O) formed(O) a(O) minority(O) government(O) with(O) the(O) support(O) of(O) the(O) Mori(B-political party) Party(I-political party) ,(O) ACT(B-political party) New(I-political party) Zealand(I-political party) and(O) United(B-political party) Future(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, location, political party, election, event, country, person and O.\nSentence: The New Zealand National Party won the most seats overall and formed a minority government with the support of the Mori Party , ACT New Zealand and United Future .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","New","Zealand","National","Party","won","the","most","seats","overall","and","formed","a","minority","government","with","the","support","of","the","Mori","Party",",","ACT","New","Zealand","and","United","Future","."],"labels":["O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["politician","organization","location","political_party","election","event","country","person"]}
{"id":"392","dataset":"crossner_politics","split":"test","instance":{"id":"392","prompt_labels":"Although(O) it(O) never(O) reached(O) high(O) shares(O) of(O) vote(O) and(O) never(O) participated(O) in(O) government(O) ,(O) the(O) party(O) had(O) close(O) relations(O) with(O) the(O) other(O) parties(O) of(O) the(O) Italian(B-political party) left(O) -(O) from(O) the(O) Italian(B-political party) Republican(I-political party) Party(I-political party) and(O) the(O) Italian(B-political party) Socialist(I-political party) Party(I-political party) to(O) the(O) Italian(B-political party) Communist(I-political party) Party(I-political party) and(O) Proletarian(B-political party) Democracy(I-political party) -(O) and(O) opened(O) its(O) ranks(O) also(O) to(O) members(O) of(O) other(O) parties(O) through(O) dual(O) membership(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, country, political party, politician, location, event, person and O.\nSentence: Although it never reached high shares of vote and never participated in government , the party had close relations with the other parties of the Italian left - from the Italian Republican Party and the Italian Socialist Party to the Italian Communist Party and Proletarian Democracy - and opened its ranks also to members of other parties through dual membership .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Although","it","never","reached","high","shares","of","vote","and","never","participated","in","government",",","the","party","had","close","relations","with","the","other","parties","of","the","Italian","left","-","from","the","Italian","Republican","Party","and","the","Italian","Socialist","Party","to","the","Italian","Communist","Party","and","Proletarian","Democracy","-","and","opened","its","ranks","also","to","members","of","other","parties","through","dual","membership","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","organization","country","political_party","politician","location","event","person"]}
{"id":"393","dataset":"crossner_politics","split":"test","instance":{"id":"393","prompt_labels":"The(O) Radicals(O) continued(O) to(O) participate(O) in(O) elections(O) through(O) the(O) Antiprohibitionists(B-political party) on(I-political party) Drugs(I-political party) list(O) ,(O) the(O) Rainbow(B-political party) Greens(I-political party) ,(O) the(O) Pannella(B-political party) List(I-political party) ,(O) the(O) Bonino(B-political party) List(I-political party) and(O) the(O) Bonino-Pannella(B-political party) List(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, organization, country, location, political party, election, politician and O.\nSentence: The Radicals continued to participate in elections through the Antiprohibitionists on Drugs list , the Rainbow Greens , the Pannella List , the Bonino List and the Bonino-Pannella List .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Radicals","continued","to","participate","in","elections","through","the","Antiprohibitionists","on","Drugs","list",",","the","Rainbow","Greens",",","the","Pannella","List",",","the","Bonino","List","and","the","Bonino-Pannella","List","."],"labels":["O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","O","O","B-political party","I-political party","O","O","B-political party","I-political party","O","O","B-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["event","person","organization","country","location","political_party","election","politician"]}
{"id":"396","dataset":"crossner_politics","split":"test","instance":{"id":"396","prompt_labels":"The(O) United(B-election) Kingdom(I-election) local(I-election) elections(I-election) of(I-election) 2004(I-election) were(O) held(O) on(O) 10(O) June(O) ,(O) as(O) part(O) of(O) the(O) 2004(O) set(O) of(O) elections(O) along(O) with(O) the(O) 2004(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) the(I-election) United(I-election) Kingdom(I-election) and(O) the(O) London(B-location) 2004(B-election) London(I-election) mayoral(I-election) election(I-election) and(O) 2004(B-election) London(I-election) Assembly(I-election) election(I-election) elections(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, politician, country, location, person, organization, election and O.\nSentence: The United Kingdom local elections of 2004 were held on 10 June , as part of the 2004 set of elections along with the 2004 European Parliament election in the United Kingdom and the London 2004 London mayoral election and 2004 London Assembly election elections .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","United","Kingdom","local","elections","of","2004","were","held","on","10","June",",","as","part","of","the","2004","set","of","elections","along","with","the","2004","European","Parliament","election","in","the","United","Kingdom","and","the","London","2004","London","mayoral","election","and","2004","London","Assembly","election","elections","."],"labels":["O","B-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","B-location","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O"],"target_index":null,"target_label":null},"label_list":["event","political_party","politician","country","location","person","organization","election"]}
{"id":"397","dataset":"crossner_politics","split":"test","instance":{"id":"397","prompt_labels":"Lpez(B-politician) Obrador(I-politician) was(O) a(O) candidate(O) for(O) the(O) third(O) time(O) in(O) the(O) 2018(B-election) Mexican(I-election) general(I-election) election(I-election) ,(O) representing(O) Juntos(B-political party) Haremos(I-political party) Historia(I-political party) ,(O) a(O) coalition(O) of(O) the(O) left-wing(B-political party) Labor(I-political party) Party(I-political party) ,(O) right-wing(B-political party) Social(I-political party) Encounter(I-political party) Party(I-political party) ,(O) and(O) National(B-political party) Regeneration(I-political party) Movement(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, country, politician, election, organization, event, political party and O.\nSentence: Lpez Obrador was a candidate for the third time in the 2018 Mexican general election , representing Juntos Haremos Historia , a coalition of the left-wing Labor Party , right-wing Social Encounter Party , and National Regeneration Movement .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Lpez","Obrador","was","a","candidate","for","the","third","time","in","the","2018","Mexican","general","election",",","representing","Juntos","Haremos","Historia",",","a","coalition","of","the","left-wing","Labor","Party",",","right-wing","Social","Encounter","Party",",","and","National","Regeneration","Movement","."],"labels":["B-politician","I-politician","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["location","person","country","politician","election","organization","event","political_party"]}
{"id":"401","dataset":"crossner_politics","split":"test","instance":{"id":"401","prompt_labels":"He(O) then(O) moved(O) to(O) UK(B-political party) Independence(I-political party) Party(I-political party) ,(O) for(O) which(O) he(O) stood(O) in(O) Kensington(B-location) and(I-location) Chelsea(I-location) at(O) the(O) 1999(B-election) Kensington(I-election) and(I-election) Chelsea(I-election) by-election(I-election) and(O) the(O) 2001(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, event, election, political party, person, location, country and O.\nSentence: He then moved to UK Independence Party , for which he stood in Kensington and Chelsea at the 1999 Kensington and Chelsea by-election and the 2001 United Kingdom general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","then","moved","to","UK","Independence","Party",",","for","which","he","stood","in","Kensington","and","Chelsea","at","the","1999","Kensington","and","Chelsea","by-election","and","the","2001","United","Kingdom","general","election","."],"labels":["O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","B-location","I-location","I-location","O","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["organization","politician","event","election","political_party","person","location","country"]}
{"id":"402","dataset":"crossner_politics","split":"test","instance":{"id":"402","prompt_labels":"He(O) unsuccessfully(O) stood(O) as(O) a(O) parliamentary(O) candidate(O) for(O) Washington(B-location) ((O) in(O) the(O) 1987(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) )(O) ,(O) Gateshead(B-location) East(I-location) ((O) in(O) the(O) 1992(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) )(O) ,(O) and(O) Tynemouth(B-location) ((O) in(O) the(O) 1997(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, election, organization, location, event, political party, country and O.\nSentence: He unsuccessfully stood as a parliamentary candidate for Washington ( in the 1987 United Kingdom general election ) , Gateshead East ( in the 1992 United Kingdom general election ) , and Tynemouth ( in the 1997 United Kingdom general election ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","unsuccessfully","stood","as","a","parliamentary","candidate","for","Washington","(","in","the","1987","United","Kingdom","general","election",")",",","Gateshead","East","(","in","the","1992","United","Kingdom","general","election",")",",","and","Tynemouth","(","in","the","1997","United","Kingdom","general","election",")","."],"labels":["O","O","O","O","O","O","O","O","B-location","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","B-location","I-location","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","B-location","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O"],"target_index":null,"target_label":null},"label_list":["politician","person","election","organization","location","event","political_party","country"]}
{"id":"403","dataset":"crossner_politics","split":"test","instance":{"id":"403","prompt_labels":"After(O) standing(O) unsuccessfully(O) at(O) West(B-location) Derbyshire(I-location) in(O) 1966(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) he(O) represented(O) Derby(B-location) North(I-location) as(O) a(O) Labour(B-politician) MP(O) from(O) 1970(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) to(O) 1983(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) when(O) he(O) was(O) defeated(O) by(O) the(O) Conservative(O) Greg(B-politician) Knight(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, person, political party, politician, country, event, election and O.\nSentence: After standing unsuccessfully at West Derbyshire in 1966 United Kingdom general election , he represented Derby North as a Labour MP from 1970 United Kingdom general election to 1983 United Kingdom general election , when he was defeated by the Conservative Greg Knight .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","standing","unsuccessfully","at","West","Derbyshire","in","1966","United","Kingdom","general","election",",","he","represented","Derby","North","as","a","Labour","MP","from","1970","United","Kingdom","general","election","to","1983","United","Kingdom","general","election",",","when","he","was","defeated","by","the","Conservative","Greg","Knight","."],"labels":["O","O","O","O","B-location","I-location","O","B-election","I-election","I-election","I-election","I-election","O","O","O","B-location","I-location","O","O","B-politician","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["organization","location","person","political_party","politician","country","event","election"]}
{"id":"404","dataset":"crossner_politics","split":"test","instance":{"id":"404","prompt_labels":"Helmer(B-politician) was(O) elected(O) to(O) the(O) European(B-organization) Parliament(I-organization) in(O) 1999(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) the(I-election) United(I-election) Kingdom(I-election) ,(O) 2004(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) the(I-election) United(I-election) Kingdom(I-election) ,(O) and(O) 2009(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) the(I-election) United(I-election) Kingdom(I-election) as(O) a(O) member(O) of(O) the(O) Conservative(B-political party) Party(I-political party) and(O) in(O) 2014(O) as(O) a(O) member(O) of(O) the(O) UK(B-political party) Independence(I-political party) Party(I-political party) ((O) UKIP(B-political party) )(O) ,(O) having(O) defected(O) from(O) the(O) Conservatives(B-political party) to(O) UKIP(B-political party) in(O) March(O) 2012(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, country, event, election, organization, politician, person, location and O.\nSentence: Helmer was elected to the European Parliament in 1999 European Parliament election in the United Kingdom , 2004 European Parliament election in the United Kingdom , and 2009 European Parliament election in the United Kingdom as a member of the Conservative Party and in 2014 as a member of the UK Independence Party ( UKIP ) , having defected from the Conservatives to UKIP in March 2012 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Helmer","was","elected","to","the","European","Parliament","in","1999","European","Parliament","election","in","the","United","Kingdom",",","2004","European","Parliament","election","in","the","United","Kingdom",",","and","2009","European","Parliament","election","in","the","United","Kingdom","as","a","member","of","the","Conservative","Party","and","in","2014","as","a","member","of","the","UK","Independence","Party","(","UKIP",")",",","having","defected","from","the","Conservatives","to","UKIP","in","March","2012","."],"labels":["B-politician","O","O","O","O","B-organization","I-organization","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","B-political party","O","B-political party","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","country","event","election","organization","politician","person","location"]}
{"id":"406","dataset":"crossner_politics","split":"test","instance":{"id":"406","prompt_labels":"The(O) party(O) became(O) active(O) participants(O) in(O) by-elections(O) ,(O) contesting(O) those(O) held(O) in(O) City(B-location) of(I-location) London(I-location) and(O) Westminster(B-location) ,(O) Beaconsfield(B-location) ,(O) Penrith(B-location) and(O) the(B-location) Border(I-location) and(O) 1983(B-election) Bermondsey(I-election) by-election(I-election) ,(O) as(O) well(O) as(O) putting(O) up(O) two(O) candidates(O) in(O) the(O) general(O) elections(O) of(O) 1979(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 1983(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, political party, person, location, politician, election, country and O.\nSentence: The party became active participants in by-elections , contesting those held in City of London and Westminster , Beaconsfield , Penrith and the Border and 1983 Bermondsey by-election , as well as putting up two candidates in the general elections of 1979 United Kingdom general election and 1983 United Kingdom general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","party","became","active","participants","in","by-elections",",","contesting","those","held","in","City","of","London","and","Westminster",",","Beaconsfield",",","Penrith","and","the","Border","and","1983","Bermondsey","by-election",",","as","well","as","putting","up","two","candidates","in","the","general","elections","of","1979","United","Kingdom","general","election","and","1983","United","Kingdom","general","election","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","O","B-location","O","B-location","O","B-location","O","B-location","I-location","O","B-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["organization","event","political_party","person","location","politician","election","country"]}
{"id":"410","dataset":"crossner_politics","split":"test","instance":{"id":"410","prompt_labels":"The(O) Italian(B-political party) Democratic(I-political party) Socialists(I-political party) ((O) SDI(B-political party) )(O) ,(O) which(O) were(O) part(O) of(O) the(O) federation(O) and(O) fight(O) the(O) 2004(O) European(O) and(O) the(O) 2005(O) regional(O) elections(O) within(O) it(O) ,(O) decided(O) not(O) to(O) take(O) part(O) of(O) the(O) joint(O) electoral(O) list(O) for(O) the(O) 2006(O) general(O) election(O) and(O) ,(O) otherwise(O) ,(O) to(O) form(O) a(O) common(O) list(O) with(O) the(O) Italian(B-political party) Radicals(I-political party) called(O) Rose(B-political party) in(I-political party) the(I-political party) Fist(I-political party) within(O) The(O) Union(O) coalition(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, organization, location, election, event, political party, country and O.\nSentence: The Italian Democratic Socialists ( SDI ) , which were part of the federation and fight the 2004 European and the 2005 regional elections within it , decided not to take part of the joint electoral list for the 2006 general election and , otherwise , to form a common list with the Italian Radicals called Rose in the Fist within The Union coalition .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Italian","Democratic","Socialists","(","SDI",")",",","which","were","part","of","the","federation","and","fight","the","2004","European","and","the","2005","regional","elections","within","it",",","decided","not","to","take","part","of","the","joint","electoral","list","for","the","2006","general","election","and",",","otherwise",",","to","form","a","common","list","with","the","Italian","Radicals","called","Rose","in","the","Fist","within","The","Union","coalition","."],"labels":["O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["politician","person","organization","location","election","event","political_party","country"]}
{"id":"413","dataset":"crossner_politics","split":"test","instance":{"id":"413","prompt_labels":"UPR(B-political party) cooperated(O) with(O) National(B-political party) Movement(I-political party) in(O) the(O) 2014(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) Poland(I-election) ,(O) 2014(B-election) Polish(I-election) local(I-election) elections(I-election) and(O) the(O) 2015(B-election) Polish(I-election) presidential(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, politician, political party, event, organization, country, election and O.\nSentence: UPR cooperated with National Movement in the 2014 European Parliament election in Poland , 2014 Polish local elections and the 2015 Polish presidential election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["UPR","cooperated","with","National","Movement","in","the","2014","European","Parliament","election","in","Poland",",","2014","Polish","local","elections","and","the","2015","Polish","presidential","election","."],"labels":["B-political party","O","O","B-political party","I-political party","O","O","B-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["person","location","politician","political_party","event","organization","country","election"]}
{"id":"418","dataset":"crossner_politics","split":"test","instance":{"id":"418","prompt_labels":"In(O) Pakistan(B-country) the(O) Jamaat-e-Islami(B-political party) Pakistan(I-political party) enjoyed(O) popular(O) support(O) especially(O) since(O) the(O) formation(O) of(O) the(O) Muttahida(B-political party) Majlis-e-Amal(I-political party) ,(O) and(O) in(O) Algeria(B-country) the(O) Islamic(B-political party) Salvation(I-political party) Front(I-political party) was(O) expected(O) to(O) win(O) the(O) cancelled(O) elections(O) in(O) 1992(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, politician, election, location, country, event, political party and O.\nSentence: In Pakistan the Jamaat-e-Islami Pakistan enjoyed popular support especially since the formation of the Muttahida Majlis-e-Amal , and in Algeria the Islamic Salvation Front was expected to win the cancelled elections in 1992 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","Pakistan","the","Jamaat-e-Islami","Pakistan","enjoyed","popular","support","especially","since","the","formation","of","the","Muttahida","Majlis-e-Amal",",","and","in","Algeria","the","Islamic","Salvation","Front","was","expected","to","win","the","cancelled","elections","in","1992","."],"labels":["O","B-country","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","B-country","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","organization","politician","election","location","country","event","political_party"]}
{"id":"423","dataset":"crossner_politics","split":"test","instance":{"id":"423","prompt_labels":"Venkataraman(B-politician) was(O) also(O) ,(O) variously(O) ,(O) a(O) member(O) of(O) the(O) Political(B-organization) Affairs(I-organization) Committee(I-organization) and(O) the(O) Economic(B-organization) Affairs(I-organization) Committee(I-organization) of(I-organization) the(I-organization) Union(I-organization) Cabinet(I-organization) ;(O) Governor(O) ,(O) International(B-organization) Monetary(I-organization) Fund(I-organization) ,(O) the(O) International(B-organization) Bank(I-organization) for(I-organization) Reconstruction(I-organization) and(I-organization) Development(I-organization) ,(O) and(O) the(O) Asian(B-organization) Development(I-organization) Bank(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, event, political party, election, politician, organization, country and O.\nSentence: Venkataraman was also , variously , a member of the Political Affairs Committee and the Economic Affairs Committee of the Union Cabinet ; Governor , International Monetary Fund , the International Bank for Reconstruction and Development , and the Asian Development Bank .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Venkataraman","was","also",",","variously",",","a","member","of","the","Political","Affairs","Committee","and","the","Economic","Affairs","Committee","of","the","Union","Cabinet",";","Governor",",","International","Monetary","Fund",",","the","International","Bank","for","Reconstruction","and","Development",",","and","the","Asian","Development","Bank","."],"labels":["B-politician","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["person","location","event","political_party","election","politician","organization","country"]}
{"id":"431","dataset":"crossner_politics","split":"test","instance":{"id":"431","prompt_labels":"Prior(O) to(O) embarking(O) on(O) his(O) political(O) career(O) ,(O) Tharoor(B-politician) also(O) served(O) on(O) the(O) board(O) of(O) overseers(O) of(O) the(O) Fletcher(B-location) School(I-location) of(I-location) Law(I-location) and(I-location) Diplomacy(I-location) ,(O) the(O) board(O) of(O) trustees(O) of(O) the(O) Aspen(B-organization) Institute(I-organization) ,(O) and(O) the(O) advisory(O) boards(O) of(O) the(O) Indo-American(B-organization) Arts(I-organization) Council(I-organization) ,(O) the(O) American(B-organization) India(I-organization) Foundation(I-organization) ,(O) the(O) World(B-organization) Policy(I-organization) Journal(I-organization) ,(O) the(O) Virtue(B-organization) Foundation(I-organization) ,(O) and(O) the(O) human(O) rights(O) organisation(O) Breakthrough(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, location, election, person, country, politician, organization and O.\nSentence: Prior to embarking on his political career , Tharoor also served on the board of overseers of the Fletcher School of Law and Diplomacy , the board of trustees of the Aspen Institute , and the advisory boards of the Indo-American Arts Council , the American India Foundation , the World Policy Journal , the Virtue Foundation , and the human rights organisation Breakthrough .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Prior","to","embarking","on","his","political","career",",","Tharoor","also","served","on","the","board","of","overseers","of","the","Fletcher","School","of","Law","and","Diplomacy",",","the","board","of","trustees","of","the","Aspen","Institute",",","and","the","advisory","boards","of","the","Indo-American","Arts","Council",",","the","American","India","Foundation",",","the","World","Policy","Journal",",","the","Virtue","Foundation",",","and","the","human","rights","organisation","Breakthrough","."],"labels":["O","O","O","O","O","O","O","O","B-politician","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","I-location","I-location","I-location","O","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","event","location","election","person","country","politician","organization"]}
{"id":"435","dataset":"crossner_politics","split":"test","instance":{"id":"435","prompt_labels":"In(O) the(O) 2009(B-election) Portuguese(I-election) legislative(I-election) election(I-election) ,(O) the(O) party(O) won(O) 21(O) seats(O) ,(O) its(O) most(O) since(O) the(O) 1985(B-election) Portuguese(I-election) legislative(I-election) election(I-election) ,(O) and(O) increased(O) it(O) to(O) 24(O) 2011(B-election) Portuguese(I-election) legislative(I-election) election(I-election) ,(O) leading(O) to(O) it(O) forming(O) a(O) coalition(O) government(O) with(O) the(O) PSD(B-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, location, election, politician, political party, organization, event and O.\nSentence: In the 2009 Portuguese legislative election , the party won 21 seats , its most since the 1985 Portuguese legislative election , and increased it to 24 2011 Portuguese legislative election , leading to it forming a coalition government with the PSD .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","2009","Portuguese","legislative","election",",","the","party","won","21","seats",",","its","most","since","the","1985","Portuguese","legislative","election",",","and","increased","it","to","24","2011","Portuguese","legislative","election",",","leading","to","it","forming","a","coalition","government","with","the","PSD","."],"labels":["O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","B-organization","O"],"target_index":null,"target_label":null},"label_list":["person","country","location","election","politician","political_party","organization","event"]}
{"id":"438","dataset":"crossner_politics","split":"test","instance":{"id":"438","prompt_labels":"After(O) the(O) fall(O) of(O) the(O) Hungarian(B-political party) Democratic(I-political party) Forum(I-political party) conservative(O) government(O) at(O) the(O) following(O) 1994(B-election) Hungarian(I-election) parliamentary(I-election) election(I-election) ,(O) SZDSZ(B-political party) surprised(O) many(O) by(O) entering(O) into(O) a(O) coalition(O) with(O) the(O) Hungarian(B-political party) Socialist(I-political party) Party(I-political party) ((O) MSZP(B-political party) )(O) ,(O) legal(O) successors(O) to(O) the(O) communist(O) Hungarian(B-political party) Socialist(I-political party) Workers(I-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, person, organization, country, election, politician, political party and O.\nSentence: After the fall of the Hungarian Democratic Forum conservative government at the following 1994 Hungarian parliamentary election , SZDSZ surprised many by entering into a coalition with the Hungarian Socialist Party ( MSZP ) , legal successors to the communist Hungarian Socialist Workers Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","the","fall","of","the","Hungarian","Democratic","Forum","conservative","government","at","the","following","1994","Hungarian","parliamentary","election",",","SZDSZ","surprised","many","by","entering","into","a","coalition","with","the","Hungarian","Socialist","Party","(","MSZP",")",",","legal","successors","to","the","communist","Hungarian","Socialist","Workers","Party","."],"labels":["O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-political party","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["event","location","person","organization","country","election","politician","political_party"]}
{"id":"445","dataset":"crossner_politics","split":"test","instance":{"id":"445","prompt_labels":"The(O) CCD(B-political party) was(O) an(O) early(O) ally(O) of(O) Silvio(B-politician) Berlusconi(I-politician) '(O) s(O) Forza(B-political party) Italia(I-political party) in(O) 1994(O) and(O) was(O) part(O) of(O) the(O) centre-right(O) Pole(B-political party) for(I-political party) Freedoms(I-political party) /(O) House(B-political party) of(I-political party) Freedoms(I-political party) since(O) its(O) establishment(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, location, political party, election, politician, organization, country and O.\nSentence: The CCD was an early ally of Silvio Berlusconi ' s Forza Italia in 1994 and was part of the centre-right Pole for Freedoms / House of Freedoms since its establishment .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","CCD","was","an","early","ally","of","Silvio","Berlusconi","'","s","Forza","Italia","in","1994","and","was","part","of","the","centre-right","Pole","for","Freedoms","/","House","of","Freedoms","since","its","establishment","."],"labels":["O","B-political party","O","O","O","O","O","B-politician","I-politician","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","person","location","political_party","election","politician","organization","country"]}
{"id":"447","dataset":"crossner_politics","split":"test","instance":{"id":"447","prompt_labels":"In(O) the(O) 2001(B-election) Italian(I-election) general(I-election) election(I-election) the(O) Christian(B-political party) Democratic(I-political party) Centre(I-political party) ((O) CCD(B-political party) )(O) ,(O) led(O) by(O) Pier(B-politician) Ferdinando(I-politician) Casini(I-politician) ,(O) and(O) the(O) United(B-political party) Christian(I-political party) Democrats(I-political party) ((O) CDU(B-political party) )(O) ,(O) a(O) 1995(O) split(O) from(O) the(O) Italian(B-political party) People(I-political party) 's(I-political party) Party(I-political party) ((O) PPI(B-political party) )(O) led(O) by(O) Rocco(B-politician) Buttiglione(I-politician) were(O) part(O) of(O) the(O) winning(O) centre-right(O) House(B-political party) of(I-political party) Freedoms(I-political party) coalition(O) ,(O) but(O) their(O) joint(O) list(O) won(O) a(O) mere(O) 3.2(O) %(O) of(O) the(O) vote(O) ((O) (O) 2.6pp(O) from(O) 1996(B-election) Italian(I-election) general(I-election) election(I-election) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, person, election, location, event, organization, country, politician and O.\nSentence: In the 2001 Italian general election the Christian Democratic Centre ( CCD ) , led by Pier Ferdinando Casini , and the United Christian Democrats ( CDU ) , a 1995 split from the Italian People 's Party ( PPI ) led by Rocco Buttiglione were part of the winning centre-right House of Freedoms coalition , but their joint list won a mere 3.2 % of the vote (  2.6pp from 1996 Italian general election ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","2001","Italian","general","election","the","Christian","Democratic","Centre","(","CCD",")",",","led","by","Pier","Ferdinando","Casini",",","and","the","United","Christian","Democrats","(","CDU",")",",","a","1995","split","from","the","Italian","People","'s","Party","(","PPI",")","led","by","Rocco","Buttiglione","were","part","of","the","winning","centre-right","House","of","Freedoms","coalition",",","but","their","joint","list","won","a","mere","3.2","%","of","the","vote","(","","2.6pp","from","1996","Italian","general","election",")","."],"labels":["O","O","B-election","I-election","I-election","I-election","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","O","B-politician","I-politician","I-politician","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","B-politician","I-politician","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","person","election","location","event","organization","country","politician"]}
{"id":"449","dataset":"crossner_politics","split":"test","instance":{"id":"449","prompt_labels":"In(O) December(O) 2010(O) the(O) UdC(B-political party) was(O) a(O) founding(O) member(O) of(O) the(O) New(B-political party) Pole(I-political party) for(I-political party) Italy(I-political party) ((O) NPI(B-political party) )(O) ,(O) along(O) with(O) Future(B-political party) and(I-political party) Freedom(I-political party) ((O) FLI(B-political party) )(O) and(O) the(O) Alliance(B-political party) for(I-political party) Italy(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, election, country, organization, event, person, political party, politician and O.\nSentence: In December 2010 the UdC was a founding member of the New Pole for Italy ( NPI ) , along with Future and Freedom ( FLI ) and the Alliance for Italy .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","December","2010","the","UdC","was","a","founding","member","of","the","New","Pole","for","Italy","(","NPI",")",",","along","with","Future","and","Freedom","(","FLI",")","and","the","Alliance","for","Italy","."],"labels":["O","O","O","O","B-political party","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["location","election","country","organization","event","person","political_party","politician"]}
{"id":"450","dataset":"crossner_politics","split":"test","instance":{"id":"450","prompt_labels":"The(O) UdC(B-political party) contested(O) the(O) 2013(B-election) Italian(I-election) general(I-election) election(I-election) as(O) part(O) of(O) the(O) With(B-political party) Monti(I-political party) for(I-political party) Italy(I-political party) coalition(O) ,(O) alongside(O) FLI(B-political party) and(O) Monti(B-politician) 's(O) Civic(B-political party) Choice(I-political party) ((O) SC(B-political party) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, location, politician, political party, country, organization, person, event and O.\nSentence: The UdC contested the 2013 Italian general election as part of the With Monti for Italy coalition , alongside FLI and Monti 's Civic Choice ( SC ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","UdC","contested","the","2013","Italian","general","election","as","part","of","the","With","Monti","for","Italy","coalition",",","alongside","FLI","and","Monti","'s","Civic","Choice","(","SC",")","."],"labels":["O","B-political party","O","O","B-election","I-election","I-election","I-election","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","B-political party","O","B-politician","O","B-political party","I-political party","O","B-political party","O","O"],"target_index":null,"target_label":null},"label_list":["election","location","politician","political_party","country","organization","person","event"]}
{"id":"451","dataset":"crossner_politics","split":"test","instance":{"id":"451","prompt_labels":"In(O) this(O) respect(O) ,(O) Casini(B-politician) and(O) his(O) followers(O) have(O) long(O) tried(O) to(O) form(O) the(O) nucleus(O) of(O) a(O) third(O) force(O) in(O) Italian(O) politics(O) ((O) e.g.(O) :(O) New(B-political party) Pole(I-political party) for(I-political party) Italy(I-political party) ,(O) With(B-political party) Monti(I-political party) for(I-political party) Italy(I-political party) ,(O) Popular(B-political party) Area(I-political party) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, election, politician, location, person, event, country, organization and O.\nSentence: In this respect , Casini and his followers have long tried to form the nucleus of a third force in Italian politics ( e.g. : New Pole for Italy , With Monti for Italy , Popular Area ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","this","respect",",","Casini","and","his","followers","have","long","tried","to","form","the","nucleus","of","a","third","force","in","Italian","politics","(","e.g.",":","New","Pole","for","Italy",",","With","Monti","for","Italy",",","Popular","Area",")","."],"labels":["O","O","O","O","B-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","election","politician","location","person","event","country","organization"]}
{"id":"454","dataset":"crossner_politics","split":"test","instance":{"id":"454","prompt_labels":"For(O) the(O) 2001(B-election) Italian(I-election) general(I-election) election(I-election) the(O) SDI(B-political party) thus(O) formed(O) an(O) unusual(O) alliance(O) ((O) The(B-political party) Sunflower(I-political party) )(O) with(O) the(O) Federation(B-political party) of(I-political party) the(I-political party) Greens(I-political party) ,(O) which(O) was(O) disbanded(O) soon(O) after(O) the(O) election(O) ,(O) due(O) to(O) political(O) divergences(O) and(O) ultimately(O) to(O) the(O) disappointing(O) electoral(O) result(O) :(O) 2.2(O) %(O) of(O) the(O) vote(O) ,(O) while(O) the(O) two(O) parties(O) '(O) combined(O) result(O) in(O) 1999(O) had(O) been(O) 4.0(O) %(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, political party, person, politician, organization, election, country and O.\nSentence: For the 2001 Italian general election the SDI thus formed an unusual alliance ( The Sunflower ) with the Federation of the Greens , which was disbanded soon after the election , due to political divergences and ultimately to the disappointing electoral result : 2.2 % of the vote , while the two parties ' combined result in 1999 had been 4.0 % .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","the","2001","Italian","general","election","the","SDI","thus","formed","an","unusual","alliance","(","The","Sunflower",")","with","the","Federation","of","the","Greens",",","which","was","disbanded","soon","after","the","election",",","due","to","political","divergences","and","ultimately","to","the","disappointing","electoral","result",":","2.2","%","of","the","vote",",","while","the","two","parties","'","combined","result","in","1999","had","been","4.0","%","."],"labels":["O","O","B-election","I-election","I-election","I-election","O","B-political party","O","O","O","O","O","O","B-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","event","political_party","person","politician","organization","election","country"]}
{"id":"455","dataset":"crossner_politics","split":"test","instance":{"id":"455","prompt_labels":"In(O) 2001(O) Claudio(B-politician) Martelli(I-politician) and(O) Bobo(B-politician) Craxi(I-politician) left(O) the(O) party(O) in(O) order(O) to(O) form(O) ,(O) along(O) with(O) Gianni(B-politician) De(I-politician) Michelis(I-politician) ,(O) the(O) New(B-political party) Italian(I-political party) Socialist(I-political party) Party(I-political party) ((O) NPSI(B-political party) )(O) ,(O) which(O) joined(O) the(O) House(B-political party) of(I-political party) Freedoms(I-political party) centre-right(O) coalition(O) ,(O) while(O) in(O) 2004(O) Giorgio(B-politician) Carta(I-politician) left(O) to(O) re-establish(O) the(O) Italian(B-political party) Democratic(I-political party) Socialist(I-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, country, location, person, organization, politician, election and O.\nSentence: In 2001 Claudio Martelli and Bobo Craxi left the party in order to form , along with Gianni De Michelis , the New Italian Socialist Party ( NPSI ) , which joined the House of Freedoms centre-right coalition , while in 2004 Giorgio Carta left to re-establish the Italian Democratic Socialist Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2001","Claudio","Martelli","and","Bobo","Craxi","left","the","party","in","order","to","form",",","along","with","Gianni","De","Michelis",",","the","New","Italian","Socialist","Party","(","NPSI",")",",","which","joined","the","House","of","Freedoms","centre-right","coalition",",","while","in","2004","Giorgio","Carta","left","to","re-establish","the","Italian","Democratic","Socialist","Party","."],"labels":["O","O","B-politician","I-politician","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","I-politician","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["event","political_party","country","location","person","organization","politician","election"]}
{"id":"458","dataset":"crossner_politics","split":"test","instance":{"id":"458","prompt_labels":"At(O) the(O) 1989(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) Italy(I-election) there(O) were(O) two(O) competing(O) green(O) parties(O) :(O) the(O) LV(B-political party) and(O) the(O) Rainbow(B-political party) Greens(I-political party) ((O) VA(B-political party) )(O) ,(O) formed(O) mainly(O) by(O) Radicals(O) ,(O) including(O) Adelaide(B-politician) Aglietta(I-politician) ,(O) Franco(B-politician) Corleone(I-politician) ,(O) Adele(B-politician) Faccio(I-politician) ,(O) Marco(B-politician) Taradash(I-politician) and(O) Francesco(B-politician) Rutelli(I-politician) ,(O) as(O) well(O) as(O) splinters(O) from(O) Proletarian(B-political party) Democracy(I-political party) ,(O) including(O) Mario(B-politician) Capanna(I-politician) ,(O) Guido(B-politician) Pollice(I-politician) ,(O) Gianni(B-politician) Tamino(I-politician) and(O) Edo(B-politician) Ronchi(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, person, politician, event, country, election, location, organization and O.\nSentence: At the 1989 European Parliament election in Italy there were two competing green parties : the LV and the Rainbow Greens ( VA ) , formed mainly by Radicals , including Adelaide Aglietta , Franco Corleone , Adele Faccio , Marco Taradash and Francesco Rutelli , as well as splinters from Proletarian Democracy , including Mario Capanna , Guido Pollice , Gianni Tamino and Edo Ronchi .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["At","the","1989","European","Parliament","election","in","Italy","there","were","two","competing","green","parties",":","the","LV","and","the","Rainbow","Greens","(","VA",")",",","formed","mainly","by","Radicals",",","including","Adelaide","Aglietta",",","Franco","Corleone",",","Adele","Faccio",",","Marco","Taradash","and","Francesco","Rutelli",",","as","well","as","splinters","from","Proletarian","Democracy",",","including","Mario","Capanna",",","Guido","Pollice",",","Gianni","Tamino","and","Edo","Ronchi","."],"labels":["O","O","B-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","B-political party","O","O","B-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","O","O","O","O","O","B-political party","I-political party","O","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["political_party","person","politician","event","country","election","location","organization"]}
{"id":"460","dataset":"crossner_politics","split":"test","instance":{"id":"460","prompt_labels":"Following(O) the(O) 1988(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) he(O) co-authored(O) Election(O) :(O) the(O) issues(O) ,(O) the(O) strategies(O) ,(O) the(O) aftermath(O) with(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) strategiest(O) Michael(B-politician) J.(I-politician) L.(I-politician) Kirby(I-politician) and(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) strategist(O) Hugh(B-politician) Segal(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, person, politician, political party, organization, country, location and O.\nSentence: Following the 1988 Canadian federal election , he co-authored Election : the issues , the strategies , the aftermath with Liberal Party of Canada strategiest Michael J. L. Kirby and Conservative Party of Canada strategist Hugh Segal .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Following","the","1988","Canadian","federal","election",",","he","co-authored","Election",":","the","issues",",","the","strategies",",","the","aftermath","with","Liberal","Party","of","Canada","strategiest","Michael","J.","L.","Kirby","and","Conservative","Party","of","Canada","strategist","Hugh","Segal","."],"labels":["O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-politician","I-politician","I-politician","I-politician","O","B-political party","I-political party","I-political party","I-political party","O","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["election","event","person","politician","political_party","organization","country","location"]}
{"id":"461","dataset":"crossner_politics","split":"test","instance":{"id":"461","prompt_labels":"The(O) World(B-organization) Bank(I-organization) ,(O) the(O) African(B-organization) Development(I-organization) Bank(I-organization) ,(O) the(O) European(B-organization) Investment(I-organization) Bank(I-organization) ,(O) bilateral(O) donors(O) ,(O) and(O) the(O) southern(B-location) African(I-location) power(O) companies(O) ,(O) have(O) all(O) expressed(O) interest(O) in(O) pursuing(O) the(O) project(O) which(O) is(O) estimated(O) to(O) cost(O) US(B-country) $(O) 80(O) billion(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, politician, event, political party, election, person, organization, location and O.\nSentence: The World Bank , the African Development Bank , the European Investment Bank , bilateral donors , and the southern African power companies , have all expressed interest in pursuing the project which is estimated to cost US $ 80 billion .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","World","Bank",",","the","African","Development","Bank",",","the","European","Investment","Bank",",","bilateral","donors",",","and","the","southern","African","power","companies",",","have","all","expressed","interest","in","pursuing","the","project","which","is","estimated","to","cost","US","$","80","billion","."],"labels":["O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","politician","event","political_party","election","person","organization","location"]}
{"id":"462","dataset":"crossner_politics","split":"test","instance":{"id":"462","prompt_labels":"Between(O) the(O) 1935(B-election) Canadian(I-election) federal(I-election) election(I-election) and(O) 1958(B-election) Canadian(I-election) federal(I-election) election(I-election) elections(O) ,(O) the(O) top(O) ranking(O) was(O) consistently(O) held(O) by(O) either(O) the(O) Co-operative(B-political party) Commonwealth(I-political party) Federation(I-political party) or(O) the(O) Labor-Progressive(B-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, election, event, political party, person, politician, location, country and O.\nSentence: Between the 1935 Canadian federal election and 1958 Canadian federal election elections , the top ranking was consistently held by either the Co-operative Commonwealth Federation or the Labor-Progressive Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Between","the","1935","Canadian","federal","election","and","1958","Canadian","federal","election","elections",",","the","top","ranking","was","consistently","held","by","either","the","Co-operative","Commonwealth","Federation","or","the","Labor-Progressive","Party","."],"labels":["O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["organization","election","event","political_party","person","politician","location","country"]}
{"id":"463","dataset":"crossner_politics","split":"test","instance":{"id":"463","prompt_labels":"Boundary(O) changes(O) took(O) place(O) for(O) the(O) 1990(B-election) Colchester(I-election) Borough(I-election) Council(I-election) election(I-election) ,(O) 2003(B-election) Colchester(I-election) Borough(I-election) Council(I-election) election(I-election) and(O) 2016(B-election) Colchester(I-election) Borough(I-election) Council(I-election) election(I-election) elections(O) leading(O) to(O) the(O) whole(O) council(O) being(O) elected(O) on(O) those(O) years(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, election, organization, location, political party, country, politician and O.\nSentence: Boundary changes took place for the 1990 Colchester Borough Council election , 2003 Colchester Borough Council election and 2016 Colchester Borough Council election elections leading to the whole council being elected on those years .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Boundary","changes","took","place","for","the","1990","Colchester","Borough","Council","election",",","2003","Colchester","Borough","Council","election","and","2016","Colchester","Borough","Council","election","elections","leading","to","the","whole","council","being","elected","on","those","years","."],"labels":["O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","event","election","organization","location","political_party","country","politician"]}
{"id":"464","dataset":"crossner_politics","split":"test","instance":{"id":"464","prompt_labels":"Throughout(O) the(O) century(O) ,(O) the(O) state(O) voted(O) for(O) the(O) Federalist(B-political party) Party(I-political party) twice(O) ,(O) the(O) Democratic-Republican(B-political party) Party(I-political party) five(O) times(O) ,(O) the(O) National(B-political party) Republican(I-political party) Party(I-political party) once(O) ,(O) the(O) Whig(B-political party) Party(I-political party) four(O) times(O) ,(O) the(O) Democratic(B-political party) Party(I-political party) ten(O) times(O) ,(O) and(O) the(O) Republican(B-political party) Party(I-political party) three(O) times(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, political party, event, person, country, election, organization and O.\nSentence: Throughout the century , the state voted for the Federalist Party twice , the Democratic-Republican Party five times , the National Republican Party once , the Whig Party four times , the Democratic Party ten times , and the Republican Party three times .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Throughout","the","century",",","the","state","voted","for","the","Federalist","Party","twice",",","the","Democratic-Republican","Party","five","times",",","the","National","Republican","Party","once",",","the","Whig","Party","four","times",",","the","Democratic","Party","ten","times",",","and","the","Republican","Party","three","times","."],"labels":["O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","B-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","O","O","O","O","B-political party","I-political party","O","O","O","O","O","B-political party","I-political party","O","O","O"],"target_index":null,"target_label":null},"label_list":["politician","location","political_party","event","person","country","election","organization"]}
{"id":"466","dataset":"crossner_politics","split":"test","instance":{"id":"466","prompt_labels":"He(O) was(O) the(O) founding(O) member(O) of(O) the(O) Co-operative(B-political party) Resources(I-political party) Centre(I-political party) ,(O) a(O) conservative(O) political(O) group(O) led(O) by(O) Senior(O) Unofficial(O) Member(O) Allen(B-politician) Lee(I-politician) in(O) 1991(O) to(O) counter(O) the(O) liberal(O) emergence(O) of(O) the(O) United(B-political party) Democrats(I-political party) of(I-political party) Hong(I-political party) Kong(I-political party) in(O) the(O) Legislative(B-organization) Council(I-organization) after(O) the(O) 1991(B-election) Hong(I-election) Kong(I-election) legislative(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, country, organization, location, event, political party, politician, person and O.\nSentence: He was the founding member of the Co-operative Resources Centre , a conservative political group led by Senior Unofficial Member Allen Lee in 1991 to counter the liberal emergence of the United Democrats of Hong Kong in the Legislative Council after the 1991 Hong Kong legislative election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","the","founding","member","of","the","Co-operative","Resources","Centre",",","a","conservative","political","group","led","by","Senior","Unofficial","Member","Allen","Lee","in","1991","to","counter","the","liberal","emergence","of","the","United","Democrats","of","Hong","Kong","in","the","Legislative","Council","after","the","1991","Hong","Kong","legislative","election","."],"labels":["O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-organization","I-organization","O","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["election","country","organization","location","event","political_party","politician","person"]}
{"id":"471","dataset":"crossner_politics","split":"test","instance":{"id":"471","prompt_labels":"With(O) two(O) homogenous(O) groups(O) ,(O) Bouckaert(O) concluded(O) ,(O) a(O) left-wing(O) bloc(O) around(O) the(O) Socialistische(B-political party) Partij(I-political party) Anders(I-political party) ,(O) ACV(B-political party) and(O) Groen(B-political party) !(I-political party) ,(O) and(O) a(O) right(O) wing(O) bloc(O) with(O) the(O) VLD(B-political party) ,(O) the(O) right(O) wing(O) of(O) the(O) Christen-Democratisch(B-political party) en(I-political party) Vlaams(I-political party) ,(O) the(O) New(B-political party) Flemish(I-political party) Alliance(I-political party) and(O) the(O) Vlaams(B-political party) Belang(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, political party, country, election, location, event, organization, person and O.\nSentence: With two homogenous groups , Bouckaert concluded , a left-wing bloc around the Socialistische Partij Anders , ACV and Groen ! , and a right wing bloc with the VLD , the right wing of the Christen-Democratisch en Vlaams , the New Flemish Alliance and the Vlaams Belang .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["With","two","homogenous","groups",",","Bouckaert","concluded",",","a","left-wing","bloc","around","the","Socialistische","Partij","Anders",",","ACV","and","Groen","!",",","and","a","right","wing","bloc","with","the","VLD",",","the","right","wing","of","the","Christen-Democratisch","en","Vlaams",",","the","New","Flemish","Alliance","and","the","Vlaams","Belang","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","B-political party","I-political party","O","O","O","O","O","O","O","O","B-political party","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["politician","political_party","country","election","location","event","organization","person"]}
{"id":"474","dataset":"crossner_politics","split":"test","instance":{"id":"474","prompt_labels":"That(O) was(O) TRUE(O) ,(O) for(O) example(O) ,(O) for(O) Synaspismos(B-political party) ,(O) Renewing(B-political party) Communist(I-political party) Ecological(I-political party) Left(I-political party) ,(O) Ecosocialists(B-political party) of(I-political party) Greece(I-political party) and(O) Unitary(B-political party) Movement(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, political party, person, organization, politician, country, location and O.\nSentence: That was TRUE , for example , for Synaspismos , Renewing Communist Ecological Left , Ecosocialists of Greece and Unitary Movement .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["That","was","TRUE",",","for","example",",","for","Synaspismos",",","Renewing","Communist","Ecological","Left",",","Ecosocialists","of","Greece","and","Unitary","Movement","."],"labels":["O","O","O","O","O","O","O","O","B-political party","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["election","event","political_party","person","organization","politician","country","location"]}
{"id":"478","dataset":"crossner_politics","split":"test","instance":{"id":"478","prompt_labels":"Over(O) the(O) course(O) of(O) the(O) next(O) 9(O) years(O) ,(O) she(O) would(O) run(O) in(O) the(O) 1999(B-election) Ontario(I-election) general(I-election) election(I-election) and(O) the(O) 2003(B-election) Ontario(I-election) general(I-election) election(I-election) ,(O) the(O) Hamilton(B-election) municipal(I-election) election(I-election) of(I-election) 2000(I-election) and(O) two(O) federal(O) elections(O) in(O) 1997(B-election) Canadian(I-election) federal(I-election) election(I-election) and(O) 2004(B-election) Canadian(I-election) federal(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, politician, person, political party, location, election, event and O.\nSentence: Over the course of the next 9 years , she would run in the 1999 Ontario general election and the 2003 Ontario general election , the Hamilton municipal election of 2000 and two federal elections in 1997 Canadian federal election and 2004 Canadian federal election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Over","the","course","of","the","next","9","years",",","she","would","run","in","the","1999","Ontario","general","election","and","the","2003","Ontario","general","election",",","the","Hamilton","municipal","election","of","2000","and","two","federal","elections","in","1997","Canadian","federal","election","and","2004","Canadian","federal","election","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["organization","country","politician","person","political_party","location","election","event"]}
{"id":"479","dataset":"crossner_politics","split":"test","instance":{"id":"479","prompt_labels":"In(O) the(O) 2006(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) Simon(B-politician) Strelchik(I-politician) achieved(O) a(O) vote(O) increase(O) of(O) 25(O) %(O) ,(O) to(O) 4405(O) votes(O) ,(O) against(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) Susan(B-politician) Kadis(I-politician) and(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) Anthony(B-politician) Reale(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, event, country, location, person, organization, political party and O.\nSentence: In the 2006 Canadian federal election , Simon Strelchik achieved a vote increase of 25 % , to 4405 votes , against Liberal Party of Canada Susan Kadis and Conservative Party of Canada Anthony Reale .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","2006","Canadian","federal","election",",","Simon","Strelchik","achieved","a","vote","increase","of","25","%",",","to","4405","votes",",","against","Liberal","Party","of","Canada","Susan","Kadis","and","Conservative","Party","of","Canada","Anthony","Reale","."],"labels":["O","O","B-election","I-election","I-election","I-election","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","B-politician","I-politician","O","B-political party","I-political party","I-political party","I-political party","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["election","politician","event","country","location","person","organization","political_party"]}
{"id":"480","dataset":"crossner_politics","split":"test","instance":{"id":"480","prompt_labels":"Since(O) the(O) publication(O) of(O) this(O) paper(O) ,(O) the(O) growth(O) diagnostics(O) strategy(O) has(O) been(O) adopted(O) by(O) a(O) number(O) of(O) international(O) institutions(O) including(O) the(O) World(B-organization) Bank(I-organization) ,(O) the(O) Inter-American(B-organization) Development(I-organization) Bank(I-organization) ,(O) the(O) Asian(B-organization) Development(I-organization) Bank(I-organization) ,(O) the(O) UK(B-country) 's(O) Department(B-organization) for(I-organization) International(I-organization) Development(I-organization) and(O) the(O) Millennium(B-organization) Challenge(I-organization) Corporation(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, election, country, location, political party, event, organization and O.\nSentence: Since the publication of this paper , the growth diagnostics strategy has been adopted by a number of international institutions including the World Bank , the Inter-American Development Bank , the Asian Development Bank , the UK 's Department for International Development and the Millennium Challenge Corporation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Since","the","publication","of","this","paper",",","the","growth","diagnostics","strategy","has","been","adopted","by","a","number","of","international","institutions","including","the","World","Bank",",","the","Inter-American","Development","Bank",",","the","Asian","Development","Bank",",","the","UK","'s","Department","for","International","Development","and","the","Millennium","Challenge","Corporation","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-country","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["politician","person","election","country","location","political_party","event","organization"]}
{"id":"481","dataset":"crossner_politics","split":"test","instance":{"id":"481","prompt_labels":"She(O) was(O) re-elected(O) in(O) the(O) 2008(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) but(O) was(O) defeated(O) in(O) the(O) 2011(B-election) Canadian(I-election) federal(I-election) election(I-election) by(O) Raymond(B-politician) Ct(I-politician) of(O) the(O) New(B-political party) Democratic(I-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, organization, political party, location, country, person, event and O.\nSentence: She was re-elected in the 2008 Canadian federal election , but was defeated in the 2011 Canadian federal election by Raymond Ct of the New Democratic Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","was","re-elected","in","the","2008","Canadian","federal","election",",","but","was","defeated","in","the","2011","Canadian","federal","election","by","Raymond","Ct","of","the","New","Democratic","Party","."],"labels":["O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-politician","I-politician","O","O","B-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["politician","election","organization","political_party","location","country","person","event"]}
{"id":"483","dataset":"crossner_politics","split":"test","instance":{"id":"483","prompt_labels":"Party(O) leader(O) Peter(B-politician) Dunne(I-politician) was(O) re-elected(O) into(O) the(O) hari(B-location) seat(O) in(O) 2008(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ,(O) 2011(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ,(O) and(O) 2014(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ,(O) becoming(O) a(O) coalition(O) partner(O) with(O) National(O) ,(O) despite(O) receiving(O) under(O) 1(O) %(O) of(O) the(O) party(O) vote(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, location, country, organization, person, politician, election and O.\nSentence: Party leader Peter Dunne was re-elected into the hari seat in 2008 New Zealand general election , 2011 New Zealand general election , and 2014 New Zealand general election , becoming a coalition partner with National , despite receiving under 1 % of the party vote .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Party","leader","Peter","Dunne","was","re-elected","into","the","hari","seat","in","2008","New","Zealand","general","election",",","2011","New","Zealand","general","election",",","and","2014","New","Zealand","general","election",",","becoming","a","coalition","partner","with","National",",","despite","receiving","under","1","%","of","the","party","vote","."],"labels":["O","O","B-politician","I-politician","O","O","O","O","B-location","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","political_party","location","country","organization","person","politician","election"]}
{"id":"486","dataset":"crossner_politics","split":"test","instance":{"id":"486","prompt_labels":"Members(O) of(O) the(O) 1948(B-political party) Progressive(I-political party) Party(I-political party) ,(O) however(O) ,(O) have(O) joined(O) the(O) later(O) state(O) Progressive(B-political party) Parties(I-political party) ,(O) thus(O) linking(O) the(O) 1948-1960s(O) group(O) to(O) the(O) Vermont(B-political party) Progressive(I-political party) Party(I-political party) ,(O) Wisconsin(B-political party) Progressive(I-political party) Party(I-political party) ,(O) Minnesota(B-political party) Progressive(I-political party) Party(I-political party) ,(O) California(B-political party) Progressive(I-political party) Party(I-political party) ,(O) Oregon(B-political party) Progressive(I-political party) Party(I-political party) ,(O) and(O) Washington(B-political party) Progressive(I-political party) Party(I-political party) ,(O) as(O) well(O) as(O) the(O) Citizens(B-political party) Party(I-political party) of(O) the(O) 1980s(O) and(O) 90s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, person, country, election, event, location, politician and O.\nSentence: Members of the 1948 Progressive Party , however , have joined the later state Progressive Parties , thus linking the 1948-1960s group to the Vermont Progressive Party , Wisconsin Progressive Party , Minnesota Progressive Party , California Progressive Party , Oregon Progressive Party , and Washington Progressive Party , as well as the Citizens Party of the 1980s and 90s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Members","of","the","1948","Progressive","Party",",","however",",","have","joined","the","later","state","Progressive","Parties",",","thus","linking","the","1948-1960s","group","to","the","Vermont","Progressive","Party",",","Wisconsin","Progressive","Party",",","Minnesota","Progressive","Party",",","California","Progressive","Party",",","Oregon","Progressive","Party",",","and","Washington","Progressive","Party",",","as","well","as","the","Citizens","Party","of","the","1980s","and","90s","."],"labels":["O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","political_party","person","country","election","event","location","politician"]}
{"id":"489","dataset":"crossner_politics","split":"test","instance":{"id":"489","prompt_labels":"He(O) ran(O) for(O) a(O) seat(O) in(O) the(O) National(B-organization) Assembly(I-organization) of(I-organization) Quebec(I-organization) in(O) the(O) Montreal-based(B-location) district(I-location) of(O) Mercier(B-location) against(O) Daniel(B-politician) Turp(I-politician) of(O) the(O) Parti(B-political party) Qubcois(I-political party) in(O) the(O) 2003(B-election) Quebec(I-election) general(I-election) election(I-election) ,(O) 2007(B-election) Quebec(I-election) general(I-election) election(I-election) and(O) 2008(B-election) Quebec(I-election) general(I-election) election(I-election) elections(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, location, country, politician, election, organization, event, person and O.\nSentence: He ran for a seat in the National Assembly of Quebec in the Montreal-based district of Mercier against Daniel Turp of the Parti Qubcois in the 2003 Quebec general election , 2007 Quebec general election and 2008 Quebec general election elections .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","ran","for","a","seat","in","the","National","Assembly","of","Quebec","in","the","Montreal-based","district","of","Mercier","against","Daniel","Turp","of","the","Parti","Qubcois","in","the","2003","Quebec","general","election",",","2007","Quebec","general","election","and","2008","Quebec","general","election","elections","."],"labels":["O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-location","I-location","O","B-location","O","B-politician","I-politician","O","O","B-political party","I-political party","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","location","country","politician","election","organization","event","person"]}
{"id":"491","dataset":"crossner_politics","split":"test","instance":{"id":"491","prompt_labels":"Jobbik(O) has(O) been(O) a(O) founding(O) member(O) of(O) the(O) Alliance(B-political party) of(I-political party) European(I-political party) National(I-political party) Movements(I-political party) ,(O) alongside(O) the(O) French(O) National(B-political party) Front(I-political party) ,(O) the(O) Ukrainian(O) Svoboda(B-political party) ,(O) Italy(B-country) 's(O) Tricolour(B-political party) Flame(I-political party) ,(O) the(O) British(B-political party) National(I-political party) Party(I-political party) ,(O) the(O) Swedish(B-political party) National(I-political party) Democrats(I-political party) ,(O) the(O) Finnish(O) Blue(B-political party) and(I-political party) White(I-political party) Front(I-political party) ,(O) the(O) Portuguese(O) National(B-political party) Renovator(I-political party) Party(I-political party) ,(O) and(O) the(O) Spanish(O) Republican(B-political party) Social(I-political party) Movement(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, election, person, politician, political party, event, location, country and O.\nSentence: Jobbik has been a founding member of the Alliance of European National Movements , alongside the French National Front , the Ukrainian Svoboda , Italy 's Tricolour Flame , the British National Party , the Swedish National Democrats , the Finnish Blue and White Front , the Portuguese National Renovator Party , and the Spanish Republican Social Movement .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Jobbik","has","been","a","founding","member","of","the","Alliance","of","European","National","Movements",",","alongside","the","French","National","Front",",","the","Ukrainian","Svoboda",",","Italy","'s","Tricolour","Flame",",","the","British","National","Party",",","the","Swedish","National","Democrats",",","the","Finnish","Blue","and","White","Front",",","the","Portuguese","National","Renovator","Party",",","and","the","Spanish","Republican","Social","Movement","."],"labels":["O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","O","O","O","B-political party","O","B-country","O","B-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["organization","election","person","politician","political_party","event","location","country"]}
{"id":"493","dataset":"crossner_politics","split":"test","instance":{"id":"493","prompt_labels":"Butcher(B-politician) was(O) a(O) member(O) of(O) the(O) centre-right(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) before(O) that(O) party(O) merged(O) with(O) the(O) more(O) right-wing(O) Canadian(B-political party) Alliance(I-political party) to(O) create(O) the(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) in(O) 2003(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, election, location, event, person, politician, political party, country and O.\nSentence: Butcher was a member of the centre-right Progressive Conservative Party of Canada before that party merged with the more right-wing Canadian Alliance to create the Conservative Party of Canada in 2003 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Butcher","was","a","member","of","the","centre-right","Progressive","Conservative","Party","of","Canada","before","that","party","merged","with","the","more","right-wing","Canadian","Alliance","to","create","the","Conservative","Party","of","Canada","in","2003","."],"labels":["B-politician","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","election","location","event","person","politician","political_party","country"]}
{"id":"494","dataset":"crossner_politics","split":"test","instance":{"id":"494","prompt_labels":"The(O) People(O) 's(O) party(O) remained(O) the(O) dominant(O) power(O) of(O) the(O) right(O) until(O) 1950(B-election) Greek(I-election) legislative(I-election) election(I-election) ,(O) but(O) ,(O) in(O) 1951(B-election) Greek(I-election) legislative(I-election) election(I-election) ,(O) the(O) Greek(B-political party) Rally(I-political party) of(O) retired(O) General(O) Alexandros(B-politician) Papagos(I-politician) swept(O) the(O) election(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, person, politician, political party, country, organization, location and O.\nSentence: The People 's party remained the dominant power of the right until 1950 Greek legislative election , but , in 1951 Greek legislative election , the Greek Rally of retired General Alexandros Papagos swept the election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","People","'s","party","remained","the","dominant","power","of","the","right","until","1950","Greek","legislative","election",",","but",",","in","1951","Greek","legislative","election",",","the","Greek","Rally","of","retired","General","Alexandros","Papagos","swept","the","election","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","B-election","I-election","I-election","I-election","O","O","B-political party","I-political party","O","O","O","B-politician","I-politician","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","election","person","politician","political_party","country","organization","location"]}
{"id":"495","dataset":"crossner_politics","split":"test","instance":{"id":"495","prompt_labels":"Incumbent(O) Democrat(O) Zell(B-politician) Miller(I-politician) ,(O) who(O) was(O) appointed(O) by(O) Governor(O) Roy(B-politician) Barnes(I-politician) to(O) replace(O) the(O) late(O) Paul(B-politician) Coverdell(I-politician) ,(O) won(O) re-election(O) to(O) serve(O) the(O) remainder(O) of(O) the(O) term(O) ,(O) beating(O) Republican(O) Mack(B-politician) Mattingly(I-politician) ,(O) former(O) Ambassador(O) to(O) Seychelles(B-country) and(O) former(O) U.S.(B-country) Senator(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, organization, election, political party, location, event, politician and O.\nSentence: Incumbent Democrat Zell Miller , who was appointed by Governor Roy Barnes to replace the late Paul Coverdell , won re-election to serve the remainder of the term , beating Republican Mack Mattingly , former Ambassador to Seychelles and former U.S. Senator .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Incumbent","Democrat","Zell","Miller",",","who","was","appointed","by","Governor","Roy","Barnes","to","replace","the","late","Paul","Coverdell",",","won","re-election","to","serve","the","remainder","of","the","term",",","beating","Republican","Mack","Mattingly",",","former","Ambassador","to","Seychelles","and","former","U.S.","Senator","."],"labels":["O","O","B-politician","I-politician","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","B-country","O","O","B-country","O","O"],"target_index":null,"target_label":null},"label_list":["person","country","organization","election","political_party","location","event","politician"]}
{"id":"496","dataset":"crossner_politics","split":"test","instance":{"id":"496","prompt_labels":"Incumbent(O) Republican(O) Slade(B-politician) Gorton(I-politician) ran(O) for(O) a(O) third(O) consecutive(O) term(O) ((O) fourth(O) overall(O) )(O) ,(O) but(O) was(O) unseated(O) for(O) a(O) second(O) time(O) ((O) the(O) first(O) being(O) 1986(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Washington(I-election) )(O) by(O) Democratic(O) candidate(O) ,(O) former(O) Congresswoman(O) Maria(B-politician) Cantwell(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, political party, event, organization, politician, election, country and O.\nSentence: Incumbent Republican Slade Gorton ran for a third consecutive term ( fourth overall ) , but was unseated for a second time ( the first being 1986 United States Senate election in Washington ) by Democratic candidate , former Congresswoman Maria Cantwell .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Incumbent","Republican","Slade","Gorton","ran","for","a","third","consecutive","term","(","fourth","overall",")",",","but","was","unseated","for","a","second","time","(","the","first","being","1986","United","States","Senate","election","in","Washington",")","by","Democratic","candidate",",","former","Congresswoman","Maria","Cantwell","."],"labels":["O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["person","location","political_party","event","organization","politician","election","country"]}
{"id":"499","dataset":"crossner_politics","split":"test","instance":{"id":"499","prompt_labels":"The(O) party(O) ,(O) in(O) alliance(O) with(O) The(B-political party) Greens(I-political party) obtained(O) about(O) 14(O) %(O) of(O) the(O) vote(O) in(O) the(O) 1992(B-election) French(I-election) regional(I-election) elections(I-election) ;(O) but(O) the(O) 1993(B-election) French(I-election) legislative(I-election) election(I-election) was(O) disappointing(O) for(O) the(O) Green-GE(B-organization) alliance(I-organization) ,(O) as(O) it(O) failed(O) to(O) win(O) any(O) seats(O) and(O) won(O) only(O) 7(O) %(O) ((O) other(O) ecologist(O) parties(O) brought(O) the(O) score(O) up(O) to(O) 11(O) %(O) )(O) ,(O) when(O) polls(O) had(O) given(O) them(O) up(O) to(O) 16(O) %(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, election, political party, country, location, organization, politician and O.\nSentence: The party , in alliance with The Greens obtained about 14 % of the vote in the 1992 French regional elections ; but the 1993 French legislative election was disappointing for the Green-GE alliance , as it failed to win any seats and won only 7 % ( other ecologist parties brought the score up to 11 % ) , when polls had given them up to 16 % .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","party",",","in","alliance","with","The","Greens","obtained","about","14","%","of","the","vote","in","the","1992","French","regional","elections",";","but","the","1993","French","legislative","election","was","disappointing","for","the","Green-GE","alliance",",","as","it","failed","to","win","any","seats","and","won","only","7","%","(","other","ecologist","parties","brought","the","score","up","to","11","%",")",",","when","polls","had","given","them","up","to","16","%","."],"labels":["O","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","person","election","political_party","country","location","organization","politician"]}
{"id":"501","dataset":"crossner_politics","split":"test","instance":{"id":"501","prompt_labels":"In(O) 1968(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) Alabama(B-location) supported(O) native(O) son(O) and(O) American(B-political party) Independent(I-political party) Party(I-political party) candidate(O) George(B-politician) Wallace(I-politician) over(O) both(O) Richard(B-politician) Nixon(I-politician) and(O) Hubert(B-politician) Humphrey(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, election, event, person, politician, location, country, political party and O.\nSentence: In 1968 United States presidential election , Alabama supported native son and American Independent Party candidate George Wallace over both Richard Nixon and Hubert Humphrey .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1968","United","States","presidential","election",",","Alabama","supported","native","son","and","American","Independent","Party","candidate","George","Wallace","over","both","Richard","Nixon","and","Hubert","Humphrey","."],"labels":["O","B-election","I-election","I-election","I-election","I-election","O","B-location","O","O","O","O","B-political party","I-political party","I-political party","O","B-politician","I-politician","O","O","B-politician","I-politician","O","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["organization","election","event","person","politician","location","country","political_party"]}
{"id":"502","dataset":"crossner_politics","split":"test","instance":{"id":"502","prompt_labels":"Republican(O) Paul(B-politician) Coverdell(I-politician) narrowly(O) unseated(O) Democrat(O) Wyche(B-politician) Fowler(I-politician) in(O) Georgia(B-location) 's(O) other(O) United(B-country) States(I-country) Senate(O) seat(O) in(O) 1992(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Georgia(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, election, politician, political party, country, person, event, organization and O.\nSentence: Republican Paul Coverdell narrowly unseated Democrat Wyche Fowler in Georgia 's other United States Senate seat in 1992 United States Senate election in Georgia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Republican","Paul","Coverdell","narrowly","unseated","Democrat","Wyche","Fowler","in","Georgia","'s","other","United","States","Senate","seat","in","1992","United","States","Senate","election","in","Georgia","."],"labels":["O","B-politician","I-politician","O","O","O","B-politician","I-politician","O","B-location","O","O","B-country","I-country","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["location","election","politician","political_party","country","person","event","organization"]}
{"id":"504","dataset":"crossner_politics","split":"test","instance":{"id":"504","prompt_labels":"Incumbent(O) Democrat(O) Max(B-politician) Baucus(I-politician) ,(O) who(O) was(O) first(O) elected(O) in(O) 1978(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Montana(I-election) and(O) was(O) re-elected(O) in(O) 1984(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Montana(I-election) and(O) 1990(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Montana(I-election) ,(O) ran(O) for(O) re-election(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, election, country, politician, political party, event, organization and O.\nSentence: Incumbent Democrat Max Baucus , who was first elected in 1978 United States Senate election in Montana and was re-elected in 1984 United States Senate election in Montana and 1990 United States Senate election in Montana , ran for re-election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Incumbent","Democrat","Max","Baucus",",","who","was","first","elected","in","1978","United","States","Senate","election","in","Montana","and","was","re-elected","in","1984","United","States","Senate","election","in","Montana","and","1990","United","States","Senate","election","in","Montana",",","ran","for","re-election","."],"labels":["O","O","B-politician","I-politician","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","location","election","country","politician","political_party","event","organization"]}
{"id":"505","dataset":"crossner_politics","split":"test","instance":{"id":"505","prompt_labels":"The(O) election(O) was(O) a(O) rematch(O) of(O) the(O) 1990(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) North(I-election) Carolina(I-election) :(O) between(O) the(O) Republican(O) incumbent(O) Jesse(B-politician) Helms(I-politician) and(O) the(O) Democratic(O) nominee(O) Harvey(B-politician) Gantt(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, person, political party, politician, location, organization, country, event and O.\nSentence: The election was a rematch of the 1990 United States Senate election in North Carolina : between the Republican incumbent Jesse Helms and the Democratic nominee Harvey Gantt .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","election","was","a","rematch","of","the","1990","United","States","Senate","election","in","North","Carolina",":","between","the","Republican","incumbent","Jesse","Helms","and","the","Democratic","nominee","Harvey","Gantt","."],"labels":["O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O","B-politician","I-politician","O","O","O","O","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["election","person","political_party","politician","location","organization","country","event"]}
{"id":"506","dataset":"crossner_politics","split":"test","instance":{"id":"506","prompt_labels":"Warner(B-politician) was(O) endorsed(O) by(O) such(O) notable(O) figures(O) as(O) Bob(B-politician) Dole(I-politician) ,(O) George(B-politician) H.W.(I-politician) Bush(I-politician) ,(O) and(O) Colin(B-politician) Powell(I-politician) ,(O) while(O) Miller(B-politician) was(O) endorsed(O) by(O) the(O) National(B-organization) Rifle(I-organization) Association(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, event, election, person, political party, location, country and O.\nSentence: Warner was endorsed by such notable figures as Bob Dole , George H.W. Bush , and Colin Powell , while Miller was endorsed by the National Rifle Association .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Warner","was","endorsed","by","such","notable","figures","as","Bob","Dole",",","George","H.W.","Bush",",","and","Colin","Powell",",","while","Miller","was","endorsed","by","the","National","Rifle","Association","."],"labels":["B-politician","O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","I-politician","O","O","B-politician","I-politician","O","O","B-politician","O","O","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["politician","organization","event","election","person","political_party","location","country"]}
{"id":"507","dataset":"crossner_politics","split":"test","instance":{"id":"507","prompt_labels":"In(O) the(O) 1990(B-election) California(I-election) gubernatorial(I-election) election(I-election) ,(O) Republican(O) senator(O) Pete(B-politician) Wilson(I-politician) had(O) beaten(O) Democrat(O) Dianne(B-politician) Feinstein(I-politician) for(O) governor(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, election, person, location, event, organization, political party and O.\nSentence: In the 1990 California gubernatorial election , Republican senator Pete Wilson had beaten Democrat Dianne Feinstein for governor .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","1990","California","gubernatorial","election",",","Republican","senator","Pete","Wilson","had","beaten","Democrat","Dianne","Feinstein","for","governor","."],"labels":["O","O","B-election","I-election","I-election","I-election","O","O","O","B-politician","I-politician","O","O","O","B-politician","I-politician","O","O","O"],"target_index":null,"target_label":null},"label_list":["politician","country","election","person","location","event","organization","political_party"]}
{"id":"508","dataset":"crossner_politics","split":"test","instance":{"id":"508","prompt_labels":"Braun(B-politician) ((O) whose(O) victory(O) coincided(O) with(O) Bill(B-politician) Clinton(I-politician) '(O) s(O) win(O) in(O) the(O) 1992(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) and(O) 1992(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) in(I-election) Illinois(I-election) )(O) made(O) history(O) in(O) this(O) election(O) by(O) becoming(O) the(O) first(O) African-American(O) woman(O) ever(O) elected(O) to(O) the(O) U.S(B-organization) Senate(I-organization) ,(O) and(O) also(O) the(O) first(O) African-American(O) elected(O) to(O) the(O) U.S(B-organization) Senate(I-organization) as(O) a(O) Democrat(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, election, political party, location, country, event, politician and O.\nSentence: Braun ( whose victory coincided with Bill Clinton ' s win in the 1992 United States presidential election and 1992 United States presidential election in Illinois ) made history in this election by becoming the first African-American woman ever elected to the U.S Senate , and also the first African-American elected to the U.S Senate as a Democrat .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Braun","(","whose","victory","coincided","with","Bill","Clinton","'","s","win","in","the","1992","United","States","presidential","election","and","1992","United","States","presidential","election","in","Illinois",")","made","history","in","this","election","by","becoming","the","first","African-American","woman","ever","elected","to","the","U.S","Senate",",","and","also","the","first","African-American","elected","to","the","U.S","Senate","as","a","Democrat","."],"labels":["B-politician","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","person","election","political_party","location","country","event","politician"]}
{"id":"509","dataset":"crossner_politics","split":"test","instance":{"id":"509","prompt_labels":"These(O) expulsions(O) paralleled(O) similar(O) efforts(O) to(O) purge(O) the(O) Soviet(B-political party) Communist(I-political party) Party(I-political party) of(O) followers(O) of(O) Nikolai(B-politician) Bukharin(I-politician) ,(O) Alexei(B-politician) Rykov(I-politician) and(O) Mikhail(B-politician) Tomsky(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, event, election, politician, political party, location, organization and O.\nSentence: These expulsions paralleled similar efforts to purge the Soviet Communist Party of followers of Nikolai Bukharin , Alexei Rykov and Mikhail Tomsky .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["These","expulsions","paralleled","similar","efforts","to","purge","the","Soviet","Communist","Party","of","followers","of","Nikolai","Bukharin",",","Alexei","Rykov","and","Mikhail","Tomsky","."],"labels":["O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["person","country","event","election","politician","political_party","location","organization"]}
{"id":"510","dataset":"crossner_politics","split":"test","instance":{"id":"510","prompt_labels":"The(O) mission(O) was(O) named(O) after(O) and(O) headed(O) by(O) Iwakura(B-politician) Tomomi(I-politician) in(O) the(O) role(O) of(O) extraordinary(O) and(O) plenipotentiary(O) ambassador(O) ,(O) assisted(O) by(O) four(O) vice-ambassadors(O) ,(O) three(O) of(O) whom(O) ((O) kubo(B-politician) Toshimichi(I-politician) ,(O) Kido(B-politician) Takayoshi(I-politician) ,(O) and(O) It(B-politician) Hirobumi(I-politician) )(O) were(O) also(O) ministers(O) in(O) the(O) Japanese(O) government(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, organization, country, politician, political party, election, location and O.\nSentence: The mission was named after and headed by Iwakura Tomomi in the role of extraordinary and plenipotentiary ambassador , assisted by four vice-ambassadors , three of whom ( kubo Toshimichi , Kido Takayoshi , and It Hirobumi ) were also ministers in the Japanese government .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","mission","was","named","after","and","headed","by","Iwakura","Tomomi","in","the","role","of","extraordinary","and","plenipotentiary","ambassador",",","assisted","by","four","vice-ambassadors",",","three","of","whom","(","kubo","Toshimichi",",","Kido","Takayoshi",",","and","It","Hirobumi",")","were","also","ministers","in","the","Japanese","government","."],"labels":["O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","event","organization","country","politician","political_party","election","location"]}
{"id":"512","dataset":"crossner_politics","split":"test","instance":{"id":"512","prompt_labels":"Incumbent(O) John(B-politician) Melcher(I-politician) ,(O) who(O) was(O) first(O) elected(O) to(O) the(O) Senate(B-organization) in(O) 1976(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Montana(I-election) and(O) was(O) re-elected(O) in(O) 1982(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Montana(I-election) ,(O) ran(O) for(O) re-election(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, country, election, political party, politician, event, location and O.\nSentence: Incumbent John Melcher , who was first elected to the Senate in 1976 United States Senate election in Montana and was re-elected in 1982 United States Senate election in Montana , ran for re-election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Incumbent","John","Melcher",",","who","was","first","elected","to","the","Senate","in","1976","United","States","Senate","election","in","Montana","and","was","re-elected","in","1982","United","States","Senate","election","in","Montana",",","ran","for","re-election","."],"labels":["O","B-politician","I-politician","O","O","O","O","O","O","O","B-organization","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","organization","country","election","political_party","politician","event","location"]}
{"id":"515","dataset":"crossner_politics","split":"test","instance":{"id":"515","prompt_labels":"Incumbent(O) Democrat(O) Alan(B-politician) Cranston(I-politician) easily(O) won(O) re-election(O) to(O) a(O) third(O) term(O) over(O) Paul(B-politician) Gann(I-politician) ,(O) political(O) activist(O) ,(O) even(O) as(O) the(O) state(O) 's(O) former(O) Republican(O) governor(O) ,(O) Ronald(B-politician) Reagan(I-politician) ,(O) claimed(O) a(O) landslide(O) victory(O) in(O) the(O) 1980(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, politician, person, political party, election, event, country and O.\nSentence: Incumbent Democrat Alan Cranston easily won re-election to a third term over Paul Gann , political activist , even as the state 's former Republican governor , Ronald Reagan , claimed a landslide victory in the 1980 United States presidential election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Incumbent","Democrat","Alan","Cranston","easily","won","re-election","to","a","third","term","over","Paul","Gann",",","political","activist",",","even","as","the","state","'s","former","Republican","governor",",","Ronald","Reagan",",","claimed","a","landslide","victory","in","the","1980","United","States","presidential","election","."],"labels":["O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["location","organization","politician","person","political_party","election","event","country"]}
{"id":"516","dataset":"crossner_politics","split":"test","instance":{"id":"516","prompt_labels":"Republican(O) incumbent(O) Bob(B-politician) Packwood(I-politician) was(O) re-elected(O) to(O) a(O) third(O) term(O) ,(O) defeating(O) Democratic(O) state(O) senator(O) Ted(B-politician) Kulongoski(I-politician) and(O) Libertarian(O) Tonie(B-politician) Nathan(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, organization, country, political party, person, election, event and O.\nSentence: Republican incumbent Bob Packwood was re-elected to a third term , defeating Democratic state senator Ted Kulongoski and Libertarian Tonie Nathan .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Republican","incumbent","Bob","Packwood","was","re-elected","to","a","third","term",",","defeating","Democratic","state","senator","Ted","Kulongoski","and","Libertarian","Tonie","Nathan","."],"labels":["O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["politician","location","organization","country","political_party","person","election","event"]}
{"id":"517","dataset":"crossner_politics","split":"test","instance":{"id":"517","prompt_labels":"He(O) was(O) re-elected(O) in(O) 1997(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2000(B-election) Canadian(I-election) federal(I-election) election(I-election) ((O) as(O) a(O) member(O) of(O) the(O) Canadian(B-political party) Alliance(I-political party) )(O) ,(O) 2004(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2006(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) and(O) 2008(B-election) Canadian(I-election) federal(I-election) election(I-election) ((O) as(O) a(O) Conservative(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, country, location, event, organization, political party, person, politician and O.\nSentence: He was re-elected in 1997 Canadian federal election , 2000 Canadian federal election ( as a member of the Canadian Alliance ) , 2004 Canadian federal election , 2006 Canadian federal election , and 2008 Canadian federal election ( as a Conservative ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","re-elected","in","1997","Canadian","federal","election",",","2000","Canadian","federal","election","(","as","a","member","of","the","Canadian","Alliance",")",",","2004","Canadian","federal","election",",","2006","Canadian","federal","election",",","and","2008","Canadian","federal","election","(","as","a","Conservative",")","."],"labels":["O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","B-political party","I-political party","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","country","location","event","organization","political_party","person","politician"]}
{"id":"519","dataset":"crossner_politics","split":"test","instance":{"id":"519","prompt_labels":"Incumbent(O) Democrat(O) Lee(B-politician) Metcalf(I-politician) ,(O) who(O) was(O) first(O) elected(O) to(O) the(O) Senate(B-organization) in(O) 1960(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Montana(I-election) and(O) was(O) re-elected(O) in(O) 1966(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Montana(I-election) ,(O) ran(O) for(O) re-election(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, political party, politician, organization, election, location, person and O.\nSentence: Incumbent Democrat Lee Metcalf , who was first elected to the Senate in 1960 United States Senate election in Montana and was re-elected in 1966 United States Senate election in Montana , ran for re-election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Incumbent","Democrat","Lee","Metcalf",",","who","was","first","elected","to","the","Senate","in","1960","United","States","Senate","election","in","Montana","and","was","re-elected","in","1966","United","States","Senate","election","in","Montana",",","ran","for","re-election","."],"labels":["O","O","B-politician","I-politician","O","O","O","O","O","O","O","B-organization","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","event","political_party","politician","organization","election","location","person"]}
{"id":"521","dataset":"crossner_politics","split":"test","instance":{"id":"521","prompt_labels":"Green(O) deputies(O) Nol(B-politician) Mamre(I-politician) ,(O) Martine(B-politician) Billard(I-politician) and(O) Yves(B-politician) Cochet(I-politician) on(O) September(O) 10(O) ,(O) 2003(O) requested(O) a(O) Parliamentary(O) Commission(O) on(O) the(O) role(O) of(O) France(B-country) in(O) the(O) support(O) of(O) military(O) regimes(O) in(O) Latin(B-location) America(I-location) from(O) 1973(O) to(O) 1984(O) before(O) the(O) Foreign(B-organization) Affairs(I-organization) Commission(I-organization) of(I-organization) the(I-organization) National(I-organization) Assembly(I-organization) ,(O) presided(O) by(O) Edouard(B-politician) Balladur(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, politician, event, election, country, person, political party and O.\nSentence: Green deputies Nol Mamre , Martine Billard and Yves Cochet on September 10 , 2003 requested a Parliamentary Commission on the role of France in the support of military regimes in Latin America from 1973 to 1984 before the Foreign Affairs Commission of the National Assembly , presided by Edouard Balladur .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Green","deputies","Nol","Mamre",",","Martine","Billard","and","Yves","Cochet","on","September","10",",","2003","requested","a","Parliamentary","Commission","on","the","role","of","France","in","the","support","of","military","regimes","in","Latin","America","from","1973","to","1984","before","the","Foreign","Affairs","Commission","of","the","National","Assembly",",","presided","by","Edouard","Balladur","."],"labels":["O","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["organization","location","politician","event","election","country","person","political_party"]}
{"id":"523","dataset":"crossner_politics","split":"test","instance":{"id":"523","prompt_labels":")(O) Christine(B-politician) Quinn(I-politician) ,(O) the(O) Speaker(O) of(O) the(O) New(B-organization) York(I-organization) City(I-organization) Council(I-organization) ,(O) came(O) in(O) third(O) ,(O) with(O) 15.7(O) %(O) ,(O) while(O) none(O) of(O) the(O) other(O) candidates(O) ,(O) including(O) City(O) Comptroller(O) John(B-politician) Liu(I-politician) and(O) former(O) Congressman(O) Anthony(B-politician) Weiner(I-politician) ,(O) won(O) as(O) much(O) as(O) 10(O) %(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, organization, location, politician, political party, person, country and O.\nSentence: ) Christine Quinn , the Speaker of the New York City Council , came in third , with 15.7 % , while none of the other candidates , including City Comptroller John Liu and former Congressman Anthony Weiner , won as much as 10 % .","prediction_output":null,"prediction_outputs":null,"group":null,"words":[")","Christine","Quinn",",","the","Speaker","of","the","New","York","City","Council",",","came","in","third",",","with","15.7","%",",","while","none","of","the","other","candidates",",","including","City","Comptroller","John","Liu","and","former","Congressman","Anthony","Weiner",",","won","as","much","as","10","%","."],"labels":["O","B-politician","I-politician","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","event","organization","location","politician","political_party","person","country"]}
{"id":"525","dataset":"crossner_politics","split":"test","instance":{"id":"525","prompt_labels":"Mayor(O) Hylan(B-politician) ,(O) an(O) ally(O) of(O) the(O) newspaper(O) publisher(O) William(B-politician) Randolph(I-politician) Hearst(I-politician) ,(O) was(O) unseated(O) in(O) a(O) venomous(O) Democratic(O) primary(O) by(O) Gentleman(O) Jimmy(B-politician) Walker(I-politician) ,(O) the(O) Democratic(B-political party) party(I-political party) leader(O) in(O) the(O) New(B-organization) York(I-organization) State(I-organization) Senate(I-organization) ,(O) who(O) had(O) been(O) recruited(O) to(O) oppose(O) Hylan(B-politician) by(O) Hearst(B-organization) 's(O) inveterate(O) enemy(O) ,(O) Democratic(O) Governor(O) Al(B-politician) Smith(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, person, country, politician, political party, location, election and O.\nSentence: Mayor Hylan , an ally of the newspaper publisher William Randolph Hearst , was unseated in a venomous Democratic primary by Gentleman Jimmy Walker , the Democratic party leader in the New York State Senate , who had been recruited to oppose Hylan by Hearst 's inveterate enemy , Democratic Governor Al Smith .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Mayor","Hylan",",","an","ally","of","the","newspaper","publisher","William","Randolph","Hearst",",","was","unseated","in","a","venomous","Democratic","primary","by","Gentleman","Jimmy","Walker",",","the","Democratic","party","leader","in","the","New","York","State","Senate",",","who","had","been","recruited","to","oppose","Hylan","by","Hearst","'s","inveterate","enemy",",","Democratic","Governor","Al","Smith","."],"labels":["O","B-politician","O","O","O","O","O","O","O","B-politician","I-politician","I-politician","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","B-political party","I-political party","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","B-politician","O","B-organization","O","O","O","O","O","O","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["event","organization","person","country","politician","political_party","location","election"]}
{"id":"529","dataset":"crossner_politics","split":"test","instance":{"id":"529","prompt_labels":"It(O) is(O) based(O) upon(O) the(O) 1980(O) Iranian(B-event) Embassy(I-event) siege(I-event) in(O) London(B-location) and(O) stars(O) Jamie(B-person) Bell(I-person) ,(O) Abbie(B-person) Cornish(I-person) ,(O) Mark(B-person) Strong(I-person) and(O) Martin(B-person) Shaw(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, political party, person, politician, event, organization, country, election and O.\nSentence: It is based upon the 1980 Iranian Embassy siege in London and stars Jamie Bell , Abbie Cornish , Mark Strong and Martin Shaw .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","is","based","upon","the","1980","Iranian","Embassy","siege","in","London","and","stars","Jamie","Bell",",","Abbie","Cornish",",","Mark","Strong","and","Martin","Shaw","."],"labels":["O","O","O","O","O","O","B-event","I-event","I-event","O","B-location","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["location","political_party","person","politician","event","organization","country","election"]}
{"id":"530","dataset":"crossner_politics","split":"test","instance":{"id":"530","prompt_labels":"The(O) series(O) was(O) written(O) by(O) Russell(B-person) T(I-person) Davies(I-person) and(O) directed(O) by(O) Stephen(B-person) Frears(I-person) ,(O) with(O) Hugh(B-person) Grant(I-person) starring(O) as(O) Thorpe(B-person) and(O) Ben(B-person) Whishaw(I-person) as(O) Scott(B-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, event, election, organization, person, politician, political party and O.\nSentence: The series was written by Russell T Davies and directed by Stephen Frears , with Hugh Grant starring as Thorpe and Ben Whishaw as Scott .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","series","was","written","by","Russell","T","Davies","and","directed","by","Stephen","Frears",",","with","Hugh","Grant","starring","as","Thorpe","and","Ben","Whishaw","as","Scott","."],"labels":["O","O","O","O","O","B-person","I-person","I-person","O","O","O","B-person","I-person","O","O","B-person","I-person","O","O","B-person","O","B-person","I-person","O","B-person","O"],"target_index":null,"target_label":null},"label_list":["country","location","event","election","organization","person","politician","political_party"]}
{"id":"534","dataset":"crossner_politics","split":"test","instance":{"id":"534","prompt_labels":"Guests(O) have(O) included(O) Jordan(B-person) Peterson(I-person) ,(O) Coleman(B-person) Hughes(I-person) ,(O) James(B-person) Damore(I-person) ,(O) Lindsay(B-person) Shepherd(I-person) ,(O) Susan(B-person) Bradley(I-person) ,(O) Ed(B-person) the(I-person) Sock(I-person) ,(O) Adrienne(B-person) Batra(I-person) ,(O) Steven(B-person) Pinker(I-person) ,(O) Bill(B-person) Kristol(I-person) ,(O) Michael(B-person) Shermer(I-person) ,(O) Matthew(B-person) Goodwin(I-person) ,(O) Irshad(B-person) Manji(I-person) ,(O) Roger(B-person) Scruton(I-person) ,(O) Claire(B-person) Fox(I-person) ,(O) Francis(B-person) Fukuyama(I-person) ,(O) Peter(B-person) Boghossian(I-person) ,(O) Douglas(B-person) Murray(I-person) ,(O) Brian(B-person) C.(I-person) Kalt(I-person) ,(O) and(O) David(B-person) Frum(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, election, politician, country, event, organization, location, person and O.\nSentence: Guests have included Jordan Peterson , Coleman Hughes , James Damore , Lindsay Shepherd , Susan Bradley , Ed the Sock , Adrienne Batra , Steven Pinker , Bill Kristol , Michael Shermer , Matthew Goodwin , Irshad Manji , Roger Scruton , Claire Fox , Francis Fukuyama , Peter Boghossian , Douglas Murray , Brian C. Kalt , and David Frum .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Guests","have","included","Jordan","Peterson",",","Coleman","Hughes",",","James","Damore",",","Lindsay","Shepherd",",","Susan","Bradley",",","Ed","the","Sock",",","Adrienne","Batra",",","Steven","Pinker",",","Bill","Kristol",",","Michael","Shermer",",","Matthew","Goodwin",",","Irshad","Manji",",","Roger","Scruton",",","Claire","Fox",",","Francis","Fukuyama",",","Peter","Boghossian",",","Douglas","Murray",",","Brian","C.","Kalt",",","and","David","Frum","."],"labels":["O","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","I-person","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["political_party","election","politician","country","event","organization","location","person"]}
{"id":"535","dataset":"crossner_politics","split":"test","instance":{"id":"535","prompt_labels":"The(O) film(O) ,(O) which(O) is(O) produced(O) by(O) Joe(B-person) Dante(I-person) ,(O) stars(O) Danielle(B-person) Harris(I-person) and(O) Chad(B-person) Michael(I-person) Murray(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, location, political party, event, election, politician, country and O.\nSentence: The film , which is produced by Joe Dante , stars Danielle Harris and Chad Michael Murray .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","film",",","which","is","produced","by","Joe","Dante",",","stars","Danielle","Harris","and","Chad","Michael","Murray","."],"labels":["O","O","O","O","O","O","O","B-person","I-person","O","O","B-person","I-person","O","B-person","I-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["organization","person","location","political_party","event","election","politician","country"]}
{"id":"536","dataset":"crossner_politics","split":"test","instance":{"id":"536","prompt_labels":"Jacqueline(B-person) McKenzie(I-person) ,(O) Daniel(B-person) Wyllie(I-person) and(O) John(B-person) Brumpton(I-person) reprise(O) their(O) roles(O) from(O) the(O) original(O) film(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, person, event, country, election, location, politician and O.\nSentence: Jacqueline McKenzie , Daniel Wyllie and John Brumpton reprise their roles from the original film .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Jacqueline","McKenzie",",","Daniel","Wyllie","and","John","Brumpton","reprise","their","roles","from","the","original","film","."],"labels":["B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","political_party","person","event","country","election","location","politician"]}
{"id":"540","dataset":"crossner_politics","split":"test","instance":{"id":"540","prompt_labels":"Digital(O) Rights(O) Watch(O) also(O) works(O) with(O) international(O) groups(O) such(O) as(O) AccessNow.org(B-organization) ,(O) Electronic(B-organization) Frontier(I-organization) Foundation(I-organization) ,(O) Open(B-organization) Media(I-organization) Foundation(I-organization) ,(O) European(B-organization) Digital(I-organization) Rights(I-organization) ,(O) Privacy(B-organization) International(I-organization) and(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, location, organization, country, politician, election, political party and O.\nSentence: Digital Rights Watch also works with international groups such as AccessNow.org , Electronic Frontier Foundation , Open Media Foundation , European Digital Rights , Privacy International and others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Digital","Rights","Watch","also","works","with","international","groups","such","as","AccessNow.org",",","Electronic","Frontier","Foundation",",","Open","Media","Foundation",",","European","Digital","Rights",",","Privacy","International","and","others","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","event","location","organization","country","politician","election","political_party"]}
{"id":"541","dataset":"crossner_politics","split":"test","instance":{"id":"541","prompt_labels":"Organizations(O) that(O) have(O) argued(O) against(O) accommodationist(O) policies(O) in(O) the(O) United(B-country) States(I-country) include(O) Americans(B-organization) United(I-organization) for(I-organization) Separation(I-organization) of(I-organization) Church(I-organization) and(I-organization) State(I-organization) ,(O) the(O) Ayn(B-organization) Rand(I-organization) Institute(I-organization) ,(O) the(O) Freedom(B-organization) From(I-organization) Religion(I-organization) Foundation(I-organization) ,(O) the(O) Military(B-organization) Religious(I-organization) Freedom(I-organization) Foundation(I-organization) ,(O) People(B-organization) for(I-organization) the(I-organization) American(I-organization) Way(I-organization) ,(O) and(O) the(O) Secular(B-organization) Coalition(I-organization) for(I-organization) America(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, political party, person, country, politician, event, election, organization and O.\nSentence: Organizations that have argued against accommodationist policies in the United States include Americans United for Separation of Church and State , the Ayn Rand Institute , the Freedom From Religion Foundation , the Military Religious Freedom Foundation , People for the American Way , and the Secular Coalition for America .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Organizations","that","have","argued","against","accommodationist","policies","in","the","United","States","include","Americans","United","for","Separation","of","Church","and","State",",","the","Ayn","Rand","Institute",",","the","Freedom","From","Religion","Foundation",",","the","Military","Religious","Freedom","Foundation",",","People","for","the","American","Way",",","and","the","Secular","Coalition","for","America","."],"labels":["O","O","O","O","O","O","O","O","O","B-country","I-country","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["location","political_party","person","country","politician","event","election","organization"]}
{"id":"543","dataset":"crossner_politics","split":"test","instance":{"id":"543","prompt_labels":"Film(O) stars(O) and(O) celebrities(O) such(O) as(O) Patrick(B-person) Stewart(I-person) ,(O) Susan(B-person) Sarandon(I-person) ,(O) Richard(B-person) Curtis(I-person) ,(O) Lindsay(B-person) Duncan(I-person) ,(O) Mark(B-person) Rylance(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, location, election, event, politician, country, organization and O.\nSentence: Film stars and celebrities such as Patrick Stewart , Susan Sarandon , Richard Curtis , Lindsay Duncan , Mark Rylance .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Film","stars","and","celebrities","such","as","Patrick","Stewart",",","Susan","Sarandon",",","Richard","Curtis",",","Lindsay","Duncan",",","Mark","Rylance","."],"labels":["O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["person","political_party","location","election","event","politician","country","organization"]}
{"id":"545","dataset":"crossner_politics","split":"test","instance":{"id":"545","prompt_labels":"The(O) film(O) features(O) Vaibhav(B-person) Reddy(I-person) ,(O) Inigo(B-person) Prabhakaran(I-person) and(O) Sana(B-person) Althaf(I-person) ,(O) Anjena(B-person) Kirti(I-person) in(O) the(O) lead(O) roles(O) ,(O) while(O) Sampath(B-person) Raj(I-person) plays(O) a(O) supporting(O) role(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, person, location, organization, political party, country, politician, event and O.\nSentence: The film features Vaibhav Reddy , Inigo Prabhakaran and Sana Althaf , Anjena Kirti in the lead roles , while Sampath Raj plays a supporting role .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","film","features","Vaibhav","Reddy",",","Inigo","Prabhakaran","and","Sana","Althaf",",","Anjena","Kirti","in","the","lead","roles",",","while","Sampath","Raj","plays","a","supporting","role","."],"labels":["O","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","person","location","organization","political_party","country","politician","event"]}
{"id":"547","dataset":"crossner_politics","split":"test","instance":{"id":"547","prompt_labels":"In(O) addition(O) to(O) his(O) involvement(O) in(O) J-PAL(B-organization) ,(O) Olken(B-person) is(O) affiliated(O) with(O) several(O) other(O) economic(O) institutions(O) ,(O) including(O) the(O) Bureau(B-organization) for(I-organization) Research(I-organization) and(I-organization) Economic(I-organization) Analysis(I-organization) of(I-organization) Development(I-organization) ((O) BREAD(B-organization) )(O) ,(O) the(O) International(B-organization) Growth(I-organization) Centre(I-organization) ,(O) the(O) Centre(B-organization) for(I-organization) Economic(I-organization) Policy(I-organization) Research(I-organization) ((O) CEPR(B-organization) )(O) ,(O) and(O) the(O) National(B-organization) Bureau(I-organization) of(I-organization) Economic(I-organization) Research(I-organization) ((O) NBER(B-organization) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, country, political party, event, election, location, person and O.\nSentence: In addition to his involvement in J-PAL , Olken is affiliated with several other economic institutions , including the Bureau for Research and Economic Analysis of Development ( BREAD ) , the International Growth Centre , the Centre for Economic Policy Research ( CEPR ) , and the National Bureau of Economic Research ( NBER ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","addition","to","his","involvement","in","J-PAL",",","Olken","is","affiliated","with","several","other","economic","institutions",",","including","the","Bureau","for","Research","and","Economic","Analysis","of","Development","(","BREAD",")",",","the","International","Growth","Centre",",","the","Centre","for","Economic","Policy","Research","(","CEPR",")",",","and","the","National","Bureau","of","Economic","Research","(","NBER",")","."],"labels":["O","O","O","O","O","O","B-organization","O","B-person","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O"],"target_index":null,"target_label":null},"label_list":["politician","organization","country","political_party","event","election","location","person"]}
{"id":"550","dataset":"crossner_politics","split":"test","instance":{"id":"550","prompt_labels":"In(O) addition(O) Alexander(B-person) Ovechkin(I-person) ,(O) the(O) PutinTeam(B-organization) movement(I-organization) also(O) included(O) many(O) Russian(O) sportsmen(O) ,(O) actors(O) ,(O) musicians(O) and(O) social(O) activists(O) :(O) Evgeni(B-person) Malkin(I-person) ,(O) Yelena(B-person) Isinbayeva(I-person) ,(O) Sergey(B-person) Karjakin(I-person) ,(O) Sergey(B-person) Tetyukhin(I-person) ,(O) Nikolay(B-person) Rastorguyev(I-person) ,(O) Polina(B-person) Gagarina(I-person) ,(O) Dima(B-person) Bilan(I-person) ,(O) Ilya(B-person) Kovalchuk(I-person) ,(O) Nyusha(B-person) ,(O) Andrey(B-person) Merzlikin(I-person) ,(O) Pavel(B-person) Bure(I-person) ,(O) Evgeni(B-person) Plushenko(I-person) ,(O) Nikolay(B-person) Baskov(I-person) ,(O) Sergei(B-person) Krikalev(I-person) ,(O) Mikhail(B-person) Galustyan(I-person) ,(O) Alexander(B-person) Legkov(I-person) ,(O) Anton(B-person) Shipulin(I-person) ,(O) Ivan(B-person) Tcherezov(I-person) ,(O) Alexandr(B-person) Karelin(I-person) and(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, event, politician, country, organization, political party, election and O.\nSentence: In addition Alexander Ovechkin , the PutinTeam movement also included many Russian sportsmen , actors , musicians and social activists : Evgeni Malkin , Yelena Isinbayeva , Sergey Karjakin , Sergey Tetyukhin , Nikolay Rastorguyev , Polina Gagarina , Dima Bilan , Ilya Kovalchuk , Nyusha , Andrey Merzlikin , Pavel Bure , Evgeni Plushenko , Nikolay Baskov , Sergei Krikalev , Mikhail Galustyan , Alexander Legkov , Anton Shipulin , Ivan Tcherezov , Alexandr Karelin and others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","addition","Alexander","Ovechkin",",","the","PutinTeam","movement","also","included","many","Russian","sportsmen",",","actors",",","musicians","and","social","activists",":","Evgeni","Malkin",",","Yelena","Isinbayeva",",","Sergey","Karjakin",",","Sergey","Tetyukhin",",","Nikolay","Rastorguyev",",","Polina","Gagarina",",","Dima","Bilan",",","Ilya","Kovalchuk",",","Nyusha",",","Andrey","Merzlikin",",","Pavel","Bure",",","Evgeni","Plushenko",",","Nikolay","Baskov",",","Sergei","Krikalev",",","Mikhail","Galustyan",",","Alexander","Legkov",",","Anton","Shipulin",",","Ivan","Tcherezov",",","Alexandr","Karelin","and","others","."],"labels":["O","O","B-person","I-person","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","location","event","politician","country","organization","political_party","election"]}
{"id":"556","dataset":"crossner_politics","split":"test","instance":{"id":"556","prompt_labels":"She(O) hosted(O) Lucy(B-person) Stone(I-person) and(O) husband(O) Henry(B-person) Browne(I-person) Blackwell(I-person) when(O) they(O) came(O) to(O) Louisville(B-location) ,(O) Kentucky(B-location) for(O) the(O) American(B-organization) Woman(I-organization) Suffrage(I-organization) Association(I-organization) meeting(O) -(O) the(O) first(O) suffrage(O) convention(O) in(O) the(O) South(O) -(O) in(O) 1881(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, event, election, country, politician, location, political party and O.\nSentence: She hosted Lucy Stone and husband Henry Browne Blackwell when they came to Louisville , Kentucky for the American Woman Suffrage Association meeting - the first suffrage convention in the South - in 1881 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","hosted","Lucy","Stone","and","husband","Henry","Browne","Blackwell","when","they","came","to","Louisville",",","Kentucky","for","the","American","Woman","Suffrage","Association","meeting","-","the","first","suffrage","convention","in","the","South","-","in","1881","."],"labels":["O","O","B-person","I-person","O","O","B-person","I-person","I-person","O","O","O","O","B-location","O","B-location","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","organization","event","election","country","politician","location","political_party"]}
{"id":"559","dataset":"crossner_politics","split":"test","instance":{"id":"559","prompt_labels":"Omar(B-person) Chaparro(I-person) ,(O) Anglica(B-person) Vale(I-person) ,(O) Eduardo(B-person) Manzano(I-person) ,(O) and(O) Jaime(B-person) Maussan(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, political party, person, event, organization, politician, location and O.\nSentence: Omar Chaparro , Anglica Vale , Eduardo Manzano , and Jaime Maussan .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Omar","Chaparro",",","Anglica","Vale",",","Eduardo","Manzano",",","and","Jaime","Maussan","."],"labels":["B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["country","election","political_party","person","event","organization","politician","location"]}
{"id":"563","dataset":"crossner_politics","split":"test","instance":{"id":"563","prompt_labels":"The(O) bill(O) was(O) also(O) supported(O) by(O) prominent(O) think(O) tanks(O) including(O) conservative(O) groups(O) Heritage(B-organization) Action(I-organization) and(O) FreedomWorks(B-organization) ,(O) as(O) well(O) as(O) liberal(O) think(O) tanks(O) Families(B-organization) USA(I-organization) and(O) Public(B-organization) Citizen(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, politician, organization, person, event, location, political party and O.\nSentence: The bill was also supported by prominent think tanks including conservative groups Heritage Action and FreedomWorks , as well as liberal think tanks Families USA and Public Citizen .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","bill","was","also","supported","by","prominent","think","tanks","including","conservative","groups","Heritage","Action","and","FreedomWorks",",","as","well","as","liberal","think","tanks","Families","USA","and","Public","Citizen","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","B-organization","O","O","O","O","O","O","O","B-organization","I-organization","O","B-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["country","election","politician","organization","person","event","location","political_party"]}
{"id":"565","dataset":"crossner_politics","split":"test","instance":{"id":"565","prompt_labels":"The(O) speakers(O) -(O) all(O) of(O) whom(O) were(O) high(O) schoolers(O) or(O) younger(O) -(O) included(O) Marjory(B-organization) Stoneman(I-organization) Douglas(I-organization) students(O) Cameron(B-person) Kasky(I-person) ,(O) David(B-person) Hogg(I-person) ,(O) Delaney(B-person) Tarr(I-person) ,(O) Sarah(B-person) Chadwick(I-person) ,(O) Alex(B-person) Wind(I-person) ,(O) Jaclyn(B-person) Corin(I-person) ,(O) Ryan(B-person) Deitsch(I-person) ,(O) Aalayah(B-person) Eastmond(I-person) ,(O) Samantha(B-person) Fuentes(I-person) ,(O) and(O) Emma(B-person) Gonzlez(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, location, political party, organization, person, event, country, politician and O.\nSentence: The speakers - all of whom were high schoolers or younger - included Marjory Stoneman Douglas students Cameron Kasky , David Hogg , Delaney Tarr , Sarah Chadwick , Alex Wind , Jaclyn Corin , Ryan Deitsch , Aalayah Eastmond , Samantha Fuentes , and Emma Gonzlez .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","speakers","-","all","of","whom","were","high","schoolers","or","younger","-","included","Marjory","Stoneman","Douglas","students","Cameron","Kasky",",","David","Hogg",",","Delaney","Tarr",",","Sarah","Chadwick",",","Alex","Wind",",","Jaclyn","Corin",",","Ryan","Deitsch",",","Aalayah","Eastmond",",","Samantha","Fuentes",",","and","Emma","Gonzlez","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["election","location","political_party","organization","person","event","country","politician"]}
{"id":"569","dataset":"crossner_politics","split":"test","instance":{"id":"569","prompt_labels":"When(O) Sen(B-person) died(O) of(O) a(O) heart(O) attack(O) ,(O) Islam(B-person) took(O) over(O) his(O) duties(O) ,(O) managing(O) Bangladesh(B-country) 's(O) contributions(O) to(O) the(O) International(B-organization) Bank(I-organization) for(I-organization) Reconstruction(I-organization) and(I-organization) Development(I-organization) ,(O) International(B-organization) Development(I-organization) Association(I-organization) ,(O) and(O) International(B-organization) Finance(I-organization) Corporation(I-organization) ,(O) the(O) former(O) two(O) of(O) which(O) were(O) both(O) previously(O) managed(O) by(O) Sen(B-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, event, election, organization, location, political party, person, country and O.\nSentence: When Sen died of a heart attack , Islam took over his duties , managing Bangladesh 's contributions to the International Bank for Reconstruction and Development , International Development Association , and International Finance Corporation , the former two of which were both previously managed by Sen .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["When","Sen","died","of","a","heart","attack",",","Islam","took","over","his","duties",",","managing","Bangladesh","'s","contributions","to","the","International","Bank","for","Reconstruction","and","Development",",","International","Development","Association",",","and","International","Finance","Corporation",",","the","former","two","of","which","were","both","previously","managed","by","Sen","."],"labels":["O","B-person","O","O","O","O","O","O","B-person","O","O","O","O","O","O","B-country","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","B-person","O"],"target_index":null,"target_label":null},"label_list":["politician","event","election","organization","location","political_party","person","country"]}
{"id":"571","dataset":"crossner_politics","split":"test","instance":{"id":"571","prompt_labels":"In(O) March(O) 2018(O) ,(O) it(O) was(O) announced(O) Kristen(B-person) Stewart(I-person) ,(O) Jack(B-person) O(I-person) 'Connell(I-person) ,(O) Anthony(B-person) Mackie(I-person) ,(O) Margaret(B-person) Qualley(I-person) and(O) Colm(B-person) Meaney(I-person) had(O) joined(O) the(O) cast(O) of(O) the(O) film(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, election, event, location, country, organization, person, politician and O.\nSentence: In March 2018 , it was announced Kristen Stewart , Jack O 'Connell , Anthony Mackie , Margaret Qualley and Colm Meaney had joined the cast of the film .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","March","2018",",","it","was","announced","Kristen","Stewart",",","Jack","O","'Connell",",","Anthony","Mackie",",","Margaret","Qualley","and","Colm","Meaney","had","joined","the","cast","of","the","film","."],"labels":["O","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","election","event","location","country","organization","person","politician"]}
{"id":"574","dataset":"crossner_politics","split":"test","instance":{"id":"574","prompt_labels":"In(O) this(O) interview(O) conducted(O) in(O) Cambridge(B-organization) ,(O) Massachusetts(B-organization) on(O) February(O) 12(O) ,(O) 2004(O) ,(O) Chomsky(O) starts(O) by(O) responding(O) to(O) Robert(B-politician) McNamara(I-politician) '(O) s(O) comments(O) about(O) the(O) World(B-event) War(I-event) II(I-event) firebombing(B-event) of(I-event) Tokyo(I-event) in(O) The(O) Fog(O) of(O) War(O) by(O) pointing(O) out(O) that(O) definition(O) of(O) a(O) war(O) crime(O) at(O) the(O) Nuremberg(B-event) Trials(I-event) was(O) anything(O) the(O) enemy(O) did(O) that(O) the(O) Allies(O) didn(O) 't(O) do(O) and(O) goes(O) on(O) to(O) point(O) out(O) that(O) this(O) logic(O) is(O) central(O) to(O) the(O) Bush(B-politician) doctrine(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, event, political party, location, organization, person, election and O.\nSentence: In this interview conducted in Cambridge , Massachusetts on February 12 , 2004 , Chomsky starts by responding to Robert McNamara ' s comments about the World War II firebombing of Tokyo in The Fog of War by pointing out that definition of a war crime at the Nuremberg Trials was anything the enemy did that the Allies didn 't do and goes on to point out that this logic is central to the Bush doctrine .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","this","interview","conducted","in","Cambridge",",","Massachusetts","on","February","12",",","2004",",","Chomsky","starts","by","responding","to","Robert","McNamara","'","s","comments","about","the","World","War","II","firebombing","of","Tokyo","in","The","Fog","of","War","by","pointing","out","that","definition","of","a","war","crime","at","the","Nuremberg","Trials","was","anything","the","enemy","did","that","the","Allies","didn","'t","do","and","goes","on","to","point","out","that","this","logic","is","central","to","the","Bush","doctrine","."],"labels":["O","O","O","O","O","B-organization","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","B-event","I-event","I-event","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["politician","country","event","political_party","location","organization","person","election"]}
{"id":"578","dataset":"crossner_politics","split":"test","instance":{"id":"578","prompt_labels":"The(O) Purple(O) Plain(O) ,(O) also(O) known(O) as(O) Llanura(O) Roja(O) ,(O) is(O) a(O) 1954(O) British(B-event) war(I-event) film(O) ,(O) directed(O) by(O) Robert(B-person) Parrish(I-person) ,(O) with(O) Gregory(B-person) Peck(I-person) playing(O) a(O) Royal(O) Canadian(O) Air(O) Force(O) pilot(O) serving(O) in(O) the(O) Royal(O) Air(O) Force(O) in(O) the(O) Burma(B-event) Campaign(I-event) in(O) the(O) closing(O) months(O) of(O) the(O) World(B-event) War(I-event) II(I-event) ,(O) who(O) is(O) battling(O) with(O) depression(O) after(O) having(O) lost(O) his(O) wife(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, politician, organization, country, location, event, person and O.\nSentence: The Purple Plain , also known as Llanura Roja , is a 1954 British war film , directed by Robert Parrish , with Gregory Peck playing a Royal Canadian Air Force pilot serving in the Royal Air Force in the Burma Campaign in the closing months of the World War II , who is battling with depression after having lost his wife .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Purple","Plain",",","also","known","as","Llanura","Roja",",","is","a","1954","British","war","film",",","directed","by","Robert","Parrish",",","with","Gregory","Peck","playing","a","Royal","Canadian","Air","Force","pilot","serving","in","the","Royal","Air","Force","in","the","Burma","Campaign","in","the","closing","months","of","the","World","War","II",",","who","is","battling","with","depression","after","having","lost","his","wife","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","O","O","O","O","B-person","I-person","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","political_party","politician","organization","country","location","event","person"]}
{"id":"579","dataset":"crossner_politics","split":"test","instance":{"id":"579","prompt_labels":"During(O) his(O) premiership(O) ,(O) Renzi(B-politician) faced(O) several(O) challenging(O) foreign(O) policy(O) situations(O) ,(O) such(O) as(O) the(O) European(B-event) debt(I-event) crisis(I-event) ,(O) the(O) civil(B-event) war(I-event) in(O) Libya(B-country) ,(O) the(O) Ukrainian(B-event) Crisis(I-event) and(O) the(O) insurgency(B-event) of(I-event) the(I-event) Islamic(I-event) State(I-event) ((O) IS(B-country) )(O) in(O) the(O) Middle(B-location) East(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, election, country, organization, person, location, event and O.\nSentence: During his premiership , Renzi faced several challenging foreign policy situations , such as the European debt crisis , the civil war in Libya , the Ukrainian Crisis and the insurgency of the Islamic State ( IS ) in the Middle East .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","his","premiership",",","Renzi","faced","several","challenging","foreign","policy","situations",",","such","as","the","European","debt","crisis",",","the","civil","war","in","Libya",",","the","Ukrainian","Crisis","and","the","insurgency","of","the","Islamic","State","(","IS",")","in","the","Middle","East","."],"labels":["O","O","O","O","B-politician","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","B-event","I-event","O","B-country","O","O","B-event","I-event","O","O","B-event","I-event","I-event","I-event","I-event","O","B-country","O","O","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["political_party","politician","election","country","organization","person","location","event"]}
{"id":"581","dataset":"crossner_politics","split":"test","instance":{"id":"581","prompt_labels":"The(O) Arakan(B-event) Campaign(I-event) of(O) 1942-43(O) was(O) the(O) first(O) tentative(O) Allies(B-country) of(O) World(B-event) War(I-event) II(I-event) attack(O) into(O) British(O) rule(O) in(O) Burma(B-country) ,(O) following(O) the(O) Japanese(B-event) conquest(I-event) of(I-event) Burma(I-event) earlier(O) in(O) 1942(O) ,(O) during(O) the(O) Second(B-event) World(I-event) War(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, person, location, election, politician, organization, political party and O.\nSentence: The Arakan Campaign of 1942-43 was the first tentative Allies of World War II attack into British rule in Burma , following the Japanese conquest of Burma earlier in 1942 , during the Second World War .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Arakan","Campaign","of","1942-43","was","the","first","tentative","Allies","of","World","War","II","attack","into","British","rule","in","Burma",",","following","the","Japanese","conquest","of","Burma","earlier","in","1942",",","during","the","Second","World","War","."],"labels":["O","B-event","I-event","O","O","O","O","O","O","B-country","O","B-event","I-event","I-event","O","O","O","O","O","B-country","O","O","O","B-event","I-event","I-event","I-event","O","O","O","O","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["country","event","person","location","election","politician","organization","political_party"]}
{"id":"582","dataset":"crossner_politics","split":"test","instance":{"id":"582","prompt_labels":"The(O) campaign(O) 's(O) sustained(O) role(O) in(O) Russian(O) culture(O) may(O) be(O) seen(O) in(O) Tolstoy(B-person) '(O) s(O) War(O) and(O) Peace(O) ,(O) Tchaikovsky(O) '(O) s(O) 1812(O) Overture(O) ,(O) and(O) the(O) identification(O) of(O) it(O) with(O) the(O) German(B-event) invasion(I-event) of(I-event) 1941-45(I-event) ,(O) which(O) became(O) known(O) as(O) the(O) Great(B-event) Patriotic(I-event) War(I-event) in(O) the(O) Soviet(B-country) Union(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, person, election, event, country, location, organization and O.\nSentence: The campaign 's sustained role in Russian culture may be seen in Tolstoy ' s War and Peace , Tchaikovsky ' s 1812 Overture , and the identification of it with the German invasion of 1941-45 , which became known as the Great Patriotic War in the Soviet Union .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","campaign","'s","sustained","role","in","Russian","culture","may","be","seen","in","Tolstoy","'","s","War","and","Peace",",","Tchaikovsky","'","s","1812","Overture",",","and","the","identification","of","it","with","the","German","invasion","of","1941-45",",","which","became","known","as","the","Great","Patriotic","War","in","the","Soviet","Union","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","O","O","O","O","O","O","B-event","I-event","I-event","O","O","B-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["political_party","politician","person","election","event","country","location","organization"]}
{"id":"584","dataset":"crossner_politics","split":"test","instance":{"id":"584","prompt_labels":"In(O) December(O) 1914(O) he(O) was(O) appointed(O) to(O) the(O) Committee(B-organization) on(I-organization) Alleged(I-organization) German(I-organization) Outrages(I-organization) ,(O) which(O) in(O) May(O) 1915(O) reported(O) on(O) German(O) war(O) crimes(O) against(O) civilians(O) during(O) the(O) invasion(B-event) of(I-event) Belgium(I-event) in(O) the(O) opening(O) months(O) of(O) World(B-event) War(I-event) I(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, location, political party, event, country, organization, election and O.\nSentence: In December 1914 he was appointed to the Committee on Alleged German Outrages , which in May 1915 reported on German war crimes against civilians during the invasion of Belgium in the opening months of World War I .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","December","1914","he","was","appointed","to","the","Committee","on","Alleged","German","Outrages",",","which","in","May","1915","reported","on","German","war","crimes","against","civilians","during","the","invasion","of","Belgium","in","the","opening","months","of","World","War","I","."],"labels":["O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["person","politician","location","political_party","event","country","organization","election"]}
{"id":"585","dataset":"crossner_politics","split":"test","instance":{"id":"585","prompt_labels":"Madden(B-person) was(O) permitted(O) to(O) temporarily(O) leave(O) the(O) bench(O) in(O) 1945(O) to(O) assist(O) the(O) United(B-event) States(I-event) Department(I-event) of(I-event) War(I-event) and(O) the(O) United(B-organization) States(I-organization) Army(I-organization) in(O) various(O) legal(O) capacities(O) associated(O) with(O) the(O) Allies(B-country) of(O) World(B-event) War(I-event) II(I-event) administration(O) of(O) Allied-occupied(B-country) Germany(I-country) in(O) the(O) wake(O) of(O) World(B-event) War(I-event) II(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, politician, person, country, political party, election, event, organization and O.\nSentence: Madden was permitted to temporarily leave the bench in 1945 to assist the United States Department of War and the United States Army in various legal capacities associated with the Allies of World War II administration of Allied-occupied Germany in the wake of World War II .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Madden","was","permitted","to","temporarily","leave","the","bench","in","1945","to","assist","the","United","States","Department","of","War","and","the","United","States","Army","in","various","legal","capacities","associated","with","the","Allies","of","World","War","II","administration","of","Allied-occupied","Germany","in","the","wake","of","World","War","II","."],"labels":["B-person","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","B-country","O","B-event","I-event","I-event","O","O","B-country","I-country","O","O","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["location","politician","person","country","political_party","election","event","organization"]}
{"id":"588","dataset":"crossner_politics","split":"test","instance":{"id":"588","prompt_labels":"The(O) California(B-event) Campaign(I-event) ((O) 1846-1847(O) )(O) ,(O) colloquially(O) the(O) Conquest(B-event) of(I-event) California(I-event) or(O) Conquest(B-event) of(I-event) Alta(I-event) California(I-event) by(O) the(O) United(B-country) States(I-country) ,(O) was(O) an(O) early(O) military(O) campaign(O) of(O) the(O) Mexican-American(B-event) War(I-event) that(O) took(O) place(O) in(O) the(O) western(O) part(O) of(O) Mexico(B-country) 's(O) Alta(B-location) California(I-location) Department(O) ,(O) in(O) the(O) present-day(O) state(O) of(O) California(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, politician, election, country, location, person, political party, organization and O.\nSentence: The California Campaign ( 1846-1847 ) , colloquially the Conquest of California or Conquest of Alta California by the United States , was an early military campaign of the Mexican-American War that took place in the western part of Mexico 's Alta California Department , in the present-day state of California .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","California","Campaign","(","1846-1847",")",",","colloquially","the","Conquest","of","California","or","Conquest","of","Alta","California","by","the","United","States",",","was","an","early","military","campaign","of","the","Mexican-American","War","that","took","place","in","the","western","part","of","Mexico","'s","Alta","California","Department",",","in","the","present-day","state","of","California","."],"labels":["O","B-event","I-event","O","O","O","O","O","O","B-event","I-event","I-event","O","B-event","I-event","I-event","I-event","O","O","B-country","I-country","O","O","O","O","O","O","O","O","B-event","I-event","O","O","O","O","O","O","O","O","B-country","O","B-location","I-location","O","O","O","O","O","O","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["event","politician","election","country","location","person","political_party","organization"]}
{"id":"590","dataset":"crossner_politics","split":"test","instance":{"id":"590","prompt_labels":"The(O) air(O) raid(O) was(O) part(O) of(O) the(O) Allies(B-country) of(O) World(B-event) War(I-event) II(I-event) '(O) aerial(O) campaign(O) against(O) the(O) Home(O) Islands(O) of(O) the(O) Empire(B-country) of(I-country) Japan(I-country) during(O) World(B-event) War(I-event) II(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, election, location, event, political party, person, politician and O.\nSentence: The air raid was part of the Allies of World War II ' aerial campaign against the Home Islands of the Empire of Japan during World War II .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","air","raid","was","part","of","the","Allies","of","World","War","II","'","aerial","campaign","against","the","Home","Islands","of","the","Empire","of","Japan","during","World","War","II","."],"labels":["O","O","O","O","O","O","O","B-country","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","B-country","I-country","I-country","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["organization","country","election","location","event","political_party","person","politician"]}
{"id":"591","dataset":"crossner_politics","split":"test","instance":{"id":"591","prompt_labels":"215(O) Although(O) he(O) had(O) a(O) difficult(O) personal(O) relationship(O) with(O) his(O) superior(O) Wilhelm(B-politician) Canaris(I-politician) the(O) two(O) co-operated(O) closely(O) in(O) supporting(O) Canaris(B-politician) '(O) friend(O) Francisco(B-politician) Franco(I-politician) during(O) the(O) Spanish(B-event) Civil(I-event) War(I-event) .(O) John(B-person) H.(I-person) Waller(I-person) ,(O) The(O) Unseen(O) War(O) in(O) Europe(B-location) :(O) Espionage(O) and(O) Conspiracy(O) in(O) the(O) Second(B-event) World(I-event) War(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, location, political party, person, event, country, organization, politician and O.\nSentence: 215 Although he had a difficult personal relationship with his superior Wilhelm Canaris the two co-operated closely in supporting Canaris ' friend Francisco Franco during the Spanish Civil War . John H. Waller , The Unseen War in Europe : Espionage and Conspiracy in the Second World War .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["215","Although","he","had","a","difficult","personal","relationship","with","his","superior","Wilhelm","Canaris","the","two","co-operated","closely","in","supporting","Canaris","'","friend","Francisco","Franco","during","the","Spanish","Civil","War",".","John","H.","Waller",",","The","Unseen","War","in","Europe",":","Espionage","and","Conspiracy","in","the","Second","World","War","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","B-politician","O","O","B-politician","I-politician","O","O","B-event","I-event","I-event","O","B-person","I-person","I-person","O","O","O","O","O","B-location","O","O","O","O","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["election","location","political_party","person","event","country","organization","politician"]}
{"id":"592","dataset":"crossner_politics","split":"test","instance":{"id":"592","prompt_labels":"He(O) served(O) as(O) an(O) aide(O) to(O) Democratic(B-organization) National(I-organization) Committee(I-organization) chair(O) Terry(B-politician) McAuliffe(I-politician) during(O) the(O) 2003-2004(O) election(O) cycle(O) before(O) returning(O) to(O) work(O) for(O) Sen.(B-politician) Clinton(I-politician) 's(O) campaign(O) committee(O) for(O) Clinton(B-politician) 's(O) 2006(B-event) re-election(I-event) campaign(I-event) and(O) part(O) of(O) her(O) 2008(B-event) presidential(I-event) primary(I-event) campaign(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, country, election, political party, organization, event, person and O.\nSentence: He served as an aide to Democratic National Committee chair Terry McAuliffe during the 2003-2004 election cycle before returning to work for Sen. Clinton 's campaign committee for Clinton 's 2006 re-election campaign and part of her 2008 presidential primary campaign .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","served","as","an","aide","to","Democratic","National","Committee","chair","Terry","McAuliffe","during","the","2003-2004","election","cycle","before","returning","to","work","for","Sen.","Clinton","'s","campaign","committee","for","Clinton","'s","2006","re-election","campaign","and","part","of","her","2008","presidential","primary","campaign","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","B-politician","O","B-event","I-event","I-event","O","O","O","O","B-event","I-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["politician","location","country","election","political_party","organization","event","person"]}
{"id":"593","dataset":"crossner_politics","split":"test","instance":{"id":"593","prompt_labels":"UNA(B-event) campaign(I-event) manager(O) Toby(B-politician) Tiangco(I-politician) said(O) that(O) the(O) percentages(O) could(O) not(O) be(O) the(O) same(O) for(O) every(O) place(O) in(O) the(O) Philippines(B-country) ;(O) meanwhile(O) ,(O) Team(B-organization) PNoy(I-organization) spokesperson(O) Miro(B-politician) Quimbo(I-politician) said(O) that(O) it(O) is(O) a(O) result(O) of(O) a(O) great(O) message(O) campaign(O) led(O) by(O) the(O) President(O) himself(O) and(O) an(O) aggressive(O) ground(O) war(O) pursued(O) by(O) local(O) parties(O) allied(O) with(O) the(O) President(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, country, location, political party, politician, event, person, organization and O.\nSentence: UNA campaign manager Toby Tiangco said that the percentages could not be the same for every place in the Philippines ; meanwhile , Team PNoy spokesperson Miro Quimbo said that it is a result of a great message campaign led by the President himself and an aggressive ground war pursued by local parties allied with the President .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["UNA","campaign","manager","Toby","Tiangco","said","that","the","percentages","could","not","be","the","same","for","every","place","in","the","Philippines",";","meanwhile",",","Team","PNoy","spokesperson","Miro","Quimbo","said","that","it","is","a","result","of","a","great","message","campaign","led","by","the","President","himself","and","an","aggressive","ground","war","pursued","by","local","parties","allied","with","the","President","."],"labels":["B-event","I-event","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","B-organization","I-organization","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","country","location","political_party","politician","event","person","organization"]}
{"id":"594","dataset":"crossner_politics","split":"test","instance":{"id":"594","prompt_labels":"Filipinos(O) suspended(O) the(O) independence(O) campaign(O) during(O) the(O) First(B-event) World(I-event) War(I-event) and(O) supported(O) the(O) United(B-country) States(I-country) and(O) the(O) Allies(B-country) of(O) World(B-event) War(I-event) I(I-event) against(O) the(O) German(B-country) Empire(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, political party, country, election, politician, event, organization, person and O.\nSentence: Filipinos suspended the independence campaign during the First World War and supported the United States and the Allies of World War I against the German Empire .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Filipinos","suspended","the","independence","campaign","during","the","First","World","War","and","supported","the","United","States","and","the","Allies","of","World","War","I","against","the","German","Empire","."],"labels":["O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","B-country","I-country","O","O","B-country","O","B-event","I-event","I-event","O","O","B-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["location","political_party","country","election","politician","event","organization","person"]}
{"id":"598","dataset":"crossner_politics","split":"test","instance":{"id":"598","prompt_labels":"On(O) March(O) 31(O) ,(O) 2009(O) an(O) International(O) Conference(O) on(O) Afghanistan(B-country) ((O) subtitled(O) :(O) The(B-event) Afghanistan(I-event) Conference(I-event) 2009(I-event) :(O) A(O) Comprehensive(O) Strategy(O) in(O) a(O) Regional(O) Context(O) )(O) was(O) held(O) in(O) the(O) World(B-location) Forum(I-location) Convention(I-location) Center(I-location) in(O) The(B-location) Hague(I-location) where(O) members(O) of(O) the(O) international(O) community(O) discussed(O) the(O) future(O) of(O) Afghanistan(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, organization, election, person, politician, event, political party and O.\nSentence: On March 31 , 2009 an International Conference on Afghanistan ( subtitled : The Afghanistan Conference 2009 : A Comprehensive Strategy in a Regional Context ) was held in the World Forum Convention Center in The Hague where members of the international community discussed the future of Afghanistan .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","March","31",",","2009","an","International","Conference","on","Afghanistan","(","subtitled",":","The","Afghanistan","Conference","2009",":","A","Comprehensive","Strategy","in","a","Regional","Context",")","was","held","in","the","World","Forum","Convention","Center","in","The","Hague","where","members","of","the","international","community","discussed","the","future","of","Afghanistan","."],"labels":["O","O","O","O","O","O","O","O","O","B-country","O","O","O","B-event","I-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","I-location","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["country","location","organization","election","person","politician","event","political_party"]}
{"id":"603","dataset":"crossner_politics","split":"test","instance":{"id":"603","prompt_labels":"The(O) Second(B-event) Banda(I-event) Oriental(I-event) campaign(I-event) was(O) a(O) military(O) campaign(O) of(O) the(O) Argentine(B-event) War(I-event) of(I-event) Independence(I-event) ,(O) that(O) besieged(O) and(O) captured(O) the(O) Banda(B-location) Oriental(I-location) ((O) present-day(O) Uruguay(B-country) )(O) with(O) joint(O) operations(O) against(O) Montevideo(B-location) by(O) Jos(B-politician) Rondeau(I-politician) on(O) land(O) and(O) William(B-politician) Brown(I-politician) on(O) water(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, location, country, politician, event, election, organization, person and O.\nSentence: The Second Banda Oriental campaign was a military campaign of the Argentine War of Independence , that besieged and captured the Banda Oriental ( present-day Uruguay ) with joint operations against Montevideo by Jos Rondeau on land and William Brown on water .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Second","Banda","Oriental","campaign","was","a","military","campaign","of","the","Argentine","War","of","Independence",",","that","besieged","and","captured","the","Banda","Oriental","(","present-day","Uruguay",")","with","joint","operations","against","Montevideo","by","Jos","Rondeau","on","land","and","William","Brown","on","water","."],"labels":["O","B-event","I-event","I-event","I-event","O","O","O","O","O","O","B-event","I-event","I-event","I-event","O","O","O","O","O","O","B-location","I-location","O","O","B-country","O","O","O","O","O","B-location","O","B-politician","I-politician","O","O","O","B-politician","I-politician","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","location","country","politician","event","election","organization","person"]}
{"id":"604","dataset":"crossner_politics","split":"test","instance":{"id":"604","prompt_labels":"American(O) entry(O) into(O) World(B-event) War(I-event) I(I-event) came(O) in(O) the(O) immediate(O) aftermath(O) of(O) President(O) Woodrow(B-politician) Wilson(I-politician) successful(O) November(O) 1916(O) re-election(O) campaign(O) ,(O) which(O) made(O) prominent(O) use(O) of(O) the(O) slogan(O) He(O) Kept(O) Us(O) Out(O) of(O) War(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, election, event, country, politician, organization, political party, location and O.\nSentence: American entry into World War I came in the immediate aftermath of President Woodrow Wilson successful November 1916 re-election campaign , which made prominent use of the slogan He Kept Us Out of War .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["American","entry","into","World","War","I","came","in","the","immediate","aftermath","of","President","Woodrow","Wilson","successful","November","1916","re-election","campaign",",","which","made","prominent","use","of","the","slogan","He","Kept","Us","Out","of","War","."],"labels":["O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","election","event","country","politician","organization","political_party","location"]}
{"id":"606","dataset":"crossner_politics","split":"test","instance":{"id":"606","prompt_labels":"The(O) International(B-organization) Crisis(I-organization) Group(I-organization) believes(O) that(O) the(O) crisis(O) in(O) Guinea(B-country) could(O) lead(O) to(O) civil(O) war(O) in(O) Guinea(B-country) and(O) to(O) deteriorating(O) political(O) stability(O) in(O) neighboring(O) Liberia(B-country) ,(O) Sierra(B-country) Leone(I-country) and(O) Cte(B-country) d(I-country) 'Ivoire(I-country) ,(O) and(O) Guinea-Bissau(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, location, event, politician, election, organization, political party and O.\nSentence: The International Crisis Group believes that the crisis in Guinea could lead to civil war in Guinea and to deteriorating political stability in neighboring Liberia , Sierra Leone and Cte d 'Ivoire , and Guinea-Bissau .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","International","Crisis","Group","believes","that","the","crisis","in","Guinea","could","lead","to","civil","war","in","Guinea","and","to","deteriorating","political","stability","in","neighboring","Liberia",",","Sierra","Leone","and","Cte","d","'Ivoire",",","and","Guinea-Bissau","."],"labels":["O","B-organization","I-organization","I-organization","O","O","O","O","O","B-country","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","B-country","O","B-country","I-country","O","B-country","I-country","I-country","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["country","person","location","event","politician","election","organization","political_party"]}
{"id":"607","dataset":"crossner_politics","split":"test","instance":{"id":"607","prompt_labels":"It(O) has(O) been(O) featured(O) at(O) various(O) film(O) festivals(O) including(O) the(O) Toronto(B-event) Festival(I-event) of(I-event) Festivals(I-event) ;(O) the(O) Berlin(B-event) International(I-event) Film(I-event) Festival(I-event) ;(O) the(O) Cognac(B-event) Festival(I-event) du(I-event) Film(I-event) Policier(I-event) ,(I-event) Cognac(I-event) ,(I-event) France(I-event) ;(O) and(O) the(O) New(B-event) Directors(I-event) /(I-event) New(I-event) Films(I-event) Festival(I-event) in(O) New(B-location) York(I-location) City(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, election, location, person, politician, event, country and O.\nSentence: It has been featured at various film festivals including the Toronto Festival of Festivals ; the Berlin International Film Festival ; the Cognac Festival du Film Policier , Cognac , France ; and the New Directors / New Films Festival in New York City .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","has","been","featured","at","various","film","festivals","including","the","Toronto","Festival","of","Festivals",";","the","Berlin","International","Film","Festival",";","the","Cognac","Festival","du","Film","Policier",",","Cognac",",","France",";","and","the","New","Directors","/","New","Films","Festival","in","New","York","City","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","O","O","B-event","I-event","I-event","I-event","O","O","B-event","I-event","I-event","I-event","I-event","I-event","I-event","I-event","I-event","O","O","O","B-event","I-event","I-event","I-event","I-event","I-event","O","B-location","I-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["political_party","organization","election","location","person","politician","event","country"]}
{"id":"608","dataset":"crossner_politics","split":"test","instance":{"id":"608","prompt_labels":"The(O) conflict(O) began(O) when(O) the(O) Front(B-political party) for(I-political party) the(I-political party) National(I-political party) Liberation(I-political party) of(I-political party) the(I-political party) Congo(I-political party) ((O) FNLC(B-political party) )(O) ,(O) a(O) group(O) of(O) about(O) 2,000(O) Katangan(O) Congolese(O) soldiers(O) ((O) veterans(O) of(O) the(O) Congo(B-event) Crisis(I-event) ,(O) the(O) Angolan(B-event) War(I-event) of(I-event) Independence(I-event) ,(O) and(O) the(O) Angolan(B-event) Civil(I-event) War(I-event) )(O) crossed(O) the(O) border(O) into(O) Shaba(B-location) from(O) Angola(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, election, person, country, politician, organization, event, political party and O.\nSentence: The conflict began when the Front for the National Liberation of the Congo ( FNLC ) , a group of about 2,000 Katangan Congolese soldiers ( veterans of the Congo Crisis , the Angolan War of Independence , and the Angolan Civil War ) crossed the border into Shaba from Angola .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","conflict","began","when","the","Front","for","the","National","Liberation","of","the","Congo","(","FNLC",")",",","a","group","of","about","2,000","Katangan","Congolese","soldiers","(","veterans","of","the","Congo","Crisis",",","the","Angolan","War","of","Independence",",","and","the","Angolan","Civil","War",")","crossed","the","border","into","Shaba","from","Angola","."],"labels":["O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","O","O","B-event","I-event","I-event","I-event","O","O","O","B-event","I-event","I-event","O","O","O","O","O","B-location","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["location","election","person","country","politician","organization","event","political_party"]}
{"id":"612","dataset":"crossner_politics","split":"test","instance":{"id":"612","prompt_labels":"The(O) film(O) was(O) screened(O) at(O) various(O) film(O) festivals(O) ,(O) including(O) :(O) the(O) Toronto(B-event) International(I-event) Film(I-event) Festival(I-event) ,(O) Canada(B-country) ;(O) the(O) Huelva(B-event) Latin(I-event) American(I-event) Film(I-event) Festival(I-event) ,(O) Spain(B-country) ;(O) the(O) Norwegian(B-event) International(I-event) Film(I-event) Festival(I-event) ,(O) Norway(B-country) ;(O) the(O) Human(B-event) Rights(I-event) Watch(I-event) Film(I-event) Festival(I-event) ,(O) New(B-location) York(I-location) City(I-location) ;(O) the(O) Amnesty(B-event) International(I-event) Film(I-event) Festival(I-event) ,(O) Netherlands(B-country) ;(O) and(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, politician, election, political party, organization, event, country, person and O.\nSentence: The film was screened at various film festivals , including : the Toronto International Film Festival , Canada ; the Huelva Latin American Film Festival , Spain ; the Norwegian International Film Festival , Norway ; the Human Rights Watch Film Festival , New York City ; the Amnesty International Film Festival , Netherlands ; and others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","film","was","screened","at","various","film","festivals",",","including",":","the","Toronto","International","Film","Festival",",","Canada",";","the","Huelva","Latin","American","Film","Festival",",","Spain",";","the","Norwegian","International","Film","Festival",",","Norway",";","the","Human","Rights","Watch","Film","Festival",",","New","York","City",";","the","Amnesty","International","Film","Festival",",","Netherlands",";","and","others","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","O","B-country","O","O","B-event","I-event","I-event","I-event","I-event","O","B-country","O","O","B-event","I-event","I-event","I-event","O","B-country","O","O","B-event","I-event","I-event","I-event","I-event","O","B-location","I-location","I-location","O","O","B-event","I-event","I-event","I-event","O","B-country","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","politician","election","political_party","organization","event","country","person"]}
{"id":"616","dataset":"crossner_politics","split":"test","instance":{"id":"616","prompt_labels":"With(O) the(O) defeat(O) of(O) Axis(B-country) powers(I-country) in(O) the(O) North(B-event) African(I-event) campaign(I-event) in(O) May(O) 1943(O) ,(O) Winston(B-politician) Churchill(I-politician) ,(O) the(O) British(O) Prime(O) Minister(O) ,(O) who(O) at(O) least(O) as(O) far(O) back(O) as(O) the(O) Gallipoli(B-event) campaign(I-event) in(O) World(B-event) War(I-event) I(I-event) had(O) a(O) deep(O) interest(O) in(O) the(O) region(O) ,(O) turned(O) his(O) sights(O) on(O) the(O) islands(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, election, event, person, politician, country, location and O.\nSentence: With the defeat of Axis powers in the North African campaign in May 1943 , Winston Churchill , the British Prime Minister , who at least as far back as the Gallipoli campaign in World War I had a deep interest in the region , turned his sights on the islands .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["With","the","defeat","of","Axis","powers","in","the","North","African","campaign","in","May","1943",",","Winston","Churchill",",","the","British","Prime","Minister",",","who","at","least","as","far","back","as","the","Gallipoli","campaign","in","World","War","I","had","a","deep","interest","in","the","region",",","turned","his","sights","on","the","islands","."],"labels":["O","O","O","O","B-country","I-country","O","O","B-event","I-event","I-event","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","political_party","election","event","person","politician","country","location"]}
{"id":"617","dataset":"crossner_politics","split":"test","instance":{"id":"617","prompt_labels":"On(O) February(O) 3(O) ,(O) 1865(O) ,(O) Lincoln(B-politician) and(O) Seward(B-politician) held(O) a(O) conference(O) at(O) Hampton(B-location) Roads(I-location) with(O) three(O) representatives(O) of(O) the(O) Confederate(B-organization) government(I-organization) -(O) Vice(O) President(O) Alexander(B-politician) H.(I-politician) Stephens(I-politician) ,(O) Senator(O) Robert(B-politician) M.(I-politician) T.(I-politician) Hunter(I-politician) ,(O) and(O) Assistant(O) Secretary(O) of(O) War(O) John(B-politician) Archibald(I-politician) Campbell(I-politician) -(O) to(O) discuss(O) terms(O) to(O) end(O) the(O) war(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, location, political party, event, election, politician, country and O.\nSentence: On February 3 , 1865 , Lincoln and Seward held a conference at Hampton Roads with three representatives of the Confederate government - Vice President Alexander H. Stephens , Senator Robert M. T. Hunter , and Assistant Secretary of War John Archibald Campbell - to discuss terms to end the war .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","February","3",",","1865",",","Lincoln","and","Seward","held","a","conference","at","Hampton","Roads","with","three","representatives","of","the","Confederate","government","-","Vice","President","Alexander","H.","Stephens",",","Senator","Robert","M.","T.","Hunter",",","and","Assistant","Secretary","of","War","John","Archibald","Campbell","-","to","discuss","terms","to","end","the","war","."],"labels":["O","O","O","O","O","O","B-politician","O","B-politician","O","O","O","O","B-location","I-location","O","O","O","O","O","B-organization","I-organization","O","O","O","B-politician","I-politician","I-politician","O","O","B-politician","I-politician","I-politician","I-politician","O","O","O","O","O","O","B-politician","I-politician","I-politician","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","organization","location","political_party","event","election","politician","country"]}
{"id":"619","dataset":"crossner_politics","split":"test","instance":{"id":"619","prompt_labels":"In(O) light(O) of(O) the(O) economic(O) and(O) political(O) challenges(O) many(O) countries(O) in(O) the(O) region(O) had(O) faced(O) since(O) the(O) 2001(B-event) Quebec(I-event) City(I-event) Summit(I-event) of(I-event) the(I-event) Americas(I-event) and(O) because(O) 14(O) Heads(O) of(O) State(O) and(O) Government(O) in(O) the(O) Hemisphere(O) had(O) assumed(O) office(O) since(O) then(O) ,(O) the(O) Government(O) of(O) Canada(B-country) proposed(O) holding(O) a(O) Special(B-event) Summit(I-event) of(I-event) the(I-event) Americas(I-event) to(O) reinforce(O) hemispheric(O) unity(O) before(O) the(O) Fourth(B-event) Summit(I-event) of(I-event) the(I-event) Americas(I-event) ,(O) which(O) took(O) place(O) in(O) Mar(B-location) del(I-location) Plata(I-location) ,(O) Argentina(B-country) ,(O) in(O) 2005(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, politician, election, political party, person, country, location, organization and O.\nSentence: In light of the economic and political challenges many countries in the region had faced since the 2001 Quebec City Summit of the Americas and because 14 Heads of State and Government in the Hemisphere had assumed office since then , the Government of Canada proposed holding a Special Summit of the Americas to reinforce hemispheric unity before the Fourth Summit of the Americas , which took place in Mar del Plata , Argentina , in 2005 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","light","of","the","economic","and","political","challenges","many","countries","in","the","region","had","faced","since","the","2001","Quebec","City","Summit","of","the","Americas","and","because","14","Heads","of","State","and","Government","in","the","Hemisphere","had","assumed","office","since","then",",","the","Government","of","Canada","proposed","holding","a","Special","Summit","of","the","Americas","to","reinforce","hemispheric","unity","before","the","Fourth","Summit","of","the","Americas",",","which","took","place","in","Mar","del","Plata",",","Argentina",",","in","2005","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","B-event","I-event","I-event","I-event","I-event","O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","O","O","O","O","O","B-location","I-location","I-location","O","B-country","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","politician","election","political_party","person","country","location","organization"]}
{"id":"622","dataset":"crossner_politics","split":"test","instance":{"id":"622","prompt_labels":"253(O) was(O) the(O) final(O) Allies(B-organization) of(I-organization) World(I-organization) War(I-organization) II(I-organization) attack(O) during(O) the(O) Italian(B-event) Campaign(I-event) in(O) the(O) final(O) stages(O) of(O) the(O) Second(B-event) World(I-event) War(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, political party, country, politician, event, organization, election and O.\nSentence: 253 was the final Allies of World War II attack during the Italian Campaign in the final stages of the Second World War .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["253","was","the","final","Allies","of","World","War","II","attack","during","the","Italian","Campaign","in","the","final","stages","of","the","Second","World","War","."],"labels":["O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-event","I-event","O","O","O","O","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["person","location","political_party","country","politician","event","organization","election"]}
{"id":"626","dataset":"crossner_politics","split":"test","instance":{"id":"626","prompt_labels":"The(O) Weixian-Guangling-Nuanquan(B-event) Campaign(I-event) ((O) (B-event) )(O) was(O) a(O) campaign(O) fought(O) in(O) Wei(B-location) ((O) (B-location) )(O) County(O) and(O) Warm(B-location) Spring(I-location) ((O) Nuanquan(B-location) (B-location) )(O) of(O) Chahar(B-location) ((O) province(O) )(O) and(O) Guanling(B-location) ((O) (B-location) )(O) of(O) Shanxi(B-location) ,(O) and(O) it(O) was(O) a(O) clash(O) between(O) the(O) communist(O) s(O) and(O) the(O) former(O) Kuomintang(B-political party) turned(O) Japanese(O) puppet(O) regime(O) force(O) who(O) rejoined(O) the(O) Kuomintang(B-political party) after(O) World(B-event) War(I-event) II(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, person, organization, politician, event, location, country and O.\nSentence: The Weixian-Guangling-Nuanquan Campaign (  ) was a campaign fought in Wei (  ) County and Warm Spring ( Nuanquan  ) of Chahar ( province ) and Guanling (  ) of Shanxi , and it was a clash between the communist s and the former Kuomintang turned Japanese puppet regime force who rejoined the Kuomintang after World War II .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Weixian-Guangling-Nuanquan","Campaign","(","",")","was","a","campaign","fought","in","Wei","(","",")","County","and","Warm","Spring","(","Nuanquan","",")","of","Chahar","(","province",")","and","Guanling","(","",")","of","Shanxi",",","and","it","was","a","clash","between","the","communist","s","and","the","former","Kuomintang","turned","Japanese","puppet","regime","force","who","rejoined","the","Kuomintang","after","World","War","II","."],"labels":["O","B-event","I-event","O","B-event","O","O","O","O","O","O","B-location","O","B-location","O","O","O","B-location","I-location","O","B-location","B-location","O","O","B-location","O","O","O","O","B-location","O","B-location","O","O","B-location","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","O","O","O","O","O","O","O","O","B-political party","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["election","political_party","person","organization","politician","event","location","country"]}
{"id":"627","dataset":"crossner_politics","split":"test","instance":{"id":"627","prompt_labels":"Datong(B-event) -Puzhou(I-event) Campaign(I-event) ((O) (B-event) )(O) is(O) a(O) campaign(O) Communist(B-political party) Party(I-political party) of(I-political party) China(I-political party) fought(O) against(O) the(O) Kuomintang(B-political party) during(O) the(O) Chinese(B-event) Civil(I-event) War(I-event) in(O) the(O) post-(B-event) World(I-event) War(I-event) II(I-event) era(O) in(O) Shanxi(B-location) ,(O) and(O) resulted(O) in(O) communist(O) victory(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, politician, event, political party, organization, election, location and O.\nSentence: Datong -Puzhou Campaign (  ) is a campaign Communist Party of China fought against the Kuomintang during the Chinese Civil War in the post- World War II era in Shanxi , and resulted in communist victory .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Datong","-Puzhou","Campaign","(","",")","is","a","campaign","Communist","Party","of","China","fought","against","the","Kuomintang","during","the","Chinese","Civil","War","in","the","post-","World","War","II","era","in","Shanxi",",","and","resulted","in","communist","victory","."],"labels":["B-event","I-event","I-event","O","B-event","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","B-political party","O","O","B-event","I-event","I-event","O","O","B-event","I-event","I-event","I-event","O","O","B-location","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","country","politician","event","political_party","organization","election","location"]}
{"id":"628","dataset":"crossner_politics","split":"test","instance":{"id":"628","prompt_labels":"Russia(B-country) has(O) also(O) been(O) engaged(O) on(O) its(O) own(O) ,(O) also(O) largely(O) internally(O) focused(O) ,(O) counter-terrorism(O) campaign(O) often(O) termed(O) a(O) war(O) on(O) terror(O) ,(O) during(O) the(O) Second(B-event) Chechen(I-event) War(I-event) ,(O) the(O) Insurgency(B-event) in(I-event) the(I-event) North(I-event) Caucasus(I-event) ,(O) and(O) the(O) Russian(B-event) military(I-event) intervention(I-event) in(I-event) the(I-event) Syrian(I-event) Civil(I-event) War(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, country, person, politician, political party, event, election and O.\nSentence: Russia has also been engaged on its own , also largely internally focused , counter-terrorism campaign often termed a war on terror , during the Second Chechen War , the Insurgency in the North Caucasus , and the Russian military intervention in the Syrian Civil War .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Russia","has","also","been","engaged","on","its","own",",","also","largely","internally","focused",",","counter-terrorism","campaign","often","termed","a","war","on","terror",",","during","the","Second","Chechen","War",",","the","Insurgency","in","the","North","Caucasus",",","and","the","Russian","military","intervention","in","the","Syrian","Civil","War","."],"labels":["B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","B-event","I-event","I-event","I-event","I-event","O","O","O","B-event","I-event","I-event","I-event","I-event","I-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["location","organization","country","person","politician","political_party","event","election"]}
{"id":"634","dataset":"crossner_politics","split":"test","instance":{"id":"634","prompt_labels":"Hitler(B-politician) however(O) cut(O) a(O) deal(O) with(O) Joseph(B-politician) Stalin(I-politician) to(O) divide(O) Eastern(B-location) Europe(I-location) ;(O) when(O) Germany(B-country) did(O) invade(O) Poland(B-country) in(O) September(O) 1939(O) ,(O) Britain(B-country) and(O) France(B-country) declared(O) war(O) ;(O) the(O) British(B-organization) Commonwealth(I-organization) followed(O) London(B-location) 's(O) lead(O) Donald(B-politician) Cameron(I-politician) Watt(I-politician) ,(O) How(O) War(O) Came(O) :(O) Immediate(O) Origins(O) of(O) the(O) Second(O) World(O) War(O) ,(O) 1938-39(O) ((O) 1990(O) )(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, event, person, country, organization, election, location and O.\nSentence: Hitler however cut a deal with Joseph Stalin to divide Eastern Europe ; when Germany did invade Poland in September 1939 , Britain and France declared war ; the British Commonwealth followed London 's lead Donald Cameron Watt , How War Came : Immediate Origins of the Second World War , 1938-39 ( 1990 )","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Hitler","however","cut","a","deal","with","Joseph","Stalin","to","divide","Eastern","Europe",";","when","Germany","did","invade","Poland","in","September","1939",",","Britain","and","France","declared","war",";","the","British","Commonwealth","followed","London","'s","lead","Donald","Cameron","Watt",",","How","War","Came",":","Immediate","Origins","of","the","Second","World","War",",","1938-39","(","1990",")"],"labels":["B-politician","O","O","O","O","O","B-politician","I-politician","O","O","B-location","I-location","O","O","B-country","O","O","B-country","O","O","O","O","B-country","O","B-country","O","O","O","O","B-organization","I-organization","O","B-location","O","O","B-politician","I-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","politician","event","person","country","organization","election","location"]}
{"id":"635","dataset":"crossner_politics","split":"test","instance":{"id":"635","prompt_labels":"Lines(O) was(O) nominated(O) for(O) the(O) best(O) film(O) in(O) 2016(B-event) Tallinn(I-event) Black(I-event) Nights(I-event) Film(I-event) Festival(I-event) ,(O) in(O) 37th(B-event) Fantasporto(I-event) International(I-event) Film(I-event) Festival(I-event) ,(O) in(O) 2017(B-event) Los(I-event) Angeles(I-event) Greek(I-event) Film(I-event) Festival(I-event) ,(O) in(O) 30th(B-event) Athens(I-event) Panorama(I-event) of(I-event) European(I-event) Cinema(I-event) ,(O) in(O) 10th(B-event) this(I-event) human(I-event) world(I-event) in(I-event) Vienna(I-event) ,(O) in(O) 2018(B-event) Hellas(I-event) FilmBox(I-event) on(I-event) Berlin(I-event) ,(O) in(O) 8th(B-event) Philosophical(I-event) IFF(I-event) in(I-event) Macedonia(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, election, politician, political party, person, organization, country and O.\nSentence: Lines was nominated for the best film in 2016 Tallinn Black Nights Film Festival , in 37th Fantasporto International Film Festival , in 2017 Los Angeles Greek Film Festival , in 30th Athens Panorama of European Cinema , in 10th this human world in Vienna , in 2018 Hellas FilmBox on Berlin , in 8th Philosophical IFF in Macedonia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Lines","was","nominated","for","the","best","film","in","2016","Tallinn","Black","Nights","Film","Festival",",","in","37th","Fantasporto","International","Film","Festival",",","in","2017","Los","Angeles","Greek","Film","Festival",",","in","30th","Athens","Panorama","of","European","Cinema",",","in","10th","this","human","world","in","Vienna",",","in","2018","Hellas","FilmBox","on","Berlin",",","in","8th","Philosophical","IFF","in","Macedonia","."],"labels":["O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","I-event","O","O","B-event","I-event","I-event","I-event","I-event","O","O","B-event","I-event","I-event","I-event","I-event","I-event","O","O","B-event","I-event","I-event","I-event","I-event","I-event","O","O","B-event","I-event","I-event","I-event","I-event","I-event","O","O","B-event","I-event","I-event","I-event","I-event","O","O","B-event","I-event","I-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["event","location","election","politician","political_party","person","organization","country"]}
{"id":"637","dataset":"crossner_politics","split":"test","instance":{"id":"637","prompt_labels":"According(O) to(O) the(O) Mueller(O) Report(O) ,(O) the(O) second(O) method(O) of(O) Russian(O) interference(O) saw(O) the(O) Russian(B-organization) intelligence(I-organization) service(I-organization) ,(O) the(O) GRU(B-organization) ,(O) hacking(O) into(O) email(O) accounts(O) owned(O) by(O) volunteers(O) and(O) employees(O) of(O) the(O) Clinton(B-event) presidential(I-event) campaign(I-event) ,(O) including(O) that(O) of(O) campaign(O) chairman(O) John(B-politician) Podesta(I-politician) ,(O) and(O) also(O) hacking(O) into(O) the(O) computer(O) networks(O) of(O) the(O) Democratic(B-organization) Congressional(I-organization) Campaign(I-organization) Committee(I-organization) ((O) DCCC(B-organization) )(O) and(O) the(O) Democratic(B-organization) National(I-organization) Committee(I-organization) ((O) DNC(B-organization) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, person, political party, country, election, politician, organization and O.\nSentence: According to the Mueller Report , the second method of Russian interference saw the Russian intelligence service , the GRU , hacking into email accounts owned by volunteers and employees of the Clinton presidential campaign , including that of campaign chairman John Podesta , and also hacking into the computer networks of the Democratic Congressional Campaign Committee ( DCCC ) and the Democratic National Committee ( DNC ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["According","to","the","Mueller","Report",",","the","second","method","of","Russian","interference","saw","the","Russian","intelligence","service",",","the","GRU",",","hacking","into","email","accounts","owned","by","volunteers","and","employees","of","the","Clinton","presidential","campaign",",","including","that","of","campaign","chairman","John","Podesta",",","and","also","hacking","into","the","computer","networks","of","the","Democratic","Congressional","Campaign","Committee","(","DCCC",")","and","the","Democratic","National","Committee","(","DNC",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O"],"target_index":null,"target_label":null},"label_list":["location","event","person","political_party","country","election","politician","organization"]}
{"id":"638","dataset":"crossner_politics","split":"test","instance":{"id":"638","prompt_labels":"In(O) the(O) summer(O) of(O) 1914(O) ,(O) he(O) took(O) part(O) in(O) the(O) meetings(O) of(O) the(O) Crown(B-organization) Council(I-organization) held(O) at(O) Sinaia(B-location) ,(O) arguing(O) for(O) Romania(B-country) 's(O) neutrality(O) in(O) World(B-event) War(I-event) I.(I-event) Two(O) years(O) later(O) ,(O) he(O) took(O) part(O) in(O) the(O) Crown(B-organization) Council(I-organization) meeting(O) at(O) Cotroceni(B-location) Palace(I-location) ,(O) voting(O) for(O) Romania(B-country) 's(O) entry(O) into(O) the(O) war(O) on(O) the(O) side(O) of(O) the(O) Allies(B-organization) of(I-organization) World(I-organization) War(I-organization) I(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, person, country, location, politician, event, political party and O.\nSentence: In the summer of 1914 , he took part in the meetings of the Crown Council held at Sinaia , arguing for Romania 's neutrality in World War I. Two years later , he took part in the Crown Council meeting at Cotroceni Palace , voting for Romania 's entry into the war on the side of the Allies of World War I .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","summer","of","1914",",","he","took","part","in","the","meetings","of","the","Crown","Council","held","at","Sinaia",",","arguing","for","Romania","'s","neutrality","in","World","War","I.","Two","years","later",",","he","took","part","in","the","Crown","Council","meeting","at","Cotroceni","Palace",",","voting","for","Romania","'s","entry","into","the","war","on","the","side","of","the","Allies","of","World","War","I","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","B-location","O","O","O","B-country","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","B-location","I-location","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["election","organization","person","country","location","politician","event","political_party"]}
{"id":"639","dataset":"crossner_politics","split":"test","instance":{"id":"639","prompt_labels":"The(O) second(O) method(O) of(O) Russian(O) interference(O) saw(O) the(O) Russian(B-organization) intelligence(I-organization) service(I-organization) ,(O) the(O) GRU(B-organization) ,(O) hacking(O) into(O) email(O) accounts(O) owned(O) by(O) volunteers(O) and(O) employees(O) of(O) the(O) Clinton(B-event) presidential(I-event) campaign(I-event) ,(O) including(O) that(O) of(O) campaign(O) chairman(O) John(B-politician) Podesta(I-politician) ,(O) and(O) also(O) hacking(O) into(O) the(O) computer(O) networks(O) of(O) the(O) Democratic(B-organization) Congressional(I-organization) Campaign(I-organization) Committee(I-organization) ((O) DCCC(B-organization) )(O) and(O) the(O) Democratic(B-organization) National(I-organization) Committee(I-organization) ((O) DNC(B-organization) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, politician, event, election, country, person, organization, political party and O.\nSentence: The second method of Russian interference saw the Russian intelligence service , the GRU , hacking into email accounts owned by volunteers and employees of the Clinton presidential campaign , including that of campaign chairman John Podesta , and also hacking into the computer networks of the Democratic Congressional Campaign Committee ( DCCC ) and the Democratic National Committee ( DNC ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","second","method","of","Russian","interference","saw","the","Russian","intelligence","service",",","the","GRU",",","hacking","into","email","accounts","owned","by","volunteers","and","employees","of","the","Clinton","presidential","campaign",",","including","that","of","campaign","chairman","John","Podesta",",","and","also","hacking","into","the","computer","networks","of","the","Democratic","Congressional","Campaign","Committee","(","DCCC",")","and","the","Democratic","National","Committee","(","DNC",")","."],"labels":["O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O"],"target_index":null,"target_label":null},"label_list":["location","politician","event","election","country","person","organization","political_party"]}
{"id":"644","dataset":"crossner_politics","split":"test","instance":{"id":"644","prompt_labels":"The(O) 2018(B-event) North(I-event) Korea-United(I-event) States(I-event) Singapore(I-event) Summit(I-event) ,(O) commonly(O) known(O) as(O) Singapore(B-event) Summit(I-event) ,(O) was(O) a(O) summit(O) meeting(O) between(O) North(B-country) Korean(I-country) Chairman(O) Kim(B-politician) Jong-un(I-politician) and(O) U.S.(B-country) President(O) Donald(B-politician) Trump(I-politician) ,(O) held(O) at(O) the(O) Capella(B-location) Hotel(I-location) ,(O) Sentosa(B-location) ,(O) Singapore(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, country, person, election, politician, political party, event and O.\nSentence: The 2018 North Korea-United States Singapore Summit , commonly known as Singapore Summit , was a summit meeting between North Korean Chairman Kim Jong-un and U.S. President Donald Trump , held at the Capella Hotel , Sentosa , Singapore .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","2018","North","Korea-United","States","Singapore","Summit",",","commonly","known","as","Singapore","Summit",",","was","a","summit","meeting","between","North","Korean","Chairman","Kim","Jong-un","and","U.S.","President","Donald","Trump",",","held","at","the","Capella","Hotel",",","Sentosa",",","Singapore","."],"labels":["O","B-event","I-event","I-event","I-event","I-event","I-event","O","O","O","O","B-event","I-event","O","O","O","O","O","O","B-country","I-country","O","B-politician","I-politician","O","B-country","O","B-politician","I-politician","O","O","O","O","B-location","I-location","O","B-location","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["organization","location","country","person","election","politician","political_party","event"]}
{"id":"645","dataset":"crossner_politics","split":"test","instance":{"id":"645","prompt_labels":"In(O) the(O) decade(O) leading(O) up(O) to(O) the(O) Anschluss(B-event) and(O) to(O) World(B-event) War(I-event) II(I-event) ,(O) implacable(O) hostility(O) between(O) Social(B-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Austria(I-political party) and(O) Christian(O) Conservatives(O) had(O) eroded(O) civil(O) society(O) ,(O) undermined(O) democratic(O) institutions(O) ,(O) and(O) sparked(O) a(O) civil(B-event) war(I-event) ;(O) the(O) Christian(B-political party) Social(I-political party) Party(I-political party) eventually(O) manufactured(O) a(O) constitutional(O) crisis(O) that(O) allowed(O) it(O) to(O) replace(O) the(O) Republic(B-country) of(I-country) Austria(I-country) with(O) the(O) Federal(B-country) State(I-country) of(I-country) Austria(I-country) ,(O) a(O) Fascist(B-country) single-party(I-country) state(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, politician, location, person, country, election, political party, organization and O.\nSentence: In the decade leading up to the Anschluss and to World War II , implacable hostility between Social Democratic Party of Austria and Christian Conservatives had eroded civil society , undermined democratic institutions , and sparked a civil war ; the Christian Social Party eventually manufactured a constitutional crisis that allowed it to replace the Republic of Austria with the Federal State of Austria , a Fascist single-party state .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","decade","leading","up","to","the","Anschluss","and","to","World","War","II",",","implacable","hostility","between","Social","Democratic","Party","of","Austria","and","Christian","Conservatives","had","eroded","civil","society",",","undermined","democratic","institutions",",","and","sparked","a","civil","war",";","the","Christian","Social","Party","eventually","manufactured","a","constitutional","crisis","that","allowed","it","to","replace","the","Republic","of","Austria","with","the","Federal","State","of","Austria",",","a","Fascist","single-party","state","."],"labels":["O","O","O","O","O","O","O","B-event","O","O","B-event","I-event","I-event","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","I-country","O","O","B-country","I-country","I-country","I-country","O","O","B-country","I-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["event","politician","location","person","country","election","political_party","organization"]}
{"id":"647","dataset":"crossner_politics","split":"test","instance":{"id":"647","prompt_labels":"The(O) 2019(B-event) North(I-event) Korea-United(I-event) States(I-event) Hanoi(I-event) Summit(I-event) ,(O) commonly(O) known(O) as(O) the(O) Hanoi(B-event) Summit(I-event) ,(O) was(O) a(O) two-day(O) summit(O) meeting(O) between(O) North(B-country) Korea(I-country) n(O) Chairman(O) Kim(B-politician) Jong-un(I-politician) and(O) United(B-country) States(I-country) President(O) Donald(B-politician) Trump(I-politician) ,(O) held(O) at(O) the(O) Metropole(B-location) Hotel(I-location) in(O) Hanoi(B-location) ,(O) Vietnam(B-location) ,(O) on(O) February(O) 27-28(O) ,(O) 2019(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, organization, country, political party, event, person, location and O.\nSentence: The 2019 North Korea-United States Hanoi Summit , commonly known as the Hanoi Summit , was a two-day summit meeting between North Korea n Chairman Kim Jong-un and United States President Donald Trump , held at the Metropole Hotel in Hanoi , Vietnam , on February 27-28 , 2019 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","2019","North","Korea-United","States","Hanoi","Summit",",","commonly","known","as","the","Hanoi","Summit",",","was","a","two-day","summit","meeting","between","North","Korea","n","Chairman","Kim","Jong-un","and","United","States","President","Donald","Trump",",","held","at","the","Metropole","Hotel","in","Hanoi",",","Vietnam",",","on","February","27-28",",","2019","."],"labels":["O","B-event","I-event","I-event","I-event","I-event","I-event","O","O","O","O","O","B-event","I-event","O","O","O","O","O","O","O","B-country","I-country","O","O","B-politician","I-politician","O","B-country","I-country","O","B-politician","I-politician","O","O","O","O","B-location","I-location","O","B-location","O","B-location","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["politician","election","organization","country","political_party","event","person","location"]}
{"id":"648","dataset":"crossner_politics","split":"test","instance":{"id":"648","prompt_labels":"War(O) returned(O) in(O) September(O) 1939(O) when(O) the(O) French(O) and(O) British(O) governments(O) declared(O) war(O) on(O) Nazi(B-country) Germany(I-country) in(O) response(O) to(O) the(O) German(B-event) invasion(I-event) of(I-event) Poland(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, event, politician, country, location, person, organization and O.\nSentence: War returned in September 1939 when the French and British governments declared war on Nazi Germany in response to the German invasion of Poland .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["War","returned","in","September","1939","when","the","French","and","British","governments","declared","war","on","Nazi","Germany","in","response","to","the","German","invasion","of","Poland","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O","O","O","O","B-event","I-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["election","political_party","event","politician","country","location","person","organization"]}
{"id":"649","dataset":"crossner_politics","split":"test","instance":{"id":"649","prompt_labels":"Buttigieg(B-politician) 's(O) campaign(O) manager(O) ,(O) Mike(B-politician) Schmuhl(I-politician) ,(O) had(O) previously(O) been(O) the(O) campaign(O) manager(O) on(O) Joe(B-politician) Donnelly(I-politician) '(O) s(O) 2010(O) congressional(O) campaign(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, location, politician, political party, person, event, election and O.\nSentence: Buttigieg 's campaign manager , Mike Schmuhl , had previously been the campaign manager on Joe Donnelly ' s 2010 congressional campaign .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Buttigieg","'s","campaign","manager",",","Mike","Schmuhl",",","had","previously","been","the","campaign","manager","on","Joe","Donnelly","'","s","2010","congressional","campaign","."],"labels":["B-politician","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","country","location","politician","political_party","person","event","election"]}
{"id":"650","dataset":"crossner_politics","split":"test","instance":{"id":"650","prompt_labels":"The(O) Mueller(O) Report(O) showed(O) that(O) despite(O) assertions(O) by(O) Hope(B-politician) Hicks(I-politician) and(O) Jason(B-politician) Miller(I-politician) in(O) September(O) 2016(O) that(O) Carter(B-politician) Page(I-politician) never(O) had(O) any(O) involvement(O) with(O) the(O) campaign(O) ,(O) Page(B-politician) actually(O) produced(O) work(O) for(O) the(O) campaign(O) ,(O) traveled(O) with(O) Trump(B-politician) to(O) a(O) campaign(O) speech(O) and(O) Chief(O) policy(O) adviser(O) Sam(B-person) Clovis(I-person) expressed(O) appreciation(O) for(O) Page(B-politician) 's(O) work(O) and(O) praised(O) his(O) work(O) to(O) other(O) Campaign(O) officials(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, country, election, politician, organization, person, political party and O.\nSentence: The Mueller Report showed that despite assertions by Hope Hicks and Jason Miller in September 2016 that Carter Page never had any involvement with the campaign , Page actually produced work for the campaign , traveled with Trump to a campaign speech and Chief policy adviser Sam Clovis expressed appreciation for Page 's work and praised his work to other Campaign officials .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Mueller","Report","showed","that","despite","assertions","by","Hope","Hicks","and","Jason","Miller","in","September","2016","that","Carter","Page","never","had","any","involvement","with","the","campaign",",","Page","actually","produced","work","for","the","campaign",",","traveled","with","Trump","to","a","campaign","speech","and","Chief","policy","adviser","Sam","Clovis","expressed","appreciation","for","Page","'s","work","and","praised","his","work","to","other","Campaign","officials","."],"labels":["O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","B-politician","O","O","O","O","O","O","O","O","O","B-politician","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","B-politician","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","event","country","election","politician","organization","person","political_party"]}
{"id":"0","dataset":"crossner_science","split":"test","instance":{"id":"0","prompt_labels":"ESA(B-organization) 's(O) Advanced(B-organization) Concepts(I-organization) Team(I-organization) also(O) demonstrated(O) theoretically(O) that(O) a(O) deflection(O) of(O) 99942(B-astronomical object) Apophis(I-astronomical object) could(O) be(O) achieved(O) by(O) sending(O) a(O) spacecraft(O) weighing(O) less(O) than(O) a(O) tonne(O) to(O) impact(O) against(O) the(O) asteroid(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, chemical element, award, academic journal, protein, person, location, chemical compound, enzyme, country, university, astronomical object, scientist, organization, theory, event and O.\nSentence: ESA 's Advanced Concepts Team also demonstrated theoretically that a deflection of 99942 Apophis could be achieved by sending a spacecraft weighing less than a tonne to impact against the asteroid .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["ESA","'s","Advanced","Concepts","Team","also","demonstrated","theoretically","that","a","deflection","of","99942","Apophis","could","be","achieved","by","sending","a","spacecraft","weighing","less","than","a","tonne","to","impact","against","the","asteroid","."],"labels":["B-organization","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","B-astronomical object","I-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["discipline","chemical_element","award","academic_journal","protein","person","location","chemical_compound","enzyme","country","university","astronomical_object","scientist","organization","theory","event"]}
{"id":"1","dataset":"crossner_science","split":"test","instance":{"id":"1","prompt_labels":"Studies(O) in(O) human(O) neutrophils(O) first(O) detected(O) a(O) plasma(O) membrane(O) -localized(O) site(O) which(O) reversibly(O) bound(O) 5-oxo-ETE(B-chemical compound) and(O) had(O) the(O) attributes(O) of(O) a(O) Gi(B-protein) alpha(I-protein) subunit(I-protein) -linked(O) G(B-protein) protein-coupled(I-protein) receptor(I-protein) based(O) on(O) the(O) ability(O) of(O) 5-oxo-ETE(B-chemical compound) to(O) activate(O) this(O) class(O) of(O) membrane(O) G(B-protein) proteins(I-protein) by(O) a(O) pertussis(O) toxin(O) -sensitive(O) mechanism(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, discipline, academic journal, astronomical object, country, enzyme, protein, event, person, organization, award, theory, chemical compound, scientist, university, chemical element and O.\nSentence: Studies in human neutrophils first detected a plasma membrane -localized site which reversibly bound 5-oxo-ETE and had the attributes of a Gi alpha subunit -linked G protein-coupled receptor based on the ability of 5-oxo-ETE to activate this class of membrane G proteins by a pertussis toxin -sensitive mechanism .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Studies","in","human","neutrophils","first","detected","a","plasma","membrane","-localized","site","which","reversibly","bound","5-oxo-ETE","and","had","the","attributes","of","a","Gi","alpha","subunit","-linked","G","protein-coupled","receptor","based","on","the","ability","of","5-oxo-ETE","to","activate","this","class","of","membrane","G","proteins","by","a","pertussis","toxin","-sensitive","mechanism","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","O","O","O","O","O","O","B-protein","I-protein","I-protein","O","B-protein","I-protein","I-protein","O","O","O","O","O","B-chemical compound","O","O","O","O","O","O","B-protein","I-protein","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","discipline","academic_journal","astronomical_object","country","enzyme","protein","event","person","organization","award","theory","chemical_compound","scientist","university","chemical_element"]}
{"id":"2","dataset":"crossner_science","split":"test","instance":{"id":"2","prompt_labels":"That(O) first(O) evening(O) session(O) was(O) organized(O) by(O) Jack(B-person) Yardley(I-person) from(O) Johns(B-university) Hopkins(I-university) University(I-university) ,(O) and(O) included(O) Henry(B-person) Appelman(I-person) ((O) University(B-university) of(I-university) Michigan(I-university) )(O) ,(O) Harvey(B-scientist) Goldman(I-scientist) ((O) Beth(B-organization) Israel(I-organization) Deaconess(I-organization) Medical(I-organization) Center(I-organization) and(O) Harvard(B-university) Medical(I-university) School(I-university) )(O) ,(O) Bill(B-scientist) Hawk(I-scientist) ((O) The(O) Cleveland(B-organization) Clinic(I-organization) )(O) ,(O) Tom(B-person) Kent(I-person) ((O) University(B-university) of(I-university) Iowa(I-university) )(O) ,(O) Si-Chun(B-scientist) Ming(I-scientist) ((O) Temple(B-university) University(I-university) )(O) ,(O) Tom(B-scientist) Norris(I-scientist) ((O) University(B-university) of(I-university) Washington(I-university) )(O) ,(O) and(O) Robert(B-scientist) Riddell(I-scientist) ((O) University(B-university) of(I-university) Chicago(I-university) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, chemical compound, event, enzyme, discipline, chemical element, theory, protein, location, organization, university, person, academic journal, scientist, astronomical object, country and O.\nSentence: That first evening session was organized by Jack Yardley from Johns Hopkins University , and included Henry Appelman ( University of Michigan ) , Harvey Goldman ( Beth Israel Deaconess Medical Center and Harvard Medical School ) , Bill Hawk ( The Cleveland Clinic ) , Tom Kent ( University of Iowa ) , Si-Chun Ming ( Temple University ) , Tom Norris ( University of Washington ) , and Robert Riddell ( University of Chicago ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["That","first","evening","session","was","organized","by","Jack","Yardley","from","Johns","Hopkins","University",",","and","included","Henry","Appelman","(","University","of","Michigan",")",",","Harvey","Goldman","(","Beth","Israel","Deaconess","Medical","Center","and","Harvard","Medical","School",")",",","Bill","Hawk","(","The","Cleveland","Clinic",")",",","Tom","Kent","(","University","of","Iowa",")",",","Si-Chun","Ming","(","Temple","University",")",",","Tom","Norris","(","University","of","Washington",")",",","and","Robert","Riddell","(","University","of","Chicago",")","."],"labels":["O","O","O","O","O","O","O","B-person","I-person","O","B-university","I-university","I-university","O","O","O","B-person","I-person","O","B-university","I-university","I-university","O","O","B-scientist","I-scientist","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-university","I-university","I-university","O","O","B-scientist","I-scientist","O","O","B-organization","I-organization","O","O","B-person","I-person","O","B-university","I-university","I-university","O","O","B-scientist","I-scientist","O","B-university","I-university","O","O","B-scientist","I-scientist","O","B-university","I-university","I-university","O","O","O","B-scientist","I-scientist","O","B-university","I-university","I-university","O","O"],"target_index":null,"target_label":null},"label_list":["award","chemical_compound","event","enzyme","discipline","chemical_element","theory","protein","location","organization","university","person","academic_journal","scientist","astronomical_object","country"]}
{"id":"4","dataset":"crossner_science","split":"test","instance":{"id":"4","prompt_labels":"He(O) also(O) won(O) the(O) Copley(B-award) Medal(I-award) in(O) 1907(O) ,(O) the(O) Henry(B-award) Draper(I-award) Medal(I-award) in(O) 1916(O) and(O) the(O) Gold(B-award) Medal(I-award) of(I-award) the(I-award) Royal(I-award) Astronomical(I-award) Society(I-award) in(O) 1923(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, scientist, astronomical object, university, country, academic journal, protein, chemical compound, enzyme, location, organization, person, theory, chemical element, event, discipline and O.\nSentence: He also won the Copley Medal in 1907 , the Henry Draper Medal in 1916 and the Gold Medal of the Royal Astronomical Society in 1923 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","also","won","the","Copley","Medal","in","1907",",","the","Henry","Draper","Medal","in","1916","and","the","Gold","Medal","of","the","Royal","Astronomical","Society","in","1923","."],"labels":["O","O","O","O","B-award","I-award","O","O","O","O","B-award","I-award","I-award","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","scientist","astronomical_object","university","country","academic_journal","protein","chemical_compound","enzyme","location","organization","person","theory","chemical_element","event","discipline"]}
{"id":"6","dataset":"crossner_science","split":"test","instance":{"id":"6","prompt_labels":"In(O) February(O) 2016(O) ,(O) he(O) was(O) one(O) of(O) the(O) four(O) scientists(O) of(O) LIGO(B-organization) Scientific(I-organization) Collaboration(I-organization) /(O) Virgo(B-organization) interferometer(I-organization) collaboration(O) presenting(O) at(O) the(O) press(O) conference(O) for(O) the(O) announcement(O) that(O) the(O) first(B-event) direct(I-event) gravitational(I-event) wave(I-event) observation(I-event) had(O) been(O) made(O) in(O) September(O) 2015(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, chemical compound, organization, theory, person, academic journal, astronomical object, location, scientist, chemical element, university, country, protein, award, event, enzyme and O.\nSentence: In February 2016 , he was one of the four scientists of LIGO Scientific Collaboration / Virgo interferometer collaboration presenting at the press conference for the announcement that the first direct gravitational wave observation had been made in September 2015 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","February","2016",",","he","was","one","of","the","four","scientists","of","LIGO","Scientific","Collaboration","/","Virgo","interferometer","collaboration","presenting","at","the","press","conference","for","the","announcement","that","the","first","direct","gravitational","wave","observation","had","been","made","in","September","2015","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["discipline","chemical_compound","organization","theory","person","academic_journal","astronomical_object","location","scientist","chemical_element","university","country","protein","award","event","enzyme"]}
{"id":"8","dataset":"crossner_science","split":"test","instance":{"id":"8","prompt_labels":"On(O) 9(O) May(O) 1821(O) he(O) married(O) in(O) Klein(B-location) Lankum(I-location) with(O) Camper(B-person) 's(O) third(O) child(O) and(O) second(O) daughter(O) Frederika(B-person) Theodora(I-person) Ernestina(I-person) Camper(I-person) ((O) 1799-1834(O) )(O) ,(O) who(O) was(O) herself(O) an(O) amateur-scientist(O) who(O) accompanied(O) him(O) on(O) voyages(O) to(O) Georges(B-scientist) Cuvier(I-scientist) in(O) France(B-country) and(O) Humphry(B-scientist) Davy(I-scientist) in(O) England(B-country) ;(O) she(O) made(O) drawings(O) of(O) his(O) specimens(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, enzyme, academic journal, scientist, protein, person, theory, country, astronomical object, event, location, organization, university, chemical compound, discipline, award and O.\nSentence: On 9 May 1821 he married in Klein Lankum with Camper 's third child and second daughter Frederika Theodora Ernestina Camper ( 1799-1834 ) , who was herself an amateur-scientist who accompanied him on voyages to Georges Cuvier in France and Humphry Davy in England ; she made drawings of his specimens .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","9","May","1821","he","married","in","Klein","Lankum","with","Camper","'s","third","child","and","second","daughter","Frederika","Theodora","Ernestina","Camper","(","1799-1834",")",",","who","was","herself","an","amateur-scientist","who","accompanied","him","on","voyages","to","Georges","Cuvier","in","France","and","Humphry","Davy","in","England",";","she","made","drawings","of","his","specimens","."],"labels":["O","O","O","O","O","O","O","B-location","I-location","O","B-person","O","O","O","O","O","O","B-person","I-person","I-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-country","O","B-scientist","I-scientist","O","B-country","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","enzyme","academic_journal","scientist","protein","person","theory","country","astronomical_object","event","location","organization","university","chemical_compound","discipline","award"]}
{"id":"9","dataset":"crossner_science","split":"test","instance":{"id":"9","prompt_labels":"That(O) is(O) ,(O) cell(O) stimulation(O) causes(O) DHA(B-chemical compound) to(O) be(O) released(O) from(O) the(O) sn-2(O) position(O) of(O) their(O) membrane-bound(O) cellular(O) phospholipid(O) pools(O) through(O) the(O) action(O) of(O) a(O) Phospholipase(B-enzyme) A2(I-enzyme) -type(I-enzyme) enzyme(I-enzyme) and(O) the(O) subsequent(O) attack(O) of(O) the(O) released(O) DHA(B-chemical compound) by(O) CYP450(B-enzyme) epoxidases(I-enzyme) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, theory, scientist, university, chemical compound, location, protein, academic journal, event, organization, astronomical object, award, chemical element, discipline, country, person and O.\nSentence: That is , cell stimulation causes DHA to be released from the sn-2 position of their membrane-bound cellular phospholipid pools through the action of a Phospholipase A2 -type enzyme and the subsequent attack of the released DHA by CYP450 epoxidases .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["That","is",",","cell","stimulation","causes","DHA","to","be","released","from","the","sn-2","position","of","their","membrane-bound","cellular","phospholipid","pools","through","the","action","of","a","Phospholipase","A2","-type","enzyme","and","the","subsequent","attack","of","the","released","DHA","by","CYP450","epoxidases","."],"labels":["O","O","O","O","O","O","B-chemical compound","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-enzyme","I-enzyme","I-enzyme","I-enzyme","O","O","O","O","O","O","O","B-chemical compound","O","B-enzyme","I-enzyme","O"],"target_index":null,"target_label":null},"label_list":["enzyme","theory","scientist","university","chemical_compound","location","protein","academic_journal","event","organization","astronomical_object","award","chemical_element","discipline","country","person"]}
{"id":"10","dataset":"crossner_science","split":"test","instance":{"id":"10","prompt_labels":"Football(O) is(O) also(O) a(O) common(O) sport(O) and(O) the(O) multi-use(O) Saifur(B-location) Rahman(I-location) Stadium(I-location) and(O) Netaji(B-location) Subhas(I-location) Chandra(I-location) Bose(I-location) Stadium(I-location) s(O) are(O) known(O) to(O) host(O) football(O) matches(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, academic journal, discipline, award, organization, theory, chemical element, event, astronomical object, chemical compound, university, enzyme, location, scientist, country, person and O.\nSentence: Football is also a common sport and the multi-use Saifur Rahman Stadium and Netaji Subhas Chandra Bose Stadium s are known to host football matches .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Football","is","also","a","common","sport","and","the","multi-use","Saifur","Rahman","Stadium","and","Netaji","Subhas","Chandra","Bose","Stadium","s","are","known","to","host","football","matches","."],"labels":["O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","O","B-location","I-location","I-location","I-location","I-location","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["protein","academic_journal","discipline","award","organization","theory","chemical_element","event","astronomical_object","chemical_compound","university","enzyme","location","scientist","country","person"]}
{"id":"11","dataset":"crossner_science","split":"test","instance":{"id":"11","prompt_labels":"Khan(B-scientist) 's(O) research(O) focus(O) is(O) on(O) developing(O) new(O) inhibitors(O) against(O) multidrug(O) resistant(O) clinical(O) strains(O) with(O) special(O) interest(O) on(O) extended(B-enzyme) spectrum(I-enzyme) beta(I-enzyme) lactamases(I-enzyme) ((O) ESBL(B-enzyme) )(O) such(O) as(O) NDM-1(B-enzyme) and(O) Beta-lactamase(B-enzyme) ,(O) using(O) QSARR(O) modeling(O) and(O) structure-based(O) virtual(O) screening(O) methods(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, chemical element, university, person, organization, chemical compound, theory, discipline, location, astronomical object, country, academic journal, event, scientist, award, protein and O.\nSentence: Khan 's research focus is on developing new inhibitors against multidrug resistant clinical strains with special interest on extended spectrum beta lactamases ( ESBL ) such as NDM-1 and Beta-lactamase , using QSARR modeling and structure-based virtual screening methods .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Khan","'s","research","focus","is","on","developing","new","inhibitors","against","multidrug","resistant","clinical","strains","with","special","interest","on","extended","spectrum","beta","lactamases","(","ESBL",")","such","as","NDM-1","and","Beta-lactamase",",","using","QSARR","modeling","and","structure-based","virtual","screening","methods","."],"labels":["B-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-enzyme","I-enzyme","I-enzyme","I-enzyme","O","B-enzyme","O","O","O","B-enzyme","O","B-enzyme","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["enzyme","chemical_element","university","person","organization","chemical_compound","theory","discipline","location","astronomical_object","country","academic_journal","event","scientist","award","protein"]}
{"id":"12","dataset":"crossner_science","split":"test","instance":{"id":"12","prompt_labels":"Polyethylene(B-chemical compound) terephthalate(I-chemical compound) ((O) PET(B-chemical compound) )(O) bottles(O) are(O) made(O) from(O) ethylene(B-chemical compound) and(O) P-Xylene(B-chemical compound) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, university, person, award, astronomical object, discipline, country, chemical element, location, event, theory, organization, chemical compound, protein, academic journal, enzyme and O.\nSentence: Polyethylene terephthalate ( PET ) bottles are made from ethylene and P-Xylene .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Polyethylene","terephthalate","(","PET",")","bottles","are","made","from","ethylene","and","P-Xylene","."],"labels":["B-chemical compound","I-chemical compound","O","B-chemical compound","O","O","O","O","O","B-chemical compound","O","B-chemical compound","O"],"target_index":null,"target_label":null},"label_list":["scientist","university","person","award","astronomical_object","discipline","country","chemical_element","location","event","theory","organization","chemical_compound","protein","academic_journal","enzyme"]}
{"id":"13","dataset":"crossner_science","split":"test","instance":{"id":"13","prompt_labels":"Most(O) widely(O) used(O) to(O) assess(O) potential(O) synthetic(O) lethal(O) interactions(O) is(O) using(O) siRNA(O) and(O) CRISPR(O) to(O) modify(O) target(O) genes(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, theory, university, event, academic journal, award, chemical element, person, enzyme, location, protein, organization, astronomical object, discipline, chemical compound, country and O.\nSentence: Most widely used to assess potential synthetic lethal interactions is using siRNA and CRISPR to modify target genes .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Most","widely","used","to","assess","potential","synthetic","lethal","interactions","is","using","siRNA","and","CRISPR","to","modify","target","genes","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","theory","university","event","academic_journal","award","chemical_element","person","enzyme","location","protein","organization","astronomical_object","discipline","chemical_compound","country"]}
{"id":"14","dataset":"crossner_science","split":"test","instance":{"id":"14","prompt_labels":"In(O) the(O) low(O) pressure(O) process(O) -olefins(B-chemical compound) ((O) e.g.(O) 1-Butene(B-chemical compound) or(O) 1-Hexene(B-chemical compound) )(O) may(O) be(O) added(O) ,(O) which(O) are(O) incorporated(O) in(O) the(O) polymer(O) chain(O) during(O) polymerization(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, chemical compound, university, award, person, protein, discipline, event, organization, enzyme, location, theory, scientist, academic journal, astronomical object, chemical element and O.\nSentence: In the low pressure process -olefins ( e.g. 1-Butene or 1-Hexene ) may be added , which are incorporated in the polymer chain during polymerization .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","low","pressure","process","-olefins","(","e.g.","1-Butene","or","1-Hexene",")","may","be","added",",","which","are","incorporated","in","the","polymer","chain","during","polymerization","."],"labels":["O","O","O","O","O","B-chemical compound","O","O","B-chemical compound","O","B-chemical compound","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","chemical_compound","university","award","person","protein","discipline","event","organization","enzyme","location","theory","scientist","academic_journal","astronomical_object","chemical_element"]}
{"id":"15","dataset":"crossner_science","split":"test","instance":{"id":"15","prompt_labels":"Cory(B-scientist) 's(O) work(O) has(O) been(O) published(O) in(O) research(O) journals(O) including(O) Blood(B-academic journal) ((O) journal(O) )(O) ,(O) The(B-academic journal) EMBO(I-academic journal) Journal(I-academic journal) ,(O) Nature(B-academic journal) ((O) journal(O) )(O) ,(O) Cell(B-academic journal) Death(I-academic journal) &(I-academic journal) amp(I-academic journal) ;(I-academic journal) Differentiation(I-academic journal) ,(O) and(O) Proceedings(B-academic journal) of(I-academic journal) the(I-academic journal) National(I-academic journal) Academy(I-academic journal) of(I-academic journal) Sciences(I-academic journal) of(I-academic journal) the(I-academic journal) United(I-academic journal) States(I-academic journal) of(I-academic journal) America(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, enzyme, chemical element, discipline, person, protein, country, university, scientist, chemical compound, astronomical object, academic journal, location, organization, theory and O.\nSentence: Cory 's work has been published in research journals including Blood ( journal ) , The EMBO Journal , Nature ( journal ) , Cell Death & amp ; Differentiation , and Proceedings of the National Academy of Sciences of the United States of America .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Cory","'s","work","has","been","published","in","research","journals","including","Blood","(","journal",")",",","The","EMBO","Journal",",","Nature","(","journal",")",",","Cell","Death","&","amp",";","Differentiation",",","and","Proceedings","of","the","National","Academy","of","Sciences","of","the","United","States","of","America","."],"labels":["B-scientist","O","O","O","O","O","O","O","O","O","B-academic journal","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["event","award","enzyme","chemical_element","discipline","person","protein","country","university","scientist","chemical_compound","astronomical_object","academic_journal","location","organization","theory"]}
{"id":"16","dataset":"crossner_science","split":"test","instance":{"id":"16","prompt_labels":"Albarn(B-person) teamed(O) up(O) with(O) Robert(B-person) 3D(I-person) Del(I-person) Naja(I-person) of(O) Massive(B-organization) Attack(I-organization) and(O) worked(O) with(O) Stop(B-organization) the(I-organization) War(I-organization) Coalition(I-organization) ,(O) CND(B-organization) and(O) the(O) Muslim(B-organization) Association(I-organization) of(I-organization) Britain(I-organization) to(O) organise(O) campaigns(O) to(O) raise(O) awareness(O) of(O) the(O) potential(O) dangers(O) of(O) the(O) UK(B-country) 's(O) involvement(O) in(O) the(O) war(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, award, academic journal, event, country, discipline, scientist, location, theory, person, university, enzyme, astronomical object, chemical element, organization, chemical compound and O.\nSentence: Albarn teamed up with Robert 3D Del Naja of Massive Attack and worked with Stop the War Coalition , CND and the Muslim Association of Britain to organise campaigns to raise awareness of the potential dangers of the UK 's involvement in the war .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Albarn","teamed","up","with","Robert","3D","Del","Naja","of","Massive","Attack","and","worked","with","Stop","the","War","Coalition",",","CND","and","the","Muslim","Association","of","Britain","to","organise","campaigns","to","raise","awareness","of","the","potential","dangers","of","the","UK","'s","involvement","in","the","war","."],"labels":["B-person","O","O","O","B-person","I-person","I-person","I-person","O","B-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["protein","award","academic_journal","event","country","discipline","scientist","location","theory","person","university","enzyme","astronomical_object","chemical_element","organization","chemical_compound"]}
{"id":"19","dataset":"crossner_science","split":"test","instance":{"id":"19","prompt_labels":"It(O) displays(O) the(O) mean(O) time(O) ,(O) sidereal(O) ,(O) ((O) or(O) star(O) )(O) ,(O) time(O) and(O) the(O) motions(O) of(O) the(O) sun(B-astronomical object) ,(O) moon(B-astronomical object) and(O) the(O) five(O) then-known(O) planets(O) Venus(B-astronomical object) ,(O) Mars(B-astronomical object) ,(O) Saturn(B-astronomical object) ,(O) Mercury(B-astronomical object) ,(O) and(O) Jupiter(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, theory, scientist, person, country, award, enzyme, discipline, location, university, chemical element, event, organization, academic journal, protein, chemical compound and O.\nSentence: It displays the mean time , sidereal , ( or star ) , time and the motions of the sun , moon and the five then-known planets Venus , Mars , Saturn , Mercury , and Jupiter .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","displays","the","mean","time",",","sidereal",",","(","or","star",")",",","time","and","the","motions","of","the","sun",",","moon","and","the","five","then-known","planets","Venus",",","Mars",",","Saturn",",","Mercury",",","and","Jupiter","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","theory","scientist","person","country","award","enzyme","discipline","location","university","chemical_element","event","organization","academic_journal","protein","chemical_compound"]}
{"id":"21","dataset":"crossner_science","split":"test","instance":{"id":"21","prompt_labels":"In(O) the(O) second(O) season(O) ,(O) married(O) couple(O) Rose(B-person) Nadler(I-person) ((O) L.(B-person) Scott(I-person) Caldwell(I-person) )(O) and(O) Bernard(B-person) Nadler(I-person) ((O) Sam(B-person) Anderson(I-person) )(O) ,(O) separated(O) on(O) opposite(O) sides(O) of(O) the(O) island(O) ((O) she(O) with(O) the(O) main(O) characters(O) ,(O) he(O) with(O) the(O) tail(O) section(O) survivors(O) )(O) ,(O) were(O) featured(O) in(O) a(O) flashback(O) episode(O) after(O) being(O) reunited(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, scientist, academic journal, organization, chemical compound, chemical element, event, theory, person, discipline, country, award, enzyme, university, protein, astronomical object and O.\nSentence: In the second season , married couple Rose Nadler ( L. Scott Caldwell ) and Bernard Nadler ( Sam Anderson ) , separated on opposite sides of the island ( she with the main characters , he with the tail section survivors ) , were featured in a flashback episode after being reunited .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","second","season",",","married","couple","Rose","Nadler","(","L.","Scott","Caldwell",")","and","Bernard","Nadler","(","Sam","Anderson",")",",","separated","on","opposite","sides","of","the","island","(","she","with","the","main","characters",",","he","with","the","tail","section","survivors",")",",","were","featured","in","a","flashback","episode","after","being","reunited","."],"labels":["O","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","I-person","O","O","B-person","I-person","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","scientist","academic_journal","organization","chemical_compound","chemical_element","event","theory","person","discipline","country","award","enzyme","university","protein","astronomical_object"]}
{"id":"22","dataset":"crossner_science","split":"test","instance":{"id":"22","prompt_labels":"In(O) the(O) presence(O) of(O) Casup2(B-chemical element) +(I-chemical element) /(I-chemical element) sup(I-chemical element) ,(O) calmodulin(B-protein) and(O) S100(B-protein) protein(I-protein) bind(O) calponin(B-protein) at(O) the(O) region(O) of(O) actin-binding(O) sites(O) and(O) reverse(O) calponin(B-protein) 's(O) inhibition(O) of(O) myosin(B-enzyme) MgATPase(I-enzyme) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, organization, event, person, enzyme, protein, astronomical object, location, chemical element, scientist, university, theory, country, award, discipline, chemical compound and O.\nSentence: In the presence of Casup2 + / sup , calmodulin and S100 protein bind calponin at the region of actin-binding sites and reverse calponin 's inhibition of myosin MgATPase .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","presence","of","Casup2","+","/","sup",",","calmodulin","and","S100","protein","bind","calponin","at","the","region","of","actin-binding","sites","and","reverse","calponin","'s","inhibition","of","myosin","MgATPase","."],"labels":["O","O","O","O","B-chemical element","I-chemical element","I-chemical element","I-chemical element","O","B-protein","O","B-protein","I-protein","O","B-protein","O","O","O","O","O","O","O","O","B-protein","O","O","O","B-enzyme","I-enzyme","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","organization","event","person","enzyme","protein","astronomical_object","location","chemical_element","scientist","university","theory","country","award","discipline","chemical_compound"]}
{"id":"23","dataset":"crossner_science","split":"test","instance":{"id":"23","prompt_labels":"The(O) amplified(O) fragment(O) ((O) amplicon(O) )(O) is(O) incubated(O) with(O) the(O) restriction(O) enzyme(O) HaeIII(B-enzyme) ,(O) comprising(O) the(O) SNP(O) in(O) their(O) recognition(O) sequence(O) GGCC(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, discipline, scientist, location, event, organization, university, enzyme, protein, theory, astronomical object, person, chemical element, chemical compound, academic journal, award and O.\nSentence: The amplified fragment ( amplicon ) is incubated with the restriction enzyme HaeIII , comprising the SNP in their recognition sequence GGCC .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","amplified","fragment","(","amplicon",")","is","incubated","with","the","restriction","enzyme","HaeIII",",","comprising","the","SNP","in","their","recognition","sequence","GGCC","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-enzyme","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","discipline","scientist","location","event","organization","university","enzyme","protein","theory","astronomical_object","person","chemical_element","chemical_compound","academic_journal","award"]}
{"id":"25","dataset":"crossner_science","split":"test","instance":{"id":"25","prompt_labels":"Some(O) asteroid(O) s(O) ,(O) also(O) named(O) after(O) the(O) same(O) Shakespearean(O) characters(O) ,(O) share(O) names(O) with(O) moons(O) of(O) Uranus(B-astronomical object) :(O) 171(B-astronomical object) Ophelia(I-astronomical object) ,(O) 218(B-astronomical object) Bianca(I-astronomical object) ,(O) 593(B-astronomical object) Titania(I-astronomical object) ,(O) 666(B-astronomical object) Desdemona(I-astronomical object) ,(O) 763(B-astronomical object) Cupido(I-astronomical object) ,(O) and(O) 2758(B-astronomical object) Cordelia(I-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, award, discipline, country, scientist, astronomical object, chemical compound, academic journal, location, organization, protein, chemical element, university, theory, enzyme and O.\nSentence: Some asteroid s , also named after the same Shakespearean characters , share names with moons of Uranus : 171 Ophelia , 218 Bianca , 593 Titania , 666 Desdemona , 763 Cupido , and 2758 Cordelia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","asteroid","s",",","also","named","after","the","same","Shakespearean","characters",",","share","names","with","moons","of","Uranus",":","171","Ophelia",",","218","Bianca",",","593","Titania",",","666","Desdemona",",","763","Cupido",",","and","2758","Cordelia","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","B-astronomical object","I-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["person","event","award","discipline","country","scientist","astronomical_object","chemical_compound","academic_journal","location","organization","protein","chemical_element","university","theory","enzyme"]}
{"id":"27","dataset":"crossner_science","split":"test","instance":{"id":"27","prompt_labels":"GAR(B-chemical compound) +(O) 10-Formyltetrahydrofolate(B-chemical compound) (O) Phosphoribosyl-N-formylglycineamide(B-chemical compound) +(O) Tetrahydrofolic(B-chemical compound) acid(I-chemical compound)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, chemical compound, location, country, chemical element, discipline, academic journal, person, astronomical object, event, university, organization, theory, scientist, enzyme, award and O.\nSentence: GAR + 10-Formyltetrahydrofolate  Phosphoribosyl-N-formylglycineamide + Tetrahydrofolic acid","prediction_output":null,"prediction_outputs":null,"group":null,"words":["GAR","+","10-Formyltetrahydrofolate","","Phosphoribosyl-N-formylglycineamide","+","Tetrahydrofolic","acid"],"labels":["B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","I-chemical compound"],"target_index":null,"target_label":null},"label_list":["protein","chemical_compound","location","country","chemical_element","discipline","academic_journal","person","astronomical_object","event","university","organization","theory","scientist","enzyme","award"]}
{"id":"28","dataset":"crossner_science","split":"test","instance":{"id":"28","prompt_labels":"Alferov(B-scientist) and(O) colleagues(O) worked(O) on(O) Gallium(B-chemical compound) arsenide(I-chemical compound) and(O) Aluminium(B-chemical compound) arsenide(I-chemical compound) III-V(O) heterojunctions(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, country, astronomical object, person, theory, chemical compound, award, protein, event, enzyme, discipline, location, academic journal, organization, university, chemical element and O.\nSentence: Alferov and colleagues worked on Gallium arsenide and Aluminium arsenide III-V heterojunctions .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Alferov","and","colleagues","worked","on","Gallium","arsenide","and","Aluminium","arsenide","III-V","heterojunctions","."],"labels":["B-scientist","O","O","O","O","B-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","O","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","country","astronomical_object","person","theory","chemical_compound","award","protein","event","enzyme","discipline","location","academic_journal","organization","university","chemical_element"]}
{"id":"29","dataset":"crossner_science","split":"test","instance":{"id":"29","prompt_labels":"She(O) has(O) listed(O) Christina(B-person) Aguilera(I-person) ,(O) Jessica(B-person) Simpson(I-person) ,(O) Bobbie(B-person) Gentry(I-person) ,(O) Janis(B-person) Joplin(I-person) ,(O) Shelby(B-person) Lynne(I-person) ,(O) and(O) Shania(B-person) Twain(I-person) as(O) her(O) musical(O) influences(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, person, award, protein, country, academic journal, event, chemical element, enzyme, organization, chemical compound, astronomical object, theory, location, scientist, discipline and O.\nSentence: She has listed Christina Aguilera , Jessica Simpson , Bobbie Gentry , Janis Joplin , Shelby Lynne , and Shania Twain as her musical influences .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","has","listed","Christina","Aguilera",",","Jessica","Simpson",",","Bobbie","Gentry",",","Janis","Joplin",",","Shelby","Lynne",",","and","Shania","Twain","as","her","musical","influences","."],"labels":["O","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","O","B-person","I-person","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","person","award","protein","country","academic_journal","event","chemical_element","enzyme","organization","chemical_compound","astronomical_object","theory","location","scientist","discipline"]}
{"id":"30","dataset":"crossner_science","split":"test","instance":{"id":"30","prompt_labels":"The(O) Zeitschrift(B-academic journal) fr(I-academic journal) Naturforschung(I-academic journal) ((O) English(O) :(O) Journal(B-academic journal) for(I-academic journal) Nature(I-academic journal) Research(I-academic journal) )(O) was(O) established(O) in(O) 1946(O) by(O) the(O) Max(B-organization) Planck(I-organization) Institute(I-organization) and(O) the(B-academic journal) physical(I-academic journal) sciences(I-academic journal) ((I-academic journal) Zeitschrift(I-academic journal) fr(I-academic journal) Naturforschung(I-academic journal) A(I-academic journal) )(I-academic journal) were(O) separated(O) from(O) the(O) other(O) natural(O) sciences(O) ((O) Part(O) B(O) )(O) from(O) 1947(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, university, theory, discipline, chemical element, location, chemical compound, person, astronomical object, scientist, organization, academic journal, protein, country, enzyme and O.\nSentence: The Zeitschrift fr Naturforschung ( English : Journal for Nature Research ) was established in 1946 by the Max Planck Institute and the physical sciences ( Zeitschrift fr Naturforschung A ) were separated from the other natural sciences ( Part B ) from 1947 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Zeitschrift","fr","Naturforschung","(","English",":","Journal","for","Nature","Research",")","was","established","in","1946","by","the","Max","Planck","Institute","and","the","physical","sciences","(","Zeitschrift","fr","Naturforschung","A",")","were","separated","from","the","other","natural","sciences","(","Part","B",")","from","1947","."],"labels":["O","B-academic journal","I-academic journal","I-academic journal","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","award","university","theory","discipline","chemical_element","location","chemical_compound","person","astronomical_object","scientist","organization","academic_journal","protein","country","enzyme"]}
{"id":"32","dataset":"crossner_science","split":"test","instance":{"id":"32","prompt_labels":"He(O) is(O) the(O) recipient(O) of(O) a(O) Lifetime(B-award) Achievement(I-award) Award(I-award) from(I-award) the(I-award) The(I-award) Recording(I-award) Academy(I-award) and(O) a(O) Lifetime(B-award) Achievement(I-award) Award(I-award) from(I-award) the(I-award) Rhythm(I-award) and(I-award) Blues(I-award) Foundation(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, enzyme, theory, organization, event, country, scientist, chemical compound, discipline, astronomical object, person, protein, award, location, chemical element, academic journal and O.\nSentence: He is the recipient of a Lifetime Achievement Award from the The Recording Academy and a Lifetime Achievement Award from the Rhythm and Blues Foundation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","is","the","recipient","of","a","Lifetime","Achievement","Award","from","the","The","Recording","Academy","and","a","Lifetime","Achievement","Award","from","the","Rhythm","and","Blues","Foundation","."],"labels":["O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["university","enzyme","theory","organization","event","country","scientist","chemical_compound","discipline","astronomical_object","person","protein","award","location","chemical_element","academic_journal"]}
{"id":"33","dataset":"crossner_science","split":"test","instance":{"id":"33","prompt_labels":"Self-harmful(O) T(O) cells(O) ,(O) further(O) referred(O) to(O) as(O) autoreactive(O) T(O) cells(O) ,(O) originate(O) in(O) the(O) thymus(O) because(O) of(O) the(O) stochastic(O) process(O) called(O) V(O) ((O) D(O) )(O) J(O) recombination(O) which(O) conducts(O) the(O) generation(O) of(O) T-cell(B-protein) receptor(I-protein) ((O) TCRs(B-protein) )(O) and(O) enables(O) their(O) limitless(O) variability(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, organization, award, discipline, person, location, enzyme, protein, astronomical object, theory, university, chemical element, country, event, chemical compound, scientist and O.\nSentence: Self-harmful T cells , further referred to as autoreactive T cells , originate in the thymus because of the stochastic process called V ( D ) J recombination which conducts the generation of T-cell receptor ( TCRs ) and enables their limitless variability .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Self-harmful","T","cells",",","further","referred","to","as","autoreactive","T","cells",",","originate","in","the","thymus","because","of","the","stochastic","process","called","V","(","D",")","J","recombination","which","conducts","the","generation","of","T-cell","receptor","(","TCRs",")","and","enables","their","limitless","variability","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-protein","I-protein","O","B-protein","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","organization","award","discipline","person","location","enzyme","protein","astronomical_object","theory","university","chemical_element","country","event","chemical_compound","scientist"]}
{"id":"34","dataset":"crossner_science","split":"test","instance":{"id":"34","prompt_labels":"He(O) has(O) chaired(O) the(O) Southern(B-organization) Growth(I-organization) Policies(I-organization) Board(I-organization) ,(O) the(O) Southern(B-organization) Regional(I-organization) Education(I-organization) Board(I-organization) ,(O) the(B-organization) Southern(I-organization) Technology(I-organization) Council(I-organization) ,(O) the(O) Interstate(B-organization) Oil(I-organization) and(I-organization) Gas(I-organization) Compact(I-organization) Commission(I-organization) ,(O) and(O) the(O) Education(B-organization) Commission(I-organization) of(I-organization) the(I-organization) States(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, theory, country, astronomical object, scientist, location, event, chemical element, discipline, enzyme, person, chemical compound, organization, award, protein, academic journal and O.\nSentence: He has chaired the Southern Growth Policies Board , the Southern Regional Education Board , the Southern Technology Council , the Interstate Oil and Gas Compact Commission , and the Education Commission of the States .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","has","chaired","the","Southern","Growth","Policies","Board",",","the","Southern","Regional","Education","Board",",","the","Southern","Technology","Council",",","the","Interstate","Oil","and","Gas","Compact","Commission",",","and","the","Education","Commission","of","the","States","."],"labels":["O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["university","theory","country","astronomical_object","scientist","location","event","chemical_element","discipline","enzyme","person","chemical_compound","organization","award","protein","academic_journal"]}
{"id":"38","dataset":"crossner_science","split":"test","instance":{"id":"38","prompt_labels":"Currently(O) ,(O) three(O) major(O) groups(O) of(O) DNA(O) binding(O) proteins(O) have(O) been(O) predominantly(O) used(O) for(O) epigenome(O) editing(O) :(O) Zinc(B-protein) finger(I-protein) proteins(I-protein) ,(O) Transcription(B-protein) Activator-Like(I-protein) Effectors(I-protein) ((O) TALEs(B-protein) )(O) and(O) nuclease(O) deficient(O) Cas9(B-protein) fusions(O) ((O) CRISPR(B-protein) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, award, protein, event, location, enzyme, discipline, person, organization, country, university, theory, academic journal, scientist, chemical compound, astronomical object and O.\nSentence: Currently , three major groups of DNA binding proteins have been predominantly used for epigenome editing : Zinc finger proteins , Transcription Activator-Like Effectors ( TALEs ) and nuclease deficient Cas9 fusions ( CRISPR ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Currently",",","three","major","groups","of","DNA","binding","proteins","have","been","predominantly","used","for","epigenome","editing",":","Zinc","finger","proteins",",","Transcription","Activator-Like","Effectors","(","TALEs",")","and","nuclease","deficient","Cas9","fusions","(","CRISPR",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-protein","I-protein","I-protein","O","B-protein","I-protein","I-protein","O","B-protein","O","O","O","O","B-protein","O","O","B-protein","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","award","protein","event","location","enzyme","discipline","person","organization","country","university","theory","academic_journal","scientist","chemical_compound","astronomical_object"]}
{"id":"43","dataset":"crossner_science","split":"test","instance":{"id":"43","prompt_labels":"Further(O) story(O) arcs(O) which(O) provided(O) a(O) particular(O) focus(O) on(O) individual(O) characters(O) include(O) Addison(B-person) Montgomery-Shepherd(I-person) ((O) Kate(B-person) Walsh(I-person) )(O) facing(O) Derek(B-person) Shepherd(I-person) '(O) s(O) ((O) Patrick(B-person) Dempsey(I-person) )(O) desire(O) to(O) divorce(O) her(O) ,(O) Izzie(B-person) Stevens(I-person) ((O) Katherine(B-person) Heigl(I-person) )(O) coping(O) with(O) Denny(B-person) Duquette(I-person) '(O) s(O) ((O) Jeffrey(B-person) Dean(I-person) Morgan(I-person) )(O) death(O) and(O) the(O) repercussions(O) of(O) her(O) choice(O) to(O) quit(O) the(O) internship(O) program(O) ,(O) and(O) Meredith(B-person) Grey(I-person) ((O) Ellen(B-person) Pompeo(I-person) )(O) struggling(O) with(O) the(O) choice(O) between(O) Shepherd(B-person) and(O) Finn(B-person) Dandridge(I-person) ((O) Chris(B-person) O(I-person) 'Donnell(I-person) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, organization, chemical element, discipline, person, protein, academic journal, astronomical object, university, award, theory, country, chemical compound, scientist, enzyme and O.\nSentence: Further story arcs which provided a particular focus on individual characters include Addison Montgomery-Shepherd ( Kate Walsh ) facing Derek Shepherd ' s ( Patrick Dempsey ) desire to divorce her , Izzie Stevens ( Katherine Heigl ) coping with Denny Duquette ' s ( Jeffrey Dean Morgan ) death and the repercussions of her choice to quit the internship program , and Meredith Grey ( Ellen Pompeo ) struggling with the choice between Shepherd and Finn Dandridge ( Chris O 'Donnell ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Further","story","arcs","which","provided","a","particular","focus","on","individual","characters","include","Addison","Montgomery-Shepherd","(","Kate","Walsh",")","facing","Derek","Shepherd","'","s","(","Patrick","Dempsey",")","desire","to","divorce","her",",","Izzie","Stevens","(","Katherine","Heigl",")","coping","with","Denny","Duquette","'","s","(","Jeffrey","Dean","Morgan",")","death","and","the","repercussions","of","her","choice","to","quit","the","internship","program",",","and","Meredith","Grey","(","Ellen","Pompeo",")","struggling","with","the","choice","between","Shepherd","and","Finn","Dandridge","(","Chris","O","'Donnell",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","O","B-person","I-person","O","O","O","B-person","I-person","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","O","O","B-person","I-person","O","O","O","B-person","I-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","O","O","O","O","O","B-person","O","B-person","I-person","O","B-person","I-person","I-person","O","O"],"target_index":null,"target_label":null},"label_list":["event","location","organization","chemical_element","discipline","person","protein","academic_journal","astronomical_object","university","award","theory","country","chemical_compound","scientist","enzyme"]}
{"id":"44","dataset":"crossner_science","split":"test","instance":{"id":"44","prompt_labels":"Non-governmental(O) agencies(O) such(O) as(O) the(O) International(B-organization) Planned(I-organization) Parenthood(I-organization) Federation(I-organization) and(O) Marie(B-organization) Stopes(I-organization) International(I-organization) provide(O) contraceptive(O) advice(O) for(O) young(O) women(O) worldwide(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, chemical compound, organization, discipline, scientist, person, location, university, academic journal, astronomical object, protein, chemical element, award, country, enzyme, event and O.\nSentence: Non-governmental agencies such as the International Planned Parenthood Federation and Marie Stopes International provide contraceptive advice for young women worldwide .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Non-governmental","agencies","such","as","the","International","Planned","Parenthood","Federation","and","Marie","Stopes","International","provide","contraceptive","advice","for","young","women","worldwide","."],"labels":["O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["theory","chemical_compound","organization","discipline","scientist","person","location","university","academic_journal","astronomical_object","protein","chemical_element","award","country","enzyme","event"]}
{"id":"45","dataset":"crossner_science","split":"test","instance":{"id":"45","prompt_labels":"Together(O) ,(O) they(O) activate(O) multiple(O) signal(O) transduction(O) pathways(O) including(O) oxidant(O) stress(O) ,(O) tyrosine(B-enzyme) kinase(I-enzyme) s(O) ,(O) Protein(B-enzyme) kinase(I-enzyme) C(I-enzyme) ,(O) and(O) MAPK(B-enzyme) s(O) leading(O) to(O) activation(O) of(O) transcription(O) factors(O) such(O) as(O) NF-B(B-protein) ,(O) and(O) dysregulation(O) of(O) epigenetic(O) mechanisms(O) including(O) HKme(O) ,(O) histone(O) lysine(O) acetylation(O) ,(O) and(O) DNA(O) methylation(O) via(O) the(O) action(O) of(O) corresponding(O) methyltransferases(B-enzyme) ,(O) demethylases(B-enzyme) ,(O) acetylases(B-enzyme) ,(O) and(O) deacetylases(B-enzyme) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, university, location, astronomical object, organization, country, scientist, chemical compound, theory, enzyme, event, protein, chemical element, discipline, academic journal and O.\nSentence: Together , they activate multiple signal transduction pathways including oxidant stress , tyrosine kinase s , Protein kinase C , and MAPK s leading to activation of transcription factors such as NF-B , and dysregulation of epigenetic mechanisms including HKme , histone lysine acetylation , and DNA methylation via the action of corresponding methyltransferases , demethylases , acetylases , and deacetylases .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Together",",","they","activate","multiple","signal","transduction","pathways","including","oxidant","stress",",","tyrosine","kinase","s",",","Protein","kinase","C",",","and","MAPK","s","leading","to","activation","of","transcription","factors","such","as","NF-B",",","and","dysregulation","of","epigenetic","mechanisms","including","HKme",",","histone","lysine","acetylation",",","and","DNA","methylation","via","the","action","of","corresponding","methyltransferases",",","demethylases",",","acetylases",",","and","deacetylases","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-enzyme","I-enzyme","O","O","B-enzyme","I-enzyme","I-enzyme","O","O","B-enzyme","O","O","O","O","O","O","O","O","O","B-protein","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-enzyme","O","B-enzyme","O","B-enzyme","O","O","B-enzyme","O"],"target_index":null,"target_label":null},"label_list":["person","award","university","location","astronomical_object","organization","country","scientist","chemical_compound","theory","enzyme","event","protein","chemical_element","discipline","academic_journal"]}
{"id":"46","dataset":"crossner_science","split":"test","instance":{"id":"46","prompt_labels":"Young(B-scientist) 's(O) work(O) on(O) squid(O) giant(O) axons(O) was(O) utilized(O) by(O) Andrew(B-scientist) Huxley(I-scientist) and(O) Alan(B-scientist) Hodgkin(I-scientist) who(O) in(O) 1963(O) received(O) the(O) Nobel(B-award) Prize(I-award) for(O) their(O) work(O) on(O) the(O) conduction(O) of(O) action(O) potentials(O) along(O) nerve(O) fibres(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, protein, chemical element, chemical compound, location, organization, enzyme, university, discipline, astronomical object, event, scientist, country, academic journal, theory and O.\nSentence: Young 's work on squid giant axons was utilized by Andrew Huxley and Alan Hodgkin who in 1963 received the Nobel Prize for their work on the conduction of action potentials along nerve fibres .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Young","'s","work","on","squid","giant","axons","was","utilized","by","Andrew","Huxley","and","Alan","Hodgkin","who","in","1963","received","the","Nobel","Prize","for","their","work","on","the","conduction","of","action","potentials","along","nerve","fibres","."],"labels":["B-scientist","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O","O","B-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","person","protein","chemical_element","chemical_compound","location","organization","enzyme","university","discipline","astronomical_object","event","scientist","country","academic_journal","theory"]}
{"id":"47","dataset":"crossner_science","split":"test","instance":{"id":"47","prompt_labels":"The(O) group(O) included(O) the(O) physicists(O) Walther(B-scientist) Bothe(I-scientist) ,(O) Robert(B-scientist) Dpel(I-scientist) ,(O) Hans(B-scientist) Geiger(I-scientist) ,(O) Wolfgang(B-scientist) Gentner(I-scientist) ((O) probably(O) sent(O) by(O) Walther(B-scientist) Bothe(I-scientist) )(O) ,(O) Wilhelm(B-scientist) Hanle(I-scientist) ,(O) Gerhard(B-scientist) Hoffmann(I-scientist) ,(O) and(O) Georg(B-scientist) Joos(I-scientist) ;(O) Peter(B-scientist) Debye(I-scientist) was(O) invited(O) ,(O) but(O) he(O) did(O) not(O) attend(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, astronomical object, event, country, theory, chemical element, location, organization, academic journal, scientist, discipline, university, person, chemical compound, award, enzyme and O.\nSentence: The group included the physicists Walther Bothe , Robert Dpel , Hans Geiger , Wolfgang Gentner ( probably sent by Walther Bothe ) , Wilhelm Hanle , Gerhard Hoffmann , and Georg Joos ; Peter Debye was invited , but he did not attend .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","group","included","the","physicists","Walther","Bothe",",","Robert","Dpel",",","Hans","Geiger",",","Wolfgang","Gentner","(","probably","sent","by","Walther","Bothe",")",",","Wilhelm","Hanle",",","Gerhard","Hoffmann",",","and","Georg","Joos",";","Peter","Debye","was","invited",",","but","he","did","not","attend","."],"labels":["O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["protein","astronomical_object","event","country","theory","chemical_element","location","organization","academic_journal","scientist","discipline","university","person","chemical_compound","award","enzyme"]}
{"id":"49","dataset":"crossner_science","split":"test","instance":{"id":"49","prompt_labels":"During(O) his(O) academic(O) career(O) he(O) served(O) on(O) the(O) editorial(O) boards(O) of(O) the(O) Journal(B-academic journal) of(I-academic journal) Rational(I-academic journal) Mechanics(I-academic journal) and(I-academic journal) Analysis(I-academic journal) ,(O) Archive(B-academic journal) for(I-academic journal) Rational(I-academic journal) Mechanics(I-academic journal) and(I-academic journal) Analysis(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Elasticity(I-academic journal) ,(O) and(O) the(O) International(B-academic journal) Journal(I-academic journal) of(I-academic journal) Solids(I-academic journal) and(I-academic journal) Structures(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, discipline, astronomical object, theory, chemical element, award, chemical compound, event, person, country, enzyme, organization, university, location, protein, scientist and O.\nSentence: During his academic career he served on the editorial boards of the Journal of Rational Mechanics and Analysis , Archive for Rational Mechanics and Analysis , Journal of Elasticity , and the International Journal of Solids and Structures .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","his","academic","career","he","served","on","the","editorial","boards","of","the","Journal","of","Rational","Mechanics","and","Analysis",",","Archive","for","Rational","Mechanics","and","Analysis",",","Journal","of","Elasticity",",","and","the","International","Journal","of","Solids","and","Structures","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","discipline","astronomical_object","theory","chemical_element","award","chemical_compound","event","person","country","enzyme","organization","university","location","protein","scientist"]}
{"id":"52","dataset":"crossner_science","split":"test","instance":{"id":"52","prompt_labels":"Theoretical(O) modelling(O) of(O) two(O) of(O) these(O) super-Earths(O) ,(O) Kepler-62e(B-astronomical object) and(O) Kepler-62f(B-astronomical object) ,(O) suggests(O) both(O) could(O) be(O) solid(O) ,(O) either(O) rocky(O) or(O) rocky(O) with(O) frozen(O) water(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, chemical compound, academic journal, event, country, theory, organization, chemical element, location, enzyme, scientist, award, protein, university, astronomical object, person and O.\nSentence: Theoretical modelling of two of these super-Earths , Kepler-62e and Kepler-62f , suggests both could be solid , either rocky or rocky with frozen water .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Theoretical","modelling","of","two","of","these","super-Earths",",","Kepler-62e","and","Kepler-62f",",","suggests","both","could","be","solid",",","either","rocky","or","rocky","with","frozen","water","."],"labels":["O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["discipline","chemical_compound","academic_journal","event","country","theory","organization","chemical_element","location","enzyme","scientist","award","protein","university","astronomical_object","person"]}
{"id":"54","dataset":"crossner_science","split":"test","instance":{"id":"54","prompt_labels":"Nature(B-organization) Research(I-organization) also(O) publishes(O) other(O) specialized(O) journals(O) including(O) Nature(B-academic journal) Neuroscience(I-academic journal) ,(O) Nature(B-academic journal) Biotechnology(I-academic journal) ,(O) Nature(B-academic journal) Methods(I-academic journal) ,(O) the(O) Nature(O) Clinical(O) Practice(O) series(O) of(O) journals(O) ,(O) Nature(B-academic journal) Structural(I-academic journal) &(I-academic journal) Molecular(I-academic journal) Biology(I-academic journal) ,(O) Nature(B-academic journal) Chemistry(I-academic journal) ,(O) and(O) the(O) Nature(O) Reviews(O) series(O) of(O) journals(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, person, enzyme, award, discipline, event, university, academic journal, country, theory, scientist, astronomical object, protein, location, organization, chemical compound and O.\nSentence: Nature Research also publishes other specialized journals including Nature Neuroscience , Nature Biotechnology , Nature Methods , the Nature Clinical Practice series of journals , Nature Structural & Molecular Biology , Nature Chemistry , and the Nature Reviews series of journals .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Nature","Research","also","publishes","other","specialized","journals","including","Nature","Neuroscience",",","Nature","Biotechnology",",","Nature","Methods",",","the","Nature","Clinical","Practice","series","of","journals",",","Nature","Structural","&","Molecular","Biology",",","Nature","Chemistry",",","and","the","Nature","Reviews","series","of","journals","."],"labels":["B-organization","I-organization","O","O","O","O","O","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","person","enzyme","award","discipline","event","university","academic_journal","country","theory","scientist","astronomical_object","protein","location","organization","chemical_compound"]}
{"id":"60","dataset":"crossner_science","split":"test","instance":{"id":"60","prompt_labels":"In(O) 2007(O) the(O) British(B-organization) Biochemical(I-organization) Society(I-organization) was(O) given(O) a(O) grant(O) by(O) the(O) Wellcome(B-organization) Trust(I-organization) to(O) catalogue(O) and(O) preserve(O) the(O) 35(O) laboratory(O) notebooks(O) in(O) which(O) Sanger(B-scientist) recorded(O) his(O) research(O) from(O) 1944(O) to(O) 1983(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, university, enzyme, person, chemical compound, award, event, discipline, organization, theory, location, chemical element, academic journal, protein, scientist, astronomical object and O.\nSentence: In 2007 the British Biochemical Society was given a grant by the Wellcome Trust to catalogue and preserve the 35 laboratory notebooks in which Sanger recorded his research from 1944 to 1983 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2007","the","British","Biochemical","Society","was","given","a","grant","by","the","Wellcome","Trust","to","catalogue","and","preserve","the","35","laboratory","notebooks","in","which","Sanger","recorded","his","research","from","1944","to","1983","."],"labels":["O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","B-scientist","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","university","enzyme","person","chemical_compound","award","event","discipline","organization","theory","location","chemical_element","academic_journal","protein","scientist","astronomical_object"]}
{"id":"64","dataset":"crossner_science","split":"test","instance":{"id":"64","prompt_labels":"The(O) WH2(O) domain(O) is(O) found(O) as(O) a(O) modular(O) part(O) of(O) larger(O) proteins(O) ;(O) it(O) can(O) be(O) associated(O) with(O) the(O) WH1(O) domain(O) or(O) EVH1(O) domain(O) and(O) with(O) the(O) CRIB(O) domain(O) ,(O) and(O) the(O) WH2(O) domain(O) can(O) occur(O) as(O) a(O) tandem(O) repeat(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, protein, event, academic journal, country, university, chemical compound, organization, discipline, chemical element, award, astronomical object, theory, enzyme, scientist, location and O.\nSentence: The WH2 domain is found as a modular part of larger proteins ; it can be associated with the WH1 domain or EVH1 domain and with the CRIB domain , and the WH2 domain can occur as a tandem repeat .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","WH2","domain","is","found","as","a","modular","part","of","larger","proteins",";","it","can","be","associated","with","the","WH1","domain","or","EVH1","domain","and","with","the","CRIB","domain",",","and","the","WH2","domain","can","occur","as","a","tandem","repeat","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","protein","event","academic_journal","country","university","chemical_compound","organization","discipline","chemical_element","award","astronomical_object","theory","enzyme","scientist","location"]}
{"id":"65","dataset":"crossner_science","split":"test","instance":{"id":"65","prompt_labels":"The(O) song(O) was(O) also(O) nominated(O) for(O) a(O) Golden(B-award) Globe(I-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Song(I-award) and(O) an(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Song(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, university, academic journal, person, organization, award, protein, astronomical object, discipline, theory, chemical element, enzyme, country, scientist, event, location and O.\nSentence: The song was also nominated for a Golden Globe Award for Best Original Song and an Academy Award for Best Original Song .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","song","was","also","nominated","for","a","Golden","Globe","Award","for","Best","Original","Song","and","an","Academy","Award","for","Best","Original","Song","."],"labels":["O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","university","academic_journal","person","organization","award","protein","astronomical_object","discipline","theory","chemical_element","enzyme","country","scientist","event","location"]}
{"id":"67","dataset":"crossner_science","split":"test","instance":{"id":"67","prompt_labels":"The(O) journal(O) is(O) abstracted(O) and(O) indexed(O) in(O) Applied(B-academic journal) Mechanics(I-academic journal) Reviews(I-academic journal) ,(O) Current(O) Contents(O) /(O) Engineering(B-discipline) ,(O) Computing(B-discipline) &(I-discipline) Technology(I-discipline) ,(O) Current(O) Contents(O) /(O) Physics(B-discipline) ,(O) Chemical(B-discipline) ,(O) ;(O) Earth(B-discipline) Sciences(I-discipline) ,(O) Compendex(O) ,(O) Inspec(O) ,(O) Mathematical(B-academic journal) Reviews(I-academic journal) ,(O) Scopus(O) ,(O) and(O) Zentralblatt(O) MATH(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, protein, theory, chemical element, award, enzyme, scientist, chemical compound, astronomical object, country, university, location, academic journal, event, person, organization and O.\nSentence: The journal is abstracted and indexed in Applied Mechanics Reviews , Current Contents / Engineering , Computing & Technology , Current Contents / Physics , Chemical , ; Earth Sciences , Compendex , Inspec , Mathematical Reviews , Scopus , and Zentralblatt MATH .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","journal","is","abstracted","and","indexed","in","Applied","Mechanics","Reviews",",","Current","Contents","/","Engineering",",","Computing","&","Technology",",","Current","Contents","/","Physics",",","Chemical",",",";","Earth","Sciences",",","Compendex",",","Inspec",",","Mathematical","Reviews",",","Scopus",",","and","Zentralblatt","MATH","."],"labels":["O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","O","O","O","B-discipline","O","B-discipline","I-discipline","I-discipline","O","O","O","O","B-discipline","O","B-discipline","O","O","B-discipline","I-discipline","O","O","O","O","O","B-academic journal","I-academic journal","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["discipline","protein","theory","chemical_element","award","enzyme","scientist","chemical_compound","astronomical_object","country","university","location","academic_journal","event","person","organization"]}
{"id":"69","dataset":"crossner_science","split":"test","instance":{"id":"69","prompt_labels":"In(O) the(O) 1950s(O) ,(O) James(B-scientist) D.(I-scientist) Watson(I-scientist) ,(O) Francis(B-scientist) Crick(I-scientist) ,(O) Rosalind(B-scientist) Franklin(I-scientist) ,(O) and(O) Maurice(B-scientist) Wilkins(I-scientist) were(O) instrumental(O) in(O) solving(O) DNA(O) structure(O) and(O) suggesting(O) its(O) relationship(O) with(O) the(O) genetic(O) transfer(O) of(O) information(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, scientist, organization, chemical compound, enzyme, award, protein, discipline, academic journal, event, chemical element, theory, astronomical object, university, location and O.\nSentence: In the 1950s , James D. Watson , Francis Crick , Rosalind Franklin , and Maurice Wilkins were instrumental in solving DNA structure and suggesting its relationship with the genetic transfer of information .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","1950s",",","James","D.","Watson",",","Francis","Crick",",","Rosalind","Franklin",",","and","Maurice","Wilkins","were","instrumental","in","solving","DNA","structure","and","suggesting","its","relationship","with","the","genetic","transfer","of","information","."],"labels":["O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","person","scientist","organization","chemical_compound","enzyme","award","protein","discipline","academic_journal","event","chemical_element","theory","astronomical_object","university","location"]}
{"id":"71","dataset":"crossner_science","split":"test","instance":{"id":"71","prompt_labels":"Ring1A(B-protein) and(O) Ring1B(B-protein) ((O) which(O) are(O) also(O) known(O) as(O) Rnf1(B-protein) and(O) Rnf2(B-protein) ,(O) respectively(O) )(O) are(O) Ubiquitin(B-enzyme) ligase(I-enzyme) that(O) mark(O) lysine(O) 119(O) of(O) histone(O) H2A(O) with(O) a(O) single(O) ubiquitin(O) group(O) ((O) H2AK119ub(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, chemical compound, theory, academic journal, protein, chemical element, enzyme, event, university, organization, location, scientist, person, award, astronomical object, discipline and O.\nSentence: Ring1A and Ring1B ( which are also known as Rnf1 and Rnf2 , respectively ) are Ubiquitin ligase that mark lysine 119 of histone H2A with a single ubiquitin group ( H2AK119ub ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Ring1A","and","Ring1B","(","which","are","also","known","as","Rnf1","and","Rnf2",",","respectively",")","are","Ubiquitin","ligase","that","mark","lysine","119","of","histone","H2A","with","a","single","ubiquitin","group","(","H2AK119ub",")","."],"labels":["B-protein","O","B-protein","O","O","O","O","O","O","B-protein","O","B-protein","O","O","O","O","B-enzyme","I-enzyme","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","chemical_compound","theory","academic_journal","protein","chemical_element","enzyme","event","university","organization","location","scientist","person","award","astronomical_object","discipline"]}
{"id":"73","dataset":"crossner_science","split":"test","instance":{"id":"73","prompt_labels":"On(O) 11(O) August(O) 2014(O) ,(O) astronomers(O) released(O) studies(O) ,(O) using(O) the(O) Atacama(O) Large(O) Millimeter(O) /(O) Submillimeter(O) Array(O) ((O) ALMA(O) )(O) for(O) the(O) first(O) time(O) ,(O) that(O) detailed(O) the(O) distribution(O) of(O) HCN(O) ,(O) Hydrogen(B-chemical compound) isocyanide(I-chemical compound) ,(O) Formaldehyde(B-chemical compound) ,(O) and(O) dust(O) inside(O) the(O) comae(O) of(O) comet(O) s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, chemical element, country, protein, chemical compound, organization, astronomical object, academic journal, enzyme, scientist, discipline, theory, event, person, university and O.\nSentence: On 11 August 2014 , astronomers released studies , using the Atacama Large Millimeter / Submillimeter Array ( ALMA ) for the first time , that detailed the distribution of HCN , Hydrogen isocyanide , Formaldehyde , and dust inside the comae of comet s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","11","August","2014",",","astronomers","released","studies",",","using","the","Atacama","Large","Millimeter","/","Submillimeter","Array","(","ALMA",")","for","the","first","time",",","that","detailed","the","distribution","of","HCN",",","Hydrogen","isocyanide",",","Formaldehyde",",","and","dust","inside","the","comae","of","comet","s","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","I-chemical compound","O","B-chemical compound","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","location","chemical_element","country","protein","chemical_compound","organization","astronomical_object","academic_journal","enzyme","scientist","discipline","theory","event","person","university"]}
{"id":"74","dataset":"crossner_science","split":"test","instance":{"id":"74","prompt_labels":"He(O) is(O) currently(O) on(O) the(O) editorial(O) board(O) of(O) more(O) than(O) 10(O) scientific(O) journals(O) ,(O) among(O) them(O) Chaos(B-academic journal) ,(O) Philosophical(B-academic journal) Transactions(I-academic journal) of(I-academic journal) the(I-academic journal) Royal(I-academic journal) Society(I-academic journal) A(I-academic journal) ,(O) PLOS(B-academic journal) One(I-academic journal) ,(O) European(B-academic journal) Journal(I-academic journal) of(I-academic journal) Physics(I-academic journal) ,(O) J.(B-academic journal) Nonlinear(I-academic journal) Science(I-academic journal) and(O) Nonlinear(B-academic journal) Processes(I-academic journal) in(I-academic journal) Geophysics(I-academic journal) and(O) of(O) the(B-academic journal) Springer(I-academic journal) Series(I-academic journal) Complexity(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, astronomical object, university, award, location, organization, enzyme, chemical compound, event, person, chemical element, protein, scientist, country, theory, academic journal and O.\nSentence: He is currently on the editorial board of more than 10 scientific journals , among them Chaos , Philosophical Transactions of the Royal Society A , PLOS One , European Journal of Physics , J. Nonlinear Science and Nonlinear Processes in Geophysics and of the Springer Series Complexity .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","is","currently","on","the","editorial","board","of","more","than","10","scientific","journals",",","among","them","Chaos",",","Philosophical","Transactions","of","the","Royal","Society","A",",","PLOS","One",",","European","Journal","of","Physics",",","J.","Nonlinear","Science","and","Nonlinear","Processes","in","Geophysics","and","of","the","Springer","Series","Complexity","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["discipline","astronomical_object","university","award","location","organization","enzyme","chemical_compound","event","person","chemical_element","protein","scientist","country","theory","academic_journal"]}
{"id":"76","dataset":"crossner_science","split":"test","instance":{"id":"76","prompt_labels":"He(O) has(O) co-authored(O) a(O) number(O) of(O) frequently-cited(O) articles(O) in(O) Circulation(B-academic journal) Research(I-academic journal) ,(O) the(O) Journal(B-academic journal) of(I-academic journal) Biological(I-academic journal) Chemistry(I-academic journal) ,(O) and(O) the(O) American(B-academic journal) Journal(I-academic journal) of(I-academic journal) Physiology(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, organization, chemical compound, theory, protein, event, person, country, discipline, chemical element, astronomical object, academic journal, scientist, location, award, university and O.\nSentence: He has co-authored a number of frequently-cited articles in Circulation Research , the Journal of Biological Chemistry , and the American Journal of Physiology .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","has","co-authored","a","number","of","frequently-cited","articles","in","Circulation","Research",",","the","Journal","of","Biological","Chemistry",",","and","the","American","Journal","of","Physiology","."],"labels":["O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["enzyme","organization","chemical_compound","theory","protein","event","person","country","discipline","chemical_element","astronomical_object","academic_journal","scientist","location","award","university"]}
{"id":"79","dataset":"crossner_science","split":"test","instance":{"id":"79","prompt_labels":"In(O) 1827(O) he(O) was(O) elected(O) member(O) of(O) the(O) Royal(B-organization) Swedish(I-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) and(O) in(O) 1829(O) he(O) won(O) the(O) Gold(B-award) Medal(I-award) of(I-award) the(I-award) Royal(I-award) Astronomical(I-award) Society(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, chemical compound, theory, person, astronomical object, protein, event, university, discipline, enzyme, country, location, scientist, academic journal, organization, award and O.\nSentence: In 1827 he was elected member of the Royal Swedish Academy of Sciences , and in 1829 he won the Gold Medal of the Royal Astronomical Society .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1827","he","was","elected","member","of","the","Royal","Swedish","Academy","of","Sciences",",","and","in","1829","he","won","the","Gold","Medal","of","the","Royal","Astronomical","Society","."],"labels":["O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","chemical_compound","theory","person","astronomical_object","protein","event","university","discipline","enzyme","country","location","scientist","academic_journal","organization","award"]}
{"id":"80","dataset":"crossner_science","split":"test","instance":{"id":"80","prompt_labels":"The(O) album(O) earned(O) Usher(B-person) numerous(O) awards(O) ,(O) including(O) two(O) Grammy(B-award) Award(I-award) s(O) ,(O) three(O) Billboard(B-award) Music(I-award) Awards(I-award) and(O) a(O) BET(B-award) Awards(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, chemical compound, organization, theory, location, scientist, discipline, university, enzyme, event, award, protein, chemical element, academic journal, astronomical object and O.\nSentence: The album earned Usher numerous awards , including two Grammy Award s , three Billboard Music Awards and a BET Awards .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","album","earned","Usher","numerous","awards",",","including","two","Grammy","Award","s",",","three","Billboard","Music","Awards","and","a","BET","Awards","."],"labels":["O","O","O","B-person","O","O","O","O","O","B-award","I-award","O","O","O","B-award","I-award","I-award","O","O","B-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["person","country","chemical_compound","organization","theory","location","scientist","discipline","university","enzyme","event","award","protein","chemical_element","academic_journal","astronomical_object"]}
{"id":"81","dataset":"crossner_science","split":"test","instance":{"id":"81","prompt_labels":"Amphitrite(B-astronomical object) ((O) minor(O) planet(O) designation(O) :(O) 29(B-astronomical object) Amphridite(I-astronomical object) )(O) is(O) one(O) of(O) the(O) largest(O) S-type(O) asteroid(O) s(O) ,(O) approximately(O) in(O) diameter(O) ,(O) and(O) probably(O) third(O) largest(O) after(O) 15(B-astronomical object) Eunomia(I-astronomical object) and(O) 3(B-astronomical object) Juno(I-astronomical object) ,(O) although(O) 7(B-astronomical object) Iris(I-astronomical object) and(O) 532(B-astronomical object) Herculina(I-astronomical object) are(O) similar(O) in(O) size(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, chemical compound, enzyme, location, chemical element, scientist, discipline, academic journal, university, award, country, organization, astronomical object, protein, theory and O.\nSentence: Amphitrite ( minor planet designation : 29 Amphridite ) is one of the largest S-type asteroid s , approximately in diameter , and probably third largest after 15 Eunomia and 3 Juno , although 7 Iris and 532 Herculina are similar in size .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Amphitrite","(","minor","planet","designation",":","29","Amphridite",")","is","one","of","the","largest","S-type","asteroid","s",",","approximately","in","diameter",",","and","probably","third","largest","after","15","Eunomia","and","3","Juno",",","although","7","Iris","and","532","Herculina","are","similar","in","size","."],"labels":["B-astronomical object","O","O","O","O","O","B-astronomical object","I-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","event","chemical_compound","enzyme","location","chemical_element","scientist","discipline","academic_journal","university","award","country","organization","astronomical_object","protein","theory"]}
{"id":"84","dataset":"crossner_science","split":"test","instance":{"id":"84","prompt_labels":"He(O) competed(O) in(O) Swimming(O) at(O) the(O) 1988(B-event) Summer(I-event) Olympics(I-event) at(O) the(O) 1988(B-event) Summer(I-event) Olympics(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, theory, country, discipline, protein, award, chemical compound, person, organization, location, event, enzyme, chemical element, university, astronomical object, scientist and O.\nSentence: He competed in Swimming at the 1988 Summer Olympics at the 1988 Summer Olympics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","competed","in","Swimming","at","the","1988","Summer","Olympics","at","the","1988","Summer","Olympics","."],"labels":["O","O","O","O","O","O","B-event","I-event","I-event","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","theory","country","discipline","protein","award","chemical_compound","person","organization","location","event","enzyme","chemical_element","university","astronomical_object","scientist"]}
{"id":"87","dataset":"crossner_science","split":"test","instance":{"id":"87","prompt_labels":"In(O) recognition(O) of(O) his(O) contribution(O) to(O) the(O) creation(O) of(O) modern(O) electrical(B-discipline) science(I-discipline) ,(O) an(O) international(O) convention(O) ,(O) signed(O) at(O) the(O) 1881(B-event) International(I-event) Exposition(I-event) of(I-event) Electricity(I-event) ,(O) established(O) the(O) ampere(O) as(O) a(O) standard(O) unit(O) of(O) electrical(O) measurement(O) ,(O) along(O) with(O) the(O) coulomb(O) ,(O) volt(O) ,(O) ohm(O) ,(O) and(O) watt(O) ,(O) which(O) are(O) named(O) ,(O) respectively(O) ,(O) after(O) Ampre(B-scientist) 's(O) contemporaries(O) Charles-Augustin(B-scientist) de(I-scientist) Coulomb(I-scientist) of(O) France(B-country) ,(O) Alessandro(B-scientist) Volta(I-scientist) of(O) Italy(B-country) ,(O) Georg(B-scientist) Ohm(I-scientist) of(O) Germany(B-country) ,(O) and(O) James(B-scientist) Watt(I-scientist) of(O) Scotland(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, enzyme, chemical compound, person, organization, event, theory, protein, astronomical object, chemical element, country, location, scientist, discipline, university, academic journal and O.\nSentence: In recognition of his contribution to the creation of modern electrical science , an international convention , signed at the 1881 International Exposition of Electricity , established the ampere as a standard unit of electrical measurement , along with the coulomb , volt , ohm , and watt , which are named , respectively , after Ampre 's contemporaries Charles-Augustin de Coulomb of France , Alessandro Volta of Italy , Georg Ohm of Germany , and James Watt of Scotland .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","recognition","of","his","contribution","to","the","creation","of","modern","electrical","science",",","an","international","convention",",","signed","at","the","1881","International","Exposition","of","Electricity",",","established","the","ampere","as","a","standard","unit","of","electrical","measurement",",","along","with","the","coulomb",",","volt",",","ohm",",","and","watt",",","which","are","named",",","respectively",",","after","Ampre","'s","contemporaries","Charles-Augustin","de","Coulomb","of","France",",","Alessandro","Volta","of","Italy",",","Georg","Ohm","of","Germany",",","and","James","Watt","of","Scotland","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-discipline","I-discipline","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","O","O","B-scientist","I-scientist","I-scientist","O","B-country","O","B-scientist","I-scientist","O","B-country","O","B-scientist","I-scientist","O","B-country","O","O","B-scientist","I-scientist","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["award","enzyme","chemical_compound","person","organization","event","theory","protein","astronomical_object","chemical_element","country","location","scientist","discipline","university","academic_journal"]}
{"id":"88","dataset":"crossner_science","split":"test","instance":{"id":"88","prompt_labels":"Five(O) out(O) of(O) Finland(B-person) 's(O) 15(O) universities(O) ref(O) group(O) =(O) note(O) These(O) are(O) Aalto(B-university) University(I-university) ,(O) the(O) University(B-university) of(I-university) Helsinki(I-university) ,(O) the(O) University(B-university) of(I-university) the(I-university) Arts(I-university) Helsinki(I-university) ,(O) the(O) Hanken(B-university) School(I-university) of(I-university) Economics(I-university) ,(O) and(O) the(O) National(B-university) Defence(I-university) University(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, university, person, astronomical object, award, event, theory, chemical compound, chemical element, country, academic journal, discipline, location, enzyme, scientist, organization and O.\nSentence: Five out of Finland 's 15 universities ref group = note These are Aalto University , the University of Helsinki , the University of the Arts Helsinki , the Hanken School of Economics , and the National Defence University .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Five","out","of","Finland","'s","15","universities","ref","group","=","note","These","are","Aalto","University",",","the","University","of","Helsinki",",","the","University","of","the","Arts","Helsinki",",","the","Hanken","School","of","Economics",",","and","the","National","Defence","University","."],"labels":["O","O","O","B-person","O","O","O","O","O","O","O","O","O","B-university","I-university","O","O","B-university","I-university","I-university","O","O","B-university","I-university","I-university","I-university","I-university","O","O","B-university","I-university","I-university","I-university","O","O","O","B-university","I-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["protein","university","person","astronomical_object","award","event","theory","chemical_compound","chemical_element","country","academic_journal","discipline","location","enzyme","scientist","organization"]}
{"id":"89","dataset":"crossner_science","split":"test","instance":{"id":"89","prompt_labels":"The(O) extended(O) double(O) stranded(O) adapters(O) are(O) cleaved(O) by(O) Restriction(B-enzyme) enzyme(I-enzyme) at(O) a(O) specific(O) restriction(O) site(O) located(O) at(O) 3(O) side(O) of(O) the(O) tag(O) sequence(O) and(O) will(O) results(O) in(O) a(O) 3(O) -dT(O) overhang(O) that(O) will(O) be(O) ligated(O) to(O) the(O) 3(O) -dA(O) overhang(O) on(O) DNA(O) libraries(O) in(O) adapter(O) ligation(O) step(O) ((O) Figure(O) 1(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, university, enzyme, person, location, country, academic journal, event, award, discipline, scientist, astronomical object, organization, chemical element, protein, theory and O.\nSentence: The extended double stranded adapters are cleaved by Restriction enzyme at a specific restriction site located at 3 side of the tag sequence and will results in a 3 -dT overhang that will be ligated to the 3 -dA overhang on DNA libraries in adapter ligation step ( Figure 1 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","extended","double","stranded","adapters","are","cleaved","by","Restriction","enzyme","at","a","specific","restriction","site","located","at","3","side","of","the","tag","sequence","and","will","results","in","a","3","-dT","overhang","that","will","be","ligated","to","the","3","-dA","overhang","on","DNA","libraries","in","adapter","ligation","step","(","Figure","1",")","."],"labels":["O","O","O","O","O","O","O","O","B-enzyme","I-enzyme","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","university","enzyme","person","location","country","academic_journal","event","award","discipline","scientist","astronomical_object","organization","chemical_element","protein","theory"]}
{"id":"90","dataset":"crossner_science","split":"test","instance":{"id":"90","prompt_labels":"50000(B-astronomical object) Quaoar(I-astronomical object) and(O) 90377(B-astronomical object) Sedna(I-astronomical object) are(O) two(O) Solar(O) System(O) objects(O) discovered(O) in(O) this(O) way(O) by(O) Michael(B-scientist) E.(I-scientist) Brown(I-scientist) and(O) others(O) at(O) Caltech(B-university) using(O) the(O) Palomar(B-location) Observatory(I-location) '(O) s(O) Samuel(O) Oschin(O) telescope(O) of(O) and(O) the(O) Palomar-Quest(O) large-area(O) CCD(O) camera(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, academic journal, award, chemical compound, protein, discipline, scientist, organization, person, event, chemical element, country, astronomical object, enzyme, location, university and O.\nSentence: 50000 Quaoar and 90377 Sedna are two Solar System objects discovered in this way by Michael E. Brown and others at Caltech using the Palomar Observatory ' s Samuel Oschin telescope of and the Palomar-Quest large-area CCD camera .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["50000","Quaoar","and","90377","Sedna","are","two","Solar","System","objects","discovered","in","this","way","by","Michael","E.","Brown","and","others","at","Caltech","using","the","Palomar","Observatory","'","s","Samuel","Oschin","telescope","of","and","the","Palomar-Quest","large-area","CCD","camera","."],"labels":["B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","O","O","B-university","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["theory","academic_journal","award","chemical_compound","protein","discipline","scientist","organization","person","event","chemical_element","country","astronomical_object","enzyme","location","university"]}
{"id":"91","dataset":"crossner_science","split":"test","instance":{"id":"91","prompt_labels":"He(O) represented(O) his(O) country(O) at(O) the(O) 2017(B-event) World(I-event) Championships(I-event) in(I-event) Athletics(I-event) without(O) reaching(O) the(O) semifinals(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, enzyme, chemical element, event, organization, academic journal, location, award, theory, university, chemical compound, country, person, discipline, protein, scientist and O.\nSentence: He represented his country at the 2017 World Championships in Athletics without reaching the semifinals .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","represented","his","country","at","the","2017","World","Championships","in","Athletics","without","reaching","the","semifinals","."],"labels":["O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","enzyme","chemical_element","event","organization","academic_journal","location","award","theory","university","chemical_compound","country","person","discipline","protein","scientist"]}
{"id":"93","dataset":"crossner_science","split":"test","instance":{"id":"93","prompt_labels":"The(O) apparently(O) regulated(O) genes(O) encode(O) subunits(O) of(O) RNA(B-enzyme) polymerase(I-enzyme) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, theory, chemical compound, enzyme, person, academic journal, event, chemical element, award, organization, location, country, protein, astronomical object, university, discipline and O.\nSentence: The apparently regulated genes encode subunits of RNA polymerase .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","apparently","regulated","genes","encode","subunits","of","RNA","polymerase","."],"labels":["O","O","O","O","O","O","O","B-enzyme","I-enzyme","O"],"target_index":null,"target_label":null},"label_list":["scientist","theory","chemical_compound","enzyme","person","academic_journal","event","chemical_element","award","organization","location","country","protein","astronomical_object","university","discipline"]}
{"id":"94","dataset":"crossner_science","split":"test","instance":{"id":"94","prompt_labels":"STAT6(O) ,(O) IRF4(B-protein) ,(O) GATA3(O) are(O) absolutely(O) required(O) for(O) TsubH(O) /(O) sub9(O) cell(O) development(O) and(O) other(O) such(O) as(O) PU.1(B-protein) ,(O) BATF(O) ,(O) NF-B(B-protein) ,(O) NFAT1(O) ,(O) STAT5(B-protein) ,(O) AP-1(O) transcription(O) factor(O) contribute(O) to(O) TsubH(O) /(O) sub9(O) sub-population(O) commitment(O) and(O) to(O) IL-9(O) production(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, theory, country, event, academic journal, scientist, discipline, chemical compound, astronomical object, person, award, protein, organization, university, enzyme, chemical element and O.\nSentence: STAT6 , IRF4 , GATA3 are absolutely required for TsubH / sub9 cell development and other such as PU.1 , BATF , NF-B , NFAT1 , STAT5 , AP-1 transcription factor contribute to TsubH / sub9 sub-population commitment and to IL-9 production .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["STAT6",",","IRF4",",","GATA3","are","absolutely","required","for","TsubH","/","sub9","cell","development","and","other","such","as","PU.1",",","BATF",",","NF-B",",","NFAT1",",","STAT5",",","AP-1","transcription","factor","contribute","to","TsubH","/","sub9","sub-population","commitment","and","to","IL-9","production","."],"labels":["O","O","B-protein","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-protein","O","O","O","B-protein","O","O","O","B-protein","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","theory","country","event","academic_journal","scientist","discipline","chemical_compound","astronomical_object","person","award","protein","organization","university","enzyme","chemical_element"]}
{"id":"96","dataset":"crossner_science","split":"test","instance":{"id":"96","prompt_labels":"These(O) include(O) the(O) Naval(B-location) Base(I-location) San(I-location) Diego(I-location) ,(O) Marine(B-location) Corps(I-location) Base(I-location) Camp(I-location) Pendleton(I-location) ,(O) Marine(B-location) Corps(I-location) Air(I-location) Station(I-location) Miramar(I-location) ,(O) and(O) Naval(B-location) Air(I-location) Station(I-location) North(I-location) Island(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, location, academic journal, theory, university, astronomical object, protein, scientist, country, enzyme, award, event, discipline, person, chemical compound, organization and O.\nSentence: These include the Naval Base San Diego , Marine Corps Base Camp Pendleton , Marine Corps Air Station Miramar , and Naval Air Station North Island .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["These","include","the","Naval","Base","San","Diego",",","Marine","Corps","Base","Camp","Pendleton",",","Marine","Corps","Air","Station","Miramar",",","and","Naval","Air","Station","North","Island","."],"labels":["O","O","O","B-location","I-location","I-location","I-location","O","B-location","I-location","I-location","I-location","I-location","O","B-location","I-location","I-location","I-location","I-location","O","O","B-location","I-location","I-location","I-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","location","academic_journal","theory","university","astronomical_object","protein","scientist","country","enzyme","award","event","discipline","person","chemical_compound","organization"]}
{"id":"98","dataset":"crossner_science","split":"test","instance":{"id":"98","prompt_labels":"He(O) was(O) a(O) life(O) member(O) of(O) the(O) Franklin(B-organization) Institute(I-organization) ,(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Engineering(I-organization) and(O) the(O) Society(B-organization) for(I-organization) Advancement(I-organization) of(I-organization) Management(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, astronomical object, academic journal, country, award, organization, person, scientist, university, enzyme, chemical element, location, event, theory, chemical compound, discipline and O.\nSentence: He was a life member of the Franklin Institute , the National Academy of Engineering and the Society for Advancement of Management .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","a","life","member","of","the","Franklin","Institute",",","the","National","Academy","of","Engineering","and","the","Society","for","Advancement","of","Management","."],"labels":["O","O","O","O","O","O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["protein","astronomical_object","academic_journal","country","award","organization","person","scientist","university","enzyme","chemical_element","location","event","theory","chemical_compound","discipline"]}
{"id":"99","dataset":"crossner_science","split":"test","instance":{"id":"99","prompt_labels":"The(O) principal(O) candidates(O) for(O) the(O) origin(O) of(O) Mars(B-astronomical object) '(O) methane(B-chemical compound) include(O) non-biological(O) processes(O) such(O) as(O) Properties(O) of(O) water(O) -rock(O) reactions(O) ,(O) radiolysis(O) of(O) water(O) ,(O) and(O) pyrite(O) formation(O) ,(O) all(O) of(O) which(O) produce(O) Hydrogen(B-chemical element) that(O) could(O) then(O) generate(O) methane(B-chemical compound) and(O) other(O) hydrocarbons(B-chemical compound) via(O) Fischer-Tropsch(O) synthesis(O) with(O) Carbon(B-chemical compound) monoxide(I-chemical compound) and(O) COsub2(B-chemical compound) /(I-chemical compound) sub(I-chemical compound) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, event, discipline, award, protein, scientist, location, organization, enzyme, astronomical object, university, chemical element, academic journal, person, chemical compound, country and O.\nSentence: The principal candidates for the origin of Mars ' methane include non-biological processes such as Properties of water -rock reactions , radiolysis of water , and pyrite formation , all of which produce Hydrogen that could then generate methane and other hydrocarbons via Fischer-Tropsch synthesis with Carbon monoxide and COsub2 / sub .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","principal","candidates","for","the","origin","of","Mars","'","methane","include","non-biological","processes","such","as","Properties","of","water","-rock","reactions",",","radiolysis","of","water",",","and","pyrite","formation",",","all","of","which","produce","Hydrogen","that","could","then","generate","methane","and","other","hydrocarbons","via","Fischer-Tropsch","synthesis","with","Carbon","monoxide","and","COsub2","/","sub","."],"labels":["O","O","O","O","O","O","O","B-astronomical object","O","B-chemical compound","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical element","O","O","O","O","B-chemical compound","O","O","B-chemical compound","O","O","O","O","B-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","I-chemical compound","O"],"target_index":null,"target_label":null},"label_list":["theory","event","discipline","award","protein","scientist","location","organization","enzyme","astronomical_object","university","chemical_element","academic_journal","person","chemical_compound","country"]}
{"id":"100","dataset":"crossner_science","split":"test","instance":{"id":"100","prompt_labels":"Such(O) a(O) definition(O) of(O) the(O) term(O) planet(O) could(O) also(O) have(O) led(O) to(O) changes(O) in(O) classification(O) for(O) the(O) trans-Neptunian(O) object(O) s(O) ,(O) and(O) the(O) asteroids(O) 4(B-astronomical object) Vesta(I-astronomical object) ,(O) 2(B-astronomical object) Pallas(I-astronomical object) ,(O) and(O) 10(B-astronomical object) Hygiea(I-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, enzyme, scientist, astronomical object, protein, organization, country, location, theory, discipline, academic journal, university, chemical element, award, person, event and O.\nSentence: Such a definition of the term planet could also have led to changes in classification for the trans-Neptunian object s , and the asteroids 4 Vesta , 2 Pallas , and 10 Hygiea .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Such","a","definition","of","the","term","planet","could","also","have","led","to","changes","in","classification","for","the","trans-Neptunian","object","s",",","and","the","asteroids","4","Vesta",",","2","Pallas",",","and","10","Hygiea","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","B-astronomical object","I-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","enzyme","scientist","astronomical_object","protein","organization","country","location","theory","discipline","academic_journal","university","chemical_element","award","person","event"]}
{"id":"101","dataset":"crossner_science","split":"test","instance":{"id":"101","prompt_labels":"The(O) minor(O) planets(O) 1062(B-astronomical object) Ljuba(I-astronomical object) and(O) 1086(B-astronomical object) Nata(I-astronomical object) were(O) also(O) named(O) after(O) Soviet(O) female(O) paratroopers(O) Lyuba(B-person) Berlin(I-person) ((O) 1915-1936(O) )(O) and(O) Nata(B-person) Babushkina(I-person) ((O) 1915-1936(O) )(O) ,(O) respectively(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, astronomical object, award, discipline, university, country, theory, location, organization, person, protein, chemical element, enzyme, event, scientist, academic journal and O.\nSentence: The minor planets 1062 Ljuba and 1086 Nata were also named after Soviet female paratroopers Lyuba Berlin ( 1915-1936 ) and Nata Babushkina ( 1915-1936 ) , respectively .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","minor","planets","1062","Ljuba","and","1086","Nata","were","also","named","after","Soviet","female","paratroopers","Lyuba","Berlin","(","1915-1936",")","and","Nata","Babushkina","(","1915-1936",")",",","respectively","."],"labels":["O","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","B-person","I-person","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","astronomical_object","award","discipline","university","country","theory","location","organization","person","protein","chemical_element","enzyme","event","scientist","academic_journal"]}
{"id":"102","dataset":"crossner_science","split":"test","instance":{"id":"102","prompt_labels":"This(O) minor(O) planet(O) was(O) named(O) for(O) Aesculapius(B-astronomical object) ,(O) the(O) Greek(O) and(O) Roman(O) demigod(O) of(O) medicine(B-discipline) and(O) healing(O) ,(O) son(O) of(O) Apollo(B-person) and(O) Coronis(B-person) ,(O) after(O) whom(O) the(O) asteroids(O) 158(B-astronomical object) Koronis(I-astronomical object) and(O) 1862(B-astronomical object) Apollo(I-astronomical object) are(O) named(O) ,(O) respectively(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, chemical element, scientist, event, discipline, academic journal, award, chemical compound, organization, astronomical object, enzyme, location, protein, theory, university and O.\nSentence: This minor planet was named for Aesculapius , the Greek and Roman demigod of medicine and healing , son of Apollo and Coronis , after whom the asteroids 158 Koronis and 1862 Apollo are named , respectively .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","minor","planet","was","named","for","Aesculapius",",","the","Greek","and","Roman","demigod","of","medicine","and","healing",",","son","of","Apollo","and","Coronis",",","after","whom","the","asteroids","158","Koronis","and","1862","Apollo","are","named",",","respectively","."],"labels":["O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","B-discipline","O","O","O","O","O","B-person","O","B-person","O","O","O","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","person","chemical_element","scientist","event","discipline","academic_journal","award","chemical_compound","organization","astronomical_object","enzyme","location","protein","theory","university"]}
{"id":"104","dataset":"crossner_science","split":"test","instance":{"id":"104","prompt_labels":"It(O) is(O) probably(O) the(O) fifth-(O) most-massive(O) asteroid(O) after(O) Ceres(B-astronomical object) ,(O) 4(B-astronomical object) Vesta(I-astronomical object) ,(O) 2(B-astronomical object) Pallas(I-astronomical object) ,(O) and(O) 10(B-astronomical object) Hygiea(I-astronomical object) ,(O) with(O) a(O) mass(O) estimated(O) to(O) be(O) 1.2(O) %(O) of(O) the(O) mass(O) of(O) the(O) entire(O) asteroid(O) belt(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, chemical element, person, university, academic journal, location, organization, astronomical object, discipline, chemical compound, country, event, protein, theory, award, scientist and O.\nSentence: It is probably the fifth- most-massive asteroid after Ceres , 4 Vesta , 2 Pallas , and 10 Hygiea , with a mass estimated to be 1.2 % of the mass of the entire asteroid belt .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","is","probably","the","fifth-","most-massive","asteroid","after","Ceres",",","4","Vesta",",","2","Pallas",",","and","10","Hygiea",",","with","a","mass","estimated","to","be","1.2","%","of","the","mass","of","the","entire","asteroid","belt","."],"labels":["O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","B-astronomical object","I-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["enzyme","chemical_element","person","university","academic_journal","location","organization","astronomical_object","discipline","chemical_compound","country","event","protein","theory","award","scientist"]}
{"id":"107","dataset":"crossner_science","split":"test","instance":{"id":"107","prompt_labels":"Closer(O) to(O) the(O) city(O) ,(O) several(O) sporting(O) complexes(O) are(O) located(O) ,(O) including(O) the(O) Hobart(B-location) Aquatic(I-location) Centre(I-location) ,(O) Hobart(B-location) International(I-location) Tennis(I-location) Centre(I-location) the(O) Domain(B-location) Athletic(I-location) Centre(I-location) ,(O) and(O) the(O) Hobart(B-location) TCA(I-location) Ground(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, university, protein, person, chemical element, enzyme, chemical compound, location, organization, astronomical object, event, scientist, academic journal, award, theory, country and O.\nSentence: Closer to the city , several sporting complexes are located , including the Hobart Aquatic Centre , Hobart International Tennis Centre the Domain Athletic Centre , and the Hobart TCA Ground .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Closer","to","the","city",",","several","sporting","complexes","are","located",",","including","the","Hobart","Aquatic","Centre",",","Hobart","International","Tennis","Centre","the","Domain","Athletic","Centre",",","and","the","Hobart","TCA","Ground","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","O","B-location","I-location","I-location","I-location","O","B-location","I-location","I-location","O","O","O","B-location","I-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["discipline","university","protein","person","chemical_element","enzyme","chemical_compound","location","organization","astronomical_object","event","scientist","academic_journal","award","theory","country"]}
{"id":"109","dataset":"crossner_science","split":"test","instance":{"id":"109","prompt_labels":"Juilfs(B-scientist) was(O) a(O) theoretical(O) physics(O) assistant(O) from(O) 1938(O) to(O) 1945(O) at(O) the(O) Kaiser-Wilhelm(B-organization) Institut(I-organization) fr(I-organization) Physik(I-organization) ((O) KWIP(B-organization) ,(O) Kaiser(B-organization) Wilhelm(I-organization) Institute(I-organization) for(I-organization) Physics(I-organization) ;(O) today(O) ,(O) the(O) Max-Planck(B-organization) Institut(I-organization) fr(I-organization) Physik(I-organization) )(O) ,(O) first(O) for(O) Max(B-scientist) von(I-scientist) Laue(I-scientist) and(O) from(O) 1943(O) to(O) Werner(B-scientist) Heisenberg(I-scientist) and(O) Hentschel(B-scientist) ,(O) 1996(O) ,(O) Appendix(O) F(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, academic journal, person, location, chemical compound, scientist, chemical element, enzyme, theory, country, event, university, organization, discipline, protein, award and O.\nSentence: Juilfs was a theoretical physics assistant from 1938 to 1945 at the Kaiser-Wilhelm Institut fr Physik ( KWIP , Kaiser Wilhelm Institute for Physics ; today , the Max-Planck Institut fr Physik ) , first for Max von Laue and from 1943 to Werner Heisenberg and Hentschel , 1996 , Appendix F .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Juilfs","was","a","theoretical","physics","assistant","from","1938","to","1945","at","the","Kaiser-Wilhelm","Institut","fr","Physik","(","KWIP",",","Kaiser","Wilhelm","Institute","for","Physics",";","today",",","the","Max-Planck","Institut","fr","Physik",")",",","first","for","Max","von","Laue","and","from","1943","to","Werner","Heisenberg","and","Hentschel",",","1996",",","Appendix","F","."],"labels":["B-scientist","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","B-scientist","I-scientist","I-scientist","O","O","O","O","B-scientist","I-scientist","O","B-scientist","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","academic_journal","person","location","chemical_compound","scientist","chemical_element","enzyme","theory","country","event","university","organization","discipline","protein","award"]}
{"id":"110","dataset":"crossner_science","split":"test","instance":{"id":"110","prompt_labels":"Excessive(O) amounts(O) of(O) chlorine(B-chemical element) in(O) the(O) buffer(O) should(O) also(O) be(O) avoided(O) ,(O) since(O) this(O) will(O) overlap(O) with(O) the(O) sulfur(B-chemical element) peak(O) ;(O) Potassium(B-chemical compound) bromide(I-chemical compound) and(O) Sodium(B-chemical compound) bromide(I-chemical compound) are(O) suitable(O) alternatives(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, academic journal, chemical compound, country, event, university, chemical element, award, protein, scientist, person, organization, location, discipline, astronomical object, enzyme and O.\nSentence: Excessive amounts of chlorine in the buffer should also be avoided , since this will overlap with the sulfur peak ; Potassium bromide and Sodium bromide are suitable alternatives .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Excessive","amounts","of","chlorine","in","the","buffer","should","also","be","avoided",",","since","this","will","overlap","with","the","sulfur","peak",";","Potassium","bromide","and","Sodium","bromide","are","suitable","alternatives","."],"labels":["O","O","O","B-chemical element","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical element","O","O","B-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["theory","academic_journal","chemical_compound","country","event","university","chemical_element","award","protein","scientist","person","organization","location","discipline","astronomical_object","enzyme"]}
{"id":"111","dataset":"crossner_science","split":"test","instance":{"id":"111","prompt_labels":"The(O) original(O) body(O) farm(O) is(O) the(O) University(B-university) of(I-university) Tennessee(I-university) Anthropological(I-university) Research(I-university) Facility(I-university) located(O) a(O) few(O) miles(O) from(O) downtown(O) on(O) Alcoa(B-location) Highway(I-location) in(I-location) Tennessee(B-location) ,(O) behind(O) the(O) University(B-university) of(I-university) Tennessee(I-university) Medical(I-university) Center(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, person, enzyme, event, organization, award, theory, academic journal, location, country, chemical element, astronomical object, scientist, university, protein, discipline and O.\nSentence: The original body farm is the University of Tennessee Anthropological Research Facility located a few miles from downtown on Alcoa Highway in Tennessee , behind the University of Tennessee Medical Center .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","original","body","farm","is","the","University","of","Tennessee","Anthropological","Research","Facility","located","a","few","miles","from","downtown","on","Alcoa","Highway","in","Tennessee",",","behind","the","University","of","Tennessee","Medical","Center","."],"labels":["O","O","O","O","O","O","B-university","I-university","I-university","I-university","I-university","I-university","O","O","O","O","O","O","O","B-location","I-location","I-location","B-location","O","O","O","B-university","I-university","I-university","I-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","person","enzyme","event","organization","award","theory","academic_journal","location","country","chemical_element","astronomical_object","scientist","university","protein","discipline"]}
{"id":"112","dataset":"crossner_science","split":"test","instance":{"id":"112","prompt_labels":"These(O) experimental(O) and(O) mathematical(O) analyses(O) were(O) applied(O) to(O) several(O) areas(O) of(O) geophysics(B-discipline) :(O) Earth(B-astronomical object) 's(O) shape(O) ,(O) density(O) ,(O) and(O) gravity(O) field(O) ((O) Pierre(B-scientist) Bouguer(I-scientist) ,(O) Alexis(B-scientist) Clairaut(I-scientist) and(O) Henry(B-scientist) Cavendish(I-scientist) )(O) ,(O) Earth(B-astronomical object) 's(O) magnetic(O) field(O) ((O) Alexander(B-scientist) von(I-scientist) Humboldt(I-scientist) ,(O) Edmund(B-scientist) Halley(I-scientist) and(O) Carl(B-scientist) Friedrich(I-scientist) Gauss(I-scientist) )(O) ,(O) seismology(B-discipline) ((O) John(B-scientist) Milne(I-scientist) and(O) Robert(B-scientist) Mallet(I-scientist) )(O) ,(O) and(O) the(O) Earth(B-astronomical object) 's(O) age(O) ,(O) heat(O) and(O) radioactivity(O) ((O) Arthur(B-scientist) Holmes(I-scientist) and(O) William(B-scientist) Thomson(I-scientist) ,(I-scientist) 1st(I-scientist) Baron(I-scientist) Kelvin(I-scientist) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, enzyme, theory, university, organization, chemical element, academic journal, event, discipline, scientist, person, country, chemical compound, award, protein, location and O.\nSentence: These experimental and mathematical analyses were applied to several areas of geophysics : Earth 's shape , density , and gravity field ( Pierre Bouguer , Alexis Clairaut and Henry Cavendish ) , Earth 's magnetic field ( Alexander von Humboldt , Edmund Halley and Carl Friedrich Gauss ) , seismology ( John Milne and Robert Mallet ) , and the Earth 's age , heat and radioactivity ( Arthur Holmes and William Thomson , 1st Baron Kelvin ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["These","experimental","and","mathematical","analyses","were","applied","to","several","areas","of","geophysics",":","Earth","'s","shape",",","density",",","and","gravity","field","(","Pierre","Bouguer",",","Alexis","Clairaut","and","Henry","Cavendish",")",",","Earth","'s","magnetic","field","(","Alexander","von","Humboldt",",","Edmund","Halley","and","Carl","Friedrich","Gauss",")",",","seismology","(","John","Milne","and","Robert","Mallet",")",",","and","the","Earth","'s","age",",","heat","and","radioactivity","(","Arthur","Holmes","and","William","Thomson",",","1st","Baron","Kelvin",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-discipline","O","B-astronomical object","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-astronomical object","O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","O","B-discipline","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","I-scientist","I-scientist","I-scientist","O","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","enzyme","theory","university","organization","chemical_element","academic_journal","event","discipline","scientist","person","country","chemical_compound","award","protein","location"]}
{"id":"113","dataset":"crossner_science","split":"test","instance":{"id":"113","prompt_labels":"The(O) three(O) required(O) enzyme(O) activities(O) are(O) :(O) exonuclease(B-enzyme) ,(O) DNA(B-enzyme) polymerase(I-enzyme) ,(O) and(O) DNA(B-enzyme) ligase(I-enzyme) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, country, person, chemical element, organization, location, award, enzyme, scientist, theory, university, event, chemical compound, academic journal, astronomical object, discipline and O.\nSentence: The three required enzyme activities are : exonuclease , DNA polymerase , and DNA ligase .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","three","required","enzyme","activities","are",":","exonuclease",",","DNA","polymerase",",","and","DNA","ligase","."],"labels":["O","O","O","O","O","O","O","B-enzyme","O","B-enzyme","I-enzyme","O","O","B-enzyme","I-enzyme","O"],"target_index":null,"target_label":null},"label_list":["protein","country","person","chemical_element","organization","location","award","enzyme","scientist","theory","university","event","chemical_compound","academic_journal","astronomical_object","discipline"]}
{"id":"114","dataset":"crossner_science","split":"test","instance":{"id":"114","prompt_labels":"Computer(O) programs(O) that(O) solve(O) these(O) problems(O) have(O) been(O) used(O) to(O) research(O) a(O) broad(O) range(O) of(O) scientific(O) topics(O) from(O) Adenosine(B-chemical compound) diphosphate(I-chemical compound) to(O) breast(O) cancer(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, chemical element, theory, chemical compound, location, academic journal, enzyme, scientist, event, discipline, university, astronomical object, protein, country, organization, award and O.\nSentence: Computer programs that solve these problems have been used to research a broad range of scientific topics from Adenosine diphosphate to breast cancer .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Computer","programs","that","solve","these","problems","have","been","used","to","research","a","broad","range","of","scientific","topics","from","Adenosine","diphosphate","to","breast","cancer","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","I-chemical compound","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","chemical_element","theory","chemical_compound","location","academic_journal","enzyme","scientist","event","discipline","university","astronomical_object","protein","country","organization","award"]}
{"id":"117","dataset":"crossner_science","split":"test","instance":{"id":"117","prompt_labels":"All(O) of(O) these(O) low-numbered(O) asteroids(O) have(O) numbers(O) between(O) and(O) were(O) discovered(O) between(O) 1876(O) and(O) the(O) 1930s(O) ,(O) predominantly(O) by(O) astronomers(O) Auguste(B-scientist) Charlois(I-scientist) ,(O) Johann(B-scientist) Palisa(I-scientist) ,(O) Max(B-scientist) Wolf(I-scientist) and(O) Karl(B-scientist) Reinmuth(I-scientist) ((O) also(O) see(O) category(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, organization, event, university, person, chemical element, discipline, protein, enzyme, theory, award, country, chemical compound, location, academic journal, astronomical object and O.\nSentence: All of these low-numbered asteroids have numbers between and were discovered between 1876 and the 1930s , predominantly by astronomers Auguste Charlois , Johann Palisa , Max Wolf and Karl Reinmuth ( also see category ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["All","of","these","low-numbered","asteroids","have","numbers","between","and","were","discovered","between","1876","and","the","1930s",",","predominantly","by","astronomers","Auguste","Charlois",",","Johann","Palisa",",","Max","Wolf","and","Karl","Reinmuth","(","also","see","category",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","organization","event","university","person","chemical_element","discipline","protein","enzyme","theory","award","country","chemical_compound","location","academic_journal","astronomical_object"]}
{"id":"123","dataset":"crossner_science","split":"test","instance":{"id":"123","prompt_labels":"The(O) planets(O) of(O) the(O) Solar(O) System(O) are(O) divided(O) into(O) two(O) groups(O) :(O) the(O) four(O) inner(O) planets(O) are(O) the(O) terrestrial(O) planet(O) s(O) ((O) Mercury(B-astronomical object) ,(O) Venus(B-astronomical object) ,(O) Earth(B-astronomical object) and(O) Mars(B-astronomical object) )(O) ,(O) with(O) relatively(O) small(O) sizes(O) and(O) rocky(O) surfaces(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, discipline, country, chemical element, location, protein, theory, academic journal, scientist, award, event, person, astronomical object, enzyme, chemical compound, university and O.\nSentence: The planets of the Solar System are divided into two groups : the four inner planets are the terrestrial planet s ( Mercury , Venus , Earth and Mars ) , with relatively small sizes and rocky surfaces .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","planets","of","the","Solar","System","are","divided","into","two","groups",":","the","four","inner","planets","are","the","terrestrial","planet","s","(","Mercury",",","Venus",",","Earth","and","Mars",")",",","with","relatively","small","sizes","and","rocky","surfaces","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","discipline","country","chemical_element","location","protein","theory","academic_journal","scientist","award","event","person","astronomical_object","enzyme","chemical_compound","university"]}
{"id":"125","dataset":"crossner_science","split":"test","instance":{"id":"125","prompt_labels":"With(O) a(O) diameter(O) of(O) approximately(O) 10(O) kilometers(O) ,(O) Amundsenia(B-astronomical object) is(O) one(O) of(O) the(O) largest(O) mid-sized(O) Mars-crossing(O) asteroid(O) s(O) such(O) as(O) 1139(B-astronomical object) Atami(I-astronomical object) ((O) 9.35(O) km(O) )(O) ,(O) 1474(B-astronomical object) Beira(I-astronomical object) ((O) 14.9(O) km(O) )(O) ,(O) 1011(B-astronomical object) Laodamia(I-astronomical object) ((O) 7.39(O) km(O) )(O) ,(O) 1727(B-astronomical object) Mette(I-astronomical object) ((O) est(O) 9(O) km(O) )(O) ,(O) 1131(B-astronomical object) Porzia(I-astronomical object) ((O) 7.13(O) km(O) )(O) ,(O) 1235(B-astronomical object) Schorria(I-astronomical object) ((O) est(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, discipline, event, location, chemical element, scientist, theory, protein, country, organization, person, award, enzyme, astronomical object, chemical compound, academic journal and O.\nSentence: With a diameter of approximately 10 kilometers , Amundsenia is one of the largest mid-sized Mars-crossing asteroid s such as 1139 Atami ( 9.35 km ) , 1474 Beira ( 14.9 km ) , 1011 Laodamia ( 7.39 km ) , 1727 Mette ( est 9 km ) , 1131 Porzia ( 7.13 km ) , 1235 Schorria ( est .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["With","a","diameter","of","approximately","10","kilometers",",","Amundsenia","is","one","of","the","largest","mid-sized","Mars-crossing","asteroid","s","such","as","1139","Atami","(","9.35","km",")",",","1474","Beira","(","14.9","km",")",",","1011","Laodamia","(","7.39","km",")",",","1727","Mette","(","est","9","km",")",",","1131","Porzia","(","7.13","km",")",",","1235","Schorria","(","est","."],"labels":["O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","I-astronomical object","O","O","O","O","O","B-astronomical object","I-astronomical object","O","O","O","O","O","B-astronomical object","I-astronomical object","O","O","O","O","O","B-astronomical object","I-astronomical object","O","O","O","O","O","O","B-astronomical object","I-astronomical object","O","O","O","O","O","B-astronomical object","I-astronomical object","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","discipline","event","location","chemical_element","scientist","theory","protein","country","organization","person","award","enzyme","astronomical_object","chemical_compound","academic_journal"]}
{"id":"127","dataset":"crossner_science","split":"test","instance":{"id":"127","prompt_labels":"Such(O) measures(O) include(O) proofreading(O) by(O) some(O) DNA(B-enzyme) polymerase(I-enzyme) during(O) replication(O) ,(O) DNA(B-protein) mismatch(I-protein) repair(I-protein) following(O) replication(O) ,(O) DNA(O) proofreading(O) and(O) repair(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, theory, person, protein, award, event, organization, enzyme, country, chemical element, location, university, scientist, discipline, academic journal, chemical compound and O.\nSentence: Such measures include proofreading by some DNA polymerase during replication , DNA mismatch repair following replication , DNA proofreading and repair .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Such","measures","include","proofreading","by","some","DNA","polymerase","during","replication",",","DNA","mismatch","repair","following","replication",",","DNA","proofreading","and","repair","."],"labels":["O","O","O","O","O","O","B-enzyme","I-enzyme","O","O","O","B-protein","I-protein","I-protein","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","theory","person","protein","award","event","organization","enzyme","country","chemical_element","location","university","scientist","discipline","academic_journal","chemical_compound"]}
{"id":"129","dataset":"crossner_science","split":"test","instance":{"id":"129","prompt_labels":"CCR3(B-protein) is(O) a(O) receptor(O) for(O) multiple(O) inflammatory(O) /(O) inducible(O) CC(B-protein) chemokines(I-protein) ,(O) including(O) CCL11(B-protein) ,(O) CCL26(B-protein) ,(O) CCL7(B-protein) ,(O) CCL13(B-protein) ,(O) CCL15(B-protein) ,(O) CCL24(B-protein) and(O) CCL5(B-protein) that(O) attract(O) eosinophils(O) ,(O) and(O) CCL28(B-protein) that(O) attracts(O) B(O) and(O) T(O) lymphocyte(O) s(O) to(O) mucosal(O) tissue(O) s.ref(O) name(O) =(O) Youn(O) /(O) It(O) is(O) most(O) highly(O) expressed(O) in(O) both(O) eosinophils(O) and(O) basophils(O) ,(O) but(O) can(O) also(O) be(O) found(O) in(O) Th1(O) and(O) Th2(O) cells(O) and(O) airway(O) epithelial(O) cells(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, event, scientist, academic journal, university, award, protein, location, chemical compound, enzyme, person, chemical element, organization, theory, country, discipline and O.\nSentence: CCR3 is a receptor for multiple inflammatory / inducible CC chemokines , including CCL11 , CCL26 , CCL7 , CCL13 , CCL15 , CCL24 and CCL5 that attract eosinophils , and CCL28 that attracts B and T lymphocyte s to mucosal tissue s.ref name = Youn / It is most highly expressed in both eosinophils and basophils , but can also be found in Th1 and Th2 cells and airway epithelial cells .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["CCR3","is","a","receptor","for","multiple","inflammatory","/","inducible","CC","chemokines",",","including","CCL11",",","CCL26",",","CCL7",",","CCL13",",","CCL15",",","CCL24","and","CCL5","that","attract","eosinophils",",","and","CCL28","that","attracts","B","and","T","lymphocyte","s","to","mucosal","tissue","s.ref","name","=","Youn","/","It","is","most","highly","expressed","in","both","eosinophils","and","basophils",",","but","can","also","be","found","in","Th1","and","Th2","cells","and","airway","epithelial","cells","."],"labels":["B-protein","O","O","O","O","O","O","O","O","B-protein","I-protein","O","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","O","O","O","O","B-protein","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","event","scientist","academic_journal","university","award","protein","location","chemical_compound","enzyme","person","chemical_element","organization","theory","country","discipline"]}
{"id":"130","dataset":"crossner_science","split":"test","instance":{"id":"130","prompt_labels":"On(O) 11(O) September(O) 2010(O) ,(O) Jones(B-person) performed(O) for(O) an(O) audience(O) of(O) 50,000(O) at(O) the(O) Help(B-organization) for(I-organization) Heroes(I-organization) charity(B-event) concert(I-event) at(O) Twickenham(B-location) Stadium(I-location) performing(O) Strange(O) Things(O) Are(O) Happening(O) Every(O) Day(O) and(O) his(O) hit(O) Green(O) ,(O) Green(O) Grass(O) of(O) Home(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, scientist, award, location, event, protein, discipline, enzyme, chemical element, university, academic journal, astronomical object, theory, person, organization, country and O.\nSentence: On 11 September 2010 , Jones performed for an audience of 50,000 at the Help for Heroes charity concert at Twickenham Stadium performing Strange Things Are Happening Every Day and his hit Green , Green Grass of Home .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","11","September","2010",",","Jones","performed","for","an","audience","of","50,000","at","the","Help","for","Heroes","charity","concert","at","Twickenham","Stadium","performing","Strange","Things","Are","Happening","Every","Day","and","his","hit","Green",",","Green","Grass","of","Home","."],"labels":["O","O","O","O","O","B-person","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","B-event","I-event","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","scientist","award","location","event","protein","discipline","enzyme","chemical_element","university","academic_journal","astronomical_object","theory","person","organization","country"]}
{"id":"131","dataset":"crossner_science","split":"test","instance":{"id":"131","prompt_labels":"Mining(O) interests(O) Thomas(B-person) Ewing(I-person) and(O) George(B-person) W.(I-person) Grayson(I-person) ,(O) then(O) miner(O) Edward(B-person) Blewett(I-person) ,(O) Judge(O) Hiram(B-person) G.(I-person) Bond(I-person) of(O) Denver(B-location) and(O) New(B-location) York(I-location) City(I-location) ,(O) and(O) Seattle(B-location) publisher(O) Leigh(B-person) S.(I-person) J.(I-person) Hunt(I-person) ,(O) funded(O) further(O) work(O) and(O) soon(O) a(O) wagon(O) road(O) was(O) built(O) over(O) Barlow(B-location) Pass(I-location) to(O) join(O) the(O) Sauk(B-location) wagon(I-location) road(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, astronomical object, location, theory, chemical element, academic journal, discipline, award, event, enzyme, country, organization, scientist, person, university, protein and O.\nSentence: Mining interests Thomas Ewing and George W. Grayson , then miner Edward Blewett , Judge Hiram G. Bond of Denver and New York City , and Seattle publisher Leigh S. J. Hunt , funded further work and soon a wagon road was built over Barlow Pass to join the Sauk wagon road .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Mining","interests","Thomas","Ewing","and","George","W.","Grayson",",","then","miner","Edward","Blewett",",","Judge","Hiram","G.","Bond","of","Denver","and","New","York","City",",","and","Seattle","publisher","Leigh","S.","J.","Hunt",",","funded","further","work","and","soon","a","wagon","road","was","built","over","Barlow","Pass","to","join","the","Sauk","wagon","road","."],"labels":["O","O","B-person","I-person","O","B-person","I-person","I-person","O","O","O","B-person","I-person","O","O","B-person","I-person","I-person","O","B-location","O","B-location","I-location","I-location","O","O","B-location","O","B-person","I-person","I-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","B-location","I-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","astronomical_object","location","theory","chemical_element","academic_journal","discipline","award","event","enzyme","country","organization","scientist","person","university","protein"]}
{"id":"132","dataset":"crossner_science","split":"test","instance":{"id":"132","prompt_labels":"Camilla(B-astronomical object) is(O) the(O) 6th(O) trinary(O) asteroid(O) that(O) has(O) been(O) discovered(O) in(O) the(O) asteroid(O) belt(O) ,(O) after(O) 87(B-astronomical object) Sylvia(I-astronomical object) ,(O) 45(B-astronomical object) Eugenia(I-astronomical object) ,(O) 216(B-astronomical object) Kleopatra(I-astronomical object) ,(O) 93(B-astronomical object) Minerva(I-astronomical object) and(O) 130(B-astronomical object) Elektra(I-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, award, astronomical object, person, organization, country, chemical element, protein, enzyme, scientist, event, discipline, academic journal, theory, location, university and O.\nSentence: Camilla is the 6th trinary asteroid that has been discovered in the asteroid belt , after 87 Sylvia , 45 Eugenia , 216 Kleopatra , 93 Minerva and 130 Elektra .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Camilla","is","the","6th","trinary","asteroid","that","has","been","discovered","in","the","asteroid","belt",",","after","87","Sylvia",",","45","Eugenia",",","216","Kleopatra",",","93","Minerva","and","130","Elektra","."],"labels":["B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","award","astronomical_object","person","organization","country","chemical_element","protein","enzyme","scientist","event","discipline","academic_journal","theory","location","university"]}
{"id":"134","dataset":"crossner_science","split":"test","instance":{"id":"134","prompt_labels":"Topics(O) included(O) matter(O) creation(O) and(O) annihilation(O) ,(O) the(O) fundamental(O) interaction(O) s(O) ,(O) elementary(O) particle(O) s(O) and(O) their(O) currents(O) ,(O) hadron(O) ic(O) and(O) lepton(O) ic(O) physics(B-discipline) ,(O) and(O) the(O) parton(O) model(O) ,(O) published(O) in(O) professional(O) peer-reviewed(O) scientific(O) journal(O) s(O) including(O) Nuclear(B-academic journal) Physics(I-academic journal) B(I-academic journal) ,(O) Australian(B-academic journal) Journal(I-academic journal) of(I-academic journal) Physics(I-academic journal) ,(O) Nuovo(B-academic journal) Cimento(I-academic journal) ,(O) and(O) Physical(B-academic journal) Review(I-academic journal) D(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, academic journal, country, astronomical object, organization, chemical compound, event, scientist, discipline, person, chemical element, enzyme, university, protein, location, award and O.\nSentence: Topics included matter creation and annihilation , the fundamental interaction s , elementary particle s and their currents , hadron ic and lepton ic physics , and the parton model , published in professional peer-reviewed scientific journal s including Nuclear Physics B , Australian Journal of Physics , Nuovo Cimento , and Physical Review D .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Topics","included","matter","creation","and","annihilation",",","the","fundamental","interaction","s",",","elementary","particle","s","and","their","currents",",","hadron","ic","and","lepton","ic","physics",",","and","the","parton","model",",","published","in","professional","peer-reviewed","scientific","journal","s","including","Nuclear","Physics","B",",","Australian","Journal","of","Physics",",","Nuovo","Cimento",",","and","Physical","Review","D","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-discipline","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","O","B-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["theory","academic_journal","country","astronomical_object","organization","chemical_compound","event","scientist","discipline","person","chemical_element","enzyme","university","protein","location","award"]}
{"id":"139","dataset":"crossner_science","split":"test","instance":{"id":"139","prompt_labels":"Some(O) additional(O) examples(O) include(O) 990(B-astronomical object) Yerkes(I-astronomical object) ,(O) 991(B-astronomical object) McDonalda(I-astronomical object) ,(O) and(O) 992(B-astronomical object) Swasey(I-astronomical object) around(O) this(O) time(O) ;(O) many(O) more(O) minor(O) planets(O) would(O) be(O) discovered(O) at(O) the(O) observatory(O) in(O) the(O) following(O) decades(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, award, enzyme, country, discipline, organization, person, astronomical object, scientist, theory, university, event, academic journal, protein, chemical compound, location and O.\nSentence: Some additional examples include 990 Yerkes , 991 McDonalda , and 992 Swasey around this time ; many more minor planets would be discovered at the observatory in the following decades .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","additional","examples","include","990","Yerkes",",","991","McDonalda",",","and","992","Swasey","around","this","time",";","many","more","minor","planets","would","be","discovered","at","the","observatory","in","the","following","decades","."],"labels":["O","O","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","B-astronomical object","I-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","award","enzyme","country","discipline","organization","person","astronomical_object","scientist","theory","university","event","academic_journal","protein","chemical_compound","location"]}
{"id":"144","dataset":"crossner_science","split":"test","instance":{"id":"144","prompt_labels":"In(O) 2006(O) ,(O) his(O) work(O) with(O) WITNESS(B-organization) and(O) his(O) long-standing(O) support(O) of(O) peace(O) and(O) human(O) rights(O) causes(O) was(O) recognised(O) by(O) the(O) Nobel(B-award) Peace(I-award) Prize(I-award) Laureates(O) with(O) the(O) Man(B-award) of(I-award) Peace(I-award) award(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, astronomical object, enzyme, organization, scientist, university, protein, country, discipline, award, chemical element, chemical compound, theory, person, location, academic journal and O.\nSentence: In 2006 , his work with WITNESS and his long-standing support of peace and human rights causes was recognised by the Nobel Peace Prize Laureates with the Man of Peace award .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2006",",","his","work","with","WITNESS","and","his","long-standing","support","of","peace","and","human","rights","causes","was","recognised","by","the","Nobel","Peace","Prize","Laureates","with","the","Man","of","Peace","award","."],"labels":["O","O","O","O","O","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["event","astronomical_object","enzyme","organization","scientist","university","protein","country","discipline","award","chemical_element","chemical_compound","theory","person","location","academic_journal"]}
{"id":"147","dataset":"crossner_science","split":"test","instance":{"id":"147","prompt_labels":"In(O) order(O) to(O) reset(O) the(O) protein(O) to(O) its(O) original(O) ,(O) basic(O) state(O) ,(O) it(O) needs(O) ATP(B-enzyme) synthase(I-enzyme) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, enzyme, organization, event, chemical compound, protein, scientist, discipline, award, location, academic journal, chemical element, theory, country, person, university and O.\nSentence: In order to reset the protein to its original , basic state , it needs ATP synthase .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","order","to","reset","the","protein","to","its","original",",","basic","state",",","it","needs","ATP","synthase","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-enzyme","I-enzyme","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","enzyme","organization","event","chemical_compound","protein","scientist","discipline","award","location","academic_journal","chemical_element","theory","country","person","university"]}
{"id":"149","dataset":"crossner_science","split":"test","instance":{"id":"149","prompt_labels":"In(O) November(O) 2018(O) a(O) group(O) of(O) Chinese(O) scientists(O) published(O) research(O) in(O) the(O) journal(O) Molecular(B-academic journal) Therapy(I-academic journal) detailing(O) their(O) use(O) of(O) CRISPR(O) technology(O) to(O) correct(O) a(O) single(O) mistaken(O) amino(B-chemical compound) acid(I-chemical compound) successfully(O) in(O) 16(O) out(O) of(O) 18(O) attempts(O) in(O) a(O) human(O) embryo(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, chemical compound, event, university, protein, enzyme, chemical element, location, award, discipline, academic journal, theory, scientist, astronomical object, person and O.\nSentence: In November 2018 a group of Chinese scientists published research in the journal Molecular Therapy detailing their use of CRISPR technology to correct a single mistaken amino acid successfully in 16 out of 18 attempts in a human embryo .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","November","2018","a","group","of","Chinese","scientists","published","research","in","the","journal","Molecular","Therapy","detailing","their","use","of","CRISPR","technology","to","correct","a","single","mistaken","amino","acid","successfully","in","16","out","of","18","attempts","in","a","human","embryo","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","I-chemical compound","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","country","chemical_compound","event","university","protein","enzyme","chemical_element","location","award","discipline","academic_journal","theory","scientist","astronomical_object","person"]}
{"id":"150","dataset":"crossner_science","split":"test","instance":{"id":"150","prompt_labels":"With(O) the(O) first(O) group(O) interacts(O) one(O) plaque(B-protein) protein(I-protein) without(O) PDZ(O) domain(O) ,(O) called(O) cingulin(B-protein) ,(O) which(O) plays(O) a(O) key(O) role(O) in(O) the(O) cell(O) adhesion(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, theory, protein, organization, enzyme, chemical compound, location, discipline, scientist, chemical element, country, university, academic journal, astronomical object, event, person and O.\nSentence: With the first group interacts one plaque protein without PDZ domain , called cingulin , which plays a key role in the cell adhesion .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["With","the","first","group","interacts","one","plaque","protein","without","PDZ","domain",",","called","cingulin",",","which","plays","a","key","role","in","the","cell","adhesion","."],"labels":["O","O","O","O","O","O","B-protein","I-protein","O","O","O","O","O","B-protein","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","theory","protein","organization","enzyme","chemical_compound","location","discipline","scientist","chemical_element","country","university","academic_journal","astronomical_object","event","person"]}
{"id":"151","dataset":"crossner_science","split":"test","instance":{"id":"151","prompt_labels":"The(O) story(O) instead(O) focuses(O) on(O) the(O) Doctor(O) 's(O) companion(O) ,(O) Donna(B-person) Noble(I-person) ((O) Catherine(B-person) Tate(I-person) )(O) and(O) her(O) encounters(O) with(O) former(O) companion(O) Rose(B-person) Tyler(I-person) ((O) Billie(B-person) Piper(I-person) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, organization, person, academic journal, scientist, discipline, university, chemical compound, location, protein, event, theory, award, astronomical object, country, chemical element and O.\nSentence: The story instead focuses on the Doctor 's companion , Donna Noble ( Catherine Tate ) and her encounters with former companion Rose Tyler ( Billie Piper ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","story","instead","focuses","on","the","Doctor","'s","companion",",","Donna","Noble","(","Catherine","Tate",")","and","her","encounters","with","former","companion","Rose","Tyler","(","Billie","Piper",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","O"],"target_index":null,"target_label":null},"label_list":["enzyme","organization","person","academic_journal","scientist","discipline","university","chemical_compound","location","protein","event","theory","award","astronomical_object","country","chemical_element"]}
{"id":"153","dataset":"crossner_science","split":"test","instance":{"id":"153","prompt_labels":"As(O) early(O) as(O) 1807(O) the(O) degree(O) of(O) LL.D.(O) was(O) conferred(O) upon(O) Brewster(B-person) by(O) Marischal(B-university) College(I-university) ,(O) Aberdeen(B-location) ;(O) in(O) 1815(O) he(O) was(O) elected(O) a(O) Fellow(B-award) of(I-award) the(I-award) Royal(I-award) Society(I-award) ,(O) and(O) received(O) the(O) Copley(B-award) Medal(I-award) ;(O) in(O) 1818(O) he(O) received(O) the(O) Rumford(B-award) Medal(I-award) of(O) the(O) society(O) ;(O) and(O) in(O) 1816(O) the(O) French(B-organization) Institute(I-organization) awarded(O) him(O) one-half(O) of(O) the(O) prize(O) of(O) three(O) thousand(O) francs(O) for(O) the(O) two(O) most(O) important(O) discoveries(O) in(O) physical(B-discipline) science(I-discipline) made(O) in(O) Europe(B-location) during(O) the(O) two(O) preceding(O) years(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, chemical element, enzyme, discipline, university, country, academic journal, location, scientist, astronomical object, organization, protein, chemical compound, person, theory, event and O.\nSentence: As early as 1807 the degree of LL.D. was conferred upon Brewster by Marischal College , Aberdeen ; in 1815 he was elected a Fellow of the Royal Society , and received the Copley Medal ; in 1818 he received the Rumford Medal of the society ; and in 1816 the French Institute awarded him one-half of the prize of three thousand francs for the two most important discoveries in physical science made in Europe during the two preceding years .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["As","early","as","1807","the","degree","of","LL.D.","was","conferred","upon","Brewster","by","Marischal","College",",","Aberdeen",";","in","1815","he","was","elected","a","Fellow","of","the","Royal","Society",",","and","received","the","Copley","Medal",";","in","1818","he","received","the","Rumford","Medal","of","the","society",";","and","in","1816","the","French","Institute","awarded","him","one-half","of","the","prize","of","three","thousand","francs","for","the","two","most","important","discoveries","in","physical","science","made","in","Europe","during","the","two","preceding","years","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-person","O","B-university","I-university","O","B-location","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","O","B-award","I-award","O","O","O","O","O","O","B-award","I-award","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-discipline","I-discipline","O","O","B-location","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","chemical_element","enzyme","discipline","university","country","academic_journal","location","scientist","astronomical_object","organization","protein","chemical_compound","person","theory","event"]}
{"id":"154","dataset":"crossner_science","split":"test","instance":{"id":"154","prompt_labels":"There(O) are(O) several(O) descriptions(O) and(O) discussions(O) about(O) a(O) philosophical(B-discipline) theory(O) of(O) the(O) water(O) cycle(O) by(O) Vitruvius(B-person) ,(O) Leonardo(B-person) da(I-person) Vinci(I-person) and(O) Bernard(B-person) Palissy(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, event, chemical compound, enzyme, university, organization, academic journal, location, protein, award, discipline, country, scientist, person, theory, chemical element and O.\nSentence: There are several descriptions and discussions about a philosophical theory of the water cycle by Vitruvius , Leonardo da Vinci and Bernard Palissy .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["There","are","several","descriptions","and","discussions","about","a","philosophical","theory","of","the","water","cycle","by","Vitruvius",",","Leonardo","da","Vinci","and","Bernard","Palissy","."],"labels":["O","O","O","O","O","O","O","O","B-discipline","O","O","O","O","O","O","B-person","O","B-person","I-person","I-person","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","event","chemical_compound","enzyme","university","organization","academic_journal","location","protein","award","discipline","country","scientist","person","theory","chemical_element"]}
{"id":"157","dataset":"crossner_science","split":"test","instance":{"id":"157","prompt_labels":"ABC(B-organization) Family(I-organization) Worldwide(I-organization) Inc(I-organization) ((O) ABC(O) Family(O) Channel(O) )(O) ,(O) Regent(B-university) University(I-university) ,(O) the(O) American(B-organization) Center(I-organization) for(I-organization) Law(I-organization) &(I-organization) Justice(I-organization) ((O) ACLJ(B-organization) )(O) ,(O) the(O) Founders(B-organization) Inn(I-organization) and(I-organization) Conference(I-organization) Center(I-organization) ,(O) the(O) Christian(B-organization) Coalition(I-organization) of(I-organization) America(I-organization) ,(O) an(O) L-1011(B-organization) Flying(I-organization) Hospital(I-organization) ,(O) Operation(B-organization) Blessing(I-organization) International(I-organization) ,(O) and(O) CBN(B-organization) Asia(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, chemical element, country, protein, academic journal, theory, university, discipline, award, person, chemical compound, enzyme, organization, location, astronomical object, event and O.\nSentence: ABC Family Worldwide Inc ( ABC Family Channel ) , Regent University , the American Center for Law & Justice ( ACLJ ) , the Founders Inn and Conference Center , the Christian Coalition of America , an L-1011 Flying Hospital , Operation Blessing International , and CBN Asia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["ABC","Family","Worldwide","Inc","(","ABC","Family","Channel",")",",","Regent","University",",","the","American","Center","for","Law","&","Justice","(","ACLJ",")",",","the","Founders","Inn","and","Conference","Center",",","the","Christian","Coalition","of","America",",","an","L-1011","Flying","Hospital",",","Operation","Blessing","International",",","and","CBN","Asia","."],"labels":["B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","B-university","I-university","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["scientist","chemical_element","country","protein","academic_journal","theory","university","discipline","award","person","chemical_compound","enzyme","organization","location","astronomical_object","event"]}
{"id":"161","dataset":"crossner_science","split":"test","instance":{"id":"161","prompt_labels":"In(O) 1896(O) ,(O) he(O) translated(O) ,(O) from(O) German(O) ,(O) Heimat(O) ,(O) a(O) play(O) in(O) four(O) acts(O) by(O) Hermann(B-person) Sudermann(I-person) ,(O) renamed(O) Magda(B-person) and(O) played(O) by(O) Henry(B-person) Stephenson(I-person) and(O) Charles(B-person) Waldron(I-person) in(O) a(O) Broadway(O) theatre(O) production(O) in(O) New(B-location) York(I-location) City(I-location) ,(O) New(B-location) York(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, award, scientist, event, country, organization, chemical element, astronomical object, academic journal, discipline, protein, location, chemical compound, university, enzyme, person and O.\nSentence: In 1896 , he translated , from German , Heimat , a play in four acts by Hermann Sudermann , renamed Magda and played by Henry Stephenson and Charles Waldron in a Broadway theatre production in New York City , New York .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1896",",","he","translated",",","from","German",",","Heimat",",","a","play","in","four","acts","by","Hermann","Sudermann",",","renamed","Magda","and","played","by","Henry","Stephenson","and","Charles","Waldron","in","a","Broadway","theatre","production","in","New","York","City",",","New","York","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","B-person","O","O","O","B-person","I-person","O","B-person","I-person","O","O","O","O","O","O","B-location","I-location","I-location","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["theory","award","scientist","event","country","organization","chemical_element","astronomical_object","academic_journal","discipline","protein","location","chemical_compound","university","enzyme","person"]}
{"id":"162","dataset":"crossner_science","split":"test","instance":{"id":"162","prompt_labels":"It(O) was(O) discovered(O) on(O) 24(O) September(O) 1960(O) ,(O) by(O) Ingrid(B-scientist) van(I-scientist) Houten-Groeneveld(I-scientist) and(O) Cornelis(B-scientist) Johannes(I-scientist) van(I-scientist) Houten(I-scientist) at(O) Leiden(B-location) ,(O) on(O) photographic(O) plates(O) taken(O) by(O) Tom(B-person) Gehrels(I-person) at(O) Palomar(B-location) Observatory(I-location) in(O) the(O) United(B-country) States(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, theory, enzyme, scientist, organization, chemical element, astronomical object, university, country, location, protein, academic journal, person, discipline, event, chemical compound and O.\nSentence: It was discovered on 24 September 1960 , by Ingrid van Houten-Groeneveld and Cornelis Johannes van Houten at Leiden , on photographic plates taken by Tom Gehrels at Palomar Observatory in the United States .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","discovered","on","24","September","1960",",","by","Ingrid","van","Houten-Groeneveld","and","Cornelis","Johannes","van","Houten","at","Leiden",",","on","photographic","plates","taken","by","Tom","Gehrels","at","Palomar","Observatory","in","the","United","States","."],"labels":["O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","I-scientist","O","B-location","O","O","O","O","O","O","B-person","I-person","O","B-location","I-location","O","O","B-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["award","theory","enzyme","scientist","organization","chemical_element","astronomical_object","university","country","location","protein","academic_journal","person","discipline","event","chemical_compound"]}
{"id":"167","dataset":"crossner_science","split":"test","instance":{"id":"167","prompt_labels":"In(O) June(O) 2011(O) ,(O) during(O) the(O) Second(B-event) CoRoT(I-event) Symposium(I-event) ,(O) the(O) probe(O) added(O) ten(O) new(O) objects(O) to(O) the(O) Exoplanet(O) catalogue(O) :(O) CoRoT-16b(B-astronomical object) ,(O) CoRoT-17b(B-astronomical object) ,(O) CoRoT-18b(B-astronomical object) ,(O) CoRoT-19b(B-astronomical object) ,(O) CoRoT-20b(B-astronomical object) ,(O) CoRoT-21b(B-astronomical object) ,(O) CoRoT-22b(B-astronomical object) ,(O) CoRoT-23b(B-astronomical object) ,(O) CoRoT-24b(B-astronomical object) ,(O) CoRoT-24c(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, enzyme, academic journal, location, theory, scientist, country, astronomical object, award, organization, person, protein, discipline, chemical element, university, chemical compound and O.\nSentence: In June 2011 , during the Second CoRoT Symposium , the probe added ten new objects to the Exoplanet catalogue : CoRoT-16b , CoRoT-17b , CoRoT-18b , CoRoT-19b , CoRoT-20b , CoRoT-21b , CoRoT-22b , CoRoT-23b , CoRoT-24b , CoRoT-24c .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","June","2011",",","during","the","Second","CoRoT","Symposium",",","the","probe","added","ten","new","objects","to","the","Exoplanet","catalogue",":","CoRoT-16b",",","CoRoT-17b",",","CoRoT-18b",",","CoRoT-19b",",","CoRoT-20b",",","CoRoT-21b",",","CoRoT-22b",",","CoRoT-23b",",","CoRoT-24b",",","CoRoT-24c","."],"labels":["O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["event","enzyme","academic_journal","location","theory","scientist","country","astronomical_object","award","organization","person","protein","discipline","chemical_element","university","chemical_compound"]}
{"id":"168","dataset":"crossner_science","split":"test","instance":{"id":"168","prompt_labels":"When(O) he(O) was(O) about(O) eighteen(O) he(O) went(O) to(O) the(O) University(B-university) of(I-university) Copenhagen(I-university) and(O) afterwards(O) studied(O) at(O) University(B-university) of(I-university) Rostock(I-university) and(I-university) Wittenberg(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, enzyme, chemical compound, discipline, scientist, academic journal, organization, event, theory, award, protein, chemical element, person, country, university, location and O.\nSentence: When he was about eighteen he went to the University of Copenhagen and afterwards studied at University of Rostock and Wittenberg .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["When","he","was","about","eighteen","he","went","to","the","University","of","Copenhagen","and","afterwards","studied","at","University","of","Rostock","and","Wittenberg","."],"labels":["O","O","O","O","O","O","O","O","O","B-university","I-university","I-university","O","O","O","O","B-university","I-university","I-university","I-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","enzyme","chemical_compound","discipline","scientist","academic_journal","organization","event","theory","award","protein","chemical_element","person","country","university","location"]}
{"id":"169","dataset":"crossner_science","split":"test","instance":{"id":"169","prompt_labels":"Together(O) with(O) Roger(B-scientist) Guillemin(I-scientist) he(O) described(O) the(O) neurohormone(O) GnRH(O) that(O) controls(O) Follicle-stimulating(B-protein) hormone(I-protein) and(O) Luteinizing(B-protein) hormone(I-protein) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, country, enzyme, person, organization, chemical element, astronomical object, scientist, protein, theory, award, chemical compound, location, academic journal, discipline, event and O.\nSentence: Together with Roger Guillemin he described the neurohormone GnRH that controls Follicle-stimulating hormone and Luteinizing hormone .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Together","with","Roger","Guillemin","he","described","the","neurohormone","GnRH","that","controls","Follicle-stimulating","hormone","and","Luteinizing","hormone","."],"labels":["O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","B-protein","I-protein","O","B-protein","I-protein","O"],"target_index":null,"target_label":null},"label_list":["university","country","enzyme","person","organization","chemical_element","astronomical_object","scientist","protein","theory","award","chemical_compound","location","academic_journal","discipline","event"]}
{"id":"172","dataset":"crossner_science","split":"test","instance":{"id":"172","prompt_labels":"The(O) invitees(O) included(O) Walther(B-scientist) Bothe(I-scientist) ,(O) Siegfried(B-scientist) Flgge(I-scientist) ,(O) Hans(B-scientist) Geiger(I-scientist) ,(O) Otto(B-scientist) Hahn(I-scientist) ,(O) Paul(B-scientist) Harteck(I-scientist) ,(O) Gerhard(B-scientist) Hoffmann(I-scientist) ,(O) Josef(B-scientist) Mattauch(I-scientist) ,(O) and(O) Georg(B-scientist) Stetter(I-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, university, chemical element, organization, event, academic journal, scientist, discipline, protein, award, chemical compound, theory, enzyme, person, astronomical object, location and O.\nSentence: The invitees included Walther Bothe , Siegfried Flgge , Hans Geiger , Otto Hahn , Paul Harteck , Gerhard Hoffmann , Josef Mattauch , and Georg Stetter .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","invitees","included","Walther","Bothe",",","Siegfried","Flgge",",","Hans","Geiger",",","Otto","Hahn",",","Paul","Harteck",",","Gerhard","Hoffmann",",","Josef","Mattauch",",","and","Georg","Stetter","."],"labels":["O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","O"],"target_index":null,"target_label":null},"label_list":["country","university","chemical_element","organization","event","academic_journal","scientist","discipline","protein","award","chemical_compound","theory","enzyme","person","astronomical_object","location"]}
{"id":"175","dataset":"crossner_science","split":"test","instance":{"id":"175","prompt_labels":"MACROH2A(O) deposition(O) has(O) been(O) suggested(O) to(O) be(O) regulated(O) by(O) the(O) Cullin(B-protein) -(I-protein) SPOP(I-protein) -(I-protein) RBX1(I-protein) ligase(I-protein) complex(I-protein) and(O) is(O) actively(O) involved(O) in(O) stable(O) X(O) inactivation(O) ,(O) likely(O) through(O) the(O) formation(O) of(O) an(O) additional(O) layer(O) of(O) epigenetic(O) silencing(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, award, organization, astronomical object, enzyme, university, location, protein, academic journal, chemical compound, country, chemical element, event, person, discipline, theory and O.\nSentence: MACROH2A deposition has been suggested to be regulated by the Cullin - SPOP - RBX1 ligase complex and is actively involved in stable X inactivation , likely through the formation of an additional layer of epigenetic silencing .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["MACROH2A","deposition","has","been","suggested","to","be","regulated","by","the","Cullin","-","SPOP","-","RBX1","ligase","complex","and","is","actively","involved","in","stable","X","inactivation",",","likely","through","the","formation","of","an","additional","layer","of","epigenetic","silencing","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-protein","I-protein","I-protein","I-protein","I-protein","I-protein","I-protein","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","award","organization","astronomical_object","enzyme","university","location","protein","academic_journal","chemical_compound","country","chemical_element","event","person","discipline","theory"]}
{"id":"177","dataset":"crossner_science","split":"test","instance":{"id":"177","prompt_labels":"On(O) 13(O) February(O) 2015(O) ,(O) scientists(O) ((O) including(O) Geoffrey(B-scientist) Marcy(I-scientist) ,(O) Seth(B-scientist) Shostak(I-scientist) ,(O) Frank(B-scientist) Drake(I-scientist) and(O) David(B-scientist) Brin(I-scientist) )(O) at(O) a(O) convention(O) of(O) the(O) American(B-organization) Association(I-organization) for(I-organization) the(I-organization) Advancement(I-organization) of(I-organization) Science(I-organization) ,(O) discussed(O) Active(O) SETI(O) and(O) whether(O) transmitting(O) a(O) message(O) to(O) possible(O) intelligent(O) extraterrestrials(O) in(O) the(O) Cosmos(O) was(O) a(O) good(O) idea(O) ;(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, protein, university, discipline, chemical compound, scientist, academic journal, theory, astronomical object, organization, award, chemical element, enzyme, country, event, person and O.\nSentence: On 13 February 2015 , scientists ( including Geoffrey Marcy , Seth Shostak , Frank Drake and David Brin ) at a convention of the American Association for the Advancement of Science , discussed Active SETI and whether transmitting a message to possible intelligent extraterrestrials in the Cosmos was a good idea ;","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","13","February","2015",",","scientists","(","including","Geoffrey","Marcy",",","Seth","Shostak",",","Frank","Drake","and","David","Brin",")","at","a","convention","of","the","American","Association","for","the","Advancement","of","Science",",","discussed","Active","SETI","and","whether","transmitting","a","message","to","possible","intelligent","extraterrestrials","in","the","Cosmos","was","a","good","idea",";"],"labels":["O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","protein","university","discipline","chemical_compound","scientist","academic_journal","theory","astronomical_object","organization","award","chemical_element","enzyme","country","event","person"]}
{"id":"178","dataset":"crossner_science","split":"test","instance":{"id":"178","prompt_labels":"Arizona(B-university) State(I-university) University(I-university) offers(O) undergraduate(O) student(O) housing(O) on(O) four(O) metropolitan(O) Phoenix(B-location) campuses(I-location) ((O) Arizona(B-university) State(I-university) University(I-university) Tempe(I-university) campus(I-university) ,(O) Arizona(B-university) State(I-university) University(I-university) Polytechnic(I-university) campus(I-university) ,(O) Arizona(B-university) State(I-university) University(I-university) Downtown(I-university) Phoenix(I-university) campus(I-university) ,(O) and(O) Arizona(B-university) State(I-university) University(I-university) West(I-university) campus(I-university) )(O) ,(O) plus(O) the(O) ASU(B-university) Colleges(I-university) at(I-university) Lake(I-university) Havasu(I-university) City(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, award, event, academic journal, scientist, university, organization, chemical compound, chemical element, discipline, person, enzyme, theory, astronomical object, protein and O.\nSentence: Arizona State University offers undergraduate student housing on four metropolitan Phoenix campuses ( Arizona State University Tempe campus , Arizona State University Polytechnic campus , Arizona State University Downtown Phoenix campus , and Arizona State University West campus ) , plus the ASU Colleges at Lake Havasu City .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Arizona","State","University","offers","undergraduate","student","housing","on","four","metropolitan","Phoenix","campuses","(","Arizona","State","University","Tempe","campus",",","Arizona","State","University","Polytechnic","campus",",","Arizona","State","University","Downtown","Phoenix","campus",",","and","Arizona","State","University","West","campus",")",",","plus","the","ASU","Colleges","at","Lake","Havasu","City","."],"labels":["B-university","I-university","I-university","O","O","O","O","O","O","O","B-location","I-location","O","B-university","I-university","I-university","I-university","I-university","O","B-university","I-university","I-university","I-university","I-university","O","B-university","I-university","I-university","I-university","I-university","I-university","O","O","B-university","I-university","I-university","I-university","I-university","O","O","O","O","B-university","I-university","I-university","I-university","I-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["location","country","award","event","academic_journal","scientist","university","organization","chemical_compound","chemical_element","discipline","person","enzyme","theory","astronomical_object","protein"]}
{"id":"180","dataset":"crossner_science","split":"test","instance":{"id":"180","prompt_labels":"The(O) International(B-organization) Astronomical(I-organization) Union(I-organization) names(O) all(O) colles(O) ((O) small(O) hills(O) )(O) on(O) Saturn(B-astronomical object) '(O) s(O) moon(O) Titan(B-astronomical object) after(O) characters(O) in(O) Tolkien(B-scientist) 's(O) work(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, astronomical object, academic journal, university, scientist, theory, person, country, chemical element, chemical compound, organization, protein, discipline, event, enzyme, location and O.\nSentence: The International Astronomical Union names all colles ( small hills ) on Saturn ' s moon Titan after characters in Tolkien 's work .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","International","Astronomical","Union","names","all","colles","(","small","hills",")","on","Saturn","'","s","moon","Titan","after","characters","in","Tolkien","'s","work","."],"labels":["O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","B-astronomical object","O","O","O","B-scientist","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","astronomical_object","academic_journal","university","scientist","theory","person","country","chemical_element","chemical_compound","organization","protein","discipline","event","enzyme","location"]}
{"id":"182","dataset":"crossner_science","split":"test","instance":{"id":"182","prompt_labels":"A(O) pupil(O) of(O) Peter(B-scientist) Gustav(I-scientist) Lejeune(I-scientist) Dirichlet(I-scientist) ,(O) Gabriel(B-scientist) Lam(I-scientist) and(O) Augustin-Louis(B-scientist) Cauchy(I-scientist) Bjerknes(I-scientist) worked(O) for(O) the(O) rest(O) of(O) his(O) life(O) in(O) the(O) field(O) of(O) hydrodynamics(B-discipline) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, protein, academic journal, location, astronomical object, university, chemical element, discipline, theory, event, scientist, chemical compound, organization, award, person, enzyme and O.\nSentence: A pupil of Peter Gustav Lejeune Dirichlet , Gabriel Lam and Augustin-Louis Cauchy Bjerknes worked for the rest of his life in the field of hydrodynamics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","pupil","of","Peter","Gustav","Lejeune","Dirichlet",",","Gabriel","Lam","and","Augustin-Louis","Cauchy","Bjerknes","worked","for","the","rest","of","his","life","in","the","field","of","hydrodynamics","."],"labels":["O","O","O","B-scientist","I-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","B-discipline","O"],"target_index":null,"target_label":null},"label_list":["country","protein","academic_journal","location","astronomical_object","university","chemical_element","discipline","theory","event","scientist","chemical_compound","organization","award","person","enzyme"]}
{"id":"183","dataset":"crossner_science","split":"test","instance":{"id":"183","prompt_labels":"He(O) served(O) as(O) editor(O) of(O) Optics(B-academic journal) Communications(I-academic journal) ((O) 1987-2006(O) )(O) and(O) was(O) member(O) of(O) the(O) editorial(O) board(O) at(O) Physical(B-academic journal) Review(I-academic journal) A(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, person, discipline, scientist, university, organization, event, chemical element, astronomical object, chemical compound, enzyme, country, theory, location, academic journal, award and O.\nSentence: He served as editor of Optics Communications ( 1987-2006 ) and was member of the editorial board at Physical Review A .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","served","as","editor","of","Optics","Communications","(","1987-2006",")","and","was","member","of","the","editorial","board","at","Physical","Review","A","."],"labels":["O","O","O","O","O","B-academic journal","I-academic journal","O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["protein","person","discipline","scientist","university","organization","event","chemical_element","astronomical_object","chemical_compound","enzyme","country","theory","location","academic_journal","award"]}
{"id":"185","dataset":"crossner_science","split":"test","instance":{"id":"185","prompt_labels":"Esaki(B-scientist) moved(O) back(O) to(O) Japan(B-country) in(O) 1992(O) ,(O) subsequently(O) ,(O) he(O) served(O) as(O) president(O) of(O) the(O) University(B-university) of(I-university) Tsukuba(I-university) and(O) Shibaura(B-university) Institute(I-university) of(I-university) Technology(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, enzyme, chemical element, award, person, event, scientist, university, organization, academic journal, location, country, astronomical object, chemical compound, theory, discipline and O.\nSentence: Esaki moved back to Japan in 1992 , subsequently , he served as president of the University of Tsukuba and Shibaura Institute of Technology .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Esaki","moved","back","to","Japan","in","1992",",","subsequently",",","he","served","as","president","of","the","University","of","Tsukuba","and","Shibaura","Institute","of","Technology","."],"labels":["B-scientist","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","B-university","I-university","I-university","O","B-university","I-university","I-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["protein","enzyme","chemical_element","award","person","event","scientist","university","organization","academic_journal","location","country","astronomical_object","chemical_compound","theory","discipline"]}
{"id":"187","dataset":"crossner_science","split":"test","instance":{"id":"187","prompt_labels":"The(O) quarry(O) is(O) at(O) Herniss(B-location) ,(O) to(O) the(O) north(O) of(O) the(O) A(B-location) 394(I-location) road(I-location) ,(O) between(O) Rame(B-location) and(O) Longdowns(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, award, organization, chemical compound, discipline, chemical element, university, country, academic journal, location, event, protein, enzyme, theory, scientist, person and O.\nSentence: The quarry is at Herniss , to the north of the A 394 road , between Rame and Longdowns .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","quarry","is","at","Herniss",",","to","the","north","of","the","A","394","road",",","between","Rame","and","Longdowns","."],"labels":["O","O","O","O","B-location","O","O","O","O","O","O","B-location","I-location","I-location","O","O","B-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","award","organization","chemical_compound","discipline","chemical_element","university","country","academic_journal","location","event","protein","enzyme","theory","scientist","person"]}
{"id":"191","dataset":"crossner_science","split":"test","instance":{"id":"191","prompt_labels":"Comparative(O) table(O) of(O) countries(O) with(O) very(O) high(O) human(O) development(O) ((O) same(O) or(O) higher(O) than(O) 0.8(O) )(O) ,(O) according(O) to(O) UNDP(B-organization) ;(O) OECD(B-organization) members(O) ;(O) advanced(O) economies(O) ,(O) according(O) to(O) the(O) International(B-organization) Monetary(I-organization) Fund(I-organization) ;(O) high(O) income(O) economies(O) ,(O) according(O) to(O) the(O) World(B-organization) Bank(I-organization) and(O) income(O) per(O) capita(O) ((O) purchasing(O) power(O) parity(O) )(O) higher(O) than(O) $(O) 22,000(O) ,(O) according(O) to(O) the(O) International(B-organization) Monetary(I-organization) Fund(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, person, astronomical object, location, chemical compound, enzyme, protein, organization, award, event, discipline, academic journal, country, scientist, chemical element, theory and O.\nSentence: Comparative table of countries with very high human development ( same or higher than 0.8 ) , according to UNDP ; OECD members ; advanced economies , according to the International Monetary Fund ; high income economies , according to the World Bank and income per capita ( purchasing power parity ) higher than $ 22,000 , according to the International Monetary Fund .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Comparative","table","of","countries","with","very","high","human","development","(","same","or","higher","than","0.8",")",",","according","to","UNDP",";","OECD","members",";","advanced","economies",",","according","to","the","International","Monetary","Fund",";","high","income","economies",",","according","to","the","World","Bank","and","income","per","capita","(","purchasing","power","parity",")","higher","than","$","22,000",",","according","to","the","International","Monetary","Fund","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","B-organization","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["university","person","astronomical_object","location","chemical_compound","enzyme","protein","organization","award","event","discipline","academic_journal","country","scientist","chemical_element","theory"]}
{"id":"192","dataset":"crossner_science","split":"test","instance":{"id":"192","prompt_labels":"Pathogenic(O) spirochetes(O) ,(O) including(O) B.(O) burgdorferi(O) and(O) T.(O) pallidum(O) ,(O) use(O) their(O) proteolipid(O) Bacterial(O) adhesin(O) s(O) to(O) stick(O) to(O) victim(O) cells(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, theory, scientist, person, enzyme, country, astronomical object, chemical compound, event, chemical element, location, discipline, protein, organization, award, academic journal and O.\nSentence: Pathogenic spirochetes , including B. burgdorferi and T. pallidum , use their proteolipid Bacterial adhesin s to stick to victim cells .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Pathogenic","spirochetes",",","including","B.","burgdorferi","and","T.","pallidum",",","use","their","proteolipid","Bacterial","adhesin","s","to","stick","to","victim","cells","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","theory","scientist","person","enzyme","country","astronomical_object","chemical_compound","event","chemical_element","location","discipline","protein","organization","award","academic_journal"]}
{"id":"195","dataset":"crossner_science","split":"test","instance":{"id":"195","prompt_labels":"He(O) studied(O) physics(B-discipline) at(O) the(O) University(B-university) of(I-university) Heidelberg(I-university) ,(O) the(O) University(B-university) of(I-university) Lausanne(I-university) ,(O) and(O) ,(O) ultimately(O) ,(O) the(O) Humboldt(B-university) University(I-university) of(I-university) Berlin(I-university) under(O) Max(B-scientist) Planck(I-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, protein, location, award, organization, academic journal, event, scientist, country, discipline, person, university, chemical element, chemical compound, enzyme, theory and O.\nSentence: He studied physics at the University of Heidelberg , the University of Lausanne , and , ultimately , the Humboldt University of Berlin under Max Planck .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","studied","physics","at","the","University","of","Heidelberg",",","the","University","of","Lausanne",",","and",",","ultimately",",","the","Humboldt","University","of","Berlin","under","Max","Planck","."],"labels":["O","O","B-discipline","O","O","B-university","I-university","I-university","O","O","B-university","I-university","I-university","O","O","O","O","O","O","B-university","I-university","I-university","I-university","O","B-scientist","I-scientist","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","protein","location","award","organization","academic_journal","event","scientist","country","discipline","person","university","chemical_element","chemical_compound","enzyme","theory"]}
{"id":"196","dataset":"crossner_science","split":"test","instance":{"id":"196","prompt_labels":"He(O) entered(O) the(O) accredited(O) Illinois(B-university) Central(I-university) College(I-university) ,(O) then(O) transferred(O) to(O) the(O) unaccredited(O) Midwestern(B-university) Baptist(I-university) College(I-university) in(O) 1972(O) ,(O) attaining(O) a(O) Bachelor(O) of(O) Religious(B-discipline) Education(I-discipline) in(O) 1974(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, person, chemical element, theory, university, chemical compound, discipline, award, country, protein, location, academic journal, enzyme, organization, event, astronomical object and O.\nSentence: He entered the accredited Illinois Central College , then transferred to the unaccredited Midwestern Baptist College in 1972 , attaining a Bachelor of Religious Education in 1974 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","entered","the","accredited","Illinois","Central","College",",","then","transferred","to","the","unaccredited","Midwestern","Baptist","College","in","1972",",","attaining","a","Bachelor","of","Religious","Education","in","1974","."],"labels":["O","O","O","O","B-university","I-university","I-university","O","O","O","O","O","O","B-university","I-university","I-university","O","O","O","O","O","O","O","B-discipline","I-discipline","O","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","person","chemical_element","theory","university","chemical_compound","discipline","award","country","protein","location","academic_journal","enzyme","organization","event","astronomical_object"]}
{"id":"197","dataset":"crossner_science","split":"test","instance":{"id":"197","prompt_labels":"The(O) Chun-Tsung(B-award) scholarships(I-award) ,(O) supervised(O) by(O) the(O) United(B-organization) Board(I-organization) for(I-organization) Christian(I-organization) Higher(I-organization) Education(I-organization) in(I-organization) Asia(I-organization) ((O) New(B-location) York(I-location) )(O) ,(O) are(O) awarded(O) to(O) undergraduates(O) ,(O) usually(O) in(O) their(O) 2nd(O) or(O) 3rd(O) year(O) ,(O) at(O) six(O) universities(O) ,(O) which(O) are(O) Shanghai(B-university) Jiaotong(I-university) University(I-university) ,(O) Fudan(B-university) University(I-university) ,(O) Lanzhou(B-university) University(I-university) ,(O) Soochow(B-university) University(I-university) ,(O) Peking(B-university) University(I-university) and(O) Taiwan(B-university) National(I-university) Tsing(I-university) Hua(I-university) University(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, chemical element, country, discipline, award, protein, astronomical object, location, organization, enzyme, academic journal, event, scientist, university, chemical compound, person and O.\nSentence: The Chun-Tsung scholarships , supervised by the United Board for Christian Higher Education in Asia ( New York ) , are awarded to undergraduates , usually in their 2nd or 3rd year , at six universities , which are Shanghai Jiaotong University , Fudan University , Lanzhou University , Soochow University , Peking University and Taiwan National Tsing Hua University .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Chun-Tsung","scholarships",",","supervised","by","the","United","Board","for","Christian","Higher","Education","in","Asia","(","New","York",")",",","are","awarded","to","undergraduates",",","usually","in","their","2nd","or","3rd","year",",","at","six","universities",",","which","are","Shanghai","Jiaotong","University",",","Fudan","University",",","Lanzhou","University",",","Soochow","University",",","Peking","University","and","Taiwan","National","Tsing","Hua","University","."],"labels":["O","B-award","I-award","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-university","I-university","I-university","O","B-university","I-university","O","B-university","I-university","O","B-university","I-university","O","B-university","I-university","O","B-university","I-university","I-university","I-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["theory","chemical_element","country","discipline","award","protein","astronomical_object","location","organization","enzyme","academic_journal","event","scientist","university","chemical_compound","person"]}
{"id":"198","dataset":"crossner_science","split":"test","instance":{"id":"198","prompt_labels":"This(O) leads(O) to(O) enhanced(O) microbicidal(O) capacity(O) and(O) secretion(O) of(O) high(O) levels(O) of(O) pro-inflammatory(O) cytokines(O) :(O) e.g.(O) IFN-(O) ,(O) IL-1(O) ,(O) IL-6(O) ,(O) Interleukin(O) 12(O) ,(O) Interleukin(O) 23(O) and(O) TNF(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, academic journal, protein, event, chemical element, organization, enzyme, scientist, location, award, astronomical object, theory, country, chemical compound, university, person and O.\nSentence: This leads to enhanced microbicidal capacity and secretion of high levels of pro-inflammatory cytokines : e.g. IFN- , IL-1 , IL-6 , Interleukin 12 , Interleukin 23 and TNF .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","leads","to","enhanced","microbicidal","capacity","and","secretion","of","high","levels","of","pro-inflammatory","cytokines",":","e.g.","IFN-",",","IL-1",",","IL-6",",","Interleukin","12",",","Interleukin","23","and","TNF","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["discipline","academic_journal","protein","event","chemical_element","organization","enzyme","scientist","location","award","astronomical_object","theory","country","chemical_compound","university","person"]}
{"id":"199","dataset":"crossner_science","split":"test","instance":{"id":"199","prompt_labels":"He(O) was(O) appointed(O) as(O) assistant(O) to(O) the(O) astronomer(O) Giovanni(B-scientist) Battista(I-scientist) Donati(I-scientist) in(O) 1862(O) ,(O) professor(O) at(O) the(O) technological(B-location) institute(I-location) of(I-location) Bologna(I-location) in(O) 1864(O) ,(O) professor(O) of(O) physics(B-discipline) at(O) the(O) University(B-university) of(I-university) Cagliari(I-university) in(O) 1873(O) ,(O) and(O) ,(O) finally(O) ,(O) successor(O) to(O) his(O) father(O) in(O) 1881(O) in(O) the(O) chair(O) of(O) technological(B-discipline) physics(I-discipline) at(O) the(O) University(B-university) of(I-university) Pisa(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, theory, event, enzyme, chemical element, person, chemical compound, astronomical object, location, discipline, country, university, organization, protein, scientist, award and O.\nSentence: He was appointed as assistant to the astronomer Giovanni Battista Donati in 1862 , professor at the technological institute of Bologna in 1864 , professor of physics at the University of Cagliari in 1873 , and , finally , successor to his father in 1881 in the chair of technological physics at the University of Pisa .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","appointed","as","assistant","to","the","astronomer","Giovanni","Battista","Donati","in","1862",",","professor","at","the","technological","institute","of","Bologna","in","1864",",","professor","of","physics","at","the","University","of","Cagliari","in","1873",",","and",",","finally",",","successor","to","his","father","in","1881","in","the","chair","of","technological","physics","at","the","University","of","Pisa","."],"labels":["O","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","O","O","O","O","O","B-location","I-location","I-location","I-location","O","O","O","O","O","B-discipline","O","O","B-university","I-university","I-university","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-discipline","I-discipline","O","O","B-university","I-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","theory","event","enzyme","chemical_element","person","chemical_compound","astronomical_object","location","discipline","country","university","organization","protein","scientist","award"]}
{"id":"200","dataset":"crossner_science","split":"test","instance":{"id":"200","prompt_labels":"The(O) Arts(B-organization) Centre(I-organization) Melbourne(I-organization) ((O) which(O) includes(O) the(O) State(B-location) Theatre(I-location) ,(O) Hamer(B-location) Hall(I-location) ,(O) the(O) Playhouse(B-location) and(O) the(O) Fairfax(B-location) Studio(I-location) )(O) ,(O) and(O) the(O) Melbourne(B-location) Recital(I-location) Centre(I-location) are(O) located(O) just(O) to(O) the(O) south(O) of(O) the(O) CBD(B-location) ,(O) with(O) the(O) Sidney(B-location) Myer(I-location) Music(I-location) Bowl(I-location) in(O) parklands(B-location) to(O) the(O) east(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, chemical compound, location, enzyme, protein, academic journal, theory, university, event, discipline, astronomical object, person, scientist, chemical element, award, country and O.\nSentence: The Arts Centre Melbourne ( which includes the State Theatre , Hamer Hall , the Playhouse and the Fairfax Studio ) , and the Melbourne Recital Centre are located just to the south of the CBD , with the Sidney Myer Music Bowl in parklands to the east .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Arts","Centre","Melbourne","(","which","includes","the","State","Theatre",",","Hamer","Hall",",","the","Playhouse","and","the","Fairfax","Studio",")",",","and","the","Melbourne","Recital","Centre","are","located","just","to","the","south","of","the","CBD",",","with","the","Sidney","Myer","Music","Bowl","in","parklands","to","the","east","."],"labels":["O","B-organization","I-organization","I-organization","O","O","O","O","B-location","I-location","O","B-location","I-location","O","O","B-location","O","O","B-location","I-location","O","O","O","O","B-location","I-location","I-location","O","O","O","O","O","O","O","O","B-location","O","O","O","B-location","I-location","I-location","I-location","O","B-location","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","chemical_compound","location","enzyme","protein","academic_journal","theory","university","event","discipline","astronomical_object","person","scientist","chemical_element","award","country"]}
{"id":"201","dataset":"crossner_science","split":"test","instance":{"id":"201","prompt_labels":"In(O) addition(O) ,(O) he(O) is(O) the(O) founding(O) editor(O) of(O) Theoretical(B-academic journal) Population(I-academic journal) Biology(I-academic journal) ((O) 1971-2013(O) )(O) and(O) an(O) associate(O) editor(O) of(O) Genetics(B-academic journal) ,(O) Human(B-academic journal) Genetics(I-academic journal) ,(O) Annals(B-academic journal) of(I-academic journal) Human(I-academic journal) Genetics(I-academic journal) ,(O) Annals(B-academic journal) of(I-academic journal) Human(I-academic journal) Biology(I-academic journal) ,(O) and(O) Complexity(B-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, chemical element, astronomical object, location, theory, person, university, chemical compound, scientist, protein, event, academic journal, enzyme, award, organization, country and O.\nSentence: In addition , he is the founding editor of Theoretical Population Biology ( 1971-2013 ) and an associate editor of Genetics , Human Genetics , Annals of Human Genetics , Annals of Human Biology , and Complexity .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","addition",",","he","is","the","founding","editor","of","Theoretical","Population","Biology","(","1971-2013",")","and","an","associate","editor","of","Genetics",",","Human","Genetics",",","Annals","of","Human","Genetics",",","Annals","of","Human","Biology",",","and","Complexity","."],"labels":["O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","O","O","O","B-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","B-academic journal","O"],"target_index":null,"target_label":null},"label_list":["discipline","chemical_element","astronomical_object","location","theory","person","university","chemical_compound","scientist","protein","event","academic_journal","enzyme","award","organization","country"]}
{"id":"202","dataset":"crossner_science","split":"test","instance":{"id":"202","prompt_labels":"Marsden(B-astronomical object) was(O) discovered(O) on(O) 24(O) March(O) 1971(O) ,(O) by(O) Dutch(O) astronomer(O) couple(O) Ingrid(B-scientist) van(I-scientist) Houten-Groeneveld(I-scientist) and(O) Cornelis(B-scientist) van(I-scientist) Houten(I-scientist) at(O) Leiden(B-location) ,(O) on(O) photographic(O) plates(O) taken(O) by(O) Dutch-American(O) astronomer(O) Tom(B-scientist) Gehrels(I-scientist) at(O) Palomar(B-location) Observatory(I-location) ,(O) California(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, award, chemical compound, event, theory, scientist, country, person, location, chemical element, organization, enzyme, protein, academic journal, discipline, university and O.\nSentence: Marsden was discovered on 24 March 1971 , by Dutch astronomer couple Ingrid van Houten-Groeneveld and Cornelis van Houten at Leiden , on photographic plates taken by Dutch-American astronomer Tom Gehrels at Palomar Observatory , California .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Marsden","was","discovered","on","24","March","1971",",","by","Dutch","astronomer","couple","Ingrid","van","Houten-Groeneveld","and","Cornelis","van","Houten","at","Leiden",",","on","photographic","plates","taken","by","Dutch-American","astronomer","Tom","Gehrels","at","Palomar","Observatory",",","California","."],"labels":["B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","B-location","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-location","I-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","award","chemical_compound","event","theory","scientist","country","person","location","chemical_element","organization","enzyme","protein","academic_journal","discipline","university"]}
{"id":"203","dataset":"crossner_science","split":"test","instance":{"id":"203","prompt_labels":"Jeffrey(B-scientist) S.(I-scientist) Moore(I-scientist) has(O) published(O) over(O) 300(O) articles(O) in(O) journals(O) such(O) as(O) Macromolecules(B-academic journal) ,(O) the(O) Journal(B-academic journal) of(I-academic journal) Chemical(I-academic journal) Education(I-academic journal) ,(O) Advanced(B-academic journal) Materials(I-academic journal) and(O) the(O) Journal(B-academic journal) of(I-academic journal) Materials(I-academic journal) Chemistry(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, chemical element, astronomical object, location, country, person, chemical compound, enzyme, theory, organization, protein, academic journal, discipline, award, scientist, university and O.\nSentence: Jeffrey S. Moore has published over 300 articles in journals such as Macromolecules , the Journal of Chemical Education , Advanced Materials and the Journal of Materials Chemistry .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Jeffrey","S.","Moore","has","published","over","300","articles","in","journals","such","as","Macromolecules",",","the","Journal","of","Chemical","Education",",","Advanced","Materials","and","the","Journal","of","Materials","Chemistry","."],"labels":["B-scientist","I-scientist","I-scientist","O","O","O","O","O","O","O","O","O","B-academic journal","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["event","chemical_element","astronomical_object","location","country","person","chemical_compound","enzyme","theory","organization","protein","academic_journal","discipline","award","scientist","university"]}
{"id":"205","dataset":"crossner_science","split":"test","instance":{"id":"205","prompt_labels":"His(O) work(O) has(O) appeared(O) in(O) leading(O) scientific(O) journals(O) such(O) as(O) Nature(B-academic journal) ,(O) Science(B-academic journal) ,(O) Nature(B-academic journal) Genetics(I-academic journal) ,(O) Molecular(B-academic journal) Biology(I-academic journal) and(I-academic journal) Evolution(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Evolutionary(I-academic journal) Biology(I-academic journal) ,(O) as(O) well(O) as(O) popular(O) magazines(O) such(O) as(O) Scientific(B-academic journal) American(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, enzyme, chemical compound, discipline, university, person, country, academic journal, scientist, protein, chemical element, award, event, theory, astronomical object and O.\nSentence: His work has appeared in leading scientific journals such as Nature , Science , Nature Genetics , Molecular Biology and Evolution , Journal of Evolutionary Biology , as well as popular magazines such as Scientific American .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","work","has","appeared","in","leading","scientific","journals","such","as","Nature",",","Science",",","Nature","Genetics",",","Molecular","Biology","and","Evolution",",","Journal","of","Evolutionary","Biology",",","as","well","as","popular","magazines","such","as","Scientific","American","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-academic journal","O","B-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["organization","location","enzyme","chemical_compound","discipline","university","person","country","academic_journal","scientist","protein","chemical_element","award","event","theory","astronomical_object"]}
{"id":"206","dataset":"crossner_science","split":"test","instance":{"id":"206","prompt_labels":"The(O) city(O) is(O) home(O) to(O) the(O) Department(B-organization) of(I-organization) Physical(I-organization) Education(I-organization) and(I-organization) Sport(I-organization) Science(I-organization) of(O) the(O) Aristotle(B-university) University(I-university) of(I-university) Thessaloniki(I-university) ((O) )(O) and(O) the(O) Serres(B-university) Campus(I-university) of(I-university) the(I-university) International(I-university) Hellenic(I-university) University(I-university) ((O) former(O) Technological(B-university) Educational(I-university) Institute(I-university) of(I-university) Central(I-university) Macedonia(I-university) )(O) ,(O) composed(O) of(O) the(O) Faculty(B-organization) of(I-organization) Engineering(I-organization) ,(O) the(O) Faculty(B-organization) of(I-organization) Economics(I-organization) and(I-organization) Management(I-organization) ,(O) and(O) the(O) Department(B-organization) of(I-organization) Interior(I-organization) Architecture(I-organization) and(I-organization) Design(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, chemical compound, country, event, theory, person, university, enzyme, academic journal, chemical element, scientist, protein, award, location, astronomical object, organization and O.\nSentence: The city is home to the Department of Physical Education and Sport Science of the Aristotle University of Thessaloniki ( ) and the Serres Campus of the International Hellenic University ( former Technological Educational Institute of Central Macedonia ) , composed of the Faculty of Engineering , the Faculty of Economics and Management , and the Department of Interior Architecture and Design .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","city","is","home","to","the","Department","of","Physical","Education","and","Sport","Science","of","the","Aristotle","University","of","Thessaloniki","(",")","and","the","Serres","Campus","of","the","International","Hellenic","University","(","former","Technological","Educational","Institute","of","Central","Macedonia",")",",","composed","of","the","Faculty","of","Engineering",",","the","Faculty","of","Economics","and","Management",",","and","the","Department","of","Interior","Architecture","and","Design","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-university","I-university","I-university","I-university","O","O","O","O","B-university","I-university","I-university","I-university","I-university","I-university","I-university","O","O","B-university","I-university","I-university","I-university","I-university","I-university","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["discipline","chemical_compound","country","event","theory","person","university","enzyme","academic_journal","chemical_element","scientist","protein","award","location","astronomical_object","organization"]}
{"id":"207","dataset":"crossner_science","split":"test","instance":{"id":"207","prompt_labels":"Bangladesh(B-country) is(O) a(O) member(O) of(O) the(O) UN(B-organization) ,(O) World(B-organization) Trade(I-organization) Organization(I-organization) ,(O) International(B-organization) Monetary(I-organization) Fund(I-organization) ,(O) the(O) World(B-organization) Bank(I-organization) ,(O) Asian(B-organization) Development(I-organization) Bank(I-organization) ,(O) OIC(B-organization) ,(O) Islamic(B-organization) Development(I-organization) Bank(I-organization) ,(O) SAARC(B-organization) ,(O) BIMSTEC(B-organization) and(O) the(O) Islamic(B-organization) Military(I-organization) Counter(I-organization) Terrorism(I-organization) Coalition(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, country, theory, university, location, chemical compound, organization, astronomical object, enzyme, chemical element, person, event, protein, academic journal, discipline, scientist and O.\nSentence: Bangladesh is a member of the UN , World Trade Organization , International Monetary Fund , the World Bank , Asian Development Bank , OIC , Islamic Development Bank , SAARC , BIMSTEC and the Islamic Military Counter Terrorism Coalition .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Bangladesh","is","a","member","of","the","UN",",","World","Trade","Organization",",","International","Monetary","Fund",",","the","World","Bank",",","Asian","Development","Bank",",","OIC",",","Islamic","Development","Bank",",","SAARC",",","BIMSTEC","and","the","Islamic","Military","Counter","Terrorism","Coalition","."],"labels":["B-country","O","O","O","O","O","B-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","O","B-organization","I-organization","I-organization","O","B-organization","O","B-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["award","country","theory","university","location","chemical_compound","organization","astronomical_object","enzyme","chemical_element","person","event","protein","academic_journal","discipline","scientist"]}
{"id":"208","dataset":"crossner_science","split":"test","instance":{"id":"208","prompt_labels":"Flgge(B-scientist) also(O) extended(O) Niels(B-scientist) Bohr(I-scientist) and(O) John(B-scientist) Archibald(I-scientist) Wheeler(I-scientist) theory(B-theory) of(I-theory) nuclear(I-theory) fission(I-theory) published(O) in(O) 1939(O) .(O) Niels(B-scientist) Bohr(I-scientist) and(O) J.(B-scientist) A.(I-scientist) Wheeler(I-scientist) Mechanism(O) of(O) nuclear(O) fission(O) ,(O) Phys(B-discipline) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, university, scientist, event, award, academic journal, country, organization, person, discipline, astronomical object, chemical compound, theory, chemical element, protein, location and O.\nSentence: Flgge also extended Niels Bohr and John Archibald Wheeler theory of nuclear fission published in 1939 . Niels Bohr and J. A. Wheeler Mechanism of nuclear fission , Phys .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Flgge","also","extended","Niels","Bohr","and","John","Archibald","Wheeler","theory","of","nuclear","fission","published","in","1939",".","Niels","Bohr","and","J.","A.","Wheeler","Mechanism","of","nuclear","fission",",","Phys","."],"labels":["B-scientist","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","B-theory","I-theory","I-theory","I-theory","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","O","O","O","O","B-discipline","O"],"target_index":null,"target_label":null},"label_list":["enzyme","university","scientist","event","award","academic_journal","country","organization","person","discipline","astronomical_object","chemical_compound","theory","chemical_element","protein","location"]}
{"id":"209","dataset":"crossner_science","split":"test","instance":{"id":"209","prompt_labels":"His(O) work(O) won(O) him(O) the(O) Rumford(B-award) Medal(I-award) of(I-award) the(I-award) Royal(I-award) Society(I-award) in(O) 1838(O) ,(O) and(O) in(O) 1843(O) he(O) received(O) its(O) Royal(B-award) Medal(I-award) for(O) a(O) paper(O) on(O) the(O) Transparency(O) of(O) the(O) Atmosphere(O) and(O) the(O) Laws(O) of(O) Extinction(O) of(O) the(O) Sun(B-astronomical object) 's(O) Rays(O) passing(O) through(O) it(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, protein, academic journal, university, chemical compound, enzyme, astronomical object, location, award, discipline, organization, country, event, scientist, person, theory and O.\nSentence: His work won him the Rumford Medal of the Royal Society in 1838 , and in 1843 he received its Royal Medal for a paper on the Transparency of the Atmosphere and the Laws of Extinction of the Sun 's Rays passing through it .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","work","won","him","the","Rumford","Medal","of","the","Royal","Society","in","1838",",","and","in","1843","he","received","its","Royal","Medal","for","a","paper","on","the","Transparency","of","the","Atmosphere","and","the","Laws","of","Extinction","of","the","Sun","'s","Rays","passing","through","it","."],"labels":["O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","protein","academic_journal","university","chemical_compound","enzyme","astronomical_object","location","award","discipline","organization","country","event","scientist","person","theory"]}
{"id":"214","dataset":"crossner_science","split":"test","instance":{"id":"214","prompt_labels":"The(O) Right(O) Stuff(O) won(O) four(O) Academy(O) Awards(O) :(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Sound(I-award) Editing(I-award) ((O) Jay(B-person) Boekelheide(I-person) )(O) ,(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Film(I-award) Editing(I-award) ,(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Score(I-award) and(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Sound(I-award) Mixing(I-award) ((O) Mark(B-person) Berger(I-person) ,(O) Tom(B-person) Scott(I-person) ,(O) Randy(B-person) Thom(I-person) and(O) David(B-person) MacMillan(I-person) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, university, person, organization, astronomical object, enzyme, chemical compound, location, event, award, discipline, academic journal, scientist, theory, country, chemical element and O.\nSentence: The Right Stuff won four Academy Awards : for Academy Award for Best Sound Editing ( Jay Boekelheide ) , for Academy Award for Best Film Editing , for Academy Award for Best Original Score and for Academy Award for Best Sound Mixing ( Mark Berger , Tom Scott , Randy Thom and David MacMillan ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Right","Stuff","won","four","Academy","Awards",":","for","Academy","Award","for","Best","Sound","Editing","(","Jay","Boekelheide",")",",","for","Academy","Award","for","Best","Film","Editing",",","for","Academy","Award","for","Best","Original","Score","and","for","Academy","Award","for","Best","Sound","Mixing","(","Mark","Berger",",","Tom","Scott",",","Randy","Thom","and","David","MacMillan",")","."],"labels":["O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-person","I-person","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","O"],"target_index":null,"target_label":null},"label_list":["protein","university","person","organization","astronomical_object","enzyme","chemical_compound","location","event","award","discipline","academic_journal","scientist","theory","country","chemical_element"]}
{"id":"216","dataset":"crossner_science","split":"test","instance":{"id":"216","prompt_labels":"The(O) Royal(B-organization) Society(I-organization) elected(O) him(O) a(O) Fellow(B-award) of(I-award) the(I-award) Royal(I-award) Society(I-award) in(O) June(O) 1877(O) and(O) bestowed(O) their(O) Rumford(B-award) Medal(I-award) ((O) 1894(O) )(O) ,(O) Davy(B-award) ((O) 1909(O) )(O) ,(O) and(O) Copley(B-award) Medal(I-award) ((O) 1916(O) )(O) medals(O) upon(O) him(O) for(O) his(O) work(O) ,(O) as(O) well(O) as(O) inviting(O) him(O) to(O) deliver(O) their(O) Bakerian(O) Lecture(O) in(O) 1901(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, protein, country, chemical compound, university, discipline, academic journal, event, person, enzyme, award, astronomical object, organization, chemical element, scientist, theory and O.\nSentence: The Royal Society elected him a Fellow of the Royal Society in June 1877 and bestowed their Rumford Medal ( 1894 ) , Davy ( 1909 ) , and Copley Medal ( 1916 ) medals upon him for his work , as well as inviting him to deliver their Bakerian Lecture in 1901 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Royal","Society","elected","him","a","Fellow","of","the","Royal","Society","in","June","1877","and","bestowed","their","Rumford","Medal","(","1894",")",",","Davy","(","1909",")",",","and","Copley","Medal","(","1916",")","medals","upon","him","for","his","work",",","as","well","as","inviting","him","to","deliver","their","Bakerian","Lecture","in","1901","."],"labels":["O","B-organization","I-organization","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","B-award","I-award","O","O","O","O","B-award","O","O","O","O","O","B-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","protein","country","chemical_compound","university","discipline","academic_journal","event","person","enzyme","award","astronomical_object","organization","chemical_element","scientist","theory"]}
{"id":"218","dataset":"crossner_science","split":"test","instance":{"id":"218","prompt_labels":"2060(B-astronomical object) Chiron(I-astronomical object) is(O) a(O) small(O) Solar(O) System(O) body(O) in(O) the(O) outer(O) Solar(O) System(O) ,(O) orbit(O) ing(O) the(O) Sun(B-astronomical object) between(O) Saturn(B-astronomical object) and(O) Uranus(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, person, theory, enzyme, protein, organization, award, university, country, astronomical object, chemical element, academic journal, location, scientist, discipline, event and O.\nSentence: 2060 Chiron is a small Solar System body in the outer Solar System , orbit ing the Sun between Saturn and Uranus .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["2060","Chiron","is","a","small","Solar","System","body","in","the","outer","Solar","System",",","orbit","ing","the","Sun","between","Saturn","and","Uranus","."],"labels":["B-astronomical object","I-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","person","theory","enzyme","protein","organization","award","university","country","astronomical_object","chemical_element","academic_journal","location","scientist","discipline","event"]}
{"id":"220","dataset":"crossner_science","split":"test","instance":{"id":"220","prompt_labels":"In(O) 1753(O) ,(O) he(O) was(O) elected(O) a(O) foreign(O) member(O) of(O) the(O) Royal(B-organization) Swedish(I-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) and(O) on(O) 15(O) December(O) 1769(O) a(O) foreign(O) member(O) of(O) the(O) Royal(B-organization) Danish(I-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) and(I-organization) Letters(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, astronomical object, theory, chemical compound, protein, discipline, location, academic journal, person, award, enzyme, scientist, event, university, country, organization and O.\nSentence: In 1753 , he was elected a foreign member of the Royal Swedish Academy of Sciences , and on 15 December 1769 a foreign member of the Royal Danish Academy of Sciences and Letters .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1753",",","he","was","elected","a","foreign","member","of","the","Royal","Swedish","Academy","of","Sciences",",","and","on","15","December","1769","a","foreign","member","of","the","Royal","Danish","Academy","of","Sciences","and","Letters","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","astronomical_object","theory","chemical_compound","protein","discipline","location","academic_journal","person","award","enzyme","scientist","event","university","country","organization"]}
{"id":"221","dataset":"crossner_science","split":"test","instance":{"id":"221","prompt_labels":"A(O) research(O) into(O) the(O) mutual(O) perturbations(O) of(O) Jupiter(B-astronomical object) and(O) Saturn(B-astronomical object) secured(O) for(O) him(O) the(O) prize(O) of(O) the(O) Berlin(B-organization) Academy(I-organization) in(O) 1830(O) ,(O) and(O) a(O) memoir(O) on(O) cometary(O) disturbances(O) was(O) crowned(O) by(O) the(O) Paris(B-organization) Academy(I-organization) in(O) 1850(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, organization, location, academic journal, chemical compound, astronomical object, award, protein, university, theory, enzyme, scientist, event, person, country, discipline and O.\nSentence: A research into the mutual perturbations of Jupiter and Saturn secured for him the prize of the Berlin Academy in 1830 , and a memoir on cometary disturbances was crowned by the Paris Academy in 1850 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","research","into","the","mutual","perturbations","of","Jupiter","and","Saturn","secured","for","him","the","prize","of","the","Berlin","Academy","in","1830",",","and","a","memoir","on","cometary","disturbances","was","crowned","by","the","Paris","Academy","in","1850","."],"labels":["O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","organization","location","academic_journal","chemical_compound","astronomical_object","award","protein","university","theory","enzyme","scientist","event","person","country","discipline"]}
{"id":"222","dataset":"crossner_science","split":"test","instance":{"id":"222","prompt_labels":"The(O) lithophile(O) elements(O) include(O) :(O) Al(B-chemical element) ,(O) Boron(B-chemical element) ,(O) Barium(B-chemical element) ,(O) Beryllium(B-chemical element) ,(O) Bromine(B-chemical element) ,(O) Calcium(B-chemical element) ,(O) Chlorine(B-chemical element) ,(O) Cr(B-chemical element) ,(O) Caesium(B-chemical element) ,(O) Fluorine(B-chemical element) ,(O) Iodine(B-chemical element) ,(O) Hf(B-chemical element) ,(O) Potassium(B-chemical element) ,(O) Lithium(B-chemical element) ,(O) Magnesium(B-chemical element) ,(O) Sodium(B-chemical element) ,(O) Niobium(B-chemical element) ,(O) O(B-chemical element) ,(O) P(B-chemical element) ,(O) Rubidium(B-chemical element) ,(O) Sc(B-chemical element) ,(O) Si(B-chemical element) ,(O) Strontium(B-chemical element) ,(O) Ta(B-chemical element) ,(O) Th(B-chemical element) ,(O) Ti(B-chemical element) ,(O) U(B-chemical element) ,(O) V(B-chemical element) ,(O) Y(B-chemical element) ,(O) Zirconium(B-chemical element) ,(O) W(B-chemical element) and(O) the(O) lanthanides(B-chemical element) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, organization, astronomical object, person, discipline, event, theory, chemical compound, location, award, academic journal, chemical element, country, protein, scientist, enzyme and O.\nSentence: The lithophile elements include : Al , Boron , Barium , Beryllium , Bromine , Calcium , Chlorine , Cr , Caesium , Fluorine , Iodine , Hf , Potassium , Lithium , Magnesium , Sodium , Niobium , O , P , Rubidium , Sc , Si , Strontium , Ta , Th , Ti , U , V , Y , Zirconium , W and the lanthanides .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","lithophile","elements","include",":","Al",",","Boron",",","Barium",",","Beryllium",",","Bromine",",","Calcium",",","Chlorine",",","Cr",",","Caesium",",","Fluorine",",","Iodine",",","Hf",",","Potassium",",","Lithium",",","Magnesium",",","Sodium",",","Niobium",",","O",",","P",",","Rubidium",",","Sc",",","Si",",","Strontium",",","Ta",",","Th",",","Ti",",","U",",","V",",","Y",",","Zirconium",",","W","and","the","lanthanides","."],"labels":["O","O","O","O","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","O","B-chemical element","O"],"target_index":null,"target_label":null},"label_list":["university","organization","astronomical_object","person","discipline","event","theory","chemical_compound","location","award","academic_journal","chemical_element","country","protein","scientist","enzyme"]}
{"id":"223","dataset":"crossner_science","split":"test","instance":{"id":"223","prompt_labels":"He(O) has(O) been(O) elected(O) to(O) the(O) US(B-organization) National(I-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) and(O) Britain(B-location) 's(O) Royal(B-organization) Society(I-organization) ,(O) as(O) well(O) as(O) to(O) the(O) American(B-organization) Philosophical(I-organization) Society(I-organization) and(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, university, academic journal, person, chemical compound, chemical element, discipline, location, country, protein, theory, organization, astronomical object, scientist, enzyme and O.\nSentence: He has been elected to the US National Academy of Sciences and Britain 's Royal Society , as well as to the American Philosophical Society and the American Academy of Arts and Sciences .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","has","been","elected","to","the","US","National","Academy","of","Sciences","and","Britain","'s","Royal","Society",",","as","well","as","to","the","American","Philosophical","Society","and","the","American","Academy","of","Arts","and","Sciences","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-location","O","B-organization","I-organization","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["award","event","university","academic_journal","person","chemical_compound","chemical_element","discipline","location","country","protein","theory","organization","astronomical_object","scientist","enzyme"]}
{"id":"225","dataset":"crossner_science","split":"test","instance":{"id":"225","prompt_labels":"Histone(B-protein) s(O) ,(O) gene(O) regulatory(O) proteins(O) ,(O) DNA(B-enzyme) polymerase(I-enzyme) and(O) RNA(B-enzyme) polymerase(I-enzyme) s(O) ,(O) and(O) other(O) substances(O) essential(O) for(O) nuclear(O) activities(O) must(O) be(O) imported(O) from(O) the(O) cytoplasm(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, scientist, chemical compound, organization, protein, event, academic journal, location, theory, award, astronomical object, country, discipline, person, chemical element, university and O.\nSentence: Histone s , gene regulatory proteins , DNA polymerase and RNA polymerase s , and other substances essential for nuclear activities must be imported from the cytoplasm .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Histone","s",",","gene","regulatory","proteins",",","DNA","polymerase","and","RNA","polymerase","s",",","and","other","substances","essential","for","nuclear","activities","must","be","imported","from","the","cytoplasm","."],"labels":["B-protein","O","O","O","O","O","O","B-enzyme","I-enzyme","O","B-enzyme","I-enzyme","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["enzyme","scientist","chemical_compound","organization","protein","event","academic_journal","location","theory","award","astronomical_object","country","discipline","person","chemical_element","university"]}
{"id":"228","dataset":"crossner_science","split":"test","instance":{"id":"228","prompt_labels":"Against(O) a(O) Crooked(O) Sky(O) is(O) a(O) 1975(O) American(O) Western(O) film(O) directed(O) by(O) Earl(B-person) Bellamy(I-person) and(O) starring(O) Richard(B-person) Boone(I-person) ,(O) Stewart(B-person) Petersen(I-person) ,(O) and(O) Henry(B-person) Wilcoxon(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, chemical compound, organization, protein, award, event, academic journal, enzyme, discipline, astronomical object, country, theory, university, scientist, location, chemical element and O.\nSentence: Against a Crooked Sky is a 1975 American Western film directed by Earl Bellamy and starring Richard Boone , Stewart Petersen , and Henry Wilcoxon .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Against","a","Crooked","Sky","is","a","1975","American","Western","film","directed","by","Earl","Bellamy","and","starring","Richard","Boone",",","Stewart","Petersen",",","and","Henry","Wilcoxon","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","B-person","I-person","O","B-person","I-person","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["person","chemical_compound","organization","protein","award","event","academic_journal","enzyme","discipline","astronomical_object","country","theory","university","scientist","location","chemical_element"]}
{"id":"232","dataset":"crossner_science","split":"test","instance":{"id":"232","prompt_labels":"The(O) episode(O) was(O) written(O) by(O) series(O) creator(O) Shonda(B-person) Rhimes(I-person) and(O) directed(O) by(O) co-executive(O) producer(O) Peter(B-person) Horton(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, country, location, chemical element, person, enzyme, astronomical object, scientist, theory, organization, chemical compound, university, award, protein, event, discipline and O.\nSentence: The episode was written by series creator Shonda Rhimes and directed by co-executive producer Peter Horton .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","episode","was","written","by","series","creator","Shonda","Rhimes","and","directed","by","co-executive","producer","Peter","Horton","."],"labels":["O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","country","location","chemical_element","person","enzyme","astronomical_object","scientist","theory","organization","chemical_compound","university","award","protein","event","discipline"]}
{"id":"236","dataset":"crossner_science","split":"test","instance":{"id":"236","prompt_labels":"Michalak(B-person) 's(O) estimate(O) depends(O) on(O) the(O) masses(O) of(O) 19(B-astronomical object) Fortuna(I-astronomical object) ,(O) 29(B-astronomical object) Amphitrite(I-astronomical object) ,(O) and(O) 16(B-astronomical object) Psyche(I-astronomical object) ;(O) thus(O) this(O) mass(O) was(O) obtained(O) assuming(O) an(O) incomplete(O) dynamical(O) model(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, person, enzyme, university, award, theory, organization, academic journal, event, protein, astronomical object, country, discipline, chemical compound, location, chemical element and O.\nSentence: Michalak 's estimate depends on the masses of 19 Fortuna , 29 Amphitrite , and 16 Psyche ; thus this mass was obtained assuming an incomplete dynamical model .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Michalak","'s","estimate","depends","on","the","masses","of","19","Fortuna",",","29","Amphitrite",",","and","16","Psyche",";","thus","this","mass","was","obtained","assuming","an","incomplete","dynamical","model","."],"labels":["B-person","O","O","O","O","O","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","B-astronomical object","I-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","person","enzyme","university","award","theory","organization","academic_journal","event","protein","astronomical_object","country","discipline","chemical_compound","location","chemical_element"]}
{"id":"237","dataset":"crossner_science","split":"test","instance":{"id":"237","prompt_labels":"Such(O) covalent(O) substances(O) are(O) usually(O) gases(O) ,(O) for(O) example(O) ,(O) HCl(B-chemical compound) ,(O) Sulfur(B-chemical compound) dioxide(I-chemical compound) ,(O) Carbon(B-chemical compound) dioxide(I-chemical compound) ,(O) and(O) Methane(B-chemical compound) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, organization, chemical element, astronomical object, award, person, country, theory, university, discipline, enzyme, event, location, academic journal, chemical compound, protein and O.\nSentence: Such covalent substances are usually gases , for example , HCl , Sulfur dioxide , Carbon dioxide , and Methane .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Such","covalent","substances","are","usually","gases",",","for","example",",","HCl",",","Sulfur","dioxide",",","Carbon","dioxide",",","and","Methane","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-chemical compound","O","B-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","O","O","B-chemical compound","O"],"target_index":null,"target_label":null},"label_list":["scientist","organization","chemical_element","astronomical_object","award","person","country","theory","university","discipline","enzyme","event","location","academic_journal","chemical_compound","protein"]}
{"id":"238","dataset":"crossner_science","split":"test","instance":{"id":"238","prompt_labels":"Directors(O) of(O) the(O) botanical(O) garden(O) from(O) this(O) period(O) included(O) Karl(B-scientist) Julius(I-scientist) Perleb(I-scientist) ,(O) Fridolin(B-scientist) Karl(I-scientist) Leopold(I-scientist) Spenner(I-scientist) ,(O) Alexander(B-scientist) Braun(I-scientist) ,(O) Carl(B-scientist) Wilhelm(I-scientist) von(I-scientist) Ngeli(I-scientist) ,(O) Heinrich(B-scientist) Anton(I-scientist) de(I-scientist) Bary(I-scientist) and(O) Julius(B-scientist) von(I-scientist) Sachs(I-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, organization, theory, astronomical object, discipline, chemical compound, scientist, chemical element, person, enzyme, event, award, university, protein, academic journal and O.\nSentence: Directors of the botanical garden from this period included Karl Julius Perleb , Fridolin Karl Leopold Spenner , Alexander Braun , Carl Wilhelm von Ngeli , Heinrich Anton de Bary and Julius von Sachs .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Directors","of","the","botanical","garden","from","this","period","included","Karl","Julius","Perleb",",","Fridolin","Karl","Leopold","Spenner",",","Alexander","Braun",",","Carl","Wilhelm","von","Ngeli",",","Heinrich","Anton","de","Bary","and","Julius","von","Sachs","."],"labels":["O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O"],"target_index":null,"target_label":null},"label_list":["country","location","organization","theory","astronomical_object","discipline","chemical_compound","scientist","chemical_element","person","enzyme","event","award","university","protein","academic_journal"]}
{"id":"239","dataset":"crossner_science","split":"test","instance":{"id":"239","prompt_labels":"The(O) most(O) influential(O) people(O) were(O) Erich(B-person) Schumann(I-person) ,(O) Abraham(B-person) Esau(I-person) ,(O) Walther(B-person) Gerlach(I-person) ,(O) and(O) Kurt(B-person) Diebner(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, country, enzyme, organization, discipline, award, chemical element, theory, event, location, scientist, academic journal, astronomical object, chemical compound, person, protein and O.\nSentence: The most influential people were Erich Schumann , Abraham Esau , Walther Gerlach , and Kurt Diebner .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","most","influential","people","were","Erich","Schumann",",","Abraham","Esau",",","Walther","Gerlach",",","and","Kurt","Diebner","."],"labels":["O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["university","country","enzyme","organization","discipline","award","chemical_element","theory","event","location","scientist","academic_journal","astronomical_object","chemical_compound","person","protein"]}
{"id":"240","dataset":"crossner_science","split":"test","instance":{"id":"240","prompt_labels":"Paleontologists(B-discipline) -(O) Mary(B-scientist) Anning(I-scientist) -(O) Robert(B-scientist) T.(I-scientist) Bakker(I-scientist) -(O) Barnum(B-scientist) Brown(I-scientist) -(O) William(B-scientist) Buckland(I-scientist) -(O) Edward(B-scientist) Drinker(I-scientist) Cope(I-scientist) -(O) Jack(B-scientist) Horner(I-scientist) -(O) Gideon(B-scientist) Mantell(I-scientist) -(O) Othniel(B-scientist) Charles(I-scientist) Marsh(I-scientist) -(O) John(B-scientist) Ostrom(I-scientist) -(O) Dong(B-scientist) Zhiming(I-scientist) br(O) /(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, academic journal, theory, chemical element, astronomical object, scientist, discipline, enzyme, chemical compound, organization, person, event, award, country, university, protein and O.\nSentence: Paleontologists - Mary Anning - Robert T. Bakker - Barnum Brown - William Buckland - Edward Drinker Cope - Jack Horner - Gideon Mantell - Othniel Charles Marsh - John Ostrom - Dong Zhiming br /","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Paleontologists","-","Mary","Anning","-","Robert","T.","Bakker","-","Barnum","Brown","-","William","Buckland","-","Edward","Drinker","Cope","-","Jack","Horner","-","Gideon","Mantell","-","Othniel","Charles","Marsh","-","John","Ostrom","-","Dong","Zhiming","br","/"],"labels":["B-discipline","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O"],"target_index":null,"target_label":null},"label_list":["location","academic_journal","theory","chemical_element","astronomical_object","scientist","discipline","enzyme","chemical_compound","organization","person","event","award","country","university","protein"]}
{"id":"244","dataset":"crossner_science","split":"test","instance":{"id":"244","prompt_labels":"Other(O) studied(O) asteroids(O) included(O) 15(B-astronomical object) Eunomia(I-astronomical object) ,(O) 43(B-astronomical object) Ariadne(I-astronomical object) ,(O) 44(B-astronomical object) Nysa(I-astronomical object) ,(O) and(O) 624(B-astronomical object) Hektor(I-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, award, organization, event, academic journal, scientist, location, country, chemical compound, discipline, protein, theory, person, astronomical object, chemical element, university and O.\nSentence: Other studied asteroids included 15 Eunomia , 43 Ariadne , 44 Nysa , and 624 Hektor .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","studied","asteroids","included","15","Eunomia",",","43","Ariadne",",","44","Nysa",",","and","624","Hektor","."],"labels":["O","O","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","B-astronomical object","I-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["enzyme","award","organization","event","academic_journal","scientist","location","country","chemical_compound","discipline","protein","theory","person","astronomical_object","chemical_element","university"]}
{"id":"246","dataset":"crossner_science","split":"test","instance":{"id":"246","prompt_labels":"Audiences(O) in(O) Monte(B-location) Carlo(I-location) ,(O) Warsaw(B-location) and(O) Buenos(B-location) Aires(I-location) also(O) heard(O) Caruso(B-person) sing(O) during(O) this(O) pivotal(O) phase(O) of(O) his(O) career(O) and(O) ,(O) in(O) 1899-1900(O) ,(O) he(O) appeared(O) before(O) the(O) tsar(O) and(O) the(O) Russian(O) aristocracy(O) at(O) the(O) Mariinsky(B-location) Theatre(I-location) in(O) Saint(O) Petersburg(O) and(O) the(O) Bolshoi(B-location) Theatre(I-location) in(O) Moscow(B-location) as(O) part(O) of(O) a(O) touring(O) company(O) of(O) first-class(O) Italian(O) singers(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, event, award, enzyme, academic journal, chemical compound, protein, discipline, university, astronomical object, person, location, country, scientist, organization, theory and O.\nSentence: Audiences in Monte Carlo , Warsaw and Buenos Aires also heard Caruso sing during this pivotal phase of his career and , in 1899-1900 , he appeared before the tsar and the Russian aristocracy at the Mariinsky Theatre in Saint Petersburg and the Bolshoi Theatre in Moscow as part of a touring company of first-class Italian singers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Audiences","in","Monte","Carlo",",","Warsaw","and","Buenos","Aires","also","heard","Caruso","sing","during","this","pivotal","phase","of","his","career","and",",","in","1899-1900",",","he","appeared","before","the","tsar","and","the","Russian","aristocracy","at","the","Mariinsky","Theatre","in","Saint","Petersburg","and","the","Bolshoi","Theatre","in","Moscow","as","part","of","a","touring","company","of","first-class","Italian","singers","."],"labels":["O","O","B-location","I-location","O","B-location","O","B-location","I-location","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","B-location","I-location","O","B-location","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","event","award","enzyme","academic_journal","chemical_compound","protein","discipline","university","astronomical_object","person","location","country","scientist","organization","theory"]}
{"id":"247","dataset":"crossner_science","split":"test","instance":{"id":"247","prompt_labels":"Another(O) small(O) epoch(O) seen(O) as(O) a(O) golden(O) age(O) is(O) the(O) unification(O) of(O) electricity(B-discipline) ,(O) magnetism(B-discipline) ,(O) and(O) optics(B-discipline) because(O) of(O) 19th(O) century(O) notables(O) ,(O) including(O) Michael(B-scientist) Faraday(I-scientist) ,(O) James(B-scientist) Clerk(I-scientist) Maxwell(I-scientist) ,(O) and(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, astronomical object, award, protein, discipline, location, scientist, university, academic journal, chemical element, person, organization, theory, chemical compound, country, enzyme and O.\nSentence: Another small epoch seen as a golden age is the unification of electricity , magnetism , and optics because of 19th century notables , including Michael Faraday , James Clerk Maxwell , and others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Another","small","epoch","seen","as","a","golden","age","is","the","unification","of","electricity",",","magnetism",",","and","optics","because","of","19th","century","notables",",","including","Michael","Faraday",",","James","Clerk","Maxwell",",","and","others","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-discipline","O","B-discipline","O","O","B-discipline","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","astronomical_object","award","protein","discipline","location","scientist","university","academic_journal","chemical_element","person","organization","theory","chemical_compound","country","enzyme"]}
{"id":"248","dataset":"crossner_science","split":"test","instance":{"id":"248","prompt_labels":"This(O) culminates(O) in(O) the(O) translocation(O) of(O) the(O) NF-B(B-protein) transcription(O) factor(O) Relish(O) ,(O) leading(O) to(O) production(O) of(O) antimicrobial(O) peptides(O) and(O) other(O) effectors(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, chemical compound, chemical element, enzyme, award, organization, scientist, theory, country, protein, university, astronomical object, event, academic journal, person, location and O.\nSentence: This culminates in the translocation of the NF-B transcription factor Relish , leading to production of antimicrobial peptides and other effectors .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","culminates","in","the","translocation","of","the","NF-B","transcription","factor","Relish",",","leading","to","production","of","antimicrobial","peptides","and","other","effectors","."],"labels":["O","O","O","O","O","O","O","B-protein","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["discipline","chemical_compound","chemical_element","enzyme","award","organization","scientist","theory","country","protein","university","astronomical_object","event","academic_journal","person","location"]}
{"id":"250","dataset":"crossner_science","split":"test","instance":{"id":"250","prompt_labels":"The(O) first(O) one(O) ,(O) Annales(B-academic journal) de(I-academic journal) physique(I-academic journal) ,(O) was(O) latterly(O) published(O) by(O) EDP(B-organization) Sciences(I-organization) under(O) the(O) same(O) name(O) up(O) to(O) 2009(O) ,(O) when(O) it(O) became(O) integrated(O) in(O) the(O) European(B-academic journal) Physical(I-academic journal) Journal(I-academic journal) series(O) as(O) the(O) European(B-academic journal) Physical(I-academic journal) Journal(I-academic journal) H(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, university, scientist, discipline, protein, chemical element, theory, location, academic journal, country, event, chemical compound, enzyme, person, astronomical object, organization and O.\nSentence: The first one , Annales de physique , was latterly published by EDP Sciences under the same name up to 2009 , when it became integrated in the European Physical Journal series as the European Physical Journal H .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","first","one",",","Annales","de","physique",",","was","latterly","published","by","EDP","Sciences","under","the","same","name","up","to","2009",",","when","it","became","integrated","in","the","European","Physical","Journal","series","as","the","European","Physical","Journal","H","."],"labels":["O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["award","university","scientist","discipline","protein","chemical_element","theory","location","academic_journal","country","event","chemical_compound","enzyme","person","astronomical_object","organization"]}
{"id":"251","dataset":"crossner_science","split":"test","instance":{"id":"251","prompt_labels":"Many(O) scientists(O) such(O) as(O) James(B-scientist) Clerk(I-scientist) Maxwell(I-scientist) ,(O) Oliver(B-scientist) Heaviside(I-scientist) and(O) Heinrich(B-scientist) Hertz(I-scientist) unsuccessfully(O) attempted(O) to(O) solve(O) these(O) problems(O) by(O) incorporating(O) either(O) Fresnel(B-scientist) or(O) Stokes(B-scientist) '(O) theories(O) into(O) Maxwell(B-theory) 's(I-theory) new(I-theory) electromagnetic(I-theory) laws(I-theory) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, organization, country, university, award, astronomical object, enzyme, academic journal, chemical element, person, location, discipline, scientist, chemical compound, event, theory and O.\nSentence: Many scientists such as James Clerk Maxwell , Oliver Heaviside and Heinrich Hertz unsuccessfully attempted to solve these problems by incorporating either Fresnel or Stokes ' theories into Maxwell 's new electromagnetic laws .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Many","scientists","such","as","James","Clerk","Maxwell",",","Oliver","Heaviside","and","Heinrich","Hertz","unsuccessfully","attempted","to","solve","these","problems","by","incorporating","either","Fresnel","or","Stokes","'","theories","into","Maxwell","'s","new","electromagnetic","laws","."],"labels":["O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","B-scientist","O","B-scientist","O","O","O","B-theory","I-theory","I-theory","I-theory","I-theory","O"],"target_index":null,"target_label":null},"label_list":["protein","organization","country","university","award","astronomical_object","enzyme","academic_journal","chemical_element","person","location","discipline","scientist","chemical_compound","event","theory"]}
{"id":"252","dataset":"crossner_science","split":"test","instance":{"id":"252","prompt_labels":"Furry(B-scientist) made(O) important(O) contributions(O) to(O) the(O) early(O) development(O) of(O) Quantum(B-theory) Field(I-theory) Theory(I-theory) with(O) J.(B-scientist) Robert(I-scientist) Oppenheimer(I-scientist) ,(O) Vladimir(B-scientist) Fock(I-scientist) ,(O) and(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, enzyme, chemical element, country, person, theory, chemical compound, university, organization, scientist, award, event, protein, astronomical object, location, discipline and O.\nSentence: Furry made important contributions to the early development of Quantum Field Theory with J. Robert Oppenheimer , Vladimir Fock , and others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Furry","made","important","contributions","to","the","early","development","of","Quantum","Field","Theory","with","J.","Robert","Oppenheimer",",","Vladimir","Fock",",","and","others","."],"labels":["B-scientist","O","O","O","O","O","O","O","O","B-theory","I-theory","I-theory","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","enzyme","chemical_element","country","person","theory","chemical_compound","university","organization","scientist","award","event","protein","astronomical_object","location","discipline"]}
{"id":"253","dataset":"crossner_science","split":"test","instance":{"id":"253","prompt_labels":"While(O) Caruso(B-person) sang(O) at(O) such(O) venues(O) as(O) La(B-location) Scala(I-location) in(O) Milan(B-location) ,(O) the(O) Royal(B-location) Opera(I-location) House(I-location) ,(O) in(O) London(B-location) ,(O) the(O) Mariinsky(B-location) Theatre(I-location) in(O) Saint(B-location) Petersburg(I-location) ,(O) and(O) the(O) Teatro(B-location) Coln(I-location) in(O) Buenos(B-location) Aires(I-location) ,(O) he(O) appeared(O) most(O) often(O) at(O) the(O) Metropolitan(B-location) Opera(I-location) in(O) New(B-location) York(I-location) City(I-location) where(O) he(O) was(O) the(O) leading(O) tenor(O) for(O) 18(O) consecutive(O) seasons(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, astronomical object, academic journal, country, award, discipline, chemical element, theory, scientist, chemical compound, protein, location, organization, event, person, university and O.\nSentence: While Caruso sang at such venues as La Scala in Milan , the Royal Opera House , in London , the Mariinsky Theatre in Saint Petersburg , and the Teatro Coln in Buenos Aires , he appeared most often at the Metropolitan Opera in New York City where he was the leading tenor for 18 consecutive seasons .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["While","Caruso","sang","at","such","venues","as","La","Scala","in","Milan",",","the","Royal","Opera","House",",","in","London",",","the","Mariinsky","Theatre","in","Saint","Petersburg",",","and","the","Teatro","Coln","in","Buenos","Aires",",","he","appeared","most","often","at","the","Metropolitan","Opera","in","New","York","City","where","he","was","the","leading","tenor","for","18","consecutive","seasons","."],"labels":["O","B-person","O","O","O","O","O","B-location","I-location","O","B-location","O","O","B-location","I-location","I-location","O","O","B-location","O","O","B-location","I-location","O","B-location","I-location","O","O","O","B-location","I-location","O","B-location","I-location","O","O","O","O","O","O","O","B-location","I-location","O","B-location","I-location","I-location","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["enzyme","astronomical_object","academic_journal","country","award","discipline","chemical_element","theory","scientist","chemical_compound","protein","location","organization","event","person","university"]}
{"id":"255","dataset":"crossner_science","split":"test","instance":{"id":"255","prompt_labels":"2016(O) he(O) has(O) been(O) awarded(O) the(O) Kavli(B-award) Prize(I-award) in(O) Nanoscience(B-discipline) together(O) with(O) Gerd(B-scientist) Binnig(I-scientist) and(O) Calvin(B-scientist) Quate(I-scientist) for(O) the(O) Scanning(O) Force(O) Microscope(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, discipline, location, university, academic journal, protein, country, enzyme, scientist, organization, astronomical object, chemical compound, person, chemical element, award, event and O.\nSentence: 2016 he has been awarded the Kavli Prize in Nanoscience together with Gerd Binnig and Calvin Quate for the Scanning Force Microscope .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["2016","he","has","been","awarded","the","Kavli","Prize","in","Nanoscience","together","with","Gerd","Binnig","and","Calvin","Quate","for","the","Scanning","Force","Microscope","."],"labels":["O","O","O","O","O","O","B-award","I-award","O","B-discipline","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["theory","discipline","location","university","academic_journal","protein","country","enzyme","scientist","organization","astronomical_object","chemical_compound","person","chemical_element","award","event"]}
{"id":"256","dataset":"crossner_science","split":"test","instance":{"id":"256","prompt_labels":"A(O) nucleosome(O) consists(O) of(O) 2(O) sets(O) of(O) 4(O) histones(O) :(O) H2A(B-protein) ,(O) H2B(B-protein) ,(O) Histone(B-protein) H3(I-protein) ,(O) and(O) Histone(B-protein) H4(I-protein) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, protein, person, organization, location, chemical element, enzyme, chemical compound, discipline, event, theory, award, country, academic journal, astronomical object, university and O.\nSentence: A nucleosome consists of 2 sets of 4 histones : H2A , H2B , Histone H3 , and Histone H4 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","nucleosome","consists","of","2","sets","of","4","histones",":","H2A",",","H2B",",","Histone","H3",",","and","Histone","H4","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-protein","O","B-protein","O","B-protein","I-protein","O","O","B-protein","I-protein","O"],"target_index":null,"target_label":null},"label_list":["scientist","protein","person","organization","location","chemical_element","enzyme","chemical_compound","discipline","event","theory","award","country","academic_journal","astronomical_object","university"]}
{"id":"257","dataset":"crossner_science","split":"test","instance":{"id":"257","prompt_labels":"In(O) 1978(O) ,(O) he(O) was(O) also(O) heavily(O) influenced(O) by(O) the(O) visit(O) of(O) Simon(B-scientist) van(I-scientist) der(I-scientist) Meer(I-scientist) and(O) Carlo(B-scientist) Rubbia(I-scientist) to(O) the(O) laboratory(O) ,(O) speaking(O) on(O) the(O) possibilities(O) of(O) stochastic(O) phase(O) space(O) cooling(O) of(O) antiproton(O) beams(O) and(O) the(O) exciting(O) possibilities(O) with(O) proton-antiproton(O) collisions(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, enzyme, theory, person, discipline, university, chemical compound, organization, country, event, chemical element, location, academic journal, protein, astronomical object, scientist and O.\nSentence: In 1978 , he was also heavily influenced by the visit of Simon van der Meer and Carlo Rubbia to the laboratory , speaking on the possibilities of stochastic phase space cooling of antiproton beams and the exciting possibilities with proton-antiproton collisions .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1978",",","he","was","also","heavily","influenced","by","the","visit","of","Simon","van","der","Meer","and","Carlo","Rubbia","to","the","laboratory",",","speaking","on","the","possibilities","of","stochastic","phase","space","cooling","of","antiproton","beams","and","the","exciting","possibilities","with","proton-antiproton","collisions","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","enzyme","theory","person","discipline","university","chemical_compound","organization","country","event","chemical_element","location","academic_journal","protein","astronomical_object","scientist"]}
{"id":"259","dataset":"crossner_science","split":"test","instance":{"id":"259","prompt_labels":"A(O) 1990(O) Michael(B-person) Winner(I-person) film(O) Bullseye(O) !(O) ,(O) starring(O) Michael(B-person) Caine(I-person) and(O) Roger(B-person) Moore(I-person) ,(O) referenced(O) the(O) Fleischmann(O) and(O) Pons(O) experiment(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, chemical element, theory, astronomical object, enzyme, scientist, protein, organization, chemical compound, location, university, event, academic journal, country, award, person and O.\nSentence: A 1990 Michael Winner film Bullseye ! , starring Michael Caine and Roger Moore , referenced the Fleischmann and Pons experiment .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","1990","Michael","Winner","film","Bullseye","!",",","starring","Michael","Caine","and","Roger","Moore",",","referenced","the","Fleischmann","and","Pons","experiment","."],"labels":["O","O","B-person","I-person","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["discipline","chemical_element","theory","astronomical_object","enzyme","scientist","protein","organization","chemical_compound","location","university","event","academic_journal","country","award","person"]}
{"id":"263","dataset":"crossner_science","split":"test","instance":{"id":"263","prompt_labels":"((O) See(O) Noyori(O) asymmetric(O) hydrogenation(O) )(O) Asymmetric(O) hydrogenation(O) of(O) an(O) alkene(B-chemical compound) in(O) the(O) presence(O) of(O) ((O) ((O) S(B-chemical element) )(O) -(O) BINAP(B-chemical compound) )(O) Ru(B-chemical element) ((O) Acetate(B-chemical compound) )(O) sub2(O) /(O) sub(O) is(O) used(O) for(O) the(O) commercial(O) production(O) of(O) enantiomerically(O) pure(O) ((O) 97(O) %(O) ee(B-chemical compound) )(O) naproxen(B-chemical compound) ,(O) used(O) as(O) an(O) anti-inflammatory(O) drug(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, person, discipline, theory, chemical element, enzyme, academic journal, university, location, award, protein, event, astronomical object, country, organization, scientist and O.\nSentence: ( See Noyori asymmetric hydrogenation ) Asymmetric hydrogenation of an alkene in the presence of ( ( S ) - BINAP ) Ru ( Acetate ) sub2 / sub is used for the commercial production of enantiomerically pure ( 97 % ee ) naproxen , used as an anti-inflammatory drug .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["(","See","Noyori","asymmetric","hydrogenation",")","Asymmetric","hydrogenation","of","an","alkene","in","the","presence","of","(","(","S",")","-","BINAP",")","Ru","(","Acetate",")","sub2","/","sub","is","used","for","the","commercial","production","of","enantiomerically","pure","(","97","%","ee",")","naproxen",",","used","as","an","anti-inflammatory","drug","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-chemical compound","O","O","O","O","O","O","B-chemical element","O","O","B-chemical compound","O","B-chemical element","O","B-chemical compound","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","O","B-chemical compound","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","person","discipline","theory","chemical_element","enzyme","academic_journal","university","location","award","protein","event","astronomical_object","country","organization","scientist"]}
{"id":"264","dataset":"crossner_science","split":"test","instance":{"id":"264","prompt_labels":"At(O) the(O) end(O) of(O) May(O) ,(O) Mercury(B-astronomical object) ,(O) Venus(B-astronomical object) and(O) Jupiter(B-astronomical object) went(O) through(O) a(O) series(O) of(O) conjunctions(O) only(O) a(O) few(O) days(O) apart(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, enzyme, event, location, organization, country, theory, award, scientist, academic journal, chemical compound, protein, person, chemical element, discipline, university and O.\nSentence: At the end of May , Mercury , Venus and Jupiter went through a series of conjunctions only a few days apart .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["At","the","end","of","May",",","Mercury",",","Venus","and","Jupiter","went","through","a","series","of","conjunctions","only","a","few","days","apart","."],"labels":["O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","enzyme","event","location","organization","country","theory","award","scientist","academic_journal","chemical_compound","protein","person","chemical_element","discipline","university"]}
{"id":"266","dataset":"crossner_science","split":"test","instance":{"id":"266","prompt_labels":"Ernst(B-scientist) Mayr(I-scientist) remarks(O) that(O) the(O) theory(O) was(O) hotly(O) contested(O) by(O) some(O) famous(O) geneticists(O) :(O) William(B-scientist) Bateson(I-scientist) ,(O) Wilhelm(B-scientist) Johannsen(I-scientist) ,(O) Richard(B-scientist) Goldschmidt(I-scientist) and(O) T.H.(B-scientist) Morgan(I-scientist) ,(O) all(O) of(O) a(O) rather(O) dogmatic(O) turn(O) of(O) mind(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, country, discipline, event, theory, award, chemical compound, organization, chemical element, scientist, person, location, university, protein, enzyme, astronomical object and O.\nSentence: Ernst Mayr remarks that the theory was hotly contested by some famous geneticists : William Bateson , Wilhelm Johannsen , Richard Goldschmidt and T.H. Morgan , all of a rather dogmatic turn of mind .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Ernst","Mayr","remarks","that","the","theory","was","hotly","contested","by","some","famous","geneticists",":","William","Bateson",",","Wilhelm","Johannsen",",","Richard","Goldschmidt","and","T.H.","Morgan",",","all","of","a","rather","dogmatic","turn","of","mind","."],"labels":["B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","country","discipline","event","theory","award","chemical_compound","organization","chemical_element","scientist","person","location","university","protein","enzyme","astronomical_object"]}
{"id":"267","dataset":"crossner_science","split":"test","instance":{"id":"267","prompt_labels":"He(O) calculated(O) the(O) orbit(O) s(O) of(O) the(O) moons(O) of(O) Uranus(B-astronomical object) and(O) Saturn(B-astronomical object) ,(O) obtaining(O) the(O) first(O) value(O) for(O) Uranus(B-astronomical object) '(O) mass(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, person, chemical compound, enzyme, location, scientist, organization, award, protein, academic journal, chemical element, astronomical object, university, event, country, discipline and O.\nSentence: He calculated the orbit s of the moons of Uranus and Saturn , obtaining the first value for Uranus ' mass .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","calculated","the","orbit","s","of","the","moons","of","Uranus","and","Saturn",",","obtaining","the","first","value","for","Uranus","'","mass","."],"labels":["O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","B-astronomical object","O","O","O"],"target_index":null,"target_label":null},"label_list":["theory","person","chemical_compound","enzyme","location","scientist","organization","award","protein","academic_journal","chemical_element","astronomical_object","university","event","country","discipline"]}
{"id":"268","dataset":"crossner_science","split":"test","instance":{"id":"268","prompt_labels":"Some(O) known(O) inhibitors(O) of(O) the(O) caveolae(O) pathway(O) are(O) Filipin(B-chemical compound) III(I-chemical compound) ,(O) Genistein(B-chemical compound) and(O) Nystatin(B-chemical compound) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, academic journal, organization, chemical compound, astronomical object, chemical element, location, theory, discipline, country, event, scientist, protein, university, award, enzyme and O.\nSentence: Some known inhibitors of the caveolae pathway are Filipin III , Genistein and Nystatin .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","known","inhibitors","of","the","caveolae","pathway","are","Filipin","III",",","Genistein","and","Nystatin","."],"labels":["O","O","O","O","O","O","O","O","B-chemical compound","I-chemical compound","O","B-chemical compound","O","B-chemical compound","O"],"target_index":null,"target_label":null},"label_list":["person","academic_journal","organization","chemical_compound","astronomical_object","chemical_element","location","theory","discipline","country","event","scientist","protein","university","award","enzyme"]}
{"id":"269","dataset":"crossner_science","split":"test","instance":{"id":"269","prompt_labels":"Three(O) of(O) the(O) country(O) 's(O) largest(O) universities(O) are(O) located(O) in(O) central(B-location) Thessaloniki(I-location) :(O) Aristotle(B-university) University(I-university) of(I-university) Thessaloniki(I-university) ,(O) the(O) University(B-university) of(I-university) Macedonia(I-university) and(O) the(O) International(B-university) Hellenic(I-university) University(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, chemical element, protein, country, organization, award, discipline, chemical compound, person, event, astronomical object, scientist, theory, academic journal, location, university and O.\nSentence: Three of the country 's largest universities are located in central Thessaloniki : Aristotle University of Thessaloniki , the University of Macedonia and the International Hellenic University .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Three","of","the","country","'s","largest","universities","are","located","in","central","Thessaloniki",":","Aristotle","University","of","Thessaloniki",",","the","University","of","Macedonia","and","the","International","Hellenic","University","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-university","I-university","I-university","I-university","O","O","B-university","I-university","I-university","O","O","B-university","I-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["enzyme","chemical_element","protein","country","organization","award","discipline","chemical_compound","person","event","astronomical_object","scientist","theory","academic_journal","location","university"]}
{"id":"270","dataset":"crossner_science","split":"test","instance":{"id":"270","prompt_labels":"Walter(B-scientist) Kohn(I-scientist) ,(O) for(O) his(O) development(O) of(O) the(O) density-functional(B-theory) theory(I-theory) ,(O) and(O) John(B-scientist) Pople(I-scientist) ,(O) for(O) his(O) development(O) of(O) computational(O) methods(O) in(O) quantum(B-discipline) chemistry(I-discipline) ,(O) received(O) the(O) 1998(O) Nobel(B-award) Prize(I-award) in(I-award) Chemistry(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, chemical element, discipline, university, organization, scientist, enzyme, theory, person, chemical compound, event, location, astronomical object, award, academic journal, country and O.\nSentence: Walter Kohn , for his development of the density-functional theory , and John Pople , for his development of computational methods in quantum chemistry , received the 1998 Nobel Prize in Chemistry .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Walter","Kohn",",","for","his","development","of","the","density-functional","theory",",","and","John","Pople",",","for","his","development","of","computational","methods","in","quantum","chemistry",",","received","the","1998","Nobel","Prize","in","Chemistry","."],"labels":["B-scientist","I-scientist","O","O","O","O","O","O","B-theory","I-theory","O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","B-discipline","I-discipline","O","O","O","O","B-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["protein","chemical_element","discipline","university","organization","scientist","enzyme","theory","person","chemical_compound","event","location","astronomical_object","award","academic_journal","country"]}
{"id":"271","dataset":"crossner_science","split":"test","instance":{"id":"271","prompt_labels":"Max(B-scientist) Planck(I-scientist) ,(O) Albert(B-scientist) Einstein(I-scientist) ,(O) and(O) Niels(B-scientist) Bohr(I-scientist) postulated(O) the(O) occurrence(O) of(O) energy(O) in(O) discrete(O) quantities(O) ((O) quanta(O) )(O) in(O) order(O) to(O) explain(O) phenomena(O) such(O) as(O) the(O) spectrum(O) of(O) black-body(O) radiation(O) ,(O) the(O) photoelectric(B-theory) effect(I-theory) ,(O) and(O) the(O) stability(O) and(O) spectra(O) of(O) atoms(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, theory, enzyme, university, astronomical object, protein, discipline, country, chemical compound, person, event, chemical element, academic journal, award, scientist, organization and O.\nSentence: Max Planck , Albert Einstein , and Niels Bohr postulated the occurrence of energy in discrete quantities ( quanta ) in order to explain phenomena such as the spectrum of black-body radiation , the photoelectric effect , and the stability and spectra of atoms .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Max","Planck",",","Albert","Einstein",",","and","Niels","Bohr","postulated","the","occurrence","of","energy","in","discrete","quantities","(","quanta",")","in","order","to","explain","phenomena","such","as","the","spectrum","of","black-body","radiation",",","the","photoelectric","effect",",","and","the","stability","and","spectra","of","atoms","."],"labels":["B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-theory","I-theory","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","theory","enzyme","university","astronomical_object","protein","discipline","country","chemical_compound","person","event","chemical_element","academic_journal","award","scientist","organization"]}
{"id":"273","dataset":"crossner_science","split":"test","instance":{"id":"273","prompt_labels":"When(O) the(O) Royal(B-organization) Society(I-organization) in(O) London(B-location) published(O) the(O) groundbreaking(O) work(O) of(O) an(O) Italian(O) lensmaker(O) in(O) their(O) journal(O) Philosophical(B-academic journal) Transactions(I-academic journal) of(I-academic journal) the(I-academic journal) Royal(I-academic journal) Society(I-academic journal) ,(O) de(O) Graaf(O) wrote(O) to(O) the(O) editor(O) of(O) the(O) journal(O) ,(O) Henry(B-scientist) Oldenburg(I-scientist) ,(O) with(O) a(O) ringing(O) endorsement(O) of(O) van(B-scientist) Leeuwenhoek(I-scientist) 's(O) microscopes(O) which(O) ,(O) he(O) claimed(O) ,(O) far(O) surpass(O) those(O) which(O) we(O) have(O) hitherto(O) seen(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, academic journal, university, discipline, organization, event, theory, scientist, country, person, award, astronomical object, location, chemical element, protein, enzyme and O.\nSentence: When the Royal Society in London published the groundbreaking work of an Italian lensmaker in their journal Philosophical Transactions of the Royal Society , de Graaf wrote to the editor of the journal , Henry Oldenburg , with a ringing endorsement of van Leeuwenhoek 's microscopes which , he claimed , far surpass those which we have hitherto seen .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["When","the","Royal","Society","in","London","published","the","groundbreaking","work","of","an","Italian","lensmaker","in","their","journal","Philosophical","Transactions","of","the","Royal","Society",",","de","Graaf","wrote","to","the","editor","of","the","journal",",","Henry","Oldenburg",",","with","a","ringing","endorsement","of","van","Leeuwenhoek","'s","microscopes","which",",","he","claimed",",","far","surpass","those","which","we","have","hitherto","seen","."],"labels":["O","O","B-organization","I-organization","O","B-location","O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","O","O","O","O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","academic_journal","university","discipline","organization","event","theory","scientist","country","person","award","astronomical_object","location","chemical_element","protein","enzyme"]}
{"id":"276","dataset":"crossner_science","split":"test","instance":{"id":"276","prompt_labels":"As(O) examples(O) ,(O) antibiotic(O) drug(O) s(O) of(O) the(O) tetracycline(B-chemical compound) and(O) quinolone(B-chemical compound) families(O) are(O) chelators(O) of(O) Fe(B-chemical element) sup2(I-chemical element) +(I-chemical element) /(I-chemical element) sup(I-chemical element) ,(O) Calcium(B-chemical element) sup2(I-chemical element) +(I-chemical element) /(I-chemical element) sup(I-chemical element) ,(O) and(O) Magnesium(B-chemical element) sup2(I-chemical element) +(I-chemical element) /(I-chemical element) sup(I-chemical element) ions(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, university, person, protein, enzyme, discipline, astronomical object, event, country, organization, academic journal, chemical compound, theory, chemical element, scientist and O.\nSentence: As examples , antibiotic drug s of the tetracycline and quinolone families are chelators of Fe sup2 + / sup , Calcium sup2 + / sup , and Magnesium sup2 + / sup ions .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["As","examples",",","antibiotic","drug","s","of","the","tetracycline","and","quinolone","families","are","chelators","of","Fe","sup2","+","/","sup",",","Calcium","sup2","+","/","sup",",","and","Magnesium","sup2","+","/","sup","ions","."],"labels":["O","O","O","O","O","O","O","O","B-chemical compound","O","B-chemical compound","O","O","O","O","B-chemical element","I-chemical element","I-chemical element","I-chemical element","I-chemical element","O","B-chemical element","I-chemical element","I-chemical element","I-chemical element","I-chemical element","O","O","B-chemical element","I-chemical element","I-chemical element","I-chemical element","I-chemical element","O","O"],"target_index":null,"target_label":null},"label_list":["award","location","university","person","protein","enzyme","discipline","astronomical_object","event","country","organization","academic_journal","chemical_compound","theory","chemical_element","scientist"]}
{"id":"279","dataset":"crossner_science","split":"test","instance":{"id":"279","prompt_labels":"Since(O) then(O) ,(O) names(O) have(O) been(O) given(O) to(O) 134(O) additional(O) satellites(O) :(O) 57(O) satellites(O) of(O) Jupiter(B-astronomical object) ,(O) 43(O) of(O) Saturn(B-astronomical object) ,(O) 22(O) of(O) Uranus(B-astronomical object) ,(O) 12(O) of(O) Neptune(B-astronomical object) ,(O) 5(O) of(O) Pluto(B-astronomical object) ,(O) 1(O) of(O) Eris(B-astronomical object) ,(O) and(O) 2(O) of(O) Haumea(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, location, chemical compound, enzyme, award, academic journal, theory, protein, organization, event, discipline, country, university, scientist, person, chemical element and O.\nSentence: Since then , names have been given to 134 additional satellites : 57 satellites of Jupiter , 43 of Saturn , 22 of Uranus , 12 of Neptune , 5 of Pluto , 1 of Eris , and 2 of Haumea .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Since","then",",","names","have","been","given","to","134","additional","satellites",":","57","satellites","of","Jupiter",",","43","of","Saturn",",","22","of","Uranus",",","12","of","Neptune",",","5","of","Pluto",",","1","of","Eris",",","and","2","of","Haumea","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","B-astronomical object","O","O","O","B-astronomical object","O","O","O","B-astronomical object","O","O","O","B-astronomical object","O","O","O","B-astronomical object","O","O","O","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","location","chemical_compound","enzyme","award","academic_journal","theory","protein","organization","event","discipline","country","university","scientist","person","chemical_element"]}
{"id":"280","dataset":"crossner_science","split":"test","instance":{"id":"280","prompt_labels":"His(O) research(O) group(O) 's(O) developments(O) in(O) the(O) solar(B-discipline) and(I-discipline) plasmonics(I-discipline) field(I-discipline) have(O) been(O) featured(O) in(O) Scientific(O) American(O) and(O) in(O) research(O) papers(O) in(O) Science(B-academic journal) ,(O) Nature(B-academic journal) Materials(I-academic journal) ,(O) Nature(B-academic journal) Photonics(I-academic journal) and(O) Advanced(B-academic journal) Materials(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, academic journal, chemical element, person, enzyme, award, discipline, scientist, theory, organization, astronomical object, chemical compound, event, country, location, protein and O.\nSentence: His research group 's developments in the solar and plasmonics field have been featured in Scientific American and in research papers in Science , Nature Materials , Nature Photonics and Advanced Materials .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","research","group","'s","developments","in","the","solar","and","plasmonics","field","have","been","featured","in","Scientific","American","and","in","research","papers","in","Science",",","Nature","Materials",",","Nature","Photonics","and","Advanced","Materials","."],"labels":["O","O","O","O","O","O","O","B-discipline","I-discipline","I-discipline","I-discipline","O","O","O","O","O","O","O","O","O","O","O","B-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["university","academic_journal","chemical_element","person","enzyme","award","discipline","scientist","theory","organization","astronomical_object","chemical_compound","event","country","location","protein"]}
{"id":"282","dataset":"crossner_science","split":"test","instance":{"id":"282","prompt_labels":"Other(O) work(O) included(O) an(O) improved(O) determination(O) of(O) the(O) rotation(O) period(O) of(O) Mars(B-astronomical object) ,(O) the(O) discovery(O) that(O) the(O) Martian(O) polar(O) caps(O) vary(O) seasonally(O) ,(O) the(O) discovery(O) of(O) Titania(B-astronomical object) and(O) Oberon(B-astronomical object) ((O) moons(O) of(O) Uranus(B-astronomical object) )(O) and(O) Enceladus(B-astronomical object) and(O) Mimas(B-astronomical object) ((O) moons(O) of(O) Saturn(B-astronomical object) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, theory, location, person, discipline, university, award, protein, scientist, enzyme, chemical element, country, academic journal, chemical compound, event, organization and O.\nSentence: Other work included an improved determination of the rotation period of Mars , the discovery that the Martian polar caps vary seasonally , the discovery of Titania and Oberon ( moons of Uranus ) and Enceladus and Mimas ( moons of Saturn ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","work","included","an","improved","determination","of","the","rotation","period","of","Mars",",","the","discovery","that","the","Martian","polar","caps","vary","seasonally",",","the","discovery","of","Titania","and","Oberon","(","moons","of","Uranus",")","and","Enceladus","and","Mimas","(","moons","of","Saturn",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","B-astronomical object","O","O","B-astronomical object","O","B-astronomical object","O","O","O","B-astronomical object","O","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","theory","location","person","discipline","university","award","protein","scientist","enzyme","chemical_element","country","academic_journal","chemical_compound","event","organization"]}
{"id":"284","dataset":"crossner_science","split":"test","instance":{"id":"284","prompt_labels":"The(O) visitor(O) starts(O) with(O) the(O) sun(B-astronomical object) on(O) the(O) eastern(O) summit(O) of(O) the(O) Hill(B-location) ,(O) and(O) following(O) the(O) trail(O) to(O) the(O) west(O) from(O) the(O) Sun(B-astronomical object) ,(O) in(O) the(O) direction(O) of(O) Mills(B-location) Observatory(I-location) ,(O) he(O) or(O) she(O) will(O) encounter(O) another(O) eight(O) rocks(O) representing(O) the(O) planets(O) Mercury(B-astronomical object) ,(O) Venus(B-astronomical object) ,(O) Earth(B-astronomical object) ,(O) Mars(B-astronomical object) ,(O) Jupiter(B-astronomical object) ,(O) Saturn(B-astronomical object) ,(O) Uranus(B-astronomical object) and(O) Neptune(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, country, chemical compound, chemical element, person, event, scientist, astronomical object, university, academic journal, location, award, organization, theory, discipline, protein and O.\nSentence: The visitor starts with the sun on the eastern summit of the Hill , and following the trail to the west from the Sun , in the direction of Mills Observatory , he or she will encounter another eight rocks representing the planets Mercury , Venus , Earth , Mars , Jupiter , Saturn , Uranus and Neptune .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","visitor","starts","with","the","sun","on","the","eastern","summit","of","the","Hill",",","and","following","the","trail","to","the","west","from","the","Sun",",","in","the","direction","of","Mills","Observatory",",","he","or","she","will","encounter","another","eight","rocks","representing","the","planets","Mercury",",","Venus",",","Earth",",","Mars",",","Jupiter",",","Saturn",",","Uranus","and","Neptune","."],"labels":["O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","B-location","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["enzyme","country","chemical_compound","chemical_element","person","event","scientist","astronomical_object","university","academic_journal","location","award","organization","theory","discipline","protein"]}
{"id":"285","dataset":"crossner_science","split":"test","instance":{"id":"285","prompt_labels":"Simpson(B-person) 's(O) other(O) great(O) passions(O) were(O) astronomy(B-discipline) ((O) he(O) was(O) a(O) member(O) of(O) the(O) British(B-organization) Astronomical(I-organization) Association(I-organization) and(O) -(O) unusually(O) for(O) an(O) amateur(O) -(O) was(O) made(O) a(O) Fellow(B-award) of(I-award) the(I-award) Royal(I-award) Astronomical(I-award) Society(I-award) )(O) and(O) pacifism(O) ,(O) specifically(O) addressed(O) in(O) the(O) title(O) of(O) his(O) Tenth(O) String(O) Quartet(O) ,(O) For(O) Peace(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, university, country, enzyme, award, astronomical object, theory, organization, protein, chemical element, chemical compound, academic journal, scientist, discipline, person and O.\nSentence: Simpson 's other great passions were astronomy ( he was a member of the British Astronomical Association and - unusually for an amateur - was made a Fellow of the Royal Astronomical Society ) and pacifism , specifically addressed in the title of his Tenth String Quartet , For Peace .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Simpson","'s","other","great","passions","were","astronomy","(","he","was","a","member","of","the","British","Astronomical","Association","and","-","unusually","for","an","amateur","-","was","made","a","Fellow","of","the","Royal","Astronomical","Society",")","and","pacifism",",","specifically","addressed","in","the","title","of","his","Tenth","String","Quartet",",","For","Peace","."],"labels":["B-person","O","O","O","O","O","B-discipline","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","location","university","country","enzyme","award","astronomical_object","theory","organization","protein","chemical_element","chemical_compound","academic_journal","scientist","discipline","person"]}
{"id":"286","dataset":"crossner_science","split":"test","instance":{"id":"286","prompt_labels":"Palsson(B-scientist) serves(O) on(O) the(O) editorial(O) board(O) of(O) several(O) scientific(O) journals(O) including(O) Annals(B-academic journal) of(I-academic journal) Biomedical(I-academic journal) Engineering(I-academic journal) ,(O) Biotechnology(B-academic journal) and(I-academic journal) Bioengineering(I-academic journal) ,(O) Metabolic(B-academic journal) Engineering(I-academic journal) and(O) Molecular(B-academic journal) Systems(I-academic journal) Biology(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, chemical element, country, award, protein, event, university, organization, theory, enzyme, academic journal, scientist, chemical compound, location, person, astronomical object and O.\nSentence: Palsson serves on the editorial board of several scientific journals including Annals of Biomedical Engineering , Biotechnology and Bioengineering , Metabolic Engineering and Molecular Systems Biology .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Palsson","serves","on","the","editorial","board","of","several","scientific","journals","including","Annals","of","Biomedical","Engineering",",","Biotechnology","and","Bioengineering",",","Metabolic","Engineering","and","Molecular","Systems","Biology","."],"labels":["B-scientist","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["discipline","chemical_element","country","award","protein","event","university","organization","theory","enzyme","academic_journal","scientist","chemical_compound","location","person","astronomical_object"]}
{"id":"288","dataset":"crossner_science","split":"test","instance":{"id":"288","prompt_labels":"As(O) examples(O) ,(O) for(O) Greeks(O) ,(O) Constantine(B-person) XI(I-person) Palaiologos(I-person) and(O) Kolokotronis(B-person) ;(O) and(O) for(O) Serbs(O) ,(O) Milo(B-person) Obili(I-person) and(O) Tzar(B-person) Lazar(I-person) ;(O) for(O) Montenegrins(O) ,(O) ura(B-person) I(I-person) Bali(I-person) and(O) Ivan(B-person) Crnojevi(I-person) ;(O) for(O) Albanians(O) ,(O) George(B-person) Kastrioti(I-person) Skanderbeg(I-person) ;(O) for(O) ethnic(O) Macedonians(O) ,(O) Nikola(B-person) Karev(I-person) for(O) Bulgarians(O) ,(O) Vasil(B-person) Levski(I-person) ,(O) Georgi(B-person) Sava(I-person) Rakovski(I-person) and(O) Hristo(B-person) Botev(I-person) and(O) for(O) Croats(O) ,(O) Nikola(B-person) ubi(I-person) Zrinjski(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, discipline, chemical element, enzyme, academic journal, organization, astronomical object, university, country, award, chemical compound, event, location, scientist, person, protein and O.\nSentence: As examples , for Greeks , Constantine XI Palaiologos and Kolokotronis ; and for Serbs , Milo Obili and Tzar Lazar ; for Montenegrins , ura I Bali and Ivan Crnojevi ; for Albanians , George Kastrioti Skanderbeg ; for ethnic Macedonians , Nikola Karev for Bulgarians , Vasil Levski , Georgi Sava Rakovski and Hristo Botev and for Croats , Nikola ubi Zrinjski .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["As","examples",",","for","Greeks",",","Constantine","XI","Palaiologos","and","Kolokotronis",";","and","for","Serbs",",","Milo","Obili","and","Tzar","Lazar",";","for","Montenegrins",",","ura","I","Bali","and","Ivan","Crnojevi",";","for","Albanians",",","George","Kastrioti","Skanderbeg",";","for","ethnic","Macedonians",",","Nikola","Karev","for","Bulgarians",",","Vasil","Levski",",","Georgi","Sava","Rakovski","and","Hristo","Botev","and","for","Croats",",","Nikola","ubi","Zrinjski","."],"labels":["O","O","O","O","O","O","B-person","I-person","I-person","O","B-person","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","O","O","O","B-person","I-person","I-person","O","B-person","I-person","O","O","O","O","B-person","I-person","I-person","O","O","O","O","O","B-person","I-person","O","O","O","B-person","I-person","O","B-person","I-person","I-person","O","B-person","I-person","O","O","O","O","B-person","I-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["theory","discipline","chemical_element","enzyme","academic_journal","organization","astronomical_object","university","country","award","chemical_compound","event","location","scientist","person","protein"]}
{"id":"290","dataset":"crossner_science","split":"test","instance":{"id":"290","prompt_labels":"The(O) concept(O) of(O) dtournement(O) has(O) had(O) a(O) popular(O) influence(O) amongst(O) contemporary(O) radicals(O) ,(O) and(O) the(O) technique(O) can(O) be(O) seen(O) in(O) action(O) in(O) the(O) present(O) day(O) when(O) looking(O) at(O) the(O) work(O) of(O) Culture(O) Jammer(O) s(O) including(O) the(O) Cacophony(B-organization) Society(I-organization) ,(O) Billboard(B-event) Liberation(I-event) Front(I-event) ,(O) monochrom(B-organization) ,(O) Occupy(B-event) Movements(I-event) and(O) Adbusters(B-organization) ,(O) whose(O) subvertisements(O) detourn(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, university, country, protein, enzyme, location, scientist, theory, chemical element, organization, person, astronomical object, event, chemical compound, award, discipline and O.\nSentence: The concept of dtournement has had a popular influence amongst contemporary radicals , and the technique can be seen in action in the present day when looking at the work of Culture Jammer s including the Cacophony Society , Billboard Liberation Front , monochrom , Occupy Movements and Adbusters , whose subvertisements detourn .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","concept","of","dtournement","has","had","a","popular","influence","amongst","contemporary","radicals",",","and","the","technique","can","be","seen","in","action","in","the","present","day","when","looking","at","the","work","of","Culture","Jammer","s","including","the","Cacophony","Society",",","Billboard","Liberation","Front",",","monochrom",",","Occupy","Movements","and","Adbusters",",","whose","subvertisements","detourn","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","B-event","I-event","I-event","O","B-organization","O","B-event","I-event","O","B-organization","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","university","country","protein","enzyme","location","scientist","theory","chemical_element","organization","person","astronomical_object","event","chemical_compound","award","discipline"]}
{"id":"292","dataset":"crossner_science","split":"test","instance":{"id":"292","prompt_labels":"All(O) of(O) these(O) low-numbered(O) asteroids(O) have(O) numbers(O) between(O) and(O) were(O) discovered(O) between(O) 1876(O) and(O) the(O) 1930s(O) ,(O) predominantly(O) by(O) astronomers(O) Auguste(B-scientist) Charlois(I-scientist) ,(O) Johann(B-scientist) Palisa(I-scientist) ,(O) Max(B-scientist) Wolf(I-scientist) and(O) Karl(B-scientist) Reinmuth(I-scientist) ((O) also(O) see(O) category(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, astronomical object, event, chemical compound, scientist, enzyme, country, protein, university, academic journal, theory, chemical element, award, organization, location, person and O.\nSentence: All of these low-numbered asteroids have numbers between and were discovered between 1876 and the 1930s , predominantly by astronomers Auguste Charlois , Johann Palisa , Max Wolf and Karl Reinmuth ( also see category ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["All","of","these","low-numbered","asteroids","have","numbers","between","and","were","discovered","between","1876","and","the","1930s",",","predominantly","by","astronomers","Auguste","Charlois",",","Johann","Palisa",",","Max","Wolf","and","Karl","Reinmuth","(","also","see","category",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["discipline","astronomical_object","event","chemical_compound","scientist","enzyme","country","protein","university","academic_journal","theory","chemical_element","award","organization","location","person"]}
{"id":"293","dataset":"crossner_science","split":"test","instance":{"id":"293","prompt_labels":"It(O) was(O) developed(O) as(O) a(O) treatment(O) for(O) hepatitis(O) C(O) ,(O) acting(O) as(O) a(O) NS5B(B-chemical compound) RNA(I-chemical compound) polymerase(I-chemical compound) inhibitor(I-chemical compound) ,(O) but(O) while(O) it(O) showed(O) a(O) good(O) safety(O) profile(O) in(O) clinical(O) trials(O) ,(O) it(O) was(O) not(O) sufficiently(O) effective(O) to(O) be(O) used(O) as(O) a(O) stand-alone(O) agent(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, theory, university, organization, event, astronomical object, academic journal, enzyme, protein, person, country, location, chemical element, chemical compound, award, discipline and O.\nSentence: It was developed as a treatment for hepatitis C , acting as a NS5B RNA polymerase inhibitor , but while it showed a good safety profile in clinical trials , it was not sufficiently effective to be used as a stand-alone agent .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","developed","as","a","treatment","for","hepatitis","C",",","acting","as","a","NS5B","RNA","polymerase","inhibitor",",","but","while","it","showed","a","good","safety","profile","in","clinical","trials",",","it","was","not","sufficiently","effective","to","be","used","as","a","stand-alone","agent","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","I-chemical compound","I-chemical compound","I-chemical compound","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","theory","university","organization","event","astronomical_object","academic_journal","enzyme","protein","person","country","location","chemical_element","chemical_compound","award","discipline"]}
{"id":"300","dataset":"crossner_science","split":"test","instance":{"id":"300","prompt_labels":"In(O) 1348(O) ,(O) Giovanni(B-scientist) Dondi(I-scientist) built(O) the(O) first(O) known(O) clock(O) driven(O) mechanism(O) which(O) displays(O) the(O) ecliptical(O) position(O) of(O) Moon(B-astronomical object) ,(O) Sun(B-astronomical object) ,(O) Mercury(B-astronomical object) ,(O) Venus(B-astronomical object) ,(O) Mars(B-astronomical object) ,(O) Jupiter(B-astronomical object) and(O) Saturn(B-astronomical object) according(O) to(O) the(O) complicated(O) Ptolemaic(B-theory) planetary(I-theory) theories(I-theory) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, chemical compound, academic journal, chemical element, university, enzyme, astronomical object, person, organization, discipline, theory, scientist, award, country, protein and O.\nSentence: In 1348 , Giovanni Dondi built the first known clock driven mechanism which displays the ecliptical position of Moon , Sun , Mercury , Venus , Mars , Jupiter and Saturn according to the complicated Ptolemaic planetary theories .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1348",",","Giovanni","Dondi","built","the","first","known","clock","driven","mechanism","which","displays","the","ecliptical","position","of","Moon",",","Sun",",","Mercury",",","Venus",",","Mars",",","Jupiter","and","Saturn","according","to","the","complicated","Ptolemaic","planetary","theories","."],"labels":["O","O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","O","O","B-theory","I-theory","I-theory","O"],"target_index":null,"target_label":null},"label_list":["event","location","chemical_compound","academic_journal","chemical_element","university","enzyme","astronomical_object","person","organization","discipline","theory","scientist","award","country","protein"]}
{"id":"301","dataset":"crossner_science","split":"test","instance":{"id":"301","prompt_labels":"H3K27me3(B-chemical compound) is(O) an(O) epigenetic(O) modification(O) to(O) the(O) DNA(O) packaging(O) protein(O) Histone(B-protein) H3(I-protein) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, discipline, chemical element, academic journal, award, enzyme, astronomical object, theory, protein, country, event, scientist, chemical compound, location, university, person and O.\nSentence: H3K27me3 is an epigenetic modification to the DNA packaging protein Histone H3 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["H3K27me3","is","an","epigenetic","modification","to","the","DNA","packaging","protein","Histone","H3","."],"labels":["B-chemical compound","O","O","O","O","O","O","O","O","O","B-protein","I-protein","O"],"target_index":null,"target_label":null},"label_list":["organization","discipline","chemical_element","academic_journal","award","enzyme","astronomical_object","theory","protein","country","event","scientist","chemical_compound","location","university","person"]}
{"id":"302","dataset":"crossner_science","split":"test","instance":{"id":"302","prompt_labels":"In(O) 1994(O) ,(O) Tooze(B-scientist) moved(O) to(O) London(B-location) to(O) set(O) up(O) a(O) lab(O) studying(O) the(O) biogenesis(O) of(O) Secretion(O) at(O) the(O) Imperial(B-organization) Cancer(I-organization) Research(I-organization) Fund(I-organization) ,(O) later(O) the(O) Cancer(B-organization) Research(I-organization) UK(I-organization) London(I-organization) Research(I-organization) Institute(I-organization) ((O) and(O) now(O) part(O) of(O) the(O) Francis(B-organization) Crick(I-organization) Institute(I-organization) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, university, astronomical object, organization, scientist, enzyme, event, chemical compound, discipline, chemical element, theory, location, award, person, protein, country and O.\nSentence: In 1994 , Tooze moved to London to set up a lab studying the biogenesis of Secretion at the Imperial Cancer Research Fund , later the Cancer Research UK London Research Institute ( and now part of the Francis Crick Institute ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1994",",","Tooze","moved","to","London","to","set","up","a","lab","studying","the","biogenesis","of","Secretion","at","the","Imperial","Cancer","Research","Fund",",","later","the","Cancer","Research","UK","London","Research","Institute","(","and","now","part","of","the","Francis","Crick","Institute",")","."],"labels":["O","O","O","B-scientist","O","O","B-location","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","university","astronomical_object","organization","scientist","enzyme","event","chemical_compound","discipline","chemical_element","theory","location","award","person","protein","country"]}
{"id":"303","dataset":"crossner_science","split":"test","instance":{"id":"303","prompt_labels":"He(O) also(O) included(O) perturbations(O) due(O) to(O) the(O) other(O) planets(O) ((O) principally(O) Jupiter(B-astronomical object) and(O) Venus(B-astronomical object) )(O) and(O) also(O) accounted(O) for(O) the(O) more(O) difficult(O) problem(O) of(O) the(O) non-spherical(O) nature(O) of(O) the(O) Earth(B-astronomical object) and(O) Moon(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, chemical compound, discipline, location, country, event, protein, award, chemical element, scientist, organization, person, university, theory, astronomical object, academic journal and O.\nSentence: He also included perturbations due to the other planets ( principally Jupiter and Venus ) and also accounted for the more difficult problem of the non-spherical nature of the Earth and Moon .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","also","included","perturbations","due","to","the","other","planets","(","principally","Jupiter","and","Venus",")","and","also","accounted","for","the","more","difficult","problem","of","the","non-spherical","nature","of","the","Earth","and","Moon","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["enzyme","chemical_compound","discipline","location","country","event","protein","award","chemical_element","scientist","organization","person","university","theory","astronomical_object","academic_journal"]}
{"id":"304","dataset":"crossner_science","split":"test","instance":{"id":"304","prompt_labels":"Among(O) the(O) biggest(O) YEC(O) organizations(O) are(O) Answers(B-organization) in(I-organization) Genesis(I-organization) ,(O) Institute(B-organization) for(I-organization) Creation(I-organization) Research(I-organization) ,(O) and(O) Creation(B-organization) Ministries(I-organization) International(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, discipline, protein, scientist, organization, chemical compound, award, astronomical object, theory, location, chemical element, country, university, person, academic journal, event and O.\nSentence: Among the biggest YEC organizations are Answers in Genesis , Institute for Creation Research , and Creation Ministries International .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Among","the","biggest","YEC","organizations","are","Answers","in","Genesis",",","Institute","for","Creation","Research",",","and","Creation","Ministries","International","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["enzyme","discipline","protein","scientist","organization","chemical_compound","award","astronomical_object","theory","location","chemical_element","country","university","person","academic_journal","event"]}
{"id":"306","dataset":"crossner_science","split":"test","instance":{"id":"306","prompt_labels":"The(O) journal(O) is(O) published(O) by(O) Taylor(B-organization) and(I-organization) Francis(I-organization) on(O) behalf(O) of(O) the(O) American(B-organization) Association(I-organization) of(I-organization) Geographers(I-organization) ,(O) of(O) which(O) it(O) is(O) one(O) of(O) four(O) official(O) journals(O) ,(O) the(O) others(O) being(O) The(B-academic journal) Professional(I-academic journal) Geographer(I-academic journal) ,(O) AAG(B-academic journal) Review(I-academic journal) of(I-academic journal) Books(I-academic journal) ,(O) GeoHumanities(B-academic journal) ,(O) and(O) African(B-academic journal) Geographical(I-academic journal) Review(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, event, academic journal, chemical element, university, chemical compound, discipline, award, protein, country, person, astronomical object, organization, scientist, enzyme, location and O.\nSentence: The journal is published by Taylor and Francis on behalf of the American Association of Geographers , of which it is one of four official journals , the others being The Professional Geographer , AAG Review of Books , GeoHumanities , and African Geographical Review .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","journal","is","published","by","Taylor","and","Francis","on","behalf","of","the","American","Association","of","Geographers",",","of","which","it","is","one","of","four","official","journals",",","the","others","being","The","Professional","Geographer",",","AAG","Review","of","Books",",","GeoHumanities",",","and","African","Geographical","Review","."],"labels":["O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","O","O","B-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["theory","event","academic_journal","chemical_element","university","chemical_compound","discipline","award","protein","country","person","astronomical_object","organization","scientist","enzyme","location"]}
{"id":"308","dataset":"crossner_science","split":"test","instance":{"id":"308","prompt_labels":"Jared(B-scientist) Diamond(I-scientist) describes(O) an(O) Evil(O) Quartet(O) of(O) habitat(O) destruction(O) ,(O) overkill(O) ,(O) introduced(O) species(O) and(O) secondary(O) extinctions.small(O) which(O) has(O) been(O) adopted(O) by(O) major(O) international(O) conservation(O) organizations(O) such(O) as(O) the(O) US(B-organization) Nature(I-organization) Conservancy(I-organization) ,(O) the(O) World(B-organization) Wildlife(I-organization) Fund(I-organization) ,(O) Conservation(B-organization) International(I-organization) and(O) BirdLife(B-organization) International(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, scientist, academic journal, chemical compound, organization, award, astronomical object, theory, country, event, discipline, university, chemical element, enzyme, location, protein and O.\nSentence: Jared Diamond describes an Evil Quartet of habitat destruction , overkill , introduced species and secondary extinctions.small which has been adopted by major international conservation organizations such as the US Nature Conservancy , the World Wildlife Fund , Conservation International and BirdLife International .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Jared","Diamond","describes","an","Evil","Quartet","of","habitat","destruction",",","overkill",",","introduced","species","and","secondary","extinctions.small","which","has","been","adopted","by","major","international","conservation","organizations","such","as","the","US","Nature","Conservancy",",","the","World","Wildlife","Fund",",","Conservation","International","and","BirdLife","International","."],"labels":["B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["person","scientist","academic_journal","chemical_compound","organization","award","astronomical_object","theory","country","event","discipline","university","chemical_element","enzyme","location","protein"]}
{"id":"309","dataset":"crossner_science","split":"test","instance":{"id":"309","prompt_labels":"Typical(O) COC(B-chemical compound) material(O) will(O) have(O) a(O) higher(O) modulus(O) than(O) HDPE(B-chemical compound) and(O) Polypropylene(B-chemical compound) ,(O) similar(O) to(O) Polyethylene(B-chemical compound) terephthalate(I-chemical compound) or(O) PC(B-chemical compound) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, astronomical object, theory, chemical element, organization, event, chemical compound, location, person, discipline, award, enzyme, university, scientist, academic journal, protein and O.\nSentence: Typical COC material will have a higher modulus than HDPE and Polypropylene , similar to Polyethylene terephthalate or PC .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Typical","COC","material","will","have","a","higher","modulus","than","HDPE","and","Polypropylene",",","similar","to","Polyethylene","terephthalate","or","PC","."],"labels":["O","B-chemical compound","O","O","O","O","O","O","O","B-chemical compound","O","B-chemical compound","O","O","O","B-chemical compound","I-chemical compound","O","B-chemical compound","O"],"target_index":null,"target_label":null},"label_list":["country","astronomical_object","theory","chemical_element","organization","event","chemical_compound","location","person","discipline","award","enzyme","university","scientist","academic_journal","protein"]}
{"id":"310","dataset":"crossner_science","split":"test","instance":{"id":"310","prompt_labels":"Red(O) Mars(O) won(O) the(O) BSFA(B-award) Award(I-award) in(O) 1992(O) and(O) Nebula(B-award) Award(I-award) for(I-award) Best(I-award) Novel(I-award) in(O) 1993(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, event, theory, person, location, chemical compound, chemical element, country, enzyme, scientist, discipline, award, university, academic journal, organization, protein and O.\nSentence: Red Mars won the BSFA Award in 1992 and Nebula Award for Best Novel in 1993 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Red","Mars","won","the","BSFA","Award","in","1992","and","Nebula","Award","for","Best","Novel","in","1993","."],"labels":["O","O","O","O","B-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","event","theory","person","location","chemical_compound","chemical_element","country","enzyme","scientist","discipline","award","university","academic_journal","organization","protein"]}
{"id":"311","dataset":"crossner_science","split":"test","instance":{"id":"311","prompt_labels":"1(O) The(O) eight(O) planets(O) are(O) :(O) Mercury(B-astronomical object) ,(O) Venus(B-astronomical object) ,(O) Earth(B-astronomical object) ,(O) Mars(B-astronomical object) ,(O) Jupiter(B-astronomical object) ,(O) Saturn(B-astronomical object) ,(O) Uranus(B-astronomical object) ,(O) and(O) Neptune(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, chemical element, enzyme, astronomical object, location, person, award, university, country, discipline, event, chemical compound, organization, academic journal, theory, scientist and O.\nSentence: 1 The eight planets are : Mercury , Venus , Earth , Mars , Jupiter , Saturn , Uranus , and Neptune .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["1","The","eight","planets","are",":","Mercury",",","Venus",",","Earth",",","Mars",",","Jupiter",",","Saturn",",","Uranus",",","and","Neptune","."],"labels":["O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["protein","chemical_element","enzyme","astronomical_object","location","person","award","university","country","discipline","event","chemical_compound","organization","academic_journal","theory","scientist"]}
{"id":"313","dataset":"crossner_science","split":"test","instance":{"id":"313","prompt_labels":"These(O) include(O) the(O) TRUE(B-organization) Blue(I-organization) Crew(I-organization) ,(O) Antipodean(B-organization) Resistance(I-organization) ,(O) the(O) Australian(B-organization) Defence(I-organization) League(I-organization) ,(O) National(B-organization) Action(I-organization) ((O) Australia(B-country) )(O) ,(O) the(O) Q(B-organization) Society(I-organization) ,(O) Reclaim(B-organization) Australia(I-organization) and(O) the(O) Lads(B-organization) Society(I-organization) ((O) formerly(O) United(B-organization) Patriots(I-organization) Front(I-organization) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, academic journal, country, location, astronomical object, theory, enzyme, university, award, person, discipline, chemical element, event, protein, organization, chemical compound and O.\nSentence: These include the TRUE Blue Crew , Antipodean Resistance , the Australian Defence League , National Action ( Australia ) , the Q Society , Reclaim Australia and the Lads Society ( formerly United Patriots Front ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["These","include","the","TRUE","Blue","Crew",",","Antipodean","Resistance",",","the","Australian","Defence","League",",","National","Action","(","Australia",")",",","the","Q","Society",",","Reclaim","Australia","and","the","Lads","Society","(","formerly","United","Patriots","Front",")","."],"labels":["O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-country","O","O","O","B-organization","I-organization","O","B-organization","I-organization","O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","academic_journal","country","location","astronomical_object","theory","enzyme","university","award","person","discipline","chemical_element","event","protein","organization","chemical_compound"]}
{"id":"314","dataset":"crossner_science","split":"test","instance":{"id":"314","prompt_labels":"He(O) was(O) named(O) one(O) of(O) the(O) most(O) valued(O) reviewers(O) of(O) 2010(O) by(O) the(O) Editors(O) of(O) Elsevier(B-organization) and(O) Nuclear(B-academic journal) Physics(I-academic journal) Other(O) journals(O) for(O) which(O) Dr.(O) Poenaru(B-scientist) peer(O) review(O) ed(O) articles(O) include(O) Physical(B-academic journal) Review(I-academic journal) Letters(I-academic journal) ,(O) Physical(B-academic journal) Review(I-academic journal) C(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Physics(I-academic journal) G(I-academic journal) :(O) Nuclear(B-academic journal) and(I-academic journal) Particle(I-academic journal) Physics(I-academic journal) and(O) Canadian(B-academic journal) Journal(I-academic journal) of(I-academic journal) Physics(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, chemical compound, location, country, discipline, chemical element, award, theory, enzyme, protein, scientist, academic journal, astronomical object, person, university, event and O.\nSentence: He was named one of the most valued reviewers of 2010 by the Editors of Elsevier and Nuclear Physics Other journals for which Dr. Poenaru peer review ed articles include Physical Review Letters , Physical Review C , Journal of Physics G : Nuclear and Particle Physics and Canadian Journal of Physics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","named","one","of","the","most","valued","reviewers","of","2010","by","the","Editors","of","Elsevier","and","Nuclear","Physics","Other","journals","for","which","Dr.","Poenaru","peer","review","ed","articles","include","Physical","Review","Letters",",","Physical","Review","C",",","Journal","of","Physics","G",":","Nuclear","and","Particle","Physics","and","Canadian","Journal","of","Physics","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","B-academic journal","I-academic journal","O","O","O","O","O","B-scientist","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["organization","chemical_compound","location","country","discipline","chemical_element","award","theory","enzyme","protein","scientist","academic_journal","astronomical_object","person","university","event"]}
{"id":"317","dataset":"crossner_science","split":"test","instance":{"id":"317","prompt_labels":"For(O) example(O) ,(O) LHsub2(B-chemical compound) /(I-chemical compound) sub(I-chemical compound) /(I-chemical compound) LOx(I-chemical compound) bipropellant(O) produces(O) higher(O) Isubsp(B-chemical compound) /(I-chemical compound) sub(I-chemical compound) but(O) lower(O) thrust(O) than(O) RP-1(B-chemical compound) /(I-chemical compound) LOx(I-chemical compound) due(O) to(O) the(O) exhaust(O) gases(O) having(O) a(O) lower(O) density(O) and(O) higher(O) velocity(O) ((O) Properties(O) of(O) water(B-chemical compound) vs(O) Carbon(B-chemical compound) dioxide(I-chemical compound) and(O) Hsub2(B-chemical compound) /(I-chemical compound) subO(I-chemical compound) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, enzyme, protein, scientist, country, award, event, chemical element, location, organization, theory, astronomical object, chemical compound, academic journal, person, discipline and O.\nSentence: For example , LHsub2 / sub / LOx bipropellant produces higher Isubsp / sub but lower thrust than RP-1 / LOx due to the exhaust gases having a lower density and higher velocity ( Properties of water vs Carbon dioxide and Hsub2 / subO ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","example",",","LHsub2","/","sub","/","LOx","bipropellant","produces","higher","Isubsp","/","sub","but","lower","thrust","than","RP-1","/","LOx","due","to","the","exhaust","gases","having","a","lower","density","and","higher","velocity","(","Properties","of","water","vs","Carbon","dioxide","and","Hsub2","/","subO",")","."],"labels":["O","O","O","B-chemical compound","I-chemical compound","I-chemical compound","I-chemical compound","I-chemical compound","O","O","O","B-chemical compound","I-chemical compound","I-chemical compound","O","O","O","O","B-chemical compound","I-chemical compound","I-chemical compound","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","O","B-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","I-chemical compound","O","O"],"target_index":null,"target_label":null},"label_list":["university","enzyme","protein","scientist","country","award","event","chemical_element","location","organization","theory","astronomical_object","chemical_compound","academic_journal","person","discipline"]}
{"id":"318","dataset":"crossner_science","split":"test","instance":{"id":"318","prompt_labels":"The(O) term(O) can(O) also(O) be(O) used(O) to(O) describe(O) the(O) motion(O) of(O) a(O) satellite(O) across(O) its(O) parent(O) planet(O) ,(O) for(O) instance(O) one(O) of(O) the(O) Galilean(O) satellites(O) ((O) Io(B-astronomical object) ,(O) Europa(B-astronomical object) ,(O) Ganymede(B-astronomical object) ,(O) Callisto(B-astronomical object) )(O) across(O) Jupiter(B-astronomical object) ,(O) as(O) seen(O) from(O) Earth(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, astronomical object, enzyme, protein, organization, university, person, chemical element, discipline, chemical compound, academic journal, country, theory, scientist, award and O.\nSentence: The term can also be used to describe the motion of a satellite across its parent planet , for instance one of the Galilean satellites ( Io , Europa , Ganymede , Callisto ) across Jupiter , as seen from Earth .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","term","can","also","be","used","to","describe","the","motion","of","a","satellite","across","its","parent","planet",",","for","instance","one","of","the","Galilean","satellites","(","Io",",","Europa",",","Ganymede",",","Callisto",")","across","Jupiter",",","as","seen","from","Earth","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","B-astronomical object","O","O","O","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["event","location","astronomical_object","enzyme","protein","organization","university","person","chemical_element","discipline","chemical_compound","academic_journal","country","theory","scientist","award"]}
{"id":"320","dataset":"crossner_science","split":"test","instance":{"id":"320","prompt_labels":"It(O) was(O) discovered(O) during(O) the(O) Palomar-Leiden(O) survey(O) on(O) 24(O) September(O) 1960(O) ,(O) by(O) Ingrid(B-scientist) van(I-scientist) Houten-Groeneveld(I-scientist) and(O) Cornelis(B-scientist) van(I-scientist) Houten(I-scientist) at(O) Leiden(B-location) ,(O) and(O) Tom(B-scientist) Gehrels(I-scientist) at(O) Palomar(B-location) Observatory(I-location) in(O) California(B-location) ,(O) United(B-country) States(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, academic journal, event, astronomical object, protein, person, scientist, country, university, award, chemical compound, location, discipline, chemical element, organization, enzyme and O.\nSentence: It was discovered during the Palomar-Leiden survey on 24 September 1960 , by Ingrid van Houten-Groeneveld and Cornelis van Houten at Leiden , and Tom Gehrels at Palomar Observatory in California , United States .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","discovered","during","the","Palomar-Leiden","survey","on","24","September","1960",",","by","Ingrid","van","Houten-Groeneveld","and","Cornelis","van","Houten","at","Leiden",",","and","Tom","Gehrels","at","Palomar","Observatory","in","California",",","United","States","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","B-location","O","O","B-scientist","I-scientist","O","B-location","I-location","O","B-location","O","B-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["theory","academic_journal","event","astronomical_object","protein","person","scientist","country","university","award","chemical_compound","location","discipline","chemical_element","organization","enzyme"]}
{"id":"321","dataset":"crossner_science","split":"test","instance":{"id":"321","prompt_labels":"The(O) British(O) English(O) spelling(O) of(O) this(O) compound(O) is(O) hydrogen(B-chemical compound) sulphide(I-chemical compound) ,(O) but(O) this(O) spelling(O) is(O) not(O) recommended(O) by(O) the(O) International(B-organization) Union(I-organization) of(I-organization) Pure(I-organization) and(I-organization) Applied(I-organization) Chemistry(I-organization) ((O) IUPAC(B-organization) )(O) or(O) the(O) Royal(B-organization) Society(I-organization) of(I-organization) Chemistry(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, event, person, award, theory, protein, location, university, enzyme, chemical element, chemical compound, discipline, scientist, organization, country, astronomical object and O.\nSentence: The British English spelling of this compound is hydrogen sulphide , but this spelling is not recommended by the International Union of Pure and Applied Chemistry ( IUPAC ) or the Royal Society of Chemistry .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","British","English","spelling","of","this","compound","is","hydrogen","sulphide",",","but","this","spelling","is","not","recommended","by","the","International","Union","of","Pure","and","Applied","Chemistry","(","IUPAC",")","or","the","Royal","Society","of","Chemistry","."],"labels":["O","O","O","O","O","O","O","O","B-chemical compound","I-chemical compound","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","event","person","award","theory","protein","location","university","enzyme","chemical_element","chemical_compound","discipline","scientist","organization","country","astronomical_object"]}
{"id":"327","dataset":"crossner_science","split":"test","instance":{"id":"327","prompt_labels":"Sakharov(B-astronomical object) was(O) discovered(O) on(O) 24(O) September(O) 1960(O) ,(O) by(O) the(O) Dutch(O) astronomers(O) Ingrid(B-scientist) van(I-scientist) Houten-Groeneveld(I-scientist) and(O) Cornelis(B-scientist) van(I-scientist) Houten(I-scientist) ,(O) on(O) photographic(O) plates(O) taken(O) by(O) Dutch-American(O) astronomer(O) Tom(B-person) Gehrels(I-person) at(O) the(O) U.S.(B-location) Palomar(I-location) Observatory(I-location) in(O) California(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, chemical compound, academic journal, location, theory, organization, event, astronomical object, protein, university, award, person, enzyme, discipline, chemical element, country and O.\nSentence: Sakharov was discovered on 24 September 1960 , by the Dutch astronomers Ingrid van Houten-Groeneveld and Cornelis van Houten , on photographic plates taken by Dutch-American astronomer Tom Gehrels at the U.S. Palomar Observatory in California .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Sakharov","was","discovered","on","24","September","1960",",","by","the","Dutch","astronomers","Ingrid","van","Houten-Groeneveld","and","Cornelis","van","Houten",",","on","photographic","plates","taken","by","Dutch-American","astronomer","Tom","Gehrels","at","the","U.S.","Palomar","Observatory","in","California","."],"labels":["B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","O","O","O","O","O","O","O","B-person","I-person","O","O","B-location","I-location","I-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["scientist","chemical_compound","academic_journal","location","theory","organization","event","astronomical_object","protein","university","award","person","enzyme","discipline","chemical_element","country"]}
{"id":"329","dataset":"crossner_science","split":"test","instance":{"id":"329","prompt_labels":";(O) the(O) most(O) exclusive(O) sports(O) clubs(O) :(O) CCI(B-organization) ,(O) The(B-organization) Willingdon(I-organization) Sports(I-organization) Club(I-organization) as(O) well(O) as(O) Bombay(B-organization) Gymkhana(I-organization) and(O) the(O) most(O) expensive(O) hospitals(O) :(O) Breach(B-location) Candy(I-location) Hospital(I-location) ,(O) Bombay(B-location) Hospital(I-location) ,(O) Jaslok(B-location) Hospital(I-location) and(O) Hurkisondas(B-organization) Hospital(I-organization) ;(O) in(O) the(O) nation(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, astronomical object, academic journal, chemical compound, scientist, country, person, organization, protein, chemical element, enzyme, university, event, theory, discipline and O.\nSentence: ; the most exclusive sports clubs : CCI , The Willingdon Sports Club as well as Bombay Gymkhana and the most expensive hospitals : Breach Candy Hospital , Bombay Hospital , Jaslok Hospital and Hurkisondas Hospital ; in the nation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":[";","the","most","exclusive","sports","clubs",":","CCI",",","The","Willingdon","Sports","Club","as","well","as","Bombay","Gymkhana","and","the","most","expensive","hospitals",":","Breach","Candy","Hospital",",","Bombay","Hospital",",","Jaslok","Hospital","and","Hurkisondas","Hospital",";","in","the","nation","."],"labels":["O","O","O","O","O","O","O","B-organization","O","B-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","O","O","O","O","O","O","B-location","I-location","I-location","O","B-location","I-location","O","B-location","I-location","O","B-organization","I-organization","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","location","astronomical_object","academic_journal","chemical_compound","scientist","country","person","organization","protein","chemical_element","enzyme","university","event","theory","discipline"]}
{"id":"331","dataset":"crossner_science","split":"test","instance":{"id":"331","prompt_labels":"In(O) season(O) three(O) ,(O) two(O) actors(O) were(O) promoted(O) from(O) recurring(O) to(O) starring(O) roles(O) :(O) Henry(B-person) Ian(I-person) Cusick(I-person) as(O) former(O) Scottish(O) soldier(O) Desmond(B-person) Hume(I-person) ;(O) and(O) Michael(B-person) Emerson(I-person) as(O) the(O) manipulative(O) leader(O) of(O) the(O) Others(O) ,(O) Ben(B-person) Linus(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, protein, organization, country, theory, astronomical object, discipline, academic journal, event, scientist, university, chemical compound, chemical element, enzyme, award and O.\nSentence: In season three , two actors were promoted from recurring to starring roles : Henry Ian Cusick as former Scottish soldier Desmond Hume ; and Michael Emerson as the manipulative leader of the Others , Ben Linus .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","season","three",",","two","actors","were","promoted","from","recurring","to","starring","roles",":","Henry","Ian","Cusick","as","former","Scottish","soldier","Desmond","Hume",";","and","Michael","Emerson","as","the","manipulative","leader","of","the","Others",",","Ben","Linus","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","I-person","O","O","O","O","B-person","I-person","O","O","B-person","I-person","O","O","O","O","O","O","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["person","location","protein","organization","country","theory","astronomical_object","discipline","academic_journal","event","scientist","university","chemical_compound","chemical_element","enzyme","award"]}
{"id":"333","dataset":"crossner_science","split":"test","instance":{"id":"333","prompt_labels":"Saha(B-person) also(O) invented(O) an(O) instrument(O) to(O) measure(O) the(O) weight(O) and(O) pressure(O) of(O) solar(O) rays(O) and(O) helped(O) to(O) build(O) several(O) scientific(O) institutions(O) ,(O) such(O) as(O) the(O) Physics(B-organization) Department(I-organization) in(O) University(B-university) of(I-university) Allahabad(I-university) and(O) the(O) Saha(B-organization) Institute(I-organization) of(I-organization) Nuclear(I-organization) Physics(I-organization) in(O) Calcutta(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, country, event, organization, astronomical object, award, chemical compound, scientist, person, academic journal, discipline, chemical element, university, enzyme, theory, location and O.\nSentence: Saha also invented an instrument to measure the weight and pressure of solar rays and helped to build several scientific institutions , such as the Physics Department in University of Allahabad and the Saha Institute of Nuclear Physics in Calcutta .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Saha","also","invented","an","instrument","to","measure","the","weight","and","pressure","of","solar","rays","and","helped","to","build","several","scientific","institutions",",","such","as","the","Physics","Department","in","University","of","Allahabad","and","the","Saha","Institute","of","Nuclear","Physics","in","Calcutta","."],"labels":["B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","B-university","I-university","I-university","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["protein","country","event","organization","astronomical_object","award","chemical_compound","scientist","person","academic_journal","discipline","chemical_element","university","enzyme","theory","location"]}
{"id":"336","dataset":"crossner_science","split":"test","instance":{"id":"336","prompt_labels":"The(O) conclusion(O) of(O) the(O) scientists(O) was(O) that(O) further(O) effort(O) was(O) needed(O) in(O) to(O) improve(O) the(O) precision(O) and(O) efficiency(O) of(O) CRISPR(O) gene(O) editing(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, university, chemical compound, country, astronomical object, academic journal, enzyme, award, organization, person, location, theory, chemical element, protein, scientist, discipline and O.\nSentence: The conclusion of the scientists was that further effort was needed in to improve the precision and efficiency of CRISPR gene editing .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","conclusion","of","the","scientists","was","that","further","effort","was","needed","in","to","improve","the","precision","and","efficiency","of","CRISPR","gene","editing","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","university","chemical_compound","country","astronomical_object","academic_journal","enzyme","award","organization","person","location","theory","chemical_element","protein","scientist","discipline"]}
{"id":"337","dataset":"crossner_science","split":"test","instance":{"id":"337","prompt_labels":"In(O) the(O) second(O) step(O) react(O) Phosphoribosylamine(B-chemical compound) ,(O) glycine(B-chemical compound) and(O) ATP(B-chemical compound) to(O) create(O) Glycineamide(B-chemical compound) ribonucleotide(I-chemical compound) ,(O) ADP(B-chemical compound) ,(O) and(O) pyrophosphate(B-chemical compound) -(O) catalyzed(O) by(O) phosphoribosylamine(B-enzyme) -(I-enzyme) glycine(I-enzyme) ligase(I-enzyme) ((O) GAR(B-enzyme) synthetase(I-enzyme) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, protein, university, discipline, person, event, academic journal, country, location, organization, award, chemical compound, enzyme, theory, scientist, astronomical object and O.\nSentence: In the second step react Phosphoribosylamine , glycine and ATP to create Glycineamide ribonucleotide , ADP , and pyrophosphate - catalyzed by phosphoribosylamine - glycine ligase ( GAR synthetase ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","second","step","react","Phosphoribosylamine",",","glycine","and","ATP","to","create","Glycineamide","ribonucleotide",",","ADP",",","and","pyrophosphate","-","catalyzed","by","phosphoribosylamine","-","glycine","ligase","(","GAR","synthetase",")","."],"labels":["O","O","O","O","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","O","B-chemical compound","I-chemical compound","O","B-chemical compound","O","O","B-chemical compound","O","O","O","B-enzyme","I-enzyme","I-enzyme","I-enzyme","O","B-enzyme","I-enzyme","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","protein","university","discipline","person","event","academic_journal","country","location","organization","award","chemical_compound","enzyme","theory","scientist","astronomical_object"]}
{"id":"338","dataset":"crossner_science","split":"test","instance":{"id":"338","prompt_labels":"Schweickart(B-scientist) is(O) a(O) fellow(B-award) of(I-award) the(I-award) American(I-award) Astronautical(I-award) Society(I-award) and(O) the(O) International(B-organization) Academy(I-organization) of(I-organization) Astronautics(I-organization) ,(O) and(O) an(O) associate(O) fellow(B-award) of(I-award) the(I-award) American(I-award) Institute(I-award) of(I-award) Aeronautics(I-award) and(I-award) Astronautics(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, location, astronomical object, organization, protein, person, chemical compound, enzyme, discipline, academic journal, event, chemical element, award, university, country, theory and O.\nSentence: Schweickart is a fellow of the American Astronautical Society and the International Academy of Astronautics , and an associate fellow of the American Institute of Aeronautics and Astronautics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Schweickart","is","a","fellow","of","the","American","Astronautical","Society","and","the","International","Academy","of","Astronautics",",","and","an","associate","fellow","of","the","American","Institute","of","Aeronautics","and","Astronautics","."],"labels":["B-scientist","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["scientist","location","astronomical_object","organization","protein","person","chemical_compound","enzyme","discipline","academic_journal","event","chemical_element","award","university","country","theory"]}
{"id":"339","dataset":"crossner_science","split":"test","instance":{"id":"339","prompt_labels":"The(O) scientists(O) used(O) tripronuclear(O) ((O) 3PN(O) )(O) zygotes(O) ,(O) zygotes(O) fertilized(O) by(O) two(O) sperm(O) and(O) therefore(O) non-viable(O) ,(O) to(O) investigate(O) CRISPR(O) -mediated(O) gene(O) editing(O) in(O) human(O) cells(O) ,(O) something(O) that(O) had(O) never(O) been(O) attempted(O) before(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, chemical compound, scientist, theory, organization, award, location, university, discipline, astronomical object, event, protein, chemical element, country, person, academic journal and O.\nSentence: The scientists used tripronuclear ( 3PN ) zygotes , zygotes fertilized by two sperm and therefore non-viable , to investigate CRISPR -mediated gene editing in human cells , something that had never been attempted before .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","scientists","used","tripronuclear","(","3PN",")","zygotes",",","zygotes","fertilized","by","two","sperm","and","therefore","non-viable",",","to","investigate","CRISPR","-mediated","gene","editing","in","human","cells",",","something","that","had","never","been","attempted","before","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["enzyme","chemical_compound","scientist","theory","organization","award","location","university","discipline","astronomical_object","event","protein","chemical_element","country","person","academic_journal"]}
{"id":"341","dataset":"crossner_science","split":"test","instance":{"id":"341","prompt_labels":"Soon(O) he(O) established(O) closer(O) contact(O) to(O) Wilhelm(B-scientist) Wien(I-scientist) ,(O) Arnold(B-scientist) Sommerfeld(I-scientist) ,(O) Johannes(B-scientist) Stark(I-scientist) ,(O) and(O) Albert(B-scientist) Einstein(I-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, chemical element, theory, country, chemical compound, event, astronomical object, organization, protein, award, university, scientist, location, discipline, person, academic journal and O.\nSentence: Soon he established closer contact to Wilhelm Wien , Arnold Sommerfeld , Johannes Stark , and Albert Einstein .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Soon","he","established","closer","contact","to","Wilhelm","Wien",",","Arnold","Sommerfeld",",","Johannes","Stark",",","and","Albert","Einstein","."],"labels":["O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","O"],"target_index":null,"target_label":null},"label_list":["enzyme","chemical_element","theory","country","chemical_compound","event","astronomical_object","organization","protein","award","university","scientist","location","discipline","person","academic_journal"]}
{"id":"344","dataset":"crossner_science","split":"test","instance":{"id":"344","prompt_labels":"Dyson(B-scientist) presented(O) his(O) observations(O) of(O) the(O) solar(B-event) eclipse(I-event) of(I-event) May(I-event) 29(I-event) ,(I-event) 1919(I-event) to(O) a(O) joint(O) meeting(O) of(O) the(O) Royal(B-organization) Society(I-organization) and(O) Royal(B-organization) Astronomical(I-organization) Society(I-organization) on(O) 6(O) November(O) 1919(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, protein, country, event, organization, academic journal, university, theory, person, enzyme, award, chemical compound, discipline, chemical element, location, astronomical object and O.\nSentence: Dyson presented his observations of the solar eclipse of May 29 , 1919 to a joint meeting of the Royal Society and Royal Astronomical Society on 6 November 1919 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Dyson","presented","his","observations","of","the","solar","eclipse","of","May","29",",","1919","to","a","joint","meeting","of","the","Royal","Society","and","Royal","Astronomical","Society","on","6","November","1919","."],"labels":["B-scientist","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","I-event","I-event","O","O","O","O","O","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","protein","country","event","organization","academic_journal","university","theory","person","enzyme","award","chemical_compound","discipline","chemical_element","location","astronomical_object"]}
{"id":"345","dataset":"crossner_science","split":"test","instance":{"id":"345","prompt_labels":"The(O) discovery(O) of(O) the(O) asteroid(O) 3(B-astronomical object) Juno(I-astronomical object) by(O) Karl(B-scientist) Ludwig(I-scientist) Harding(I-scientist) and(O) 4(B-astronomical object) Vesta(I-astronomical object) by(O) Olbers(B-scientist) ,(O) buttressed(O) his(O) hypothesis(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, discipline, chemical compound, country, academic journal, event, scientist, protein, organization, university, theory, chemical element, astronomical object, person, enzyme, location and O.\nSentence: The discovery of the asteroid 3 Juno by Karl Ludwig Harding and 4 Vesta by Olbers , buttressed his hypothesis .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","discovery","of","the","asteroid","3","Juno","by","Karl","Ludwig","Harding","and","4","Vesta","by","Olbers",",","buttressed","his","hypothesis","."],"labels":["O","O","O","O","O","B-astronomical object","I-astronomical object","O","B-scientist","I-scientist","I-scientist","O","B-astronomical object","I-astronomical object","O","B-scientist","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","discipline","chemical_compound","country","academic_journal","event","scientist","protein","organization","university","theory","chemical_element","astronomical_object","person","enzyme","location"]}
{"id":"346","dataset":"crossner_science","split":"test","instance":{"id":"346","prompt_labels":"During(O) their(O) maturation(O) in(O) the(O) thymus(O) ,(O) they(O) undergo(O) a(O) process(O) called(O) V(O) ((O) D(O) )(O) J(O) recombination(O) which(O) conducts(O) the(O) development(O) of(O) T-cell(B-protein) receptor(I-protein) ((O) TCR(B-protein) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, scientist, enzyme, theory, astronomical object, organization, location, chemical compound, protein, university, event, country, academic journal, chemical element, award, discipline and O.\nSentence: During their maturation in the thymus , they undergo a process called V ( D ) J recombination which conducts the development of T-cell receptor ( TCR ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","their","maturation","in","the","thymus",",","they","undergo","a","process","called","V","(","D",")","J","recombination","which","conducts","the","development","of","T-cell","receptor","(","TCR",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-protein","I-protein","O","B-protein","O","O"],"target_index":null,"target_label":null},"label_list":["person","scientist","enzyme","theory","astronomical_object","organization","location","chemical_compound","protein","university","event","country","academic_journal","chemical_element","award","discipline"]}
{"id":"348","dataset":"crossner_science","split":"test","instance":{"id":"348","prompt_labels":"LINE1(O) transposable(O) elements(O) have(O) been(O) identified(O) as(O) targets(O) for(O) ionizing(O) radiation(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, chemical element, enzyme, organization, theory, scientist, discipline, person, country, location, chemical compound, university, event, astronomical object, protein, academic journal and O.\nSentence: LINE1 transposable elements have been identified as targets for ionizing radiation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["LINE1","transposable","elements","have","been","identified","as","targets","for","ionizing","radiation","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","chemical_element","enzyme","organization","theory","scientist","discipline","person","country","location","chemical_compound","university","event","astronomical_object","protein","academic_journal"]}
{"id":"349","dataset":"crossner_science","split":"test","instance":{"id":"349","prompt_labels":"Staudinger(B-scientist) ,(O) who(O) initially(O) wanted(O) to(O) become(O) a(O) botanist(O) ,(O) studied(O) chemistry(B-discipline) at(O) the(O) Martin(B-university) Luther(I-university) University(I-university) of(I-university) Halle-Wittenberg(I-university) ,(O) at(O) the(O) Technische(B-university) Universitt(I-university) Darmstadt(I-university) and(O) at(O) the(O) Ludwig(B-university) Maximilian(I-university) University(I-university) of(I-university) Munich(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, university, location, academic journal, enzyme, astronomical object, discipline, theory, protein, chemical element, country, person, organization, event, chemical compound, scientist and O.\nSentence: Staudinger , who initially wanted to become a botanist , studied chemistry at the Martin Luther University of Halle-Wittenberg , at the Technische Universitt Darmstadt and at the Ludwig Maximilian University of Munich .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Staudinger",",","who","initially","wanted","to","become","a","botanist",",","studied","chemistry","at","the","Martin","Luther","University","of","Halle-Wittenberg",",","at","the","Technische","Universitt","Darmstadt","and","at","the","Ludwig","Maximilian","University","of","Munich","."],"labels":["B-scientist","O","O","O","O","O","O","O","O","O","O","B-discipline","O","O","B-university","I-university","I-university","I-university","I-university","O","O","O","B-university","I-university","I-university","O","O","O","B-university","I-university","I-university","I-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["award","university","location","academic_journal","enzyme","astronomical_object","discipline","theory","protein","chemical_element","country","person","organization","event","chemical_compound","scientist"]}
{"id":"350","dataset":"crossner_science","split":"test","instance":{"id":"350","prompt_labels":"Chinese(B-academic journal) Physics(I-academic journal) Letters(I-academic journal) is(O) a(O) part(O) of(O) a(O) small(O) group(O) of(O) four(O) journals(O) from(O) the(O) Chinese(B-organization) Physical(I-organization) Society(I-organization) ,(O) the(O) other(O) three(O) are(O) :(O) Communications(B-academic journal) in(I-academic journal) Theoretical(I-academic journal) Physics(I-academic journal) ((O) in(O) English(O) ,(O) subtitled(O) Chinese(B-academic journal) Physics(I-academic journal) A(I-academic journal) )(O) ,(O) Chinese(B-academic journal) Physics(I-academic journal) B(I-academic journal) ((O) in(O) English(O) )(O) ,(O) and(O) Chinese(B-academic journal) Physics(I-academic journal) C(I-academic journal) ((O) in(O) English(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, award, protein, scientist, discipline, person, theory, organization, enzyme, astronomical object, country, location, chemical compound, event, chemical element, university and O.\nSentence: Chinese Physics Letters is a part of a small group of four journals from the Chinese Physical Society , the other three are : Communications in Theoretical Physics ( in English , subtitled Chinese Physics A ) , Chinese Physics B ( in English ) , and Chinese Physics C ( in English ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Chinese","Physics","Letters","is","a","part","of","a","small","group","of","four","journals","from","the","Chinese","Physical","Society",",","the","other","three","are",":","Communications","in","Theoretical","Physics","(","in","English",",","subtitled","Chinese","Physics","A",")",",","Chinese","Physics","B","(","in","English",")",",","and","Chinese","Physics","C","(","in","English",")","."],"labels":["B-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","O","B-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","award","protein","scientist","discipline","person","theory","organization","enzyme","astronomical_object","country","location","chemical_compound","event","chemical_element","university"]}
{"id":"354","dataset":"crossner_science","split":"test","instance":{"id":"354","prompt_labels":"Fraser(B-person) has(O) been(O) an(O) editor(O) for(O) journals(O) mBio(B-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Bacteriology(I-academic journal) ,(O) Microbial(B-academic journal) Genomics(I-academic journal) ((O) journal(O) )(O) ,(O) Molecular(B-academic journal) Case(I-academic journal) Studies(I-academic journal) ,(O) and(O) DNA(B-academic journal) and(I-academic journal) Cell(I-academic journal) Biology(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, country, scientist, chemical compound, organization, academic journal, protein, chemical element, person, university, discipline, award, astronomical object, event, location, enzyme and O.\nSentence: Fraser has been an editor for journals mBio , Journal of Bacteriology , Microbial Genomics ( journal ) , Molecular Case Studies , and DNA and Cell Biology .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Fraser","has","been","an","editor","for","journals","mBio",",","Journal","of","Bacteriology",",","Microbial","Genomics","(","journal",")",",","Molecular","Case","Studies",",","and","DNA","and","Cell","Biology","."],"labels":["B-person","O","O","O","O","O","O","B-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["theory","country","scientist","chemical_compound","organization","academic_journal","protein","chemical_element","person","university","discipline","award","astronomical_object","event","location","enzyme"]}
{"id":"355","dataset":"crossner_science","split":"test","instance":{"id":"355","prompt_labels":"Aluminium(B-chemical compound) hydroxide(I-chemical compound) ,(O) ferrous(B-chemical compound) hydroxide(I-chemical compound) ,(O) Hydroxide(B-chemical compound) ,(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, academic journal, chemical element, scientist, location, chemical compound, organization, award, person, event, protein, enzyme, university, astronomical object, country, discipline and O.\nSentence: Aluminium hydroxide , ferrous hydroxide , Hydroxide ,","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Aluminium","hydroxide",",","ferrous","hydroxide",",","Hydroxide",","],"labels":["B-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","O","B-chemical compound","O"],"target_index":null,"target_label":null},"label_list":["theory","academic_journal","chemical_element","scientist","location","chemical_compound","organization","award","person","event","protein","enzyme","university","astronomical_object","country","discipline"]}
{"id":"357","dataset":"crossner_science","split":"test","instance":{"id":"357","prompt_labels":"Juno(B-astronomical object) was(O) originally(O) considered(O) a(O) planet(O) ,(O) along(O) with(O) 1(B-astronomical object) Ceres(I-astronomical object) ,(O) 2(B-astronomical object) Pallas(I-astronomical object) ,(O) and(O) 4(B-astronomical object) Vesta(I-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, chemical compound, event, award, theory, scientist, location, academic journal, protein, organization, astronomical object, discipline, university, enzyme, chemical element, person and O.\nSentence: Juno was originally considered a planet , along with 1 Ceres , 2 Pallas , and 4 Vesta .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Juno","was","originally","considered","a","planet",",","along","with","1","Ceres",",","2","Pallas",",","and","4","Vesta","."],"labels":["B-astronomical object","O","O","O","O","O","O","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","B-astronomical object","I-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["country","chemical_compound","event","award","theory","scientist","location","academic_journal","protein","organization","astronomical_object","discipline","university","enzyme","chemical_element","person"]}
{"id":"358","dataset":"crossner_science","split":"test","instance":{"id":"358","prompt_labels":"Henry(B-scientist) Taube(B-scientist) ,(O) Ph.D(O) ,(O) M.Sc(O) ,(O) B.Sc.(O) ,(O) Fellow(B-award) of(I-award) the(I-award) Royal(I-award) Society(I-award) of(I-award) Canada(I-award) ((O) November(O) 30(O) ,(O) 1915(O) &(O) amp(O) ;(O) ndash(O) ;(O) November(O) 16(O) ,(O) 2005(O) )(O) was(O) a(O) Canadian(O) -born(O) American(O) chemist(O) noted(O) for(O) having(O) been(O) awarded(O) the(O) 1983(O) Nobel(B-award) Prize(I-award) in(I-award) Chemistry(I-award) for(O) his(O) work(O) in(O) the(O) mechanisms(O) of(O) electron-transfer(O) reactions(O) ,(O) especially(O) in(O) metal(O) complexes(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, organization, academic journal, scientist, event, astronomical object, theory, country, enzyme, location, discipline, university, person, award, chemical compound, protein and O.\nSentence: Henry Taube , Ph.D , M.Sc , B.Sc. , Fellow of the Royal Society of Canada ( November 30 , 1915 & amp ; ndash ; November 16 , 2005 ) was a Canadian -born American chemist noted for having been awarded the 1983 Nobel Prize in Chemistry for his work in the mechanisms of electron-transfer reactions , especially in metal complexes .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Henry","Taube",",","Ph.D",",","M.Sc",",","B.Sc.",",","Fellow","of","the","Royal","Society","of","Canada","(","November","30",",","1915","&","amp",";","ndash",";","November","16",",","2005",")","was","a","Canadian","-born","American","chemist","noted","for","having","been","awarded","the","1983","Nobel","Prize","in","Chemistry","for","his","work","in","the","mechanisms","of","electron-transfer","reactions",",","especially","in","metal","complexes","."],"labels":["B-scientist","B-scientist","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","organization","academic_journal","scientist","event","astronomical_object","theory","country","enzyme","location","discipline","university","person","award","chemical_compound","protein"]}
{"id":"360","dataset":"crossner_science","split":"test","instance":{"id":"360","prompt_labels":"In(O) recognition(O) of(O) the(O) vast(O) amount(O) of(O) scientific(O) information(O) amassed(O) by(O) both(O) rovers(O) ,(O) two(O) asteroid(O) s(O) have(O) been(O) named(O) in(O) their(O) honor(O) :(O) 37452(B-astronomical object) Spirit(I-astronomical object) and(O) 39382(B-astronomical object) Opportunity(I-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, academic journal, event, chemical element, country, chemical compound, theory, award, person, astronomical object, university, location, protein, organization, enzyme, discipline and O.\nSentence: In recognition of the vast amount of scientific information amassed by both rovers , two asteroid s have been named in their honor : 37452 Spirit and 39382 Opportunity .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","recognition","of","the","vast","amount","of","scientific","information","amassed","by","both","rovers",",","two","asteroid","s","have","been","named","in","their","honor",":","37452","Spirit","and","39382","Opportunity","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["scientist","academic_journal","event","chemical_element","country","chemical_compound","theory","award","person","astronomical_object","university","location","protein","organization","enzyme","discipline"]}
{"id":"362","dataset":"crossner_science","split":"test","instance":{"id":"362","prompt_labels":"Under(O) typical(O) dark(O) sky(O) conditions(O) Uranus(B-astronomical object) ((O) magnitude(O) +(O) 5.8(O) )(O) can(O) be(O) seen(O) as(O) well(O) with(O) averted(O) vision(O) ,(O) as(O) can(O) the(O) asteroid(O) 4(B-astronomical object) Vesta(I-astronomical object) at(O) its(O) brighter(O) oppositions(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, enzyme, country, scientist, protein, astronomical object, location, theory, organization, chemical element, person, event, chemical compound, university, discipline, award and O.\nSentence: Under typical dark sky conditions Uranus ( magnitude + 5.8 ) can be seen as well with averted vision , as can the asteroid 4 Vesta at its brighter oppositions .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Under","typical","dark","sky","conditions","Uranus","(","magnitude","+","5.8",")","can","be","seen","as","well","with","averted","vision",",","as","can","the","asteroid","4","Vesta","at","its","brighter","oppositions","."],"labels":["O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","I-astronomical object","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","enzyme","country","scientist","protein","astronomical_object","location","theory","organization","chemical_element","person","event","chemical_compound","university","discipline","award"]}
{"id":"363","dataset":"crossner_science","split":"test","instance":{"id":"363","prompt_labels":"This(O) methylation(O) causes(O) other(O) factors(O) like(O) Histone(B-enzyme) deacetylase(I-enzyme) ((O) HDACs(B-enzyme) )(O) to(O) bind(O) to(O) the(O) chromosome(O) and(O) propagate(O) heterochromatin(O) formation(O) ,(O) even(O) into(O) active(O) gene(O) regions(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, chemical element, country, chemical compound, enzyme, location, protein, academic journal, organization, discipline, award, event, astronomical object, person, university, scientist and O.\nSentence: This methylation causes other factors like Histone deacetylase ( HDACs ) to bind to the chromosome and propagate heterochromatin formation , even into active gene regions .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","methylation","causes","other","factors","like","Histone","deacetylase","(","HDACs",")","to","bind","to","the","chromosome","and","propagate","heterochromatin","formation",",","even","into","active","gene","regions","."],"labels":["O","O","O","O","O","O","B-enzyme","I-enzyme","O","B-enzyme","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["theory","chemical_element","country","chemical_compound","enzyme","location","protein","academic_journal","organization","discipline","award","event","astronomical_object","person","university","scientist"]}
{"id":"366","dataset":"crossner_science","split":"test","instance":{"id":"366","prompt_labels":"It(O) works(O) as(O) a(O) transmitter(O) from(O) and(O) to(O) the(O) tight(O) junction(O) ,(O) because(O) of(O) its(O) association(O) with(O) signaling(O) molecules(O) ((O) Phosphoinositide(B-enzyme) 3-kinase(I-enzyme) ,(O) Protein(B-enzyme) kinase(I-enzyme) C(I-enzyme) ,(O) YES(B-enzyme) ,(O) Protein(B-enzyme) phosphatase(I-enzyme) 2(I-enzyme) ,(I-enzyme) 1(I-enzyme) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, location, protein, theory, chemical compound, discipline, person, event, academic journal, chemical element, organization, award, enzyme, university, country, astronomical object and O.\nSentence: It works as a transmitter from and to the tight junction , because of its association with signaling molecules ( Phosphoinositide 3-kinase , Protein kinase C , YES , Protein phosphatase 2 , 1 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","works","as","a","transmitter","from","and","to","the","tight","junction",",","because","of","its","association","with","signaling","molecules","(","Phosphoinositide","3-kinase",",","Protein","kinase","C",",","YES",",","Protein","phosphatase","2",",","1",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-enzyme","I-enzyme","O","B-enzyme","I-enzyme","I-enzyme","O","B-enzyme","O","B-enzyme","I-enzyme","I-enzyme","I-enzyme","I-enzyme","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","location","protein","theory","chemical_compound","discipline","person","event","academic_journal","chemical_element","organization","award","enzyme","university","country","astronomical_object"]}
{"id":"367","dataset":"crossner_science","split":"test","instance":{"id":"367","prompt_labels":"Other(O) enzymes(O) such(O) as(O) ((O) Transglutaminase(B-enzyme) )(O) control(O) chromatin(O) remodeling(O) through(O) proteins(O) such(O) as(O) sirtuin1(B-protein) ((O) SIRT1(B-protein) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, organization, location, discipline, award, university, theory, person, chemical element, event, country, protein, astronomical object, chemical compound, academic journal, enzyme and O.\nSentence: Other enzymes such as ( Transglutaminase ) control chromatin remodeling through proteins such as sirtuin1 ( SIRT1 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","enzymes","such","as","(","Transglutaminase",")","control","chromatin","remodeling","through","proteins","such","as","sirtuin1","(","SIRT1",")","."],"labels":["O","O","O","O","O","B-enzyme","O","O","O","O","O","O","O","O","B-protein","O","B-protein","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","organization","location","discipline","award","university","theory","person","chemical_element","event","country","protein","astronomical_object","chemical_compound","academic_journal","enzyme"]}
{"id":"368","dataset":"crossner_science","split":"test","instance":{"id":"368","prompt_labels":"However(O) ,(O) various(O) histone(O) modifications(O) are(O) placed(O) by(O) epigenetic(O) modifiers(O) such(O) as(O) DNA(B-protein) methyltransferase(I-protein) in(O) neurons(O) and(O) these(O) marks(O) regulate(O) gene(O) expression(O) throughout(O) the(O) life(O) span(O) of(O) the(O) neuron(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, astronomical object, enzyme, academic journal, award, chemical compound, protein, university, discipline, theory, organization, event, person, chemical element, location, scientist and O.\nSentence: However , various histone modifications are placed by epigenetic modifiers such as DNA methyltransferase in neurons and these marks regulate gene expression throughout the life span of the neuron .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["However",",","various","histone","modifications","are","placed","by","epigenetic","modifiers","such","as","DNA","methyltransferase","in","neurons","and","these","marks","regulate","gene","expression","throughout","the","life","span","of","the","neuron","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-protein","I-protein","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","astronomical_object","enzyme","academic_journal","award","chemical_compound","protein","university","discipline","theory","organization","event","person","chemical_element","location","scientist"]}
{"id":"370","dataset":"crossner_science","split":"test","instance":{"id":"370","prompt_labels":"She(O) also(O) played(O) at(O) 1986(O) ,(O) 1989(O) ,(O) 1993(O) ,(O) 1995(O) AFC(B-event) Championship(I-event) ,(O) 1990(O) ,(O) Football(O) at(O) the(O) 1994(O) Asian(B-event) Games(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, chemical compound, country, scientist, organization, award, chemical element, theory, protein, academic journal, event, discipline, astronomical object, person, enzyme, location and O.\nSentence: She also played at 1986 , 1989 , 1993 , 1995 AFC Championship , 1990 , Football at the 1994 Asian Games .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","also","played","at","1986",",","1989",",","1993",",","1995","AFC","Championship",",","1990",",","Football","at","the","1994","Asian","Games","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","O","O","O","O","O","O","O","B-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["university","chemical_compound","country","scientist","organization","award","chemical_element","theory","protein","academic_journal","event","discipline","astronomical_object","person","enzyme","location"]}
{"id":"372","dataset":"crossner_science","split":"test","instance":{"id":"372","prompt_labels":"Pauli(B-scientist) spent(O) a(O) year(O) at(O) the(O) University(B-university) of(I-university) Gttingen(I-university) as(O) the(O) assistant(O) to(O) Max(B-scientist) Born(I-scientist) ,(O) and(O) the(O) following(O) year(O) at(O) the(O) Institute(B-organization) for(I-organization) Theoretical(I-organization) Physics(I-organization) in(O) University(B-university) of(I-university) Copenhagen(I-university) ,(O) which(O) later(O) became(O) the(O) Niels(B-organization) Bohr(I-organization) Institute(I-organization) in(O) 1965(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, university, protein, organization, enzyme, person, academic journal, country, chemical element, discipline, award, location, scientist, astronomical object, event, chemical compound and O.\nSentence: Pauli spent a year at the University of Gttingen as the assistant to Max Born , and the following year at the Institute for Theoretical Physics in University of Copenhagen , which later became the Niels Bohr Institute in 1965 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Pauli","spent","a","year","at","the","University","of","Gttingen","as","the","assistant","to","Max","Born",",","and","the","following","year","at","the","Institute","for","Theoretical","Physics","in","University","of","Copenhagen",",","which","later","became","the","Niels","Bohr","Institute","in","1965","."],"labels":["B-scientist","O","O","O","O","O","B-university","I-university","I-university","O","O","O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-university","I-university","I-university","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O"],"target_index":null,"target_label":null},"label_list":["theory","university","protein","organization","enzyme","person","academic_journal","country","chemical_element","discipline","award","location","scientist","astronomical_object","event","chemical_compound"]}
{"id":"375","dataset":"crossner_science","split":"test","instance":{"id":"375","prompt_labels":"DSBs(O) can(O) be(O) artificially(O) induced(O) using(O) genome(O) editing(O) technologies(O) such(O) as(O) CRISPR(O) or(O) TALEN(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, chemical element, university, chemical compound, theory, organization, location, person, academic journal, country, discipline, astronomical object, enzyme, award, event, protein and O.\nSentence: DSBs can be artificially induced using genome editing technologies such as CRISPR or TALEN .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["DSBs","can","be","artificially","induced","using","genome","editing","technologies","such","as","CRISPR","or","TALEN","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","chemical_element","university","chemical_compound","theory","organization","location","person","academic_journal","country","discipline","astronomical_object","enzyme","award","event","protein"]}
{"id":"378","dataset":"crossner_science","split":"test","instance":{"id":"378","prompt_labels":"In(O) 1968(O) and(O) 1970(O) ,(O) Roger(B-scientist) Penrose(I-scientist) ,(O) Stephen(B-scientist) Hawking(I-scientist) ,(O) and(O) George(B-scientist) F.(I-scientist) R.(I-scientist) Ellis(I-scientist) published(O) papers(O) where(O) they(O) showed(O) that(O) mathematical(O) singularities(O) were(O) an(O) inevitable(O) initial(O) condition(O) of(O) relativistic(O) models(O) of(O) the(O) Big(B-theory) Bang(I-theory) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, event, chemical element, location, scientist, protein, organization, discipline, enzyme, theory, chemical compound, award, astronomical object, academic journal, university and O.\nSentence: In 1968 and 1970 , Roger Penrose , Stephen Hawking , and George F. R. Ellis published papers where they showed that mathematical singularities were an inevitable initial condition of relativistic models of the Big Bang .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1968","and","1970",",","Roger","Penrose",",","Stephen","Hawking",",","and","George","F.","R.","Ellis","published","papers","where","they","showed","that","mathematical","singularities","were","an","inevitable","initial","condition","of","relativistic","models","of","the","Big","Bang","."],"labels":["O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","I-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-theory","I-theory","O"],"target_index":null,"target_label":null},"label_list":["person","country","event","chemical_element","location","scientist","protein","organization","discipline","enzyme","theory","chemical_compound","award","astronomical_object","academic_journal","university"]}
{"id":"382","dataset":"crossner_science","split":"test","instance":{"id":"382","prompt_labels":"In(O) chemistry(B-discipline) ,(O) Erwin(B-scientist) Schrdinger(I-scientist) ,(O) Linus(B-scientist) Pauling(I-scientist) ,(O) Robert(B-scientist) S.(I-scientist) Mulliken(I-scientist) and(O) others(O) noted(O) that(O) the(O) consequence(O) of(O) Heisenberg(B-theory) 's(I-theory) relation(I-theory) was(O) that(O) the(O) electron(O) ,(O) as(O) a(O) wave(O) packet(O) ,(O) could(O) not(O) be(O) considered(O) to(O) have(O) an(O) exact(O) location(O) in(O) its(O) orbital(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, protein, award, theory, organization, event, country, academic journal, location, enzyme, astronomical object, discipline, scientist, chemical element, person, university and O.\nSentence: In chemistry , Erwin Schrdinger , Linus Pauling , Robert S. Mulliken and others noted that the consequence of Heisenberg 's relation was that the electron , as a wave packet , could not be considered to have an exact location in its orbital .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","chemistry",",","Erwin","Schrdinger",",","Linus","Pauling",",","Robert","S.","Mulliken","and","others","noted","that","the","consequence","of","Heisenberg","'s","relation","was","that","the","electron",",","as","a","wave","packet",",","could","not","be","considered","to","have","an","exact","location","in","its","orbital","."],"labels":["O","B-discipline","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","O","O","O","O","O","O","B-theory","I-theory","I-theory","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","protein","award","theory","organization","event","country","academic_journal","location","enzyme","astronomical_object","discipline","scientist","chemical_element","person","university"]}
{"id":"383","dataset":"crossner_science","split":"test","instance":{"id":"383","prompt_labels":"After(O) 3(O) years(O) at(O) King(B-university) 's(I-university) College(I-university) London(I-university) ((O) contemporary(O) with(O) Rosalind(B-scientist) Franklin(I-scientist) )(O) she(O) moved(O) to(O) the(O) University(B-university) of(I-university) Sheffield(I-university) in(O) 1955(O) as(O) a(O) demonstrator(O) in(O) the(O) Biochemistry(B-discipline) department(O) ((O) now(O) Molecular(B-discipline) Biology(I-discipline) and(O) Biotechnology(B-discipline) )(O) ,(O) obtaining(O) an(O) MRC(B-organization) grant(O) to(O) study(O) the(O) iron(O) storage(O) protein(O) Ferritin(B-protein) ,(O) publishing(O) preliminary(O) X-ray(O) diffraction(O) data(O) in(O) the(O) 1st(O) volume(O) of(O) the(O) Journal(B-academic journal) of(I-academic journal) Molecular(I-academic journal) Biology(I-academic journal) in(O) 1959(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, protein, chemical element, enzyme, event, award, scientist, location, theory, person, discipline, organization, academic journal, country, astronomical object, chemical compound and O.\nSentence: After 3 years at King 's College London ( contemporary with Rosalind Franklin ) she moved to the University of Sheffield in 1955 as a demonstrator in the Biochemistry department ( now Molecular Biology and Biotechnology ) , obtaining an MRC grant to study the iron storage protein Ferritin , publishing preliminary X-ray diffraction data in the 1st volume of the Journal of Molecular Biology in 1959 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","3","years","at","King","'s","College","London","(","contemporary","with","Rosalind","Franklin",")","she","moved","to","the","University","of","Sheffield","in","1955","as","a","demonstrator","in","the","Biochemistry","department","(","now","Molecular","Biology","and","Biotechnology",")",",","obtaining","an","MRC","grant","to","study","the","iron","storage","protein","Ferritin",",","publishing","preliminary","X-ray","diffraction","data","in","the","1st","volume","of","the","Journal","of","Molecular","Biology","in","1959","."],"labels":["O","O","O","O","B-university","I-university","I-university","I-university","O","O","O","B-scientist","I-scientist","O","O","O","O","O","B-university","I-university","I-university","O","O","O","O","O","O","O","B-discipline","O","O","O","B-discipline","I-discipline","O","B-discipline","O","O","O","O","B-organization","O","O","O","O","O","O","O","B-protein","O","O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","protein","chemical_element","enzyme","event","award","scientist","location","theory","person","discipline","organization","academic_journal","country","astronomical_object","chemical_compound"]}
{"id":"386","dataset":"crossner_science","split":"test","instance":{"id":"386","prompt_labels":"The(O) claims(O) were(O) initially(O) submitted(O) to(O) the(O) International(B-academic journal) Journal(I-academic journal) of(I-academic journal) Astrobiology(I-academic journal) ,(O) which(O) rejected(O) the(O) paper(O) ,(O) but(O) were(O) eventually(O) published(O) by(O) the(O) fringe(O) Journal(B-academic journal) of(I-academic journal) Cosmology(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, discipline, location, event, university, country, academic journal, organization, protein, enzyme, chemical compound, award, scientist, astronomical object, person, chemical element and O.\nSentence: The claims were initially submitted to the International Journal of Astrobiology , which rejected the paper , but were eventually published by the fringe Journal of Cosmology .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","claims","were","initially","submitted","to","the","International","Journal","of","Astrobiology",",","which","rejected","the","paper",",","but","were","eventually","published","by","the","fringe","Journal","of","Cosmology","."],"labels":["O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["theory","discipline","location","event","university","country","academic_journal","organization","protein","enzyme","chemical_compound","award","scientist","astronomical_object","person","chemical_element"]}
{"id":"390","dataset":"crossner_science","split":"test","instance":{"id":"390","prompt_labels":"This(O) was(O) shown(O) by(O) Artur(B-scientist) Avila(I-scientist) and(O) Svetlana(B-scientist) Jitomirskaya(I-scientist) solving(O) the(O) by-then(O) famous(O) ten(O) martini(O) problem(O) with(O) respect(O) to(O) the(O) parameters(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, discipline, person, location, astronomical object, theory, university, award, chemical compound, protein, event, organization, academic journal, country, scientist, chemical element and O.\nSentence: This was shown by Artur Avila and Svetlana Jitomirskaya solving the by-then famous ten martini problem with respect to the parameters ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","was","shown","by","Artur","Avila","and","Svetlana","Jitomirskaya","solving","the","by-then","famous","ten","martini","problem","with","respect","to","the","parameters",")","."],"labels":["O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["enzyme","discipline","person","location","astronomical_object","theory","university","award","chemical_compound","protein","event","organization","academic_journal","country","scientist","chemical_element"]}
{"id":"391","dataset":"crossner_science","split":"test","instance":{"id":"391","prompt_labels":"Uranus(B-astronomical object) and(O) 4(B-astronomical object) Vesta(I-astronomical object) had(O) most(O) probably(O) been(O) seen(O) but(O) could(O) not(O) be(O) recognized(O) as(O) planets(O) because(O) they(O) appear(O) so(O) faint(O) even(O) at(O) maximum(O) brightness(O) ;(O) Uranus(B-astronomical object) '(O) magnitude(O) varies(O) from(O) +(O) 5.3supm(O) /(O) sup(O) to(O) +(O) 5.9supm(O) /(O) sup(O) ,(O) and(O) Vesta(B-astronomical object) 's(O) from(O) +(O) 5.2supm(O) /(O) sup(O) to(O) +(O) 8.5supm(O) /(O) sup(O) ((O) so(O) that(O) it(O) is(O) only(O) visible(O) near(O) its(O) opposition(O) dates(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, award, chemical compound, location, scientist, person, discipline, chemical element, country, astronomical object, protein, organization, event, enzyme, theory, academic journal and O.\nSentence: Uranus and 4 Vesta had most probably been seen but could not be recognized as planets because they appear so faint even at maximum brightness ; Uranus ' magnitude varies from + 5.3supm / sup to + 5.9supm / sup , and Vesta 's from + 5.2supm / sup to + 8.5supm / sup ( so that it is only visible near its opposition dates ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Uranus","and","4","Vesta","had","most","probably","been","seen","but","could","not","be","recognized","as","planets","because","they","appear","so","faint","even","at","maximum","brightness",";","Uranus","'","magnitude","varies","from","+","5.3supm","/","sup","to","+","5.9supm","/","sup",",","and","Vesta","'s","from","+","5.2supm","/","sup","to","+","8.5supm","/","sup","(","so","that","it","is","only","visible","near","its","opposition","dates",")","."],"labels":["B-astronomical object","O","B-astronomical object","I-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","award","chemical_compound","location","scientist","person","discipline","chemical_element","country","astronomical_object","protein","organization","event","enzyme","theory","academic_journal"]}
{"id":"392","dataset":"crossner_science","split":"test","instance":{"id":"392","prompt_labels":"While(O) he(O) was(O) there(O) ,(O) he(O) was(O) among(O) a(O) group(O) of(O) physicists(O) including(O) Bruno(B-scientist) Zumino(I-scientist) ,(O) Harry(B-scientist) Lehmann(I-scientist) ,(O) Wolfhart(B-scientist) Zimmermann(I-scientist) ,(O) Kurt(B-scientist) Symanzik(I-scientist) ,(O) Gerhard(B-scientist) Lders(I-scientist) ,(O) Reinhard(B-scientist) Oehme(I-scientist) ,(O) Vladimir(B-scientist) Glaser(I-scientist) ,(O) and(O) Carl(B-scientist) Friedrich(I-scientist) von(I-scientist) Weizscker(I-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, enzyme, protein, academic journal, person, chemical element, location, organization, theory, event, discipline, university, astronomical object, chemical compound, scientist, award and O.\nSentence: While he was there , he was among a group of physicists including Bruno Zumino , Harry Lehmann , Wolfhart Zimmermann , Kurt Symanzik , Gerhard Lders , Reinhard Oehme , Vladimir Glaser , and Carl Friedrich von Weizscker .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["While","he","was","there",",","he","was","among","a","group","of","physicists","including","Bruno","Zumino",",","Harry","Lehmann",",","Wolfhart","Zimmermann",",","Kurt","Symanzik",",","Gerhard","Lders",",","Reinhard","Oehme",",","Vladimir","Glaser",",","and","Carl","Friedrich","von","Weizscker","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","I-scientist","I-scientist","O"],"target_index":null,"target_label":null},"label_list":["country","enzyme","protein","academic_journal","person","chemical_element","location","organization","theory","event","discipline","university","astronomical_object","chemical_compound","scientist","award"]}
{"id":"394","dataset":"crossner_science","split":"test","instance":{"id":"394","prompt_labels":"Amdahl(B-person) was(O) named(O) an(O) IBM(O) Fellow(O) in(O) 1965(O) ,(O) became(O) a(O) member(O) of(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Engineering(I-organization) in(O) 1967(O) and(O) was(O) recognized(O) as(O) the(O) Centennial(O) Alumnus(O) of(O) South(B-university) Dakota(I-university) State(I-university) University(I-university) in(O) 1986(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, person, discipline, chemical element, organization, event, chemical compound, theory, location, university, academic journal, enzyme, award, scientist, protein, country and O.\nSentence: Amdahl was named an IBM Fellow in 1965 , became a member of the National Academy of Engineering in 1967 and was recognized as the Centennial Alumnus of South Dakota State University in 1986 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Amdahl","was","named","an","IBM","Fellow","in","1965",",","became","a","member","of","the","National","Academy","of","Engineering","in","1967","and","was","recognized","as","the","Centennial","Alumnus","of","South","Dakota","State","University","in","1986","."],"labels":["B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","B-university","I-university","I-university","I-university","O","O","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","person","discipline","chemical_element","organization","event","chemical_compound","theory","location","university","academic_journal","enzyme","award","scientist","protein","country"]}
{"id":"399","dataset":"crossner_science","split":"test","instance":{"id":"399","prompt_labels":"Mars(B-astronomical object) has(O) four(O) known(O) co-orbital(O) asteroids(O) ((O) 5261(B-astronomical object) Eureka(I-astronomical object) ,(O) ,(O) all(O) at(O) the(O) Lagrangian(O) point(O) s(O) )(O) ,(O) and(O) Jupiter(B-astronomical object) has(O) many(O) ((O) an(O) estimated(O) one(O) million(O) greater(O) than(O) 1(O) km(O) in(O) diameter(O) ,(O) the(O) Jovian(O) trojans(O) )(O) ;(O) there(O) are(O) also(O) other(O) small(O) co-orbital(O) moons(O) in(O) the(O) Saturnian(O) system(O) :(O) Telesto(B-astronomical object) and(O) Calypso(B-astronomical object) with(O) Tethys(B-astronomical object) ,(O) and(O) Helene(B-astronomical object) and(O) Polydeuces(B-astronomical object) with(O) Dione(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, chemical element, scientist, theory, academic journal, astronomical object, chemical compound, location, country, event, person, organization, enzyme, discipline, protein, university and O.\nSentence: Mars has four known co-orbital asteroids ( 5261 Eureka , , all at the Lagrangian point s ) , and Jupiter has many ( an estimated one million greater than 1 km in diameter , the Jovian trojans ) ; there are also other small co-orbital moons in the Saturnian system : Telesto and Calypso with Tethys , and Helene and Polydeuces with Dione .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Mars","has","four","known","co-orbital","asteroids","(","5261","Eureka",",",",","all","at","the","Lagrangian","point","s",")",",","and","Jupiter","has","many","(","an","estimated","one","million","greater","than","1","km","in","diameter",",","the","Jovian","trojans",")",";","there","are","also","other","small","co-orbital","moons","in","the","Saturnian","system",":","Telesto","and","Calypso","with","Tethys",",","and","Helene","and","Polydeuces","with","Dione","."],"labels":["B-astronomical object","O","O","O","O","O","O","B-astronomical object","I-astronomical object","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["award","chemical_element","scientist","theory","academic_journal","astronomical_object","chemical_compound","location","country","event","person","organization","enzyme","discipline","protein","university"]}
{"id":"400","dataset":"crossner_science","split":"test","instance":{"id":"400","prompt_labels":"A(O) key(O) step(O) in(O) nitrification(O) is(O) the(O) oxidation(O) of(O) ammonia(B-chemical compound) to(O) hydroxylamine(B-chemical compound) ((O) NH2OH(B-chemical compound) )(O) catalyzed(O) by(O) the(O) enzyme(O) Ammonia(B-enzyme) monooxygenase(I-enzyme) ((O) AMO(B-enzyme) )(O) .Arciero(B-person) ,(I-person) D.(I-person) ;(O) Vannelli(B-person) ,(I-person) T.(I-person) ;(O) Logan(B-person) ,(I-person) M.(I-person) ;(O) Hopper(B-person) ,(I-person) A.(I-person) B.Degradation(O) of(O) trichloroethylene(B-chemical compound) by(O) the(O) ammonia-oxidizing(O) bacterium(O) Nitrosomonas(O) europaea(O) Biochem(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, chemical compound, organization, event, country, location, protein, university, theory, academic journal, enzyme, award, person, astronomical object, scientist, discipline and O.\nSentence: A key step in nitrification is the oxidation of ammonia to hydroxylamine ( NH2OH ) catalyzed by the enzyme Ammonia monooxygenase ( AMO ) .Arciero , D. ; Vannelli , T. ; Logan , M. ; Hopper , A. B.Degradation of trichloroethylene by the ammonia-oxidizing bacterium Nitrosomonas europaea Biochem .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","key","step","in","nitrification","is","the","oxidation","of","ammonia","to","hydroxylamine","(","NH2OH",")","catalyzed","by","the","enzyme","Ammonia","monooxygenase","(","AMO",")",".Arciero",",","D.",";","Vannelli",",","T.",";","Logan",",","M.",";","Hopper",",","A.","B.Degradation","of","trichloroethylene","by","the","ammonia-oxidizing","bacterium","Nitrosomonas","europaea","Biochem","."],"labels":["O","O","O","O","O","O","O","O","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","O","O","O","O","B-enzyme","I-enzyme","O","B-enzyme","O","B-person","I-person","I-person","O","B-person","I-person","I-person","O","B-person","I-person","I-person","O","B-person","I-person","I-person","O","O","B-chemical compound","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","chemical_compound","organization","event","country","location","protein","university","theory","academic_journal","enzyme","award","person","astronomical_object","scientist","discipline"]}
{"id":"403","dataset":"crossner_science","split":"test","instance":{"id":"403","prompt_labels":"He(O) was(O) elected(O) a(O) Fellow(B-award) of(I-award) the(I-award) Royal(I-award) Society(I-award) in(O) March(O) 1827(O) and(O) was(O) awarded(O) their(O) Royal(B-award) Medal(I-award) the(O) same(O) year(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, country, protein, organization, event, academic journal, discipline, enzyme, university, person, award, location, astronomical object, theory, chemical element, chemical compound and O.\nSentence: He was elected a Fellow of the Royal Society in March 1827 and was awarded their Royal Medal the same year .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","elected","a","Fellow","of","the","Royal","Society","in","March","1827","and","was","awarded","their","Royal","Medal","the","same","year","."],"labels":["O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","B-award","I-award","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","country","protein","organization","event","academic_journal","discipline","enzyme","university","person","award","location","astronomical_object","theory","chemical_element","chemical_compound"]}
{"id":"404","dataset":"crossner_science","split":"test","instance":{"id":"404","prompt_labels":"Fischer(B-person) then(O) attended(O) the(O) University(B-university) of(I-university) Bonn(I-university) in(O) 1871(O) ,(O) but(O) switched(O) to(O) the(O) University(B-university) of(I-university) Strasbourg(I-university) in(O) 1872(O) with(O) his(O) study(O) of(O) phthaleins(B-chemical compound) ,(O) and(O) was(O) appointed(O) to(O) a(O) position(O) at(O) the(O) university(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, protein, enzyme, award, chemical element, chemical compound, organization, event, scientist, discipline, person, country, astronomical object, academic journal, theory, location and O.\nSentence: Fischer then attended the University of Bonn in 1871 , but switched to the University of Strasbourg in 1872 with his study of phthaleins , and was appointed to a position at the university .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Fischer","then","attended","the","University","of","Bonn","in","1871",",","but","switched","to","the","University","of","Strasbourg","in","1872","with","his","study","of","phthaleins",",","and","was","appointed","to","a","position","at","the","university","."],"labels":["B-person","O","O","O","B-university","I-university","I-university","O","O","O","O","O","O","O","B-university","I-university","I-university","O","O","O","O","O","O","B-chemical compound","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","protein","enzyme","award","chemical_element","chemical_compound","organization","event","scientist","discipline","person","country","astronomical_object","academic_journal","theory","location"]}
{"id":"405","dataset":"crossner_science","split":"test","instance":{"id":"405","prompt_labels":"The(O) hydroxyl(B-chemical compound) radical(I-chemical compound) can(O) damage(O) virtually(O) all(O) types(O) of(O) macromolecules(O) :(O) carbohydrates(B-chemical compound) ,(O) nucleic(B-chemical compound) acids(I-chemical compound) ((O) mutation(O) s(O) )(O) ,(O) lipids(B-chemical compound) ((O) lipid(O) peroxidation(O) )(O) ,(O) and(O) amino(B-chemical compound) acids(I-chemical compound) ((O) e.g.(O) conversion(O) of(O) Phenylalanine(B-chemical compound) to(O) m(B-chemical compound) -(I-chemical compound) Tyrosine(I-chemical compound) and(O) o(B-chemical compound) -(I-chemical compound) Tyrosine(I-chemical compound) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, person, academic journal, chemical element, chemical compound, theory, enzyme, country, event, award, protein, organization, university, location, discipline, scientist and O.\nSentence: The hydroxyl radical can damage virtually all types of macromolecules : carbohydrates , nucleic acids ( mutation s ) , lipids ( lipid peroxidation ) , and amino acids ( e.g. conversion of Phenylalanine to m - Tyrosine and o - Tyrosine ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","hydroxyl","radical","can","damage","virtually","all","types","of","macromolecules",":","carbohydrates",",","nucleic","acids","(","mutation","s",")",",","lipids","(","lipid","peroxidation",")",",","and","amino","acids","(","e.g.","conversion","of","Phenylalanine","to","m","-","Tyrosine","and","o","-","Tyrosine",")","."],"labels":["O","B-chemical compound","I-chemical compound","O","O","O","O","O","O","O","O","B-chemical compound","O","B-chemical compound","I-chemical compound","O","O","O","O","O","B-chemical compound","O","O","O","O","O","O","B-chemical compound","I-chemical compound","O","O","O","O","B-chemical compound","O","B-chemical compound","I-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","I-chemical compound","O","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","person","academic_journal","chemical_element","chemical_compound","theory","enzyme","country","event","award","protein","organization","university","location","discipline","scientist"]}
{"id":"406","dataset":"crossner_science","split":"test","instance":{"id":"406","prompt_labels":"Notable(O) companions(O) from(O) the(O) earlier(O) series(O) included(O) Romana(B-person) ((O) Mary(B-person) Tamm(I-person) and(O) Lalla(B-person) Ward(I-person) )(O) ,(O) a(O) Time(B-person) Lady(I-person) ;(O) Sarah(B-person) Jane(I-person) Smith(I-person) ((O) Elisabeth(B-person) Sladen(I-person) )(O) ;(O) and(O) Jo(B-person) Grant(I-person) ((O) Katy(B-person) Manning(I-person) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, protein, theory, enzyme, person, discipline, event, organization, chemical element, astronomical object, academic journal, university, chemical compound, award, scientist and O.\nSentence: Notable companions from the earlier series included Romana ( Mary Tamm and Lalla Ward ) , a Time Lady ; Sarah Jane Smith ( Elisabeth Sladen ) ; and Jo Grant ( Katy Manning ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Notable","companions","from","the","earlier","series","included","Romana","(","Mary","Tamm","and","Lalla","Ward",")",",","a","Time","Lady",";","Sarah","Jane","Smith","(","Elisabeth","Sladen",")",";","and","Jo","Grant","(","Katy","Manning",")","."],"labels":["O","O","O","O","O","O","O","B-person","O","B-person","I-person","O","B-person","I-person","O","O","O","B-person","I-person","O","B-person","I-person","I-person","O","B-person","I-person","O","O","O","B-person","I-person","O","B-person","I-person","O","O"],"target_index":null,"target_label":null},"label_list":["country","location","protein","theory","enzyme","person","discipline","event","organization","chemical_element","astronomical_object","academic_journal","university","chemical_compound","award","scientist"]}
{"id":"410","dataset":"crossner_science","split":"test","instance":{"id":"410","prompt_labels":"They(O) also(O) noticed(O) the(O) similarity(O) between(O) this(O) test(O) theory(O) and(O) Lorentz(B-theory) ether(I-theory) theory(I-theory) of(O) Hendrik(B-scientist) Lorentz(I-scientist) ,(O) Joseph(B-scientist) Larmor(I-scientist) and(O) Henri(B-scientist) Poincar(I-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, discipline, chemical element, person, scientist, theory, university, location, protein, enzyme, award, chemical compound, event, academic journal, astronomical object and O.\nSentence: They also noticed the similarity between this test theory and Lorentz ether theory of Hendrik Lorentz , Joseph Larmor and Henri Poincar .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["They","also","noticed","the","similarity","between","this","test","theory","and","Lorentz","ether","theory","of","Hendrik","Lorentz",",","Joseph","Larmor","and","Henri","Poincar","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-theory","I-theory","I-theory","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O"],"target_index":null,"target_label":null},"label_list":["organization","country","discipline","chemical_element","person","scientist","theory","university","location","protein","enzyme","award","chemical_compound","event","academic_journal","astronomical_object"]}
{"id":"411","dataset":"crossner_science","split":"test","instance":{"id":"411","prompt_labels":"It(O) was(O) founded(O) by(O) William(B-scientist) Bateson(I-scientist) and(O) Edith(B-scientist) Rebecca(I-scientist) Saunders(I-scientist) in(O) 1919(O) and(O) celebrates(O) its(O) centenary(O) year(O) in(O) 2019(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, person, country, discipline, award, enzyme, chemical compound, theory, academic journal, event, protein, scientist, chemical element, location, organization, astronomical object and O.\nSentence: It was founded by William Bateson and Edith Rebecca Saunders in 1919 and celebrates its centenary year in 2019 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","founded","by","William","Bateson","and","Edith","Rebecca","Saunders","in","1919","and","celebrates","its","centenary","year","in","2019","."],"labels":["O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","person","country","discipline","award","enzyme","chemical_compound","theory","academic_journal","event","protein","scientist","chemical_element","location","organization","astronomical_object"]}
{"id":"414","dataset":"crossner_science","split":"test","instance":{"id":"414","prompt_labels":"Szilard(B-scientist) initially(O) attended(O) Budapest(B-university) University(I-university) of(I-university) Technology(I-university) and(I-university) Economics(I-university) in(O) Budapest(B-location) ,(O) but(O) his(O) engineering(O) studies(O) were(O) interrupted(O) by(O) service(O) in(O) the(O) Austro-Hungarian(B-organization) Army(I-organization) during(O) World(B-event) War(I-event) I.(I-event) He(O) left(O) Hungary(B-country) for(O) Germany(B-country) in(O) 1919(O) ,(O) enrolling(O) at(O) Technical(B-university) University(I-university) of(I-university) Berlin(I-university) ,(O) but(O) became(O) bored(O) with(O) engineering(O) and(O) transferred(O) to(O) Humboldt(B-university) University(I-university) of(I-university) Berlin(I-university) ,(O) where(O) he(O) studied(O) physics(B-discipline) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, theory, university, scientist, award, academic journal, enzyme, chemical element, organization, location, country, discipline, person, protein, event, astronomical object and O.\nSentence: Szilard initially attended Budapest University of Technology and Economics in Budapest , but his engineering studies were interrupted by service in the Austro-Hungarian Army during World War I. He left Hungary for Germany in 1919 , enrolling at Technical University of Berlin , but became bored with engineering and transferred to Humboldt University of Berlin , where he studied physics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Szilard","initially","attended","Budapest","University","of","Technology","and","Economics","in","Budapest",",","but","his","engineering","studies","were","interrupted","by","service","in","the","Austro-Hungarian","Army","during","World","War","I.","He","left","Hungary","for","Germany","in","1919",",","enrolling","at","Technical","University","of","Berlin",",","but","became","bored","with","engineering","and","transferred","to","Humboldt","University","of","Berlin",",","where","he","studied","physics","."],"labels":["B-scientist","O","O","B-university","I-university","I-university","I-university","I-university","I-university","O","B-location","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","B-event","I-event","I-event","O","O","B-country","O","B-country","O","O","O","O","O","B-university","I-university","I-university","I-university","O","O","O","O","O","O","O","O","O","B-university","I-university","I-university","I-university","O","O","O","O","B-discipline","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","theory","university","scientist","award","academic_journal","enzyme","chemical_element","organization","location","country","discipline","person","protein","event","astronomical_object"]}
{"id":"417","dataset":"crossner_science","split":"test","instance":{"id":"417","prompt_labels":"to(O) University(B-university) of(I-university) Gttingen(I-university) in(O) 1913(O) ,(O) to(O) ETH(B-university) Zurich(I-university) in(O) 1920(O) ,(O) to(O) University(B-university) of(I-university) Leipzig(I-university) in(O) 1927(O) ,(O) and(O) in(O) 1934(O) to(O) Humboldt(B-university) University(I-university) of(I-university) Berlin(I-university) ,(O) where(O) ,(O) succeeding(O) Einstein(B-scientist) ,(O) he(O) became(O) director(O) of(O) the(O) Kaiser(B-organization) Wilhelm(I-organization) Institute(I-organization) for(I-organization) Physics(I-organization) ((O) now(O) named(O) the(O) Max-Planck-Institut(B-organization) )(O) whose(O) facilities(O) were(O) built(O) only(O) during(O) Debye(B-scientist) 's(O) era(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, chemical compound, location, organization, theory, astronomical object, country, person, protein, academic journal, chemical element, enzyme, award, university, discipline, event and O.\nSentence: to University of Gttingen in 1913 , to ETH Zurich in 1920 , to University of Leipzig in 1927 , and in 1934 to Humboldt University of Berlin , where , succeeding Einstein , he became director of the Kaiser Wilhelm Institute for Physics ( now named the Max-Planck-Institut ) whose facilities were built only during Debye 's era .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["to","University","of","Gttingen","in","1913",",","to","ETH","Zurich","in","1920",",","to","University","of","Leipzig","in","1927",",","and","in","1934","to","Humboldt","University","of","Berlin",",","where",",","succeeding","Einstein",",","he","became","director","of","the","Kaiser","Wilhelm","Institute","for","Physics","(","now","named","the","Max-Planck-Institut",")","whose","facilities","were","built","only","during","Debye","'s","era","."],"labels":["O","B-university","I-university","I-university","O","O","O","O","B-university","I-university","O","O","O","O","B-university","I-university","I-university","O","O","O","O","O","O","O","B-university","I-university","I-university","I-university","O","O","O","O","B-scientist","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","B-organization","O","O","O","O","O","O","O","B-scientist","O","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","chemical_compound","location","organization","theory","astronomical_object","country","person","protein","academic_journal","chemical_element","enzyme","award","university","discipline","event"]}
{"id":"418","dataset":"crossner_science","split":"test","instance":{"id":"418","prompt_labels":"She(O) is(O) an(O) active(O) voice(O) in(O) numerous(O) committees(O) of(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) the(O) American(B-organization) Association(I-organization) for(I-organization) the(I-organization) Advancement(I-organization) of(I-organization) Science(I-organization) ((O) AAAS(B-organization) )(O) ,(O) and(O) the(O) National(B-organization) Science(I-organization) Foundation(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, academic journal, chemical compound, discipline, enzyme, organization, theory, astronomical object, university, location, award, protein, event, person, chemical element, country and O.\nSentence: She is an active voice in numerous committees of the National Academy of Sciences , the American Association for the Advancement of Science ( AAAS ) , and the National Science Foundation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","is","an","active","voice","in","numerous","committees","of","the","National","Academy","of","Sciences",",","the","American","Association","for","the","Advancement","of","Science","(","AAAS",")",",","and","the","National","Science","Foundation","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["scientist","academic_journal","chemical_compound","discipline","enzyme","organization","theory","astronomical_object","university","location","award","protein","event","person","chemical_element","country"]}
{"id":"419","dataset":"crossner_science","split":"test","instance":{"id":"419","prompt_labels":"East(B-location) Malaysia(I-location) currently(O) has(O) two(O) public(O) universities(O) ,(O) namely(O) Universiti(B-university) Malaysia(I-university) Sarawak(I-university) ((O) UNIMAS(B-university) )(O) and(O) Universiti(B-university) Malaysia(I-university) Sabah(I-university) ((O) UMS(B-university) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, discipline, academic journal, scientist, event, enzyme, astronomical object, university, chemical element, country, chemical compound, theory, protein, location, organization and O.\nSentence: East Malaysia currently has two public universities , namely Universiti Malaysia Sarawak ( UNIMAS ) and Universiti Malaysia Sabah ( UMS ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["East","Malaysia","currently","has","two","public","universities",",","namely","Universiti","Malaysia","Sarawak","(","UNIMAS",")","and","Universiti","Malaysia","Sabah","(","UMS",")","."],"labels":["B-location","I-location","O","O","O","O","O","O","O","B-university","I-university","I-university","O","B-university","O","O","B-university","I-university","I-university","O","B-university","O","O"],"target_index":null,"target_label":null},"label_list":["award","person","discipline","academic_journal","scientist","event","enzyme","astronomical_object","university","chemical_element","country","chemical_compound","theory","protein","location","organization"]}
{"id":"420","dataset":"crossner_science","split":"test","instance":{"id":"420","prompt_labels":"He(O) became(O) an(O) expert(O) on(O) the(O) electrical(O) conductivity(O) of(O) gases(O) ,(O) the(O) properties(O) of(O) ion(O) s(O) ,(O) and(O) the(O) behavior(O) of(O) atmospheric(O) electricity(O) ,(O) publishing(O) in(O) journals(O) including(O) the(O) Physical(B-academic journal) Review(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Applied(I-academic journal) Physics(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Chemical(I-academic journal) Physics(I-academic journal) ,(O) and(O) the(O) Journal(B-academic journal) of(I-academic journal) Atmospheric(I-academic journal) Electricity(I-academic journal) and(I-academic journal) Terrestrial(I-academic journal) Magnetism(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, enzyme, scientist, event, discipline, theory, country, protein, location, chemical element, academic journal, chemical compound, astronomical object, person, award, organization and O.\nSentence: He became an expert on the electrical conductivity of gases , the properties of ion s , and the behavior of atmospheric electricity , publishing in journals including the Physical Review , Journal of Applied Physics , Journal of Chemical Physics , and the Journal of Atmospheric Electricity and Terrestrial Magnetism .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","became","an","expert","on","the","electrical","conductivity","of","gases",",","the","properties","of","ion","s",",","and","the","behavior","of","atmospheric","electricity",",","publishing","in","journals","including","the","Physical","Review",",","Journal","of","Applied","Physics",",","Journal","of","Chemical","Physics",",","and","the","Journal","of","Atmospheric","Electricity","and","Terrestrial","Magnetism","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["university","enzyme","scientist","event","discipline","theory","country","protein","location","chemical_element","academic_journal","chemical_compound","astronomical_object","person","award","organization"]}
{"id":"421","dataset":"crossner_science","split":"test","instance":{"id":"421","prompt_labels":"Interaction(O) of(O) PAMPs(O) with(O) PRPs(O) ((O) pattern-recognition(O) proteins(O) )(O) activates(O) a(O) series(O) of(O) Serine(B-enzyme) protease(I-enzyme) and(O) those(O) proteolytically(O) cleave(O) the(O) prophenoloxidase(O) ((O) proPO(O) )(O) zymogen(O) and(O) activate(O) phenoxidase(O) ((O) PO(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, award, theory, university, organization, country, academic journal, event, protein, astronomical object, location, chemical compound, discipline, chemical element, person, scientist and O.\nSentence: Interaction of PAMPs with PRPs ( pattern-recognition proteins ) activates a series of Serine protease and those proteolytically cleave the prophenoloxidase ( proPO ) zymogen and activate phenoxidase ( PO ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Interaction","of","PAMPs","with","PRPs","(","pattern-recognition","proteins",")","activates","a","series","of","Serine","protease","and","those","proteolytically","cleave","the","prophenoloxidase","(","proPO",")","zymogen","and","activate","phenoxidase","(","PO",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-enzyme","I-enzyme","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["enzyme","award","theory","university","organization","country","academic_journal","event","protein","astronomical_object","location","chemical_compound","discipline","chemical_element","person","scientist"]}
{"id":"424","dataset":"crossner_science","split":"test","instance":{"id":"424","prompt_labels":"Taurasi(B-person) also(O) received(O) many(O) personal(O) accolades(O) at(O) UConn(B-university) including(O) the(O) 2003(O) and(O) 2004(O) Naismith(B-award) College(I-award) Player(I-award) of(I-award) the(I-award) Year(I-award) awards(I-award) ,(O) the(O) 2003(O) Wade(B-award) Trophy(I-award) ,(O) the(O) 2003(O) and(O) 2004(O) Honda(B-award) Sports(I-award) Award(I-award) and(O) the(O) 2003(O) Associated(B-award) Press(I-award) Player(I-award) of(I-award) the(I-award) Year(I-award) award(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, astronomical object, event, university, country, person, protein, academic journal, theory, organization, discipline, enzyme, location, chemical compound, award, scientist and O.\nSentence: Taurasi also received many personal accolades at UConn including the 2003 and 2004 Naismith College Player of the Year awards , the 2003 Wade Trophy , the 2003 and 2004 Honda Sports Award and the 2003 Associated Press Player of the Year award .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Taurasi","also","received","many","personal","accolades","at","UConn","including","the","2003","and","2004","Naismith","College","Player","of","the","Year","awards",",","the","2003","Wade","Trophy",",","the","2003","and","2004","Honda","Sports","Award","and","the","2003","Associated","Press","Player","of","the","Year","award","."],"labels":["B-person","O","O","O","O","O","O","B-university","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","B-award","I-award","O","O","O","O","O","B-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","astronomical_object","event","university","country","person","protein","academic_journal","theory","organization","discipline","enzyme","location","chemical_compound","award","scientist"]}
{"id":"426","dataset":"crossner_science","split":"test","instance":{"id":"426","prompt_labels":"The(O) entire(O) set(O) is(O) processed(O) through(O) cycles(O) of(O) :(O) ((O) a(O) )(O) hybridization(O) at(O) 60(O) (O) C(O) ;(O) ((O) b(O) )(O) elongation(O) via(O) Taq(B-enzyme) polymerase(I-enzyme) and(O) a(O) standard(O) ligase(B-enzyme) ;(O) and(O) ((O) c(O) )(O) denaturation(O) at(O) 95(O) (O) C(O) ,(O) forming(O) progressively(O) longer(O) contiguous(O) strands(O) and(O) ultimately(O) resulting(O) in(O) the(O) final(O) genome(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, enzyme, theory, award, chemical element, scientist, discipline, academic journal, protein, chemical compound, organization, university, location, person, country, astronomical object and O.\nSentence: The entire set is processed through cycles of : ( a ) hybridization at 60  C ; ( b ) elongation via Taq polymerase and a standard ligase ; and ( c ) denaturation at 95  C , forming progressively longer contiguous strands and ultimately resulting in the final genome .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","entire","set","is","processed","through","cycles","of",":","(","a",")","hybridization","at","60","","C",";","(","b",")","elongation","via","Taq","polymerase","and","a","standard","ligase",";","and","(","c",")","denaturation","at","95","","C",",","forming","progressively","longer","contiguous","strands","and","ultimately","resulting","in","the","final","genome","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-enzyme","I-enzyme","O","O","O","B-enzyme","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","enzyme","theory","award","chemical_element","scientist","discipline","academic_journal","protein","chemical_compound","organization","university","location","person","country","astronomical_object"]}
{"id":"427","dataset":"crossner_science","split":"test","instance":{"id":"427","prompt_labels":"This(O) includes(O) Pluto(B-astronomical object) ,(O) which(O) is(O) constrained(O) in(O) its(O) orbit(O) by(O) the(O) gravity(O) of(O) Neptune(B-astronomical object) and(O) shares(O) its(O) orbital(O) neighbourhood(O) with(O) many(O) Kuiper(O) belt(O) objects(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, event, award, chemical compound, university, theory, enzyme, person, organization, protein, chemical element, country, discipline, scientist, location, academic journal and O.\nSentence: This includes Pluto , which is constrained in its orbit by the gravity of Neptune and shares its orbital neighbourhood with many Kuiper belt objects .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","includes","Pluto",",","which","is","constrained","in","its","orbit","by","the","gravity","of","Neptune","and","shares","its","orbital","neighbourhood","with","many","Kuiper","belt","objects","."],"labels":["O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","event","award","chemical_compound","university","theory","enzyme","person","organization","protein","chemical_element","country","discipline","scientist","location","academic_journal"]}
{"id":"428","dataset":"crossner_science","split":"test","instance":{"id":"428","prompt_labels":"The(O) importance(O) of(O) the(O) predicted(O) return(O) based(O) on(O) the(O) calculation(O) by(O) Encke(B-scientist) was(O) rewarded(O) by(O) the(O) Royal(B-organization) Astronomical(I-organization) Society(I-organization) in(O) London(B-location) by(O) presenting(O) their(O) Gold(B-award) Medal(I-award) of(I-award) the(I-award) Royal(I-award) Astronomical(I-award) Society(I-award) to(O) him(O) in(O) 1824(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, person, award, academic journal, discipline, event, astronomical object, chemical compound, enzyme, organization, protein, chemical element, theory, location, scientist, country and O.\nSentence: The importance of the predicted return based on the calculation by Encke was rewarded by the Royal Astronomical Society in London by presenting their Gold Medal of the Royal Astronomical Society to him in 1824 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","importance","of","the","predicted","return","based","on","the","calculation","by","Encke","was","rewarded","by","the","Royal","Astronomical","Society","in","London","by","presenting","their","Gold","Medal","of","the","Royal","Astronomical","Society","to","him","in","1824","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-scientist","O","O","O","O","B-organization","I-organization","I-organization","O","B-location","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","person","award","academic_journal","discipline","event","astronomical_object","chemical_compound","enzyme","organization","protein","chemical_element","theory","location","scientist","country"]}
{"id":"432","dataset":"crossner_science","split":"test","instance":{"id":"432","prompt_labels":"Early(O) pioneers(O) also(O) include(O) Alexey(B-scientist) Grigorevich(I-scientist) Ivakhnenko(I-scientist) ,(O) Teuvo(B-scientist) Kohonen(I-scientist) ,(O) Stephen(B-scientist) Grossberg(I-scientist) ,(O) Kunihiko(B-scientist) Fukushima(I-scientist) ,(O) Christoph(B-scientist) von(I-scientist) der(I-scientist) Malsburg(I-scientist) ,(O) David(B-scientist) Willshaw(I-scientist) ,(O) Shun-Ichi(B-scientist) Amari(I-scientist) ,(O) Bernard(B-scientist) Widrow(I-scientist) ,(O) John(B-scientist) Hopfield(I-scientist) ,(O) Eduardo(B-scientist) R.(I-scientist) Caianiello(I-scientist) ,(O) and(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, protein, academic journal, scientist, country, chemical element, discipline, organization, university, astronomical object, event, person, award, theory, location, enzyme and O.\nSentence: Early pioneers also include Alexey Grigorevich Ivakhnenko , Teuvo Kohonen , Stephen Grossberg , Kunihiko Fukushima , Christoph von der Malsburg , David Willshaw , Shun-Ichi Amari , Bernard Widrow , John Hopfield , Eduardo R. Caianiello , and others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Early","pioneers","also","include","Alexey","Grigorevich","Ivakhnenko",",","Teuvo","Kohonen",",","Stephen","Grossberg",",","Kunihiko","Fukushima",",","Christoph","von","der","Malsburg",",","David","Willshaw",",","Shun-Ichi","Amari",",","Bernard","Widrow",",","John","Hopfield",",","Eduardo","R.","Caianiello",",","and","others","."],"labels":["O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","protein","academic_journal","scientist","country","chemical_element","discipline","organization","university","astronomical_object","event","person","award","theory","location","enzyme"]}
{"id":"433","dataset":"crossner_science","split":"test","instance":{"id":"433","prompt_labels":"The(O) others(O) :(O) William(B-scientist) Henry(I-scientist) Bragg(I-scientist) ((O) 1915(O) )(O) and(O) William(B-scientist) Lawrence(I-scientist) Bragg(I-scientist) ((O) 1915(O) )(O) ;(O) J.(B-scientist) J.(I-scientist) Thomson(I-scientist) ((O) 1906(O) )(O) and(O) George(B-scientist) Paget(I-scientist) Thomson(I-scientist) ((O) 1937(O) )(O) ;(O) and(O) Manne(B-scientist) Siegbahn(I-scientist) ((O) 1924(O) )(O) and(O) Kai(B-scientist) M.(I-scientist) Siegbahn(I-scientist) ((O) 1981(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, event, chemical compound, country, chemical element, person, theory, enzyme, organization, university, protein, location, award, discipline, academic journal, astronomical object and O.\nSentence: The others : William Henry Bragg ( 1915 ) and William Lawrence Bragg ( 1915 ) ; J. J. Thomson ( 1906 ) and George Paget Thomson ( 1937 ) ; and Manne Siegbahn ( 1924 ) and Kai M. Siegbahn ( 1981 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","others",":","William","Henry","Bragg","(","1915",")","and","William","Lawrence","Bragg","(","1915",")",";","J.","J.","Thomson","(","1906",")","and","George","Paget","Thomson","(","1937",")",";","and","Manne","Siegbahn","(","1924",")","and","Kai","M.","Siegbahn","(","1981",")","."],"labels":["O","O","O","B-scientist","I-scientist","I-scientist","O","O","O","O","B-scientist","I-scientist","I-scientist","O","O","O","O","B-scientist","I-scientist","I-scientist","O","O","O","O","B-scientist","I-scientist","I-scientist","O","O","O","O","O","B-scientist","I-scientist","O","O","O","O","B-scientist","I-scientist","I-scientist","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","event","chemical_compound","country","chemical_element","person","theory","enzyme","organization","university","protein","location","award","discipline","academic_journal","astronomical_object"]}
{"id":"434","dataset":"crossner_science","split":"test","instance":{"id":"434","prompt_labels":"On(O) Broadway(B-organization) ,(O) the(O) song(O) was(O) originally(O) performed(O) by(O) Chita(B-person) Rivera(I-person) ,(O) with(O) Candy(B-person) Brown(I-person) ,(O) Cheryl(B-person) Clark(I-person) ,(O) Graciela(B-person) Daniele(I-person) ,(O) Michon(B-person) Peacock(I-person) and(O) Pamela(B-person) Sousa(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, organization, astronomical object, person, enzyme, award, location, theory, country, protein, event, chemical element, academic journal, scientist, chemical compound, university and O.\nSentence: On Broadway , the song was originally performed by Chita Rivera , with Candy Brown , Cheryl Clark , Graciela Daniele , Michon Peacock and Pamela Sousa .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","Broadway",",","the","song","was","originally","performed","by","Chita","Rivera",",","with","Candy","Brown",",","Cheryl","Clark",",","Graciela","Daniele",",","Michon","Peacock","and","Pamela","Sousa","."],"labels":["O","B-organization","O","O","O","O","O","O","O","B-person","I-person","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["discipline","organization","astronomical_object","person","enzyme","award","location","theory","country","protein","event","chemical_element","academic_journal","scientist","chemical_compound","university"]}
{"id":"435","dataset":"crossner_science","split":"test","instance":{"id":"435","prompt_labels":"Sydney(B-scientist) Goldstein(I-scientist) ,(O) who(O) also(O) wrote(O) the(O) Royal(B-organization) Society(I-organization) memoir(O) for(O) Krmn(B-scientist) ,(O) reviewed(O) the(O) autobiography(O) Sydney(B-scientist) Goldstein(I-scientist) ((O) 1968(O) )(O) Journal(B-academic journal) of(I-academic journal) Fluid(I-academic journal) Mechanics(I-academic journal) 33(O) ((O) 2(O) )(O) and(O) remembered(O) an(O) eminent(O) engineer(O) and(O) scientist(O) ,(O) warm-hearted(O) and(O) witty(O) ,(O) much(O) traveled(O) ,(O) well-known(O) by(O) many(O) ,(O) devoted(O) to(O) international(O) collaboration(O) ,(O) who(O) ,(O) in(O) his(O) own(O) words(O) ,(O) as(O) a(O) scientist(O) found(O) the(O) military(O) '(O) the(O) most(O) comfortable(O) group(O) to(O) deal(O) with(O) '(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, discipline, event, country, scientist, award, person, astronomical object, chemical element, university, protein, location, academic journal, organization, enzyme, theory and O.\nSentence: Sydney Goldstein , who also wrote the Royal Society memoir for Krmn , reviewed the autobiography Sydney Goldstein ( 1968 ) Journal of Fluid Mechanics 33 ( 2 ) and remembered an eminent engineer and scientist , warm-hearted and witty , much traveled , well-known by many , devoted to international collaboration , who , in his own words , as a scientist found the military ' the most comfortable group to deal with ' .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Sydney","Goldstein",",","who","also","wrote","the","Royal","Society","memoir","for","Krmn",",","reviewed","the","autobiography","Sydney","Goldstein","(","1968",")","Journal","of","Fluid","Mechanics","33","(","2",")","and","remembered","an","eminent","engineer","and","scientist",",","warm-hearted","and","witty",",","much","traveled",",","well-known","by","many",",","devoted","to","international","collaboration",",","who",",","in","his","own","words",",","as","a","scientist","found","the","military","'","the","most","comfortable","group","to","deal","with","'","."],"labels":["B-scientist","I-scientist","O","O","O","O","O","B-organization","I-organization","O","O","B-scientist","O","O","O","O","B-scientist","I-scientist","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","discipline","event","country","scientist","award","person","astronomical_object","chemical_element","university","protein","location","academic_journal","organization","enzyme","theory"]}
{"id":"440","dataset":"crossner_science","split":"test","instance":{"id":"440","prompt_labels":"Investigations(O) conducted(O) at(O) the(O) site(O) by(O) William(B-scientist) Duncan(I-scientist) Strong(I-scientist) ,(O) Waldo(B-scientist) Rudolph(I-scientist) Wedel(I-scientist) ,(O) and(O) A.(B-scientist) T.(I-scientist) Hill(I-scientist) were(O) instrumental(O) in(O) the(O) development(O) of(O) Great(O) Plains(O) archaeology(O) ,(O) and(O) of(O) Pawnee(O) archaeology(O) in(O) particular(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, academic journal, award, person, chemical compound, theory, astronomical object, event, scientist, organization, chemical element, country, protein, discipline, location, enzyme and O.\nSentence: Investigations conducted at the site by William Duncan Strong , Waldo Rudolph Wedel , and A. T. Hill were instrumental in the development of Great Plains archaeology , and of Pawnee archaeology in particular .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Investigations","conducted","at","the","site","by","William","Duncan","Strong",",","Waldo","Rudolph","Wedel",",","and","A.","T.","Hill","were","instrumental","in","the","development","of","Great","Plains","archaeology",",","and","of","Pawnee","archaeology","in","particular","."],"labels":["O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","O","B-scientist","I-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","academic_journal","award","person","chemical_compound","theory","astronomical_object","event","scientist","organization","chemical_element","country","protein","discipline","location","enzyme"]}
{"id":"441","dataset":"crossner_science","split":"test","instance":{"id":"441","prompt_labels":"In(O) fact(O) ,(O) when(O) math(O) E(O) /(O) math(O) belongs(O) to(O) the(O) spectrum(O) ,(O) the(O) inequality(O) becomes(O) an(O) equality(O) ((O) the(O) Aubry-Andr(B-theory) formula(I-theory) )(O) ,(O) proved(O) by(O) Jean(B-scientist) Bourgain(I-scientist) and(O) Svetlana(B-scientist) Jitomirskaya(I-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, enzyme, university, location, award, event, discipline, academic journal, protein, scientist, theory, person, chemical element, country, chemical compound, organization and O.\nSentence: In fact , when math E / math belongs to the spectrum , the inequality becomes an equality ( the Aubry-Andr formula ) , proved by Jean Bourgain and Svetlana Jitomirskaya .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","fact",",","when","math","E","/","math","belongs","to","the","spectrum",",","the","inequality","becomes","an","equality","(","the","Aubry-Andr","formula",")",",","proved","by","Jean","Bourgain","and","Svetlana","Jitomirskaya","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-theory","I-theory","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","enzyme","university","location","award","event","discipline","academic_journal","protein","scientist","theory","person","chemical_element","country","chemical_compound","organization"]}
{"id":"443","dataset":"crossner_science","split":"test","instance":{"id":"443","prompt_labels":"There(O) are(O) four(O) community(O) colleges(O) in(O) the(O) valley(O) ;(O) Los(B-university) Angeles(I-university) Valley(I-university) College(I-university) in(O) Valley(B-location) Glen(I-location) ,(O) Los(B-university) Angeles(I-university) Mission(I-university) College(I-university) in(O) Sylmar(B-location) ,(O) Los(B-university) Angeles(I-university) Pierce(I-university) College(I-university) in(O) Woodland(B-location) Hills(I-location) and(O) Glendale(B-university) Community(I-university) College(I-university) in(O) the(O) College(B-location) Hills(I-location) neighborhood(I-location) of(I-location) Glendale(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, astronomical object, scientist, chemical compound, discipline, event, location, person, organization, enzyme, award, country, academic journal, theory, chemical element, university and O.\nSentence: There are four community colleges in the valley ; Los Angeles Valley College in Valley Glen , Los Angeles Mission College in Sylmar , Los Angeles Pierce College in Woodland Hills and Glendale Community College in the College Hills neighborhood of Glendale .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["There","are","four","community","colleges","in","the","valley",";","Los","Angeles","Valley","College","in","Valley","Glen",",","Los","Angeles","Mission","College","in","Sylmar",",","Los","Angeles","Pierce","College","in","Woodland","Hills","and","Glendale","Community","College","in","the","College","Hills","neighborhood","of","Glendale","."],"labels":["O","O","O","O","O","O","O","O","O","B-university","I-university","I-university","I-university","O","B-location","I-location","O","B-university","I-university","I-university","I-university","O","B-location","O","B-university","I-university","I-university","I-university","O","B-location","I-location","O","B-university","I-university","I-university","O","O","B-location","I-location","I-location","I-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["protein","astronomical_object","scientist","chemical_compound","discipline","event","location","person","organization","enzyme","award","country","academic_journal","theory","chemical_element","university"]}
{"id":"445","dataset":"crossner_science","split":"test","instance":{"id":"445","prompt_labels":"To(O) occur(O) ,(O) this(O) lobopodia-based(O) fibroblast(O) migration(O) needs(O) Nesprin(B-protein) ,(O) Integrin(B-protein) ,(O) RhoA(B-protein) ,(O) ROCK(B-protein) and(O) myosin(B-protein) II(I-protein) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, award, theory, scientist, enzyme, location, country, chemical element, event, discipline, academic journal, organization, astronomical object, university, protein, person and O.\nSentence: To occur , this lobopodia-based fibroblast migration needs Nesprin , Integrin , RhoA , ROCK and myosin II .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["To","occur",",","this","lobopodia-based","fibroblast","migration","needs","Nesprin",",","Integrin",",","RhoA",",","ROCK","and","myosin","II","."],"labels":["O","O","O","O","O","O","O","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","I-protein","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","award","theory","scientist","enzyme","location","country","chemical_element","event","discipline","academic_journal","organization","astronomical_object","university","protein","person"]}
{"id":"446","dataset":"crossner_science","split":"test","instance":{"id":"446","prompt_labels":"Martin(B-scientist) Karplus(I-scientist) ,(O) Michael(B-scientist) Levitt(I-scientist) and(O) Arieh(B-scientist) Warshel(I-scientist) received(O) the(O) 2013(O) Nobel(B-award) Prize(I-award) in(I-award) Chemistry(I-award) for(O) the(O) development(O) of(O) multiscale(O) models(O) for(O) complex(O) chemical(O) systems(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, award, person, enzyme, astronomical object, country, protein, academic journal, theory, event, chemical element, organization, scientist, discipline, university, location and O.\nSentence: Martin Karplus , Michael Levitt and Arieh Warshel received the 2013 Nobel Prize in Chemistry for the development of multiscale models for complex chemical systems .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Martin","Karplus",",","Michael","Levitt","and","Arieh","Warshel","received","the","2013","Nobel","Prize","in","Chemistry","for","the","development","of","multiscale","models","for","complex","chemical","systems","."],"labels":["B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","B-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","award","person","enzyme","astronomical_object","country","protein","academic_journal","theory","event","chemical_element","organization","scientist","discipline","university","location"]}
{"id":"447","dataset":"crossner_science","split":"test","instance":{"id":"447","prompt_labels":"The(O) group(O) included(O) the(O) physicists(O) Walther(B-scientist) Bothe(I-scientist) ,(O) Robert(B-scientist) Dpel(I-scientist) ,(O) Hans(B-scientist) Geiger(I-scientist) ,(O) Wolfgang(B-scientist) Gentner(I-scientist) ,(O) Wilhelm(B-scientist) Hanle(I-scientist) ,(O) Gerhard(B-scientist) Hoffmann(I-scientist) ,(O) and(O) Joos(B-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, chemical element, person, event, location, discipline, astronomical object, chemical compound, theory, enzyme, academic journal, protein, scientist, country, university and O.\nSentence: The group included the physicists Walther Bothe , Robert Dpel , Hans Geiger , Wolfgang Gentner , Wilhelm Hanle , Gerhard Hoffmann , and Joos .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","group","included","the","physicists","Walther","Bothe",",","Robert","Dpel",",","Hans","Geiger",",","Wolfgang","Gentner",",","Wilhelm","Hanle",",","Gerhard","Hoffmann",",","and","Joos","."],"labels":["O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-scientist","O"],"target_index":null,"target_label":null},"label_list":["organization","award","chemical_element","person","event","location","discipline","astronomical_object","chemical_compound","theory","enzyme","academic_journal","protein","scientist","country","university"]}
{"id":"448","dataset":"crossner_science","split":"test","instance":{"id":"448","prompt_labels":"In(O) 1991(O) ,(O) he(O) obtained(O) a(O) Doctor(O) of(O) Science(O) degree(O) in(O) Physics(B-discipline) and(O) Mathematics(B-discipline) from(O) Moscow(B-university) State(I-university) University(I-university) and(O) moved(O) to(O) Tokyo(B-location) ((O) Japan(B-country) )(O) in(O) 1993(O) to(O) teach(O) astronomy(B-discipline) in(O) Hitotsubashi(B-university) University(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, academic journal, scientist, location, protein, award, chemical compound, country, enzyme, event, theory, person, discipline, university, astronomical object, organization and O.\nSentence: In 1991 , he obtained a Doctor of Science degree in Physics and Mathematics from Moscow State University and moved to Tokyo ( Japan ) in 1993 to teach astronomy in Hitotsubashi University .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1991",",","he","obtained","a","Doctor","of","Science","degree","in","Physics","and","Mathematics","from","Moscow","State","University","and","moved","to","Tokyo","(","Japan",")","in","1993","to","teach","astronomy","in","Hitotsubashi","University","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-discipline","O","B-discipline","O","B-university","I-university","I-university","O","O","O","B-location","O","B-country","O","O","O","O","O","B-discipline","O","B-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","academic_journal","scientist","location","protein","award","chemical_compound","country","enzyme","event","theory","person","discipline","university","astronomical_object","organization"]}
{"id":"449","dataset":"crossner_science","split":"test","instance":{"id":"449","prompt_labels":"For(O) his(O) discovery(O) of(O) the(O) neutron(O) ,(O) Chadwick(B-scientist) was(O) awarded(O) the(O) Hughes(B-award) Medal(I-award) by(O) the(O) Royal(B-organization) Society(I-organization) in(O) 1932(O) ,(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Physics(I-award) in(O) 1935(O) ,(O) the(O) Copley(B-award) Medal(I-award) in(O) 1950(O) and(O) the(O) Franklin(B-award) Medal(I-award) in(O) 1951(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, academic journal, event, scientist, award, protein, enzyme, chemical compound, chemical element, discipline, person, astronomical object, location, organization, country, theory and O.\nSentence: For his discovery of the neutron , Chadwick was awarded the Hughes Medal by the Royal Society in 1932 , the Nobel Prize in Physics in 1935 , the Copley Medal in 1950 and the Franklin Medal in 1951 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","his","discovery","of","the","neutron",",","Chadwick","was","awarded","the","Hughes","Medal","by","the","Royal","Society","in","1932",",","the","Nobel","Prize","in","Physics","in","1935",",","the","Copley","Medal","in","1950","and","the","Franklin","Medal","in","1951","."],"labels":["O","O","O","O","O","O","O","B-scientist","O","O","O","B-award","I-award","O","O","B-organization","I-organization","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O","O","B-award","I-award","O","O","O","O","B-award","I-award","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","academic_journal","event","scientist","award","protein","enzyme","chemical_compound","chemical_element","discipline","person","astronomical_object","location","organization","country","theory"]}
{"id":"450","dataset":"crossner_science","split":"test","instance":{"id":"450","prompt_labels":"Wilson(B-person) won(O) gold(O) medals(O) in(O) the(O) 800(O) metres(O) at(O) both(O) the(O) 2011(B-event) World(I-event) Youth(I-event) Championships(I-event) in(I-event) Athletics(I-event) and(O) the(O) 2012(B-event) World(I-event) Junior(I-event) Championships(I-event) in(I-event) Athletics(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, event, astronomical object, chemical element, country, academic journal, location, organization, chemical compound, protein, enzyme, discipline, scientist, award, person, theory and O.\nSentence: Wilson won gold medals in the 800 metres at both the 2011 World Youth Championships in Athletics and the 2012 World Junior Championships in Athletics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Wilson","won","gold","medals","in","the","800","metres","at","both","the","2011","World","Youth","Championships","in","Athletics","and","the","2012","World","Junior","Championships","in","Athletics","."],"labels":["B-person","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","I-event","O","O","B-event","I-event","I-event","I-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["university","event","astronomical_object","chemical_element","country","academic_journal","location","organization","chemical_compound","protein","enzyme","discipline","scientist","award","person","theory"]}
{"id":"452","dataset":"crossner_science","split":"test","instance":{"id":"452","prompt_labels":"Chicago(B-location) 's(O) cultural(O) heavyweights(O) ,(O) such(O) as(O) the(O) Art(B-organization) Institute(I-organization) of(I-organization) Chicago(I-organization) ,(O) the(O) Goodman(B-location) Theatre(I-location) ,(O) the(O) Chicago(B-location) Theatre(I-location) ,(O) the(O) Lyric(B-location) Opera(I-location) at(O) the(O) Civic(B-location) Opera(I-location) House(I-location) building(O) ,(O) and(O) the(O) Chicago(B-location) Symphony(I-location) Orchestra(I-location) ,(O) are(O) also(O) in(O) this(O) area(O) ,(O) as(O) is(O) the(O) historic(O) Palmer(B-location) House(I-location) Hilton(I-location) hotel(I-location) ,(O) found(O) on(O) East(B-location) Monroe(I-location) Street(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, theory, discipline, organization, person, event, university, protein, academic journal, chemical compound, location, astronomical object, enzyme, country, chemical element, scientist and O.\nSentence: Chicago 's cultural heavyweights , such as the Art Institute of Chicago , the Goodman Theatre , the Chicago Theatre , the Lyric Opera at the Civic Opera House building , and the Chicago Symphony Orchestra , are also in this area , as is the historic Palmer House Hilton hotel , found on East Monroe Street .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Chicago","'s","cultural","heavyweights",",","such","as","the","Art","Institute","of","Chicago",",","the","Goodman","Theatre",",","the","Chicago","Theatre",",","the","Lyric","Opera","at","the","Civic","Opera","House","building",",","and","the","Chicago","Symphony","Orchestra",",","are","also","in","this","area",",","as","is","the","historic","Palmer","House","Hilton","hotel",",","found","on","East","Monroe","Street","."],"labels":["B-location","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-location","I-location","O","O","B-location","I-location","O","O","B-location","I-location","O","O","B-location","I-location","I-location","O","O","O","O","B-location","I-location","I-location","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","I-location","O","O","O","B-location","I-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["award","theory","discipline","organization","person","event","university","protein","academic_journal","chemical_compound","location","astronomical_object","enzyme","country","chemical_element","scientist"]}
{"id":"454","dataset":"crossner_science","split":"test","instance":{"id":"454","prompt_labels":"He(O) has(O) also(O) received(O) honorary(O) doctorates(O) from(O) the(O) universities(B-university) of(I-university) Newcastle(I-university) ,(O) Surrey(B-location) ,(O) Tel(B-university) Aviv(I-university) University(I-university) ,(O) McGill(B-location) ,(O) Simon(B-university) Fraser(I-university) University(I-university) and(O) the(O) University(B-university) of(I-university) Troms(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, astronomical object, award, organization, chemical element, chemical compound, discipline, enzyme, protein, university, person, theory, event, scientist, academic journal and O.\nSentence: He has also received honorary doctorates from the universities of Newcastle , Surrey , Tel Aviv University , McGill , Simon Fraser University and the University of Troms .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","has","also","received","honorary","doctorates","from","the","universities","of","Newcastle",",","Surrey",",","Tel","Aviv","University",",","McGill",",","Simon","Fraser","University","and","the","University","of","Troms","."],"labels":["O","O","O","O","O","O","O","O","B-university","I-university","I-university","O","B-location","O","B-university","I-university","I-university","O","B-location","O","B-university","I-university","I-university","O","O","B-university","I-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["location","country","astronomical_object","award","organization","chemical_element","chemical_compound","discipline","enzyme","protein","university","person","theory","event","scientist","academic_journal"]}
{"id":"456","dataset":"crossner_science","split":"test","instance":{"id":"456","prompt_labels":"A(O) conjunction(O) of(O) Venus(B-astronomical object) and(O) Jupiter(B-astronomical object) occurred(O) on(O) 1(O) December(O) 2008(O) ,(O) and(O) several(O) hours(O) later(O) both(O) planets(O) separately(O) reached(O) conjunction(O) with(O) the(O) crescent(O) Moon(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, university, chemical element, event, theory, organization, academic journal, chemical compound, award, person, enzyme, scientist, astronomical object, location, country, discipline and O.\nSentence: A conjunction of Venus and Jupiter occurred on 1 December 2008 , and several hours later both planets separately reached conjunction with the crescent Moon .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","conjunction","of","Venus","and","Jupiter","occurred","on","1","December","2008",",","and","several","hours","later","both","planets","separately","reached","conjunction","with","the","crescent","Moon","."],"labels":["O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["protein","university","chemical_element","event","theory","organization","academic_journal","chemical_compound","award","person","enzyme","scientist","astronomical_object","location","country","discipline"]}
{"id":"460","dataset":"crossner_science","split":"test","instance":{"id":"460","prompt_labels":"San(B-organization) Juan(I-organization) de(I-organization) Dios(I-organization) Hospital(I-organization) ,(O) Philippine(B-organization) Children(I-organization) 's(I-organization) Medical(I-organization) Center(I-organization) ,(O) San(B-organization) Lazaro(I-organization) Hospital(I-organization) ,(O) Ospital(B-organization) ng(I-organization) Muntinlupa(I-organization) ,(O) Lung(B-organization) Center(I-organization) of(I-organization) the(I-organization) Philippines(I-organization) ,(O) the(O) National(B-organization) Kidney(I-organization) and(I-organization) Transplant(I-organization) Institute(I-organization) ,(O) Manila(B-organization) Adventist(I-organization) Medical(I-organization) Center(I-organization) and(O) Las(B-organization) Pias(I-organization) Hospital(I-organization) also(O) made(O) steps(O) to(O) ban(O) the(O) toxic(O) chemical(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, university, discipline, chemical compound, protein, person, organization, academic journal, chemical element, country, astronomical object, scientist, event, enzyme, award, location and O.\nSentence: San Juan de Dios Hospital , Philippine Children 's Medical Center , San Lazaro Hospital , Ospital ng Muntinlupa , Lung Center of the Philippines , the National Kidney and Transplant Institute , Manila Adventist Medical Center and Las Pias Hospital also made steps to ban the toxic chemical .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["San","Juan","de","Dios","Hospital",",","Philippine","Children","'s","Medical","Center",",","San","Lazaro","Hospital",",","Ospital","ng","Muntinlupa",",","Lung","Center","of","the","Philippines",",","the","National","Kidney","and","Transplant","Institute",",","Manila","Adventist","Medical","Center","and","Las","Pias","Hospital","also","made","steps","to","ban","the","toxic","chemical","."],"labels":["B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["theory","university","discipline","chemical_compound","protein","person","organization","academic_journal","chemical_element","country","astronomical_object","scientist","event","enzyme","award","location"]}
{"id":"461","dataset":"crossner_science","split":"test","instance":{"id":"461","prompt_labels":"A(O) similar(O) example(O) is(O) given(O) by(O) the(O) Senegalese(O) sole(O) ((O) Solea(O) senegalensis(O) )(O) ,(O) which(O) ,(O) when(O) acclimated(O) to(O) temperatures(O) of(O) 26(O) (O) C(O) ,(O) produced(O) a(O) significantly(O) higher(O) amount(O) of(O) taurine(B-chemical compound) ,(O) Glutamic(B-chemical compound) acid(I-chemical compound) ,(O) Gamma-Aminobutyric(B-chemical compound) acid(I-chemical compound) and(O) glycine(B-chemical compound) compared(O) to(O) acclimation(O) to(O) 12(O) (O) C.(O) This(O) may(O) mean(O) that(O) the(O) aforementioned(O) compounds(O) aid(O) in(O) antioxidant(B-theory) defense(I-theory) ,(O) osmoregulatory(O) processes(O) ,(O) or(O) energetic(O) purposes(O) at(O) these(O) temperatures(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, country, university, organization, award, scientist, astronomical object, discipline, chemical compound, person, chemical element, academic journal, protein, location, event, theory and O.\nSentence: A similar example is given by the Senegalese sole ( Solea senegalensis ) , which , when acclimated to temperatures of 26  C , produced a significantly higher amount of taurine , Glutamic acid , Gamma-Aminobutyric acid and glycine compared to acclimation to 12  C. This may mean that the aforementioned compounds aid in antioxidant defense , osmoregulatory processes , or energetic purposes at these temperatures .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","similar","example","is","given","by","the","Senegalese","sole","(","Solea","senegalensis",")",",","which",",","when","acclimated","to","temperatures","of","26","","C",",","produced","a","significantly","higher","amount","of","taurine",",","Glutamic","acid",",","Gamma-Aminobutyric","acid","and","glycine","compared","to","acclimation","to","12","","C.","This","may","mean","that","the","aforementioned","compounds","aid","in","antioxidant","defense",",","osmoregulatory","processes",",","or","energetic","purposes","at","these","temperatures","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","O","B-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","O","B-chemical compound","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-theory","I-theory","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["enzyme","country","university","organization","award","scientist","astronomical_object","discipline","chemical_compound","person","chemical_element","academic_journal","protein","location","event","theory"]}
{"id":"462","dataset":"crossner_science","split":"test","instance":{"id":"462","prompt_labels":"Richard(B-scientist) Laurence(I-scientist) Millington(I-scientist) Synge(I-scientist) Fellow(B-award) of(I-award) the(I-award) Royal(I-award) Society(I-award) FRSE(B-organization) FRIC(B-organization) FRSC(B-organization) MRIA(B-organization) ((O) Liverpool(B-location) ,(O) 28(O) October(O) 1914(O) -(O) Norwich(B-location) ,(O) 18(O) August(O) 1994(O) )(O) was(O) a(O) British(O) biochemist(O) ,(O) and(O) shared(O) the(O) 1952(O) Nobel(B-award) Prize(I-award) in(I-award) Chemistry(I-award) for(O) the(O) invention(O) of(O) partition(O) chromatography(O) with(O) Archer(B-scientist) Martin(I-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, event, academic journal, astronomical object, theory, protein, country, scientist, location, chemical compound, chemical element, university, enzyme, organization, discipline and O.\nSentence: Richard Laurence Millington Synge Fellow of the Royal Society FRSE FRIC FRSC MRIA ( Liverpool , 28 October 1914 - Norwich , 18 August 1994 ) was a British biochemist , and shared the 1952 Nobel Prize in Chemistry for the invention of partition chromatography with Archer Martin .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Richard","Laurence","Millington","Synge","Fellow","of","the","Royal","Society","FRSE","FRIC","FRSC","MRIA","(","Liverpool",",","28","October","1914","-","Norwich",",","18","August","1994",")","was","a","British","biochemist",",","and","shared","the","1952","Nobel","Prize","in","Chemistry","for","the","invention","of","partition","chromatography","with","Archer","Martin","."],"labels":["B-scientist","I-scientist","I-scientist","I-scientist","B-award","I-award","I-award","I-award","I-award","B-organization","B-organization","B-organization","B-organization","O","B-location","O","O","O","O","O","B-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O","O","O","O","O","B-scientist","I-scientist","O"],"target_index":null,"target_label":null},"label_list":["award","person","event","academic_journal","astronomical_object","theory","protein","country","scientist","location","chemical_compound","chemical_element","university","enzyme","organization","discipline"]}
{"id":"463","dataset":"crossner_science","split":"test","instance":{"id":"463","prompt_labels":"In(O) December(O) 2015(O) ,(O) the(O) Joint(B-organization) Working(I-organization) Party(I-organization) of(O) international(O) scientific(O) bodies(O) International(B-organization) Union(I-organization) of(I-organization) Pure(I-organization) and(I-organization) Applied(I-organization) Chemistry(I-organization) ((O) IUPAC(B-organization) )(O) and(O) International(B-organization) Union(I-organization) of(I-organization) Pure(I-organization) and(I-organization) Applied(I-organization) Physics(I-organization) ((O) IUPAP(B-organization) )(O) recognized(O) the(O) element(O) 's(O) discovery(O) and(O) assigned(O) the(O) priority(O) of(O) the(O) discovery(O) to(O) the(O) Dubna-Livermore(B-scientist) collaboration(O) ..(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, scientist, chemical element, person, country, astronomical object, enzyme, award, academic journal, event, location, theory, university, discipline, protein, chemical compound and O.\nSentence: In December 2015 , the Joint Working Party of international scientific bodies International Union of Pure and Applied Chemistry ( IUPAC ) and International Union of Pure and Applied Physics ( IUPAP ) recognized the element 's discovery and assigned the priority of the discovery to the Dubna-Livermore collaboration ..","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","December","2015",",","the","Joint","Working","Party","of","international","scientific","bodies","International","Union","of","Pure","and","Applied","Chemistry","(","IUPAC",")","and","International","Union","of","Pure","and","Applied","Physics","(","IUPAP",")","recognized","the","element","'s","discovery","and","assigned","the","priority","of","the","discovery","to","the","Dubna-Livermore","collaboration",".."],"labels":["O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","O","O"],"target_index":null,"target_label":null},"label_list":["organization","scientist","chemical_element","person","country","astronomical_object","enzyme","award","academic_journal","event","location","theory","university","discipline","protein","chemical_compound"]}
{"id":"467","dataset":"crossner_science","split":"test","instance":{"id":"467","prompt_labels":"Friedmann(B-location) Peak(I-location) ,(O) in(O) the(O) Darwin(B-location) Mountains(I-location) of(O) Antarctica(B-country) ,(O) where(O) she(O) co-discovered(O) endolith(O) ic(O) microorganisms(O) in(O) the(O) Beacon(B-location) sandstone(I-location) ,(O) is(O) named(O) after(O) her(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, astronomical object, university, enzyme, country, protein, chemical element, scientist, person, organization, chemical compound, academic journal, theory, event, discipline, award and O.\nSentence: Friedmann Peak , in the Darwin Mountains of Antarctica , where she co-discovered endolith ic microorganisms in the Beacon sandstone , is named after her .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Friedmann","Peak",",","in","the","Darwin","Mountains","of","Antarctica",",","where","she","co-discovered","endolith","ic","microorganisms","in","the","Beacon","sandstone",",","is","named","after","her","."],"labels":["B-location","I-location","O","O","O","B-location","I-location","O","B-country","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","astronomical_object","university","enzyme","country","protein","chemical_element","scientist","person","organization","chemical_compound","academic_journal","theory","event","discipline","award"]}
{"id":"468","dataset":"crossner_science","split":"test","instance":{"id":"468","prompt_labels":"Three(O) of(O) the(O) newly(O) confirmed(O) exoplanets(O) were(O) found(O) to(O) orbit(O) within(O) habitable(O) zone(O) s(O) of(O) their(O) related(O) star(O) s(O) :(O) two(O) of(O) the(O) three(O) ,(O) Kepler-438b(B-astronomical object) and(O) Kepler-442b(B-astronomical object) ,(O) are(O) near-Earth-size(O) and(O) likely(O) rocky(O) ;(O) the(O) third(O) ,(O) Kepler-440b(B-astronomical object) ,(O) is(O) a(O) super-Earth(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, discipline, university, scientist, enzyme, chemical compound, chemical element, organization, award, astronomical object, event, academic journal, country, theory, person, location and O.\nSentence: Three of the newly confirmed exoplanets were found to orbit within habitable zone s of their related star s : two of the three , Kepler-438b and Kepler-442b , are near-Earth-size and likely rocky ; the third , Kepler-440b , is a super-Earth .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Three","of","the","newly","confirmed","exoplanets","were","found","to","orbit","within","habitable","zone","s","of","their","related","star","s",":","two","of","the","three",",","Kepler-438b","and","Kepler-442b",",","are","near-Earth-size","and","likely","rocky",";","the","third",",","Kepler-440b",",","is","a","super-Earth","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["protein","discipline","university","scientist","enzyme","chemical_compound","chemical_element","organization","award","astronomical_object","event","academic_journal","country","theory","person","location"]}
{"id":"469","dataset":"crossner_science","split":"test","instance":{"id":"469","prompt_labels":"In(O) the(O) middle(O) of(O) the(O) century(O) ,(O) the(O) Boone(B-organization) and(I-organization) Crockett(I-organization) Club(I-organization) and(O) the(O) Safari(B-organization) Club(I-organization) International(I-organization) developed(O) complex(O) scoring(O) systems(O) based(O) on(O) various(O) dimensions(O) and(O) the(O) number(O) of(O) tines(O) or(O) points(O) ,(O) and(O) they(O) keep(O) extensive(O) records(O) of(O) high-scoring(O) antlers(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, astronomical object, academic journal, location, organization, discipline, protein, country, scientist, event, theory, university, person, enzyme, chemical compound, chemical element and O.\nSentence: In the middle of the century , the Boone and Crockett Club and the Safari Club International developed complex scoring systems based on various dimensions and the number of tines or points , and they keep extensive records of high-scoring antlers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","middle","of","the","century",",","the","Boone","and","Crockett","Club","and","the","Safari","Club","International","developed","complex","scoring","systems","based","on","various","dimensions","and","the","number","of","tines","or","points",",","and","they","keep","extensive","records","of","high-scoring","antlers","."],"labels":["O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","astronomical_object","academic_journal","location","organization","discipline","protein","country","scientist","event","theory","university","person","enzyme","chemical_compound","chemical_element"]}
{"id":"470","dataset":"crossner_science","split":"test","instance":{"id":"470","prompt_labels":"In(O) 1958(O) ,(O) George(B-scientist) Beadle(I-scientist) and(O) Edward(B-scientist) Tatum(I-scientist) received(O) the(O) Nobel(B-award) Prize(I-award) for(O) work(O) in(O) fungi(O) showing(O) that(O) one(B-theory) gene(I-theory) produces(I-theory) one(I-theory) enzyme(I-theory) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, astronomical object, chemical compound, chemical element, protein, location, enzyme, organization, scientist, university, country, academic journal, event, award, theory, person and O.\nSentence: In 1958 , George Beadle and Edward Tatum received the Nobel Prize for work in fungi showing that one gene produces one enzyme .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1958",",","George","Beadle","and","Edward","Tatum","received","the","Nobel","Prize","for","work","in","fungi","showing","that","one","gene","produces","one","enzyme","."],"labels":["O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-award","I-award","O","O","O","O","O","O","B-theory","I-theory","I-theory","I-theory","I-theory","O"],"target_index":null,"target_label":null},"label_list":["discipline","astronomical_object","chemical_compound","chemical_element","protein","location","enzyme","organization","scientist","university","country","academic_journal","event","award","theory","person"]}
{"id":"471","dataset":"crossner_science","split":"test","instance":{"id":"471","prompt_labels":"Oxidants(B-chemical compound) are(O) usually(O) chemical(O) substances(O) with(O) elements(O) in(O) high(O) oxidation(O) states(O) ,(O) or(O) else(O) highly(O) electronegative(O) elements(O) ((O) Osub2(B-chemical compound) /(I-chemical compound) sub(I-chemical compound) ,(O) Fluorine(B-chemical element) ,(O) Chlorine(B-chemical element) ,(O) Bromine(B-chemical element) )(O) that(O) can(O) gain(O) extra(O) electrons(O) by(O) oxidizing(O) another(O) substance(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, country, chemical compound, event, enzyme, person, protein, chemical element, organization, academic journal, location, university, astronomical object, scientist, award, theory and O.\nSentence: Oxidants are usually chemical substances with elements in high oxidation states , or else highly electronegative elements ( Osub2 / sub , Fluorine , Chlorine , Bromine ) that can gain extra electrons by oxidizing another substance .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Oxidants","are","usually","chemical","substances","with","elements","in","high","oxidation","states",",","or","else","highly","electronegative","elements","(","Osub2","/","sub",",","Fluorine",",","Chlorine",",","Bromine",")","that","can","gain","extra","electrons","by","oxidizing","another","substance","."],"labels":["B-chemical compound","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","I-chemical compound","I-chemical compound","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["discipline","country","chemical_compound","event","enzyme","person","protein","chemical_element","organization","academic_journal","location","university","astronomical_object","scientist","award","theory"]}
{"id":"472","dataset":"crossner_science","split":"test","instance":{"id":"472","prompt_labels":"While(O) there(O) ,(O) Bugs(O) meets(O) Marvin(O) the(O) Martian(O) who(O) is(O) trying(O) to(O) blow(O) up(O) Earth(B-astronomical object) with(O) the(O) Illudium(O) Q-36(O) Explosive(O) Space(O) Modulator(O) ((O) in(O) reality(O) a(O) stick(O) of(O) dynamite(O) ;(O) this(O) is(O) the(O) same(O) device(O) he(O) tried(O) to(O) use(O) in(O) his(O) debut(O) short(O) Haredevil(O) Hare(O) ,(O) though(O) that(O) Space(O) Modulator(O) was(O) Uranium(B-astronomical object) rather(O) than(O) Illudium(O) )(O) because(O) it(O) obstructs(O) his(O) view(O) of(O) Venus(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, scientist, astronomical object, country, person, academic journal, theory, enzyme, protein, award, organization, university, event, chemical compound, discipline, chemical element and O.\nSentence: While there , Bugs meets Marvin the Martian who is trying to blow up Earth with the Illudium Q-36 Explosive Space Modulator ( in reality a stick of dynamite ; this is the same device he tried to use in his debut short Haredevil Hare , though that Space Modulator was Uranium rather than Illudium ) because it obstructs his view of Venus .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["While","there",",","Bugs","meets","Marvin","the","Martian","who","is","trying","to","blow","up","Earth","with","the","Illudium","Q-36","Explosive","Space","Modulator","(","in","reality","a","stick","of","dynamite",";","this","is","the","same","device","he","tried","to","use","in","his","debut","short","Haredevil","Hare",",","though","that","Space","Modulator","was","Uranium","rather","than","Illudium",")","because","it","obstructs","his","view","of","Venus","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["location","scientist","astronomical_object","country","person","academic_journal","theory","enzyme","protein","award","organization","university","event","chemical_compound","discipline","chemical_element"]}
{"id":"474","dataset":"crossner_science","split":"test","instance":{"id":"474","prompt_labels":"The(O) next(O) year(O) ,(O) in(O) 1995(O) ,(O) a(O) film(O) adaptation(O) of(O) the(O) book(O) ,(O) Apollo(O) 13(O) ,(O) was(O) released(O) ,(O) directed(O) by(O) Ron(B-person) Howard(I-person) and(O) starring(O) Tom(B-person) Hanks(I-person) as(O) Lovell(B-person) ,(O) Bill(B-person) Paxton(I-person) as(O) Haise(B-person) ,(O) Kevin(B-person) Bacon(I-person) as(O) Swigert(B-person) ,(O) Gary(B-person) Sinise(I-person) as(O) Mattingly(B-person) ,(O) Ed(B-person) Harris(I-person) as(O) Kranz(B-person) ,(O) and(O) Kathleen(B-person) Quinlan(I-person) as(O) Marilyn(B-person) Lovell(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, award, chemical compound, organization, astronomical object, event, discipline, country, university, academic journal, enzyme, location, person, scientist, chemical element, theory and O.\nSentence: The next year , in 1995 , a film adaptation of the book , Apollo 13 , was released , directed by Ron Howard and starring Tom Hanks as Lovell , Bill Paxton as Haise , Kevin Bacon as Swigert , Gary Sinise as Mattingly , Ed Harris as Kranz , and Kathleen Quinlan as Marilyn Lovell .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","next","year",",","in","1995",",","a","film","adaptation","of","the","book",",","Apollo","13",",","was","released",",","directed","by","Ron","Howard","and","starring","Tom","Hanks","as","Lovell",",","Bill","Paxton","as","Haise",",","Kevin","Bacon","as","Swigert",",","Gary","Sinise","as","Mattingly",",","Ed","Harris","as","Kranz",",","and","Kathleen","Quinlan","as","Marilyn","Lovell","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","B-person","I-person","O","B-person","O","B-person","I-person","O","B-person","O","B-person","I-person","O","B-person","O","B-person","I-person","O","B-person","O","B-person","I-person","O","B-person","O","O","B-person","I-person","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["protein","award","chemical_compound","organization","astronomical_object","event","discipline","country","university","academic_journal","enzyme","location","person","scientist","chemical_element","theory"]}
{"id":"476","dataset":"crossner_science","split":"test","instance":{"id":"476","prompt_labels":"The(O) awards(O) that(O) Mayr(B-scientist) received(O) include(O) the(O) National(B-award) Medal(I-award) of(I-award) Science(I-award) ,(O) the(O) Balzan(B-award) Prize(I-award) ,(O) the(O) Sarton(B-award) Medal(I-award) of(I-award) the(I-award) History(I-award) of(I-award) Science(I-award) Society(I-award) ,(O) the(O) International(B-award) Prize(I-award) for(I-award) Biology(I-award) ,(O) the(O) Loye(B-award) and(I-award) Alden(I-award) Miller(I-award) Research(I-award) Award(I-award) ,(O) and(O) the(O) Lewis(B-award) Thomas(I-award) Prize(I-award) for(I-award) Writing(I-award) about(I-award) Science(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, theory, chemical element, astronomical object, academic journal, enzyme, event, location, award, person, scientist, university, country, organization, chemical compound, protein and O.\nSentence: The awards that Mayr received include the National Medal of Science , the Balzan Prize , the Sarton Medal of the History of Science Society , the International Prize for Biology , the Loye and Alden Miller Research Award , and the Lewis Thomas Prize for Writing about Science .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","awards","that","Mayr","received","include","the","National","Medal","of","Science",",","the","Balzan","Prize",",","the","Sarton","Medal","of","the","History","of","Science","Society",",","the","International","Prize","for","Biology",",","the","Loye","and","Alden","Miller","Research","Award",",","and","the","Lewis","Thomas","Prize","for","Writing","about","Science","."],"labels":["O","O","O","B-scientist","O","O","O","B-award","I-award","I-award","I-award","O","O","B-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["discipline","theory","chemical_element","astronomical_object","academic_journal","enzyme","event","location","award","person","scientist","university","country","organization","chemical_compound","protein"]}
{"id":"477","dataset":"crossner_science","split":"test","instance":{"id":"477","prompt_labels":"In(O) 1893(O) he(O) received(O) the(O) Royal(B-organization) Society(I-organization) '(O) s(O) Copley(B-award) Medal(I-award) ,(O) then(O) the(O) most(O) prestigious(O) scientific(O) prize(O) in(O) the(O) world(O) ,(O) for(O) his(O) researches(O) and(O) discoveries(O) in(O) physical(B-discipline) science(I-discipline) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, astronomical object, award, chemical compound, discipline, protein, scientist, location, person, chemical element, event, university, theory, academic journal, enzyme, country and O.\nSentence: In 1893 he received the Royal Society ' s Copley Medal , then the most prestigious scientific prize in the world , for his researches and discoveries in physical science .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1893","he","received","the","Royal","Society","'","s","Copley","Medal",",","then","the","most","prestigious","scientific","prize","in","the","world",",","for","his","researches","and","discoveries","in","physical","science","."],"labels":["O","O","O","O","O","B-organization","I-organization","O","O","B-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-discipline","I-discipline","O"],"target_index":null,"target_label":null},"label_list":["organization","astronomical_object","award","chemical_compound","discipline","protein","scientist","location","person","chemical_element","event","university","theory","academic_journal","enzyme","country"]}
{"id":"478","dataset":"crossner_science","split":"test","instance":{"id":"478","prompt_labels":"Santos(B-scientist) 's(O) scientific(O) work(O) has(O) appeared(O) in(O) journals(O) such(O) as(O) Psychological(B-academic journal) Science(I-academic journal) ,(O) Animal(B-academic journal) Cognition(I-academic journal) ,(O) Developmental(B-academic journal) Science(I-academic journal) ,(O) Current(B-academic journal) Biology(I-academic journal) ,(O) Animal(B-academic journal) Behavior(I-academic journal) ,(I-academic journal) and(I-academic journal) Cognition(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, academic journal, astronomical object, chemical compound, event, chemical element, scientist, theory, discipline, location, country, organization, protein, award, university, enzyme and O.\nSentence: Santos 's scientific work has appeared in journals such as Psychological Science , Animal Cognition , Developmental Science , Current Biology , Animal Behavior , and Cognition .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Santos","'s","scientific","work","has","appeared","in","journals","such","as","Psychological","Science",",","Animal","Cognition",",","Developmental","Science",",","Current","Biology",",","Animal","Behavior",",","and","Cognition","."],"labels":["B-scientist","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["person","academic_journal","astronomical_object","chemical_compound","event","chemical_element","scientist","theory","discipline","location","country","organization","protein","award","university","enzyme"]}
{"id":"480","dataset":"crossner_science","split":"test","instance":{"id":"480","prompt_labels":"His(O) father(O) encouraged(O) him(O) to(O) apply(O) to(O) United(B-university) States(I-university) Military(I-university) Academy(I-university) ,(O) but(O) he(O) decided(O) to(O) enroll(O) in(O) the(O) United(B-university) States(I-university) Naval(I-university) Academy(I-university) instead(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, discipline, event, location, protein, chemical element, astronomical object, university, academic journal, theory, chemical compound, scientist, enzyme, award, organization, country and O.\nSentence: His father encouraged him to apply to United States Military Academy , but he decided to enroll in the United States Naval Academy instead .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","father","encouraged","him","to","apply","to","United","States","Military","Academy",",","but","he","decided","to","enroll","in","the","United","States","Naval","Academy","instead","."],"labels":["O","O","O","O","O","O","O","B-university","I-university","I-university","I-university","O","O","O","O","O","O","O","O","B-university","I-university","I-university","I-university","O","O"],"target_index":null,"target_label":null},"label_list":["person","discipline","event","location","protein","chemical_element","astronomical_object","university","academic_journal","theory","chemical_compound","scientist","enzyme","award","organization","country"]}
{"id":"483","dataset":"crossner_science","split":"test","instance":{"id":"483","prompt_labels":"It(O) was(O) discovered(O) on(O) 22(O) February(O) 1938(O) ,(O) by(O) Finnish(O) astronomer(O) Yrj(B-scientist) Visl(I-scientist) at(O) Turku(B-scientist) Observatory(I-scientist) in(O) Southwest(B-location) Finland(I-location) ,(O) and(O) later(O) named(O) for(O) Johannes(B-scientist) Gabriel(I-scientist) Gran(I-scientist) ,(O) rector(O) of(O) the(O) University(B-university) of(I-university) Turku(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, organization, chemical element, enzyme, event, university, theory, protein, location, discipline, scientist, country, astronomical object, person, academic journal, award and O.\nSentence: It was discovered on 22 February 1938 , by Finnish astronomer Yrj Visl at Turku Observatory in Southwest Finland , and later named for Johannes Gabriel Gran , rector of the University of Turku .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","discovered","on","22","February","1938",",","by","Finnish","astronomer","Yrj","Visl","at","Turku","Observatory","in","Southwest","Finland",",","and","later","named","for","Johannes","Gabriel","Gran",",","rector","of","the","University","of","Turku","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-location","I-location","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","O","O","O","B-university","I-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","organization","chemical_element","enzyme","event","university","theory","protein","location","discipline","scientist","country","astronomical_object","person","academic_journal","award"]}
{"id":"484","dataset":"crossner_science","split":"test","instance":{"id":"484","prompt_labels":"Her(O) research(O) group(O) demonstrated(O) that(O) one(O) such(O) small(O) protein(O) ,(O) AcrZ(B-protein) ,(O) binds(O) to(O) the(O) multidrug(O) efflux(O) pump(O) protein(O) Acriflavine(O) resistance(O) protein(O) family(O) to(O) affect(O) its(O) ability(O) to(O) export(O) certain(O) classes(O) of(O) antibiotics(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, discipline, event, astronomical object, chemical compound, enzyme, protein, organization, scientist, award, academic journal, person, location, theory, country, university and O.\nSentence: Her research group demonstrated that one such small protein , AcrZ , binds to the multidrug efflux pump protein Acriflavine resistance protein family to affect its ability to export certain classes of antibiotics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Her","research","group","demonstrated","that","one","such","small","protein",",","AcrZ",",","binds","to","the","multidrug","efflux","pump","protein","Acriflavine","resistance","protein","family","to","affect","its","ability","to","export","certain","classes","of","antibiotics","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-protein","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","discipline","event","astronomical_object","chemical_compound","enzyme","protein","organization","scientist","award","academic_journal","person","location","theory","country","university"]}
{"id":"485","dataset":"crossner_science","split":"test","instance":{"id":"485","prompt_labels":"Starling(B-scientist) 's(O) successor(O) ,(O) Archibald(B-scientist) Hill(I-scientist) ,(O) fostered(O) the(O) career(O) of(O) Bernard(B-scientist) Katz(I-scientist) ,(O) whose(O) long(O) association(O) with(O) UCL(B-university) began(O) in(O) 1935(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, event, protein, discipline, organization, location, chemical element, astronomical object, chemical compound, scientist, academic journal, country, theory, enzyme, award, person and O.\nSentence: Starling 's successor , Archibald Hill , fostered the career of Bernard Katz , whose long association with UCL began in 1935 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Starling","'s","successor",",","Archibald","Hill",",","fostered","the","career","of","Bernard","Katz",",","whose","long","association","with","UCL","began","in","1935","."],"labels":["B-scientist","O","O","O","B-scientist","I-scientist","O","O","O","O","O","B-scientist","I-scientist","O","O","O","O","O","B-university","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","event","protein","discipline","organization","location","chemical_element","astronomical_object","chemical_compound","scientist","academic_journal","country","theory","enzyme","award","person"]}
{"id":"487","dataset":"crossner_science","split":"test","instance":{"id":"487","prompt_labels":"Though(O) Mendeleev(B-scientist) was(O) widely(O) honored(O) by(O) scientific(O) organizations(O) all(O) over(O) Europe(B-location) ,(O) including(O) ((O) in(O) 1882(O) )(O) the(O) Davy(B-award) Medal(I-award) from(I-award) the(I-award) Royal(I-award) Society(I-award) of(I-award) London(I-award) ((O) which(O) later(O) also(O) awarded(O) him(O) the(O) Copley(B-award) Medal(I-award) in(O) 1905(O) )(O) ,(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, discipline, event, location, scientist, chemical compound, protein, enzyme, country, chemical element, award, theory, university, organization, academic journal, astronomical object and O.\nSentence: Though Mendeleev was widely honored by scientific organizations all over Europe , including ( in 1882 ) the Davy Medal from the Royal Society of London ( which later also awarded him the Copley Medal in 1905 ) ,","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Though","Mendeleev","was","widely","honored","by","scientific","organizations","all","over","Europe",",","including","(","in","1882",")","the","Davy","Medal","from","the","Royal","Society","of","London","(","which","later","also","awarded","him","the","Copley","Medal","in","1905",")",","],"labels":["O","B-scientist","O","O","O","O","O","O","O","O","B-location","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","B-award","I-award","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","discipline","event","location","scientist","chemical_compound","protein","enzyme","country","chemical_element","award","theory","university","organization","academic_journal","astronomical_object"]}
{"id":"489","dataset":"crossner_science","split":"test","instance":{"id":"489","prompt_labels":"The(O) results(O) of(O) his(O) preliminary(O) investigations(O) were(O) published(O) in(O) the(O) journal(O) Physikalische(B-academic journal) Zeitschrift(I-academic journal) in(O) 1916(O) ((O) a(O) full(O) paper(O) was(O) later(O) published(O) in(O) Zeitschrift(B-academic journal) fr(I-academic journal) Physik(I-academic journal) in(O) 1922(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, enzyme, event, theory, person, astronomical object, scientist, protein, university, academic journal, chemical element, award, discipline, country, location, chemical compound and O.\nSentence: The results of his preliminary investigations were published in the journal Physikalische Zeitschrift in 1916 ( a full paper was later published in Zeitschrift fr Physik in 1922 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","results","of","his","preliminary","investigations","were","published","in","the","journal","Physikalische","Zeitschrift","in","1916","(","a","full","paper","was","later","published","in","Zeitschrift","fr","Physik","in","1922",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","enzyme","event","theory","person","astronomical_object","scientist","protein","university","academic_journal","chemical_element","award","discipline","country","location","chemical_compound"]}
{"id":"490","dataset":"crossner_science","split":"test","instance":{"id":"490","prompt_labels":"Hund(B-scientist) worked(O) at(O) the(O) Universities(O) of(O) University(B-university) of(I-university) Rostock(I-university) ,(O) Leipzig(B-location) ,(O) University(B-university) of(I-university) Jena(I-university) ,(O) Frankfurt(B-location) am(I-location) Main(I-location) ,(O) and(O) University(B-university) of(I-university) Gttingen(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, person, country, enzyme, academic journal, chemical element, protein, discipline, event, theory, university, scientist, astronomical object, location, chemical compound and O.\nSentence: Hund worked at the Universities of University of Rostock , Leipzig , University of Jena , Frankfurt am Main , and University of Gttingen .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Hund","worked","at","the","Universities","of","University","of","Rostock",",","Leipzig",",","University","of","Jena",",","Frankfurt","am","Main",",","and","University","of","Gttingen","."],"labels":["B-scientist","O","O","O","O","O","B-university","I-university","I-university","O","B-location","O","B-university","I-university","I-university","O","B-location","I-location","I-location","O","O","B-university","I-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["organization","award","person","country","enzyme","academic_journal","chemical_element","protein","discipline","event","theory","university","scientist","astronomical_object","location","chemical_compound"]}
{"id":"492","dataset":"crossner_science","split":"test","instance":{"id":"492","prompt_labels":"For(O) example(O) ,(O) calcium(O) antagonist(O) s(O) ,(O) which(O) are(O) used(O) to(O) treat(O) coronary(O) heart(O) disease(O) ,(O) hypertension(O) ,(O) and(O) cardiac(O) arrhythmia(O) ,(O) act(O) by(O) inhibiting(O) L-type(O) calcium(O) channel(O) or(O) T-type(O) calcium(O) channel(O) calcium(B-chemical element) channels(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, protein, enzyme, astronomical object, organization, chemical compound, location, scientist, academic journal, country, discipline, theory, person, chemical element, university and O.\nSentence: For example , calcium antagonist s , which are used to treat coronary heart disease , hypertension , and cardiac arrhythmia , act by inhibiting L-type calcium channel or T-type calcium channel calcium channels .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","example",",","calcium","antagonist","s",",","which","are","used","to","treat","coronary","heart","disease",",","hypertension",",","and","cardiac","arrhythmia",",","act","by","inhibiting","L-type","calcium","channel","or","T-type","calcium","channel","calcium","channels","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical element","O","O"],"target_index":null,"target_label":null},"label_list":["event","award","protein","enzyme","astronomical_object","organization","chemical_compound","location","scientist","academic_journal","country","discipline","theory","person","chemical_element","university"]}
{"id":"493","dataset":"crossner_science","split":"test","instance":{"id":"493","prompt_labels":"The(O) report(O) on(O) the(O) B(O) 8(O) experiment(O) was(O) written(O) by(O) Fritz(B-scientist) Bopp(I-scientist) ,(O) Erich(B-scientist) Fischer(I-scientist) ,(O) Werner(B-scientist) Heisenberg(I-scientist) ,(O) and(O) Karl(B-scientist) Wirtz(I-scientist) from(O) the(O) KWIP(B-organization) and(O) Walther(B-scientist) Bothe(I-scientist) ,(O) Peter(B-scientist) Jensen(I-scientist) ,(O) and(O) Oskar(B-scientist) Ritter(I-scientist) from(O) the(O) KWImF.Hentschel(B-organization) and(O) Hentschel(B-location) ,(O) 1996(O) ,(O) 377(O) and(O) 377n73(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, location, protein, scientist, enzyme, country, event, discipline, person, award, astronomical object, theory, chemical element, organization, academic journal, chemical compound and O.\nSentence: The report on the B 8 experiment was written by Fritz Bopp , Erich Fischer , Werner Heisenberg , and Karl Wirtz from the KWIP and Walther Bothe , Peter Jensen , and Oskar Ritter from the KWImF.Hentschel and Hentschel , 1996 , 377 and 377n73 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","report","on","the","B","8","experiment","was","written","by","Fritz","Bopp",",","Erich","Fischer",",","Werner","Heisenberg",",","and","Karl","Wirtz","from","the","KWIP","and","Walther","Bothe",",","Peter","Jensen",",","and","Oskar","Ritter","from","the","KWImF.Hentschel","and","Hentschel",",","1996",",","377","and","377n73","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","O","O","B-organization","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","O","O","B-organization","O","B-location","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","location","protein","scientist","enzyme","country","event","discipline","person","award","astronomical_object","theory","chemical_element","organization","academic_journal","chemical_compound"]}
{"id":"494","dataset":"crossner_science","split":"test","instance":{"id":"494","prompt_labels":"In(O) the(O) 1960s(O) ,(O) Ehlers(B-scientist) collaborated(O) with(O) Felix(B-scientist) Pirani(I-scientist) and(O) Alfred(B-scientist) Schild(I-scientist) on(O) a(O) constructive-axiomatic(O) approach(O) to(O) general(B-theory) relativity(I-theory) :(O) a(O) way(O) of(O) deriving(O) the(O) theory(O) from(O) a(O) minimal(O) set(O) of(O) elementary(O) objects(O) and(O) a(O) set(O) of(O) axioms(O) specifying(O) these(O) objects(O) '(O) properties(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, location, academic journal, astronomical object, person, enzyme, protein, award, discipline, event, theory, chemical element, chemical compound, university, scientist and O.\nSentence: In the 1960s , Ehlers collaborated with Felix Pirani and Alfred Schild on a constructive-axiomatic approach to general relativity : a way of deriving the theory from a minimal set of elementary objects and a set of axioms specifying these objects ' properties .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","1960s",",","Ehlers","collaborated","with","Felix","Pirani","and","Alfred","Schild","on","a","constructive-axiomatic","approach","to","general","relativity",":","a","way","of","deriving","the","theory","from","a","minimal","set","of","elementary","objects","and","a","set","of","axioms","specifying","these","objects","'","properties","."],"labels":["O","O","O","O","B-scientist","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O","O","B-theory","I-theory","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","country","location","academic_journal","astronomical_object","person","enzyme","protein","award","discipline","event","theory","chemical_element","chemical_compound","university","scientist"]}
{"id":"495","dataset":"crossner_science","split":"test","instance":{"id":"495","prompt_labels":"Amphitrite(B-astronomical object) ((O) minor(O) planet(O) designation(O) :(O) 29(B-astronomical object) Amphridite(I-astronomical object) )(O) is(O) one(O) of(O) the(O) largest(O) S-type(O) asteroid(O) s(O) ,(O) approximately(O) in(O) diameter(O) ,(O) and(O) probably(O) third(O) largest(O) after(O) 15(B-astronomical object) Eunomia(I-astronomical object) and(O) 3(B-astronomical object) Juno(I-astronomical object) ,(O) although(O) 7(B-astronomical object) Iris(I-astronomical object) and(O) 532(B-astronomical object) Herculina(I-astronomical object) are(O) similar(O) in(O) size(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, academic journal, chemical compound, enzyme, discipline, astronomical object, organization, theory, award, protein, person, university, country, chemical element, event, scientist and O.\nSentence: Amphitrite ( minor planet designation : 29 Amphridite ) is one of the largest S-type asteroid s , approximately in diameter , and probably third largest after 15 Eunomia and 3 Juno , although 7 Iris and 532 Herculina are similar in size .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Amphitrite","(","minor","planet","designation",":","29","Amphridite",")","is","one","of","the","largest","S-type","asteroid","s",",","approximately","in","diameter",",","and","probably","third","largest","after","15","Eunomia","and","3","Juno",",","although","7","Iris","and","532","Herculina","are","similar","in","size","."],"labels":["B-astronomical object","O","O","O","O","O","B-astronomical object","I-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","academic_journal","chemical_compound","enzyme","discipline","astronomical_object","organization","theory","award","protein","person","university","country","chemical_element","event","scientist"]}
{"id":"499","dataset":"crossner_science","split":"test","instance":{"id":"499","prompt_labels":"There(O) ,(O) he(O) worked(O) with(O) scientists(O) including(O) Luis(B-scientist) Walter(I-scientist) Alvarez(I-scientist) ,(O) John(B-scientist) von(I-scientist) Neumann(I-scientist) and(O) George(B-scientist) Kistiakowsky(I-scientist) on(O) the(O) development(O) of(O) the(O) high(O) explosives(O) and(O) exploding-bridgewire(O) detonator(O) s(O) required(O) by(O) atomic(O) bomb(O) s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, event, country, academic journal, organization, award, chemical compound, enzyme, discipline, scientist, protein, location, chemical element, person, astronomical object, university and O.\nSentence: There , he worked with scientists including Luis Walter Alvarez , John von Neumann and George Kistiakowsky on the development of the high explosives and exploding-bridgewire detonator s required by atomic bomb s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["There",",","he","worked","with","scientists","including","Luis","Walter","Alvarez",",","John","von","Neumann","and","George","Kistiakowsky","on","the","development","of","the","high","explosives","and","exploding-bridgewire","detonator","s","required","by","atomic","bomb","s","."],"labels":["O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["theory","event","country","academic_journal","organization","award","chemical_compound","enzyme","discipline","scientist","protein","location","chemical_element","person","astronomical_object","university"]}
{"id":"500","dataset":"crossner_science","split":"test","instance":{"id":"500","prompt_labels":"He(O) has(O) collaborated(O) with(O) ,(O) among(O) others(O) ,(O) Luciano(B-scientist) Pietronero(I-scientist) ,(O) Benoit(B-scientist) Mandelbrot(I-scientist) ,(O) Betz(B-scientist) Halloran(I-scientist) ,(O) Ira(B-scientist) Longini(I-scientist) ,(O) and(O) David(B-scientist) Lazer(I-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, theory, country, person, scientist, event, chemical element, location, university, protein, discipline, chemical compound, astronomical object, academic journal, award, enzyme and O.\nSentence: He has collaborated with , among others , Luciano Pietronero , Benoit Mandelbrot , Betz Halloran , Ira Longini , and David Lazer .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","has","collaborated","with",",","among","others",",","Luciano","Pietronero",",","Benoit","Mandelbrot",",","Betz","Halloran",",","Ira","Longini",",","and","David","Lazer","."],"labels":["O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","O"],"target_index":null,"target_label":null},"label_list":["organization","theory","country","person","scientist","event","chemical_element","location","university","protein","discipline","chemical_compound","astronomical_object","academic_journal","award","enzyme"]}
{"id":"502","dataset":"crossner_science","split":"test","instance":{"id":"502","prompt_labels":"The(O) Nobel(B-award) Prize(I-award) in(I-award) Chemistry(I-award) 1997(O) was(O) divided(O) ,(O) one(O) half(O) jointly(O) to(O) Paul(B-scientist) D.(I-scientist) Boyer(I-scientist) and(O) John(B-scientist) E.(I-scientist) Walker(I-scientist) for(O) their(O) elucidation(O) of(O) the(O) enzymatic(O) mechanism(O) underlying(O) the(O) synthesis(O) of(O) adenosine(B-chemical compound) triphosphate(I-chemical compound) ((O) ATP(B-chemical compound) )(O) and(O) the(O) other(O) half(O) to(O) Jens(B-scientist) C.(I-scientist) Skou(I-scientist) for(O) the(O) first(O) discovery(O) of(O) an(O) ion-transporting(B-enzyme) enzyme(I-enzyme) ,(O) Na(O) +(O) ,(O) K(O) +(O) -ATPase(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, scientist, chemical compound, award, location, theory, person, event, astronomical object, discipline, country, university, chemical element, academic journal, enzyme, organization and O.\nSentence: The Nobel Prize in Chemistry 1997 was divided , one half jointly to Paul D. Boyer and John E. Walker for their elucidation of the enzymatic mechanism underlying the synthesis of adenosine triphosphate ( ATP ) and the other half to Jens C. Skou for the first discovery of an ion-transporting enzyme , Na + , K + -ATPase .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Nobel","Prize","in","Chemistry","1997","was","divided",",","one","half","jointly","to","Paul","D.","Boyer","and","John","E.","Walker","for","their","elucidation","of","the","enzymatic","mechanism","underlying","the","synthesis","of","adenosine","triphosphate","(","ATP",")","and","the","other","half","to","Jens","C.","Skou","for","the","first","discovery","of","an","ion-transporting","enzyme",",","Na","+",",","K","+","-ATPase","."],"labels":["O","B-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","I-chemical compound","O","B-chemical compound","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","O","O","O","O","O","B-enzyme","I-enzyme","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["protein","scientist","chemical_compound","award","location","theory","person","event","astronomical_object","discipline","country","university","chemical_element","academic_journal","enzyme","organization"]}
{"id":"505","dataset":"crossner_science","split":"test","instance":{"id":"505","prompt_labels":"Some(O) notable(O) examples(O) include(O) Journal(B-academic journal) of(I-academic journal) Computational(I-academic journal) Biology(I-academic journal) and(O) PLOS(B-academic journal) Computational(I-academic journal) Biology(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, protein, location, award, enzyme, event, organization, chemical compound, scientist, astronomical object, discipline, theory, university, country, academic journal, person and O.\nSentence: Some notable examples include Journal of Computational Biology and PLOS Computational Biology .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","notable","examples","include","Journal","of","Computational","Biology","and","PLOS","Computational","Biology","."],"labels":["O","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","protein","location","award","enzyme","event","organization","chemical_compound","scientist","astronomical_object","discipline","theory","university","country","academic_journal","person"]}
{"id":"509","dataset":"crossner_science","split":"test","instance":{"id":"509","prompt_labels":"Two(O) American(O) explorers(O) claimed(O) to(O) reach(O) the(O) North(B-location) Pole(I-location) ;(O) Frederick(B-person) Cook(I-person) in(O) 1908(O) and(O) Robert(B-person) Peary(I-person) in(O) 1909(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, enzyme, academic journal, location, protein, theory, scientist, country, chemical compound, university, event, organization, award, astronomical object, chemical element, person and O.\nSentence: Two American explorers claimed to reach the North Pole ; Frederick Cook in 1908 and Robert Peary in 1909 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Two","American","explorers","claimed","to","reach","the","North","Pole",";","Frederick","Cook","in","1908","and","Robert","Peary","in","1909","."],"labels":["O","O","O","O","O","O","O","B-location","I-location","O","B-person","I-person","O","O","O","B-person","I-person","O","O","O"],"target_index":null,"target_label":null},"label_list":["discipline","enzyme","academic_journal","location","protein","theory","scientist","country","chemical_compound","university","event","organization","award","astronomical_object","chemical_element","person"]}
{"id":"510","dataset":"crossner_science","split":"test","instance":{"id":"510","prompt_labels":"A(O) number(O) of(O) Pluto(B-astronomical object) -as-planet(O) proponents(O) ,(O) in(O) particular(O) Alan(B-scientist) Stern(I-scientist) ,(O) head(O) of(O) NASA(B-organization) 's(O) New(O) Horizons(O) mission(O) to(O) Pluto(B-astronomical object) ,(O) have(O) circulated(O) a(O) petition(O) among(O) astronomers(O) to(O) alter(O) the(O) definition(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, discipline, astronomical object, protein, enzyme, academic journal, organization, award, event, chemical element, person, theory, country, chemical compound, university, location and O.\nSentence: A number of Pluto -as-planet proponents , in particular Alan Stern , head of NASA 's New Horizons mission to Pluto , have circulated a petition among astronomers to alter the definition .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","number","of","Pluto","-as-planet","proponents",",","in","particular","Alan","Stern",",","head","of","NASA","'s","New","Horizons","mission","to","Pluto",",","have","circulated","a","petition","among","astronomers","to","alter","the","definition","."],"labels":["O","O","O","B-astronomical object","O","O","O","O","O","B-scientist","I-scientist","O","O","O","B-organization","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","discipline","astronomical_object","protein","enzyme","academic_journal","organization","award","event","chemical_element","person","theory","country","chemical_compound","university","location"]}
{"id":"511","dataset":"crossner_science","split":"test","instance":{"id":"511","prompt_labels":"In(O) 1972(O) ,(O) LaHaye(B-person) helped(O) establish(O) the(O) Institute(B-organization) for(I-organization) Creation(I-organization) Research(I-organization) at(O) San(B-university) Diego(I-university) Christian(I-university) College(I-university) in(O) El(B-location) Cajon(I-location) ,(O) California(B-location) ,(O) along(O) with(O) Henry(B-person) M.(I-person) Morris(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, person, discipline, protein, country, event, academic journal, organization, award, enzyme, chemical element, chemical compound, location, astronomical object, university, theory and O.\nSentence: In 1972 , LaHaye helped establish the Institute for Creation Research at San Diego Christian College in El Cajon , California , along with Henry M. Morris .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1972",",","LaHaye","helped","establish","the","Institute","for","Creation","Research","at","San","Diego","Christian","College","in","El","Cajon",",","California",",","along","with","Henry","M.","Morris","."],"labels":["O","O","O","B-person","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-university","I-university","I-university","I-university","O","B-location","I-location","O","B-location","O","O","O","B-person","I-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["scientist","person","discipline","protein","country","event","academic_journal","organization","award","enzyme","chemical_element","chemical_compound","location","astronomical_object","university","theory"]}
{"id":"512","dataset":"crossner_science","split":"test","instance":{"id":"512","prompt_labels":"Derek(B-scientist) Abbott(I-scientist) ,(O) Paul(B-scientist) C.W.(I-scientist) Davies(I-scientist) ,(O) and(O) Arun(B-scientist) K.(I-scientist) Pati(I-scientist) ,(O) Quantum(O) Aspects(O) of(O) Life(O) ,(O) Imperial(B-organization) College(I-organization) Press(I-organization) ,(O) 2008(O) ,(O) p(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, astronomical object, university, chemical compound, scientist, country, enzyme, person, academic journal, organization, theory, event, discipline, chemical element, location, award and O.\nSentence: Derek Abbott , Paul C.W. Davies , and Arun K. Pati , Quantum Aspects of Life , Imperial College Press , 2008 , p .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Derek","Abbott",",","Paul","C.W.","Davies",",","and","Arun","K.","Pati",",","Quantum","Aspects","of","Life",",","Imperial","College","Press",",","2008",",","p","."],"labels":["B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","O","B-scientist","I-scientist","I-scientist","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["protein","astronomical_object","university","chemical_compound","scientist","country","enzyme","person","academic_journal","organization","theory","event","discipline","chemical_element","location","award"]}
{"id":"515","dataset":"crossner_science","split":"test","instance":{"id":"515","prompt_labels":"Mary(B-person) Baker(I-person) Eddy(I-person) made(O) the(O) Boston(B-location) Mother(I-location) Church(I-location) of(I-location) Christian(I-location) Science(I-location) the(O) world(O) headquarters(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, enzyme, organization, astronomical object, university, scientist, discipline, theory, award, chemical compound, person, academic journal, location, protein, chemical element and O.\nSentence: Mary Baker Eddy made the Boston Mother Church of Christian Science the world headquarters .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Mary","Baker","Eddy","made","the","Boston","Mother","Church","of","Christian","Science","the","world","headquarters","."],"labels":["B-person","I-person","I-person","O","O","B-location","I-location","I-location","I-location","I-location","I-location","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","event","enzyme","organization","astronomical_object","university","scientist","discipline","theory","award","chemical_compound","person","academic_journal","location","protein","chemical_element"]}
{"id":"517","dataset":"crossner_science","split":"test","instance":{"id":"517","prompt_labels":"cAMP(B-chemical compound) also(O) ,(O) activates(O) a(O) protein(O) called(O) protein(B-enzyme) kinase(I-enzyme) A(I-enzyme) ((O) Protein(B-enzyme) kinase(I-enzyme) A(I-enzyme) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, organization, award, chemical element, discipline, country, astronomical object, academic journal, enzyme, chemical compound, scientist, event, protein, location, theory, person and O.\nSentence: cAMP also , activates a protein called protein kinase A ( Protein kinase A ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["cAMP","also",",","activates","a","protein","called","protein","kinase","A","(","Protein","kinase","A",")","."],"labels":["B-chemical compound","O","O","O","O","O","O","B-enzyme","I-enzyme","I-enzyme","O","B-enzyme","I-enzyme","I-enzyme","O","O"],"target_index":null,"target_label":null},"label_list":["university","organization","award","chemical_element","discipline","country","astronomical_object","academic_journal","enzyme","chemical_compound","scientist","event","protein","location","theory","person"]}
{"id":"518","dataset":"crossner_science","split":"test","instance":{"id":"518","prompt_labels":"She(O) is(O) a(O) fellow(B-award) of(I-award) the(I-award) American(I-award) Association(I-award) for(I-award) the(I-award) Advancement(I-award) of(I-award) Science(I-award) ,(O) American(B-organization) Chemical(I-organization) Society(I-organization) and(O) SPIE(B-organization) and(O) serves(O) on(O) the(O) advisory(O) board(O) for(O) ACS(B-academic journal) Nano(I-academic journal) ,(O) Advanced(B-academic journal) Functional(I-academic journal) Materials(I-academic journal) ,(O) Advanced(B-academic journal) Energy(I-academic journal) Materials(I-academic journal) ,(O) Chemical(B-academic journal) Communications(I-academic journal) ,(O) Chemistry(B-academic journal) of(I-academic journal) Materials(I-academic journal) ,(O) Materials(B-academic journal) Today(I-academic journal) ,(O) Nanoscale(B-academic journal) ,(O) and(O) NPG(B-academic journal) Asia(I-academic journal) Materials(I-academic journal) and(O) the(O) board(O) of(O) directors(O) for(O) the(O) Materials(B-organization) Research(I-organization) Society(I-organization) and(O) the(O) Polymers(B-organization) Materials(I-organization) Science(I-organization) and(I-organization) Engineering(I-organization) division(I-organization) of(I-organization) the(I-organization) American(I-organization) Chemical(I-organization) Society(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, academic journal, protein, enzyme, university, organization, country, theory, person, scientist, award, discipline, location, event, astronomical object, chemical element and O.\nSentence: She is a fellow of the American Association for the Advancement of Science , American Chemical Society and SPIE and serves on the advisory board for ACS Nano , Advanced Functional Materials , Advanced Energy Materials , Chemical Communications , Chemistry of Materials , Materials Today , Nanoscale , and NPG Asia Materials and the board of directors for the Materials Research Society and the Polymers Materials Science and Engineering division of the American Chemical Society .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","is","a","fellow","of","the","American","Association","for","the","Advancement","of","Science",",","American","Chemical","Society","and","SPIE","and","serves","on","the","advisory","board","for","ACS","Nano",",","Advanced","Functional","Materials",",","Advanced","Energy","Materials",",","Chemical","Communications",",","Chemistry","of","Materials",",","Materials","Today",",","Nanoscale",",","and","NPG","Asia","Materials","and","the","board","of","directors","for","the","Materials","Research","Society","and","the","Polymers","Materials","Science","and","Engineering","division","of","the","American","Chemical","Society","."],"labels":["O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","O","O","B-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","academic_journal","protein","enzyme","university","organization","country","theory","person","scientist","award","discipline","location","event","astronomical_object","chemical_element"]}
{"id":"519","dataset":"crossner_science","split":"test","instance":{"id":"519","prompt_labels":"By(O) the(O) mid(O) 19th(O) century(O) ,(O) Edward(B-scientist) Frankland(I-scientist) ,(O) August(B-scientist) Kekul(I-scientist) ,(O) A.S.(B-scientist) Couper(I-scientist) ,(O) Alexander(B-scientist) Butlerov(I-scientist) ,(O) and(O) Hermann(B-scientist) Kolbe(I-scientist) ,(O) building(O) on(O) the(O) theory(B-theory) of(I-theory) radicals(I-theory) ,(O) developed(O) the(O) theory(B-theory) of(I-theory) valency(I-theory) ,(O) originally(O) called(O) combining(O) power(O) ,(O) in(O) which(O) compounds(O) were(O) joined(O) owing(O) to(O) an(O) attraction(O) of(O) positive(O) and(O) negative(O) poles(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, theory, academic journal, organization, astronomical object, university, scientist, event, chemical element, chemical compound, enzyme, protein, location, discipline, award, person and O.\nSentence: By the mid 19th century , Edward Frankland , August Kekul , A.S. Couper , Alexander Butlerov , and Hermann Kolbe , building on the theory of radicals , developed the theory of valency , originally called combining power , in which compounds were joined owing to an attraction of positive and negative poles .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["By","the","mid","19th","century",",","Edward","Frankland",",","August","Kekul",",","A.S.","Couper",",","Alexander","Butlerov",",","and","Hermann","Kolbe",",","building","on","the","theory","of","radicals",",","developed","the","theory","of","valency",",","originally","called","combining","power",",","in","which","compounds","were","joined","owing","to","an","attraction","of","positive","and","negative","poles","."],"labels":["O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","O","O","O","O","B-theory","I-theory","I-theory","O","O","O","B-theory","I-theory","I-theory","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","theory","academic_journal","organization","astronomical_object","university","scientist","event","chemical_element","chemical_compound","enzyme","protein","location","discipline","award","person"]}
{"id":"520","dataset":"crossner_science","split":"test","instance":{"id":"520","prompt_labels":"This(O) interaction(O) stabilizes(O) both(O) TOC1(O) and(O) PRR5(O) and(O) prevents(O) their(O) degradation(O) by(O) the(O) F-box(B-protein) protein(I-protein) ZEITLUPE(B-protein) ((O) ZTL(B-protein) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, academic journal, location, scientist, enzyme, astronomical object, award, person, chemical element, country, chemical compound, theory, organization, university, protein, event and O.\nSentence: This interaction stabilizes both TOC1 and PRR5 and prevents their degradation by the F-box protein ZEITLUPE ( ZTL ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","interaction","stabilizes","both","TOC1","and","PRR5","and","prevents","their","degradation","by","the","F-box","protein","ZEITLUPE","(","ZTL",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-protein","I-protein","B-protein","O","B-protein","O","O"],"target_index":null,"target_label":null},"label_list":["discipline","academic_journal","location","scientist","enzyme","astronomical_object","award","person","chemical_element","country","chemical_compound","theory","organization","university","protein","event"]}
{"id":"521","dataset":"crossner_science","split":"test","instance":{"id":"521","prompt_labels":"The(O) strong(O) lensing(O) was(O) predicted(O) by(O) Albert(B-scientist) Einstein(I-scientist) '(O) s(O) general(B-theory) theory(I-theory) of(I-theory) relativity(I-theory) and(O) observationally(O) discovered(O) by(O) Dennis(B-scientist) Walsh(I-scientist) ,(O) Bob(B-scientist) Carswell(I-scientist) ,(O) and(O) Ray(B-scientist) Weymann(I-scientist) in(O) 1979(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, person, discipline, theory, organization, chemical element, country, university, enzyme, astronomical object, location, academic journal, award, event, scientist, protein and O.\nSentence: The strong lensing was predicted by Albert Einstein ' s general theory of relativity and observationally discovered by Dennis Walsh , Bob Carswell , and Ray Weymann in 1979 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","strong","lensing","was","predicted","by","Albert","Einstein","'","s","general","theory","of","relativity","and","observationally","discovered","by","Dennis","Walsh",",","Bob","Carswell",",","and","Ray","Weymann","in","1979","."],"labels":["O","O","O","O","O","O","B-scientist","I-scientist","O","O","B-theory","I-theory","I-theory","I-theory","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","person","discipline","theory","organization","chemical_element","country","university","enzyme","astronomical_object","location","academic_journal","award","event","scientist","protein"]}
{"id":"523","dataset":"crossner_science","split":"test","instance":{"id":"523","prompt_labels":"It(O) is(O) one(O) of(O) the(O) five(O) Nobel(B-award) Prize(I-award) s(O) established(O) by(O) the(O) will(O) of(O) Alfred(B-scientist) Nobel(I-scientist) in(O) 1895(O) and(O) awarded(O) since(O) 1901(O) ;(O) the(O) others(O) being(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Chemistry(I-award) ,(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) ,(O) Nobel(B-award) Peace(I-award) Prize(I-award) ,(O) and(O) Nobel(B-award) Prize(I-award) in(I-award) Physiology(I-award) or(I-award) Medicine(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, organization, discipline, chemical element, academic journal, award, astronomical object, protein, person, chemical compound, location, university, event, country, enzyme, theory and O.\nSentence: It is one of the five Nobel Prize s established by the will of Alfred Nobel in 1895 and awarded since 1901 ; the others being the Nobel Prize in Chemistry , Nobel Prize in Literature , Nobel Peace Prize , and Nobel Prize in Physiology or Medicine .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","is","one","of","the","five","Nobel","Prize","s","established","by","the","will","of","Alfred","Nobel","in","1895","and","awarded","since","1901",";","the","others","being","the","Nobel","Prize","in","Chemistry",",","Nobel","Prize","in","Literature",",","Nobel","Peace","Prize",",","and","Nobel","Prize","in","Physiology","or","Medicine","."],"labels":["O","O","O","O","O","O","B-award","I-award","O","O","O","O","O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","O","B-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["scientist","organization","discipline","chemical_element","academic_journal","award","astronomical_object","protein","person","chemical_compound","location","university","event","country","enzyme","theory"]}
{"id":"526","dataset":"crossner_science","split":"test","instance":{"id":"526","prompt_labels":"Phosphoinositide(O) signaling(O) is(O) also(O) altered(O) ,(O) with(O) elevated(O) levels(O) of(O) phospholipase(B-enzyme) C(I-enzyme) ,(O) protein(B-enzyme) kinase(I-enzyme) C(I-enzyme) ,(O) and(O) Gq(B-protein) alpha(I-protein) subunit(I-protein) being(O) reported(O) in(O) bipolar(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, scientist, enzyme, location, event, organization, protein, award, country, astronomical object, theory, chemical element, chemical compound, university, academic journal, discipline and O.\nSentence: Phosphoinositide signaling is also altered , with elevated levels of phospholipase C , protein kinase C , and Gq alpha subunit being reported in bipolar .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Phosphoinositide","signaling","is","also","altered",",","with","elevated","levels","of","phospholipase","C",",","protein","kinase","C",",","and","Gq","alpha","subunit","being","reported","in","bipolar","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-enzyme","I-enzyme","O","B-enzyme","I-enzyme","I-enzyme","O","O","B-protein","I-protein","I-protein","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","scientist","enzyme","location","event","organization","protein","award","country","astronomical_object","theory","chemical_element","chemical_compound","university","academic_journal","discipline"]}
{"id":"527","dataset":"crossner_science","split":"test","instance":{"id":"527","prompt_labels":"The(O) journal(O) has(O) been(O) cited(O) most(O) often(O) by(O) the(O) following(O) journals(O) Journal(B-academic journal) of(I-academic journal) Clinical(I-academic journal) Pathology(I-academic journal) ,(O) Histopathology(B-academic journal) ,(O) Archives(B-academic journal) of(I-academic journal) Pathology(I-academic journal) ;(O) Laboratory(B-academic journal) Medicine(I-academic journal) ,(O) Human(B-academic journal) Pathology(I-academic journal) ,(O) and(O) the(O) World(B-academic journal) Journal(I-academic journal) of(I-academic journal) Gastroenterology(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, discipline, chemical element, chemical compound, university, location, enzyme, person, organization, country, theory, award, event, scientist, astronomical object, protein and O.\nSentence: The journal has been cited most often by the following journals Journal of Clinical Pathology , Histopathology , Archives of Pathology ; Laboratory Medicine , Human Pathology , and the World Journal of Gastroenterology .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","journal","has","been","cited","most","often","by","the","following","journals","Journal","of","Clinical","Pathology",",","Histopathology",",","Archives","of","Pathology",";","Laboratory","Medicine",",","Human","Pathology",",","and","the","World","Journal","of","Gastroenterology","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","discipline","chemical_element","chemical_compound","university","location","enzyme","person","organization","country","theory","award","event","scientist","astronomical_object","protein"]}
{"id":"529","dataset":"crossner_science","split":"test","instance":{"id":"529","prompt_labels":"From(O) 1917(O) ,(O) he(O) worked(O) a(O) few(O) years(O) with(O) Niels(B-scientist) Bohr(I-scientist) in(O) the(O) University(B-university) of(I-university) Copenhagen(I-university) and(O) received(O) his(O) doctoral(O) degree(O) at(O) the(O) University(B-university) College(I-university) of(I-university) Stockholm(I-university) ((O) now(O) Stockholm(B-university) University(I-university) )(O) in(O) 1921(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, chemical element, enzyme, protein, discipline, scientist, organization, theory, astronomical object, person, chemical compound, award, country, university, academic journal, event and O.\nSentence: From 1917 , he worked a few years with Niels Bohr in the University of Copenhagen and received his doctoral degree at the University College of Stockholm ( now Stockholm University ) in 1921 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["From","1917",",","he","worked","a","few","years","with","Niels","Bohr","in","the","University","of","Copenhagen","and","received","his","doctoral","degree","at","the","University","College","of","Stockholm","(","now","Stockholm","University",")","in","1921","."],"labels":["O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","O","B-university","I-university","I-university","O","O","O","O","O","O","O","B-university","I-university","I-university","I-university","O","O","B-university","I-university","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","chemical_element","enzyme","protein","discipline","scientist","organization","theory","astronomical_object","person","chemical_compound","award","country","university","academic_journal","event"]}
{"id":"531","dataset":"crossner_science","split":"test","instance":{"id":"531","prompt_labels":"Type(B-astronomical object) II(I-astronomical object) supernovae(I-astronomical object) mainly(O) synthesize(O) oxygen(B-chemical element) and(O) the(O) alpha-elements(O) ((O) Ne(B-chemical element) ,(O) Mg(B-chemical element) ,(O) Si(B-chemical element) ,(O) S(B-chemical element) ,(O) Ar(B-chemical element) ,(O) Ca(B-chemical element) and(O) Ti(B-chemical element) )(O) while(O) Type(B-astronomical object) Ia(I-astronomical object) supernova(I-astronomical object) e(O) mainly(O) produce(O) elements(O) of(O) the(O) iron(O) peak(O) ((O) Ti(B-chemical element) ,(O) V(B-chemical element) ,(O) Cr(B-chemical element) ,(O) Manganese(B-chemical element) ,(O) Fe(B-chemical element) ,(O) Cobalt(B-chemical element) and(O) Nickel(B-chemical element) )(O) but(O) also(O) alpha-elements(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, astronomical object, discipline, enzyme, chemical element, country, location, university, event, chemical compound, theory, person, academic journal, scientist, award, organization and O.\nSentence: Type II supernovae mainly synthesize oxygen and the alpha-elements ( Ne , Mg , Si , S , Ar , Ca and Ti ) while Type Ia supernova e mainly produce elements of the iron peak ( Ti , V , Cr , Manganese , Fe , Cobalt and Nickel ) but also alpha-elements .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Type","II","supernovae","mainly","synthesize","oxygen","and","the","alpha-elements","(","Ne",",","Mg",",","Si",",","S",",","Ar",",","Ca","and","Ti",")","while","Type","Ia","supernova","e","mainly","produce","elements","of","the","iron","peak","(","Ti",",","V",",","Cr",",","Manganese",",","Fe",",","Cobalt","and","Nickel",")","but","also","alpha-elements","."],"labels":["B-astronomical object","I-astronomical object","I-astronomical object","O","O","B-chemical element","O","O","O","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","O","B-astronomical object","I-astronomical object","I-astronomical object","O","O","O","O","O","O","O","O","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["protein","astronomical_object","discipline","enzyme","chemical_element","country","location","university","event","chemical_compound","theory","person","academic_journal","scientist","award","organization"]}
{"id":"534","dataset":"crossner_science","split":"test","instance":{"id":"534","prompt_labels":"He(O) then(O) stated(O) that(O) ,(O) based(O) on(O) his(O) data(O) on(O) various(O) comet(O) s(O) ,(O) the(O) size(O) of(O) the(O) Solar(O) system(O) can(O) be(O) 100(O) AU(O) or(O) even(O) more(O) ,(O) and(O) that(O) it(O) could(O) be(O) other(O) planet(O) s(O) there(O) that(O) perturb(O) the(O) orbit(O) of(O) Uranus(B-astronomical object) ((O) although(O) the(O) position(O) of(O) the(O) eventual(O) Neptune(B-astronomical object) was(O) not(O) calculated(O) until(O) much(O) later(O) by(O) Urbain(B-scientist) Le(I-scientist) Verrier(I-scientist) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, chemical element, event, astronomical object, theory, organization, location, scientist, person, country, chemical compound, academic journal, discipline, award, enzyme, university and O.\nSentence: He then stated that , based on his data on various comet s , the size of the Solar system can be 100 AU or even more , and that it could be other planet s there that perturb the orbit of Uranus ( although the position of the eventual Neptune was not calculated until much later by Urbain Le Verrier ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","then","stated","that",",","based","on","his","data","on","various","comet","s",",","the","size","of","the","Solar","system","can","be","100","AU","or","even","more",",","and","that","it","could","be","other","planet","s","there","that","perturb","the","orbit","of","Uranus","(","although","the","position","of","the","eventual","Neptune","was","not","calculated","until","much","later","by","Urbain","Le","Verrier",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","O"],"target_index":null,"target_label":null},"label_list":["protein","chemical_element","event","astronomical_object","theory","organization","location","scientist","person","country","chemical_compound","academic_journal","discipline","award","enzyme","university"]}
{"id":"535","dataset":"crossner_science","split":"test","instance":{"id":"535","prompt_labels":"At(O) Institute(B-organization) G(I-organization) were(O) Heinz(B-scientist) Barwich(I-scientist) ,(O) Werner(B-scientist) Hartmann(I-scientist) ,(O) and(O) Justus(B-scientist) Mhlenpfordt(I-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, person, organization, university, academic journal, scientist, astronomical object, discipline, chemical element, enzyme, location, country, theory, chemical compound, protein and O.\nSentence: At Institute G were Heinz Barwich , Werner Hartmann , and Justus Mhlenpfordt .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["At","Institute","G","were","Heinz","Barwich",",","Werner","Hartmann",",","and","Justus","Mhlenpfordt","."],"labels":["O","B-organization","I-organization","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","O"],"target_index":null,"target_label":null},"label_list":["event","award","person","organization","university","academic_journal","scientist","astronomical_object","discipline","chemical_element","enzyme","location","country","theory","chemical_compound","protein"]}
{"id":"536","dataset":"crossner_science","split":"test","instance":{"id":"536","prompt_labels":"He(O) was(O) a(O) member(O) of(O) the(O) Experimental(B-organization) Aircraft(I-organization) Association(I-organization) ,(O) the(O) Space(B-organization) Pioneers(I-organization) ,(O) the(O) Confederate(B-organization) Air(I-organization) Force(I-organization) ,(O) the(O) Order(B-organization) of(I-organization) Daedalians(I-organization) ,(O) the(O) National(B-organization) Rifle(I-organization) Association(I-organization) of(O) America(B-country) ,(O) the(O) Veterans(B-organization) of(I-organization) Foreign(I-organization) Wars(I-organization) ,(O) and(O) the(O) Fraternal(B-organization) Order(I-organization) of(I-organization) Eagles(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, astronomical object, award, scientist, academic journal, country, protein, chemical element, theory, chemical compound, event, location, university, organization, enzyme, person and O.\nSentence: He was a member of the Experimental Aircraft Association , the Space Pioneers , the Confederate Air Force , the Order of Daedalians , the National Rifle Association of America , the Veterans of Foreign Wars , and the Fraternal Order of Eagles .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","a","member","of","the","Experimental","Aircraft","Association",",","the","Space","Pioneers",",","the","Confederate","Air","Force",",","the","Order","of","Daedalians",",","the","National","Rifle","Association","of","America",",","the","Veterans","of","Foreign","Wars",",","and","the","Fraternal","Order","of","Eagles","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","B-country","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["discipline","astronomical_object","award","scientist","academic_journal","country","protein","chemical_element","theory","chemical_compound","event","location","university","organization","enzyme","person"]}
{"id":"538","dataset":"crossner_science","split":"test","instance":{"id":"538","prompt_labels":"The(O) staff(O) also(O) produce(O) articles(O) on(O) human(O) and(O) animal(O) genetics(B-discipline) topics(O) such(O) as(O) gene(O) splicing(O) ,(O) CRISPR(O) ,(O) government(O) regulation(O) ,(O) bioethics(B-discipline) ,(O) use(O) of(O) stem(O) cells(O) ,(O) transhumanism(B-theory) ,(O) nanotechnology(B-discipline) and(O) synthetic(B-discipline) biology(I-discipline) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, event, astronomical object, location, scientist, person, chemical compound, academic journal, award, enzyme, protein, chemical element, theory, discipline, organization, country and O.\nSentence: The staff also produce articles on human and animal genetics topics such as gene splicing , CRISPR , government regulation , bioethics , use of stem cells , transhumanism , nanotechnology and synthetic biology .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","staff","also","produce","articles","on","human","and","animal","genetics","topics","such","as","gene","splicing",",","CRISPR",",","government","regulation",",","bioethics",",","use","of","stem","cells",",","transhumanism",",","nanotechnology","and","synthetic","biology","."],"labels":["O","O","O","O","O","O","O","O","O","B-discipline","O","O","O","O","O","O","O","O","O","O","O","B-discipline","O","O","O","O","O","O","B-theory","O","B-discipline","O","B-discipline","I-discipline","O"],"target_index":null,"target_label":null},"label_list":["university","event","astronomical_object","location","scientist","person","chemical_compound","academic_journal","award","enzyme","protein","chemical_element","theory","discipline","organization","country"]}
{"id":"539","dataset":"crossner_science","split":"test","instance":{"id":"539","prompt_labels":"He(O) has(O) won(O) awards(O) from(O) the(O) American(B-organization) Psychological(I-organization) Association(I-organization) ,(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) the(O) Royal(B-organization) Institution(I-organization) ,(O) the(O) Cognitive(B-organization) Neuroscience(I-organization) Society(I-organization) and(O) the(O) American(B-organization) Humanist(I-organization) Association(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, event, country, award, chemical compound, person, chemical element, scientist, enzyme, organization, location, astronomical object, theory, university, protein, discipline and O.\nSentence: He has won awards from the American Psychological Association , the National Academy of Sciences , the Royal Institution , the Cognitive Neuroscience Society and the American Humanist Association .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","has","won","awards","from","the","American","Psychological","Association",",","the","National","Academy","of","Sciences",",","the","Royal","Institution",",","the","Cognitive","Neuroscience","Society","and","the","American","Humanist","Association","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","event","country","award","chemical_compound","person","chemical_element","scientist","enzyme","organization","location","astronomical_object","theory","university","protein","discipline"]}
{"id":"542","dataset":"crossner_science","split":"test","instance":{"id":"542","prompt_labels":"Mattauch(B-scientist) and(O) Fritz(B-scientist) Strassmann(I-scientist) actively(O) supported(O) the(O) proposed(O) appointment(O) of(O) Lise(B-scientist) Meitner(I-scientist) as(O) head(O) of(O) the(O) physics(B-organization) department(I-organization) of(I-organization) the(I-organization) University(I-organization) of(I-organization) Mainz(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, location, country, organization, award, person, chemical compound, scientist, protein, university, event, theory, enzyme, astronomical object, chemical element, discipline and O.\nSentence: Mattauch and Fritz Strassmann actively supported the proposed appointment of Lise Meitner as head of the physics department of the University of Mainz .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Mattauch","and","Fritz","Strassmann","actively","supported","the","proposed","appointment","of","Lise","Meitner","as","head","of","the","physics","department","of","the","University","of","Mainz","."],"labels":["B-scientist","O","B-scientist","I-scientist","O","O","O","O","O","O","B-scientist","I-scientist","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","location","country","organization","award","person","chemical_compound","scientist","protein","university","event","theory","enzyme","astronomical_object","chemical_element","discipline"]}
